<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.CV](#cs.CV) [Total: 72]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People](https://arxiv.org/abs/2510.20886)
*Gabriel Grand,Valerio Pepe,Jacob Andreas,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: The paper benchmarks LM agents in strategic information-seeking tasks and develops Bayesian Experimental Design methods to improve their rational decision-making, achieving significant performance gains over baseline LMs and even outperforming humans and frontier models.


<details>
  <summary>Details</summary>
Motivation: Many high-stakes AI applications require rational information-seeking and hypothesis formation, but current LM agents struggle with strategic decision-making in resource-constrained environments.

Method: Developed Collaborative Battleship task to test agent behavior, then created novel Monte Carlo inference strategies based on Bayesian Experimental Design principles for both Spotter (answer generation) and Captain (question selection) agents.

Result: BED methods boosted Spotter accuracy by 14.7%, increased Captain information gain by 0.227 bits (94.2% of noise ceiling), improved targeting by 0.303-0.374 F1, and enabled weaker LMs to outperform humans (82% win rate) and frontier models (67% win rate vs GPT-5) at 1% of GPT-5's cost.

Conclusion: The Bayesian Experimental Design approach significantly enhances LM agents' rational information-seeking capabilities and demonstrates general applicability across different strategic tasks like Guess Who? with substantial accuracy improvements.

Abstract: Many high-stakes applications of AI require forming data-driven hypotheses
and making targeted guesses; e.g., in scientific and diagnostic settings. Given
limited resources, to what extent do agents based on language models (LMs) act
rationally? We develop methods to benchmark and enhance agentic
information-seeking, drawing on insights from human behavior. First, we
introduce a strategic decision-oriented dialogue task called Collaborative
Battleship, in which a partially-informed Captain must balance exploration
(asking questions) and action (taking shots), while a fully-informed Spotter
must provide accurate answers under an information bottleneck. Compared to
human players (N=42), we find that LM agents struggle to ground answers in
context, generate informative questions, and select high-value actions. Next,
to address these gaps, we develop novel Monte Carlo inference strategies for
LMs based on principles from Bayesian Experimental Design (BED). For Spotter
agents, our approach boosts accuracy by up to 14.7% absolute over LM-only
baselines; for Captain agents, it raises expected information gain (EIG) by up
to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these
components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs,
such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and
frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. We
replicate these findings on Guess Who? where our methods significantly boost
accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for
building rational information-seeking agents.

</details>


### [2] [Code-enabled language models can outperform reasoning models on diverse tasks](https://arxiv.org/abs/2510.20909)
*Cedegao E. Zhang,Cédric Colas,Gabriel Poesia,Joshua B. Tenenbaum,Jacob Andreas*

Main category: cs.CL

TL;DR: CodeAdapt enables standard instruction LMs to achieve reasoning performance comparable to or better than specialized reasoning models without finetuning, using code-augmented reasoning and few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Reasoning models require extensive computation and data to train, and are slow/expensive to run. The paper aims to show that standard instruction LMs can be strong reasoners without finetuning.

Method: CodeAdapt combines CodeAct framework (interleaving natural language reasoning with code execution) with few-shot bootstrap in-context learning from just 5 training problems.

Result: CodeAdapt enabled 3 LMs to outperform corresponding reasoning models by up to 22.9% on average over 8 tasks while being 10-81% more token efficient, and delivered superior performance on 6 tasks when averaged over 4 models (up to 35.7%).

Conclusion: CodeAdapt-style learning is robust and domain general, and code-enabled LMs are cognitively grounded systems that provide a strong foundation for in-weight reinforcement learning.

Abstract: Reasoning models (RMs), language models (LMs) trained with reinforcement
learning to produce long-form natural language reasoning, have been remarkably
successful, but they still require large amounts of computation and data to
train, and can be slow and expensive to run. In this paper, we show that
standard instruct LMs can already be elicited to be strong reasoners at a level
comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs
R1) without finetuning, across diverse domains from instruction following and
creative generation to mathematical reasoning. This is achieved by CodeAdapt,
our simple recipe that combines the CodeAct framework, where LMs interleave
natural language reasoning with code execution in a multi-step fashion, with
few-shot bootstrap in-context learning from as few as five training problems.
Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables
three LMs to outperform the corresponding RMs on average over eight tasks (up
to 22.9%) while being 10-81% more token efficient, and delivers superior
performance on six tasks when averaged over the four models (up to 35.7%).
Furthermore, the code-augmented reasoning traces display rich and varied
problem-solving strategies. Our findings support that (1) CodeAdapt-style
learning and reasoning may be robust and domain general and (2) code-enabled
LMs are cognitively grounded and powerful systems, potentially providing a
strong foundation for in-weight reinforcement learning.

</details>


### [3] [FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction](https://arxiv.org/abs/2510.20926)
*Natasha Johnson,Amanda Bertsch,Maria-Emil Deal,Emma Strubell*

Main category: cs.CL

TL;DR: FICSIM is a new dataset for evaluating embedding models on long-form fiction similarity tasks, addressing limitations of existing datasets that focus on short texts and coarse-grained similarity.


<details>
  <summary>Details</summary>
Motivation: Current embedding similarity datasets are inadequate for computational literary studies due to focus on short texts and coarse-grained similarity, combined with data contamination concerns in public-domain literature.

Method: Assembled a dataset of long-form, recently written fiction with similarity scores along 12 axes based on author-produced metadata and validated by digital humanities scholars, while prioritizing author agency and informed consent.

Result: Evaluation of embedding models showed they tend to focus on surface-level features rather than semantic categories useful for computational literary studies tasks.

Conclusion: FICSIM provides a valuable benchmark for evaluating embedding models in literary domain tasks, revealing current limitations in capturing meaningful semantic similarities for computational literary studies.

Abstract: As language models become capable of processing increasingly long and complex
texts, there has been growing interest in their application within
computational literary studies. However, evaluating the usefulness of these
models for such tasks remains challenging due to the cost of fine-grained
annotation for long-form texts and the data contamination concerns inherent in
using public-domain literature. Current embedding similarity datasets are not
suitable for evaluating literary-domain tasks because of a focus on
coarse-grained similarity and primarily on very short text. We assemble and
release FICSIM, a dataset of long-form, recently written fiction, including
scores along 12 axes of similarity informed by author-produced metadata and
validated by digital humanities scholars. We evaluate a suite of embedding
models on this task, demonstrating a tendency across models to focus on
surface-level features over semantic categories that would be useful for
computational literary studies tasks. Throughout our data-collection process,
we prioritize author agency and rely on continual, informed author consent.

</details>


### [4] [Do LLMs Truly Understand When a Precedent Is Overruled?](https://arxiv.org/abs/2510.20941)
*Li Zhang,Jaromir Savelka,Kevin Ashley*

Main category: cs.CL

TL;DR: LLMs show degraded performance in identifying overruling relationships in Supreme Court cases, revealing era sensitivity, shallow reasoning, and context-dependent reasoning failures.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' ability to understand long legal documents through realistic, high-stakes tasks, addressing the gap in existing simplified evaluations.

Method: Assessment of state-of-the-art LLMs on identifying overruling relationships from 236 U.S. Supreme Court case pairs, focusing on temporal reasoning and legal comprehension.

Result: Models show era sensitivity (poor performance on historical cases), shallow reasoning (relying on heuristics), and context-dependent reasoning failures (producing temporally impossible relationships).

Conclusion: The benchmark addresses critical gaps in long-context evaluation and reveals fundamental limitations in LLMs' legal reasoning capabilities.

Abstract: Large language models (LLMs) with extended context windows show promise for
complex legal reasoning tasks, yet their ability to understand long legal
documents remains insufficiently evaluated. Developing long-context benchmarks
that capture realistic, high-stakes tasks remains a significant challenge in
the field, as most existing evaluations rely on simplified synthetic tasks that
fail to represent the complexity of real-world document understanding.
Overruling relationships are foundational to common-law doctrine and commonly
found in judicial opinions. They provide a focused and important testbed for
long-document legal understanding that closely resembles what legal
professionals actually do. We present an assessment of state-of-the-art LLMs on
identifying overruling relationships from U.S. Supreme Court cases using a
dataset of 236 case pairs. Our evaluation reveals three critical limitations:
(1) era sensitivity -- the models show degraded performance on historical cases
compared to modern ones, revealing fundamental temporal bias in their training;
(2) shallow reasoning -- models rely on shallow logical heuristics rather than
deep legal comprehension; and (3) context-dependent reasoning failures --
models produce temporally impossible relationships in complex open-ended tasks
despite maintaining basic temporal awareness in simple contexts. Our work
contributes a benchmark that addresses the critical gap in realistic
long-context evaluation, providing an environment that mirrors the complexity
and stakes of actual legal reasoning tasks.

</details>


### [5] [Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting](https://arxiv.org/abs/2510.20957)
*Josh McGiff,Khanh-Tung Tran,William Mulcahy,Dáibhidh Ó Luinín,Jake Dalzell,Róisín Ní Bhroin,Adam Burke,Barry O'Sullivan,Hoang D. Nguyen,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: Irish-BLiMP is the first benchmark for evaluating linguistic competence in Irish, showing humans outperform LLMs by 16.6% and revealing a 18.1% gap between open- and closed-source models.


<details>
  <summary>Details</summary>
Motivation: To create the first systematic framework for evaluating grammatical competence in Irish, an endangered language, and address the lack of benchmarks for low-resource languages.

Method: Manually constructed 1020 minimal pairs across 11 linguistic features through a team of fluent Irish speakers, then evaluated both LLMs and human participants on syntactic knowledge.

Result: Humans achieved 90.1% accuracy vs 73.5% for the best model (GPT-5), with humans outperforming all models across all features. Open- and closed-source LLMs showed 18.1% performance gap. Models and humans struggled on different grammatical aspects.

Conclusion: Irish-BLiMP provides the first systematic evaluation framework for Irish grammatical competence and highlights significant gaps in LLM performance, offering a valuable benchmark for low-resource language research.

Abstract: We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), the
first dataset and framework designed for fine-grained evaluation of linguistic
competence in the Irish language, an endangered language. Drawing on a variety
of linguistic literature and grammar reference works, we manually constructed
and reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,
through a team of fluent Irish speakers. We evaluate both existing Large
Language Models (LLMs) and fluent human participants on their syntactic
knowledge of Irish. Our findings show that humans outperform all models across
all linguistic features, achieving 16.6% higher accuracy on average. Moreover,
a substantial performance gap of 18.1% persists between open- and closed-source
LLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracy
compared to 90.1% by human. Interestingly, human participants and models
struggle on different aspects of Irish grammar, thus highlighting a difference
in representation learned by the models. Overall, Irish-BLiMP provides the
first systematic framework for evaluating the grammatical competence of LLMs in
Irish and offers a valuable benchmark for advancing research on linguistic
understanding in low-resource languages.

</details>


### [6] [Can Confidence Estimates Decide When Chain-of-thought is Necessary for Llms?](https://arxiv.org/abs/2510.21007)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.CL

TL;DR: The paper proposes confidence-gated chain-of-thought (CoT) prompting, where models invoke reasoning only when confidence in direct answers is low, to reduce unnecessary token usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: CoT prompting improves reasoning but increases token usage unnecessarily on tasks where it provides little benefit or even harms performance. Current models allow CoT length control, but it's unclear when to use CoT effectively.

Method: Systematic study of four training-free confidence estimation methods for CoT gating, comparing them to random baseline and oracle. Evaluates when models should invoke reasoning based on confidence in direct answers.

Result: Existing training-free confidence measures can reduce redundant CoT and outperform randomly invoked CoT. However, utility varies with dataset and model, making practical deployment challenging.

Conclusion: Current confidence-gated CoT methods show potential but have limitations. The study highlights both strengths and failure modes, paving the way for more reliable adaptive CoT gating.

Abstract: Chain-of-thought (CoT) prompting has emerged as a common technique for
enhancing the reasoning abilities of large language models (LLMs). While
extended reasoning can boost accuracy on complex tasks, it is often unnecessary
and substantially increases token usage, limiting the practicality of reasoning
models in many scenarios. Recent models, such as GPT-OSS and Qwen3, expose
controls that enable users to adjust the length of CoT or determine whether it
is used at all. Yet, it remains unclear when CoT should be used: on some tasks
it improves performance, while on others it provides little benefit or even
harms performance. We address this challenge with confidence-gated CoT, where a
model invokes reasoning only when confidence in its direct answer is low. To
this end, we present the first systematic study of training-free confidence
estimation methods for CoT gating. Specifically, we evaluate four training-free
confidence estimation methods and compare them to a random baseline and an
oracle that always knows when CoT is needed. Through extensive experiments, we
show that existing training-free confidence measures can reduce redundant CoT
and outperform randomly invoked CoT. However, the utility of individual
confidence measures is inconsistent, varying with both the dataset and the
model, underscoring the difficulty of deploying confidence-gated CoT in
practice. By analysing both strengths and failure modes, our study highlights
the potential and limitations of current methods and paves the way toward more
reliable adaptive gating of CoT.

</details>


### [7] [Input Matters: Evaluating Input Structure's Impact on LLM Summaries of Sports Play-by-Play](https://arxiv.org/abs/2510.21034)
*Barkavi Sundararajan,Somayajulu Sripada,Ehud Reiter*

Main category: cs.CL

TL;DR: JSON input structure reduces factual errors in LLM-generated NBA game summaries by 65-69% compared to unstructured input, with structured formats significantly improving factual accuracy.


<details>
  <summary>Details</summary>
Motivation: To quantify how input structure affects hallucinations and factual errors when LLMs generate summaries from sports data, addressing concerns about reliability in accuracy-critical domains.

Method: Manual annotation of 3,312 factual errors across 180 NBA game summaries generated by Llama-3.1-70B and Qwen2.5-72B from three input formats: row-structured, JSON, and unstructured play-by-play data.

Result: JSON input reduced error rates by 69% for Llama and 65% for Qwen compared to unstructured input; row-structured input reduced errors by 54% for Llama and 51% for Qwen. Input structure accounted for over 80% of variance in error rates.

Conclusion: Input structure has a strong effect on factual accuracy in LLM-generated summaries, with JSON format providing the most reliable results for sports reporting applications.

Abstract: A major concern when deploying LLMs in accuracy-critical domains such as
sports reporting is that the generated text may not faithfully reflect the
input data. We quantify how input structure affects hallucinations and other
factual errors in LLM-generated summaries of NBA play-by-play data, across
three formats: row-structured, JSON and unstructured. We manually annotated
3,312 factual errors across 180 game summaries produced by two models,
Llama-3.1-70B and Qwen2.5-72B. Input structure has a strong effect: JSON input
reduces error rates by 69% for Llama and 65% for Qwen compared to unstructured
input, while row-structured input reduces errors by 54% for Llama and 51% for
Qwen. A two-way repeated measures ANOVA shows that input structure accounts for
over 80% of the variance in error rates, with Tukey HSD post hoc tests
confirming statistically significant differences between all input formats.

</details>


### [8] [Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection](https://arxiv.org/abs/2510.21049)
*Atoosa Chegini,Hamid Kazemi,Garrett Souza,Maria Safi,Yang Song,Samy Bengio,Sinead Williamson,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: Reasoning in LLMs improves overall accuracy but underperforms in precision-sensitive tasks requiring low false positive rates, while non-reasoning approaches dominate in strict precision regimes.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate reasoning's suitability for precision-sensitive classification tasks under low false positive rate regimes, particularly for safety and hallucination detection.

Method: Comparative analysis of reasoning-augmented (Think On) vs non-reasoning (Think Off) approaches across fine-tuned and zero-shot settings using standard LLMs and Large Reasoning Models, with evaluation of token-based scoring vs self-verbalized confidence.

Result: Think On improves overall accuracy but underperforms at low-FPR thresholds; Think Off dominates in precision-sensitive regimes; token-based scoring outperforms self-verbalized confidence; ensemble approach recovers strengths of both modes.

Conclusion: Reasoning is a double-edged tool: beneficial for average accuracy but often ill-suited for applications requiring strict precision, with non-reasoning approaches better for low-FPR requirements.

Abstract: Reasoning has become a central paradigm for large language models (LLMs),
consistently boosting accuracy across diverse benchmarks. Yet its suitability
for precision-sensitive tasks remains unclear. We present the first systematic
study of reasoning for classification tasks under strict low false positive
rate (FPR) regimes. Our analysis covers two tasks--safety detection and
hallucination detection--evaluated in both fine-tuned and zero-shot settings,
using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a
clear trade-off: Think On (reasoning-augmented) generation improves overall
accuracy, but underperforms at the low-FPR thresholds essential for practical
use. In contrast, Think Off (no reasoning during inference) dominates in these
precision-sensitive regimes, with Think On surpassing only when higher FPRs are
acceptable. In addition, we find token-based scoring substantially outperforms
self-verbalized confidence for precision-sensitive deployments. Finally, a
simple ensemble of the two modes recovers the strengths of each. Taken
together, our findings position reasoning as a double-edged tool: beneficial
for average accuracy, but often ill-suited for applications requiring strict
precision.

</details>


### [9] [Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization](https://arxiv.org/abs/2510.21059)
*Mahmud Wasif Nafee,Maiqi Jiang,Haipeng Chen,Yanfu Zhang*

Main category: cs.CL

TL;DR: DR-IKE is a lightweight framework for in-context knowledge editing that dynamically selects demonstrations based on their utility for editing, improving edit success by up to 17.1% and reducing latency by 41.6% compared to static approaches.


<details>
  <summary>Details</summary>
Motivation: Current in-context knowledge editors rely on static demonstration sets chosen by surface-level similarity, leading to quantity-quality trade-offs and lack of adaptivity to task difficulty.

Method: Trains a BERT retriever with REINFORCE to rank demonstrations by editing reward, and employs a learnable threshold to prune low-value examples, dynamically adjusting prompt length based on task difficulty.

Result: On COUNTERFACT benchmark: improves edit success by up to 17.1%, reduces latency by 41.6%, and preserves accuracy on unrelated queries.

Conclusion: DR-IKE demonstrates scalable and adaptive knowledge editing that works with black-box LLMs through forward passes only, without modifying model weights.

Abstract: Large language models (LLMs) excel at factual recall yet still propagate
stale or incorrect knowledge. In-context knowledge editing offers a
gradient-free remedy suitable for black-box APIs, but current editors rely on
static demonstration sets chosen by surface-level similarity, leading to two
persistent obstacles: (i) a quantity-quality trade-off, and (ii) lack of
adaptivity to task difficulty. We address these issues by dynamically selecting
supporting demonstrations according to their utility for the edit. We propose
Dynamic Retriever for In-Context Knowledge Editing (DR-IKE), a lightweight
framework that (1) trains a BERT retriever with REINFORCE to rank
demonstrations by editing reward, and (2) employs a learnable threshold to
prune low-value examples, shortening the prompt when the edit is easy and
expanding it when the task is hard. DR-IKE performs editing without modifying
model weights, relying solely on forward passes for compatibility with
black-box LLMs. On the COUNTERFACT benchmark, it improves edit success by up to
17.1%, reduces latency by 41.6%, and preserves accuracy on unrelated queries,
demonstrating scalable and adaptive knowledge editing. The code is available at
https://github.com/mwnafee/DR-IKE .

</details>


### [10] [Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering](https://arxiv.org/abs/2510.21068)
*William Christian,Daniel Adamlu,Adrian Yu,Derwin Suhartono*

Main category: cs.CL

TL;DR: This paper adapts Retrieval-Augmented Generation (RAG) systems to Indonesian language, using machine translation for data augmentation and implementing an adaptive classifier for question complexity.


<details>
  <summary>Details</summary>
Motivation: To address the language gap in QA systems, as state-of-the-art performance is predominantly in English, and to bridge this gap for Indonesian language.

Method: Employed Adaptive RAG system with a classifier to distinguish question complexity, used machine translation for data augmentation due to limited Indonesian datasets.

Result: Reliable question complexity classifier was achieved, but significant inconsistencies in multi-retrieval answering strategy negatively impacted overall evaluation.

Conclusion: The study highlights both promise and challenges of question answering in low-resource languages, suggesting directions for future improvement.

Abstract: Question Answering (QA) has seen significant improvements with the
advancement of machine learning models, further studies enhanced this question
answering system by retrieving external information, called Retrieval-Augmented
Generation (RAG) to produce more accurate and informative answers. However,
these state-of-the-art-performance is predominantly in English language. To
address this gap we made an effort of bridging language gaps by incorporating
Adaptive RAG system to Indonesian language. Adaptive RAG system integrates a
classifier whose task is to distinguish the question complexity, which in turn
determines the strategy for answering the question. To overcome the limited
availability of Indonesian language dataset, our study employs machine
translation as data augmentation approach. Experiments show reliable question
complexity classifier; however, we observed significant inconsistencies in
multi-retrieval answering strategy which negatively impacted the overall
evaluation when this strategy was applied. These findings highlight both the
promise and challenges of question answering in low-resource language
suggesting directions for future improvement.

</details>


### [11] [CDrugRed: A Chinese Drug Recommendation Dataset for Discharge Medications in Metabolic Diseases](https://arxiv.org/abs/2510.21084)
*Juntao Li,Haobin Yuan,Ling Luo,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: CDrugRed is the first publicly available Chinese drug recommendation dataset for metabolic diseases, containing 5,894 de-identified EHR records from 3,190 patients, with benchmarking showing current LLMs achieve limited performance (F1: 0.5648).


<details>
  <summary>Details</summary>
Motivation: The scarcity of publicly available, real-world EHR datasets in languages other than English hampers the development of intelligent drug recommendation systems, particularly for Chinese clinical applications.

Method: Created CDrugRed dataset with comprehensive patient information including demographics, medical history, clinical course, and discharge diagnoses, then benchmarked state-of-the-art LLMs on discharge medication recommendation task.

Result: Supervised fine-tuning improves model performance but the best model only achieves F1 score of 0.5648 and Jaccard score of 0.4477, indicating substantial room for improvement.

Conclusion: CDrugRed establishes a challenging benchmark for clinical drug recommendation systems and serves as a valuable resource for developing more robust and accurate drug recommendation models in Chinese healthcare settings.

Abstract: Intelligent drug recommendation based on Electronic Health Records (EHRs) is
critical for improving for improving the quality and efficiency of clinical
decision-making. By leveraging large-scale patient data, drug recommendation
systems can assist physicians in selecting the most appropriate medications
according to a patient's medical history, diagnoses, laboratory results, and
comorbidities. However, the advancement of such systems is significantly
hampered by the scarcity of publicly available, real-world EHR datasets,
particularly in languages other than English. In this work, we present
CDrugRed, a first publicly available Chinese drug recommendation dataset
focused on discharge medications for metabolic diseases. The dataset includes
5,894 de-identified records from 3,190 patients, containing comprehensive
information such as patient demographics, medical history, clinical course, and
discharge diagnoses. We assess the utility of CDrugRed by benchmarking several
state-of-the-art large language models (LLMs) on the discharge medication
recommendation task. Experimental results show that while supervised
fine-tuning improves model performance, there remains substantial room for
improvement, with the best model achieving the F1 score of 0.5648 and Jaccard
score of 0.4477. This result highlights the complexity of the clinical drug
recommendation task and establishes CDrugRed as a challenging and valuable
resource for developing more robust and accurate drug recommendation systems.
The dataset is publicly available to the research community under the data
usage agreements at https://github.com/DUTIR-BioNLP/CDrugRed.

</details>


### [12] [Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only](https://arxiv.org/abs/2510.21090)
*Qingru Zhang,Liang Qiu,Ilgee Hong,Zhenghao Xu,Tianyi Liu,Shiyang Li,Rongzhi Zhang,Zheng Li,Lihong Li,Bing Yin,Chao Zhang,Jianshu Chen,Haoming Jiang,Tuo Zhao*

Main category: cs.CL

TL;DR: Self-Rewarding PPO is a novel fine-tuning method that combines SFT and PPO with a self-rewarding mechanism using log policy ratios, improving generalization without human preference data.


<details>
  <summary>Details</summary>
Motivation: Traditional SFT suffers from overfitting and poor generalization in limited-data scenarios due to its off-policy nature, requiring a more robust fine-tuning approach.

Method: Proposes Self-Rewarding PPO that uses on-policy fine-tuning with a reward function defined as the log policy ratio between SFT model and pretrained base model, enabling implicit reward signals without human annotations.

Result: Empirical evaluation shows Self-Rewarding PPO consistently outperforms traditional SFT methods across various NLP tasks, especially in data-scarce scenarios.

Conclusion: The approach effectively addresses SFT limitations by improving generalization, data efficiency, and robustness through on-policy fine-tuning with self-rewarding mechanisms.

Abstract: Supervised fine-tuning (SFT) has emerged as a crucial method for aligning
large language models (LLMs) with human-annotated demonstrations. However, SFT,
being an off-policy approach similar to behavior cloning, often struggles with
overfitting and poor out-of-domain generalization, especially in limited-data
scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel
fine-tuning method that leverages on-policy techniques to enhance
generalization performance. Our approach combines the strengths of SFT and
proximal policy optimization (PPO) to achieve more effective alignment from
demonstration data. At its core is a reward function designed as the log policy
ratio between the SFT model and the pretrained base model. This function serves
as an implicit reward signal, using the pretrained policy as a baseline and the
SFT policy as a target. By doing so, it enables on-policy fine-tuning without
relying on human preference annotations. The integration of this self-rewarding
mechanism with PPO addresses key limitations of SFT, improving generalization,
data efficiency, and robustness. Our empirical evaluation across a range of
natural language processing tasks demonstrates that Self-Rewarding PPO
consistently outperforms traditional SFT methods. The results highlight the
effectiveness of our approach in aligning LLMs using demonstration data,
particularly in scenarios where high-quality annotated data is scarce.

</details>


### [13] [The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection](https://arxiv.org/abs/2510.21118)
*Qiang Ding,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: Proposes VeriGray, a new faithfulness benchmark for LLM summarization that addresses annotation ambiguity by introducing an 'Out-Dependent' category for cases requiring external knowledge verification.


<details>
  <summary>Details</summary>
Motivation: Existing faithfulness benchmarks suffer from annotation ambiguity due to ill-defined boundaries of permissible external knowledge, leading to inconsistent labeling of what constitutes faithful summarization.

Method: Introduces a novel faithfulness annotation framework with an intermediate 'Out-Dependent' category, and constructs VeriGray benchmark using this framework to detect unfaithfulness in summarization.

Result: Even SOTA LLMs like GPT-5 exhibit ~6% hallucinated sentences in summarization, with ~8% of sentences falling into the Out-Dependent category, highlighting annotation ambiguity challenges.

Conclusion: The benchmark poses significant challenges to baseline methods, indicating substantial room for improvement in unfaithfulness detection and the importance of resolving annotation ambiguity.

Abstract: Ensuring that Large Language Models (LLMs) generate summaries faithful to a
given source document is essential for real-world applications. While prior
research has explored LLM faithfulness, existing benchmarks suffer from
annotation ambiguity, primarily due to the ill-defined boundary of permissible
external knowledge in generated outputs. For instance, common sense is often
incorporated into responses and labeled as "faithful", yet the acceptable
extent of such knowledge remains unspecified, leading to inconsistent
annotations. To address this issue, we propose a novel faithfulness annotation
framework, which introduces an intermediate category, Out-Dependent, to
classify cases where external knowledge is required for verification. Using
this framework, we construct VeriGray (Verification with the Gray Zone) -- a
new unfaithfulness detection benchmark in summarization. Statistics reveal that
even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\sim 6\%$ of sentences)
in summarization tasks. Moreover, a substantial proportion ($\sim 8\%$ on
average of models) of generated sentences fall into the Out-Dependent category,
underscoring the importance of resolving annotation ambiguity in unfaithfulness
detection benchmarks. Experiments demonstrate that our benchmark poses
significant challenges to multiple baseline methods, indicating considerable
room for future improvement.

</details>


### [14] [Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications](https://arxiv.org/abs/2510.21131)
*Guangxin Su,Hanchen Wang,Jianwei Wang,Wenjie Zhang,Ying Zhang,Jian Pei*

Main category: cs.CL

TL;DR: This survey systematically reviews the integration of Large Language Models (LLMs) and Text-Attributed Graphs (TAGs), presenting a taxonomy of orchestration strategies and discussing applications across recommendation systems, biomedical analysis, and knowledge-intensive QA.


<details>
  <summary>Details</summary>
Motivation: LLMs excel in semantic understanding but lack structured reasoning, while TAGs provide explicit relational structures but lack semantic depth. Their integration offers complementary benefits for enhanced representation learning and improved reasoning.

Method: Introduces a taxonomy covering two directions: LLM for TAG (enriching graph tasks) and TAG for LLM (improving LLM reasoning). Categorizes orchestration into sequential, parallel, and multi-module frameworks, and discusses TAG-specific pretraining, prompting, and parameter-efficient fine-tuning.

Result: The survey summarizes empirical insights, curates available datasets, and demonstrates diverse applications across multiple domains including recommendation systems, biomedical analysis, and knowledge-intensive question answering.

Conclusion: The integration of LLMs and TAGs represents a promising research direction that combines the strengths of both approaches, with open challenges remaining in areas such as orchestration strategies and domain-specific applications.

Abstract: Large Language Models (LLMs) have achieved remarkable success in natural
language processing through strong semantic understanding and generation.
However, their black-box nature limits structured and multi-hop reasoning. In
contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures
enriched with textual context, yet often lack semantic depth. Recent research
shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG
representation learning and improving the reasoning and interpretability of
LLMs. This survey provides the first systematic review of LLM--TAG integration
from an orchestration perspective. We introduce a novel taxonomy covering two
fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and
TAG for LLM, where structured graphs improve LLM reasoning. We categorize
orchestration strategies into sequential, parallel, and multi-module
frameworks, and discuss advances in TAG-specific pretraining, prompting, and
parameter-efficient fine-tuning. Beyond methodology, we summarize empirical
insights, curate available datasets, and highlight diverse applications across
recommendation systems, biomedical analysis, and knowledge-intensive question
answering. Finally, we outline open challenges and promising research
directions, aiming to guide future work at the intersection of language and
graph learning.

</details>


### [15] [Social Simulations with Large Language Model Risk Utopian Illusion](https://arxiv.org/abs/2510.21180)
*Ning Bian,Xianpei Han,Hongyu Lin,Baolei Wu,Jun Wang*

Main category: cs.CL

TL;DR: LLMs simulate overly idealized human behavior with social desirability bias, creating 'Utopian' societies that lack real human complexity, calling for more socially grounded models.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze how LLMs diverge from authentic human behavior in social contexts, addressing risks of misinterpretation in scientific studies and unintended consequences in real-world applications.

Method: A systematic framework simulating multi-agent interactions through chatroom-style conversations and analyzing them across five linguistic dimensions to examine emergent social cognitive biases.

Result: LLMs do not faithfully reproduce genuine human behavior but reflect overly idealized versions shaped by social desirability bias, showing social role bias, primacy effect, and positivity bias.

Conclusion: LLMs create 'Utopian' societies lacking real human complexity and variability, calling for more socially grounded models that capture the diversity of human social behavior.

Abstract: Reliable simulation of human behavior is essential for explaining,
predicting, and intervening in our society. Recent advances in large language
models (LLMs) have shown promise in emulating human behaviors, interactions,
and decision-making, offering a powerful new lens for social science studies.
However, the extent to which LLMs diverge from authentic human behavior in
social contexts remains underexplored, posing risks of misinterpretation in
scientific studies and unintended consequences in real-world applications.
Here, we introduce a systematic framework for analyzing LLMs' behavior in
social simulation. Our approach simulates multi-agent interactions through
chatroom-style conversations and analyzes them across five linguistic
dimensions, providing a simple yet effective method to examine emergent social
cognitive biases. We conduct extensive experiments involving eight
representative LLMs across three families. Our findings reveal that LLMs do not
faithfully reproduce genuine human behavior but instead reflect overly
idealized versions of it, shaped by the social desirability bias. In
particular, LLMs show social role bias, primacy effect, and positivity bias,
resulting in "Utopian" societies that lack the complexity and variability of
real human interactions. These findings call for more socially grounded LLMs
that capture the diversity of human social behavior.

</details>


### [16] [Estonian Native Large Language Model Benchmark](https://arxiv.org/abs/2510.21193)
*Helena Grete Lillepalu,Tanel Alumäe*

Main category: cs.CL

TL;DR: A new comprehensive benchmark for evaluating LLMs in Estonian language using seven diverse native datasets, with evaluation of 32 models using both human and LLM-as-judge methods.


<details>
  <summary>Details</summary>
Motivation: Limited availability of LLM benchmarks for Estonian language and lack of comprehensive performance comparison across different LLMs on Estonian tasks.

Method: Created benchmark from seven diverse native Estonian datasets assessing knowledge, grammar, vocabulary, summarization, and contextual comprehension. Evaluated 6 base models and 26 instruction-tuned models using human evaluation and LLM-as-judge methods.

Result: Human evaluation showed moderate to high correlation with benchmark results. Claude 3.7 Sonnet demonstrated strong alignment with human ratings as an LLM judge.

Conclusion: Top-performing LLMs can effectively support evaluation of Estonian-language models, and the benchmark provides comprehensive assessment capabilities for Estonian LLMs.

Abstract: The availability of LLM benchmarks for the Estonian language is limited, and
a comprehensive evaluation comparing the performance of different LLMs on
Estonian tasks has yet to be conducted. We introduce a new benchmark for
evaluating LLMs in Estonian, based on seven diverse datasets. These datasets
assess general and domain-specific knowledge, understanding of Estonian grammar
and vocabulary, summarization abilities, contextual comprehension, and more.
The datasets are all generated from native Estonian sources without using
machine translation. We compare the performance of base models,
instruction-tuned open-source models, and commercial models. Our evaluation
includes 6 base models and 26 instruction-tuned models. To assess the results,
we employ both human evaluation and LLM-as-a-judge methods. Human evaluation
scores showed moderate to high correlation with benchmark evaluations,
depending on the dataset. Claude 3.7 Sonnet, used as an LLM judge, demonstrated
strong alignment with human ratings, indicating that top-performing LLMs can
effectively support the evaluation of Estonian-language models.

</details>


### [17] [The "Right" Discourse on Migration: Analysing Migration-Related Tweets in Right and Far-Right Political Movements](https://arxiv.org/abs/2510.21220)
*Nishan Chatterjee,Veronika Bajt,Ana Zwitter Vitez,Senja Pollak*

Main category: cs.CL

TL;DR: This paper analyzes far-right Twitter discourse using NLP and sociological methods to understand migration-related hate speech and persuasion techniques.


<details>
  <summary>Details</summary>
Motivation: To understand how right-wing extremist ideologies spread on social media and impact political outcomes, particularly focusing on migration discourse.

Method: Uses state-of-the-art natural language processing techniques combined with sociological insights to analyze the MIGR-TWIT corpus of far-right tweets in English and French.

Result: Uncovers patterns of discourse surrounding migration, hate speech, and persuasion techniques employed by right and far-right actors on Twitter.

Conclusion: The integrated linguistic, sociological, and computational approach provides cross-disciplinary insights into societal dynamics and helps understand contemporary challenges posed by right-wing extremism on social media.

Abstract: The rise of right-wing populism in Europe has brought to the forefront the
significance of analysing social media discourse to understand the
dissemination of extremist ideologies and their impact on political outcomes.
Twitter, as a platform for interaction and mobilisation, provides a unique
window into the everyday communication of far-right supporters. In this paper,
we propose a methodology that uses state-of-the-art natural language processing
techniques with sociological insights to analyse the MIGR-TWIT corpus of
far-right tweets in English and French. We aim to uncover patterns of discourse
surrounding migration, hate speech, and persuasion techniques employed by right
and far-right actors. By integrating linguistic, sociological, and
computational approaches, we seek to offer cross-disciplinary insights into
societal dynamics and contribute to a better understanding of contemporary
challenges posed by right-wing extremism on social media platforms.

</details>


### [18] [DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services](https://arxiv.org/abs/2510.21228)
*Xiang Li,Huizi Yu,Wenkong Wang,Yiran Wu,Jiayan Zhou,Wenyue Hua,Xinxin Lin,Wenjia Tan,Lexuan Zhu,Bingyi Chen,Guang Chen,Ming-Li Chen,Yang Zhou,Zhao Li,Themistocles L. Assimes,Yongfeng Zhang,Qingyun Wu,Xin Ma,Lingyao Li,Lizhou Fan*

Main category: cs.CL

TL;DR: Developed and evaluated a taxonomy-grounded LLM-powered multi-agent system for simulating emergency medical dispatch scenarios, achieving high performance in dispatch effectiveness and guidance efficacy.


<details>
  <summary>Details</summary>
Motivation: Emergency medical dispatch faces challenges from caller distress, ambiguity, and cognitive load. LLMs and multi-agent systems offer opportunities to augment dispatchers and improve emergency response workflows.

Method: Created clinical taxonomy (32 chief complaints, 6 caller identities) and six-phase call protocol. Built AutoGen-based multi-agent system with Caller and Dispatcher Agents using fact commons for clinical plausibility. Evaluated with physician assessment of 100 cases and automated linguistic analysis.

Result: High performance with excellent Dispatch Effectiveness (94% correct agent contact) and Guidance Efficacy (91% cases with advice). Automated metrics showed neutral sentiment (73.7%), high readability (Flesch 80.9), and polite style (60% polite, 0% impolite). Substantial inter-rater agreement (Gwe's AC1 > 0.70).

Conclusion: The taxonomy-grounded multi-agent system successfully simulates diverse, clinically plausible dispatch scenarios with high fidelity, supporting use for dispatcher training, protocol evaluation, and as foundation for real-time decision support in emergency response.

Abstract: Objective: Emergency medical dispatch (EMD) is a high-stakes process
challenged by caller distress, ambiguity, and cognitive load. Large Language
Models (LLMs) and Multi-Agent Systems (MAS) offer opportunities to augment
dispatchers. This study aimed to develop and evaluate a taxonomy-grounded,
LLM-powered multi-agent system for simulating realistic EMD scenarios. Methods:
We constructed a clinical taxonomy (32 chief complaints, 6 caller identities
from MIMIC-III) and a six-phase call protocol. Using this framework, we
developed an AutoGen-based MAS with Caller and Dispatcher Agents. The system
grounds interactions in a fact commons to ensure clinical plausibility and
mitigate misinformation. We used a hybrid evaluation framework: four physicians
assessed 100 simulated cases for "Guidance Efficacy" and "Dispatch
Effectiveness," supplemented by automated linguistic analysis (sentiment,
readability, politeness). Results: Human evaluation, with substantial
inter-rater agreement (Gwe's AC1 > 0.70), confirmed the system's high
performance. It demonstrated excellent Dispatch Effectiveness (e.g., 94 %
contacting the correct potential other agents) and Guidance Efficacy (advice
provided in 91 % of cases), both rated highly by physicians. Algorithmic
metrics corroborated these findings, indicating a predominantly neutral
affective profile (73.7 % neutral sentiment; 90.4 % neutral emotion), high
readability (Flesch 80.9), and a consistently polite style (60.0 % polite; 0 %
impolite). Conclusion: Our taxonomy-grounded MAS simulates diverse, clinically
plausible dispatch scenarios with high fidelity. Findings support its use for
dispatcher training, protocol evaluation, and as a foundation for real-time
decision support. This work outlines a pathway for safely integrating advanced
AI agents into emergency response workflows.

</details>


### [19] [Correlation Dimension of Auto-Regressive Large Language Models](https://arxiv.org/abs/2510.21258)
*Xin Du,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: The paper introduces correlation dimension, a fractal-geometric measure, to quantify the epistemological complexity of text as perceived by language models, addressing limitations of conventional metrics like perplexity.


<details>
  <summary>Details</summary>
Motivation: Current LLMs exhibit puzzling behaviors like repetition and incoherence despite low perplexity, highlighting the limitation of conventional evaluation metrics that focus on local prediction accuracy while ignoring long-range structural complexity.

Method: Introduces correlation dimension, a fractal-geometric measure of self-similarity that captures hierarchical recurrence structure of language, bridging local and global properties in a unified framework.

Result: Correlation dimension reveals three distinct phases during pretraining, reflects context-dependent complexity, indicates hallucination tendency, and reliably detects multiple forms of degeneration in generated text. The method is computationally efficient, robust to quantization, and applicable across autoregressive architectures.

Conclusion: Correlation dimension provides fresh insight into the generative dynamics of LLMs and serves as an effective tool for evaluating text complexity and model behavior beyond traditional perplexity-based metrics.

Abstract: Large language models (LLMs) have achieved remarkable progress in natural
language generation, yet they continue to display puzzling behaviors -- such as
repetition and incoherence -- even when exhibiting low perplexity. This
highlights a key limitation of conventional evaluation metrics, which emphasize
local prediction accuracy while overlooking long-range structural complexity.
We introduce correlation dimension, a fractal-geometric measure of
self-similarity, to quantify the epistemological complexity of text as
perceived by a language model. This measure captures the hierarchical
recurrence structure of language, bridging local and global properties in a
unified framework. Through extensive experiments, we show that correlation
dimension (1) reveals three distinct phases during pretraining, (2) reflects
context-dependent complexity, (3) indicates a model's tendency toward
hallucination, and (4) reliably detects multiple forms of degeneration in
generated text. The method is computationally efficient, robust to model
quantization (down to 4-bit precision), broadly applicable across
autoregressive architectures (e.g., Transformer and Mamba), and provides fresh
insight into the generative dynamics of LLMs.

</details>


### [20] [Sparser Block-Sparse Attention via Token Permutation](https://arxiv.org/abs/2510.21270)
*Xinghao Wang,Pengyu Wang,Dong Zhang,Chenkun Tan,Shaojun Zhou,Zhaoxiang Liu,Shiguo Lian,Fangxu Liu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: PBS-Attn is a plug-and-play method that uses attention permutation properties to increase block-level sparsity, improving computational efficiency in LLM prefilling while maintaining accuracy close to full attention.


<details>
  <summary>Details</summary>
Motivation: Self-attention's O(N^2) complexity is computationally expensive for long sequences, and existing block-sparse attention methods suffer from sub-optimal sparsity when important tokens are scattered across blocks.

Method: Proposed Permuted Block-Sparse Attention (PBS-Attn) that leverages attention permutation properties to enhance block-level sparsity, implemented with custom permuted-FlashAttention kernels.

Result: PBS-Attn outperforms existing block-sparse methods in accuracy, closely matches full attention baseline, and achieves up to 2.75× speedup in long-context prefilling on real-world datasets.

Conclusion: PBS-Attn provides an effective solution for scaling LLM context lengths by improving computational efficiency through enhanced block-level sparsity while maintaining model accuracy.

Abstract: Scaling the context length of large language models (LLMs) offers significant
benefits but is computationally expensive. This expense stems primarily from
the self-attention mechanism, whose $O(N^2)$ complexity with respect to
sequence length presents a major bottleneck for both memory and latency.
Fortunately, the attention matrix is often sparse, particularly for long
sequences, suggesting an opportunity for optimization. Block-sparse attention
has emerged as a promising solution that partitions sequences into blocks and
skips computation for a subset of these blocks. However, the effectiveness of
this method is highly dependent on the underlying attention patterns, which can
lead to sub-optimal block-level sparsity. For instance, important key tokens
for queries within a single block may be scattered across numerous other
blocks, leading to computational redundancy. In this work, we propose Permuted
Block-Sparse Attention (\textbf{PBS-Attn}), a plug-and-play method that
leverages the permutation properties of attention to increase block-level
sparsity and enhance the computational efficiency of LLM prefilling. We conduct
comprehensive experiments on challenging real-world long-context datasets,
demonstrating that PBS-Attn consistently outperforms existing block-sparse
attention methods in model accuracy and closely matches the full attention
baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn
achieves an end-to-end speedup of up to $2.75\times$ in long-context
prefilling, confirming its practical viability. Code available at
https://github.com/xinghaow99/pbs-attn

</details>


### [21] [PARL: Prompt-based Agents for Reinforcement Learning](https://arxiv.org/abs/2510.21306)
*Yarik Menchaca Resendiz,Roman Klinger*

Main category: cs.CL

TL;DR: PARL uses LLMs as RL agents through prompting without fine-tuning, matching traditional RL agents in simple environments but struggling with complex mathematical operations.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs as agents in reinforcement learning tasks with structured, non-linguistic reasoning rather than just language-based tasks.

Method: PARL encodes actions, states, and rewards in prompts to enable LLMs to learn through trial-and-error interaction without fine-tuning.

Result: PARL matches or outperforms traditional RL agents in simple environments but shows limitations in tasks requiring complex mathematical operations or state/action decoding.

Conclusion: LLMs can function as effective RL agents through prompting alone, though their performance is constrained by mathematical and state-decoding capabilities.

Abstract: Large language models (LLMs) have demonstrated high performance on tasks
expressed in natural language, particularly in zero- or few-shot settings.
These are typically framed as supervised (e.g., classification) or unsupervised
(e.g., clustering) problems. However, limited work evaluates LLMs as agents in
reinforcement learning (RL) tasks (e.g., playing games), where learning occurs
through interaction with an environment and a reward system. While prior work
focused on representing tasks that rely on a language representation, we study
structured, non-linguistic reasoning - such as interpreting positions in a grid
world. We therefore introduce PARL (Prompt-based Agent for Reinforcement
Learning), a method that uses LLMs as RL agents through prompting, without any
fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling
the model to learn through trial-and-error interaction. We evaluate PARL on
three standard RL tasks that do not entirely rely on natural language. We show
that it can match or outperform traditional RL agents in simple environments by
leveraging pretrained knowledge. However, we identify performance limitations
in tasks that require complex mathematical operations or decoding states and
actions.

</details>


### [22] [Efficient semantic uncertainty quantification in language models via diversity-steered sampling](https://arxiv.org/abs/2510.21310)
*Ji Won Park,Kyunghyun Cho*

Main category: cs.CL

TL;DR: A diversity-steered sampling method for LLMs that reduces semantic redundancy in QA outputs, improving uncertainty estimation efficiency without requiring gradient access to the base model.


<details>
  <summary>Details</summary>
Motivation: Estimating semantic uncertainties in LLMs for free-form QA is challenging and expensive due to the need for many generations to obtain stable estimates.

Method: Uses a diversity-steered sampler with semantic-similarity penalty from an NLI model, importance reweighting for debiasing, and control variates for variance reduction.

Result: Matches or surpasses baselines across four QA benchmarks while covering more semantic clusters with the same number of samples.

Conclusion: The modular framework serves as a drop-in enhancement for uncertainty estimation in risk-sensitive LLM deployments.

Abstract: Accurately estimating semantic aleatoric and epistemic uncertainties in large
language models (LLMs) is particularly challenging in free-form question
answering (QA), where obtaining stable estimates often requires many expensive
generations. We introduce a diversity-steered sampler that discourages
semantically redundant outputs during decoding, covers both autoregressive and
masked diffusion paradigms, and yields substantial sample-efficiency gains. The
key idea is to inject a continuous semantic-similarity penalty into the model's
proposal distribution using a natural language inference (NLI) model lightly
finetuned on partial prefixes or intermediate diffusion states. We debias
downstream uncertainty estimates with importance reweighting and shrink their
variance with control variates. Across four QA benchmarks, our method matches
or surpasses baselines while covering more semantic clusters with the same
number of samples. Being modular and requiring no gradient access to the base
LLM, the framework promises to serve as a drop-in enhancement for uncertainty
estimation in risk-sensitive model deployments.

</details>


### [23] [Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words](https://arxiv.org/abs/2510.21326)
*Gianluca Sperduti,Alejandro Moreo*

Main category: cs.CL

TL;DR: The paper investigates why NLP models remain robust to typoglycemia (scrambled letters), finding that few English words collapse into identical forms and contextual differences make disambiguation easy.


<details>
  <summary>Details</summary>
Motivation: To understand why NLP models perform well despite typoglycemia, where scrambled letters cause distinct words to have identical representations, raising questions about model robustness.

Method: Analyzed British National Corpus for word collapse quantification, evaluated BERT's disambiguation ability, and compared BERT variants trained on clean vs. typoglycemic Wikipedia text.

Result: Performance degradation from letter scrambling is smaller than expected, with few English words collapsing and contextual differences enabling easy disambiguation.

Conclusion: NLP model robustness to typoglycemia stems from limited word collapse in English and strong contextual disambiguation cues, rather than sophisticated internal mechanisms.

Abstract: Research in linguistics has shown that humans can read words with internally
scrambled letters, a phenomenon recently dubbed typoglycemia. Some specific NLP
models have recently been proposed that similarly demonstrate robustness to
such distortions by ignoring the internal order of characters by design. This
raises a fundamental question: how can models perform well when many distinct
words (e.g., form and from) collapse into identical representations under
typoglycemia? Our work, focusing exclusively on the English language, seeks to
shed light on the underlying aspects responsible for this robustness. We
hypothesize that the main reasons have to do with the fact that (i) relatively
few English words collapse under typoglycemia, and that (ii) collapsed words
tend to occur in contexts so distinct that disambiguation becomes trivial. In
our analysis, we (i) analyze the British National Corpus to quantify word
collapse and ambiguity under typoglycemia, (ii) evaluate BERT's ability to
disambiguate collapsing forms, and (iii) conduct a probing experiment by
comparing variants of BERT trained from scratch on clean versus typoglycemic
Wikipedia text; our results reveal that the performance degradation caused by
scrambling is smaller than expected.

</details>


### [24] [TripTide: A Benchmark for Adaptive Travel Planning under Disruptions](https://arxiv.org/abs/2510.21329)
*Priyanshu Karmakar,Soumyabrata Chaudhuri,Shubhojit Mallick,Manish Gupta,Abhik Jana,Shreya Ghosh*

Main category: cs.CL

TL;DR: TripTide is the first benchmark for evaluating LLMs' ability to revise travel itineraries under realistic disruptions like flight cancellations and weather closures, assessing adaptability across disruption severity and traveler tolerance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based travel planning systems like TripCraft and TravelPlanner generate personalized itineraries but lack evaluation for handling real-world disruptions that commonly occur during travel.

Method: Threefold evaluation: 1) Automatic metrics (Preservation of Intent, Responsiveness, Adaptability), 2) LLM-as-a-judge assessment, 3) Manual expert evaluation of semantic, spatial, sequential, and responsive aspects.

Result: LLMs maintain strong sequential consistency and semantic stability, with spatial deviations larger for shorter trips but decreasing with longer ones. However, disruption-handling ability declines as plan length increases.

Conclusion: TripTide establishes a benchmark for evaluating adaptability, personalization, and resilience in LLM-based travel planning under real-world uncertainty, revealing limitations in LLM robustness for longer plans.

Abstract: Recent efforts like TripCraft and TravelPlanner have advanced the use of
Large Language Models ( LLMs) for personalized, constraint aware travel
itinerary generation. Yet, real travel often faces disruptions. To address
this, we present TripTide, the first benchmark evaluating LLM's ability to
revise itineraries under realistic disruptions. TripTide models key dimensions
such as disruption severity and traveler tolerance, enabling nuanced assessment
of LLM adaptability to events like flight cancellations, weather closures, or
overbooked attractions. We conduct a threefold evaluation. First, we introduce
automatic metrics including Preservation of Intent (how well the revised plan
maintains feasibility and goals), Responsiveness (promptness and
appropriateness of disruption handling), and Adaptability (semantic, spatial,
and sequential divergence between original and revised plans). Second, we apply
an LLM-as-a-judge approach to automatically assess revision quality. Third, we
perform manual expert evaluation to verify whether revisions preserve semantic,
spatial, sequential, and responsive aspects. Our experiments show that LLMs
maintain strong sequential consistency and semantic stability, while spatial
deviations are larger for shorter trips but decrease with longer ones,
indicating that extended plans encourage better geographic coherence. However,
disruption-handling ability declines as plan length increases, highlighting
limits in LLM robustness. TripTide establishes a benchmark for evaluating
adaptability, personalization, and resilience in LLM-based travel planning
under real-world uncertainty.

</details>


### [25] [Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning](https://arxiv.org/abs/2510.21339)
*Qiang Liu,Wuganjing Song,Zhenzhou Lin,Feifan Chen,Qiaolong Cai,Chen Li,Yongduo Sui*

Main category: cs.CL

TL;DR: Multi-turn training with human feedback is not necessary for reasoning tasks - single-turn training generalizes better to both single- and multi-turn evaluations, while multi-turn strategies degrade single-turn performance.


<details>
  <summary>Details</summary>
Motivation: There's a mismatch between single-turn RL training of LLMs and real-world multi-turn interactions with human feedback, raising questions about whether multi-turn training is necessary for reasoning tasks.

Method: Compared conventional single-turn training with three multi-turn strategies on reasoning tasks, evaluating generalization to both single- and multi-turn scenarios.

Result: Models trained with single-turn setting generalized effectively to both single- and multi-turn evaluations, while multi-turn trained models showed significant degradation in single-turn reasoning performance.

Conclusion: For tasks with complete information, robust single-turn training remains more effective and reliable than multi-turn training, which provides limited benefits and can degrade reasoning capabilities.

Abstract: The reasoning capabilities of Large Language Models (LLMs) are typically
developed through the single-turn reinforcement learning, whereas real-world
applications often involve multi-turn interactions with human feedback, leading
to a potential mismatch between training and deployment conditions. In this
work, we study whether multi-turn training with human feedback is necessary for
reasoning tasks. We compare conventional single-turn training with three
multi-turn strategies and reach contrary conclusions to previous research. We
find that models trained in a single-turn setting generalize effectively to
both single- and multi-turn evaluations, while models trained with multi-turn
strategies exhibit a significant degradation in single-turn reasoning
performance. These results suggest that for tasks with complete information,
robust single-turn training remains more effective and reliable, as multi-turn
training with basic feedback provides limited benefits and can even degrade
reasoning capabilities.

</details>


### [26] [A Diagnostic Benchmark for Sweden-Related Factual Knowledge](https://arxiv.org/abs/2510.21360)
*Jenny Kunz*

Main category: cs.CL

TL;DR: The paper introduces a manually created Swedish question-answering benchmark focused on Sweden-specific knowledge to address the limitation of translated US-centric benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Swedish benchmarks are translated from US-centric sources and lack coverage of Sweden-specific knowledge, particularly about personalities and events with limited international media coverage.

Method: Created a manually written QA benchmark inspired by Swedish radio programs featuring public figures and major sports events, with English translations for cross-lingual analysis.

Result: Smaller models with stronger Swedish coverage performed comparably to three times larger multilingual models on Sweden-related facts. Continued Swedish pre-training improved factual knowledge but caused some forgetting of previously known information.

Conclusion: The dataset serves as a valuable diagnostic tool for studying language adaptation and knowledge retention in multilingual models during language-specific training.

Abstract: Many Swedish benchmarks are translated US-centric benchmarks, and therefore
not suitable for testing knowledge that is particularly relevant, or even
specific, to Sweden. We therefore introduce a manually written
question-answering benchmark specifically targeted to Sweden-related
personalities and events, many of which receive very limited coverage in
international media. Our annotators drew inspiration from a popular radio
program featuring public figures from culture and media, as well as major
sports events in Sweden. The dataset can be used to measure factual recall
across models of varying sizes and degrees of Swedish coverage, and allows to
probe cross-lingual factual consistency as to contains English translations.
Using the dataset, we find that smaller models with stronger Swedish coverage
perform comparably to a three times larger multilingual model in recalling
Sweden-related facts. We also observe that continued pre-training on Swedish
generally improves factual knowledge but also leads to forgetting of a part of
the previously known information. These results demonstrate the dataset's
potential as a diagnostic tool for studying language adaptation and knowledge
retention in multilingual models and during language adaptation.

</details>


### [27] [SindBERT, the Sailor: Charting the Seas of Turkish NLP](https://arxiv.org/abs/2510.21364)
*Raphael Scheible-Schmitt,Stefan Schweter*

Main category: cs.CL

TL;DR: SindBERT is the first large-scale RoBERTa-based encoder for Turkish, trained on 312GB of text, with competitive performance on Turkish NLP tasks but no consistent scaling advantage.


<details>
  <summary>Details</summary>
Motivation: Many morphologically rich languages like Turkish remain underrepresented in large-scale pre-training efforts, creating a need for dedicated models.

Method: Trained from scratch on 312GB of Turkish text (mC4, OSCAR23, Wikipedia) in both base and large configurations using RoBERTa architecture.

Result: SindBERT performs competitively with existing Turkish and multilingual models, with large variant achieving best scores in 2 of 4 tasks but showing no consistent scaling advantage. Corpus quality and diversity outweigh sheer data volume.

Conclusion: SindBERT provides an open resource for Turkish NLP and demonstrates the limits of scaling and importance of corpus composition in morphologically rich languages.

Abstract: Transformer models have revolutionized NLP, yet many morphologically rich
languages remain underrepresented in large-scale pre-training efforts. With
SindBERT, we set out to chart the seas of Turkish NLP, providing the first
large-scale RoBERTa-based encoder for Turkish. Trained from scratch on 312 GB
of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base
and large configurations, representing the first large-scale encoder-only
language model available for Turkish. We evaluate SindBERT on part-of-speech
tagging, named entity recognition, offensive language detection, and the
TurBLiMP linguistic acceptability benchmark. Our results show that SindBERT
performs competitively with existing Turkish and multilingual models, with the
large variant achieving the best scores in two of four tasks but showing no
consistent scaling advantage overall. This flat scaling trend, also observed
for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be
saturated. At the same time, comparisons with smaller but more curated models
such as BERTurk highlight that corpus quality and diversity can outweigh sheer
data volume. Taken together, SindBERT contributes both as an openly released
resource for Turkish NLP and as an empirical case study on the limits of
scaling and the central role of corpus composition in morphologically rich
languages. The SindBERT models are released under the MIT license and made
available in both fairseq and Huggingface formats.

</details>


### [28] [HalleluBERT: Let every token that has meaning bear its weight](https://arxiv.org/abs/2510.21372)
*Raphael Scheible-Schmitt*

Main category: cs.CL

TL;DR: HalleluBERT is a new RoBERTa-based Hebrew encoder trained from scratch on 49.1GB of Hebrew text, outperforming existing models on NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Hebrew lacks a large-scale, fully trained RoBERTa encoder, with existing models limited by corpus size, vocabulary, or training depth.

Method: Trained RoBERTa-based encoder family (base and large) from scratch on 49.1GB deduplicated Hebrew web text and Wikipedia using Hebrew-specific byte-level BPE vocabulary.

Result: Outperforms both monolingual and multilingual baselines on NER and sentiment classification benchmarks, setting new state of the art for Hebrew.

Conclusion: Demonstrates benefits of fully converged monolingual pretraining for Hebrew NLP tasks.

Abstract: Transformer-based models have advanced NLP, yet Hebrew still lacks a
large-scale RoBERTa encoder which is extensively trained. Existing models such
as HeBERT, AlephBERT, and HeRo are limited by corpus size, vocabulary, or
training depth. We present HalleluBERT, a RoBERTa-based encoder family (base
and large) trained from scratch on 49.1~GB of deduplicated Hebrew web text and
Wikipedia with a Hebrew-specific byte-level BPE vocabulary. Evaluated on NER
and sentiment classification benchmarks, HalleluBERT outperforms both
monolingual and multilingual baselines. HalleluBERT sets a new state of the art
for Hebrew and highlights the benefits of fully converged monolingual
pretraining.

</details>


### [29] [Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings](https://arxiv.org/abs/2510.21424)
*Abderrazek Abid,Thanh-Cong Ho,Fakhri Karray*

Main category: cs.CL

TL;DR: This paper explores using Vision Language Models (VLMs) for human activity recognition in healthcare, introducing a new dataset and evaluation methods to address challenges in assessing VLM performance compared to traditional deep learning models.


<details>
  <summary>Details</summary>
Motivation: VLMs show promise for healthcare applications but remain underexplored for human activity recognition (HAR) in remote health monitoring, with challenges in evaluating their dynamic, non-deterministic outputs.

Method: Introduced a descriptive caption dataset and proposed comprehensive evaluation methods to assess VLMs in HAR, conducting comparative experiments with state-of-the-art deep learning models.

Result: VLMs achieved comparable performance to traditional deep learning models and in some cases surpassed them in terms of accuracy.

Conclusion: This work provides a strong benchmark and opens new possibilities for integrating VLMs into intelligent healthcare systems for human activity recognition.

Abstract: As generative AI continues to evolve, Vision Language Models (VLMs) have
emerged as promising tools in various healthcare applications. One area that
remains relatively underexplored is their use in human activity recognition
(HAR) for remote health monitoring. VLMs offer notable strengths, including
greater flexibility and the ability to overcome some of the constraints of
traditional deep learning models. However, a key challenge in applying VLMs to
HAR lies in the difficulty of evaluating their dynamic and often
non-deterministic outputs. To address this gap, we introduce a descriptive
caption data set and propose comprehensive evaluation methods to evaluate VLMs
in HAR. Through comparative experiments with state-of-the-art deep learning
models, our findings demonstrate that VLMs achieve comparable performance and,
in some cases, even surpass conventional approaches in terms of accuracy. This
work contributes a strong benchmark and opens new possibilities for the
integration of VLMs into intelligent healthcare systems.

</details>


### [30] [Redefining Retrieval Evaluation in the Era of LLMs](https://arxiv.org/abs/2510.21440)
*Giovanni Trappolini,Florin Cuconasu,Simone Filice,Yoelle Maarek,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: UDCG is a new IR metric designed for RAG systems that addresses limitations of traditional metrics by considering LLM processing behavior and the negative impact of distracting documents.


<details>
  <summary>Details</summary>
Motivation: Traditional IR metrics assume human sequential document examination and don't account for how LLMs process retrieved documents as a whole or the negative impact of related but irrelevant documents on generation quality.

Method: Proposed a utility-based annotation schema quantifying both positive contributions of relevant passages and negative impact of distracting ones, then developed UDCG with LLM-oriented positional discount to optimize correlation with end-to-end answer accuracy.

Result: Experiments on five datasets and six LLMs show UDCG improves correlation with RAG performance by up to 36% compared to traditional metrics like nDCG, MAP, and MRR.

Conclusion: UDCG provides better alignment between IR evaluation and LLM consumers, enabling more reliable assessment of RAG components by addressing the fundamental misalignments in traditional IR metrics.

Abstract: Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,
assume that human users sequentially examine documents with diminishing
attention to lower ranks. This assumption breaks down in Retrieval Augmented
Generation (RAG) systems, where search results are consumed by Large Language
Models (LLMs), which, unlike humans, process all retrieved documents as a whole
rather than sequentially. Additionally, traditional IR metrics do not account
for related but irrelevant documents that actively degrade generation quality,
rather than merely being ignored. Due to these two major misalignments, namely
human vs. machine position discount and human relevance vs. machine utility,
classical IR metrics do not accurately predict RAG performance. We introduce a
utility-based annotation schema that quantifies both the positive contribution
of relevant passages and the negative impact of distracting ones. Building on
this foundation, we propose UDCG (Utility and Distraction-aware Cumulative
Gain), a metric using an LLM-oriented positional discount to directly optimize
the correlation with the end-to-end answer accuracy. Experiments on five
datasets and six LLMs demonstrate that UDCG improves correlation by up to 36%
compared to traditional metrics. Our work provides a critical step toward
aligning IR evaluation with LLM consumers and enables more reliable assessment
of RAG components

</details>


### [31] [REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring](https://arxiv.org/abs/2510.21445)
*Thanh Cong Ho,Farah Kharrat,Abderrazek Abid,Fakhri Karray*

Main category: cs.CL

TL;DR: REMONI is an autonomous remote health monitoring system that integrates multimodal LLMs, IoT, and wearables to continuously collect and analyze patient data, detect anomalies, and enable natural language interaction for healthcare workers.


<details>
  <summary>Details</summary>
Motivation: Address the gap in human-machine interaction in remote patient monitoring by creating a more interactive and intelligent system that reduces healthcare workload and costs.

Method: Integrates multimodal LLMs with IoT devices and wearables to collect vital signs, accelerometer data, and video clips. Uses anomaly detection modules including fall detection and emergency condition algorithms. Employs prompt engineering for seamless information integration and natural language processing for activity/emotion recognition.

Result: Developed a full-fledged prototype that demonstrates implementability and scalability for real-life scenarios. The system enables doctors to access real-time vital signs and patient states through an intelligent agent via web application.

Conclusion: REMONI shows potential to reduce medical professionals' workload and healthcare costs while providing robust remote monitoring capabilities through advanced AI integration.

Abstract: With the widespread adoption of wearable devices in our daily lives, the
demand and appeal for remote patient monitoring have significantly increased.
Most research in this field has concentrated on collecting sensor data,
visualizing it, and analyzing it to detect anomalies in specific diseases such
as diabetes, heart disease and depression. However, this domain has a notable
gap in the aspect of human-machine interaction. This paper proposes REMONI, an
autonomous REmote health MONItoring system that integrates multimodal large
language models (MLLMs), the Internet of Things (IoT), and wearable devices.
The system automatically and continuously collects vital signs, accelerometer
data from a special wearable (such as a smartwatch), and visual data in patient
video clips collected from cameras. This data is processed by an anomaly
detection module, which includes a fall detection model and algorithms to
identify and alert caregivers of the patient's emergency conditions. A
distinctive feature of our proposed system is the natural language processing
component, developed with MLLMs capable of detecting and recognizing a
patient's activity and emotion while responding to healthcare worker's
inquiries. Additionally, prompt engineering is employed to integrate all
patient information seamlessly. As a result, doctors and nurses can access
real-time vital signs and the patient's current state and mood by interacting
with an intelligent agent through a user-friendly web application. Our
experiments demonstrate that our system is implementable and scalable for
real-life scenarios, potentially reducing the workload of medical professionals
and healthcare costs. A full-fledged prototype illustrating the functionalities
of the system has been developed and being tested to demonstrate the robustness
of its various capabilities.

</details>


### [32] [MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization](https://arxiv.org/abs/2510.21473)
*Chenglong Wang,Yang Gan,Hang Zhou,Chi Hu,Yongyu Mu,Kai Song,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Jingbo Zhu,Zhengtao Yu,Tong Xiao*

Main category: cs.CL

TL;DR: The paper proposes Multi-Reward Optimization (MRO) to improve diffusion language models' reasoning performance by enhancing token correlation during denoising, achieving both better performance and faster sampling.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models (DLMs) lag behind autoregressive LLMs in reasoning performance, especially with fewer denoising steps, due to independent generation of masked tokens that fails to capture token correlation.

Method: Multi-Reward Optimization (MRO) approach using test-time scaling, reject sampling, and reinforcement learning to optimize token correlation with multiple rewards, plus group step and importance sampling strategies to reduce variance and improve efficiency.

Result: MRO improves reasoning performance and achieves significant sampling speedups while maintaining high performance on reasoning benchmarks.

Conclusion: Enhancing token correlation through MRO effectively bridges the performance gap between DLMs and LLMs in reasoning tasks while improving sampling efficiency.

Abstract: Recent advances in diffusion language models (DLMs) have presented a
promising alternative to traditional autoregressive large language models
(LLMs). However, DLMs still lag behind LLMs in reasoning performance,
especially as the number of denoising steps decreases. Our analysis reveals
that this shortcoming arises primarily from the independent generation of
masked tokens across denoising steps, which fails to capture the token
correlation. In this paper, we define two types of token correlation:
intra-sequence correlation and inter-sequence correlation, and demonstrate that
enhancing these correlations improves reasoning performance. To this end, we
propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to
consider the token correlation during the denoising process. More specifically,
our MRO approach leverages test-time scaling, reject sampling, and
reinforcement learning to directly optimize the token correlation with multiple
elaborate rewards. Additionally, we introduce group step and importance
sampling strategies to mitigate reward variance and enhance sampling
efficiency. Through extensive experiments, we demonstrate that MRO not only
improves reasoning performance but also achieves significant sampling speedups
while maintaining high performance on reasoning benchmarks.

</details>


### [33] [Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models](https://arxiv.org/abs/2510.21520)
*Omer Moussa,Mariya Toneva*

Main category: cs.CL

TL;DR: A scalable brain-tuning method that fine-tunes speech language models to jointly predict fMRI responses from multiple participants, improving brain alignment and generalization while reducing data requirements.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for aligning language models with brain responses are participant-dependent and limited by data availability, hindering generalization to new participants and population-level analyses.

Method: Fine-tune pretrained speech language models to jointly predict fMRI responses from multiple participants, creating brain-tuned models that generalize across individuals.

Result: 5-fold decrease in fMRI data needed for new participants, up to 50% increase in overall brain alignment, strong generalization to unseen datasets, and improved performance on semantic tasks.

Conclusion: Multi-participant brain-tuning demonstrates bidirectional benefits between neuroscience and AI, bridging the two fields while creating more generalizable semantic representations.

Abstract: Pretrained language models are remarkably effective in aligning with human
brain responses elicited by natural language stimuli, positioning them as
promising model organisms for studying language processing in the brain.
However, existing approaches for both estimating and improving this brain
alignment are participant-dependent and highly affected by the amount of data
available per participant, hindering both generalization to new participants
and population-level analyses. In this work, we address these limitations by
introducing a scalable, generalizable brain-tuning method, in which we
fine-tune pretrained speech language models to jointly predict fMRI responses
from multiple participants. We demonstrate that the resulting brain-tuned
models exhibit strong individual brain alignment while generalizing across
participants. Specifically, our method leads to 1) a 5-fold decrease in the
amount of fMRI data needed to predict brain data from new participants, 2) up
to a 50% increase in the overall brain alignment, and 3) strong generalization
to new unseen datasets. Furthermore, this multi-participant brain-tuning
additionally improves downstream performance on semantic tasks, suggesting that
training using brain data from multiple participants leads to more
generalizable semantic representations. Taken together, these findings
demonstrate a bidirectional benefit between neuroscience and AI, helping bridge
the gap between the two fields. We make our code and models publicly available
at https://github.com/bridge-ai-neuro/multi-brain-tuning.

</details>


### [34] [InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.21538)
*Likun Tan,Kuan-Wei Huang,Joy Shi,Kevin Wu*

Main category: cs.CL

TL;DR: The paper proposes a mechanistic approach to detect hallucinations in RAG systems by disentangling external context and parametric knowledge contributions, showing that hallucinations occur when FFN modules disproportionately inject parametric knowledge.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems generate outputs inconsistent with retrieved content, and prior hallucination detection methods conflate external context and parametric knowledge contributions, making accurate detection challenging.

Method: Compute external context scores and parametric knowledge scores across layers and attention heads in Qwen3-0.6b, then train regression-based classifiers to predict hallucinations. Evaluate against state-of-the-art LLMs and detection baselines.

Result: The method outperforms state-of-the-art detection baselines (RAGAS, TruLens, RefChecker) and classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses, demonstrating proxy-model evaluation potential.

Conclusion: Mechanistic signals serve as efficient, generalizable predictors for hallucination detection in RAG systems, enabling better disentanglement of knowledge sources and improved detection accuracy.

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to
mitigate hallucinations, yet models often generate outputs inconsistent with
retrieved content. Accurate hallucination detection requires disentangling the
contributions of external context and parametric knowledge, which prior methods
typically conflate. We investigate the mechanisms underlying RAG hallucinations
and find they arise when later-layer FFN modules disproportionately inject
parametric knowledge into the residual stream. To address this, we explore a
mechanistic detection approach based on external context scores and parametric
knowledge scores. Using Qwen3-0.6b, we compute these scores across layers and
attention heads and train regression-based classifiers to predict
hallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,
GPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,
classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,
demonstrating the potential of proxy-model evaluation. Our results highlight
mechanistic signals as efficient, generalizable predictors for hallucination
detection in RAG systems.

</details>


### [35] [Document Understanding, Measurement, and Manipulation Using Category Theory](https://arxiv.org/abs/2510.21553)
*Jared Claypoole,Yunye Gong,Noson S. Yanofsky,Ajay Divakaran*

Main category: cs.CL

TL;DR: The paper applies category theory to develop mathematical frameworks for document structure analysis, information measurement, summarization, and self-supervised improvement of large pretrained models.


<details>
  <summary>Details</summary>
Motivation: To extract multimodal document structure and develop information theoretic measures, content summarization techniques, and methods for improving large pretrained models using mathematical foundations from category theory.

Method: 1) Represent documents as categories of question-answer pairs 2) Develop orthogonalization procedure to divide document information into non-overlapping pieces 3) Implement techniques using large pretrained models 4) Use RLVR for self-supervised improvement with consistency constraints from category theory.

Result: Developed mathematical frameworks for information measurement, novel summarization techniques, document extension (exegesis), rate distortion analysis of summarization, and multimodal extension of the overall framework.

Conclusion: Category theory provides a powerful mathematical foundation for document analysis, enabling information measurement, summarization, document extension, and self-supervised improvement of large models through consistency constraints.

Abstract: We apply category theory to extract multimodal document structure which leads
us to develop information theoretic measures, content summarization and
extension, and self-supervised improvement of large pretrained models. We first
develop a mathematical representation of a document as a category of
question-answer pairs. Second, we develop an orthogonalization procedure to
divide the information contained in one or more documents into non-overlapping
pieces. The structures extracted in the first and second steps lead us to
develop methods to measure and enumerate the information contained in a
document. We also build on those steps to develop new summarization techniques,
as well as to develop a solution to a new problem viz. exegesis resulting in an
extension of the original document. Our question-answer pair methodology
enables a novel rate distortion analysis of summarization techniques. We
implement our techniques using large pretrained models, and we propose a
multimodal extension of our overall mathematical framework. Finally, we develop
a novel self-supervised method using RLVR to improve large pretrained models
using consistency constraints such as composability and closure under certain
operations that stem naturally from our category theoretic framework.

</details>


### [36] [Are the LLMs Capable of Maintaining at Least the Language Genus?](https://arxiv.org/abs/2510.21561)
*Sandra Mitrović,David Kletz,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: LLMs show sensitivity to linguistic genera, with genus-level effects present but strongly influenced by training data availability, and distinct multilingual strategies across model families.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs exhibit sensitivity to linguistic genera and how genealogical language structure shapes their multilingual behavior, as this remains underexplored despite notable variation in LLM multilingual performance.

Method: Extend prior analyses on MultiQ dataset by checking if models prefer switching to genealogically related languages when prompt language fidelity is not maintained, and investigate whether knowledge consistency is better preserved within than across genera.

Result: Genus-level effects are present but strongly conditioned by training resource availability. Distinct multilingual strategies are observed across LLM families.

Conclusion: LLMs encode aspects of genus-level structure, but training data imbalances remain the primary factor shaping their multilingual performance.

Abstract: Large Language Models (LLMs) display notable variation in multilingual
behavior, yet the role of genealogical language structure in shaping this
variation remains underexplored. In this paper, we investigate whether LLMs
exhibit sensitivity to linguistic genera by extending prior analyses on the
MultiQ dataset. We first check if models prefer to switch to genealogically
related languages when prompt language fidelity is not maintained. Next, we
investigate whether knowledge consistency is better preserved within than
across genera. We show that genus-level effects are present but strongly
conditioned by training resource availability. We further observe distinct
multilingual strategies across LLMs families. Our findings suggest that LLMs
encode aspects of genus-level structure, but training data imbalances remain
the primary factor shaping their multilingual performance.

</details>


### [37] [From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene](https://arxiv.org/abs/2510.21575)
*Mojca Brglez,Špela Vintar*

Main category: cs.CL

TL;DR: The paper introduces SloPragEval and SloPragMega, the first pragmatics understanding benchmarks for Slovene with 405 multiple-choice questions, showing current LLMs still struggle with culture-specific non-literal utterances despite overall improvements in nuanced language understanding.


<details>
  <summary>Details</summary>
Motivation: As LLMs demonstrate increasing capabilities, there is a need for more challenging evaluations that go beyond surface-level linguistic competence to include pragmatics - understanding situational meaning shaped by context, linguistic and cultural norms.

Method: Created SloPragEval and SloPragMega benchmarks containing 405 multiple-choice questions for Slovene pragmatics understanding, established human baseline through campaign, and conducted pilot evaluations with LLMs.

Result: Current models have greatly improved in understanding nuanced language but still fail to infer implied speaker meaning in non-literal utterances, especially culture-specific ones. Significant gap observed between proprietary and open-source models.

Conclusion: Benchmarks targeting nuanced language understanding and cultural knowledge must be carefully designed, preferably constructed from native data and validated with human responses.

Abstract: Large language models are demonstrating increasing capabilities, excelling at
benchmarks once considered very difficult. As their capabilities grow, there is
a need for more challenging evaluations that go beyond surface-level linguistic
competence. Namely, language competence involves not only syntax and semantics
but also pragmatics, i.e., understanding situational meaning as shaped by
context as well as linguistic and cultural norms. To contribute to this line of
research, we introduce SloPragEval and SloPragMega, the first pragmatics
understanding benchmarks for Slovene that contain altogether 405
multiple-choice questions. We discuss the difficulties of translation, describe
the campaign to establish a human baseline, and report pilot evaluations with
LLMs. Our results indicate that current models have greatly improved in
understanding nuanced language but may still fail to infer implied speaker
meaning in non-literal utterances, especially those that are culture-specific.
We also observe a significant gap between proprietary and open-source models.
Finally, we argue that benchmarks targeting nuanced language understanding and
knowledge of the target culture must be designed with care, preferably
constructed from native data, and validated with human responses.

</details>


### [38] [Automated Quality Control for Language Documentation: Detecting Phonotactic Inconsistencies in a Kokborok Wordlist](https://arxiv.org/abs/2510.21584)
*Kellen Parker van Dam,Abishek Stephen*

Main category: cs.CL

TL;DR: Unsupervised anomaly detection methods identify phonotactic inconsistencies in Kokborok-Bangla wordlists to flag potential transcription errors and borrowings, with syllable-level features outperforming character-level baselines.


<details>
  <summary>Details</summary>
Motivation: Lexical data collection in language documentation often contains transcription errors and undocumented borrowings that can mislead linguistic analysis, requiring systematic methods to improve data quality.

Method: Applied unsupervised anomaly detection using character-level and syllable-level phonotactic features to identify inconsistencies in multilingual Kokborok-Bangla wordlists.

Result: Syllable-aware features significantly outperform character-level baselines, though precision and recall remain modest due to the subtle nature of these anomalies.

Conclusion: The high-recall approach provides fieldworkers with a systematic method to flag entries requiring verification, supporting data quality improvement in low-resourced language documentation.

Abstract: Lexical data collection in language documentation often contains
transcription errors and undocumented borrowings that can mislead linguistic
analysis. We present unsupervised anomaly detection methods to identify
phonotactic inconsistencies in wordlists, applying them to a multilingual
dataset of Kokborok varieties with Bangla. Using character-level and
syllable-level phonotactic features, our algorithms identify potential
transcription errors and borrowings. While precision and recall remain modest
due to the subtle nature of these anomalies, syllable-aware features
significantly outperform character-level baselines. The high-recall approach
provides fieldworkers with a systematic method to flag entries requiring
verification, supporting data quality improvement in low-resourced language
documentation.

</details>


### [39] [RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models](https://arxiv.org/abs/2510.21604)
*Xueyuan Lin,Cehao Yang,Ye Ma,Ming Li,Rongjunchen Zhang,Yang Ni,Xiaojun Wu,Chengjin Xu,Jian Guo,Hui Xiong*

Main category: cs.CL

TL;DR: RETuning is a method that enhances LLMs' stock prediction by encouraging independent analytical reasoning through dynamic framework construction and evidence scoring, rather than relying on contextual opinions.


<details>
  <summary>Details</summary>
Motivation: LLMs currently underperform in stock prediction by following analysts' opinions without systematic reasoning, failing to weigh counterevidence crucial for reliable predictions.

Method: Proposed Reflective Evidence Tuning (RETuning) - a cold-start method that constructs analytical frameworks from diverse sources, organizes and scores evidence for price movements, and reflects to derive predictions independently.

Result: RETuning successfully unlocks LLMs' reasoning ability in finance, with inference-time scaling working even after 6 months and on out-of-distribution stocks, as models gain valuable prediction insights.

Conclusion: The approach aligns models with learned analytical frameworks, ensuring independent logical reasoning and reducing contextual influence, making LLMs more effective for financial prediction tasks.

Abstract: Recently, large language models (LLMs) have demonstrated outstanding
reasoning capabilities on mathematical and coding tasks. However, their
application to financial tasks-especially the most fundamental task of stock
movement prediction-remains underexplored. We study a three-class
classification problem (up, hold, down) and, by analyzing existing reasoning
responses, observe that: (1) LLMs follow analysts' opinions rather than exhibit
a systematic, independent analytical logic (CoTs). (2) LLMs list summaries from
different sources without weighing adversarial evidence, yet such
counterevidence is crucial for reliable prediction. It shows that the model
does not make good use of its reasoning ability to complete the task. To
address this, we propose Reflective Evidence Tuning (RETuning), a cold-start
method prior to reinforcement learning, to enhance prediction ability. While
generating CoT, RETuning encourages dynamically constructing an analytical
framework from diverse information sources, organizing and scoring evidence for
price up or down based on that framework-rather than on contextual
viewpoints-and finally reflecting to derive the prediction. This approach
maximally aligns the model with its learned analytical framework, ensuring
independent logical reasoning and reducing undue influence from context. We
also build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,
with long contexts (32K tokens) and over 200K samples. In addition to price and
news, it incorporates analysts' opinions, quantitative reports, fundamental
data, macroeconomic indicators, and similar stocks. Experiments show that
RETuning successfully unlocks the model's reasoning ability in the financial
domain. Inference-time scaling still works even after 6 months or on
out-of-distribution stocks, since the models gain valuable insights about stock
movement prediction.

</details>


### [40] [The Universal Landscape of Human Reasoning](https://arxiv.org/abs/2510.21623)
*Qiguang Chen,Jinhao Liu,Libo Qin,Yimeng Zhang,Yihao Liang,Shangxu Ren,Chengyu Luan,Dengyun Peng,Hanjing Li,Jiannan Guan,Zheng Yan,Jiaqi Wang,Mengkang Hu,Yantao Du,Zhi Chen,Xie Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: IF-Track uses LLMs as probabilistic encoders to quantify information entropy and gain at each reasoning step, providing a unified metric space to model human reasoning dynamics across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a unified, quantitative description of general human reasoning dynamics that existing accounts from classical logic to probabilistic models fail to provide.

Method: Information Flow Tracking (IF-Track) that uses large language models as probabilistic encoders to quantify information entropy and gain at each reasoning step through fine-grained analyses.

Result: IF-Track successfully models universal landscape of human reasoning behaviors, captures essential reasoning features, identifies systematic error patterns, characterizes individual differences, and reconciles single- versus dual-process theories.

Conclusion: This approach establishes a quantitative bridge between theory and measurement, offering mechanistic insights into reasoning architecture and discovering alignment between artificial and human cognition.

Abstract: Understanding how information is dynamically accumulated and transformed in
human reasoning has long challenged cognitive psychology, philosophy, and
artificial intelligence. Existing accounts, from classical logic to
probabilistic models, illuminate aspects of output or individual modelling, but
do not offer a unified, quantitative description of general human reasoning
dynamics. To solve this, we introduce Information Flow Tracking (IF-Track),
that uses large language models (LLMs) as probabilistic encoder to quantify
information entropy and gain at each reasoning step. Through fine-grained
analyses across diverse tasks, our method is the first successfully models the
universal landscape of human reasoning behaviors within a single metric space.
We show that IF-Track captures essential reasoning features, identifies
systematic error patterns, and characterizes individual differences. Applied to
discussion of advanced psychological theory, we first reconcile single- versus
dual-process theories in IF-Track and discover the alignment of artificial and
human cognition and how LLMs reshaping human reasoning process. This approach
establishes a quantitative bridge between theory and measurement, offering
mechanistic insights into the architecture of reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [41] [Preventing Shortcuts in Adapter Training via Providing the Shortcuts](https://arxiv.org/abs/2510.20887)
*Anujraaj Argo Goyal,Guocheng Gordon Qian,Huseyin Coskun,Aarush Gupta,Himmy Tam,Daniil Ostashev,Ju Hu,Dhritiman Sagar,Sergey Tulyakov,Kfir Aberman,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: Shortcut-Rerouted Adapter Training improves text-to-image generation by routing confounding factors through auxiliary modules during training, then removing them during inference to prevent attribute entanglement.


<details>
  <summary>Details</summary>
Motivation: Adapter-based training for image generators often entangles target attributes with incidental factors like pose and lighting, limiting generalization and prompt adherence.

Method: Provide shortcuts for confounding factors through auxiliary modules (ControlNet or LoRA) during adapter training, then remove these modules during inference.

Result: Improves generation quality, diversity, and prompt adherence for facial and full-body identity injection tasks.

Conclusion: When seeking disentangled representations, establish shortcuts for what should NOT be learned rather than trying to eliminate them directly.

Abstract: Adapter-based training has emerged as a key mechanism for extending the
capabilities of powerful foundation image generators, enabling personalized and
stylized text-to-image synthesis. These adapters are typically trained to
capture a specific target attribute, such as subject identity, using
single-image reconstruction objectives. However, because the input image
inevitably contains a mixture of visual factors, adapters are prone to entangle
the target attribute with incidental ones, such as pose, expression, and
lighting. This spurious correlation problem limits generalization and obstructs
the model's ability to adhere to the input text prompt. In this work, we
uncover a simple yet effective solution: provide the very shortcuts we wish to
eliminate during adapter training. In Shortcut-Rerouted Adapter Training,
confounding factors are routed through auxiliary modules, such as ControlNet or
LoRA, eliminating the incentive for the adapter to internalize them. The
auxiliary modules are then removed during inference. When applied to tasks like
facial and full-body identity injection, our approach improves generation
quality, diversity, and prompt adherence. These results point to a general
design principle in the era of large models: when seeking disentangled
representations, the most effective path may be to establish shortcuts for what
should NOT be learned.

</details>


### [42] [Video-As-Prompt: Unified Semantic Control for Video Generation](https://arxiv.org/abs/2510.20888)
*Yuxuan Bian,Xin Chen,Zenan Li,Tiancheng Zhi,Shen Sang,Linjie Luo,Qiang Xu*

Main category: cs.CV

TL;DR: VAP introduces a new paradigm for semantic video control using reference videos as prompts, achieving state-of-the-art performance without task-specific finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for semantic video control either cause artifacts from pixel-wise priors or require non-generalizable condition-specific finetuning, limiting unified control.

Method: VAP uses reference videos as semantic prompts with a plug-and-play Mixture-of-Transformers expert on a frozen Video Diffusion Transformer, guided by temporally biased position embedding.

Result: VAP achieves 38.7% user preference rate, rivaling commercial models, with strong zero-shot generalization across various applications.

Conclusion: VAP represents a significant advance toward general-purpose controllable video generation through its unified approach and strong generalization capabilities.

Abstract: Unified, generalizable semantic control in video generation remains a
critical open challenge. Existing methods either introduce artifacts by
enforcing inappropriate pixel-wise priors from structure-based controls, or
rely on non-generalizable, condition-specific finetuning or task-specific
architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes
this problem as in-context generation. VAP leverages a reference video as a
direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via
a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture
prevents catastrophic forgetting and is guided by a temporally biased position
embedding that eliminates spurious mapping priors for robust context retrieval.
To power this approach and catalyze future research, we built VAP-Data, the
largest dataset for semantic-controlled video generation with over 100K paired
videos across 100 semantic conditions. As a single unified model, VAP sets a
new state-of-the-art for open-source methods, achieving a 38.7% user preference
rate that rivals leading condition-specific commercial models. VAP's strong
zero-shot generalization and support for various downstream applications mark a
significant advance toward general-purpose, controllable video generation.

</details>


### [43] [Focal Modulation and Bidirectional Feature Fusion Network for Medical Image Segmentation](https://arxiv.org/abs/2510.20933)
*Moin Safdar,Shahzaib Iqbal,Mehwish Mehmood,Mubeen Ghafoor,Tariq M. Khan,Imran Razzak*

Main category: cs.CV

TL;DR: FM-BFF-Net combines CNNs and transformers with focal modulation attention and bidirectional feature fusion to improve medical image segmentation, achieving state-of-the-art results across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation is crucial for clinical applications, but CNNs struggle with capturing global context and long-range dependencies, limiting their ability to segment complex structures with varying sizes and boundaries.

Method: Proposes FM-BFF-Net that integrates convolutional and transformer components, uses focal modulation attention mechanism for better context awareness, and implements bidirectional feature fusion module for efficient encoder-decoder interaction across scales.

Result: Extensive experiments on eight public datasets (polyp detection, skin lesion segmentation, ultrasound imaging) show FM-BFF-Net consistently outperforms recent state-of-the-art methods in Jaccard index and Dice coefficient.

Conclusion: FM-BFF-Net effectively enhances boundary precision and robustness to variations in lesion size, shape, and contrast, demonstrating strong effectiveness and adaptability for diverse medical imaging scenarios.

Abstract: Medical image segmentation is essential for clinical applications such as
disease diagnosis, treatment planning, and disease development monitoring
because it provides precise morphological and spatial information on anatomical
structures that directly influence treatment decisions. Convolutional neural
networks significantly impact image segmentation; however, since convolution
operations are local, capturing global contextual information and long-range
dependencies is still challenging. Their capacity to precisely segment
structures with complicated borders and a variety of sizes is impacted by this
restriction. Since transformers use self-attention methods to capture global
context and long-range dependencies efficiently, integrating transformer-based
architecture with CNNs is a feasible approach to overcoming these challenges.
To address these challenges, we propose the Focal Modulation and Bidirectional
Feature Fusion Network for Medical Image Segmentation, referred to as
FM-BFF-Net in the remainder of this paper. The network combines convolutional
and transformer components, employs a focal modulation attention mechanism to
refine context awareness, and introduces a bidirectional feature fusion module
that enables efficient interaction between encoder and decoder representations
across scales. Through this design, FM-BFF-Net enhances boundary precision and
robustness to variations in lesion size, shape, and contrast. Extensive
experiments on eight publicly available datasets, including polyp detection,
skin lesion segmentation, and ultrasound imaging, show that FM-BFF-Net
consistently surpasses recent state-of-the-art methods in Jaccard index and
Dice coefficient, confirming its effectiveness and adaptability for diverse
medical imaging scenarios.

</details>


### [44] [Generative Point Tracking with Flow Matching](https://arxiv.org/abs/2510.20951)
*Mattie Tesfaldet,Adam W. Harley,Konstantinos G. Derpanis,Derek Nowrouzezahrai,Christopher Pal*

Main category: cs.CV

TL;DR: GenPT introduces a generative framework for multi-modal point trajectory tracking that captures uncertainty and multi-modality, outperforming discriminative models on occluded points while maintaining competitive performance on visible points.


<details>
  <summary>Details</summary>
Motivation: Current discriminative point tracking models are limited to regressing to a mean/mode in the presence of uncertainty and fail to capture multi-modality, especially during visual obfuscations like occlusions and appearance changes.

Method: GenPT uses a generative framework with flow matching formulation that combines iterative refinement, window-dependent prior for cross-window consistency, and variance schedule tuned for point coordinates. Inference uses best-first search on generated samples guided by model confidence.

Result: State-of-the-art tracking accuracy on occluded points in PointOdyssey, Dynamic Replica, TAP-Vid benchmarks and a new occlusion-heavy TAP-Vid variant, while maintaining competitive accuracy on visible points compared to discriminative trackers.

Conclusion: GenPT successfully captures multi-modality in point trajectories, demonstrating superior performance on occluded tracking tasks while maintaining competitive visible point tracking, addressing limitations of current discriminative approaches.

Abstract: Tracking a point through a video can be a challenging task due to uncertainty
arising from visual obfuscations, such as appearance changes and occlusions.
Although current state-of-the-art discriminative models excel in regressing
long-term point trajectory estimates -- even through occlusions -- they are
limited to regressing to a mean (or mode) in the presence of uncertainty, and
fail to capture multi-modality. To overcome this limitation, we introduce
Generative Point Tracker (GenPT), a generative framework for modelling
multi-modal trajectories. GenPT is trained with a novel flow matching
formulation that combines the iterative refinement of discriminative trackers,
a window-dependent prior for cross-window consistency, and a variance schedule
tuned specifically for point coordinates. We show how our model's generative
capabilities can be leveraged to improve point trajectory estimates by
utilizing a best-first search strategy on generated samples during inference,
guided by the model's own confidence of its predictions. Empirically, we
evaluate GenPT against the current state of the art on the standard
PointOdyssey, Dynamic Replica, and TAP-Vid benchmarks. Further, we introduce a
TAP-Vid variant with additional occlusions to assess occluded point tracking
performance and highlight our model's ability to capture multi-modality. GenPT
is capable of capturing the multi-modality in point trajectories, which
translates to state-of-the-art tracking accuracy on occluded points, while
maintaining competitive tracking accuracy on visible points compared to extant
discriminative point trackers.

</details>


### [45] [3DReasonKnee: Advancing Grounded Reasoning in Medical Vision Language Models](https://arxiv.org/abs/2510.20967)
*Sraavya Sambara,Sung Eun Kim,Xiaoman Zhang,Luyang Luo,Shreya Johri,Mohammed Baharoon,Du Hyun Ro,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 3DReasonKnee is the first 3D grounded reasoning dataset for medical images, providing 494k expert-annotated quintuples from 7,970 knee MRI volumes to enable VLMs to localize anatomical regions and perform step-by-step diagnostic reasoning.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with grounding anatomical regions in 3D medical images and performing step-by-step reasoning, which is essential for aligning with clinical diagnostic workflows and enabling trustworthy clinician-AI collaboration.

Method: Created 3DReasonKnee dataset with 494k quintuples from 7,970 3D knee MRI volumes, each containing: 3D MRI volume, diagnostic question, 3D bounding box localization, clinician-generated reasoning steps, and structured severity assessments. Involved 450+ hours of expert clinician time for manual segmentation and reasoning chain generation.

Result: Established ReasonKnee-Bench evaluation framework to assess VLM localization and diagnostic accuracy. Benchmarked five state-of-the-art VLMs to provide baseline performance for the benchmark.

Conclusion: 3DReasonKnee serves as a repository of orthopedic surgeons' diagnostic expertise and provides a vital testbed for advancing multimodal medical AI systems towards 3D, clinically aligned, localized decision-making capabilities.

Abstract: Current Vision-Language Models (VLMs) struggle to ground anatomical regions
in 3D medical images and reason about them in a step-by-step manner, a key
requirement of real-world diagnostic assessment. This ability is essential for
aligning model outputs with the diagnostic workflows clinicians use in
practice, enabling trustworthy clinician-AI collaboration. Existing 3D datasets
provide localization labels, but none support this "grounded reasoning"
ability. To address this gap, we introduce 3DReasonKnee, the first 3D grounded
reasoning dataset for medical images, which provides 494k high-quality
quintuples derived from 7,970 3D knee MRI volumes. Each quintuple includes: (1)
the 3D MRI volume, (2) a diagnostic question targeting a specific anatomical
region (3) a 3D bounding box localizing the relevant anatomical structures, (4)
clinician-generated diagnostic reasoning steps that explicitly detail the 3D
reasoning process, and (5) structured severity assessments for the relevant
anatomical region. The creation and validation of 3DReasonKnee, involving over
450 hours of expert clinician time for manually segmenting MRIs and generating
reasoning chains, ensures its superior quality and clinical relevance. We
establish ReasonKnee-Bench to evaluate localization and diagnostic accuracy,
providing insight into VLM ability to perform grounding and severity assessment
across anatomical regions and diagnostic inquiries. We benchmark five
state-of-the-art VLMs, providing baseline performance for ReasonKnee-Bench. By
providing this unique resource of expert-annotated 3D reasoning pathways,
3DReasonKnee serves as a repository of orthopedic surgeons' diagnostic
expertise and offers a vital testbed for advancing multimodal medical AI
systems towards 3D, clinically aligned, localized decision-making capabilities.
The dataset can be found in:
https://huggingface.co/datasets/rajpurkarlab/3DReasonKnee

</details>


### [46] [Thermal Polarimetric Multi-view Stereo](https://arxiv.org/abs/2510.20972)
*Takahiro Kushida,Kenichiro Tanaka*

Main category: cs.CV

TL;DR: Novel 3D shape reconstruction method using thermal polarization cues that works independent of illumination and material properties, overcoming limitations of visible polarization methods.


<details>
  <summary>Details</summary>
Motivation: To develop a 3D reconstruction method that is not dependent on illumination conditions and material properties, addressing ambiguities in visible polarization analysis.

Method: Uses multi-view thermal polarimetric imaging in long-wave infrared (LWIR) spectrum, formulating a general theory of polarization observation that avoids ambiguities present in visible light polarization.

Result: Successfully reconstructs fine details in transparent, translucent, and heterogeneous objects, outperforming existing state-of-the-art techniques.

Conclusion: Thermal polarization imaging provides a robust solution for detailed 3D shape reconstruction that is illumination- and material-independent, overcoming fundamental limitations of visible polarization methods.

Abstract: This paper introduces a novel method for detailed 3D shape reconstruction
utilizing thermal polarization cues. Unlike state-of-the-art methods, the
proposed approach is independent of illumination and material properties. In
this paper, we formulate a general theory of polarization observation and show
that long-wave infrared (LWIR) polarimetric imaging is free from the
ambiguities that affect visible polarization analyses. Subsequently, we propose
a method for recovering detailed 3D shapes using multi-view thermal
polarimetric images. Experimental results demonstrate that our approach
effectively reconstructs fine details in transparent, translucent, and
heterogeneous objects, outperforming existing techniques.

</details>


### [47] [VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models](https://arxiv.org/abs/2510.20994)
*Jesimon Barreto,Carlos Caetano,André Araujo,William Robson Schwartz*

Main category: cs.CV

TL;DR: VESSA is a self-supervised adaptation method for vision foundation models that uses object-centric videos without annotations to improve performance in domains with distribution shifts and scarce labels.


<details>
  <summary>Details</summary>
Motivation: Foundation models underperform in domains with distribution shifts and scarce labels where supervised fine-tuning is infeasible. Self-supervised learning strategies effective for language models haven't worked well for vision encoders.

Method: Uses self-distillation paradigm with multi-view object-centric videos. Carefully tunes prediction heads and deploys parameter-efficient adaptation to prevent forgetting pretrained knowledge. Leverages multi-view object observations from different frames.

Result: Consistent improvements in downstream classification tasks across 3 vision foundation models on 2 datasets, outperforming base models and previous adaptation methods.

Conclusion: VESSA enables effective self-supervised adaptation of vision foundation models to new domains using only object-centric videos, without requiring annotations.

Abstract: Foundation models have advanced computer vision by enabling strong
performance across diverse tasks through large-scale pretraining and supervised
fine-tuning. However, they may underperform in domains with distribution shifts
and scarce labels, where supervised fine-tuning may be infeasible. While
continued self-supervised learning for model adaptation is common for
generative language models, this strategy has not proven effective for
vision-centric encoder models. To address this challenge, we introduce a novel
formulation of self-supervised fine-tuning for vision foundation models, where
the model is adapted to a new domain without requiring annotations, leveraging
only short multi-view object-centric videos. Our method is referred to as
VESSA: Video-based objEct-centric Self-Supervised Adaptation for visual
foundation models. VESSA's training technique is based on a self-distillation
paradigm, where it is critical to carefully tune prediction heads and deploy
parameter-efficient adaptation techniques - otherwise, the model may quickly
forget its pretrained knowledge and reach a degraded state. VESSA benefits
significantly from multi-view object observations sourced from different frames
in an object-centric video, efficiently learning robustness to varied capture
conditions, without the need of annotations. Through comprehensive experiments
with 3 vision foundation models on 2 datasets, VESSA demonstrates consistent
improvements in downstream classification tasks, compared to the base models
and previous adaptation methods. Code is publicly available at
https://github.com/jesimonbarreto/VESSA.

</details>


### [48] [BioDet: Boosting Industrial Object Detection with Image Preprocessing Strategies](https://arxiv.org/abs/2510.21000)
*Jiaqi Hu,Hongli Xu,Junwen Huang,Peter KT Yu,Slobodan Ilic,Benjamin Busam*

Main category: cs.CV

TL;DR: A plug-in pipeline for 2D detection of unseen objects in industrial settings that enhances detection accuracy under challenging conditions like poor lighting and clutter, improving downstream 6D pose estimation.


<details>
  <summary>Details</summary>
Motivation: Existing 6D pose estimation pipelines degrade under industrial challenges like clutter, poor lighting, and complex backgrounds, making detection the critical bottleneck that needs improvement.

Method: Uses low-light image enhancement and background removal guided by open-vocabulary detection with foundation models, building on SOTA baselines to reduce domain shift and suppress false positives from raw SAM outputs.

Result: Significantly boosts detection accuracy on real-world industrial bin-picking benchmarks from BOP while incurring negligible inference overhead.

Conclusion: The proposed method is effective and practical for improving 2D detection in industrial environments, enhancing reliability for downstream pose estimation tasks.

Abstract: Accurate 6D pose estimation is essential for robotic manipulation in
industrial environments. Existing pipelines typically rely on off-the-shelf
object detectors followed by cropping and pose refinement, but their
performance degrades under challenging conditions such as clutter, poor
lighting, and complex backgrounds, making detection the critical bottleneck. In
this work, we introduce a standardized and plug-in pipeline for 2D detection of
unseen objects in industrial settings. Based on current SOTA baselines, our
approach reduces domain shift and background artifacts through low-light image
enhancement and background removal guided by open-vocabulary detection with
foundation models. This design suppresses the false positives prevalent in raw
SAM outputs, yielding more reliable detections for downstream pose estimation.
Extensive experiments on real-world industrial bin-picking benchmarks from BOP
demonstrate that our method significantly boosts detection accuracy while
incurring negligible inference overhead, showing the effectiveness and
practicality of the proposed method.

</details>


### [49] [Deep learning-based automated damage detection in concrete structures using images from earthquake events](https://arxiv.org/abs/2510.21063)
*Abdullah Turer,Yongsheng Bai,Halil Sezen,Alper Yilmaz*

Main category: cs.CV

TL;DR: Deep learning-based automated detection of exposed steel reinforcement in concrete structures after earthquakes using YOLOv11 models for damage assessment.


<details>
  <summary>Details</summary>
Motivation: Timely assessment of structural integrity after seismic events is crucial for public safety and emergency response, requiring rapid damage detection methods.

Method: Used YOLOv11 models with fine-tuning, data augmentation, and testing on datasets from 2023 Turkey Earthquakes to detect cracking, spalling, and exposed steel bars, creating a hybrid framework for damage level classification.

Result: Developed an automated classification framework that can identify building locations and structural components, and reliably determine damage levels from input images.

Conclusion: Rapid and automated damage detection following disasters is achievable through image data collection, annotation, and deep learning approaches across diverse damage contexts.

Abstract: Timely assessment of integrity of structures after seismic events is crucial
for public safety and emergency response. This study focuses on assessing the
structural damage conditions using deep learning methods to detect exposed
steel reinforcement in concrete buildings and bridges after large earthquakes.
Steel bars are typically exposed after concrete spalling or large flexural or
shear cracks. The amount and distribution of exposed steel reinforcement is an
indication of structural damage and degradation. To automatically detect
exposed steel bars, new datasets of images collected after the 2023 Turkey
Earthquakes were labeled to represent a wide variety of damaged concrete
structures. The proposed method builds upon a deep learning framework, enhanced
with fine-tuning, data augmentation, and testing on public datasets. An
automated classification framework is developed that can be used to identify
inside/outside buildings and structural components. Then, a YOLOv11 (You Only
Look Once) model is trained to detect cracking and spalling damage and exposed
bars. Another YOLO model is finetuned to distinguish different categories of
structural damage levels. All these trained models are used to create a hybrid
framework to automatically and reliably determine the damage levels from input
images. This research demonstrates that rapid and automated damage detection
following disasters is achievable across diverse damage contexts by utilizing
image data collection, annotation, and deep learning approaches.

</details>


### [50] [ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models](https://arxiv.org/abs/2510.21069)
*Pranav Saxena,Jimmy Chiun*

Main category: cs.CV

TL;DR: ZING-3D is a zero-shot framework that generates 3D scene graphs using pretrained foundation models, enabling open-vocabulary recognition, incremental updates, and geometric grounding without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene graph generation methods are limited to single-view settings, lack incremental update capabilities, and have no explicit geometric grounding, making them unsuitable for embodied robotics applications.

Method: Leverages VLM reasoning to generate rich 2D scene graphs, then grounds them in 3D using depth information. Nodes represent open-vocabulary objects with features, 3D locations, and semantic context, while edges capture spatial and semantic relations with inter-object distances.

Result: Experiments on Replica and HM3D datasets show ZING-3D effectively captures spatial and relational knowledge without task-specific training.

Conclusion: ZING-3D provides a practical solution for generating geometrically grounded 3D scene graphs suitable for downstream robotics applications, operating in a zero-shot manner using pretrained foundation models.

Abstract: Understanding and reasoning about complex 3D environments requires structured
scene representations that capture not only objects but also their semantic and
spatial relationships. While recent works on 3D scene graph generation have
leveraged pretrained VLMs without task-specific fine-tuning, they are largely
confined to single-view settings, fail to support incremental updates as new
observations arrive and lack explicit geometric grounding in 3D space, all of
which are essential for embodied scenarios. In this paper, we propose, ZING-3D,
a framework that leverages the vast knowledge of pretrained foundation models
to enable open-vocabulary recognition and generate a rich semantic
representation of the scene in a zero-shot manner while also enabling
incremental updates and geometric grounding in 3D space, making it suitable for
downstream robotics applications. Our approach leverages VLM reasoning to
generate a rich 2D scene graph, which is grounded in 3D using depth
information. Nodes represent open-vocabulary objects with features, 3D
locations, and semantic context, while edges capture spatial and semantic
relations with inter-object distances. Our experiments on scenes from the
Replica and HM3D dataset show that ZING-3D is effective at capturing spatial
and relational knowledge without the need of task-specific training.

</details>


### [51] [WaveSeg: Enhancing Segmentation Precision via High-Frequency Prior and Mamba-Driven Spectrum Decomposition](https://arxiv.org/abs/2510.21079)
*Guoan Xu,Yang Xiao,Wenjing Jia,Guangwei Gao,Guo-Jun Qi,Chia-Wen Lin*

Main category: cs.CV

TL;DR: WaveSeg is a novel decoder architecture for semantic segmentation that jointly optimizes feature refinement in spatial and wavelet domains, using wavelet-domain frequency priors and Mamba-based attention to achieve superior performance.


<details>
  <summary>Details</summary>
Motivation: Most semantic segmentation networks rely on powerful pretrained encoders but use simplistic decoders, resulting in suboptimal trade-offs between semantic context and fine-grained detail preservation.

Method: Proposes WaveSeg decoder with: 1) learning high-frequency components as explicit priors, 2) Dual Domain Operation (DDO) for multi-scale fusion, 3) Spectrum Decomposition Attention (SDA) using Mamba's linear-complexity modeling, 4) reparameterized convolutions for low-frequency preservation, and 5) residual-guided fusion for multi-scale feature integration.

Result: Extensive experiments show WaveSeg consistently outperforms state-of-the-art approaches both quantitatively and qualitatively, achieving efficient and precise segmentation on standard benchmarks.

Conclusion: WaveSeg demonstrates that leveraging wavelet-domain frequency priors with Mamba-based attention enables superior semantic segmentation performance by better balancing semantic context and fine-grained detail preservation.

Abstract: While recent semantic segmentation networks heavily rely on powerful
pretrained encoders, most employ simplistic decoders, leading to suboptimal
trade-offs between semantic context and fine-grained detail preservation. To
address this, we propose a novel decoder architecture, WaveSeg, which jointly
optimizes feature refinement in spatial and wavelet domains. Specifically,
high-frequency components are first learned from input images as explicit
priors to reinforce boundary details at early stages. A multi-scale fusion
mechanism, Dual Domain Operation (DDO), is then applied, and the novel Spectrum
Decomposition Attention (SDA) block is proposed, which is developed to leverage
Mamba's linear-complexity long-range modeling to enhance high-frequency
structural details. Meanwhile, reparameterized convolutions are applied to
preserve low-frequency semantic integrity in the wavelet domain. Finally, a
residual-guided fusion integrates multi-scale features with boundary-aware
representations at native resolution, producing semantically and structurally
rich feature maps. Extensive experiments on standard benchmarks demonstrate
that WaveSeg, leveraging wavelet-domain frequency prior with Mamba-based
attention, consistently outperforms state-of-the-art approaches both
quantitatively and qualitatively, achieving efficient and precise segmentation.

</details>


### [52] [Knowledge-Driven Vision-Language Model for Plexus Detection in Hirschsprung's Disease](https://arxiv.org/abs/2510.21083)
*Youssef Megahed,Atallah Madi,Dina El Demellawy,Adrian D. C. Chan*

Main category: cs.CV

TL;DR: A novel framework that integrates expert-derived textual concepts into a vision-language model for classifying myenteric plexus regions in Hirschsprung's disease, outperforming traditional CNN models.


<details>
  <summary>Details</summary>
Motivation: Deep learning approaches for Hirschsprung's disease diagnosis are often black boxes that don't align with physician decision-making, lacking clinical interpretability and expert knowledge integration.

Method: Integrates expert-derived textual concepts from medical sources into a Contrastive Language-Image Pre-training-based vision-language model, using LLM-generated prompts reviewed by experts and encoded with QuiltNet to align semantic cues with visual features.

Result: Achieved superior performance with 83.9% accuracy, 86.6% precision, and 87.6% specificity, outperforming CNN models including VGG-19, ResNet-18, and ResNet-50.

Conclusion: Multi-modal learning incorporating expert knowledge shows strong potential for histopathology applications, providing more clinically relevant and interpretable model outputs.

Abstract: Hirschsprung's disease is defined as the congenital absence of ganglion cells
in some segment(s) of the colon. The muscle cannot make coordinated movements
to propel stool in that section, most commonly leading to obstruction. The
diagnosis and treatment for this disease require a clear identification of
different region(s) of the myenteric plexus, where ganglion cells should be
present, on the microscopic view of the tissue slide. While deep learning
approaches, such as Convolutional Neural Networks, have performed very well in
this task, they are often treated as black boxes, with minimal understanding
gained from them, and may not conform to how a physician makes decisions. In
this study, we propose a novel framework that integrates expert-derived textual
concepts into a Contrastive Language-Image Pre-training-based vision-language
model to guide plexus classification. Using prompts derived from expert sources
(e.g., medical textbooks and papers) generated by large language models and
reviewed by our team before being encoded with QuiltNet, our approach aligns
clinically relevant semantic cues with visual features. Experimental results
show that the proposed model demonstrated superior discriminative capability
across different classification metrics as it outperformed CNN-based models,
including VGG-19, ResNet-18, and ResNet-50; achieving an accuracy of 83.9%, a
precision of 86.6%, and a specificity of 87.6%. These findings highlight the
potential of multi-modal learning in histopathology and underscore the value of
incorporating expert knowledge for more clinically relevant model outputs.

</details>


### [53] [HistRetinex: Optimizing Retinex model in Histogram Domain for Efficient Low-Light Image Enhancement](https://arxiv.org/abs/2510.21100)
*Jingtian Zhao,Xueli Xie,Jianxiang Xi,Xiaogang Yang,Haoxuan Sun*

Main category: cs.CV

TL;DR: HistRetinex extends Retinex model to histogram domain for fast low-light image enhancement, achieving better performance with significant speed improvements.


<details>
  <summary>Details</summary>
Motivation: Existing Retinex-based methods are time-consuming for large-sized images, so there's a need for faster enhancement approaches.

Method: Proposes histogram-based Retinex model using histogram location and count matrices, constructs two-level optimization model, and provides iterative formulas for illumination and reflectance histograms.

Result: Outperforms existing methods in visibility and performance metrics, executes in 1.86 seconds on 1000*664 resolution images, achieving minimum time saving of 6.67 seconds.

Conclusion: HistRetinex provides an efficient and effective solution for fast low-light image enhancement with superior performance and significant speed advantages.

Abstract: Retinex-based low-light image enhancement methods are widely used due to
their excellent performance. However, most of them are time-consuming for
large-sized images. This paper extends the Retinex model from the spatial
domain to the histogram domain, and proposes a novel histogram-based Retinex
model for fast low-light image enhancement, named HistRetinex. Firstly, we
define the histogram location matrix and the histogram count matrix, which
establish the relationship among histograms of the illumination, reflectance
and the low-light image. Secondly, based on the prior information and the
histogram-based Retinex model, we construct a novel two-level optimization
model. Through solving the optimization model, we give the iterative formulas
of the illumination histogram and the reflectance histogram, respectively.
Finally, we enhance the low-light image through matching its histogram with the
one provided by HistRetinex. Experimental results demonstrate that the
HistRetinex outperforms existing enhancement methods in both visibility and
performance metrics, while executing 1.86 seconds on 1000*664 resolution
images, achieving a minimum time saving of 6.67 seconds.

</details>


### [54] [PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments](https://arxiv.org/abs/2510.21111)
*Weijie Zhou,Xuantang Xiong,Yi Peng,Manli Tao,Chaoyang Zhao,Honghui Dong,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: Introduces Active Visual Reasoning (AVR) task for multimodal LLMs in partially observable environments, requiring agents to actively gather information through physical actions and integrate observations across multiple steps.


<details>
  <summary>Details</summary>
Motivation: Current visual reasoning in MLLMs is limited to static, fully observable settings, while real-world environments often have incomplete information due to occlusion or limited field of view. Humans actively explore environments to gather information through perception-reasoning-action loops.

Method: Created CLEVR-AVR benchmark with multi-round interactive environments, AVR-152k dataset with Chain-of-Thought annotations, and developed PhysVLM-AVR model that handles uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection.

Result: PhysVLM-AVR achieves state-of-the-art performance on CLEVR-AVR, embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath, Geometry30K). Analysis shows current embodied MLLMs struggle with active information acquisition and integration despite detecting information incompleteness.

Conclusion: The work demonstrates a fundamental gap in active reasoning capabilities of current MLLMs and provides a framework for developing agents that can actively explore and reason in partially observable environments through closed-loop perception-reasoning-action processes.

Abstract: Visual reasoning in multimodal large language models (MLLMs) has primarily
been studied in static, fully observable settings, limiting their effectiveness
in real-world environments where information is often incomplete due to
occlusion or limited field of view. Humans, in contrast, actively explore and
interact with their environment-moving, examining, and manipulating objects-to
gather information through a closed-loop process integrating perception,
reasoning, and action. Inspired by this human capability, we introduce the
Active Visual Reasoning (AVR) task, extending visual reasoning to partially
observable, interactive environments. AVR necessitates agents to: (1) actively
acquire information via sequential physical actions, (2) integrate observations
across multiple steps for coherent reasoning, and (3) dynamically adjust
decisions based on evolving visual feedback. To rigorously evaluate AVR, we
introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive
environments designed to assess both reasoning correctness and
information-gathering efficiency. We present AVR-152k, a large-scale dataset
that offers rich Chain-of-Thought (CoT) annotations detailing iterative
reasoning for uncertainty identification, action-conditioned information gain
prediction, and information-maximizing action selection, crucial for training
agents in a higher-order Markov Decision Process. Building on this, we develop
PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR,
embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath,
Geometry30K). Our analysis also reveals that current embodied MLLMs, despite
detecting information incompleteness, struggle to actively acquire and
integrate new information through interaction, highlighting a fundamental gap
in active reasoning capabilities.

</details>


### [55] [Urban 3D Change Detection Using LiDAR Sensor for HD Map Maintenance and Smart Mobility](https://arxiv.org/abs/2510.21112)
*Hezam Albagami,Haitian Wang,Xinyu Wang,Muhammad Ibrahim,Zainy M. Malakan,Abdullah M. Alqamdi,Mohammed H. Alghamdi,Ajmal Mian*

Main category: cs.CV

TL;DR: An object-centric, uncertainty-aware pipeline for 3D change detection in city-scale LiDAR data that handles registration errors, surface roughness, and object splits/merges while maintaining class consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LiDAR change detection are sensitive to alignment errors, degrade thin structures, lack object identity preservation, and don't handle uncertainty or object splits/merges effectively.

Method: Multi-resolution NDT alignment followed by point-to-plane ICP, height normalization, uncertainty estimation from registration covariance and surface roughness, geometry-based cross-epoch associations refined by semantic/instance segmentation, and class-constrained bipartite assignment with dummy handling for splits/merges.

Result: Achieved 95.2% accuracy, 90.4% mF1, and 82.6% mIoU on 15 Subiaco blocks, outperforming Triplet KPConv by 0.2% in accuracy, 0.2% in mF1, and 0.8% in mIoU, with largest improvement on Decreased class (74.8% IoU, +7.6 points).

Conclusion: The proposed uncertainty-aware, object-centric approach effectively handles real-world challenges in city-scale LiDAR change detection, particularly improving performance on decreased objects while maintaining class consistency and handling splits/merges.

Abstract: High-definition 3D city maps underpin smart transportation, digital twins,
and autonomous driving, where object level change detection across bi temporal
LiDAR enables HD map maintenance, construction monitoring, and reliable
localization. Classical DSM differencing and image based methods are sensitive
to small vertical bias, ground slope, and viewpoint mismatch and yield cellwise
outputs without object identity. Point based neural models and voxel encodings
demand large memory, assume near perfect pre alignment, degrade thin
structures, and seldom enforce class consistent association, which leaves split
or merge cases unresolved and ignores uncertainty. We propose an object
centric, uncertainty aware pipeline for city scale LiDAR that aligns epochs
with multi resolution NDT followed by point to plane ICP, normalizes height,
and derives a per location level of detection from registration covariance and
surface roughness to calibrate decisions and suppress spurious changes.
Geometry only proxies seed cross epoch associations that are refined by
semantic and instance segmentation and a class constrained bipartite assignment
with augmented dummies to handle splits and merges while preserving per class
counts. Tiled processing bounds memory without eroding narrow ground changes,
and instance level decisions combine 3D overlap, normal direction displacement,
and height and volume differences with a histogram distance, all gated by the
local level of detection to remain stable under partial overlap and sampling
variation. On 15 representative Subiaco blocks the method attains 95.2%
accuracy, 90.4% mF1, and 82.6% mIoU, exceeding Triplet KPConv by 0.2 percentage
points in accuracy, 0.2 in mF1, and 0.8 in mIoU, with the largest gain on
Decreased where IoU reaches 74.8% and improves by 7.6 points.

</details>


### [56] [Controllable-LPMoE: Adapting to Challenging Object Segmentation via Dynamic Local Priors from Mixture-of-Experts](https://arxiv.org/abs/2510.21114)
*Yanguang Sun,Jiawei Lian,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: Proposes Controllable-LPMoE, a dynamic priors-based fine-tuning method that adaptively modulates frozen foundation models using local priors for efficient object segmentation with fewer trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Full-parameter fine-tuning of large foundation models causes significant computational overhead, while existing prompt-based methods lack semantic priors, limiting model adaptability.

Method: Uses a lightweight dynamic mixed local priors extractor with heterogeneous convolutions and gating network, combined with a bi-directional interaction adapter using cosine-aligned deformable attention and channel-oriented adaptive scale enhancement.

Result: Achieves excellent segmentation performance compared to 31 state-of-the-art methods and demonstrates adaptability to multiple binary object segmentation tasks.

Conclusion: Controllable-LPMoE provides an efficient fine-tuning paradigm that enhances fine-grained perception for segmentation tasks while significantly reducing computational overhead.

Abstract: Large-scale foundation models provide powerful feature representations for
downstream object segmentation tasks. However, when adapted to specific tasks
through the full-parameter fine-tuning, the enormous parameters being updated
often results in significant computational overhead, creating a bottleneck in
training efficiency. Although existing methods attempt to fine-tune frozen
models by directly embedding trainable prompts, these prompts lack inherent
semantic priors, limiting the adaptability of large-scale models. In this
paper, we propose a novel dynamic priors-based fine-tuning paradigm with fewer
trainable parameters, dubbed Controllable-LPMoE, which adaptively modulates
frozen foundation models by dynamically controlling local priors to enhance
fine-grained perception for specific segmentation tasks. More specifically, we
construct a lightweight dynamic mixed local priors extractor that captures
diverse local priors from input images through heterogeneous convolutions while
employing a gating network to dynamically output expert priors required for the
subsequent fine-tuning. Furthermore, we design a bi-directional interaction
adapter that employs cosine-aligned deformable attention and channel-oriented
adaptive scale enhancement to interact and restructure between frozen and
trainable features, achieving efficient fine-tuning. Extensive experiments
validate the superiority of our
\href{https://github.com/CSYSI/Controllable-LPMoE} {Controllable-LPMoE}
approach, demonstrating excellent segmentation performance compared to 31
state-of-the-art (SOTA) methods and adaptability to multiple binary object
segmentation tasks.

</details>


### [57] [SafetyPairs: Isolating Safety Critical Image Features with Counterfactual Image Generation](https://arxiv.org/abs/2510.21120)
*Alec Helbling,Shruti Palaskar,Kundan Krishna,Polo Chau,Leon Gatys,Joseph Yitan Cheng*

Main category: cs.CV

TL;DR: SafetyPairs is a framework that generates counterfactual image pairs differing only in safety-relevant features to systematically study fine-grained image safety distinctions.


<details>
  <summary>Details</summary>
Motivation: Existing image safety datasets are coarse and ambiguous, lacking isolation of specific features that determine safety. Subtle changes like gestures or symbols can drastically alter safety implications.

Method: Leverage image editing models to make targeted changes that flip safety labels while preserving safety-irrelevant details, creating counterfactual pairs for systematic comparison.

Result: Created a benchmark with 3,020 SafetyPair images across 9 safety categories, revealing weaknesses in vision-language models' ability to distinguish subtle safety differences. Also improves training efficiency for guard models.

Conclusion: SafetyPairs provides the first systematic resource for studying fine-grained image safety distinctions and serves as both an evaluation benchmark and effective data augmentation strategy.

Abstract: What exactly makes a particular image unsafe? Systematically differentiating
between benign and problematic images is a challenging problem, as subtle
changes to an image, such as an insulting gesture or symbol, can drastically
alter its safety implications. However, existing image safety datasets are
coarse and ambiguous, offering only broad safety labels without isolating the
specific features that drive these differences. We introduce SafetyPairs, a
scalable framework for generating counterfactual pairs of images, that differ
only in the features relevant to the given safety policy, thus flipping their
safety label. By leveraging image editing models, we make targeted changes to
images that alter their safety labels while leaving safety-irrelevant details
unchanged. Using SafetyPairs, we construct a new safety benchmark, which serves
as a powerful source of evaluation data that highlights weaknesses in
vision-language models' abilities to distinguish between subtly different
images. Beyond evaluation, we find our pipeline serves as an effective data
augmentation strategy that improves the sample efficiency of training
lightweight guard models. We release a benchmark containing over 3,020
SafetyPair images spanning a diverse taxonomy of 9 safety categories, providing
the first systematic resource for studying fine-grained image safety
distinctions.

</details>


### [58] [NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation](https://arxiv.org/abs/2510.21122)
*Longtian Qiu,Shan Ning,Jiaxuan Sun,Xuming He*

Main category: cs.CV

TL;DR: NoisyGRPO is a multimodal RL framework that improves Chain-of-Thought reasoning in MLLMs by injecting Gaussian noise into visual inputs for better exploration and using Bayesian advantage estimation for more robust training.


<details>
  <summary>Details</summary>
Motivation: Existing RL frameworks struggle to generalize beyond training distribution when improving general CoT reasoning in multimodal large language models.

Method: Uses noise-injected exploration policy (Gaussian noise in visual inputs) and Bayesian advantage estimation that treats noise level as prior and trajectory reward as likelihood to compute robust posterior advantage estimates.

Result: Substantially improves generalization and robustness on CoT quality, general capability, and hallucination benchmarks, especially with small-scale MLLMs like Qwen2.5-VL 3B.

Conclusion: NoisyGRPO effectively enhances RL training for multimodal reasoning by combining controlled noise injection with principled Bayesian advantage estimation.

Abstract: Reinforcement learning (RL) has shown promise in enhancing the general
Chain-of-Thought (CoT) reasoning capabilities of multimodal large language
models (MLLMs). However, when applied to improve general CoT reasoning,
existing RL frameworks often struggle to generalize beyond the training
distribution. To address this, we propose NoisyGRPO, a systematic multimodal RL
framework that introduces controllable noise into visual inputs for enhanced
exploration and explicitly models the advantage estimation process via a
Bayesian framework. Specifically, NoisyGRPO improves RL training by: (1)
\textbf{Noise-Injected Exploration Policy}: Perturbing visual inputs with
Gaussian noise to encourage exploration across a wider range of visual
scenarios; and (2) \textbf{Bayesian Advantage Estimation}: Formulating
advantage estimation as a principled Bayesian inference problem, where the
injected noise level serves as a prior and the observed trajectory reward as
the likelihood. This Bayesian modeling fuses both sources of information to
compute a robust posterior estimate of trajectory advantage, effectively
guiding MLLMs to prefer visually grounded trajectories over noisy ones.
Experiments on standard CoT quality, general capability, and hallucination
benchmarks demonstrate that NoisyGRPO substantially improves generalization and
robustness, especially in RL settings with small-scale MLLMs such as Qwen2.5-VL
3B. The project page is available at
\href{https://artanic30.github.io/project_pages/NoisyGRPO/}{\texttt{https://artanic30.github.io/project\_pages/NoisyGRPO}}.

</details>


### [59] [Digital Contrast CT Pulmonary Angiography Synthesis from Non-contrast CT for Pulmonary Vascular Disease](https://arxiv.org/abs/2510.21140)
*Ying Ming,Yue Lin,Longfei Zhao,Gengwan Li,Zuopeng Tan,Bing Li,Sheng Xie,Wei Song,Qiqi Xu*

Main category: cs.CV

TL;DR: This paper proposes a method to generate Digital Contrast CTPA (DCCTPA) from Non-Contrast CT scans using CycleGAN to avoid risks of iodinated contrast agents, achieving superior image quality and enabling accurate pulmonary vessel segmentation and quantification.


<details>
  <summary>Details</summary>
Motivation: CTPA requires iodinated contrast agents that pose nephrotoxicity and allergic reaction risks, especially for high-risk patients. The study aims to create contrast-enhanced images without actual contrast injection.

Method: Used cascaded synthesizer based on Cycle-Consistent Generative Adversarial Networks (CycleGAN) trained on 410 paired CTPA and NCCT scans from three centers, with 249 for training/validation and 161 for testing.

Result: Achieved best performance with MAE: 165.12, PSNR: 20.27, SSIM: 0.98 on test set. Improved pulmonary vessel segmentation (Dice: 0.70, clDice: 0.71-0.72) and significantly better vessel volume correlation with CTPA (ICC: 0.81 vs 0.70 for NCCT).

Conclusion: The DCCTPA method successfully generates contrast-enhanced images from NCCT scans, providing valid vessel enhancement and structural preservation while eliminating contrast-related risks, with promising clinical applications.

Abstract: Computed Tomography Pulmonary Angiography (CTPA) is the reference standard
for diagnosing pulmonary vascular diseases such as Pulmonary Embolism (PE) and
Chronic Thromboembolic Pulmonary Hypertension (CTEPH). However, its reliance on
iodinated contrast agents poses risks including nephrotoxicity and allergic
reactions, particularly in high-risk patients. This study proposes a method to
generate Digital Contrast CTPA (DCCTPA) from Non-Contrast CT (NCCT) scans using
a cascaded synthesizer based on Cycle-Consistent Generative Adversarial
Networks (CycleGAN). Totally retrospective 410 paired CTPA and NCCT scans were
obtained from three centers. The model was trained and validated internally on
249 paired images. Extra dataset that comprising 161 paired images was as test
set for model generalization evaluation and downstream clinical tasks
validation. Compared with state-of-the-art (SOTA) methods, the proposed method
achieved the best comprehensive performance by evaluating quantitative metrics
(For validation, MAE: 156.28, PSNR: 20.71 and SSIM: 0.98; For test, MAE:
165.12, PSNR: 20.27 and SSIM: 0.98) and qualitative visualization,
demonstrating valid vessel enhancement, superior image fidelity and structural
preservation. The approach was further applied to downstream tasks of pulmonary
vessel segmentation and vascular quantification. On the test set, the average
Dice, clDice, and clRecall of artery and vein pulmonary segmentation was 0.70,
0.71, 0.73 and 0.70, 0.72, 0.75 respectively, all markedly improved compared
with NCCT inputs.\@ Inter-class Correlation Coefficient (ICC) for vessel volume
between DCCTPA and CTPA was significantly better than that between NCCT and
CTPA (Average ICC : 0.81 vs 0.70), indicating effective vascular enhancement in
DCCTPA, especially for small vessels.

</details>


### [60] [Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study](https://arxiv.org/abs/2510.21160)
*Guanlin Wu,Boyan Su,Yang Zhao,Pu Wang,Yichen Lin,Hao Frank Yang*

Main category: cs.CV

TL;DR: SIG is a grid-based schema that explicitly encodes spatial relationships and physical priors to improve visual-spatial intelligence evaluation in foundation models, separating spatial capability from language biases.


<details>
  <summary>Details</summary>
Motivation: Current methods proxy visual-spatial intelligence with textual prompts and VQA scoring, which obscures geometry, invites linguistic shortcuts, and weakens attribution to genuine spatial skills.

Method: Introduces Spatial Intelligence Grid (SIG) - a structured grid-based schema that encodes object layouts, inter-object relations, and physically grounded priors as a complementary channel to text for foundation-model reasoning.

Result: SIG yields consistently larger, more stable, and more comprehensive gains across all VSI metrics compared to VQA-only representations in few-shot learning with state-of-the-art multimodal LLMs. Also released SIGBench with 1.4K driving frames annotated with SIG labels and human gaze traces.

Conclusion: SIG shows promise as a data-labeling and training schema for learning visual-spatial intelligence, providing faithful, compositional representation of scene structure for foundation-model reasoning.

Abstract: How to integrate and verify spatial intelligence in foundation models remains
an open challenge. Current practice often proxies Visual-Spatial Intelligence
(VSI) with purely textual prompts and VQA-style scoring, which obscures
geometry, invites linguistic shortcuts, and weakens attribution to genuinely
spatial skills. We introduce Spatial Intelligence Grid (SIG): a structured,
grid-based schema that explicitly encodes object layouts, inter-object
relations, and physically grounded priors. As a complementary channel to text,
SIG provides a faithful, compositional representation of scene structure for
foundation-model reasoning. Building on SIG, we derive SIG-informed evaluation
metrics that quantify a model's intrinsic VSI, which separates spatial
capability from language priors. In few-shot in-context learning with
state-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG
yields consistently larger, more stable, and more comprehensive gains across
all VSI metrics compared to VQA-only representations, indicating its promise as
a data-labeling and training schema for learning VSI. We also release SIGBench,
a benchmark of 1.4K driving frames annotated with ground-truth SIG labels and
human gaze traces, supporting both grid-based machine VSI tasks and
attention-driven, human-like VSI tasks in autonomous-driving scenarios.

</details>


### [61] [Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation](https://arxiv.org/abs/2510.21167)
*Dogyun Park,Taehoon Lee,Minseok Joo,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: Blockwise Flow Matching (BFM) partitions the generative trajectory into temporal segments modeled by specialized velocity blocks, improving inference efficiency and sample quality through semantic feature guidance and feature residual approximation.


<details>
  <summary>Details</summary>
Motivation: Traditional Flow Matching models use a single large network for the entire generative trajectory, which struggles to capture distinct signal characteristics across timesteps and incurs substantial inference costs due to iterative evaluation.

Method: BFM divides the generative trajectory into multiple temporal segments, each handled by smaller specialized velocity blocks. It includes Semantic Feature Guidance to condition blocks on pretrained representations and Feature Residual Approximation to preserve semantic quality while reducing inference cost.

Result: Extensive experiments on ImageNet 256x256 show BFM establishes a substantially improved Pareto frontier over existing Flow Matching methods, achieving 2.1x to 4.9x accelerations in inference complexity at comparable generation performance.

Conclusion: BFM provides an efficient and high-fidelity alternative to traditional Flow Matching by leveraging specialized blockwise architecture and semantic feature conditioning, significantly reducing inference costs while maintaining generation quality.

Abstract: Recently, Flow Matching models have pushed the boundaries of high-fidelity
data generation across a wide range of domains. It typically employs a single
large network to learn the entire generative trajectory from noise to data.
Despite their effectiveness, this design struggles to capture distinct signal
characteristics across timesteps simultaneously and incurs substantial
inference costs due to the iterative evaluation of the entire model. To address
these limitations, we propose Blockwise Flow Matching (BFM), a novel framework
that partitions the generative trajectory into multiple temporal segments, each
modeled by smaller but specialized velocity blocks. This blockwise design
enables each block to specialize effectively in its designated interval,
improving inference efficiency and sample quality. To further enhance
generation fidelity, we introduce a Semantic Feature Guidance module that
explicitly conditions velocity blocks on semantically rich features aligned
with pretrained representations. Additionally, we propose a lightweight Feature
Residual Approximation strategy that preserves semantic quality while
significantly reducing inference cost. Extensive experiments on ImageNet
256x256 demonstrate that BFM establishes a substantially improved Pareto
frontier over existing Flow Matching methods, achieving 2.1x to 4.9x
accelerations in inference complexity at comparable generation performance.
Code is available at https://github.com/mlvlab/BFM.

</details>


### [62] [TokenCLIP: Token-wise Prompt Learning for Zero-shot Anomaly Detection](https://arxiv.org/abs/2510.21171)
*Qihang Zhou,Binbin Gao,Guansong Pang,Xin Wang,Jiming Chen,Shibo He*

Main category: cs.CV

TL;DR: TokenCLIP is a token-wise adaptation framework that enables dynamic alignment between visual and learnable textual spaces for fine-grained anomaly detection, improving over existing CLIP-based methods that use single textual spaces.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based anomaly detection methods use a single textual space to align with visual semantics, which hinders accurate capture of varied anomaly semantics across diverse objects and domains.

Method: TokenCLIP expands token-agnostic textual space into orthogonal subspaces, dynamically assigns each visual token to subspace combinations via optimal transport based on semantic affinity, and applies top-k masking to specialize subspaces for distinct visual regions.

Result: Extensive experiments demonstrate the superiority of TokenCLIP over existing methods for anomaly detection on unseen objects.

Conclusion: TokenCLIP enables fine-grained anomaly learning through dynamic token-wise alignment between visual and textual spaces, overcoming limitations of single-space alignment approaches.

Abstract: Adapting CLIP for anomaly detection on unseen objects has shown strong
potential in a zero-shot manner. However, existing methods typically rely on a
single textual space to align with visual semantics across diverse objects and
domains. The indiscriminate alignment hinders the model from accurately
capturing varied anomaly semantics. We propose TokenCLIP, a token-wise
adaptation framework that enables dynamic alignment between visual and
learnable textual spaces for fine-grained anomaly learning. Rather than mapping
all visual tokens to a single, token-agnostic textual space, TokenCLIP aligns
each token with a customized textual subspace that represents its visual
characteristics. Explicitly assigning a unique learnable textual space to each
token is computationally intractable and prone to insufficient optimization. We
instead expand the token-agnostic textual space into a set of orthogonal
subspaces, and then dynamically assign each token to a subspace combination
guided by semantic affinity, which jointly supports customized and efficient
token-wise adaptation. To this end, we formulate dynamic alignment as an
optimal transport problem, where all visual tokens in an image are transported
to textual subspaces based on semantic similarity. The transport constraints of
OT ensure sufficient optimization across subspaces and encourage them to focus
on different semantics. Solving the problem yields a transport plan that
adaptively assigns each token to semantically relevant subspaces. A top-k
masking is then applied to sparsify the plan and specialize subspaces for
distinct visual regions. Extensive experiments demonstrate the superiority of
TokenCLIP.

</details>


### [63] [KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution](https://arxiv.org/abs/2510.21182)
*Junzhe Zhang,Huixuan Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: KBE is a dynamic multimodal evaluation framework that transforms static VQA benchmarks into evolving versions using graph formulation and knowledge integration to address data contamination and saturation issues.


<details>
  <summary>Details</summary>
Motivation: Existing static benchmarks for multimodal LLMs suffer from data contamination and saturation, leading to unreliable performance evaluations that don't reflect true model capabilities.

Method: Uses graph formulation to represent VQA samples, then expands static benchmarks by integrating multimodal knowledge through reconstructing questions by re-selecting visual information and expanding questions with external textual knowledge.

Result: KBE enables difficulty-controllable evaluation and provides more comprehensive assessment of MLLM capabilities while alleviating data contamination and saturation risks.

Conclusion: The proposed Knowledge-enhanced Benchmark Evolution framework successfully addresses limitations of static benchmarks and offers a more reliable evaluation protocol for multimodal large language models.

Abstract: The rapid progress of multimodal large language models (MLLMs) calls for more
reliable evaluation protocols. Existing static benchmarks suffer from the
potential risk of data contamination and saturation, leading to inflated or
misleading performance evaluations. To address these issues, we first apply
Graph formulation to represent a static or dynamic VQA sample. With the
formulation, we propose Knowledge-enhanced Benchmark Evolution(KBE), a dynamic
multimodal evaluation framework. KBE first analyzes the original static
benchmark, then expands it by integrating multimodal knowledge, transforming
the static benchmark into a controllable, dynamic evolving version. Crucially,
KBE can both reconstruct questions by Re-selecting visual information in the
original image and expand existing questions with external textual knowledge.
It enables difficulty-controllable evaluation by adjusting the degree of
question exploration. Extensive experiments demonstrate that KBE alleviates the
risk of data contamination, data saturation, and provides a more comprehensive
assessment of MLLM capabilities.

</details>


### [64] [3rd Place Solution to ICCV LargeFineFoodAI Retrieval](https://arxiv.org/abs/2510.21198)
*Yang Zhong,Zhiming Wang,Zhaoyang Li,Jinyu Ma,Xiang Li*

Main category: cs.CV

TL;DR: 3rd place solution for ICCV LargeFineFoodAI Retrieval Competition using ensemble models with ArcFace and Circle loss, TTA, and a new diffusion-based reranking method.


<details>
  <summary>Details</summary>
Motivation: To develop an effective solution for food image retrieval competition that improves feature representation and retrieval accuracy.

Method: Trained four basic models with weighted ArcFace and Circle loss, applied TTA and ensemble, and proposed new diffusion-based k-reciprocal reranking method.

Result: Achieved 0.81219 and 0.81191 mAP@100 on public and private leaderboards respectively.

Conclusion: The proposed ensemble approach with novel reranking method effectively improved retrieval performance in the food image competition.

Abstract: This paper introduces the 3rd place solution to the ICCV LargeFineFoodAI
Retrieval Competition on Kaggle. Four basic models are independently trained
with the weighted sum of ArcFace and Circle loss, then TTA and Ensemble are
successively applied to improve feature representation ability. In addition, a
new reranking method for retrieval is proposed based on diffusion and
k-reciprocal reranking. Finally, our method scored 0.81219 and 0.81191 mAP@100
on the public and private leaderboard, respectively.

</details>


### [65] [3rd Place Solution to Large-scale Fine-grained Food Recognition](https://arxiv.org/abs/2510.21199)
*Yang Zhong,Yifan Yao,Tong Luo,Youcai Zhang,Yaqian Li*

Main category: cs.CV

TL;DR: The paper presents a 3rd-place winning solution for the LargeFineFoodAI-ICCV Workshop-Recognition challenge that combines Arcface and Circle loss for improved fine-grained food recognition performance.


<details>
  <summary>Details</summary>
Motivation: Fine-grained food recognition is important for health applications, and the authors aimed to develop an effective solution for the LargeFineFoodAI competition.

Method: Used a combination of Arcface loss and Circle loss with carefully tuned configurations, and employed model ensembling for the final results.

Result: The solution achieved 3rd place in the LargeFineFoodAI-ICCV Workshop-Recognition challenge on Kaggle.

Conclusion: Proper combination of Arcface and Circle loss can improve fine-grained food recognition performance, as demonstrated by the competition success.

Abstract: Food analysis is becoming a hot topic in health area, in which fine-grained
food recognition task plays an important role. In this paper, we describe the
details of our solution to the LargeFineFoodAI-ICCV Workshop-Recognition
challenge held on Kaggle. We find a proper combination of Arcface loss[1] and
Circle loss[9] can bring improvement to the performance. With Arcface and the
combined loss, model was trained with carefully tuned configurations and
ensembled to get the final results. Our solution won the 3rd place in the
competition.

</details>


### [66] [Improved Training Technique for Shortcut Models](https://arxiv.org/abs/2510.21250)
*Anh Nguyen,Viet Nguyen,Duc Vu,Trung Dao,Chi Tran,Toan Tran,Anh Tran*

Main category: cs.CV

TL;DR: The paper introduces iSM, a unified training framework that addresses five core limitations of shortcut models in generative modeling, achieving substantial performance improvements on ImageNet 256×256.


<details>
  <summary>Details</summary>
Motivation: Shortcut models have promising non-adversarial generative capabilities but face critical performance bottlenecks including compounding guidance artifacts, inflexible fixed guidance, frequency bias, divergent self-consistency, and curvy flow trajectories that hinder widespread adoption.

Method: Proposes iSM framework with four key improvements: Intrinsic Guidance for dynamic control over guidance strength, Multi-Level Wavelet Loss to mitigate frequency bias, Scaling Optimal Transport (sOT) for straighter generative paths and reduced variance, and Twin EMA strategy to reconcile training stability with self-consistency.

Result: Extensive experiments on ImageNet 256×256 demonstrate substantial FID improvements over baseline shortcut models across one-step, few-step, and multi-step generation, making shortcut models a viable and competitive class of generative models.

Conclusion: The iSM framework systematically resolves the core limitations of shortcut models, enabling them to become a competitive alternative in generative modeling with improved performance across different sampling modes.

Abstract: Shortcut models represent a promising, non-adversarial paradigm for
generative modeling, uniquely supporting one-step, few-step, and multi-step
sampling from a single trained network. However, their widespread adoption has
been stymied by critical performance bottlenecks. This paper tackles the five
core issues that held shortcut models back: (1) the hidden flaw of compounding
guidance, which we are the first to formalize, causing severe image artifacts;
(2) inflexible fixed guidance that restricts inference-time control; (3) a
pervasive frequency bias driven by a reliance on low-level distances in the
direct domain, which biases reconstructions toward low frequencies; (4)
divergent self-consistency arising from a conflict with EMA training; and (5)
curvy flow trajectories that impede convergence. To address these challenges,
we introduce iSM, a unified training framework that systematically resolves
each limitation. Our framework is built on four key improvements: Intrinsic
Guidance provides explicit, dynamic control over guidance strength, resolving
both compounding guidance and inflexibility. A Multi-Level Wavelet Loss
mitigates frequency bias to restore high-frequency details. Scaling Optimal
Transport (sOT) reduces training variance and learns straighter, more stable
generative paths. Finally, a Twin EMA strategy reconciles training stability
with self-consistency. Extensive experiments on ImageNet 256 x 256 demonstrate
that our approach yields substantial FID improvements over baseline shortcut
models across one-step, few-step, and multi-step generation, making shortcut
models a viable and competitive class of generative models.

</details>


### [67] [Topology Sculptor, Shape Refiner: Discrete Diffusion Model for High-Fidelity 3D Meshes Generation](https://arxiv.org/abs/2510.21264)
*Kaiyu Song,Hanjiang Lai,Yaqing Zhang,Chuangjian Cai,Yan Pan Kun Yue,Jian Yin*

Main category: cs.CV

TL;DR: TSSR is a novel method using Discrete Diffusion Models to generate high-quality 3D artist-style meshes with parallel generation, achieving up to 10,000 faces at 1024^3 resolution through decoupled training, improved architecture, and connection loss.


<details>
  <summary>Details</summary>
Motivation: To achieve highly accurate token prediction while enabling parallel generation, which offers significant advantages over sequential autoregressive methods by allowing the model to see all mesh tokens concurrently.

Method: Three key innovations: 1) Decoupled Training and Hybrid Inference separating topology sculpting and shape refinement stages, 2) Improved Hourglass Architecture with bidirectional attention and Rotational Positional Embeddings, 3) Novel Connection Loss as topological constraint.

Result: Extensive experiments show TSSR generates high-quality 3D artist-style meshes capable of achieving up to 10,000 faces at remarkable spatial resolution of 1024^3.

Conclusion: TSSR successfully demonstrates the capability to generate high-fidelity 3D meshes through parallel generation and strategic architectural innovations, with code to be released publicly.

Abstract: In this paper, we introduce Topology Sculptor, Shape Refiner (TSSR), a novel
method for generating high-quality, artist-style 3D meshes based on Discrete
Diffusion Models (DDMs). Our primary motivation for TSSR is to achieve highly
accurate token prediction while enabling parallel generation, a significant
advantage over sequential autoregressive methods. By allowing TSSR to "see" all
mesh tokens concurrently, we unlock a new level of efficiency and control. We
leverage this parallel generation capability through three key innovations: 1)
Decoupled Training and Hybrid Inference, which distinctly separates the
DDM-based generation into a topology sculpting stage and a subsequent shape
refinement stage. This strategic decoupling enables TSSR to effectively capture
both intricate local topology and overarching global shape. 2) An Improved
Hourglass Architecture, featuring bidirectional attention enriched by
face-vertex-sequence level Rotational Positional Embeddings (RoPE), thereby
capturing richer contextual information across the mesh structure. 3) A novel
Connection Loss, which acts as a topological constraint to further enhance the
realism and fidelity of the generated meshes. Extensive experiments on complex
datasets demonstrate that TSSR generates high-quality 3D artist-style meshes,
capable of achieving up to 10,000 faces at a remarkable spatial resolution of
$1024^3$. The code will be released at:
https://github.com/psky1111/Tencent-TSSR.

</details>


### [68] [Towards Physically Executable 3D Gaussian for Embodied Navigation](https://arxiv.org/abs/2510.21307)
*Bingchen Miao,Rong Wei,Zhiqi Ge,Xiaoquan sun,Shiqi Gao,Jingzhe Zhu,Renhan Wang,Siliang Tang,Jun Xiao,Rui Tang,Juncheng Li*

Main category: cs.CV

TL;DR: SAGE-3D upgrades 3D Gaussian Splatting (3DGS) with object-level semantics and physics for Visual-Language Navigation, creating executable environments that improve navigation performance by 31% on unseen tasks.


<details>
  <summary>Details</summary>
Motivation: 3DGS provides photorealistic rendering but lacks fine-grained semantics and physical executability needed for Visual-Language Navigation (VLN), creating a gap between simulation and real-world navigation.

Method: Two components: (1) Object-Centric Semantic Grounding adds object-level annotations to 3DGS, and (2) Physics-Aware Execution Jointing embeds collision objects and constructs physical interfaces. Also releases InteriorGS dataset and SAGE-Bench benchmark.

Result: 3DGS scene data is more difficult to converge but exhibits strong generalizability, improving baseline performance by 31% on VLN-CE Unseen task. Created 1K object-annotated 3DGS scenes and 2M VLN data benchmark.

Conclusion: SAGE-3D successfully bridges the sim-to-real gap for VLN by making 3DGS semantically and physically aligned, demonstrating improved navigation performance and strong generalization capabilities.

Abstract: 3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic
real-time rendering capabilities, is regarded as an effective tool for
narrowing the sim-to-real gap. However, it lacks fine-grained semantics and
physical executability for Visual-Language Navigation (VLN). To address this,
we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments
for 3D Navigation), a new paradigm that upgrades 3DGS into an executable,
semantically and physically aligned environment. It comprises two components:
(1) Object-Centric Semantic Grounding, which adds object-level fine-grained
annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds
collision objects into 3DGS and constructs rich physical interfaces. We release
InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and
introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data.
Experiments show that 3DGS scene data is more difficult to converge, while
exhibiting strong generalizability, improving baseline performance by 31% on
the VLN-CE Unseen task. The data and code will be available soon.

</details>


### [69] [FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning](https://arxiv.org/abs/2510.21311)
*Lu Zhang,Jiazuo Yu,Haomiao Xiong,Ping Hu,Yunzhi Zhuge,Huchuan Lu,You He*

Main category: cs.CV

TL;DR: FineRS is a two-stage MLLM framework using reinforcement learning for joint reasoning and segmentation of extremely small objects in high-resolution images, featuring a coarse-to-fine pipeline with global semantic exploration and localized perceptual refinement.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with precise understanding and localization of visual details in high-resolution images, especially for extra-small objects in cluttered contexts due to restricted input resolutions.

Method: Two-stage framework with Global Semantic Exploration (GSE) for instruction-guided reasoning and coarse target region generation, and Localized Perceptual Refinement (LPR) for accurate bounding box and segmentation mask refinement. Uses locate-informed retrospective reward to couple stages.

Result: Outperforms state-of-the-art MLLM-based approaches on both instruction-guided segmentation and visual reasoning tasks on FineRS-4k and public datasets.

Conclusion: FineRS effectively addresses the challenge of localizing extremely small objects in high-resolution scenes through its two-stage reinforcement learning framework and novel dataset.

Abstract: Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
across a wide range of vision-language tasks. However, due to the restricted
input resolutions, MLLMs face significant challenges in precisely understanding
and localizing visual details in high-resolution images -- particularly when
dealing with extra-small objects embedded in cluttered contexts. To address
this issue, we propose \textsc{FineRS}, a two-stage MLLM-based reinforcement
learning framework for jointly reasoning and segmenting extremely small objects
within high-resolution scenes. \textsc{FineRS} adopts a coarse-to-fine pipeline
comprising Global Semantic Exploration (GSE) and Localized Perceptual
Refinement (LPR). Specifically, GSE performs instruction-guided reasoning to
generate a textural response and a coarse target region, while LPR refines this
region to produce an accurate bounding box and segmentation mask. To couple the
two stages, we introduce a locate-informed retrospective reward, where LPR's
outputs are used to optimize GSE for more robust coarse region exploration. %
Additionally, we present \textsc{FineRS}-4k, a new dataset for evaluating MLLMs
on attribute-level reasoning and pixel-level segmentation on subtle,
small-scale targets in complex high-resolution scenes. Experimental results on
\textsc{FineRS}-4k and public datasets demonstrate that our method consistently
outperforms state-of-the-art MLLM-based approaches on both instruction-guided
segmentation and visual reasoning tasks.

</details>


### [70] [VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set](https://arxiv.org/abs/2510.21323)
*Shufan Shen,Junshu Sun,Qingming Huang,Shuhui Wang*

Main category: cs.CV

TL;DR: VL-SAE is a sparse autoencoder that interprets vision-language alignment by mapping multi-modal representations to a unified concept set, where each neuron correlates to specific concepts represented by semantically similar images and texts.


<details>
  <summary>Details</summary>
Motivation: The interpretability of vision-language alignment in VLMs remains uninvestigated due to the difficulty in mapping multi-modal representations into a unified concept set.

Method: Proposes VL-SAE with distance-based encoder and modality-specific decoders, using cosine similarity for semantic alignment and encouraging consistent neuron activations for semantically similar representations during self-supervised training.

Result: VL-SAE demonstrates superior capability in interpreting and enhancing vision-language alignment across multiple VLMs (CLIP, LLaVA), improving performance in zero-shot image classification and hallucination elimination.

Conclusion: VL-SAE successfully provides interpretability for vision-language alignment by establishing neuron-concept correlations and enhances alignment at the concept level, benefiting downstream tasks.

Abstract: The alignment of vision-language representations endows current
Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities.
However, the interpretability of the alignment component remains uninvestigated
due to the difficulty in mapping the semantics of multi-modal representations
into a unified concept set. To address this problem, we propose VL-SAE, a
sparse autoencoder that encodes vision-language representations into its hidden
activations. Each neuron in its hidden layer correlates to a concept
represented by semantically similar images and texts, thereby interpreting
these representations with a unified concept set. To establish the
neuron-concept correlation, we encourage semantically similar representations
to exhibit consistent neuron activations during self-supervised training.
First, to measure the semantic similarity of multi-modal representations, we
perform their alignment in an explicit form based on cosine similarity. Second,
we construct the VL-SAE with a distance-based encoder and two modality-specific
decoders to ensure the activation consistency of semantically similar
representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)
demonstrate the superior capability of VL-SAE in interpreting and enhancing the
vision-language alignment. For interpretation, the alignment between vision and
language representations can be understood by comparing their semantics with
concepts. For enhancement, the alignment can be strengthened by aligning
vision-language representations at the concept level, contributing to
performance improvements in downstream tasks, including zero-shot image
classification and hallucination elimination. Codes are available at
https://github.com/ssfgunner/VL-SAE.

</details>


### [71] [Morphologically Intelligent Perturbation Prediction with FORM](https://arxiv.org/abs/2510.21337)
*Reed Naidoo,Matt De Vries,Olga Fourkioti,Vicky Bousgouni,Mar Arias-Garcia,Maria Portillo-Malumbres,Chris Bakal*

Main category: cs.CV

TL;DR: FORM is a machine learning framework that predicts 3D cellular structure changes under perturbations using a morphology encoder and diffusion-based trajectory module, enabling virtual cell modeling.


<details>
  <summary>Details</summary>
Motivation: Current computational models are limited to 2D representations, failing to capture the complexity of 3D cell morphology under perturbation, which hinders accurate virtual cell development.

Method: FORM uses a morphology encoder trained via multi-channel VQGAN to learn 3D cell shape representations, and a diffusion-based perturbation trajectory module that models morphological evolution across conditions.

Result: Trained on 65,000+ 3D cell volumes, FORM supports unconditional synthesis, conditional simulation, downstream signaling prediction, combinatorial perturbation effects, and morphodynamic transitions between unseen perturbations.

Conclusion: FORM and the MorphoEval benchmark advance 3D virtual cell realization by linking morphology, perturbation, and function through high-resolution predictive simulation.

Abstract: Understanding how cells respond to external stimuli is a central challenge in
biomedical research and drug development. Current computational frameworks for
modelling cellular responses remain restricted to two-dimensional
representations, limiting their capacity to capture the complexity of cell
morphology under perturbation. This dimensional constraint poses a critical
bottleneck for the development of accurate virtual cell models. Here, we
present FORM, a machine learning framework for predicting perturbation-induced
changes in three-dimensional cellular structure. FORM consists of two
components: a morphology encoder, trained end-to-end via a novel multi-channel
VQGAN to learn compact 3D representations of cell shape, and a diffusion-based
perturbation trajectory module that captures how morphology evolves across
perturbation conditions. Trained on a large-scale dataset of over 65,000
multi-fluorescence 3D cell volumes spanning diverse chemical and genetic
perturbations, FORM supports both unconditional morphology synthesis and
conditional simulation of perturbed cell states. Beyond generation, FORM can
predict downstream signalling activity, simulate combinatorial perturbation
effects, and model morphodynamic transitions between states of unseen
perturbations. To evaluate performance, we introduce MorphoEval, a benchmarking
suite that quantifies perturbation-induced morphological changes in structural,
statistical, and biological dimensions. Together, FORM and MorphoEval work
toward the realisation of the 3D virtual cell by linking morphology,
perturbation, and function through high-resolution predictive simulation.

</details>


### [72] [CT-CLIP: A Multi-modal Fusion Framework for Robust Apple Leaf Disease Recognition in Complex Environments](https://arxiv.org/abs/2510.21346)
*Lemin Liu,Fangchao Hu,Honghua Jiang,Yaru Chen,Limin Liu,Yongliang Qiao*

Main category: cs.CV

TL;DR: CT-CLIP framework combines CNN, Transformer, and CLIP for apple leaf disease recognition, achieving over 96% accuracy by fusing local and global features and using multimodal learning.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-scale feature fusion methods fail to adequately handle the phenotypic heterogeneity of apple leaf diseases and the relationships between local and global features in complex orchard environments.

Method: Proposes CNN-Transformer-CLIP (CT-CLIP) framework with CNN for local lesion details, Vision Transformer for global structural relationships, Adaptive Feature Fusion Module for dynamic fusion, and multimodal image-text learning using pre-trained CLIP weights.

Result: Achieved 97.38% accuracy on public dataset and 96.12% on self-built dataset, outperforming baseline methods and showing strong performance under few-shot conditions.

Conclusion: CT-CLIP provides an innovative and practical solution for automated disease recognition in agriculture, significantly enhancing identification accuracy under complex environmental conditions.

Abstract: In complex orchard environments, the phenotypic heterogeneity of different
apple leaf diseases, characterized by significant variation among lesions,
poses a challenge to traditional multi-scale feature fusion methods. These
methods only integrate multi-layer features extracted by convolutional neural
networks (CNNs) and fail to adequately account for the relationships between
local and global features. Therefore, this study proposes a multi-branch
recognition framework named CNN-Transformer-CLIP (CT-CLIP). The framework
synergistically employs a CNN to extract local lesion detail features and a
Vision Transformer to capture global structural relationships. An Adaptive
Feature Fusion Module (AFFM) then dynamically fuses these features, achieving
optimal coupling of local and global information and effectively addressing the
diversity in lesion morphology and distribution. Additionally, to mitigate
interference from complex backgrounds and significantly enhance recognition
accuracy under few-shot conditions, this study proposes a multimodal image-text
learning approach. By leveraging pre-trained CLIP weights, it achieves deep
alignment between visual features and disease semantic descriptions.
Experimental results show that CT-CLIP achieves accuracies of 97.38% and 96.12%
on a publicly available apple disease and a self-built dataset, outperforming
several baseline methods. The proposed CT-CLIP demonstrates strong capabilities
in recognizing agricultural diseases, significantly enhances identification
accuracy under complex environmental conditions, provides an innovative and
practical solution for automated disease recognition in agricultural
applications.

</details>


### [73] [Dynamic Semantic-Aware Correlation Modeling for UAV Tracking](https://arxiv.org/abs/2510.21351)
*Xinyu Zhou,Tongxin Pan,Lingyi Hong,Pinxue Guo,Haijing Guo,Zhaoyu Chen,Kaixun Jiang,Wenqiang Zhang*

Main category: cs.CV

TL;DR: Proposes a dynamic semantic aware correlation modeling framework for UAV tracking that enhances semantic relevance between template and search regions, improving accuracy under challenging conditions while offering speed-accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing UAV tracking methods focus on speed but lack semantic awareness, leading to poor performance under challenges like camera motion, fast motion, and low resolution.

Method: Uses a Dynamic Semantic Relevance Generator combined with Transformer correlation maps to explore semantic relevance between template and search regions, plus a pruning method for speed optimization.

Result: Achieves competitive performance on multiple UAV tracking datasets with improved accuracy and robustness under challenging conditions.

Conclusion: The proposed framework effectively addresses semantic awareness limitations in UAV tracking, providing flexible deployment options through speed-accuracy trade-offs.

Abstract: UAV tracking can be widely applied in scenarios such as disaster rescue,
environmental monitoring, and logistics transportation. However, existing UAV
tracking methods predominantly emphasize speed and lack exploration in semantic
awareness, which hinders the search region from extracting accurate
localization information from the template. The limitation results in
suboptimal performance under typical UAV tracking challenges such as camera
motion, fast motion, and low resolution, etc. To address this issue, we propose
a dynamic semantic aware correlation modeling tracking framework. The core of
our framework is a Dynamic Semantic Relevance Generator, which, in combination
with the correlation map from the Transformer, explore semantic relevance. The
approach enhances the search region's ability to extract important information
from the template, improving accuracy and robustness under the aforementioned
challenges. Additionally, to enhance the tracking speed, we design a pruning
method for the proposed framework. Therefore, we present multiple model
variants that achieve trade-offs between speed and accuracy, enabling flexible
deployment according to the available computational resources. Experimental
results validate the effectiveness of our method, achieving competitive
performance on multiple UAV tracking datasets. The code is available at
https://github.com/zxyyxzz/DSATrack.

</details>


### [74] [Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding](https://arxiv.org/abs/2510.21356)
*Anupam Pani,Yanchao Yang*

Main category: cs.CV

TL;DR: A gaze-regularized framework that enhances Vision-Language Models for egocentric understanding tasks using gaze only during training, improving prediction accuracy without requiring gaze inputs during inference.


<details>
  <summary>Details</summary>
Motivation: Eye gaze provides valuable cues about attention and future actions, making it a powerful signal for modeling egocentric behavior in real-world scenarios like assistive robots and human-machine collaboration.

Method: Introduces a gaze-regularized attention mechanism that aligns model focus with human visual gaze during training. The method is flexible and modular, working across multiple VLM architectures that use attention, without requiring gaze inputs during inference.

Result: Improves semantic prediction scores by up to 11% for future event prediction and around 7% for current activity understanding compared to baseline models trained without gaze regularization.

Conclusion: The work establishes a foundation for using human gaze to enhance VLM predictive capabilities, demonstrating that gaze-guided training improves accuracy and robustness for egocentric understanding tasks.

Abstract: Eye gaze offers valuable cues about attention, short-term intent, and future
actions, making it a powerful signal for modeling egocentric behavior. In this
work, we propose a gaze-regularized framework that enhances VLMs for two key
egocentric understanding tasks: fine-grained future event prediction and
current activity understanding. Unlike prior approaches that rely solely on
visual inputs or use gaze as an auxiliary input signal , our method uses gaze
only during training. We introduce a gaze-regularized attention mechanism that
aligns model focus with human visual gaze. This design is flexible and modular,
allowing it to generalize across multiple VLM architectures that utilize
attention. Experimental results show that our approach improves semantic
prediction scores by up to 11 for future event prediction and around 7 for
current activity understanding, compared to the corresponding baseline models
trained without gaze regularization. These results highlight the value of
gaze-guided training in improving the accuracy and robustness of egocentric
VLMs. Overall, this work establishes a foundation for using human gaze to
enhance the predictive capabilities of VLMs in real-world scenarios like
assistive robots and human-machine collaboration. Code and additional
information is available at: https://github.com/anupampani/Gaze-VLM

</details>


### [75] [Why Registration Quality Matters: Enhancing sCT Synthesis with IMPACT-Based Registration](https://arxiv.org/abs/2510.21358)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: A unified pipeline for synthetic CT generation from MRI and CBCT using a 2.5D U-Net++ with ResNet-34 encoder, trained with combined L1 and IMPACT-Synth perceptual loss, achieving improved anatomical fidelity with IMPACT-based registration over traditional mutual information methods.


<details>
  <summary>Details</summary>
Motivation: To develop robust synthetic CT generation that addresses registration bias and improves anatomical consistency in medical imaging, particularly for radiotherapy applications.

Method: 2.5D U-Net++ with ResNet-34 encoder trained jointly across anatomical regions, using combined L1 and IMPACT-Synth perceptual loss derived from SAM and TotalSegmentator, with test-time augmentation and five-fold ensembling.

Result: IMPACT-based registration achieved more accurate alignments and improved sCT synthesis with lower MAE and better anatomical structures on local tests, though Elastix-aligned data performed better on public validation due to registration bias.

Conclusion: Registration errors propagate into supervised learning and can inflate performance metrics at the expense of anatomical fidelity; IMPACT helps mitigate this bias for more robust and generalizable sCT synthesis.

Abstract: We participated in the SynthRAD2025 challenge (Tasks 1 and 2) with a unified
pipeline for synthetic CT (sCT) generation from MRI and CBCT, implemented using
the KonfAI framework. Our model is a 2.5D U-Net++ with a ResNet-34 encoder,
trained jointly across anatomical regions and fine-tuned per region. The loss
function combined pixel-wise L1 loss with IMPACT-Synth, a perceptual loss
derived from SAM and TotalSegmentator to enhance structural fidelity. Training
was performed using AdamW (initial learning rate = 0.001, halved every 25k
steps) on patch-based, normalized, body-masked inputs (320x320 for MRI, 256x256
for CBCT), with random flipping as the only augmentation. No post-processing
was applied. Final predictions leveraged test-time augmentation and five-fold
ensembling. The best model was selected based on validation MAE. Two
registration strategies were evaluated: (i) Elastix with mutual information,
consistent with the challenge pipeline, and (ii) IMPACT, a feature-based
similarity metric leveraging pretrained segmentation networks. On the local
test sets, IMPACT-based registration achieved more accurate and anatomically
consistent alignments than mutual-information-based registration, resulting in
improved sCT synthesis with lower MAE and more realistic anatomical structures.
On the public validation set, however, models trained with Elastix-aligned data
achieved higher scores, reflecting a registration bias favoring alignment
strategies consistent with the evaluation pipeline. This highlights how
registration errors can propagate into supervised learning, influencing both
training and evaluation, and potentially inflating performance metrics at the
expense of anatomical fidelity. By promoting anatomically consistent alignment,
IMPACT helps mitigate this bias and supports the development of more robust and
generalizable sCT synthesis models.

</details>


### [76] [BADiff: Bandwidth Adaptive Diffusion Model](https://arxiv.org/abs/2510.21366)
*Xi Zhang,Hanwei Zhu,Yan Zhong,Jiamang Wang,Weisi Lin*

Main category: cs.CV

TL;DR: A framework for diffusion models to adapt image generation quality based on real-time network bandwidth constraints, enabling early-stop sampling while maintaining perceptual quality appropriate to transmission conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion models use fixed denoising steps regardless of network limitations, leading to wasted computation and quality loss when heavy compression is needed for bandwidth-constrained transmission.

Method: Joint end-to-end training strategy where diffusion models are conditioned on target quality levels derived from available bandwidth, using lightweight quality embeddings to guide adaptive denoising with minimal architectural changes.

Result: Significantly improves visual fidelity of bandwidth-adapted generations compared to naive early-stopping, maintaining appropriate perceptual quality for given transmission conditions.

Conclusion: Offers a promising solution for efficient image delivery in bandwidth-constrained environments by enabling diffusion models to adapt generation quality to real-time network conditions.

Abstract: In this work, we propose a novel framework to enable diffusion models to
adapt their generation quality based on real-time network bandwidth
constraints. Traditional diffusion models produce high-fidelity images by
performing a fixed number of denoising steps, regardless of downstream
transmission limitations. However, in practical cloud-to-device scenarios,
limited bandwidth often necessitates heavy compression, leading to loss of fine
textures and wasted computation. To address this, we introduce a joint
end-to-end training strategy where the diffusion model is conditioned on a
target quality level derived from the available bandwidth. During training, the
model learns to adaptively modulate the denoising process, enabling early-stop
sampling that maintains perceptual quality appropriate to the target
transmission condition. Our method requires minimal architectural changes and
leverages a lightweight quality embedding to guide the denoising trajectory.
Experimental results demonstrate that our approach significantly improves the
visual fidelity of bandwidth-adapted generations compared to naive
early-stopping, offering a promising solution for efficient image delivery in
bandwidth-constrained environments. Code is available at:
https://github.com/xzhang9308/BADiff.

</details>


### [77] [TerraGen: A Unified Multi-Task Layout Generation Framework for Remote Sensing Data Augmentation](https://arxiv.org/abs/2510.21391)
*Datao Tang,Hao Wang,Yudeng Xin,Hui Qiao,Dongsheng Jiang,Yin Li,Zhiheng Yu,Xiangyong Cao*

Main category: cs.CV

TL;DR: TerraGen is a unified layout-to-image generation framework for remote sensing that enables spatially controllable synthesis across multiple vision tasks like detection and segmentation, addressing limitations of task-isolated generative models.


<details>
  <summary>Details</summary>
Motivation: Current generative data augmentation frameworks for remote sensing are task-isolated (require independent models for each task) and ignore geographical information and spatial constraints, limiting their effectiveness and efficiency.

Method: Introduces a geographic-spatial layout encoder that unifies bounding box and segmentation mask inputs, with multi-scale injection scheme and mask-weighted loss to encode spatial constraints from global to fine details. Also creates a large-scale multi-task dataset with 45k images.

Result: TerraGen achieves best generation image quality across diverse tasks and serves as a universal data-augmentation generator, significantly enhancing downstream task performance with robust cross-task generalization in both full-data and few-shot scenarios.

Conclusion: TerraGen provides a unified solution for spatially controllable remote sensing image generation that overcomes task isolation limitations and demonstrates strong performance across multiple vision tasks and data scenarios.

Abstract: Remote sensing vision tasks require extensive labeled data across multiple,
interconnected domains. However, current generative data augmentation
frameworks are task-isolated, i.e., each vision task requires training an
independent generative model, and ignores the modeling of geographical
information and spatial constraints. To address these issues, we propose
\textbf{TerraGen}, a unified layout-to-image generation framework that enables
flexible, spatially controllable synthesis of remote sensing imagery for
various high-level vision tasks, e.g., detection, segmentation, and extraction.
Specifically, TerraGen introduces a geographic-spatial layout encoder that
unifies bounding box and segmentation mask inputs, combined with a multi-scale
injection scheme and mask-weighted loss to explicitly encode spatial
constraints, from global structures to fine details. Also, we construct the
first large-scale multi-task remote sensing layout generation dataset
containing 45k images and establish a standardized evaluation protocol for this
task. Experimental results show that our TerraGen can achieve the best
generation image quality across diverse tasks. Additionally, TerraGen can be
used as a universal data-augmentation generator, enhancing downstream task
performance significantly and demonstrating robust cross-task generalisation in
both full-data and few-shot scenarios.

</details>


### [78] [Depth-Supervised Fusion Network for Seamless-Free Image Stitching](https://arxiv.org/abs/2510.21396)
*Zhiying Jiang,Ruhao Yan,Zengxi Zhang,Bowei Zhang,Jinyuan Liu*

Main category: cs.CV

TL;DR: A depth-consistency-constrained image stitching method that addresses parallax issues through multi-stage alignment with depth regularization and graph-based seam optimization.


<details>
  <summary>Details</summary>
Motivation: To solve ghosting and misalignment problems in image stitching caused by significant variations in object depth and large parallax.

Method: Multi-stage mechanism with global depth regularization constraints for alignment, graph-based optimal seam computation with soft-seam diffusion, and reparameterization strategy for efficiency.

Result: The method effectively mitigates alignment errors induced by parallax and achieves natural, seamless stitching results with improved efficiency.

Conclusion: The proposed approach demonstrates superior performance against existing methods in extensive experiments, successfully addressing parallax-induced stitching challenges.

Abstract: Image stitching synthesizes images captured from multiple perspectives into a
single image with a broader field of view. The significant variations in object
depth often lead to large parallax, resulting in ghosting and misalignment in
the stitched results. To address this, we propose a
depth-consistency-constrained seamless-free image stitching method. First, to
tackle the multi-view alignment difficulties caused by parallax, a multi-stage
mechanism combined with global depth regularization constraints is developed to
enhance the alignment accuracy of the same apparent target across different
depth ranges. Second, during the multi-view image fusion process, an optimal
stitching seam is determined through graph-based low-cost computation, and a
soft-seam region is diffused to precisely locate transition areas, thereby
effectively mitigating alignment errors induced by parallax and achieving
natural and seamless stitching results. Furthermore, considering the
computational overhead in the shift regression process, a reparameterization
strategy is incorporated to optimize the structural design, significantly
improving algorithm efficiency while maintaining optimal performance. Extensive
experiments demonstrate the superior performance of the proposed method against
the existing methods. Code is available at https://github.com/DLUT-YRH/DSFN.

</details>


### [79] [MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence](https://arxiv.org/abs/2510.21406)
*Yue Feng,Jinwei Hu,Qijia Lu,Jiawei Niu,Li Tan,Shuo Yuan,Ziyi Yan,Yizhen Jia,Qingzhi He,Shiping Ge,Ethan Q. Chen,Wentong Li,Limin Wang,Jie Qin*

Main category: cs.CV

TL;DR: Proposes MUVR, a new benchmark for multi-modal untrimmed video retrieval that supports video-centric queries with long text, tags, and masks, focusing on practical long-video platform applications.


<details>
  <summary>Details</summary>
Motivation: To advance video retrieval for long-video platforms by addressing the limitations of existing methods in processing untrimmed videos and multi-modal queries, as well as MLLMs in multi-video understanding and reranking.

Method: Constructs multi-level visual correspondence covering six levels (copy, event, scene, instance, action, others) and develops three versions of MUVR (Base, Filter, QA) with comprehensive evaluation criteria including a Reranking Score.

Result: MUVR consists of 53K untrimmed videos from Bilibili with 1,050 multi-modal queries and 84K matches. Extensive evaluations reveal limitations of current retrieval methods and MLLMs in handling untrimmed videos and multi-modal queries.

Conclusion: MUVR provides a comprehensive benchmark that exposes current limitations in video retrieval and MLLM capabilities, enabling future research in multi-modal untrimmed video retrieval for long-video platforms.

Abstract: We propose the Multi-modal Untrimmed Video Retrieval task, along with a new
benchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims
to retrieve untrimmed videos containing relevant segments using multi-modal
queries. It has the following features: 1) Practical retrieval paradigm: MUVR
supports video-centric multi-modal queries, expressing fine-grained retrieval
needs through long text descriptions, video tag prompts, and mask prompts. It
adopts a one-to-many retrieval paradigm and focuses on untrimmed videos,
tailored for long-video platform applications. 2) Multi-level visual
correspondence: To cover common video categories (e.g., news, travel, dance)
and precisely define retrieval matching criteria, we construct multi-level
visual correspondence based on core video content (e.g., news events, travel
locations, dance moves) which users are interested in and want to retrieve. It
covers six levels: copy, event, scene, instance, action, and others. 3)
Comprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,
Filter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA
assesses MLLMs in a question-answering format. We also propose a Reranking
Score to evaluate the reranking ability of MLLMs. MUVR consists of 53K
untrimmed videos from the video platform Bilibili, with 1,050 multi-modal
queries and 84K matches. Extensive evaluations of 3 state-of-the-art video
retrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals
the limitations of retrieval methods in processing untrimmed videos and
multi-modal queries, as well as MLLMs in multi-video understanding and
reranking. Our code and benchmark is available at
https://github.com/debby-0527/MUVR.

</details>


### [80] [Bridging the gap to real-world language-grounded visual concept learning](https://arxiv.org/abs/2510.21412)
*Whie Jung,Semin Kim,Junee Kim,Seunghoon Hong*

Main category: cs.CV

TL;DR: A framework for adaptive visual concept learning that identifies image-related concept axes and grounds visual concepts along these axes in real-world scenes using pretrained vision-language models and universal prompting.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to language-grounded visual concept learning are limited to few predefined axes (like color and shape) and typically work only in synthetic datasets, while human intelligence can interpret scenes along rich semantic dimensions.

Method: Uses pretrained vision-language model with universal prompting strategy to identify diverse image-related axes without prior knowledge. A universal concept encoder binds visual features to discovered axes without additional parameters. Optimizes compositional anchoring objective to ensure axes can be independently manipulated.

Result: Demonstrated effectiveness on ImageNet, CelebA-HQ, and AFHQ datasets, showing superior editing capabilities across diverse real-world concepts and strong compositional generalization compared to existing methods.

Conclusion: The framework enables scalable visual concept learning that adaptively discovers and grounds concepts along meaningful axes in real-world scenes, overcoming limitations of predefined concept sets and outperforming existing approaches.

Abstract: Human intelligence effortlessly interprets visual scenes along a rich
spectrum of semantic dimensions. However, existing approaches to
language-grounded visual concept learning are limited to a few predefined
primitive axes, such as color and shape, and are typically explored in
synthetic datasets. In this work, we propose a scalable framework that
adaptively identifies image-related concept axes and grounds visual concepts
along these axes in real-world scenes. Leveraging a pretrained vision-language
model and our universal prompting strategy, our framework identifies a diverse
image-related axes without any prior knowledge. Our universal concept encoder
adaptively binds visual features to the discovered axes without introducing
additional model parameters for each concept. To ground visual concepts along
the discovered axes, we optimize a compositional anchoring objective, which
ensures that each axis can be independently manipulated without affecting
others. We demonstrate the effectiveness of our framework on subsets of
ImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across
diverse real-world concepts that are too varied to be manually predefined. Our
method also exhibits strong compositional generalization, outperforming
existing visual concept learning and text-based editing methods. The code is
available at https://github.com/whieya/Language-grounded-VCL.

</details>


### [81] [ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents](https://arxiv.org/abs/2510.21432)
*Honghua Chen,Yushi Lan,Yongwei Chen,Xingang Pan*

Main category: cs.CV

TL;DR: ArtiLatent is a generative framework that synthesizes articulated 3D objects with fine geometry, accurate articulation, and realistic appearance using a unified latent space and latent diffusion model.


<details>
  <summary>Details</summary>
Motivation: To create articulated 3D objects that maintain geometric consistency and visual realism across different articulation states, addressing the challenge of handling articulation-dependent visibility changes.

Method: Jointly models part geometry and articulation dynamics using sparse voxel representations in a unified latent space via VAE, trains latent diffusion model for sampling, and uses articulation-aware Gaussian decoder for photorealistic reconstruction.

Result: Outperforms existing approaches on furniture-like objects from PartNet-Mobility and ACD datasets in geometric consistency and appearance fidelity.

Conclusion: Provides a scalable solution for articulated 3D object synthesis and manipulation with improved visual realism across articulation configurations.

Abstract: We propose ArtiLatent, a generative framework that synthesizes human-made 3D
objects with fine-grained geometry, accurate articulation, and realistic
appearance. Our approach jointly models part geometry and articulation dynamics
by embedding sparse voxel representations and associated articulation
properties, including joint type, axis, origin, range, and part category, into
a unified latent space via a variational autoencoder. A latent diffusion model
is then trained over this space to enable diverse yet physically plausible
sampling. To reconstruct photorealistic 3D shapes, we introduce an
articulation-aware Gaussian decoder that accounts for articulation-dependent
visibility changes (e.g., revealing the interior of a drawer when opened). By
conditioning appearance decoding on articulation state, our method assigns
plausible texture features to regions that are typically occluded in static
poses, significantly improving visual realism across articulation
configurations. Extensive experiments on furniture-like objects from
PartNet-Mobility and ACD datasets demonstrate that ArtiLatent outperforms
existing approaches in geometric consistency and appearance fidelity. Our
framework provides a scalable solution for articulated 3D object synthesis and
manipulation.

</details>


### [82] [Anisotropic Pooling for LUT-realizable CNN Image Restoration](https://arxiv.org/abs/2510.21437)
*Xi Zhang,Xiaolin Wu*

Main category: cs.CV

TL;DR: The paper proposes anisotropic pooling methods to replace average pooling in LUT-based CNN image restoration, improving performance by better handling anisotropic signal structures.


<details>
  <summary>Details</summary>
Motivation: Current LUT-based CNN restoration methods use average pooling for fusion, which is ill-suited for anisotropic signal structures and limits performance.

Method: Introduces generalized median pooling and extends it with learnable data-dependent pooling coefficients that adaptively weigh contributions from differently oriented pixel patches.

Result: Experimental results on various restoration benchmarks show both perceptually and numerically superior results compared to existing LUT-realizable CNN methods.

Conclusion: Anisotropic pooling strategies significantly improve LUT-based CNN restoration performance by better handling directional signal structures.

Abstract: Table look-up realization of image restoration CNNs has the potential of
achieving competitive image quality while being much faster and resource frugal
than the straightforward CNN implementation. The main technical challenge
facing the LUT-based CNN algorithm designers is to manage the table size
without overly restricting the receptive field. The prevailing strategy is to
reuse the table for small pixel patches of different orientations (apparently
assuming a degree of isotropy) and then fuse the look-up results. The fusion is
currently done by average pooling, which we find being ill suited to
anisotropic signal structures. To alleviate the problem, we investigate and
discuss anisotropic pooling methods to replace naive averaging for improving
the performance of the current LUT-realizable CNN restoration methods. First,
we introduce the method of generalized median pooling which leads to measurable
gains over average pooling. We then extend this idea by learning data-dependent
pooling coefficients for each orientation, so that they can adaptively weigh
the contributions of differently oriented pixel patches. Experimental results
on various restoration benchmarks show that our anisotropic pooling strategy
yields both perceptually and numerically superior results compared to existing
LUT-realizable CNN methods.

</details>


### [83] [OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields](https://arxiv.org/abs/2510.21441)
*Lisa Weijler,Sebastian Koch,Fabio Poiesi,Timo Ropinski,Pedro Hermosilla*

Main category: cs.CV

TL;DR: OpenHype introduces a continuous hyperbolic latent space to model 3D scene hierarchies, overcoming limitations of existing methods that require multiple rendering passes or rely on predefined discrete hierarchies.


<details>
  <summary>Details</summary>
Motivation: To enable holistic 3D scene understanding for autonomous agents by modeling hierarchical structures, which existing implicit representation methods struggle with due to inefficiency and poor generalization.

Method: Uses hyperbolic geometry to represent scene hierarchies in a continuous latent space, allowing multi-scale relationship encoding and smooth hierarchy traversal through geodesic paths.

Result: Outperforms state-of-the-art approaches on standard benchmarks, demonstrating superior efficiency and adaptability in 3D scene understanding.

Conclusion: OpenHype successfully addresses hierarchical 3D scene modeling challenges through hyperbolic latent representations, offering improved performance and generalization capabilities.

Abstract: Modeling the inherent hierarchical structure of 3D objects and 3D scenes is
highly desirable, as it enables a more holistic understanding of environments
for autonomous agents. Accomplishing this with implicit representations, such
as Neural Radiance Fields, remains an unexplored challenge. Existing methods
that explicitly model hierarchical structures often face significant
limitations: they either require multiple rendering passes to capture
embeddings at different levels of granularity, significantly increasing
inference time, or rely on predefined, closed-set discrete hierarchies that
generalize poorly to the diverse and nuanced structures encountered by agents
in the real world. To address these challenges, we propose OpenHype, a novel
approach that represents scene hierarchies using a continuous hyperbolic latent
space. By leveraging the properties of hyperbolic geometry, OpenHype naturally
encodes multi-scale relationships and enables smooth traversal of hierarchies
through geodesic paths in latent space. Our method outperforms state-of-the-art
approaches on standard benchmarks, demonstrating superior efficiency and
adaptability in 3D scene understanding.

</details>


### [84] [PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis](https://arxiv.org/abs/2510.21447)
*Yu Yang,Zhilu Zhang,Xiang Zhang,Yihan Zeng,Hui Li,Wangmeng Zuo*

Main category: cs.CV

TL;DR: PhysWorld is a framework that uses a simulator to create diverse demonstrations for learning efficient world models of deformable objects, achieving fast and accurate predictions with 47x speedup over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Learning physics-consistent dynamics models from limited real-world video data is challenging, especially for deformable objects with spatially-varying physical properties.

Method: Constructs a physics-consistent digital twin in MPM simulator via constitutive model selection and global-to-local optimization, applies part-aware perturbations to generate diverse demonstrations, and trains a lightweight GNN-based world model embedded with physical properties.

Result: Achieves accurate and fast future predictions for various deformable objects, generalizes well to novel interactions, and enables inference speeds 47 times faster than PhysTwin.

Conclusion: PhysWorld effectively overcomes data scarcity by leveraging simulator-generated demonstrations to learn efficient world models for deformable object dynamics.

Abstract: Interactive world models that simulate object dynamics are crucial for
robotics, VR, and AR. However, it remains a significant challenge to learn
physics-consistent dynamics models from limited real-world video data,
especially for deformable objects with spatially-varying physical properties.
To overcome the challenge of data scarcity, we propose PhysWorld, a novel
framework that utilizes a simulator to synthesize physically plausible and
diverse demonstrations to learn efficient world models. Specifically, we first
construct a physics-consistent digital twin within MPM simulator via
constitutive model selection and global-to-local optimization of physical
properties. Subsequently, we apply part-aware perturbations to the physical
properties and generate various motion patterns for the digital twin,
synthesizing extensive and diverse demonstrations. Finally, using these
demonstrations, we train a lightweight GNN-based world model that is embedded
with physical properties. The real video can be used to further refine the
physical properties. PhysWorld achieves accurate and fast future predictions
for various deformable objects, and also generalizes well to novel
interactions. Experiments show that PhysWorld has competitive performance while
enabling inference speeds 47 times faster than the recent state-of-the-art
method, i.e., PhysTwin.

</details>


### [85] [MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection](https://arxiv.org/abs/2510.21449)
*Shengtian Yang,Yue Feng,Yingshi Liu,Jingrou Zhang,Jie Qin*

Main category: cs.CV

TL;DR: MoniTor is a training-free online video anomaly detection method that uses vision-language models with memory-based scoring queues and LSTM-inspired prediction to handle real-time video streams without requiring model training.


<details>
  <summary>Details</summary>
Motivation: Online Video Anomaly Detection (VAD) has received little attention due to real-time constraints and computational intensity, despite recent advances in large language models and vision-language models that could enable more nuanced anomaly understanding.

Method: Uses streaming input to pre-trained VLMs with LSTM-inspired prediction mechanism to capture temporal dependencies, plus a scoring queue and anomaly prior to dynamically store recent scores and guide LLMs in distinguishing normal vs abnormal behaviors over time.

Result: Outperforms state-of-the-art methods on UCF-Crime and XD-Violence datasets, and is competitive with weakly supervised methods despite requiring no training.

Conclusion: MoniTor effectively addresses online VAD challenges by leveraging pre-trained models with memory-based mechanisms, demonstrating strong performance without the need for training.

Abstract: Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors
within videos. Recently, offline VAD has garnered substantial research
attention, which has been invigorated by the progress in large language models
(LLMs) and vision-language models (VLMs), offering the potential for a more
nuanced understanding of anomalies. However, online VAD has seldom received
attention due to real-time constraints and computational intensity. In this
paper, we introduce a novel Memory-based online scoring queue scheme for
Training-free VAD (MoniTor), to address the inherent complexities in online
VAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the
capabilities of pre-trained large-scale models. To capture temporal
dependencies more effectively, we incorporate a novel prediction mechanism
inspired by Long Short-Term Memory (LSTM) networks. This ensures the model can
effectively model past states and leverage previous predictions to identify
anomalous behaviors. Thereby, it better understands the current frame.
Moreover, we design a scoring queue and an anomaly prior to dynamically store
recent scores and cover all anomalies in the monitoring scenario, providing
guidance for LLMs to distinguish between normal and abnormal behaviors over
time. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and
XD-Violence) containing various surveillance and real-world scenarios. The
results demonstrate that MoniTor outperforms state-of-the-art methods and is
competitive with weakly supervised methods without training. Code is available
at https://github.com/YsTvT/MoniTor.

</details>


### [86] [VidSplice: Towards Coherent Video Inpainting via Explicit Spaced Frame Guidance](https://arxiv.org/abs/2510.21461)
*Ming Xie,Junqiu Yu,Qiaole Dong,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: VidSplice is a novel video inpainting framework that decouples the task into multi-frame consistent image inpainting and masked area motion propagation, using spaced-frame priors and a CoSpliced Module to improve spatiotemporal stability.


<details>
  <summary>Details</summary>
Motivation: Existing video inpainting methods struggle with severe content degradation and overlook spatiotemporal stability, leading to insufficient control over video generation.

Method: Proposes VidSplice with spaced-frame priors, CoSpliced Module for first-frame propagation, and context controller module to constrain content distortion during generation.

Result: Extensive evaluations show competitive performance across diverse scenarios with improved foreground alignment and motion stability.

Conclusion: VidSplice outperforms existing approaches by effectively addressing spatiotemporal stability issues in video inpainting.

Abstract: Recent video inpainting methods often employ image-to-video (I2V) priors to
model temporal consistency across masked frames. While effective in moderate
cases, these methods struggle under severe content degradation and tend to
overlook spatiotemporal stability, resulting in insufficient control over the
latter parts of the video. To address these limitations, we decouple video
inpainting into two sub-tasks: multi-frame consistent image inpainting and
masked area motion propagation. We propose VidSplice, a novel framework that
introduces spaced-frame priors to guide the inpainting process with
spatiotemporal cues. To enhance spatial coherence, we design a CoSpliced Module
to perform first-frame propagation strategy that diffuses the initial frame
content into subsequent reference frames through a splicing mechanism.
Additionally, we introduce a delicate context controller module that encodes
coherent priors after frame duplication and injects the spliced video into the
I2V generative backbone, effectively constraining content distortion during
generation. Extensive evaluations demonstrate that VidSplice achieves
competitive performance across diverse video inpainting scenarios. Moreover,
its design significantly improves both foreground alignment and motion
stability, outperforming existing approaches.

</details>


### [87] [CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray Diagnosis](https://arxiv.org/abs/2510.21464)
*Yiming Tang,Wenjia Zhong,Rushi Shah,Dianbo Liu*

Main category: cs.CV

TL;DR: CXR-LanIC is a novel framework that makes chest X-ray diagnosis more interpretable by discovering task-aligned visual patterns from deep learning models, enabling transparent explanations while maintaining competitive diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for chest X-ray diagnosis are accurate but lack interpretability, limiting clinical adoption. Clinicians need transparent, verifiable explanations to trust automated diagnoses and identify potential failures.

Method: Trains transcoder-based sparse autoencoders on BiomedCLIP diagnostic classifier to decompose medical image representations into interpretable visual patterns. Uses ensemble of 100 transcoders on multimodal embeddings from MIMIC-CXR dataset to discover approximately 5,000 monosemantic patterns across various medical categories.

Result: Achieves competitive diagnostic accuracy on five key findings while providing transparent attribution where predictions decompose into 20-50 interpretable patterns with verifiable activation galleries. Discovered patterns span cardiac, pulmonary, pleural, structural, device, and artifact categories.

Conclusion: Medical AI systems can be both accurate and interpretable, supporting safer clinical deployment through transparent, clinically grounded explanations. Key innovation is extracting interpretable features from classifier trained on specific diagnostic objectives rather than general-purpose embeddings.

Abstract: Deep learning models have achieved remarkable accuracy in chest X-ray
diagnosis, yet their widespread clinical adoption remains limited by the
black-box nature of their predictions. Clinicians require transparent,
verifiable explanations to trust automated diagnoses and identify potential
failure modes. We introduce CXR-LanIC (Language-Grounded Interpretable
Classifier for Chest X-rays), a novel framework that addresses this
interpretability challenge through task-aligned pattern discovery. Our approach
trains transcoder-based sparse autoencoders on a BiomedCLIP diagnostic
classifier to decompose medical image representations into interpretable visual
patterns. By training an ensemble of 100 transcoders on multimodal embeddings
from the MIMIC-CXR dataset, we discover approximately 5,000 monosemantic
patterns spanning cardiac, pulmonary, pleural, structural, device, and artifact
categories. Each pattern exhibits consistent activation behavior across images
sharing specific radiological features, enabling transparent attribution where
predictions decompose into 20-50 interpretable patterns with verifiable
activation galleries. CXR-LanIC achieves competitive diagnostic accuracy on
five key findings while providing the foundation for natural language
explanations through planned large multimodal model annotation. Our key
innovation lies in extracting interpretable features from a classifier trained
on specific diagnostic objectives rather than general-purpose embeddings,
ensuring discovered patterns are directly relevant to clinical decision-making,
demonstrating that medical AI systems can be both accurate and interpretable,
supporting safer clinical deployment through transparent, clinically grounded
explanations.

</details>


### [88] [ITC-RWKV: Interactive Tissue-Cell Modeling with Recurrent Key-Value Aggregation for Histopathological Subtyping](https://arxiv.org/abs/2510.21479)
*Yating Huang,Qijun Yang,Lintao Xiang,Hujun Yin*

Main category: cs.CV

TL;DR: Proposes a dual-stream architecture for histopathology image analysis that integrates tissue-level context with cell-level features using recurrent transformers and bidirectional attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing pathology foundation models capture global tissue context but lack cell-level feature modeling, which is crucial for fine-grained tasks like cancer subtype classification.

Method: Dual-stream architecture with receptance-weighted key-value aggregation (recurrent transformer for cell dependencies) and bidirectional tissue-cell interaction module for mutual attention between cellular cues and tissue environment.

Result: Outperforms existing models on four histopathological subtype classification benchmarks, demonstrating superior performance in fine-grained computational pathology tasks.

Conclusion: Cell-level aggregation and tissue-cell interaction are critical components for accurate fine-grained analysis in computational pathology, enabling better integration of spatial and semantic information across scales.

Abstract: Accurate interpretation of histopathological images demands integration of
information across spatial and semantic scales, from nuclear morphology and
cellular textures to global tissue organization and disease-specific patterns.
Although recent foundation models in pathology have shown strong capabilities
in capturing global tissue context, their omission of cell-level feature
modeling remains a key limitation for fine-grained tasks such as cancer subtype
classification. To address this, we propose a dual-stream architecture that
models the interplay between macroscale tissue features and aggregated cellular
representations. To efficiently aggregate information from large cell sets, we
propose a receptance-weighted key-value aggregation model, a recurrent
transformer that captures inter-cell dependencies with linear complexity.
Furthermore, we introduce a bidirectional tissue-cell interaction module to
enable mutual attention between localized cellular cues and their surrounding
tissue environment. Experiments on four histopathological subtype
classification benchmarks show that the proposed method outperforms existing
models, demonstrating the critical role of cell-level aggregation and
tissue-cell interaction in fine-grained computational pathology.

</details>


### [89] [GRAP-MOT: Unsupervised Graph-based Position Weighted Person Multi-camera Multi-object Tracking in a Highly Congested Space](https://arxiv.org/abs/2510.21482)
*Marek Socha,Michał Marczyk,Aleksander Kempski,Michał Cogiel,Paweł Foszner,Radosław Zawiski,Michał Staniszewski*

Main category: cs.CV

TL;DR: GRAP-MOT is a novel graph-weighted approach for multi-object tracking in crowded indoor environments with overlapping camera views, using online person identification updates and position estimation to handle frequent occlusions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of person tracking in highly congested closed areas with overlapping multi-camera views where frequent occlusions occur, requiring robust identification methods.

Method: Uses graph-weighted solution with online person identification updates based on tracks and characteristic features, includes position estimation module, and deeply investigates feature extraction, tracking, and community search elements.

Result: Showed superiority over other methods on closed-area model recordings and publicly available real datasets of highly congested spaces, with better performance than methods without position data.

Conclusion: GRAP-MOT effectively handles person tracking in crowded environments with occlusions, and IDF1 metric is more adequate than MOTA for comparing MOT algorithms in such scenarios. Code and dataset are publicly available.

Abstract: GRAP-MOT is a new approach for solving the person MOT problem dedicated to
videos of closed areas with overlapping multi-camera views, where person
occlusion frequently occurs. Our novel graph-weighted solution updates a
person's identification label online based on tracks and the person's
characteristic features. To find the best solution, we deeply investigated all
elements of the MOT process, including feature extraction, tracking, and
community search. Furthermore, GRAP-MOT is equipped with a person's position
estimation module, which gives additional key information to the MOT method,
ensuring better results than methods without position data. We tested GRAP-MOT
on recordings acquired in a closed-area model and on publicly available real
datasets that fulfil the requirement of a highly congested space, showing the
superiority of our proposition. Finally, we analyzed existing metrics used to
compare MOT algorithms and concluded that IDF1 is more adequate than MOTA in
such comparisons. We made our code, along with the acquired dataset, publicly
available.

</details>


### [90] [An Automatic Detection Method for Hematoma Features in Placental Abruption Ultrasound Images Based on Few-Shot Learning](https://arxiv.org/abs/2510.21495)
*Xiaoqing Liu,Jitai Han,Hua Yan,Peng Li,Sida Tang,Ying Li,Kaiwen Zhang,Min Yu*

Main category: cs.CV

TL;DR: EH-YOLOv11n model improves placental abruption detection in ultrasound images with 78% accuracy using wavelet convolution, coordinate convolution, and cascaded group attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Traditional ultrasound diagnosis of placental abruption relies heavily on physician experience, leading to subjective bias and diagnostic inconsistencies, necessitating automated detection methods.

Method: Enhanced YOLOv11n model with wavelet convolution and coordinate convolution for frequency/spatial features, plus cascaded group attention mechanism to suppress ultrasound artifacts and occlusion interference.

Result: 78% detection accuracy, 2.5% improvement over YOLOv11n and 13.7% improvement over YOLOv8, with superior performance in precision-recall curves, confidence scores, and occlusion scenarios.

Conclusion: The model provides reliable computer-aided diagnosis for placental abruption with high accuracy and real-time processing, offering significant clinical application value.

Abstract: Placental abruption is a severe complication during pregnancy, and its early
accurate diagnosis is crucial for ensuring maternal and fetal safety.
Traditional ultrasound diagnostic methods heavily rely on physician experience,
leading to issues such as subjective bias and diagnostic inconsistencies. This
paper proposes an improved model, EH-YOLOv11n (Enhanced Hemorrhage-YOLOv11n),
based on small-sample learning, aiming to achieve automatic detection of
hematoma features in placental ultrasound images. The model enhances
performance through multidimensional optimization: it integrates wavelet
convolution and coordinate convolution to strengthen frequency and spatial
feature extraction; incorporates a cascaded group attention mechanism to
suppress ultrasound artifacts and occlusion interference, thereby improving
bounding box localization accuracy. Experimental results demonstrate a
detection accuracy of 78%, representing a 2.5% improvement over YOLOv11n and a
13.7% increase over YOLOv8. The model exhibits significant superiority in
precision-recall curves, confidence scores, and occlusion scenarios. Combining
high accuracy with real-time processing, this model provides a reliable
solution for computer-aided diagnosis of placental abruption, holding
significant clinical application value.

</details>


### [91] [GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs](https://arxiv.org/abs/2510.21501)
*Guanghao Zheng,Bowen Shi,Mingxing Xu,Ruoyu Sun,Peisen Zhao,Zhibo Zhang,Wenrui Dai,Junni Zou,Hongkai Xiong,Xiaopeng Zhang,Qi Tian*

Main category: cs.CV

TL;DR: GranViT is a Vision Transformer that integrates fine-grained feature extraction with semantic alignment to LLMs via region-level autoregressive training, achieving state-of-the-art results on fine-grained recognition, multimodal VQA, and OCR understanding.


<details>
  <summary>Details</summary>
Motivation: Existing vision encoders focus on global image representations but overlook fine-grained regional analysis, limited by scarcity of fine-grained annotated data and lack of fine-grained pre-training paradigms.

Method: Constructed Gran-29M dataset with 2M images and 180M region-level annotations; developed pretraining-adaptation framework with self-distillation; uses bounding-box-to-caption regression in pretraining and caption-to-bounding-box regression in adaptation; incorporates self-distillation for explicit localization constraints.

Result: GranViT surpasses existing vision encoders and attains strong transferability to varying LLMs; achieves state-of-the-art results on fine-grained recognition, multimodal VQA, and OCR understanding.

Conclusion: GranViT successfully addresses the limitations of existing vision encoders by integrating fine-grained feature extraction with semantic alignment to LLMs, demonstrating superior performance across multiple vision-language tasks.

Abstract: Vision encoders are indispensable for allowing impressive performance of
Multi-modal Large Language Models (MLLMs) in vision language tasks such as
visual question answering and reasoning. However, existing vision encoders
focus on global image representations but overlook fine-grained regional
analysis. They are limited in fine grained perception due to the scarcity of
fine grained annotated data and the lack of a fine grained pre-training
paradigm. In this paper, we propose GranViT, a novel Vision Transformer that
integrates fine-grained feature extraction with semantic alignment to Large
Language Models (LLMs) via region level autoregressive training. We first
construct Gran-29M, a dataset comprising 2million natural and OCR images paired
with over 180 million high-quality region-level annotations, to enable large
scale fine grained pretraining. Consequently, we develop a
pretraining-adaptation framework along with a self distillation mechanism to
train fine-grained GranViT on Gran-29M. We sufficiently exploit the
fine-grained annotations from Gran-29M to resort to bounding-box-to-caption
regression to enhance localized visual representation of the vision encoder in
the pretraining and caption-to-bounding-box regression to improve vision
feature utilization and localization for LLM in the adaptation. We further
incorporate a self distillation mechanism that imposes explicit localization
constraints on the vision encoder to strengthen its regional reasoning
capability. Extensive experiments show that GranViT surpasses existing vision
encoders and attains strong transferability to varying LLMs. Remarkably, it
achieves state-of-the-art results on fine-grained recognition, multimodal VQA,
and OCR understanding.

</details>


### [92] [Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations](https://arxiv.org/abs/2510.21512)
*Kaibo Wang,Jianda Mao,Tong Wu,Yang Xiang*

Main category: cs.CV

TL;DR: The paper proposes Foresight Guidance (FSG), a novel approach to conditional guidance in diffusion models that reframes guidance as fixed point iterations and addresses inefficiencies in existing methods like CFG.


<details>
  <summary>Details</summary>
Motivation: Current approaches to classifier-free guidance (CFG) in text-to-image diffusion models stem from divergent theoretical interpretations, limiting design space and obscuring key design choices. There's a need for a unified perspective and more efficient guidance mechanisms.

Method: The authors reframe conditional guidance as fixed point iterations seeking a golden path where latents produce consistent outputs under both conditional and unconditional generation. They introduce Foresight Guidance (FSG) which solves longer-interval subproblems in early diffusion stages with increased iterations.

Result: Extensive experiments across diverse datasets and model architectures show FSG's superiority over state-of-the-art methods in both image quality and computational efficiency.

Conclusion: The work offers novel perspectives for conditional guidance and unlocks the potential of adaptive design in diffusion models.

Abstract: Classifier-Free Guidance (CFG) is an essential component of text-to-image
diffusion models, and understanding and advancing its operational mechanisms
remains a central focus of research. Existing approaches stem from divergent
theoretical interpretations, thereby limiting the design space and obscuring
key design choices. To address this, we propose a unified perspective that
reframes conditional guidance as fixed point iterations, seeking to identify a
golden path where latents produce consistent outputs under both conditional and
unconditional generation. We demonstrate that CFG and its variants constitute a
special case of single-step short-interval iteration, which is theoretically
proven to exhibit inefficiency. To this end, we introduce Foresight Guidance
(FSG), which prioritizes solving longer-interval subproblems in early diffusion
stages with increased iterations. Extensive experiments across diverse datasets
and model architectures validate the superiority of FSG over state-of-the-art
methods in both image quality and computational efficiency. Our work offers
novel perspectives for conditional guidance and unlocks the potential of
adaptive design.

</details>


### [93] [Head Pursuit: Probing Attention Specialization in Multimodal Transformers](https://arxiv.org/abs/2510.21518)
*Lorenzo Basile,Valentino Maiorca,Diego Doimo,Francesco Locatello,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: The paper analyzes attention heads in text-generative models to understand their specialization in semantic/visual attributes, develops a method to rank heads by concept relevance, and shows that editing just 1% of heads can reliably control model outputs.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms of language and vision-language models, specifically how individual attention heads specialize in semantic or visual attributes, despite their impressive performance remaining partly understood.

Method: Reinterpret probing of intermediate activations through signal processing lens, enabling principled analysis of multiple samples and ranking attention heads based on relevance to target concepts.

Result: Found consistent patterns of head-level specialization across unimodal and multimodal transformers. Editing only 1% of heads (selected via the method) reliably suppresses or enhances targeted concepts in model output.

Conclusion: Attention layers contain interpretable and controllable structure, providing simple tools for understanding and editing large-scale generative models, validated across language and vision-language tasks.

Abstract: Language and vision-language models have shown impressive performance across
a wide range of tasks, but their internal mechanisms remain only partly
understood. In this work, we study how individual attention heads in
text-generative models specialize in specific semantic or visual attributes.
Building on an established interpretability method, we reinterpret the practice
of probing intermediate activations with the final decoding layer through the
lens of signal processing. This lets us analyze multiple samples in a
principled way and rank attention heads based on their relevance to target
concepts. Our results show consistent patterns of specialization at the head
level across both unimodal and multimodal transformers. Remarkably, we find
that editing as few as 1% of the heads, selected using our method, can reliably
suppress or enhance targeted concepts in the model output. We validate our
approach on language tasks such as question answering and toxicity mitigation,
as well as vision-language tasks including image classification and captioning.
Our findings highlight an interpretable and controllable structure within
attention layers, offering simple tools for understanding and editing
large-scale generative models.

</details>


### [94] [Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video](https://arxiv.org/abs/2510.21581)
*Ciara Rowles,Varun Jampani,Simon Donné,Shimon Vainer,Julian Parker,Zach Evans*

Main category: cs.CV

TL;DR: Foley Control is a lightweight video-guided Foley approach that connects frozen video and audio models using a small cross-attention bridge, enabling synchronized audio generation from video while maintaining prompt controllability.


<details>
  <summary>Details</summary>
Motivation: To create a video-to-audio generation system that preserves the strong capabilities of pretrained single-modality models while learning audio-video synchronization, without expensive end-to-end retraining.

Method: Learns only a compact cross-attention bridge between frozen V-JEPA2 video embeddings and Stable Audio Open DiT T2A model, inserting video cross-attention after text cross-attention, with video token pooling for efficiency.

Result: Achieves competitive temporal and semantic alignment on video-audio benchmarks with far fewer trainable parameters than multi-modal systems, while maintaining prompt-driven controllability and modularity.

Conclusion: The lightweight bridge approach effectively learns audio-video dependencies for synchronization while preserving frozen model capabilities, with potential extension to other audio modalities.

Abstract: Foley Control is a lightweight approach to video-guided Foley that keeps
pretrained single-modality models frozen and learns only a small
cross-attention bridge between them. We connect V-JEPA2 video embeddings to a
frozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact
video cross-attention after the model's existing text cross-attention, so
prompts set global semantics while video refines timing and local dynamics. The
frozen backbones retain strong marginals (video; audio given text) and the
bridge learns the audio-video dependency needed for synchronization -- without
retraining the audio prior. To cut memory and stabilize training, we pool video
tokens before conditioning. On curated video-audio benchmarks, Foley Control
delivers competitive temporal and semantic alignment with far fewer trainable
parameters than recent multi-modal systems, while preserving prompt-driven
controllability and production-friendly modularity (swap/upgrade encoders or
the T2A backbone without end-to-end retraining). Although we focus on
Video-to-Foley, the same bridge design can potentially extend to other audio
modalities (e.g., speech).

</details>


### [95] [Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation](https://arxiv.org/abs/2510.21583)
*Yifu Luo,Penghui Du,Bo Li,Sinan Du,Tiantian Zhang,Yongzhe Chang,Kai Wu,Kun Gai,Xueqian Wang*

Main category: cs.CV

TL;DR: Chunk-GRPO improves Group Relative Policy Optimization for text-to-image generation by shifting from step-level to chunk-level optimization, addressing inaccurate advantage attribution and temporal dynamics neglect.


<details>
  <summary>Details</summary>
Motivation: GRPO faces limitations in accurate advantage attribution and neglects temporal dynamics in flow-matching-based text-to-image generation.

Method: Groups consecutive steps into coherent chunks to capture temporal dynamics, optimizes policies at chunk level, and introduces optional weighted sampling strategy.

Result: Achieves superior results in both preference alignment and image quality compared to previous methods.

Conclusion: Chunk-level optimization shows promise for GRPO-based methods in text-to-image generation.

Abstract: Group Relative Policy Optimization (GRPO) has shown strong potential for
flow-matching-based text-to-image (T2I) generation, but it faces two key
limitations: inaccurate advantage attribution, and the neglect of temporal
dynamics of generation. In this work, we argue that shifting the optimization
paradigm from the step level to the chunk level can effectively alleviate these
issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level
GRPO-based approach for T2I generation. The insight is to group consecutive
steps into coherent 'chunk's that capture the intrinsic temporal dynamics of
flow matching, and to optimize policies at the chunk level. In addition, we
introduce an optional weighted sampling strategy to further enhance
performance. Extensive experiments show that ChunkGRPO achieves superior
results in both preference alignment and image quality, highlighting the
promise of chunk-level optimization for GRPO-based methods.

</details>


### [96] [MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime UAV Operations](https://arxiv.org/abs/2510.21586)
*Xuzhao Li,Xuchen Li,Shiyu Hu*

Main category: cs.CV

TL;DR: MATrack is a multiscale adaptive system for nighttime UAV tracking that addresses challenges like low-light conditions, cluttered backgrounds, and viewpoint changes through three core modules, achieving significant performance improvements over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Nighttime UAV tracking faces challenges from low-light conditions, cluttered backgrounds, and viewpoint changes that cause existing trackers to drift or fail. Current solutions like low-light enhancement introduce artifacts, while domain adaptation methods are computationally expensive.

Method: MATrack uses three core modules: Multiscale Hierarchy Blende (MHB) for feature consistency between static/dynamic templates, Adaptive Key Token Gate for object identification in complex backgrounds, and Nighttime Template Calibrator (NTC) for stable long-sequence tracking.

Result: On UAVDark135 benchmark, MATrack achieves 5.9% precision, 5.4% normalized precision, and 4.2% AUC improvements over SOTA methods, with real-time processing at 81 FPS. Real-world UAV platform tests validate reliability for critical applications.

Conclusion: MATrack provides stable and effective nighttime UAV tracking support for critical robotics applications like search and rescue and border patrol, demonstrating significant performance improvements while maintaining real-time processing capabilities.

Abstract: Nighttime UAV tracking faces significant challenges in real-world robotics
operations. Low-light conditions not only limit visual perception capabilities,
but cluttered backgrounds and frequent viewpoint changes also cause existing
trackers to drift or fail during deployment. To address these difficulties,
researchers have proposed solutions based on low-light enhancement and domain
adaptation. However, these methods still have notable shortcomings in actual
UAV systems: low-light enhancement often introduces visual artifacts, domain
adaptation methods are computationally expensive and existing lightweight
designs struggle to fully leverage dynamic object information. Based on an
in-depth analysis of these key issues, we propose MATrack-a multiscale adaptive
system designed specifically for nighttime UAV tracking. MATrack tackles the
main technical challenges of nighttime tracking through the collaborative work
of three core modules: Multiscale Hierarchy Blende (MHB) enhances feature
consistency between static and dynamic templates. Adaptive Key Token Gate
accurately identifies object information within complex backgrounds. Nighttime
Template Calibrator (NTC) ensures stable tracking performance over long
sequences. Extensive experiments show that MATrack achieves a significant
performance improvement. On the UAVDark135 benchmark, its precision, normalized
precision and AUC surpass state-of-the-art (SOTA) methods by 5.9%, 5.4% and
4.2% respectively, while maintaining a real-time processing speed of 81 FPS.
Further tests on a real-world UAV platform validate the system's reliability,
demonstrating that MATrack can provide stable and effective nighttime UAV
tracking support for critical robotics applications such as nighttime search
and rescue and border patrol.

</details>


### [97] [Restore Text First, Enhance Image Later: Two-Stage Scene Text Image Super-Resolution with Glyph Structure Guidance](https://arxiv.org/abs/2510.21590)
*Minxing Luo,Linlong Fan,Wang Qiushi,Ge Wu,Yiyan Luo,Yuhang Yu,Jinwei Chen,Yaxing Wang,Qingnan Fan,Jian Yang*

Main category: cs.CV

TL;DR: TIGER is a two-stage super-resolution framework that prioritizes text reconstruction before image enhancement to overcome the trade-off between image quality and text readability in generative super-resolution methods.


<details>
  <summary>Details</summary>
Motivation: Current generative super-resolution methods perform well on natural images but distort text, creating a fundamental trade-off between image quality and textual readability that needs to be addressed.

Method: A novel two-stage 'text-first, image-later' framework that explicitly decouples glyph restoration from image enhancement. It first reconstructs precise text structures and then uses them to guide subsequent full-image super-resolution.

Result: TIGER achieves state-of-the-art performance, enhancing readability while preserving overall image quality. The method also introduces UltraZoom-ST, the first scene text dataset with extreme zoom (×14.29) for comprehensive training and evaluation.

Conclusion: The proposed TIGER framework successfully breaks the trade-off between image quality and text readability in super-resolution through its text-first approach and glyph-to-image guidance mechanism.

Abstract: Current generative super-resolution methods show strong performance on
natural images but distort text, creating a fundamental trade-off between image
quality and textual readability. To address this, we introduce \textbf{TIGER}
(\textbf{T}ext-\textbf{I}mage \textbf{G}uided
sup\textbf{E}r-\textbf{R}esolution), a novel two-stage framework that breaks
this trade-off through a \textit{"text-first, image-later"} paradigm.
\textbf{TIGER} explicitly decouples glyph restoration from image enhancement:
it first reconstructs precise text structures and then uses them to guide
subsequent full-image super-resolution. This glyph-to-image guidance ensures
both high fidelity and visual consistency. To support comprehensive training
and evaluation, we also contribute the \textbf{UltraZoom-ST} (UltraZoom-Scene
Text), the first scene text dataset with extreme zoom (\textbf{$\times$14.29}).
Extensive experiments show that \textbf{TIGER} achieves
\textbf{state-of-the-art} performance, enhancing readability while preserving
overall image quality.

</details>


### [98] [Automated interictal epileptic spike detection from simple and noisy annotations in MEG data](https://arxiv.org/abs/2510.21596)
*Pauline Mouches,Julien Jung,Armand Demasson,Agnès Guinard,Romain Bouet,Rosalie Marchal,Romain Quentin*

Main category: cs.CV

TL;DR: Deep learning models (ANN and CNN) can detect interictal spikes in MEG recordings for epilepsy evaluation, outperforming state-of-the-art methods even with limited single-expert annotations.


<details>
  <summary>Details</summary>
Motivation: Manual detection of interictal epileptic spikes in MEG recordings is tedious, error-prone, and has moderate interrater agreement. Current automated methods are unsuitable for clinical practice due to extensive annotation requirements or lack of robustness.

Method: Proposed two deep learning architectures: feature-based ANN and CNN, trained on 59 patients' data. Used interactive machine learning to iteratively improve annotation quality using model outputs. Evaluated against state-of-the-art model on short time window classification.

Result: Both models outperformed state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) on 10 holdout test patients. Interactive learning demonstrated model robustness to noisy annotations.

Conclusion: Simple architecture models are robust for complex, imperfectly annotated data. Interactive machine learning enables faster data annotation, and the models provide efficient tools for automated interictal spike detection in clinical practice.

Abstract: In drug-resistant epilepsy, presurgical evaluation of epilepsy can be
considered. Magnetoencephalography (MEG) has been shown to be an effective exam
to inform the localization of the epileptogenic zone through the localization
of interictal epileptic spikes. Manual detection of these pathological
biomarkers remains a fastidious and error-prone task due to the high
dimensionality of MEG recordings, and interrater agreement has been reported to
be only moderate. Current automated methods are unsuitable for clinical
practice, either requiring extensively annotated data or lacking robustness on
non-typical data. In this work, we demonstrate that deep learning models can be
used for detecting interictal spikes in MEG recordings, even when only temporal
and single-expert annotations are available, which represents real-world
clinical practice. We propose two model architectures: a feature-based
artificial neural network (ANN) and a convolutional neural network (CNN),
trained on a database of 59 patients, and evaluated against a state-of-the-art
model to classify short time windows of signal. In addition, we employ an
interactive machine learning strategy to iteratively improve our data
annotation quality using intermediary model outputs. Both proposed models
outperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when
tested on 10 holdout test patients. The interactive machine learning strategy
demonstrates that our models are robust to noisy annotations. Overall, results
highlight the robustness of models with simple architectures when analyzing
complex and imperfectly annotated data. Our method of interactive machine
learning offers great potential for faster data annotation, while our models
represent useful and efficient tools for automated interictal spikes detection.

</details>


### [99] [S3OD: Towards Generalizable Salient Object Detection with Synthetic Data](https://arxiv.org/abs/2510.21605)
*Orest Kupyn,Hirokatsu Kataoka,Christian Rupprecht*

Main category: cs.CV

TL;DR: S3OD method improves salient object detection generalization using large-scale synthetic data generation and ambiguity-aware architecture, achieving 20-50% error reduction in cross-dataset performance.


<details>
  <summary>Details</summary>
Motivation: Salient object detection is data-bounded due to expensive pixel-precise annotations, forcing separate model training for related subtasks like DIS and HR-SOD.

Method: Multi-modal diffusion pipeline generates S3OD dataset with 139,000+ high-resolution images using diffusion and DINO-v3 features, with iterative generation prioritizing challenging categories. Streamlined multi-mask decoder handles ambiguity by predicting multiple valid interpretations.

Result: Models trained solely on synthetic data achieve 20-50% error reduction in cross-dataset generalization. Fine-tuned versions reach state-of-the-art performance across DIS and HR-SOD benchmarks.

Conclusion: Large-scale synthetic data generation combined with ambiguity-aware architecture dramatically improves generalization in salient object detection tasks.

Abstract: Salient object detection exemplifies data-bounded tasks where expensive
pixel-precise annotations force separate model training for related subtasks
like DIS and HR-SOD. We present a method that dramatically improves
generalization through large-scale synthetic data generation and
ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000
high-resolution images created through our multi-modal diffusion pipeline that
extracts labels from diffusion and DINO-v3 features. The iterative generation
framework prioritizes challenging categories based on model performance. We
propose a streamlined multi-mask decoder that naturally handles the inherent
ambiguity in salient object detection by predicting multiple valid
interpretations. Models trained solely on synthetic data achieve 20-50% error
reduction in cross-dataset generalization, while fine-tuned versions reach
state-of-the-art performance across DIS and HR-SOD benchmarks.

</details>


### [100] [Modest-Align: Data-Efficient Alignment for Vision-Language Models](https://arxiv.org/abs/2510.21606)
*Jiaxiang Liu,Yuan Wang,Jiawei Du,Joey Tianyi Zhou,Mingkun Xu,Zuozhu Liu*

Main category: cs.CV

TL;DR: Modest-Align is a lightweight cross-modal alignment framework that addresses overconfidence and performance degradation in resource-constrained settings through random perturbation and embedding smoothing techniques.


<details>
  <summary>Details</summary>
Motivation: Current cross-modal alignment models like CLIP suffer from overconfidence and degraded performance when operating with limited or low-quality data, particularly due to ambiguous image-text pairs and contrastive learning approaches that reinforce overconfidence.

Method: Proposes two complementary strategies: Random Perturbation (introducing controlled noise to simulate uncertainty) and Embedding Smoothing (calibrating similarity distributions in the embedding space).

Result: Outperforms state-of-the-art methods in retrieval tasks, achieving competitive results with over 100x less training data and 600x less GPU time than CLIP.

Conclusion: Modest-Align offers a practical and scalable solution for cross-modal alignment in real-world, low-resource scenarios by reducing overconfidence and improving performance on noisy or weakly aligned samples.

Abstract: Cross-modal alignment aims to map heterogeneous modalities into a shared
latent space, as exemplified by models like CLIP, which benefit from
large-scale image-text pretraining for strong recognition capabilities.
However, when operating in resource-constrained settings with limited or
low-quality data, these models often suffer from overconfidence and degraded
performance due to the prevalence of ambiguous or weakly correlated image-text
pairs. Current contrastive learning approaches, which rely on single positive
pairs, further exacerbate this issue by reinforcing overconfidence on uncertain
samples. To address these challenges, we propose Modest-Align, a lightweight
alignment framework designed for robustness and efficiency. Our approach
leverages two complementary strategies -- Random Perturbation, which introduces
controlled noise to simulate uncertainty, and Embedding Smoothing, which
calibrates similarity distributions in the embedding space. These mechanisms
collectively reduce overconfidence and improve performance on noisy or weakly
aligned samples. Extensive experiments across multiple benchmark datasets
demonstrate that Modest-Align outperforms state-of-the-art methods in retrieval
tasks, achieving competitive results with over 100x less training data and 600x
less GPU time than CLIP. Our method offers a practical and scalable solution
for cross-modal alignment in real-world, low-resource scenarios.

</details>


### [101] [Epipolar Geometry Improves Video Generation Models](https://arxiv.org/abs/2510.21615)
*Orest Kupyn,Fabian Manhardt,Federico Tombari,Christian Rupprecht*

Main category: cs.CV

TL;DR: The paper proposes using epipolar geometry constraints to improve video generation models, addressing geometric inconsistencies and unstable motion through preference-based optimization.


<details>
  <summary>Details</summary>
Motivation: Video generation models struggle with geometric inconsistencies, unstable motion, and visual artifacts despite large-scale training. 3D-consistent video generation could significantly impact downstream applications in generation and reconstruction tasks.

Method: Align diffusion models using pairwise epipolar geometry constraints via preference-based optimization. This approach enforces geometric principles without requiring end-to-end differentiability, using classical geometric constraints as more stable optimization signals than learned metrics.

Result: The method efficiently addresses unstable camera trajectories and geometric artifacts through mathematically principled geometric enforcement. Training on static scenes with dynamic cameras ensures high-quality measurements while the model generalizes to diverse dynamic content.

Conclusion: By bridging data-driven deep learning with classical geometric computer vision, the paper presents a practical method for generating spatially consistent videos without compromising visual quality.

Abstract: Video generation models have progressed tremendously through large latent
diffusion transformers trained with rectified flow techniques. Yet these models
still struggle with geometric inconsistencies, unstable motion, and visual
artifacts that break the illusion of realistic 3D scenes. 3D-consistent video
generation could significantly impact numerous downstream applications in
generation and reconstruction tasks. We explore how epipolar geometry
constraints improve modern video diffusion models. Despite massive training
data, these models fail to capture fundamental geometric principles underlying
visual content. We align diffusion models using pairwise epipolar geometry
constraints via preference-based optimization, directly addressing unstable
camera trajectories and geometric artifacts through mathematically principled
geometric enforcement. Our approach efficiently enforces geometric principles
without requiring end-to-end differentiability. Evaluation demonstrates that
classical geometric constraints provide more stable optimization signals than
modern learned metrics, which produce noisy targets that compromise alignment
quality. Training on static scenes with dynamic cameras ensures high-quality
measurements while the model generalizes effectively to diverse dynamic
content. By bridging data-driven deep learning with classical geometric
computer vision, we present a practical method for generating spatially
consistent videos without compromising visual quality.

</details>


### [102] [DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning](https://arxiv.org/abs/2510.21635)
*Ziqi Gao,Qiufu Li,Linlin Shen*

Main category: cs.CV

TL;DR: DAP-MAE is a domain-adaptive masked autoencoder method that adaptively integrates cross-domain point cloud data for pre-training, addressing data scarcity and domain misalignment issues in 3D point cloud analysis.


<details>
  <summary>Details</summary>
Motivation: Point cloud data is limited across different domains, and combining them for MAE pre-training can lead to misalignment with downstream tasks, degrading performance.

Method: Proposes DAP-MAE with a heterogeneous domain adapter that uses adaptation mode during pre-training and fusion mode during fine-tuning, plus a domain feature generator to guide feature adaptation.

Result: Achieves 95.18% in object classification on ScanObjectNN and 88.45% in facial expression recognition on Bosphorus with only one pre-training.

Conclusion: DAP-MAE effectively integrates cross-domain knowledge and achieves excellent performance across multiple point cloud analysis tasks through adaptive pre-training.

Abstract: Compared to 2D data, the scale of point cloud data in different domains
available for training, is quite limited. Researchers have been trying to
combine these data of different domains for masked autoencoder (MAE)
pre-training to leverage such a data scarcity issue. However, the prior
knowledge learned from mixed domains may not align well with the downstream 3D
point cloud analysis tasks, leading to degraded performance. To address such an
issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE),
an MAE pre-training method, to adaptively integrate the knowledge of
cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a
heterogeneous domain adapter that utilizes an adaptation mode during
pre-training, enabling the model to comprehensively learn information from
point clouds across different domains, while employing a fusion mode in the
fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a
domain feature generator to guide the adaptation of point cloud features to
various downstream tasks. With only one pre-training, DAP-MAE achieves
excellent performance across four different point cloud analysis tasks,
reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial
expression recognition on Bosphorus.

</details>


### [103] [A Dynamic Knowledge Distillation Method Based on the Gompertz Curve](https://arxiv.org/abs/2510.21649)
*Han Yang,Guangjun Qin*

Main category: cs.CV

TL;DR: Gompertz-CNN is a dynamic knowledge distillation framework that uses the Gompertz growth model to adjust distillation loss weights according to student learning stages, achieving significant accuracy improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge distillation methods fail to capture the evolving cognitive capacity of student models during training, leading to suboptimal knowledge transfer.

Method: Proposes a stage-aware distillation strategy using Gompertz curve to dynamically adjust distillation loss weights, incorporating Wasserstein distance for feature-level discrepancies and gradient matching for backward propagation alignment.

Result: Extensive experiments on CIFAR-10 and CIFAR-100 show Gompertz-CNN outperforms traditional distillation methods, achieving up to 8% accuracy gain on CIFAR-10 and 4% on CIFAR-100 across various teacher-student architectures.

Conclusion: The Gompertz-CNN framework effectively addresses limitations of traditional knowledge distillation by modeling student learning progression, resulting in superior performance across multiple benchmark datasets.

Abstract: This paper introduces a novel dynamic knowledge distillation framework,
Gompertz-CNN, which integrates the Gompertz growth model into the training
process to address the limitations of traditional knowledge distillation.
Conventional methods often fail to capture the evolving cognitive capacity of
student models, leading to suboptimal knowledge transfer. To overcome this, we
propose a stage-aware distillation strategy that dynamically adjusts the weight
of distillation loss based on the Gompertz curve, reflecting the student's
learning progression: slow initial growth, rapid mid-phase improvement, and
late-stage saturation. Our framework incorporates Wasserstein distance to
measure feature-level discrepancies and gradient matching to align backward
propagation behaviors between teacher and student models. These components are
unified under a multi-loss objective, where the Gompertz curve modulates the
influence of distillation losses over time. Extensive experiments on CIFAR-10
and CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and
MobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms
traditional distillation methods, achieving up to 8% and 4% accuracy gains on
CIFAR-10 and CIFAR-100, respectively.

</details>


### [104] [Group Inertial Poser: Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging](https://arxiv.org/abs/2510.21654)
*Ying Xue,Jiaxi Jiang,Rayan Armani,Dominik Hollidt,Yi-Chi Liao,Christian Holz*

Main category: cs.CV

TL;DR: Group Inertial Poser uses sparse wearable IMUs and UWB ranging to track multiple people's full-body motion and global trajectories, overcoming limitations of vision-based and purely IMU-based approaches.


<details>
  <summary>Details</summary>
Motivation: Purely IMU-based tracking lacks translation estimates and accurate relative positioning between individuals since inertial cues are self-referential with no spatial reference for others.

Method: Estimates absolute distances between sensor pairs from UWB ranging and fuses them with inertial observations using structured state-space models, with a novel two-step optimization for global trajectory tracking.

Result: Outperforms previous state-of-the-art methods in accuracy and robustness across synthetic and real-world data, and introduces GIP-DB - the first IMU+UWB dataset for two-person tracking.

Conclusion: Shows promise for IMU+UWB-based multi-human motion capture in the wild, enabling robust body pose and global translation estimation for multiple individuals.

Abstract: Tracking human full-body motion using sparse wearable inertial measurement
units (IMUs) overcomes the limitations of occlusion and instrumentation of the
environment inherent in vision-based approaches. However, purely IMU-based
tracking compromises translation estimates and accurate relative positioning
between individuals, as inertial cues are inherently self-referential and
provide no direct spatial reference for others. In this paper, we present a
novel approach for robustly estimating body poses and global translation for
multiple individuals by leveraging the distances between sparse wearable
sensors - both on each individual and across multiple individuals. Our method
Group Inertial Poser estimates these absolute distances between pairs of
sensors from ultra-wideband ranging (UWB) and fuses them with inertial
observations as input into structured state-space models to integrate temporal
motion patterns for precise 3D pose estimation. Our novel two-step optimization
further leverages the estimated distances for accurately tracking people's
global trajectories through the world. We also introduce GIP-DB, the first
IMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion
recordings from 14 participants. In our evaluation, Group Inertial Poser
outperforms previous state-of-the-art methods in accuracy and robustness across
synthetic and real-world data, showing the promise of IMU+UWB-based multi-human
motion capture in the wild. Code, models, dataset:
https://github.com/eth-siplab/GroupInertialPoser

</details>


### [105] [Long-tailed Species Recognition in the NACTI Wildlife Dataset](https://arxiv.org/abs/2510.21657)
*Zehua Liu,Tilo Burghardt*

Main category: cs.CV

TL;DR: This paper presents a systematic study of Long-Tail Recognition (LTR) methods for species recognition on the North America Camera Trap Images (NACTI) dataset, achieving 99.40% Top-1 accuracy and demonstrating improved generalization under domain shifts.


<details>
  <summary>Details</summary>
Motivation: The NACTI dataset exhibits severe long-tailed class imbalance where the largest 'Head' class covers >50% of 3.7M images, requiring specialized LTR methods to address this challenge in wildlife species recognition.

Method: Built on PyTorch Wildlife model, the study systematically evaluates various LTR loss functions and LTR-sensitive regularization techniques, with experiments on dataset splits and domain shift evaluation using a Reduced-Bias Test set from ENA-Detection dataset.

Result: Best configuration achieved 99.40% Top-1 accuracy on NACTI test data (vs 95.51% baseline), and 52.55% accuracy on domain shift test set (vs 51.20% with WCE loss), demonstrating improved generalization capabilities.

Conclusion: LTR-enhancing scheduler choices consistently improve performance in wildlife domain, particularly when combined with state-of-the-art LTR losses, though limitations remain including catastrophic breakdown for 'Tail' classes under severe domain shift.

Abstract: As most ''in the wild'' data collections of the natural world, the North
America Camera Trap Images (NACTI) dataset shows severe long-tailed class
imbalance, noting that the largest 'Head' class alone covers >50% of the 3.7M
images in the corpus. Building on the PyTorch Wildlife model, we present a
systematic study of Long-Tail Recognition methodologies for species recognition
on the NACTI dataset covering experiments on various LTR loss functions plus
LTR-sensitive regularisation. Our best configuration achieves 99.40% Top-1
accuracy on our NACTI test data split, substantially improving over a 95.51%
baseline using standard cross-entropy with Adam. This also improves on
previously reported top performance in MLWIC2 at 96.8% albeit using partly
unpublished (potentially different) partitioning, optimiser, and evaluation
protocols. To evaluate domain shifts (e.g. night-time captures, occlusion,
motion-blur) towards other datasets we construct a Reduced-Bias Test set from
the ENA-Detection dataset where our experimentally optimised long-tail enhanced
model achieves leading 52.55% accuracy (up from 51.20% with WCE loss),
demonstrating stronger generalisation capabilities under distribution shift. We
document the consistent improvements of LTR-enhancing scheduler choices in this
NACTI wildlife domain, particularly when in tandem with state-of-the-art LTR
losses. We finally discuss qualitative and quantitative shortcomings that LTR
methods cannot sufficiently address, including catastrophic breakdown for
'Tail' classes under severe domain shift. For maximum reproducibility we
publish all dataset splits, key code, and full network weights.

</details>


### [106] [Self-Supervised Learning of Synapse Types from EM Images](https://arxiv.org/abs/2510.21663)
*Aarav Shetty,Gary B Huang*

Main category: cs.CV

TL;DR: The paper presents an unsupervised method for classifying synapses in EM images based on spatial proximity similarity, without requiring predefined classes or labeled training data.


<details>
  <summary>Details</summary>
Motivation: Traditional synapse classification requires supervised learning with labeled examples, which limits discovery of new synapse types and requires prior knowledge of the number of classes.

Method: Proposes an unsupervised approach that leverages the observation that nearby synapses in the same neuron are more similar than randomly selected synapses from different cells, applied to Drosophila data.

Result: The method successfully separates synapses into classes without requiring the number of types to be known in advance.

Conclusion: This unsupervised approach provides a principled way to discover synapse types and select ground-truth data that spans the full range of synapse structural variation.

Abstract: Separating synapses into different classes based on their appearance in EM
images has many applications in biology. Examples may include assigning a
neurotransmitter to a particular class, or separating synapses whose strength
can be modulated from those whose strength is fixed. Traditionally, this has
been done in a supervised manner, giving the classification algorithm examples
of the different classes. Here we instead separate synapses into classes based
only on the observation that nearby synapses in the same neuron are likely more
similar than synapses chosen randomly from different cells. We apply our
methodology to data from {\it Drosophila}. Our approach has the advantage that
the number of synapse types does not need to be known in advance. It may also
provide a principled way to select ground-truth that spans the range of synapse
structure.

</details>


### [107] [Foundation Models in Dermatopathology: Skin Tissue Classification](https://arxiv.org/abs/2510.21664)
*Riya Gupta,Yiwei Zong,Dennis H. Murphree*

Main category: cs.CV

TL;DR: Evaluation of UNI and Virchow2 foundation models for classifying dermatopathology whole-slide images into melanocytic, basaloid, and squamous lesions using patch-level embeddings aggregated via mean-aggregation.


<details>
  <summary>Details</summary>
Motivation: The rapid generation of whole-slide images in dermatopathology necessitates automated methods for efficient processing and accurate classification.

Method: Used UNI and Virchow2 as feature extractors, aggregated patch-level embeddings into slide-level features using mean-aggregation, and trained multiple classifiers (logistic regression, gradient-boosted trees, random forest). Explored data augmentation and image normalization.

Result: Virchow2 outperformed UNI across most classifiers, with logistic regression achieving 90% accuracy for Virchow2 (difference not statistically significant). Mean-aggregation provided reliable slide-level feature representations.

Conclusion: Foundation models show potential for automated WSI classification, providing a scalable approach for dermatopathological diagnosis and paving the way for slide-level representation learning advancements.

Abstract: The rapid generation of whole-slide images (WSIs) in dermatopathology
necessitates automated methods for efficient processing and accurate
classification. This study evaluates the performance of two foundation models,
UNI and Virchow2, as feature extractors for classifying WSIs into three
diagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level
embeddings were aggregated into slide-level features using a mean-aggregation
strategy and subsequently used to train multiple machine learning classifiers,
including logistic regression, gradient-boosted trees, and random forest
models. Performance was assessed using precision, recall, true positive rate,
false positive rate, and the area under the receiver operating characteristic
curve (AUROC) on the test set. Results demonstrate that patch-level features
extracted using Virchow2 outperformed those extracted via UNI across most
slide-level classifiers, with logistic regression achieving the highest
accuracy (90%) for Virchow2, though the difference was not statistically
significant. The study also explored data augmentation techniques and image
normalization to enhance model robustness and generalizability. The
mean-aggregation approach provided reliable slide-level feature
representations. All experimental results and metrics were tracked and
visualized using WandB.ai, facilitating reproducibility and interpretability.
This research highlights the potential of foundation models for automated WSI
classification, providing a scalable and effective approach for
dermatopathological diagnosis while paving the way for future advancements in
slide-level representation learning.

</details>


### [108] [WorldGrow: Generating Infinite 3D World](https://arxiv.org/abs/2510.21682)
*Sikuang Li,Chen Yang,Jiemin Fang,Taoran Yi,Jia Lu,Jiazhong Cen,Lingxi Xie,Wei Shen,Qi Tian*

Main category: cs.CV

TL;DR: WorldGrow is a hierarchical framework for generating infinitely extendable 3D worlds with coherent geometry and realistic appearance, addressing limitations of existing 2D-lifting, 3D implicit representations, and object-centric models.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges: 2D-lifting approaches have geometric and appearance inconsistencies, 3D implicit representations are hard to scale, and current 3D foundation models are mostly object-centric, limiting scene-level generation.

Method: Leverages pre-trained 3D models for structured scene block generation with three core components: (1) data curation pipeline for high-quality scene blocks, (2) 3D block inpainting for context-aware scene extension, and (3) coarse-to-fine generation strategy for global layout plausibility and local fidelity.

Result: Achieves state-of-the-art performance in geometry reconstruction on 3D-FRONT dataset, supporting infinite scene generation with photorealistic and structurally consistent outputs.

Conclusion: WorldGrow demonstrates capability for constructing large-scale virtual environments and has potential for building future world models.

Abstract: We tackle the challenge of generating the infinitely extendable 3D world --
large, continuous environments with coherent geometry and realistic appearance.
Existing methods face key challenges: 2D-lifting approaches suffer from
geometric and appearance inconsistencies across views, 3D implicit
representations are hard to scale up, and current 3D foundation models are
mostly object-centric, limiting their applicability to scene-level generation.
Our key insight is leveraging strong generation priors from pre-trained 3D
models for structured scene block generation. To this end, we propose
WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our
method features three core components: (1) a data curation pipeline that
extracts high-quality scene blocks for training, making the 3D structured
latent representations suitable for scene generation; (2) a 3D block inpainting
mechanism that enables context-aware scene extension; and (3) a coarse-to-fine
generation strategy that ensures both global layout plausibility and local
geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,
WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely
supporting infinite scene generation with photorealistic and structurally
consistent outputs. These results highlight its capability for constructing
large-scale virtual environments and potential for building future world
models.

</details>


### [109] [On Thin Ice: Towards Explainable Conservation Monitoring via Attribution and Perturbations](https://arxiv.org/abs/2510.21689)
*Jiayi Zhou,Günel Aghakishiyeva,Saagar Arya,Julian Dale,James David Poling,Holly R. Houliston,Jamie N. Womble,Gregory D. Larsen,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: The paper applies post-hoc explainability methods to neural network-based seal detection in aerial imagery to build trust and identify model limitations for conservation monitoring.


<details>
  <summary>Details</summary>
Motivation: Address the lack of trust in black-box neural network models for ecological research and conservation monitoring by providing evidence for predictions and documenting deployment limitations.

Method: Train Faster R-CNN on aerial imagery to detect harbor seals, then apply multiple explainability methods: gradient-based class activation mapping (HiResCAM, LayerCAM), LIME, and perturbation-based explanations.

Result: Explanations focus on seal torsos/contours rather than background, removal of seals reduces detection confidence, and analysis reveals systematic errors like confusing seals with black ice and rocks.

Conclusion: Pairing object detection with post-hoc explainability enables moving beyond black-box predictions to create auditable, decision-supporting tools for conservation monitoring, with actionable next steps for model improvement.

Abstract: Computer vision can accelerate ecological research and conservation
monitoring, yet adoption in ecology lags in part because of a lack of trust in
black-box neural-network-based models. We seek to address this challenge by
applying post-hoc explanations to provide evidence for predictions and document
limitations that are important to field deployment. Using aerial imagery from
Glacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor
seals) and generate explanations via gradient-based class activation mapping
(HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME),
and perturbation-based explanations. We assess explanations along three axes
relevant to field use: (i) localization fidelity: whether high-attribution
regions coincide with the animal rather than background context; (ii)
faithfulness: whether deletion/insertion tests produce changes in detector
confidence; and (iii) diagnostic utility: whether explanations reveal
systematic failure modes. Explanations concentrate on seal torsos and contours
rather than surrounding ice/rock, and removal of the seals reduces detection
confidence, providing model-evidence for true positives. The analysis also
uncovers recurrent error sources, including confusion between seals and black
ice and rocks. We translate these findings into actionable next steps for model
development, including more targeted data curation and augmentation. By pairing
object detection with post-hoc explainability, we can move beyond "black-box"
predictions toward auditable, decision-supporting tools for conservation
monitoring.

</details>


### [110] [BachVid: Training-Free Video Generation with Consistent Background and Character](https://arxiv.org/abs/2510.21696)
*Han Yan,Xibin Song,Yifu Wang,Hongdong Li,Pan Ji,Chao Ma*

Main category: cs.CV

TL;DR: BachVid is a training-free method for generating multiple videos with consistent characters and backgrounds by analyzing and leveraging DiT's attention mechanism to cache and inject intermediate variables.


<details>
  <summary>Details</summary>
Motivation: Existing methods for consistent video generation require reference images or extensive training, and often only address character consistency while leaving background consistency to image-to-video models.

Method: Analyze DiT's attention mechanism to extract foreground masks and identify matching points during denoising, then cache intermediate variables from an identity video and inject them into new videos.

Result: BachVid achieves robust consistency in generated videos without requiring additional training or reference images.

Conclusion: BachVid offers a novel and efficient training-free solution for consistent video generation by leveraging DiT's inherent capabilities.

Abstract: Diffusion Transformers (DiTs) have recently driven significant progress in
text-to-video (T2V) generation. However, generating multiple videos with
consistent characters and backgrounds remains a significant challenge. Existing
methods typically rely on reference images or extensive training, and often
only address character consistency, leaving background consistency to
image-to-video models. We introduce BachVid, the first training-free method
that achieves consistent video generation without needing any reference images.
Our approach is based on a systematic analysis of DiT's attention mechanism and
intermediate features, revealing its ability to extract foreground masks and
identify matching points during the denoising process. Our method leverages
this finding by first generating an identity video and caching the intermediate
variables, and then inject these cached variables into corresponding positions
in newly generated videos, ensuring both foreground and background consistency
across multiple videos. Experimental results demonstrate that BachVid achieves
robust consistency in generated videos without requiring additional training,
offering a novel and efficient solution for consistent video generation without
relying on reference images or additional training.

</details>


### [111] [Visual Diffusion Models are Geometric Solvers](https://arxiv.org/abs/2510.21697)
*Nir Goren,Shai Yehezkel,Omer Dahary,Andrey Voynov,Or Patashnik,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: Visual diffusion models can solve hard geometric problems by treating them as image generation tasks, demonstrating this on three classic problems.


<details>
  <summary>Details</summary>
Motivation: To show that standard visual diffusion models can directly reason about geometric problems in pixel space without specialized architectures.

Method: Treat geometric problems as images and train standard visual diffusion models to transform Gaussian noise into images representing valid approximate solutions.

Result: Successfully applied to three hard geometric problems: Inscribed Square Problem, Steiner Tree Problem, and Simple Polygon Problem.

Conclusion: Image space provides a general framework for approximating hard geometric problems, bridging generative modeling and geometric reasoning.

Abstract: In this paper we show that visual diffusion models can serve as effective
geometric solvers: they can directly reason about geometric problems by working
in pixel space. We first demonstrate this on the Inscribed Square Problem, a
long-standing problem in geometry that asks whether every Jordan curve contains
four points forming a square. We then extend the approach to two other
well-known hard geometric problems: the Steiner Tree Problem and the Simple
Polygon Problem.
  Our method treats each problem instance as an image and trains a standard
visual diffusion model that transforms Gaussian noise into an image
representing a valid approximate solution that closely matches the exact one.
The model learns to transform noisy geometric structures into correct
configurations, effectively recasting geometric reasoning as image generation.
  Unlike prior work that necessitates specialized architectures and
domain-specific adaptations when applying diffusion to parametric geometric
representations, we employ a standard visual diffusion model that operates on
the visual representation of the problem. This simplicity highlights a
surprising bridge between generative modeling and geometric problem solving.
Beyond the specific problems studied here, our results point toward a broader
paradigm: operating in image space provides a general and practical framework
for approximating notoriously hard problems, and opens the door to tackling a
far wider class of challenging geometric tasks.

</details>


### [112] [Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent](https://arxiv.org/abs/2510.21704)
*Christy Li,Josep Lopez Camuñas,Jake Thomas Touchet,Jacob Andreas,Agata Lapedriza,Antonio Torralba,Tamar Rott Shaham*

Main category: cs.CV

TL;DR: An automated framework for detecting visual attribute dependencies in vision models using self-reflective agents that iteratively generate and test hypotheses about model behavior.


<details>
  <summary>Details</summary>
Motivation: To detect unintended reliance on specific visual features in vision models, which is critical for ensuring model robustness, preventing overfitting, and avoiding spurious correlations.

Method: A self-reflective agent that systematically generates and tests hypotheses about visual attributes, using iterative refinement based on experimental outcomes and self-evaluation protocols to assess findings.

Result: The agent's performance consistently improves with self-reflection, showing significant performance increase over non-reflective baselines on a benchmark of 130 models across 18 categories. Successfully identifies real-world dependencies in state-of-the-art models like CLIP and YOLOv8.

Conclusion: The self-reflective framework effectively detects visual attribute dependencies in vision models, demonstrating improved performance through iterative hypothesis testing and reflection cycles.

Abstract: When a vision model performs image recognition, which visual attributes drive
its predictions? Detecting unintended reliance on specific visual features is
critical for ensuring model robustness, preventing overfitting, and avoiding
spurious correlations. We introduce an automated framework for detecting such
dependencies in trained vision models. At the core of our method is a
self-reflective agent that systematically generates and tests hypotheses about
visual attributes that a model may rely on. This process is iterative: the
agent refines its hypotheses based on experimental outcomes and uses a
self-evaluation protocol to assess whether its findings accurately explain
model behavior. When inconsistencies arise, the agent self-reflects over its
findings and triggers a new cycle of experimentation. We evaluate our approach
on a novel benchmark of 130 models designed to exhibit diverse visual attribute
dependencies across 18 categories. Our results show that the agent's
performance consistently improves with self-reflection, with a significant
performance increase over non-reflective baselines. We further demonstrate that
the agent identifies real-world visual attribute dependencies in
state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object
detector.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [113] [Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets](https://arxiv.org/abs/2510.19944)
*Jiashi Feng,Xiu Li,Jing Lin,Jiahang Liu,Gaohong Liu,Weiqiang Lou,Su Ma,Guang Shi,Qinlong Wang,Jun Wang,Zhongcong Xu,Xuanyu Yi,Zihao Yu,Jianfeng Zhang,Yifan Zhu,Rui Chen,Jinxin Chi,Zixian Du,Li Han,Lixin Huang,Kaihua Jiang,Yuhan Li,Guan Luo,Shuguang Wang,Qianyi Wu,Fan Yang,Junyang Zhang,Xuanmeng Zhang*

Main category: eess.IV

TL;DR: Seed3D 1.0 is a foundation model that generates simulation-ready 3D assets from single images, enabling scalable content creation for physics-based world simulators while maintaining physics accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the scalability limitations of physics-based engines that require costly manual asset creation, while providing the physics accuracy that video-based methods lack for interactive learning in embodied AI training.

Method: Generates simulation-ready 3D assets from single images with accurate geometry, well-aligned textures, and realistic physically-based materials that can be directly integrated into physics engines with minimal configuration.

Result: Produces assets that can be directly deployed in robotic manipulation and simulation training, and scales to complete scene generation by assembling objects into coherent environments.

Conclusion: Seed3D 1.0 provides a foundation for advancing physics-based world simulators by enabling scalable simulation-ready content creation, bridging the gap between content diversity and physics accuracy.

Abstract: Developing embodied AI agents requires scalable training environments that
balance content diversity with physics accuracy. World simulators provide such
environments but face distinct limitations: video-based methods generate
diverse content but lack real-time physics feedback for interactive learning,
while physics-based engines provide accurate dynamics but face scalability
limitations from costly manual asset creation. We present Seed3D 1.0, a
foundation model that generates simulation-ready 3D assets from single images,
addressing the scalability challenge while maintaining physics rigor. Unlike
existing 3D generation models, our system produces assets with accurate
geometry, well-aligned textures, and realistic physically-based materials.
These assets can be directly integrated into physics engines with minimal
configuration, enabling deployment in robotic manipulation and simulation
training. Beyond individual objects, the system scales to complete scene
generation through assembling objects into coherent environments. By enabling
scalable simulation-ready content creation, Seed3D 1.0 provides a foundation
for advancing physics-based world simulators. Seed3D 1.0 is now available on
https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&tab=Gen3D

</details>


### [114] [Lightweight Classifier for Detecting Intracranial Hemorrhage in Ultrasound Data](https://arxiv.org/abs/2510.20857)
*Phat Tran,Enbai Kuang,Fred Xu*

Main category: eess.IV

TL;DR: Machine learning enables automated intracranial hemorrhage detection using portable ultrasound tissue pulsatility imaging, achieving 98% accuracy with ensemble methods after PCA transformation.


<details>
  <summary>Details</summary>
Motivation: Current CT and MRI diagnostics for traumatic brain injury-related intracranial hemorrhage are expensive, limited in availability, and infrastructure-dependent, especially in resource-constrained environments.

Method: Used ultrasound TPI signals from TBI patients with CT-confirmed labels, applied z-score normalization and PCA for dimensionality reduction, and evaluated multiple classification algorithms across three feature representations.

Result: PCA transformation significantly improved classifier performance, with ensemble methods achieving 98.0% accuracy and F1-score of 0.890, effectively handling class imbalance.

Conclusion: Machine learning-based ICH detection using portable ultrasound is feasible and has applications in emergency medicine, rural healthcare, and military settings where traditional imaging is unavailable.

Abstract: Intracranial hemorrhage (ICH) secondary to Traumatic Brain Injury (TBI)
represents a critical diagnostic challenge, with approximately 64,000
TBI-related deaths annually in the United States. Current diagnostic modalities
including Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) have
significant limitations: high cost, limited availability, and infrastructure
dependence, particularly in resource-constrained environments. This study
investigates machine learning approaches for automated ICH detection using
Ultrasound Tissue Pulsatility Imaging (TPI), a portable technique measuring
tissue displacement from hemodynamic forces during cardiac cycles. We analyze
ultrasound TPI signals comprising 30 temporal frames per cardiac cycle with
recording angle information, collected from TBI patients with CT-confirmed
ground truth labels. Our preprocessing pipeline employs z-score normalization
and Principal Component Analysis (PCA) for dimensionality reduction, retaining
components explaining 95% of cumulative variance. We systematically evaluate
multiple classification algorithms spanning probabilistic, kernel-based, neural
network, and ensemble learning approaches across three feature representations:
original 31-dimensional space, reduced subset, and PCA-transformed space.
Results demonstrate that PCA transformation substantially improves classifier
performance, with ensemble methods achieving 98.0% accuracy and F1-score of
0.890, effectively balancing precision and recall despite class imbalance.
These findings establish the feasibility of machine learning-based ICH
detection in TBI patients using portable ultrasound devices, with applications
in emergency medicine, rural healthcare, and military settings where
traditional imaging is unavailable.

</details>


### [115] [Eye-Tracking as a Tool to Quantify the Effects of CAD Display on Radiologists' Interpretation of Chest Radiographs](https://arxiv.org/abs/2510.20864)
*Daisuke Matsumoto,Tomohiro Kikuchi,Yusuke Takagi,Soichiro Kojima,Ryoma Kobayashi,Daiju Ueda,Kohei Yamamoto,Sho Kawabe,Harushi Mori*

Main category: eess.IV

TL;DR: Eye tracking study shows that concurrent bounding-box displays in chest radiograph interpretation prolong interpretation time, increase lesion dwell time and gaze path length, but reduce time to first lesion fixation.


<details>
  <summary>Details</summary>
Motivation: To quantify how concurrent reader displays like bounding-box highlights affect visual search behavior during chest radiograph interpretation using eye tracking.

Method: Three radiologists interpreted 180 chest radiographs twice (with/without bounding boxes) with eye tracking. Metrics included interpretation time, time to first fixation, lesion dwell time, gaze-path length, and lung-field coverage ratio. Linear mixed models were used for analysis.

Result: Bounding-box displays increased interpretation time by 4.9s, lesion dwell time by 1.3s, gaze-path length by 2076 pixels, and lung-field coverage by 10.5%, while reducing time to first lesion fixation by 1.3s (all p<0.001).

Conclusion: Eye tracking successfully captured measurable changes in search behavior with concurrent bounding-box displays, supporting feasibility of this approach and need for larger studies to confirm effects.

Abstract: Rationale and Objectives: Computer-aided detection systems for chest
radiographs are widely used, and concurrent reader displays, such as
bounding-box (BB) highlights, may influence the reading process. This pilot
study used eye tracking to conduct a preliminary experiment to quantify which
aspects of visual search were affected. Materials and Methods: We sampled 180
chest radiographs from the VinDR-CXR dataset: 120 with solitary pulmonary
nodules or masses and 60 without. The BBs were configured to yield an overall
display sensitivity and specificity of 80%. Three radiologists (with 11, 5, and
1 years of experience, respectively) interpreted each case twice - once with
BBs visible and once without - after a washout of >= 2 weeks. Eye movements
were recorded using an EyeTech VT3 Mini. Metrics included interpretation time,
time to first fixation on the lesion, lesion dwell time, total gaze-path
length, and lung-field coverage ratio. Outcomes were modeled using a linear
mixed model, with reading condition as a fixed effect and case and reader as
random intercepts. The primary analysis was restricted to true positives
(n=96). Results: Concurrent BB display prolonged interpretation time by 4.9 s
(p<0.001) and increased lesion dwell time by 1.3 s (p<0.001). Total gaze-path
length increased by 2,076 pixels (p<0.001), and lung-field coverage ratio
increased by 10.5% (p<0.001). Time to first fixation on the lesion was reduced
by 1.3 s (p<0.001). Conclusion: Eye tracking captured measurable alterations in
search behavior associated with concurrent BB displays during chest radiograph
interpretation. These findings support the feasibility of this approach and
highlight the need for larger studies to confirm effects and explore
implications across modalities and clinical contexts.

</details>


### [116] [Efficient Meningioma Tumor Segmentation Using Ensemble Learning](https://arxiv.org/abs/2510.21040)
*Mohammad Mahdi Danesh Pajouh,Sara Saeedi*

Main category: eess.IV

TL;DR: Proposed an ensemble-based segmentation approach combining three architectures to improve meningioma segmentation from MRI scans with reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: Meningioma segmentation from MRI is crucial but challenging, and existing deep learning methods are computationally intensive, limiting accessibility for researchers with limited hardware.

Method: Ensemble of three architectures: baseline SegResNet, attention-augmented SegResNet with concatenative skip connections, and dual-decoder U-Net with attention-gated skip connections (DDUNet), each trained for only 20 epochs.

Result: Achieved competitive performance on BraTS-MEN 2025 dataset with Lesion-Wise Dice scores of 77.30% (ET), 76.37% (TC), and 73.9% (WT) on test data.

Conclusion: The ensemble approach provides an effective and accessible tool for meningioma segmentation, demonstrating robustness even under hardware constraints, with potential clinical and research applications.

Abstract: Meningiomas represent the most prevalent form of primary brain tumors,
comprising nearly one-third of all diagnosed cases. Accurate delineation of
these tumors from MRI scans is crucial for guiding treatment strategies, yet
remains a challenging and time-consuming task in clinical practice. Recent
developments in deep learning have accelerated progress in automated tumor
segmentation; however, many advanced techniques are hindered by heavy
computational demands and long training schedules, making them less accessible
for researchers and clinicians working with limited hardware. In this work, we
propose a novel ensemble-based segmentation approach that combines three
distinct architectures: (1) a baseline SegResNet model, (2) an
attention-augmented SegResNet with concatenative skip connections, and (3) a
dual-decoder U-Net enhanced with attention-gated skip connections (DDUNet). The
ensemble aims to leverage architectural diversity to improve robustness and
accuracy while significantly reducing training demands. Each baseline model was
trained for only 20 epochs and Evaluated on the BraTS-MEN 2025 dataset. The
proposed ensemble model achieved competitive performance, with average
Lesion-Wise Dice scores of 77.30%, 76.37% and 73.9% on test dataset for
Enhancing Tumor (ET), Tumor Core (TC) and Whole Tumor (WT) respectively. These
results highlight the effectiveness of ensemble learning for brain tumor
segmentation, even under limited hardware constraints. Our proposed method
provides a practical and accessible tool for aiding the diagnosis of
meningioma, with potential impact in both clinical and research settings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [117] [AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive Refinement for Drivable-Area Segmentation](https://arxiv.org/abs/2510.21536)
*Narendhiran Vijayakumar,Sridevi. M*

Main category: cs.RO

TL;DR: AURASeg is a ground-plane semantic segmentation model that improves border precision while maintaining high accuracy through attention-guided upsampling and boundary refinement modules.


<details>
  <summary>Details</summary>
Motivation: Existing ground segmentation models struggle with fine-grained features, multi-scale processing, boundary refinement, and feature representation, especially in indoor and structured environments for robots.

Method: Uses CSP-Darknet backbone with Residual Border Refinement Module (RBRM) for edge delineation, Attention Progressive Upsampling Decoder (APUD) for feature integration, and lightweight ASPP-Lite for multi-scale context extraction.

Result: Achieves +1.26% improvement in mIoU and +1.65% improvement in segmentation precision compared to state-of-the-art models on GMRP Dataset and custom Gazebo indoor dataset.

Conclusion: The technique is feasible for autonomous perception in both indoor and outdoor environments, enabling precise border refinement with minimal impact on inference speed.

Abstract: Free space ground segmentation is essential to navigate robots and autonomous
vehicles, recognize drivable zones, and traverse efficiently. Fine-grained
features remain challenging for existing segmentation models, particularly for
robots in indoor and structured environments. These difficulties arise from
ineffective multi-scale processing, suboptimal boundary refinement, and limited
feature representation. In order to overcome these limitations, we propose
Attention-Guided Upsampling with Residual Boundary-Assistive Refinement
(AURASeg), a ground-plane semantic segmentation model that maintains high
segmentation accuracy while improving border precision. Our method uses
CSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for
accurate edge delineation and an Attention Progressive Upsampling Decoder
(APUD) for strong feature integration. We also incorporate a lightweight Atrous
Spatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context
extraction without compromising real-time performance. The proposed model beats
benchmark segmentation architectures in mIoU and F1 metrics when tested on the
Ground Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor
dataset. Our approach achieves an improvement in mean Intersection-over-Union
(mIoU) of +1.26% and segmentation precision of +1.65% compared to
state-of-the-art models. These results show that our technique is feasible for
autonomous perception in both indoor and outdoor environments, enabling precise
border refinement with minimal effect on inference speed.

</details>


### [118] [Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos](https://arxiv.org/abs/2510.21571)
*Qixiu Li,Yu Deng,Yaobo Liang,Lin Luo,Lei Zhou,Chengtang Yao,Lingqi Zeng,Zhiyuan Feng,Huizhi Liang,Sicheng Xu,Yizhong Zhang,Xi Chen,Hao Chen,Lily Sun,Dong Chen,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: Pretraining robotic manipulation VLA models using unscripted human hand videos, treating human hand as robot end-effector, achieving strong zero-shot capabilities and improved task success rates after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To leverage large-scale unannotated real-life human hand videos for robotic manipulation training, overcoming limitations of existing robot data coverage and enabling scalable VLA pretraining.

Method: Developed fully-automated holistic human activity analysis approach that transforms egocentric human videos into VLA training data with atomic-level hand activity segments, language descriptions, and 3D motion data.

Result: Created hand-VLA dataset with 1M episodes and 26M frames covering wide range of objects, dexterous tasks, and environments. Model showed strong zero-shot capabilities and improved task success rates (up to 30% improvement) after fine-tuning with small robot data.

Conclusion: This approach provides scalable foundation for VLA pretraining, advancing toward generalizable embodied intelligence by leveraging abundant human demonstration data.

Abstract: This paper presents a novel approach for pretraining robotic manipulation
Vision-Language-Action (VLA) models using a large corpus of unscripted
real-life video recordings of human hand activities. Treating human hand as
dexterous robot end-effector, we show that "in-the-wild" egocentric human
videos without any annotations can be transformed into data formats fully
aligned with existing robotic V-L-A training data in terms of task granularity
and labels. This is achieved by the development of a fully-automated holistic
human activity analysis approach for arbitrary human hand videos. This approach
can generate atomic-level hand activity segments and their language
descriptions, each accompanied with framewise 3D hand motion and camera motion.
We process a large volume of egocentric videos and create a hand-VLA training
dataset containing 1M episodes and 26M frames. This training data covers a wide
range of objects and concepts, dexterous manipulation tasks, and environment
variations in real life, vastly exceeding the coverage of existing robot data.
We design a dexterous hand VLA model architecture and pretrain the model on
this dataset. The model exhibits strong zero-shot capabilities on completely
unseen real-world observations. Additionally, fine-tuning it on a small amount
of real robot action data significantly improves task success rates and
generalization to novel objects in real robotic experiments. We also
demonstrate the appealing scaling behavior of the model's task performance with
respect to pretraining data scale. We believe this work lays a solid foundation
for scalable VLA pretraining, advancing robots toward truly generalizable
embodied intelligence.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [119] [Designing and Evaluating Hint Generation Systems for Science Education](https://arxiv.org/abs/2510.21087)
*Anubhav Jangra,Smaranda Muresan*

Main category: cs.HC

TL;DR: The paper explores using LLMs to generate educational hints that guide students without giving away answers, comparing static vs dynamic hinting strategies.


<details>
  <summary>Details</summary>
Motivation: To address concerns that general-purpose LLMs in education may hinder learning by providing direct answers, and instead promote active engagement through scaffolded hinting.

Method: Used large language models to generate hint chains for scientific topics at secondary level, compared static (pre-generated) vs dynamic (progress-adapted) hinting strategies through quantitative study with 41 participants.

Result: Found different learner preferences for hinting strategies and identified limitations of automatic evaluation metrics in capturing these preferences.

Conclusion: Highlights key design considerations for developing learner-centered educational technologies, emphasizing the need for better evaluation methods and adaptive hinting approaches.

Abstract: Large language models are influencing the education landscape, with students
relying on them in their learning process. Often implemented using
general-purpose models, these systems are likely to give away the answers,
which could hinder conceptual understanding and critical thinking. We study the
role of automatic hint generation as a pedagogical strategy to promote active
engagement with the learning content, while guiding learners toward the
answers. Focusing on scientific topics at the secondary education level, we
explore the potential of large language models to generate chains of hints that
scaffold learners without revealing answers. We compare two distinct hinting
strategies: static hints, pre-generated for each problem, and dynamic hints,
adapted to learners' progress. Through a quantitative study with 41
participants, we uncover different preferences among learners with respect to
hinting strategies, and identify the limitations of automatic evaluation
metrics to capture them. Our findings highlight key design considerations for
future research on hint generation and intelligent tutoring systems that seek
to develop learner-centered educational technologies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [120] [Cultural Alien Sampler: Open-ended art generation balancing originality and coherence](https://arxiv.org/abs/2510.20849)
*Alejandro H. Artiles,Hiromu Yakura,Levin Brinkmann,Mar Canet Sola,Hassan Abu Alhaija,Ignacio Serna,Nasim Rahaman,Bernhard Schölkopf,Iyad Rahwan*

Main category: cs.AI

TL;DR: The Cultural Alien Sampler (CAS) is a method that separates compositional fit from cultural typicality to generate creative ideas that are both original and coherent, outperforming baselines and matching human art students in originality and harmony.


<details>
  <summary>Details</summary>
Motivation: Current LLMs either default to familiar cultural patterns or sacrifice coherence when pushed toward novelty, limiting their ability to generate truly creative and internally consistent ideas in open-ended domains like art.

Method: CAS uses two fine-tuned GPT-2 models: a Concept Coherence Model that scores concept co-occurrence plausibility within artworks, and a Cultural Context Model that estimates typicality within artists' work. It selects combinations high in coherence but low in typicality.

Result: Human evaluation showed CAS outperforms random selection and GPT-4o baselines, achieving performance comparable to human art students in both originality and harmony. Quantitative analysis revealed more diverse outputs and broader conceptual space exploration than GPT-4o.

Conclusion: Artificial cultural alienness can unlock creative potential in autonomous agents by generating ideas that maintain internal consistency while deviating from learned cultural conventions.

Abstract: In open-ended domains like art, autonomous agents must generate ideas that
are both original and internally coherent, yet current Large Language Models
(LLMs) either default to familiar cultural patterns or sacrifice coherence when
pushed toward novelty. We address this by introducing the Cultural Alien
Sampler (CAS), a concept-selection method that explicitly separates
compositional fit from cultural typicality. CAS uses two GPT-2 models
fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether
concepts plausibly co-occur within artworks, and a Cultural Context Model that
estimates how typical those combinations are within individual artists' bodies
of work. CAS targets combinations that are high in coherence and low in
typicality, yielding ideas that maintain internal consistency while deviating
from learned conventions and embedded cultural context. In a human evaluation
(N = 100), our approach outperforms random selection and GPT-4o baselines and
achieves performance comparable to human art students in both perceived
originality and harmony. Additionally, a quantitative study shows that our
method produces more diverse outputs and explores a broader conceptual space
than its GPT-4o counterpart, demonstrating that artificial cultural alienness
can unlock creative potential in autonomous agents.

</details>


### [121] [When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](https://arxiv.org/abs/2510.21285)
*Yingzhi Mao,Chunkang Zhang,Junxiang Wang,Xinyan Guan,Boxi Cao,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: CoG framework improves LRM safety by correcting unsafe reasoning steps while preserving reasoning ability, addressing the safety-reasoning trade-off better than previous methods.


<details>
  <summary>Details</summary>
Motivation: LRMs show strong reasoning but are vulnerable to safety risks like harmful content and jailbreak attacks. Existing safety methods suppress reasoning ability and fail to resolve the safety-reasoning trade-off.

Method: Proposed Chain-of-Guardrail (CoG) training framework that recomposes or backtracks unsafe reasoning steps to steer models back to safe trajectories while preserving valid reasoning chains.

Result: CoG substantially improves safety across multiple reasoning and safety benchmarks while preserving comparable reasoning ability, outperforming prior methods that suffer from severe safety-reasoning trade-offs.

Conclusion: LRMs inherently can reject unsafe queries but this ability is compromised; CoG effectively addresses this by correcting unsafe reasoning without sacrificing reasoning performance.

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
reasoning tasks but remain vulnerable to severe safety risks, including harmful
content generation and jailbreak attacks. Existing mitigation strategies rely
on injecting heuristic safety signals during training, which often suppress
reasoning ability and fail to resolve the safety-reasoning trade-off. To
systematically investigate this issue, we analyze the reasoning trajectories of
diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models
override their own risk assessments and justify responding to unsafe prompts.
This finding reveals that LRMs inherently possess the ability to reject unsafe
queries, but this ability is compromised, resulting in harmful outputs.
Building on these insights, we propose the Chain-of-Guardrail (CoG), a training
framework that recomposes or backtracks unsafe reasoning steps, steering the
model back onto safe trajectories while preserving valid reasoning chains.
Extensive experiments across multiple reasoning and safety benchmarks
demonstrate that CoG substantially improves the safety of current LRMs while
preserving comparable reasoning ability, significantly outperforming prior
methods that suffer from severe safety-reasoning trade-offs.

</details>


### [122] [Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation](https://arxiv.org/abs/2510.21341)
*Lufan Chang*

Main category: cs.AI

TL;DR: Magellan is a framework that uses Monte Carlo Tree Search with hierarchical guidance to help LLMs generate more innovative ideas by steering exploration away from familiar concepts.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with true innovation, defaulting to familiar concepts from training data. Existing methods like Tree of Thoughts rely on flawed self-evaluation heuristics.

Method: Uses Monte Carlo Tree Search with hierarchical guidance: semantic compass for long-range direction and landscape-aware value function for local decisions that balances coherence, novelty, and progress.

Result: Significantly outperforms strong baselines (ReAct and ToT) in generating scientific ideas with superior plausibility and innovation.

Conclusion: Principled, guided search is more effective than unconstrained agency for creative discovery, enabling LLMs to become better innovation partners.

Abstract: Large Language Models (LLMs) often struggle with generating truly innovative
ideas, typically defaulting to high-probability, familiar concepts within their
training data's "gravity wells." While advanced search-based methods like Tree
of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by
their reliance on unprincipled, inconsistent self-evaluation heuristics to
guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel
framework that reframes creative generation as a principled, guided exploration
of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo
Tree Search (MCTS) governed by a hierarchical guidance system. For long-range
direction, a "semantic compass" vector, formulated via orthogonal projection,
steers the search towards relevant novelty. For local, step-by-step decisions,
a landscape-aware value function replaces flawed self-evaluation with an
explicit reward structure that balances intrinsic coherence, extrinsic novelty,
and narrative progress. Extensive experiments demonstrate that Magellan
significantly outperforms strong baselines, including ReAct and ToT, in
generating scientific ideas with superior plausibility and innovation. Our work
shows that for creative discovery, a principled, guided search is more
effective than unconstrained agency, paving the way for LLMs to become more
capable partners in innovation.

</details>


### [123] [DeepAgent: A General Reasoning Agent with Scalable Toolsets](https://arxiv.org/abs/2510.21618)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Guanting Dong,Jiajie Jin,Yinuo Wang,Hao Wang,Yutao Zhu,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: DeepAgent is an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution in a single reasoning process, featuring memory folding and ToolPO reinforcement learning for efficient tool use.


<details>
  <summary>Details</summary>
Motivation: Real-world tasks require external tools and long-horizon interactions, but existing agent frameworks with predefined workflows limit autonomous and global task completion.

Method: Introduces autonomous memory folding to compress past interactions into structured memories, and ToolPO reinforcement learning that uses LLM-simulated APIs with tool-call advantage attribution for fine-grained credit assignment.

Result: Outperforms baselines across eight benchmarks including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE) in both labeled-tool and open-set tool retrieval scenarios.

Conclusion: This work advances toward more general and capable agents for real-world applications by enabling autonomous thinking and efficient tool use through memory management and reinforcement learning.

Abstract: Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.

</details>


### [124] [AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite](https://arxiv.org/abs/2510.21652)
*Jonathan Bragg,Mike D'Arcy,Nishant Balepur,Dan Bareket,Bhavana Dalvi,Sergey Feldman,Dany Haddad,Jena D. Hwang,Peter Jansen,Varsha Kishore,Bodhisattwa Prasad Majumder,Aakanksha Naik,Sigal Rahamimov,Kyle Richardson,Amanpreet Singh,Harshit Surana,Aryeh Tiktinsky,Rosni Vasu,Guy Wiener,Chloe Anastasiades,Stefan Candra,Jason Dunkelberger,Dan Emery,Rob Evans,Malachi Hamada,Regan Huff,Rodney Kinney,Matt Latzke,Jaron Lochner,Ruben Lozano-Aguilera,Cecile Nguyen,Smita Rao,Amber Tanaka,Brooke Vlahos,Peter Clark,Doug Downey,Yoav Goldberg,Ashish Sabharwal,Daniel S. Weld*

Main category: cs.AI

TL;DR: AstaBench is a comprehensive benchmarking suite for AI agents in scientific research, addressing limitations of existing benchmarks by providing holistic evaluation, reproducible tools, controlled comparisons, and standardized interfaces.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for AI agents in science research are inadequate - they lack holistic measures, reproducible tools, controlled comparisons, standardized interfaces, and comprehensive baselines, making it difficult to properly evaluate agent capabilities.

Method: Developed AstaBench with 2400+ problems spanning the entire scientific discovery process across multiple domains, including a scientific research environment with production-grade search tools for reproducible evaluation, plus 9 science-optimized agent classes and numerous baselines.

Result: Evaluation of 57 agents across 22 classes revealed that despite progress on individual aspects, AI remains far from solving the challenge of science research assistance.

Conclusion: AstaBench provides the first holistic measure of agentic ability for scientific research, enabling more rigorous benchmarking and revealing current limitations of AI in science assistance.

Abstract: AI agents hold the potential to revolutionize scientific productivity by
automating literature reviews, replicating experiments, analyzing data, and
even proposing new directions of inquiry; indeed, there are now many such
agents, ranging from general-purpose "deep research" systems to specialized
science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of
these agents is critical for progress. Yet existing benchmarks fall short on
several fronts: they (1) fail to provide holistic, product-informed measures of
real-world use cases such as science research; (2) lack reproducible agent
tools necessary for a controlled comparison of core agentic capabilities; (3)
do not account for confounding variables such as model cost and tool access;
(4) do not provide standardized interfaces for quick agent prototyping and
evaluation; and (5) lack comprehensive baseline agents necessary to identify
true advances. In response, we define principles and tooling for more
rigorously benchmarking agents. Using these, we present AstaBench, a suite that
provides the first holistic measure of agentic ability to perform scientific
research, comprising 2400+ problems spanning the entire scientific discovery
process and multiple scientific domains, and including many problems inspired
by actual user requests to deployed Asta agents. Our suite comes with the first
scientific research environment with production-grade search tools that enable
controlled, reproducible evaluation, better accounting for confounders.
Alongside, we provide a comprehensive suite of nine science-optimized classes
of Asta agents and numerous baselines. Our extensive evaluation of 57 agents
across 22 agent classes reveals several interesting findings, most importantly
that despite meaningful progress on certain individual aspects, AI remains far
from solving the challenge of science research assistance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [125] [Pctx: Tokenizing Personalized Context for Generative Recommendation](https://arxiv.org/abs/2510.21276)
*Qiyong Zhong,Jiajie Su,Yunshan Ma,Julian McAuley,Yupeng Hou*

Main category: cs.IR

TL;DR: Proposes personalized context-aware tokenization for generative recommendation models, allowing items to have different semantic IDs based on user context, improving personalization.


<details>
  <summary>Details</summary>
Motivation: Existing tokenization methods are static and non-personalized, assuming universal item similarity that overlooks user-specific perspectives and preferences.

Method: Personalized context-aware tokenizer that incorporates user's historical interactions when generating semantic IDs, enabling different tokenization for same items under different user contexts.

Result: Experiments on three public datasets show up to 11.44% improvement in NDCG@10 over non-personalized action tokenization baselines.

Conclusion: Personalized tokenization enables generative recommendation models to capture multiple interpretive standards and produce more personalized predictions.

Abstract: Generative recommendation (GR) models tokenize each action into a few
discrete tokens (called semantic IDs) and autoregressively generate the next
tokens as predictions, showing advantages such as memory efficiency,
scalability, and the potential to unify retrieval and ranking. Despite these
benefits, existing tokenization methods are static and non-personalized. They
typically derive semantic IDs solely from item features, assuming a universal
item similarity that overlooks user-specific perspectives. However, under the
autoregressive paradigm, semantic IDs with the same prefixes always receive
similar probabilities, so a single fixed mapping implicitly enforces a
universal item similarity standard across all users. In practice, the same item
may be interpreted differently depending on user intentions and preferences. To
address this issue, we propose a personalized context-aware tokenizer that
incorporates a user's historical interactions when generating semantic IDs.
This design allows the same item to be tokenized into different semantic IDs
under different user contexts, enabling GR models to capture multiple
interpretive standards and produce more personalized predictions. Experiments
on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over
non-personalized action tokenization baselines. Our code is available at
https://github.com/YoungZ365/Pctx.

</details>


### [126] [Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research](https://arxiv.org/abs/2510.21603)
*Kuicai Dong,Shurui Huang,Fangda Ye,Wei Han,Zhi Zhang,Dexun Li,Wenjun Li,Qu Yang,Gang Wang,Yichao Wang,Chen Zhang,Yong Liu*

Main category: cs.IR

TL;DR: Doc-Researcher is a unified system for deep research on multimodal documents that overcomes limitations of text-only systems through multimodal parsing, adaptive retrieval, and iterative multi-agent workflows, achieving 3.4x better accuracy than state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Current deep research systems are constrained to textual web data and overlook vast knowledge embedded in multimodal documents, lacking capabilities for preserving visual semantics, maintaining structural coherence, and adaptive multimodal retrieval.

Method: Three integrated components: (1) deep multimodal parsing preserving layout structure and visual semantics with multi-granular representations, (2) systematic retrieval architecture supporting text-only, vision-only, and hybrid paradigms with dynamic granularity selection, (3) iterative multi-agent workflows that decompose queries, accumulate evidence, and synthesize answers across documents and modalities.

Result: Achieved 50.6% accuracy on M4DocBench benchmark, which is 3.4x better than state-of-the-art baselines. Introduced M4DocBench - the first benchmark for multi-modal, multi-hop, multi-document, and multi-turn deep research with 158 expert-annotated questions across 304 documents.

Conclusion: Effective document research requires fundamentally deep parsing that preserves multimodal integrity and supports iterative research, establishing a new paradigm for conducting deep research on multimodal document collections.

Abstract: Deep Research systems have revolutionized how LLMs solve complex questions
through iterative reasoning and evidence gathering. However, current systems
remain fundamentally constrained to textual web data, overlooking the vast
knowledge embedded in multimodal documents Processing such documents demands
sophisticated parsing to preserve visual semantics (figures, tables, charts,
and equations), intelligent chunking to maintain structural coherence, and
adaptive retrieval across modalities, which are capabilities absent in existing
systems. In response, we present Doc-Researcher, a unified system that bridges
this gap through three integrated components: (i) deep multimodal parsing that
preserves layout structure and visual semantics while creating multi-granular
representations from chunk to document level, (ii) systematic retrieval
architecture supporting text-only, vision-only, and hybrid paradigms with
dynamic granularity selection, and (iii) iterative multi-agent workflows that
decompose complex queries, progressively accumulate evidence, and synthesize
comprehensive answers across documents and modalities. To enable rigorous
evaluation, we introduce M4DocBench, the first benchmark for Multi-modal,
Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158
expert-annotated questions with complete evidence chains across 304 documents,
M4DocBench tests capabilities that existing benchmarks cannot assess.
Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter
than state-of-the-art baselines, validating that effective document research
requires not just better retrieval, but fundamentally deep parsing that
preserve multimodal integrity and support iterative research. Our work
establishes a new paradigm for conducting deep research on multimodal document
collections.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [127] [HIKMA: Human-Inspired Knowledge by Machine Agents through a Multi-Agent Framework for Semi-Autonomous Scientific Conferences](https://arxiv.org/abs/2510.21370)
*Zain Ul Abideen Tariq,Mahmood Al-Zubaidi,Uzair Shah,Marco Agus,Mowafa Househ*

Main category: cs.MA

TL;DR: HIKMA is the first semi-autonomous conference that integrates AI throughout the entire academic publishing pipeline, from dataset curation to archival dissemination, demonstrating AI's potential to support rather than replace traditional scholarly practices.


<details>
  <summary>Details</summary>
Motivation: To reimagine scholarly communication by integrating AI into academic publishing and presentation pipelines while maintaining intellectual property protection, transparency, and integrity.

Method: Developed the HIKMA framework with AI dataset curation, AI-based manuscript generation, AI-assisted peer review, AI-driven revision, AI conference presentation, and AI archival dissemination using language models, structured research workflows, and domain safeguards.

Result: Successfully implemented HIKMA as a testbed and proof of concept, providing insights into opportunities and challenges of AI-enabled scholarship while examining AI authorship, accountability, and human-AI collaboration.

Conclusion: HIKMA demonstrates how AI can support traditional scholarly practices without replacing them, serving as a pioneering model for AI-integrated academic communication that maintains research integrity and addresses key questions about AI's role in scholarship.

Abstract: HIKMA Semi-Autonomous Conference is the first experiment in reimagining
scholarly communication through an end-to-end integration of artificial
intelligence into the academic publishing and presentation pipeline. This paper
presents the design, implementation, and evaluation of the HIKMA framework,
which includes AI dataset curation, AI-based manuscript generation, AI-assisted
peer review, AI-driven revision, AI conference presentation, and AI archival
dissemination. By combining language models, structured research workflows, and
domain safeguards, HIKMA shows how AI can support - not replace traditional
scholarly practices while maintaining intellectual property protection,
transparency, and integrity. The conference functions as a testbed and proof of
concept, providing insights into the opportunities and challenges of AI-enabled
scholarship. It also examines questions about AI authorship, accountability,
and the role of human-AI collaboration in research.

</details>


### [128] [ColorEcosystem: Powering Personalized, Standardized, and Trustworthy Agentic Service in massive-agent Ecosystem](https://arxiv.org/abs/2510.21566)
*Fangwen Wu,Zheng Wu,Jihong Wang,Yunku Chen,Ruiguang Pei,Heyuan Huang,Xin Liao,Xingyu Lou,Huarong Deng,Zhihui Fu,Weiwen Liu,Zhuosheng Zhang,Weinan Zhang,Jun Wang*

Main category: cs.MA

TL;DR: ColorEcosystem is a blueprint for massive-agent ecosystems that addresses challenges like impersonal services, lack of standardization, and untrustworthy behavior through three key components: agent carrier for personalization, agent store for standardization, and agent audit for trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Current massive-agent ecosystems face growing challenges including impersonal service experiences, lack of standardization, and untrustworthy behavior, which need to be addressed to enable effective agentic service management at scale.

Method: The proposed ColorEcosystem consists of three key components: agent carrier (provides personalized service experiences using user-specific data and digital twins), agent store (centralized standardized platform for managing diverse agentic services), and agent audit (ensures integrity and credibility through supervision of developer and user activities).

Result: The authors have implemented part of ColorEcosystem's functionality and open-sourced the relevant code, demonstrating the practical feasibility of their approach.

Conclusion: ColorEcosystem is positioned to power personalized, standardized, and trustworthy agentic service across massive-agent ecosystems by addressing key challenges through its three-component architecture.

Abstract: With the rapid development of (multimodal) large language model-based agents,
the landscape of agentic service management has evolved from single-agent
systems to multi-agent systems, and now to massive-agent ecosystems. Current
massive-agent ecosystems face growing challenges, including impersonal service
experiences, a lack of standardization, and untrustworthy behavior. To address
these issues, we propose ColorEcosystem, a novel blueprint designed to enable
personalized, standardized, and trustworthy agentic service at scale.
Concretely, ColorEcosystem consists of three key components: agent carrier,
agent store, and agent audit. The agent carrier provides personalized service
experiences by utilizing user-specific data and creating a digital twin, while
the agent store serves as a centralized, standardized platform for managing
diverse agentic services. The agent audit, based on the supervision of
developer and user activities, ensures the integrity and credibility of both
service providers and users. Through the analysis of challenges, transitional
forms, and practical considerations, the ColorEcosystem is poised to power
personalized, standardized, and trustworthy agentic service across
massive-agent ecosystems. Meanwhile, we have also implemented part of
ColorEcosystem's functionality, and the relevant code is open-sourced at
https://github.com/opas-lab/color-ecosystem.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [129] [Can large audio language models understand child stuttering speech? speech summarization, and source separation](https://arxiv.org/abs/2510.20850)
*Chibuzor Okocha,Maya Bakri,Christan Grant*

Main category: eess.AS

TL;DR: Evaluation of large audio-language models (LALMs) on disfluent child speech in mixed-speaker settings, focusing on source separation and child-only summarization while preserving clinically relevant disfluencies.


<details>
  <summary>Details</summary>
Motivation: Child speech differs significantly from adult speech in acoustics, prosody, and language development, and disfluencies further challenge ASR and NLP systems. The behavior of recent LALMs in handling disfluent child speech remains underexplored.

Method: Evaluated several state-of-the-art LALMs in two settings: interview (mixed speakers) and reading task (single child). Tasks included single-channel source separation to isolate child speech and child-only summarization preserving clinical disfluencies. Used LLM as judge, human expert ratings, and BERTScore for evaluation.

Result: Findings delineate conditions under which LALMs produce faithful child-only summaries from mixed audio and identify where they fail. The study provides practical guidance for clinical and educational deployments.

Conclusion: The research offers insights into LALM performance on disfluent child speech and provides evaluation framework with prompts and scripts to support replication in clinical and educational applications.

Abstract: Child speech differs from adult speech in acoustics, prosody, and language
development, and disfluencies (repetitions, prolongations, blocks) further
challenge Automatic Speech Recognition (ASR) and downstream Natural Language
Processing (NLP). Recent large audio-language models (LALMs) demonstrate strong
cross-modal audio understanding; however, their behavior in disfluent child
speech remains underexplored. We evaluate several state-of-the-art LALMs in two
settings: an interview (mixed speakers) and a reading task (single child). The
tasks are (i) single-channel source separation to isolate the child and (ii)
child-only summarization that preserves clinically relevant disfluencies and
avoids adult-speech leakage.
  Evaluation combines Large Language Model (LLM) as a judge, human expert
ratings, and BERTScore (F1), and we report agreement between models and between
models and humans to assess reliability. Our findings delineate the conditions
under which LALMs produce faithful child-only summaries from mixed audio and
where they fail, offering practical guidance for clinical and educational
deployments. We provide prompts and evaluation scripts to support replication.

</details>


### [130] [Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones via Physiology-informed Tokenization](https://arxiv.org/abs/2510.20853)
*Hyungjun Yoon,Seungjoo Lee,Yu Yvonne Wu,Xiaomeng Chen,Taiting Lu,Freddy Yifei Liu,Taeckyung Lee,Hyeongheon Cha,Haochen Zhao,Gaoteng Zhao,Sung-Ju Lee,Cecilia Mascolo,Dongyao Chen,Lili Qiu*

Main category: eess.AS

TL;DR: PiMT is a physiology-informed multi-band tokenization method that decomposes ExG signals into 12 tokens and uses reconstruction tasks to learn robust representations, enabling task-agnostic ExG monitoring with superior performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Current ExG foundation models face limitations in data diversity (most collected in controlled labs) and task-specific designs that limit generalization across tasks, hindering scalable monitoring in real-world settings.

Method: Collected 50 hours of free-living ExG data with earphone-based hardware, and developed Physiology-informed Multi-band Tokenization (PiMT) that decomposes signals into 12 physiology-informed tokens followed by reconstruction tasks.

Result: PiMT consistently outperforms state-of-the-art methods across diverse tasks on the new DailySense dataset (first ExG dataset across five human senses) and four public ExG benchmarks.

Conclusion: The approach enables scalable, task-agnostic ExG monitoring in the wild by addressing data diversity gaps and providing adaptive feature recognition across the full frequency spectrum.

Abstract: Electrophysiological (ExG) signals offer valuable insights into human
physiology, yet building foundation models that generalize across everyday
tasks remains challenging due to two key limitations: (i) insufficient data
diversity, as most ExG recordings are collected in controlled labs with bulky,
expensive devices; and (ii) task-specific model designs that require tailored
processing (i.e., targeted frequency filters) and architectures, which limit
generalization across tasks. To address these challenges, we introduce an
approach for scalable, task-agnostic ExG monitoring in the wild. We collected
50 hours of unobtrusive free-living ExG data with an earphone-based hardware
prototype to narrow the data diversity gap. At the core of our approach is
Physiology-informed Multi-band Tokenization (PiMT), which decomposes ExG
signals into 12 physiology-informed tokens, followed by a reconstruction task
to learn robust representations. This enables adaptive feature recognition
across the full frequency spectrum while capturing task-relevant information.
Experiments on our new DailySense dataset-the first to enable ExG-based
analysis across five human senses-together with four public ExG benchmarks,
demonstrate that PiMT consistently outperforms state-of-the-art methods across
diverse tasks.

</details>


### [131] [Data-Centric Lessons To Improve Speech-Language Pretraining](https://arxiv.org/abs/2510.20860)
*Vishaal Udandarao,Zhiyun Lu,Xuankai Chang,Yongqiang Wang,Violet Z. Yao,Albin Madapally Jose,Fartash Faghri,Josh Gardner,Chung-Cheng Chiu*

Main category: eess.AS

TL;DR: This paper conducts data-centric exploration for pretraining speech-language models (SpeechLMs), focusing on three key research questions about data processing, synthetic dataset construction, and training sequence interleaving.


<details>
  <summary>Details</summary>
Motivation: There is a lack of controlled ablations of pretraining data processing and curation for SpeechLMs, making it challenging to understand what factors account for performance despite substantial gains in other data modalities.

Method: The authors conducted controlled data-centric ablations focusing on three research questions: (1) processing raw web-crawled audio content, (2) constructing synthetic pretraining datasets to augment web-crawled data, and (3) interleaving (text, audio) segments into training sequences.

Result: Applying insights from the data-centric ablations, the authors pretrained a 3.8B-parameter SpeechLM called SpeLangy that outperforms models up to 3x larger by 10.2% absolute performance.

Conclusion: The findings highlight the significant impact of effective data curation for speech-language pretraining and provide guidance for future data-centric exploration in SpeechLMs.

Abstract: Spoken Question-Answering (SQA) is a core capability for useful and
interactive artificial intelligence systems. Recently, several speech-language
models (SpeechLMs) have been released with a specific focus on improving their
SQA performance. However, a lack of controlled ablations of pretraining data
processing and curation makes it challenging to understand what factors account
for performance, despite substantial gains from similar studies in other data
modalities. In this work, we address this gap by conducting a data-centric
exploration for pretraining SpeechLMs. We focus on three research questions
fundamental to speech-language pretraining data: (1) how to process raw
web-crawled audio content for speech-text pretraining, (2) how to construct
synthetic pretraining datasets to augment web-crawled data and (3) how to
interleave (text, audio) segments into training sequences. We apply the
insights from our controlled data-centric ablations to pretrain a
3.8B-parameter SpeechLM, called SpeLangy, that outperforms models that are up
to 3x larger by 10.2% absolute performance. We hope our findings highlight the
impact of effective data curation for speech-language pretraining and guide
future data-centric exploration in SpeechLMs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [132] [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956)
*Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CR

TL;DR: RLMs develop self-jailbreaking behavior after benign reasoning training, using strategies like introducing benign assumptions to justify fulfilling harmful requests, despite being aware of the harmfulness.


<details>
  <summary>Details</summary>
Motivation: To investigate the surprising phenomenon of unintentional misalignment in reasoning language models where they circumvent their own safety guardrails after training on benign reasoning tasks.

Method: Analyzed multiple open-weight RLMs including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron, examining their reasoning chains and compliance behavior after benign reasoning training.

Result: RLMs become more compliant after benign reasoning training and perceive malicious requests as less harmful in their chain-of-thought, enabling self-jailbreaking. Including minimal safety reasoning data during training effectively mitigates this issue.

Conclusion: Self-jailbreaking is a systematic phenomenon in RLMs that can be addressed through targeted safety reasoning training, providing a practical path forward for maintaining safety in increasingly capable reasoning models.

Abstract: We discover a novel and surprising phenomenon of unintentional misalignment
in reasoning language models (RLMs), which we call self-jailbreaking.
Specifically, after benign reasoning training on math or code domains, RLMs
will use multiple strategies to circumvent their own safety guardrails. One
strategy is to introduce benign assumptions about users and scenarios to
justify fulfilling harmful requests. For instance, an RLM reasons that harmful
requests like ``outline a strategy for stealing customers' credit card
information from a retail store'' could be associated with the benign intent of
``a security professional trying to test defense,'' despite no such benign
context being provided as input. We observe that many open-weight RLMs,
including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,
suffer from self-jailbreaking despite being aware of the harmfulness of the
requests. We also provide a mechanistic understanding of self-jailbreaking:
RLMs are more compliant after benign reasoning training, and after
self-jailbreaking, models appear to perceive malicious requests as less harmful
in the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,
we find that including minimal safety reasoning data during training is
sufficient to ensure RLMs remain safety-aligned. Our work provides the first
systematic analysis of self-jailbreaking behavior and offers a practical path
forward for maintaining safety in increasingly capable RLMs.

</details>


### [133] [SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](https://arxiv.org/abs/2510.21459)
*Adetayo Adebimpe,Helmut Neukirchen,Thomas Welsh*

Main category: cs.CR

TL;DR: SBASH framework uses lightweight local LLMs with RAG to improve honeypot context-awareness while addressing data protection, accuracy, and latency issues.


<details>
  <summary>Details</summary>
Motivation: Honeypots need context-awareness to maximize attacker engagement, but current LLM approaches face challenges with accuracy, response time, operational costs, and data protection in cloud deployments.

Method: Proposed SBASH framework using lightweight local LLMs with Retrieval Augmented Generation (RAG) for Linux shell commands, evaluated through response time, human realism assessment, and similarity metrics (Levenshtein distance, SBert, BertScore).

Result: RAG improves accuracy for untuned models, while system-prompt-tuned LLMs without RAG achieve similar accuracy as untuned models with RAG, with slightly lower latency.

Conclusion: Local LLMs with appropriate tuning can provide effective context-awareness for honeypots while addressing data protection and performance concerns.

Abstract: Honeypots are decoy systems used for gathering valuable threat intelligence
or diverting attackers away from production systems. Maximising attacker
engagement is essential to their utility. However research has highlighted that
context-awareness, such as the ability to respond to new attack types, systems
and attacker agents, is necessary to increase engagement. Large Language Models
(LLMs) have been shown as one approach to increase context awareness but suffer
from several challenges including accuracy and timeliness of response time,
high operational costs and data-protection issues due to cloud deployment. We
propose the System-Based Attention Shell Honeypot (SBASH) framework which
manages data-protection issues through the use of lightweight local LLMs. We
investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and
non-RAG LLMs for Linux shell commands and evaluate them using several different
metrics such as response time differences, realism from human testers, and
similarity to a real system calculated with Levenshtein distance, SBert, and
BertScore. We show that RAG improves accuracy for untuned models while models
that have been tuned via a system prompt that tells the LLM to respond like a
Linux system achieve without RAG a similar accuracy as untuned with RAG, while
having a slightly lower latency.

</details>


### [134] [An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing](https://arxiv.org/abs/2510.20932)
*Reza Ahmari,Ahmad Mohammadi,Vahid Hemmati,Mohammed Mynuddin,Mahmoud Nabil Mahmoud,Parham Kebria,Abdollah Homaifar,Mehrdad Saif*

Main category: cs.CR

TL;DR: This study investigates Trojan attacks on deep learning models in Urban Air Mobility vehicles, showing significant accuracy drops from 96.4% to 73.3% when triggered, and develops an evaluation framework to detect infected models.


<details>
  <summary>Details</summary>
Motivation: To understand the security vulnerabilities of autonomous navigation systems in UAM vehicles, particularly focusing on Trojan attacks that can cause specific failures while appearing normal in other situations.

Method: Used the DroNet framework to assess UAAV vulnerability, collected custom datasets, trained models to simulate real-world conditions, and developed an evaluation framework to identify Trojan-infected models.

Result: Experiments showed significant accuracy degradation - from 96.4% on clean data to 73.3% on Trojan-triggered data, demonstrating the effectiveness of Trojan attacks in compromising UAM systems.

Conclusion: The study demonstrates serious security risks from Trojan attacks in UAM systems and provides groundwork for future research to enhance system resilience against such threats.

Abstract: This study investigates the vulnerabilities of autonomous navigation and
landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses
on Trojan attacks that target deep learning models, such as Convolutional
Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within
a model's training data. These triggers cause specific failures under certain
conditions, while the model continues to perform normally in other situations.
We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using
the DroNet framework. Our experiments showed a significant drop in accuracy,
from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To
conduct this study, we collected a custom dataset and trained models to
simulate real-world conditions. We also developed an evaluation framework
designed to identify Trojan-infected models. This work demonstrates the
potential security risks posed by Trojan attacks and lays the groundwork for
future research on enhancing the resilience of UAM systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [135] [Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification](https://arxiv.org/abs/2510.21443)
*Mohammad Amin Zadenoori,Vincenzo De Martino,Jacek Dabrowski,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: Small language models (SLMs) perform nearly as well as large language models (LLMs) in requirements classification tasks, with only 2% lower F1 score that's not statistically significant, while being up to 300 times smaller and offering better privacy, cost, and local deployability.


<details>
  <summary>Details</summary>
Motivation: LLMs show good performance in requirements engineering but have high computational costs, data sharing risks, and dependence on external services. SLMs offer a lightweight, locally deployable alternative, but their comparative performance accuracy was unclear.

Method: Comparative study of eight models (three LLMs and five SLMs) on requirements classification tasks using PROMISE, PROMISE Reclass, and SecReq datasets.

Result: LLMs achieved average F1 score only 2% higher than SLMs, but the difference was not statistically significant. SLMs almost matched LLMs performance across all datasets and even outperformed them in recall on PROMISE Reclass dataset, despite being up to 300 times smaller. Dataset characteristics were more important than model size.

Conclusion: SLMs are a valid alternative to LLMs for requirements classification, offering advantages in privacy, cost, and local deployability while maintaining comparable performance.

Abstract: [Context and motivation] Large language models (LLMs) show notable results in
natural language processing (NLP) tasks for requirements engineering (RE).
However, their use is compromised by high computational cost, data sharing
risks, and dependence on external services. In contrast, small language models
(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]
It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms
of accuracy. [Results] Our preliminary study compares eight models, including
three LLMs and five SLMs, on requirements classification tasks using the
PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although
LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not
statistically significant. SLMs almost reach LLMs performance across all
datasets and even outperform them in recall on the PROMISE Reclass dataset,
despite being up to 300 times smaller. We also found that dataset
characteristics play a more significant role in performance than model size.
[Contribution] Our study contributes with evidence that SLMs are a valid
alternative to LLMs for requirements classification, offering advantages in
privacy, cost, and local deployability.

</details>


### [136] [Wisdom and Delusion of LLM Ensembles for Code Generation and Repair](https://arxiv.org/abs/2510.21513)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: Ensembles of coding LLMs can achieve 83% higher performance than single models, but consensus-based strategies fall into a "popularity trap" while diversity-based strategies realize up to 95% of this potential.


<details>
  <summary>Details</summary>
Motivation: Current pursuit of single Large Language Models for all software engineering tasks is resource-intensive and overlooks complementarity benefits between different models.

Method: Empirical comparison of ten individual LLMs from five families and three ensembles across three software engineering benchmarks covering code generation and program repair, evaluating complementarity and various selection heuristics.

Result: Theoretical upperbound for ensemble performance is 83% above best single model. Diversity-based strategy achieves up to 95% of this potential, while consensus-based strategies amplify common but incorrect outputs.

Conclusion: Diversity-based strategies enable cost-efficient performance enhancement through small two-model ensembles, providing a practical path beyond single-model systems.

Abstract: Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems.
  To address this gap, we empirically compare ten individual LLMs from five
families, and three ensembles of these LLMs across three software engineering
benchmarks covering code generation and program repair. We assess the
complementarity between models and the performance gap between the best
individual model and the ensembles. Next, we evaluate various selection
heuristics to identify correct solutions from an ensemble's candidate pool.
  We find that the theoretical upperbound for an ensemble's performance can be
83% above the best single model. Our results show that consensus-based
strategies for selecting solutions fall into a "popularity trap," amplifying
common but incorrect outputs. In contrast, a diversity-based strategy realizes
up to 95% of this theoretical potential, and proves effective even in small
two-model ensembles, enabling a cost-efficient way to enhance performance by
leveraging multiple LLMs.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [137] [Physics-Informed Deep Learning for Improved Input Function Estimation in Motion-Blurred Dynamic [${}^{18}$F]FDG PET Images](https://arxiv.org/abs/2510.21281)
*Christian Salomonsen,Kristoffer K. Wickstrøm,Samuel Kuttner,Elisabeth Wetzer*

Main category: q-bio.QM

TL;DR: Physics-informed deep learning model (PIDLIF) predicts arterial input function from PET images using kinetic modeling loss, showing improved robustness against motion-induced blurring.


<details>
  <summary>Details</summary>
Motivation: Traditional kinetic modeling for glucose metabolism quantification requires invasive arterial input function measurement. Deep learning approaches can predict AIF non-invasively but need improved robustness.

Method: Two-tissue compartment model over myocardium and brain regions, trained on 70 FDG dPET images with measured AIF, incorporating physics-informed kinetic modeling loss during training.

Result: PIDLIF performed comparably to non-physics-informed network, but maintained high performance under severe motion-induced image degradation scenarios.

Conclusion: Physics-informed approach improves robustness by constraining the problem physiologically, enabling consistent AIF prediction even with out-of-distribution image degradation from movement blurring.

Abstract: Kinetic modeling enables \textit{in vivo} quantification of tracer uptake and
glucose metabolism in [${}^{18}$F]Fluorodeoxyglucose ([${}^{18}$F]FDG) dynamic
positron emission tomography (dPET) imaging of mice. However, kinetic modeling
requires the accurate determination of the arterial input function (AIF) during
imaging, which is time-consuming and invasive. Recent studies have shown the
efficacy of using deep learning to directly predict the input function,
surpassing established methods such as the image-derived input function (IDIF).
In this work, we trained a physics-informed deep learning-based input function
prediction model (PIDLIF) to estimate the AIF directly from the PET images,
incorporating a kinetic modeling loss during training. The proposed method uses
a two-tissue compartment model over two regions, the myocardium and brain of
the mice, and is trained on a dataset of 70 [${}^{18}$F]FDG dPET images of mice
accompanied by the measured AIF during imaging. The proposed method had
comparable performance to the network without a physics-informed loss, and when
sudden movement causing blurring in the images was simulated, the PIDLIF model
maintained high performance in severe cases of image degradation. The proposed
physics-informed method exhibits an improved robustness that is promoted by
physically constraining the problem, enforcing consistency for
out-of-distribution samples. In conclusion, the PIDLIF model offers insight
into the effects of leveraging physiological distribution mechanics in mice to
guide a deep learning-based AIF prediction network in images with severe
degradation as a result of blurring due to movement during imaging.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference](https://arxiv.org/abs/2510.21184)
*Stephen Zhao,Aidan Li,Rob Brekelmans,Roger Grosse*

Main category: cs.LG

TL;DR: RePULSe is a new RL training method that improves the tradeoff between average reward and reducing probability of undesired outputs by augmenting standard RL loss with additional loss that guides sampling of low-reward outputs.


<details>
  <summary>Details</summary>
Motivation: Standard RL approaches optimize average reward but struggle to effectively reduce probability of undesired outputs without sacrificing average-case performance. There's a need for better tradeoff between expected reward and avoiding undesirable outputs.

Method: Augments standard RL loss with additional loss that uses learned proposals to guide sampling of low-reward outputs, then reduces those outputs' probability.

Result: RePULSe produces better tradeoff of expected reward versus probability of undesired outputs and is more adversarially robust compared to standard RL alignment approaches and alternatives.

Conclusion: RePULSe effectively addresses the limitation of standard RL in balancing average reward optimization with reduction of undesirable outputs, offering improved performance and robustness.

Abstract: Reinforcement learning (RL) has become a predominant technique to align
language models (LMs) with human preferences or promote outputs which are
deemed to be desirable by a given reward function. Standard RL approaches
optimize average reward, while methods explicitly focused on reducing the
probability of undesired outputs typically come at a cost to average-case
performance. To improve this tradeoff, we introduce RePULSe, a new training
method that augments the standard RL loss with an additional loss that uses
learned proposals to guide sampling low-reward outputs, and then reduces those
outputs' probability. We run experiments demonstrating that RePULSe produces a
better tradeoff of expected reward versus the probability of undesired outputs
and is more adversarially robust, compared to standard RL alignment approaches
and alternatives.

</details>


### [139] [Leverage Unlearning to Sanitize LLMs](https://arxiv.org/abs/2510.21322)
*Antoine Boutet,Lucas Magnana*

Main category: cs.LG

TL;DR: SANI is an unlearning approach that sanitizes language models by resetting neurons in last layers and fine-tuning to remove memorized sensitive information, reducing regurgitation of confidential data.


<details>
  <summary>Details</summary>
Motivation: Fine-tuned LLMs memorize sensitive data from specialized corpora (medical reports, business data), posing privacy risks when this information is regurgitated during use.

Method: Uses erasure phase (resetting neurons in last layers to disrupt memorization) and repair phase (fine-tuning while avoiding sensitive information memorization).

Result: With few additional unlearning epochs, models are sanitized and regurgitation of sensitive information is drastically reduced.

Conclusion: SANI provides efficient sanitization for organizations that have already invested in training models on sensitive datasets, enabling safe model sharing.

Abstract: Pre-trained large language models (LLMs) are becoming useful for various
tasks. To improve their performance on certain tasks, it is necessary to
fine-tune them on specific data corpora (e.g., medical reports, business data).
These specialized data corpora may contain sensitive data (e.g., personal or
confidential data) that will be memorized by the model and likely to be
regurgitated during its subsequent use. This memorization of sensitive
information by the model poses a significant privacy or confidentiality issue.
To remove this memorization and sanitize the model without requiring costly
additional fine-tuning on a secured data corpus, we propose SANI. SANI is an
unlearning approach to sanitize language models. It relies on both an erasure
and repair phases that 1) reset certain neurons in the last layers of the model
to disrupt the memorization of fine-grained information, and then 2) fine-tune
the model while avoiding memorizing sensitive information. We comprehensively
evaluate SANI to sanitize both a model fine-tuned and specialized with medical
data by removing directly and indirectly identifiers from the memorization of
the model, and a standard pre-trained model by removing specific terms defined
as confidential information from the model. Results show that with only few
additional epochs of unlearning, the model is sanitized and the number of
regurgitations is drastically reduced. This approach can be particularly useful
for hospitals or other industries that have already spent significant resources
training models on large datasets and wish to sanitize them before sharing.

</details>


### [140] [FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models](https://arxiv.org/abs/2510.21363)
*Zihao Fu,Ryan Brown,Shun Shao,Kai Rawal,Eoin Delaney,Chris Russell*

Main category: cs.LG

TL;DR: FairImagen is a post-hoc debiasing framework that mitigates societal biases in text-to-image diffusion models by projecting prompt embeddings into a fair subspace using Fair PCA, without requiring model retraining.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models like Stable Diffusion often replicate and amplify societal biases along demographic attributes such as gender and race, creating inequitable image generation.

Method: Uses Fair Principal Component Analysis to project CLIP-based input embeddings into a subspace that minimizes group-specific information while preserving semantic content, with empirical noise injection and unified cross-demographic projection for multiple attributes.

Result: Extensive experiments show FairImagen significantly improves fairness across gender, race, and intersectional settings with moderate trade-offs in image quality and prompt fidelity, outperforming existing post-hoc methods.

Conclusion: FairImagen provides a simple, scalable, and model-agnostic solution for equitable text-to-image generation through post-hoc debiasing of prompt embeddings.

Abstract: Text-to-image diffusion models, such as Stable Diffusion, have demonstrated
remarkable capabilities in generating high-quality and diverse images from
natural language prompts. However, recent studies reveal that these models
often replicate and amplify societal biases, particularly along demographic
attributes like gender and race. In this paper, we introduce FairImagen
(https://github.com/fuzihaofzh/FairImagen), a post-hoc debiasing framework that
operates on prompt embeddings to mitigate such biases without retraining or
modifying the underlying diffusion model. Our method integrates Fair Principal
Component Analysis to project CLIP-based input embeddings into a subspace that
minimizes group-specific information while preserving semantic content. We
further enhance debiasing effectiveness through empirical noise injection and
propose a unified cross-demographic projection method that enables simultaneous
debiasing across multiple demographic attributes. Extensive experiments across
gender, race, and intersectional settings demonstrate that FairImagen
significantly improves fairness with a moderate trade-off in image quality and
prompt fidelity. Our framework outperforms existing post-hoc methods and offers
a simple, scalable, and model-agnostic solution for equitable text-to-image
generation.

</details>


### [141] [Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations](https://arxiv.org/abs/2510.21631)
*Faisal Hamman,Pasan Dissanayake,Yanjun Fu,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: CoD introduces counterfactual explanations for few-shot task-aware knowledge distillation, enabling efficient model compression with minimal data by leveraging decision boundary information.


<details>
  <summary>Details</summary>
Motivation: Existing task-aware distillation methods require large amounts of data, which is often unavailable or expensive in practical scenarios. The paper addresses the challenge of few-shot distillation where data is scarce.

Method: CoD systematically infuses counterfactual explanations (CFEs) - inputs that minimally perturb teacher model predictions - to precisely map the teacher's decision boundary with significantly fewer samples. It provides theoretical guarantees from statistical and geometric perspectives.

Result: Experiments across various datasets and LLMs show CoD outperforms standard distillation approaches in few-shot regimes (8-512 samples). CoD uses only half the original samples paired with CFEs while improving performance.

Conclusion: Counterfactual explanations effectively enable few-shot knowledge distillation by providing informative examples near decision boundaries, allowing students to better mimic teacher models with minimal data requirements.

Abstract: Knowledge distillation is a promising approach to transfer capabilities from
complex teacher models to smaller, resource-efficient student models that can
be deployed easily, particularly in task-aware scenarios. However, existing
methods of task-aware distillation typically require substantial quantities of
data which may be unavailable or expensive to obtain in many practical
scenarios. In this paper, we address this challenge by introducing a novel
strategy called Counterfactual-explanation-infused Distillation CoD for
few-shot task-aware knowledge distillation by systematically infusing
counterfactual explanations. Counterfactual explanations (CFEs) refer to inputs
that can flip the output prediction of the teacher model with minimum
perturbation. Our strategy CoD leverages these CFEs to precisely map the
teacher's decision boundary with significantly fewer samples. We provide
theoretical guarantees for motivating the role of CFEs in distillation, from
both statistical and geometric perspectives. We mathematically show that CFEs
can improve parameter estimation by providing more informative examples near
the teacher's decision boundary. We also derive geometric insights on how CFEs
effectively act as knowledge probes, helping the students mimic the teacher's
decision boundaries more effectively than standard data. We perform experiments
across various datasets and LLMs to show that CoD outperforms standard
distillation approaches in few-shot regimes (as low as 8-512 samples). Notably,
CoD only uses half of the original samples used by the baselines, paired with
their corresponding CFEs and still improves performance.

</details>


### [142] [More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in Continual Learning](https://arxiv.org/abs/2510.21019)
*Wanhao Yu,Zheng Wang,Shuteng Niu,Sen Lin,Li Yang*

Main category: cs.LG

TL;DR: ZO optimization enhances stability in continual learning by creating flatter loss landscapes that reduce forgetting, but sacrifices plasticity for new task learning. The proposed ZO-FC method combines ZO optimization for adapters with FO optimization for classifiers to balance stability and plasticity.


<details>
  <summary>Details</summary>
Motivation: To address the plasticity-stability-efficiency trilemma in continual learning using zeroth-order optimization as a memory-efficient alternative to first-order methods, particularly for on-device applications.

Method: Theoretical analysis and empirical evaluation of ZO optimization in CL, followed by proposing ZO-FC - applying ZO optimization to adapter-based PEFT modules while using FO optimization for classifiers to maintain adaptability.

Result: ZO optimization naturally leads to flatter loss landscapes that reduce forgetting but impairs plasticity due to imprecise gradient estimates. ZO-FC achieves effective balance between stability and plasticity with negligible memory overhead.

Conclusion: ZO-FC offers a practical and memory-efficient solution for on-device continual learning by leveraging ZO optimization's stability benefits while preserving FO optimization's adaptability through selective application.

Abstract: Zeroth-order (ZO) optimization has gained attention as a memory-efficient
alternative to first-order (FO) methods, particularly in settings where
gradient computation is expensive or even impractical. Beyond its memory
efficiency, in this work, we investigate ZO optimization for continual learning
(CL) as a novel approach to address the plasticity-stability-efficiency
trilemma. Through theoretical analysis and empirical evidence, we show that ZO
optimization naturally leads to flatter loss landscapes, which in turn reduce
forgetting in CL. However, this stability comes at a cost of plasticity: due to
its imprecise gradient estimates and slower convergence, ZO optimization tends
to be less effective than FO in acquiring new task-specific knowledge,
particularly under constrained training budgets. To better understand this
trade-off, we conduct a holistic evaluation of ZO optimization applied to
various existing CL methods. Our findings reveal that ZO optimization enhances
stability but often undermines plasticity, particularly when used with
learnable classifiers. Motivated by this insight, we propose ZO-FC, a simple
but effective approach that applies ZO optimization to a single adapter-based
PEFT module with FO optimized classifier. This design leverages the stability
benefits of ZO while preserving the adaptability of FO updates with negligible
memory overhead. Experiments demonstrate that ZO-FC achieves an effective
balance between stability and plasticity, offering a practical and
memory-efficient solution for on-device CL.

</details>


### [143] [Buffer layers for Test-Time Adaptation](https://arxiv.org/abs/2510.21271)
*Hyeongyu Kim,Geonhui Han,Dosik Hwang*

Main category: cs.LG

TL;DR: A novel Buffer layer approach for Test Time Adaptation that overcomes limitations of normalization-based methods by preserving pre-trained backbone integrity while improving domain shift robustness.


<details>
  <summary>Details</summary>
Motivation: Normalization-based TTA methods face challenges with small batch sizes, unstable statistics, and limited generalization to unseen domains due to reliance on training-time statistics.

Method: Introduces a Buffer layer paradigm that doesn't modify core model parameters, preserving pre-trained backbone integrity and preventing catastrophic forgetting during online adaptation.

Result: Outperforms traditional TTA methods in mitigating domain shift and enhancing model robustness, with strong resilience to forgetting and consistent performance improvements across architectures.

Conclusion: The Buffer layer provides an effective and versatile solution for real-world domain adaptation scenarios, being modular and easily integrable into existing TTA frameworks.

Abstract: In recent advancements in Test Time Adaptation (TTA), most existing
methodologies focus on updating normalization layers to adapt to the test
domain. However, the reliance on normalization-based adaptation presents key
challenges. First, normalization layers such as Batch Normalization (BN) are
highly sensitive to small batch sizes, leading to unstable and inaccurate
statistics. Moreover, normalization-based adaptation is inherently constrained
by the structure of the pre-trained model, as it relies on training-time
statistics that may not generalize well to unseen domains. These issues limit
the effectiveness of normalization-based TTA approaches, especially under
significant domain shift. In this paper, we introduce a novel paradigm based on
the concept of a Buffer layer, which addresses the fundamental limitations of
normalization layer updates. Unlike existing methods that modify the core
parameters of the model, our approach preserves the integrity of the
pre-trained backbone, inherently mitigating the risk of catastrophic forgetting
during online adaptation. Through comprehensive experimentation, we demonstrate
that our approach not only outperforms traditional methods in mitigating domain
shift and enhancing model robustness, but also exhibits strong resilience to
forgetting. Furthermore, our Buffer layer is modular and can be seamlessly
integrated into nearly all existing TTA frameworks, resulting in consistent
performance improvements across various architectures. These findings validate
the effectiveness and versatility of the proposed solution in real-world domain
adaptation scenarios. The code is available at
https://github.com/hyeongyu-kim/Buffer_TTA.

</details>


### [144] [Disentangled Representation Learning via Modular Compositional Bias](https://arxiv.org/abs/2510.21402)
*Whie Jung,Dong Hoon Lee,Seunghoon Hong*

Main category: cs.LG

TL;DR: A framework for disentangled representation learning using compositional bias and mixing strategies to handle different factor types without redesigning objectives or architectures.


<details>
  <summary>Details</summary>
Motivation: Current DRL methods require factor-specific strategies that create overhead when dealing with novel factors or multiple coexisting factors, necessitating a more flexible approach.

Method: Uses compositional bias with factor-specific mixing strategies and two objectives: prior loss for realistic remixes and compositional consistency loss for latent-image alignment.

Result: Competitive performance in attribute and object disentanglement, and uniquely achieves joint disentanglement of global style and objects.

Conclusion: The proposed framework enables flexible disentanglement of different factor types by simply adjusting mixing strategies, without modifying objectives or architectures.

Abstract: Recent disentangled representation learning (DRL) methods heavily rely on
factor specific strategies-either learning objectives for attributes or model
architectures for objects-to embed inductive biases. Such divergent approaches
result in significant overhead when novel factors of variation do not align
with prior assumptions, such as statistical independence or spatial
exclusivity, or when multiple factors coexist, as practitioners must redesign
architectures or objectives. To address this, we propose a compositional bias,
a modular inductive bias decoupled from both objectives and architectures. Our
key insight is that different factors obey distinct recombination rules in the
data distribution: global attributes are mutually exclusive, e.g., a face has
one nose, while objects share a common support (any subset of objects can
co-exist). We therefore randomly remix latents according to factor-specific
rules, i.e., a mixing strategy, and force the encoder to discover whichever
factor structure the mixing strategy reflects through two complementary
objectives: (i) a prior loss that ensures every remix decodes into a realistic
image, and (ii) the compositional consistency loss introduced by Wiedemer et
al. (arXiv:2310.05327), which aligns each composite image with its
corresponding composite latent. Under this general framework, simply adjusting
the mixing strategy enables disentanglement of attributes, objects, and even
both, without modifying the objectives or architectures. Extensive experiments
demonstrate that our method shows competitive performance in both attribute and
object disentanglement, and uniquely achieves joint disentanglement of global
style and objects. Code is available at
https://github.com/whieya/Compositional-DRL.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [145] [This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN](https://arxiv.org/abs/2510.20846)
*Dennis Tang,Jon Donnelly,Alina Jade Barnett,Lesia Semenova,Jin Jing,Peter Hadar,Ioannis Karakis,Olga Selioutski,Kehan Zhao,M. Brandon Westover,Cynthia Rudin*

Main category: q-bio.NC

TL;DR: ProtoEEG-kNN is an interpretable machine learning model for detecting interictal epileptiform discharges (IEDs) in EEG recordings that provides visual explanations based on case-based reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning models for IED detection are uninterpretable and cannot justify their conclusions, preventing doctors from leveraging their expertise to identify incorrect predictions and intervene.

Method: ProtoEEG-kNN uses case-based reasoning by comparing EEGs to similar training examples and provides visual explanations showing both IED morphology (shape) and spatial distribution (location).

Result: ProtoEEG-kNN achieves state-of-the-art accuracy in IED detection while providing explanations that experts prefer over existing approaches.

Conclusion: The model improves human-model interaction by offering interpretable reasoning that allows doctors to understand and verify model predictions.

Abstract: The presence of interictal epileptiform discharges (IEDs) in
electroencephalogram (EEG) recordings is a critical biomarker of epilepsy. Even
trained neurologists find detecting IEDs difficult, leading many practitioners
to turn to machine learning for help. While existing machine learning
algorithms can achieve strong accuracy on this task, most models are
uninterpretable and cannot justify their conclusions. Absent the ability to
understand model reasoning, doctors cannot leverage their expertise to identify
incorrect model predictions and intervene accordingly. To improve the
human-model interaction, we introduce ProtoEEG-kNN, an inherently interpretable
model that follows a simple case-based reasoning process. ProtoEEG-kNN reasons
by comparing an EEG to similar EEGs from the training set and visually
demonstrates its reasoning both in terms of IED morphology (shape) and spatial
distribution (location). We show that ProtoEEG-kNN can achieve state-of-the-art
accuracy in IED detection while providing explanations that experts prefer over
existing approaches.

</details>

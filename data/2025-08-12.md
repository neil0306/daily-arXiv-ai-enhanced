<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 96]
- [cs.CV](#cs.CV) [Total: 220]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 19]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.LG](#cs.LG) [Total: 10]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: This paper addresses the lack of Portuguese datasets for Semi-Automated Fact-Checking (SAFC) by enriching news corpora with external evidence using LLMs and search APIs.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of disinformation exceeds manual fact-checking capabilities, necessitating SAFC systems. Portuguese datasets lack external evidence integration.

Method: Uses LLMs (Gemini 1.5 Flash) to extract claims and search APIs (Google) to retrieve evidence, with data validation and preprocessing.

Result: Developed a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR, MuMiN-PT) with external evidence.

Conclusion: The approach fills a gap in Portuguese SAFC resources by integrating external evidence and improving dataset quality.

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [2] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: The paper explores dynamic prompting with retrieval-augmented generation (RAG) to improve few-shot biomedical NER performance, showing significant F1-score gains over static methods.


<details>
  <summary>Details</summary>
Motivation: Address performance challenges of LLMs in few-shot biomedical NER by leveraging dynamic prompting and RAG.

Method: Investigate dynamic prompting with RAG, selecting in-context examples based on input text similarity and updating prompts during inference. Compare static and dynamic techniques.

Result: Dynamic prompting outperforms static methods, with TF-IDF and SBERT retrieval improving F1-scores by 7.3% (5-shot) and 5.6% (10-shot). Static prompting also showed gains (11-12%).

Conclusion: Contextually adaptive prompts via RAG enhance biomedical NER, demonstrating the value of dynamic prompting in few-shot settings.

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [3] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: CarbonScaling extends neural scaling laws to include carbon emissions in LLM training, revealing inefficiencies and hardware limitations.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked carbon emissions in neural scaling laws for LLMs and provide a framework for sustainable training.

Method: Integrates neural scaling, GPU evolution, parallelism optimization, and carbon estimation into CarbonScaling.

Result: Power-law relationship between accuracy and carbon exists, but inefficiencies increase scaling. Hardware helps small models but not large ones due to overhead.

Conclusion: CarbonScaling aids in training more sustainable LLMs by highlighting inefficiencies and optimization opportunities.

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [4] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: The paper addresses tokenization inefficiencies in multilingual LLMs, proposing a novel algorithm for data composition and pretokenization strategies to improve token-to-word ratios and model performance, with significant gains in Indic scripts.


<details>
  <summary>Details</summary>
Motivation: Tokenization in multilingual LLMs is understudied, leading to inefficiencies like high token-to-word ratios and slower inference. The study aims to improve this by analyzing vocabulary size, pretokenization, and corpus composition.

Method: The study conducts experiments on Indic scripts, proposes a data composition algorithm, and evaluates pretokenization strategies to optimize tokenization efficiency.

Result: The proposed methods reduce token-to-word ratio by ~6% and achieve >40% improvement over state-of-the-art models, enhancing performance and inference speed.

Conclusion: Tokenization is a critical factor for efficient multilingual LLMs, alongside architecture and training objectives, with demonstrated improvements in Indic contexts.

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [5] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: AEALT is a supervised framework that reduces the dimensionality of LLM embeddings using an augmented autoencoder, improving efficiency and performance in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: High-dimensional LLM embeddings are inefficient and costly for downstream tasks, necessitating a method to reduce dimensionality while preserving task-relevant information.

Method: AEALT extracts embeddings from text, processes them through a supervised augmented autoencoder to learn low-dimensional latent factors, capturing nonlinear structures.

Result: AEALT outperforms raw embeddings and standard dimension reduction methods in classification, anomaly detection, and prediction tasks.

Conclusion: AEALT effectively reduces embedding dimensionality and enhances performance, demonstrating broad applicability in various NLP tasks.

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [6] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: The paper explores whether large language models (LLMs) can adaptively guide learners like expert tutors, proposing GuideEval as a benchmark and a behavior-guided finetuning strategy to improve LLM performance.


<details>
  <summary>Details</summary>
Motivation: Prior research on LLMs' tutoring capabilities overlooks adaptive guidance based on learners' cognitive states, prompting a focus on dynamic instructional strategies.

Method: The study introduces GuideEval, a benchmark evaluating pedagogical guidance through three phases: Perception, Orchestration, and Elicitation, and employs behavior-guided finetuning.

Result: Existing LLMs often fail in adaptive scaffolding for confused learners, but behavior-guided finetuning significantly enhances guidance performance.

Conclusion: The work advocates a dialogic paradigm for evaluating Socratic LLMs, shifting from content evaluation to learner-centered interaction.

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [7] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: The paper introduces an automated method for generating high-quality forget sets for post-hoc unlearning in large language models, using synthetic textbook-style data created by the models themselves.


<details>
  <summary>Details</summary>
Motivation: Large language models often encode sensitive or harmful knowledge, necessitating efficient unlearning methods without full retraining. Current approaches struggle with creating effective forget sets.

Method: A scalable, automated approach synthesizes textbook-style data via structured prompting, requiring only a domain name as input.

Result: The synthetic datasets outperform baseline alternatives and match expert-curated ones in unlearning biosecurity, cybersecurity, and Harry Potter domains. Data diversity, enhanced by a multi-step pipeline, improves unlearning utility.

Conclusion: Synthetic datasets enable practical, scalable unlearning for emerging domains without manual intervention, offering a promising solution.

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [8] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: The paper introduces BrowseComp-Plus, a benchmark for evaluating deep-research agents, addressing limitations of current benchmarks like fairness and transparency by using a fixed corpus with verified documents.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for deep-research agents rely on dynamic web APIs, hindering fair comparisons and reproducibility. The lack of control over the corpus also limits insights into the underlying LLMs' capabilities.

Method: The authors derive BrowseComp-Plus from BrowseComp, using a fixed, curated corpus with human-verified documents and challenging negatives for controlled experiments.

Result: BrowseComp-Plus effectively distinguishes system performance, e.g., GPT-5 achieves 55.9% accuracy, improving to 70.1% with Qwen3-Embedding-8B.

Conclusion: BrowseComp-Plus enables comprehensive evaluation of deep-research agents, providing insights into retrieval effectiveness, citation accuracy, and context engineering.

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [9] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
*Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: The paper explores the impact of BPE inference algorithms that don't rely on merge lists, finding minimal performance degradation in non-targeted methods, suggesting simpler, privacy-preserving tokenization.


<details>
  <summary>Details</summary>
Motivation: Recent work shows BPE merge lists expose vulnerabilities for extracting training data, prompting investigation into merge-list-free inference algorithms.

Method: Two classes of BPE inference schemes: targeted deviations (random merges, corruptions) and non-targeted methods (greedy/exact compression without merge lists).

Result: Targeted deviations degrade performance, but non-targeted methods have minimal impact on tasks like QA, translation, and generation.

Conclusion: Non-targeted merge-list-free BPE inference offers simpler, privacy-preserving tokenization without significant performance loss.

Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a
learned token vocabulary with a detailed merge list. Recent work has shown that
this merge list exposes a potential attack surface for extracting information
about language model's training data. In this paper, we explore the downstream
impact of BPE inference algorithms that do not rely on this merge list at all,
and hence differ from the encoding process during BPE training. To address this
question, we investigate two broad classes of BPE inference schemes that differ
from BPE application during training: a) targeted deviation from merge-lists
including random merge orders, and various corruptions of merge list involving
deletion/truncation, and b) non-targeted BPE inference algorithms that do not
depend on the merge list but focus on compressing the text either greedily or
exactly. Extensive experiments across diverse language modeling tasks like
accuracy-based QA benchmarks, machine translation, and open-ended generation
reveal that while targeted deviation from the merge lists exhibits significant
degradation in language model performance, the non-targeted merge-list-free
inference algorithms result in minimal impact on downstream performance that is
often much smaller than expected. These findings pave way for simpler and
potentially more privacy-preserving tokenization schemes that do not
catastrophically compromise model performance.

</details>


### [10] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
*Daniel Wang,Eli Brignac,Minjia Mao,Xiao Fang*

Main category: cs.CL

TL;DR: The study investigates stereotype and deviation biases in LLMs, revealing significant biases in demographic associations and disparities with real-world distributions.


<details>
  <summary>Details</summary>
Motivation: Concerns about limitations and risks of LLMs prompted an investigation into their biases, specifically stereotype and deviation biases.

Method: Four advanced LLMs were asked to generate profiles of individuals to examine associations between demographic groups and attributes like political affiliation, religion, and sexual orientation.

Result: All examined LLMs exhibited significant stereotype and deviation biases towards multiple groups.

Conclusion: The findings highlight biases in LLM-generated outputs and their potential harms, emphasizing the need for mitigation strategies.

Abstract: Large language models (LLMs) are widely applied across diverse domains,
raising concerns about their limitations and potential risks. In this study, we
investigate two types of bias that LLMs may display: stereotype bias and
deviation bias. Stereotype bias refers to when LLMs consistently associate
specific traits with a particular demographic group. Deviation bias reflects
the disparity between the demographic distributions extracted from
LLM-generated content and real-world demographic distributions. By asking four
advanced LLMs to generate profiles of individuals, we examine the associations
between each demographic group and attributes such as political affiliation,
religion, and sexual orientation. Our experimental results show that all
examined LLMs exhibit both significant stereotype bias and deviation bias
towards multiple groups. Our findings uncover the biases that occur when LLMs
infer user attributes and shed light on the potential harms of LLM-generated
outputs.

</details>


### [11] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
*Jonathan Shaw,Dillon Mee,Timothy Khouw,Zackary Leech,Daniel Wilson*

Main category: cs.CL

TL;DR: LLMs improve translation for low-resource languages like Kanuri using parallel sentences, but grammar alone is insufficient. Human evaluations highlight accuracy over fluency.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs can translate Kanuri, a language with minimal digital resources, using various language materials.

Method: Two datasets (health/humanitarian and general terms) were created. LLMs were tested with combinations of grammar, dictionary, and parallel sentences, compared to native and linguist translations.

Result: Parallel sentences were most effective. Grammar improved zero-shot translation but wasn't sufficient alone. LLMs excelled in accuracy over fluency.

Conclusion: LLM translation evaluation needs multidimensional metrics. Grammar alone lacks context for effective domain-specific translation.

Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context
learning to translate into previously unseen language contexts. Tanzer et al.
[2024] utilize language materials (e.g. a grammar) to improve translation
quality for Kalamang using large language models (LLMs). We focus on Kanuri, a
language that, despite having substantial speaker population, has minimal
digital resources. We design two datasets for evaluation: one focused on health
and humanitarian terms, and another containing generalized terminology,
investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar,
dictionary, and parallel sentences), we measure LLM translation effectiveness,
comparing results to native speaker translations and human linguist
performance. We evaluate using both automatic metrics and native speaker
assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data
source, outperforming other methods in human evaluations and automatic metrics.
While incorporating grammar improves over zero-shot translation, it fails as an
effective standalone data source. Human evaluations reveal that LLMs achieve
accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from
multidimensional assessment beyond simple accuracy metrics, and that grammar
alone, without parallel sentences, does not provide sufficient context for
effective domain-specific translation.

</details>


### [12] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: The paper investigates whether biased language models exhibit biased reasoning steps (chain-of-thought) and finds low correlation between biased thoughts and biased outputs.


<details>
  <summary>Details</summary>
Motivation: To understand if biased language models also display biased reasoning processes, addressing fairness concerns in deployment.

Method: Experiments on 5 large language models using fairness metrics to quantify 11 biases in thoughts and outputs.

Result: Low correlation (<0.6) between biased thoughts and biased outputs, with p-values <0.001 in most cases.

Conclusion: Biased models do not necessarily have biased reasoning steps, unlike humans.

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [13] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: The paper introduces a statistical framework to identify and quantify self-bias in LLMs when they evaluate their own outputs, ensuring genuine performance differences are not conflated with bias.


<details>
  <summary>Details</summary>
Motivation: To address the issue of LLMs systematically favoring their own outputs (self-bias) and to distinguish it from genuine performance differences, which can distort evaluations.

Method: A statistical framework models the scoring distribution differences between LLM judges' evaluations of their own outputs versus others, accounting for underlying quality via third-party judges (e.g., humans).

Result: Empirical analysis on a large dataset reveals self-bias in models like GPT-4o and Claude 3.5 Sonnet, which also exhibit family-bias (favoring outputs from models of the same family).

Conclusion: The study highlights pitfalls of using LLM judges and provides guidance to mitigate biases in automated evaluations.

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [14] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: A scalable framework using LLMs automates semantic and sentiment annotation for Japanese American Incarceration Oral History, achieving high accuracy with ChatGPT, Llama, and Qwen.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of large-scale analysis of unstructured oral history archives, particularly due to emotional complexity and high annotation costs.

Method: A multiphase approach combining expert annotation, prompt design, and LLM evaluation (ChatGPT, Llama, Qwen) for semantic and sentiment classification.

Result: ChatGPT led in semantic classification (88.71% F1), while Llama slightly outperformed in sentiment analysis (82.66%). The framework annotated 92,191 sentences from 1,002 interviews.

Conclusion: LLMs, guided by well-designed prompts, can effectively annotate large oral history collections, offering a reusable pipeline for culturally sensitive archival analysis.

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [15] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: The paper introduces multi-turn jailbreaking in LLMs, a more serious threat than single-turn jailbreaking, and proposes a benchmark (MTJ-Bench) to evaluate this vulnerability.


<details>
  <summary>Details</summary>
Motivation: Current jailbreaking methods focus on single-turn attacks, but LLMs handle multi-turn conversations, posing a greater risk if exploited.

Method: The authors construct MTJ-Bench to test multi-turn jailbreaking on various LLMs, analyzing its impact.

Result: The study reveals a new vulnerability in LLMs, highlighting the need for improved safety measures.

Conclusion: The work calls for community efforts to enhance LLM safety and deepen understanding of jailbreaking threats.

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [16] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
*Ziqi Liu,Yangbin Chen,Ziyang Zhou,Yilin Li,Mingxuan Hu,Yushan Pan,Zhijie Xu*

Main category: cs.CL

TL;DR: SEVADE is a self-evolving multi-agent framework for sarcasm detection, using dynamic reasoning and decoupled evaluation to reduce hallucination and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for sarcasm detection are limited by single-perspective analysis and hallucination, reducing accuracy and reliability.

Method: SEVADE employs a Dynamic Agentive Reasoning Engine (DARE) with specialized agents for multifaceted text analysis, followed by a rationale adjudicator (RA) for final classification.

Result: The framework achieves state-of-the-art performance, with 6.75% higher Accuracy and 6.29% better Macro-F1 score on benchmark datasets.

Conclusion: SEVADE effectively addresses limitations in sarcasm detection by combining dynamic reasoning and decoupled evaluation, outperforming existing methods.

Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing
task. Existing Large Language Model methods are often limited by
single-perspective analysis, static reasoning pathways, and a susceptibility to
hallucination when processing complex ironic rhetoric, which impacts their
accuracy and reliability. To address these challenges, we propose **SEVADE**, a
novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with
**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The
core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which
utilizes a team of specialized agents grounded in linguistic theory to perform
a multifaceted deconstruction of the text and generate a structured reasoning
chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs
the final classification based solely on this reasoning chain. This decoupled
architecture is designed to mitigate the risk of hallucination by separating
complex reasoning from the final judgment. Extensive experiments on four
benchmark datasets demonstrate that our framework achieves state-of-the-art
performance, with average improvements of **6.75%** in Accuracy and **6.29%**
in Macro-F1 score.

</details>


### [17] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
*Steven Coyne,Diana Galvan-Sosa,Ryan Spring,Camélia Guerraoui,Michael Zock,Keisuke Sakaguchi,Kentaro Inui*

Main category: cs.CL

TL;DR: The paper introduces an annotation framework for generating feedback in AWE systems, focusing on error types and generalizability to aid language learning. It evaluates methods for feedback generation using LLMs.


<details>
  <summary>Details</summary>
Motivation: Current AWE systems lack optimal design for language learning, favoring direct revisions over explanatory feedback. The study aims to improve feedback quality for learners.

Method: An annotation framework models error types and generalizability. A dataset of annotated learner errors and feedback is collected. Feedback generation methods (keyword-guided, keyword-free, template-guided) using LLMs are evaluated.

Result: Human teachers assessed feedback outputs for relevance, factuality, and comprehensibility. Comparative performance of the systems is reported.

Conclusion: The framework and dataset support better feedback generation for language learning, with evaluated methods showing potential for improving AWE systems.

Abstract: Recent advances in natural language processing (NLP) have contributed to the
development of automated writing evaluation (AWE) systems that can correct
grammatical errors. However, while these systems are effective at improving
text, they are not optimally designed for language learning. They favor direct
revisions, often with a click-to-fix functionality that can be applied without
considering the reason for the correction. Meanwhile, depending on the error
type, learners may benefit most from simple explanations and strategically
indirect hints, especially on generalizable grammatical rules. To support the
generation of such feedback, we introduce an annotation framework that models
each error's error type and generalizability. For error type classification, we
introduce a typology focused on inferring learners' knowledge gaps by
connecting their errors to specific grammatical patterns. Following this
framework, we collect a dataset of annotated learner errors and corresponding
human-written feedback comments, each labeled as a direct correction or hint.
With this data, we evaluate keyword-guided, keyword-free, and template-guided
methods of generating feedback using large language models (LLMs). Human
teachers examined each system's outputs, assessing them on grounds including
relevance, factuality, and comprehensibility. We report on the development of
the dataset and the comparative performance of the systems investigated.

</details>


### [18] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: A neural TTS system for Manipuri using Tacotron 2 and HiFi-GAN, supporting tonal phonology and under-resourced settings, validated for intelligibility and naturalness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of TTS systems for the Manipuri language and support linguistic preservation and technological inclusion.

Method: Developed a phoneme mapping (Meitei Mayek to ARPAbet), curated a single-speaker dataset, and adapted Tacotron 2 and HiFi-GAN for tonal phonology.

Result: Achieved intelligible and natural speech synthesis, validated through subjective and objective metrics.

Conclusion: The system provides a foundation for preserving Manipuri and enhancing its technological accessibility.

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and
HiFi-GAN, we introduce a neural TTS architecture adapted to support tonal
phonology and under-resourced linguistic environments. We develop a phoneme
mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
demonstrate intelligible and natural speech synthesis, validated through
subjective and objective metrics. This system lays the groundwork for
linguistic preservation and technological inclusion of Manipuri.

</details>


### [19] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: Proposes an automatic label alignment method for merging NER datasets, combining empirical and semantic similarities to unify label spaces, improving performance in low-resource domains.


<details>
  <summary>Details</summary>
Motivation: NER relies on large annotated datasets, which are costly to create. Current merging methods lack interpretability and scalability.

Method: Uses label similarity (empirical and semantic) with greedy pairwise merging to unify label spaces across datasets.

Result: Successfully merges datasets with minimal performance impact and enhances NER in a low-resource financial domain.

Conclusion: Provides an efficient, interpretable, and scalable solution for integrating multi-source NER corpora.

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [20] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: ReQAP is a system that answers complex questions by decomposing them and building an operator tree, using light-weight language models for interpretation and execution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of answering complex questions involving heterogeneous data sources while ensuring user trust and comprehensibility.

Method: Recursively decomposes questions, builds an operator tree, and uses fine-tuned light-weight language models for interpretation and execution.

Result: Demonstrates rich functionality for advanced questions and provides traceability of answers to underlying sources.

Conclusion: ReQAP effectively handles complex queries with transparency, enhancing user trust and understanding.

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [21] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
*Arpita Saggar,Jonathan C. Darling,Vania Dimitrova,Duygu Sarikaya,David C. Hogg*

Main category: cs.CL

TL;DR: The paper introduces SBS (Score-Before-Speaking), a framework for persona-based dialogue generation that unifies response learning and quality scoring, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Improving persona fidelity in dialogues is challenging due to limited data diversity. SBS aims to address this by integrating quality scoring during training.

Method: SBS trains models to correlate augmented responses with quality scores using noun-based substitution and semantic similarity. It unifies learning responses and their quality.

Result: SBS outperforms previous methods, improving persona-consistent dialogues in benchmark datasets (PERSONA-CHAT, ConvAI2).

Conclusion: Score-conditioned training enhances persona fidelity, and including scores in prompts is superior to conventional training.

Abstract: Persona-based dialogue generation is an important milestone towards building
conversational artificial intelligence. Despite the ever-improving capabilities
of large language models (LLMs), effectively integrating persona fidelity in
conversations remains challenging due to the limited diversity in existing
dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which
outperforms previous methods and yields improvements for both million and
billion-parameter models. Unlike previous methods, SBS unifies the learning of
responses and their relative quality into a single step. The key innovation is
to train a dialogue model to correlate augmented responses with a quality score
during training and then leverage this knowledge at inference. We use
noun-based substitution for augmentation and semantic similarity-based scores
as a proxy for response quality. Through extensive experiments with benchmark
datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training
allows existing models to better capture a spectrum of persona-consistent
dialogues. Our ablation studies also demonstrate that including scores in the
input prompt during training is superior to conventional training setups. Code
and further details are available at
https://arpita2512.github.io/score_before_you_speak

</details>


### [22] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: SentiDetect is a model-agnostic framework for detecting LLM-generated text by analyzing sentiment distribution stability, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods for LLM-generated text lack generalizability and are vulnerable to adversarial attacks. SentiDetect addresses this by leveraging emotional consistency in LLM outputs.

Method: SentiDetect uses two metrics—sentiment distribution consistency and preservation—to quantify stability under transformations, evaluated on diverse datasets and advanced LLMs.

Result: SentiDetect achieves significant F1 score improvements (16% for Gemini-1.5-Pro, 11% for GPT-4-0613) and robustness against paraphrasing and adversarial attacks.

Conclusion: SentiDetect offers a superior, robust solution for detecting LLM-generated text, addressing limitations of current methods.

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [23] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: A two-stage framework for Quranic QA combines model ensembling for retrieval and instruction-tuned LLMs for extraction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of Quranic QA due to linguistic complexity and semantic richness in low-resource settings.

Method: Ensemble fine-tuned Arabic models for retrieval and instruction-tuned LLMs with few-shot prompting for extraction.

Result: Achieved MAP@10 of 0.3128, MRR@10 of 0.5763 for retrieval, and pAP@10 of 0.669 for extraction.

Conclusion: Combining ensembling and instruction-tuned LLMs effectively tackles low-resource QA in specialized domains.

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [24] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
*Zhijun Tu,Hanting Chen,Siqi Liu,Chuanjian Liu,Jian Li,Jie Hu,Yunhe Wang*

Main category: cs.CL

TL;DR: The paper introduces a method for 1-bit LLM quantization that leverages pre-trained models, reducing training costs and improving accuracy through progressive training and dual-scaling compensation.


<details>
  <summary>Details</summary>
Motivation: Existing 1-bit LLM quantization methods train from scratch, leading to high costs and accuracy loss. The gap between full precision and 1-bit representations makes adaptation challenging.

Method: The proposed method uses consistent progressive training for forward and backward passes, binary-aware initialization, and dual-scaling compensation to smoothly convert floating-point weights to binarized ones.

Result: Experiments show the method outperforms existing approaches, achieving high-performance 1-bit LLMs without expensive training from scratch.

Conclusion: The method successfully leverages pre-trained models for 1-bit quantization, reducing costs and maintaining accuracy.

Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and
computational costs. However, existing methods typically train 1-bit LLMs from
scratch, failing to fully leverage pre-trained models. This results in high
training costs and notable accuracy degradation. We identify that the large gap
between full precision and 1-bit representations makes direct adaptation
difficult. In this paper, we introduce a consistent progressive training for
both forward and backward, smoothly converting the floating-point weights into
the binarized ones. Additionally, we incorporate binary-aware initialization
and dual-scaling compensation to reduce the difficulty of progressive training
and improve the performance. Experimental results on LLMs of various sizes
demonstrate that our method outperforms existing approaches. Our results show
that high-performance 1-bit LLMs can be achieved using pre-trained models,
eliminating the need for expensive training from scratch.

</details>


### [25] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
*Mao Li,Fred Conrad,Johann Gagnon-Bartsch*

Main category: cs.CL

TL;DR: Vec2Summ is a novel abstractive summarization method using semantic compression via mean vector representation and embedding inversion, with stochastic sampling for varied outputs. It addresses LLM limitations, offering scalability and semantic control.


<details>
  <summary>Details</summary>
Motivation: To overcome context-length constraints and lack of interpretability in LLM-based summarization, while enabling scalable and controllable semantic abstraction.

Method: Represents documents as a mean vector in semantic space, decodes it into summaries via embedding inversion, and introduces stochasticity via Gaussian sampling.

Result: Produces coherent summaries with thematic coverage comparable to LLMs, though with less detail, excelling in scalability and semantic control.

Conclusion: Vec2Summ is effective for scalable, semantically controlled summarization, prioritizing corpus-level abstraction over fine-grained detail.

Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames
the task as semantic compression. Vec2Summ represents a document collection
using a single mean vector in the semantic embedding space, capturing the
central meaning of the corpus. To reconstruct fluent summaries, we perform
embedding inversion -- decoding this mean vector into natural language using a
generative language model. To improve reconstruction quality and capture some
degree of topical variability, we introduce stochasticity by sampling from a
Gaussian distribution centered on the mean. This approach is loosely analogous
to bagging in ensemble learning, where controlled randomness encourages more
robust and varied outputs. Vec2Summ addresses key limitations of LLM-based
summarization methods. It avoids context-length constraints, enables
interpretable and controllable generation via semantic parameters, and scales
efficiently with corpus size -- requiring only $O(d + d^2)$ parameters.
Empirical results show that Vec2Summ produces coherent summaries for topically
focused, order-invariant corpora, with performance comparable to direct LLM
summarization in terms of thematic coverage and efficiency, albeit with less
fine-grained detail. These results underscore Vec2Summ's potential in settings
where scalability, semantic control, and corpus-level abstraction are
prioritized.

</details>


### [26] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: SEADialogues is a culturally grounded dialogue dataset for Southeast Asia, addressing gaps in existing chit-chat datasets by including eight languages, persona attributes, and culturally relevant topics.


<details>
  <summary>Details</summary>
Motivation: Existing dialogue datasets lack cultural nuances, especially for diverse regions like Southeast Asia. SEADialogues aims to fill this gap.

Method: The dataset includes dialogues in eight languages from six Southeast Asian countries, with persona attributes and culturally grounded topics.

Result: SEADialogues provides a rich resource for culturally aware dialogue systems, supporting research on human-centric language models.

Conclusion: SEADialogues advances the development of culturally relevant and personalized dialogue systems, particularly for low-resource languages in Southeast Asia.

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [27] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
*Aditya Tomar,Nihar Ranjan Sahoo,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper introduces BharatBBQ, a culturally adapted benchmark for evaluating social biases in Indian languages, addressing gaps in existing Western-focused benchmarks like BBQ.


<details>
  <summary>Details</summary>
Motivation: Existing bias benchmarks like BBQ are limited to Western contexts, failing to address biases in Indian languages and sociocultural settings.

Method: BharatBBQ is developed to assess biases in 8 Indian languages, covering 13 social categories. The dataset includes 49,108 examples expanded to 392,864 via translation and verification. Five multilingual LM families are evaluated in zero and few-shot settings.

Result: Persistent biases are found across languages and social categories, with amplified biases in Indian languages compared to English.

Conclusion: The study underscores the need for linguistically and culturally grounded benchmarks like BharatBBQ for comprehensive bias evaluation.

Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring
fairness and minimizing the reinforcement of harmful stereotypes in AI systems.
Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ),
primarily focus on Western contexts, limiting their applicability to the Indian
context. To address this gap, we introduce BharatBBQ, a culturally adapted
benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,
Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3
intersectional groups, reflecting prevalent biases in the Indian sociocultural
landscape. Our dataset contains 49,108 examples in one language that are
expanded using translation and verification to 392,864 examples in eight
different languages. We evaluate five multilingual LM families across zero and
few-shot settings, analyzing their bias and stereotypical bias scores. Our
findings highlight persistent biases across languages and social categories and
often amplified biases in Indian languages compared to English, demonstrating
the necessity of linguistically and culturally grounded benchmarks for bias
evaluation.

</details>


### [28] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: LessIsMore is a training-free sparse attention mechanism that improves efficiency and accuracy in reasoning tasks by aggregating token selections globally, achieving faster decoding and fewer tokens attended.


<details>
  <summary>Details</summary>
Motivation: Existing sparse attention methods suffer from accuracy degradation or require costly retraining, prompting the need for a more efficient and accurate solution.

Method: LessIsMore leverages global attention patterns and aggregates token selections from local heads with recent context, enabling unified cross-head token ranking.

Result: LessIsMore achieves a 1.1× decoding speed-up, attends to 2× fewer tokens without accuracy loss, and outperforms existing sparse attention methods with a 1.13× end-to-end speed-up.

Conclusion: LessIsMore offers a scalable, efficient, and accurate alternative to traditional sparse attention mechanisms for reasoning tasks.

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [29] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: The paper examines intersectional bias in LLMs, extending single-axis fairness evaluations to multiple demographic intersections. It introduces WinoIdentity, a benchmark with 245,700 prompts, and finds significant confidence disparities (up to 40%) across attributes, revealing memorization over reasoning in LLMs.


<details>
  <summary>Details</summary>
Motivation: To address concerns about AI systems reflecting societal biases, especially in critical contexts like hiring, by evaluating intersectional bias—where multiple demographic factors intersect to create unique disadvantages.

Method: The study augments the WinoBias dataset with 25 demographic markers across 10 attributes, intersecting with binary gender, creating 245,700 prompts. It introduces Coreference Confidence Disparity to measure bias through model uncertainty.

Result: Evaluation of five LLMs reveals confidence disparities up to 40% across attributes like body type and socio-economic status, with models most uncertain about doubly-disadvantaged identities. LLMs rely on memorization, not reasoning.

Conclusion: LLMs exhibit significant intersectional bias, with memorization driving performance. This highlights failures in value alignment and validity, compounding potential social harm.

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [30] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: The paper examines ASR bias as a form of disrespect toward marginalized linguistic communities, highlighting ethical dimensions like temporal taxation and identity disruption, and calls for recognizing diverse speech varieties in ASR development.


<details>
  <summary>Details</summary>
Motivation: To address the fairness implications of ASR systems, particularly how misrecognition of non-standard dialects perpetuates historical injustices and disrespect.

Method: Philosophical analysis distinguishing between neutral classification and harmful discrimination, identifying unique ethical dimensions of ASR bias.

Result: ASR bias creates asymmetric power relationships, and current fairness metrics fail to capture its ethical complexities.

Conclusion: Addressing ASR bias requires recognizing diverse speech varieties as legitimate, moving beyond technical fixes to respect linguistic diversity and autonomy.

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [31] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
*Biao Yi,Jiahao Li,Baolei Zhang,Lihai Nie,Tong Li,Tiansheng Huang,Zheli Liu*

Main category: cs.CL

TL;DR: SafeGrad introduces gradient surgery to resolve conflicting gradients in fine-tuning LLMs, ensuring safety alignment without compromising task performance.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for LLMs are vulnerable to malicious examples, degrading safety alignment as harmful ratios increase.

Method: SafeGrad uses gradient surgery to nullify harmful gradient components and employs a KL-divergence alignment loss for robustness.

Result: SafeGrad maintains robust safety at high harmful ratios without sacrificing task performance, outperforming existing defenses.

Conclusion: SafeGrad offers a state-of-the-art solution for safe fine-tuning of LLMs, balancing safety and task fidelity effectively.

Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few
malicious examples mixed into the user's fine-tuning dataset can compromise the
safety alignment of Large Language Models (LLMs). While a recognized paradigm
frames safe fine-tuning as a multi-objective optimization problem balancing
user task performance with safety alignment, we find existing solutions are
critically sensitive to the harmful ratio, with defenses degrading sharply as
harmful ratio increases. We diagnose that this failure stems from conflicting
gradients, where the user-task update directly undermines the safety objective.
To resolve this, we propose SafeGrad, a novel method that employs gradient
surgery. When a conflict is detected, SafeGrad nullifies the harmful component
of the user-task gradient by projecting it onto the orthogonal plane of the
alignment gradient, allowing the model to learn the user's task without
sacrificing safety. To further enhance robustness and data efficiency, we
employ a KL-divergence alignment loss that learns the rich, distributional
safety profile of the well-aligned foundation model. Extensive experiments show
that SafeGrad provides state-of-the-art defense across various LLMs and
datasets, maintaining robust safety even at high harmful ratios without
compromising task fidelity.

</details>


### [32] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
*Leyi Pan,Zheyu Fu,Yunpeng Zhai,Shuchang Tao,Sheng Guan,Shiyu Huang,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Felix Henry,Lijie Wen,Aiwei Liu*

Main category: cs.CL

TL;DR: The paper introduces Omni-SafetyBench, the first benchmark for evaluating safety in Omni-modal Large Language Models (OLLMs), addressing gaps in existing benchmarks. It includes 24 modality combinations and proposes tailored metrics like Safety-score and CMSC-score. Evaluation of 10 OLLMs reveals vulnerabilities, emphasizing the need for improved safety measures.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack the ability to assess safety in OLLMs under audio-visual joint inputs or cross-modal consistency, necessitating a dedicated solution.

Method: The authors develop Omni-SafetyBench with 24 modality combinations and 972 samples per variation, including audio-visual harm cases. They propose Safety-score (C-ASR, C-RR) and CMSC-score for evaluation.

Result: Evaluation of 10 OLLMs shows critical vulnerabilities: no model excels in both safety and consistency, defenses weaken with complex inputs, and severe weaknesses exist in specific modalities.

Conclusion: The benchmark and metrics highlight urgent needs for enhanced OLLM safety, providing a foundation for future improvements.

Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual
and auditory processing with text, necessitates robust safety evaluations to
mitigate harmful outputs. However, no dedicated benchmarks currently exist for
OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess
safety performance under audio-visual joint inputs or cross-modal safety
consistency. To fill this gap, we introduce Omni-SafetyBench, the first
comprehensive parallel benchmark for OLLM safety evaluation, featuring 24
modality combinations and variations with 972 samples each, including dedicated
audio-visual harm cases. Considering OLLMs' comprehension challenges with
complex omni-modal inputs and the need for cross-modal consistency evaluation,
we propose tailored metrics: a Safety-score based on conditional Attack Success
Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and
a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency
across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals
critical vulnerabilities: (1) no model excels in both overall safety and
consistency, with only 3 models achieving over 0.6 in both metrics and top
performer scoring around 0.8; (2) safety defenses weaken with complex inputs,
especially audio-visual joints; (3) severe weaknesses persist, with some models
scoring as low as 0.14 on specific modalities. Our benchmark and metrics
highlight urgent needs for enhanced OLLM safety, providing a foundation for
future improvements.

</details>


### [33] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: The paper introduces PHG-DIF, a framework for personalized headline generation that addresses click noise in historical behaviors, improving headline quality by filtering noise and modeling evolving user interests.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to account for personalized-irrelevant click noise in historical clickstreams, leading to inaccurate headlines.

Method: PHG-DIF uses dual-stage filtering to remove noise (short dwell times, abnormal click bursts) and multi-level temporal fusion for dynamic user interest modeling.

Result: PHG-DIF achieves SOTA results on the DT-PENS dataset, mitigating click noise effects and improving headline quality.

Conclusion: The proposed framework effectively addresses click noise and enhances personalized headline generation, with publicly available implementation and dataset.

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [34] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: A framework for automated extraction of schema lineage from multilingual enterprise pipelines is proposed to address semantic drift, with evaluation via SLiCE and benchmarks showing performance scaling with model size.


<details>
  <summary>Details</summary>
Motivation: Semantic drift in enterprise data pipelines compromises reproducibility and governance, affecting services like RAG and text-to-SQL.

Method: Extracts fine-grained schema lineage (source schemas, tables, transformations, aggregations) and evaluates lineage quality with SLiCE.

Result: Performance scales with model size; a 32B open-source model matches GPT series under standard prompting.

Conclusion: The framework offers a scalable, economical solution for schema-aware agents in practical applications.

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [35] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: DySK-Attn is a framework enabling LLMs to integrate real-time knowledge from a dynamic KG using sparse knowledge attention, outperforming baselines in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs' static knowledge becomes outdated quickly, and retraining or existing editing methods are inefficient or slow.

Method: Combines LLMs with a dynamic KG and uses a sparse knowledge attention mechanism for efficient, focused knowledge retrieval.

Result: Outperforms baselines like RAG and model editing in accuracy and computational efficiency for time-sensitive tasks.

Conclusion: DySK-Attn provides a scalable solution for keeping LLMs updated with dynamic knowledge.

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [36] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: TALON enhances LLM-based time series forecasting by addressing temporal heterogeneity and modality gaps, achieving superior performance with an 11% average MSE improvement.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with time series forecasting due to temporal heterogeneity and the modality gap between numerical signals and language representations.

Method: TALON uses a Heterogeneous Temporal Encoder for localized pattern modeling and a Semantic Alignment Module to bridge the modality gap.

Result: TALON outperforms state-of-the-art methods with an 11% average MSE improvement on seven benchmarks.

Conclusion: TALON demonstrates the effectiveness of pattern-aware and semantic-aware designs for adapting LLMs to time series forecasting.

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [37] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
*Chaoqun Cui,Siyuan Li,Kunkun Ma,Caiyan Jia*

Main category: cs.CL

TL;DR: The paper proposes Post Engagement Prediction (PEP), a continued pretraining strategy for PLMs to improve rumor detection by leveraging propagation structures and social media-specific features.


<details>
  <summary>Details</summary>
Motivation: PLMs underperform in social media tasks like rumor detection due to mismatches between pretraining corpora and social texts, inadequate handling of social symbols, and unsuitable pretraining tasks for modeling user engagements.

Method: Introduces PEP, a strategy to predict post relations (root, branch, parent) and curates large-scale datasets (TwitterCorpus, UTwitter, UWeibo) to train a Twitter-tailored PLM, SoLM.

Result: PEP significantly improves rumor detection performance, boosting baseline models by 1.0-3.7% accuracy and outperforming state-of-the-art methods. SoLM achieves competitive results without high-level modules.

Conclusion: PEP effectively enhances PLMs for social media tasks by learning discriminative post interaction features, demonstrating its potential for rumor detection.

Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language
Processing tasks, benefiting from large-scale pretraining and self-attention
mechanism's ability to capture long-range dependencies. However, their
performance on social media application tasks like rumor detection remains
suboptimal. We attribute this to mismatches between pretraining corpora and
social texts, inadequate handling of unique social symbols, and pretraining
tasks ill-suited for modeling user engagements implicit in propagation
structures. To address these issues, we propose a continue pretraining strategy
called Post Engagement Prediction (PEP) to infuse information from propagation
structures into PLMs. PEP makes models to predict root, branch, and parent
relations between posts, capturing interactions of stance and sentiment crucial
for rumor detection. We also curate and release large-scale Twitter corpus:
TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with
propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP
strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments
demonstrate PEP significantly boosts rumor detection performance across
universal and social media PLMs, even in few-shot scenarios. On benchmark
datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it
to outperform current state-of-the-art methods on multiple datasets. SoLM
alone, without high-level modules, also achieves competitive results,
highlighting the strategy's effectiveness in learning discriminative post
interaction features.

</details>


### [38] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: The paper investigates interpretability of neural networks in speech processing, focusing on lexical stress prediction using CNNs and LRP for analysis.


<details>
  <summary>Details</summary>
Motivation: To understand what informs neural network decisions in speech processing, specifically for lexical stress prediction, and to interpret their behavior.

Method: Automatically constructed a dataset of English disyllabic words, trained CNN architectures for stress prediction, and used LRP for interpretability analysis.

Result: Achieved 92% accuracy; LRP revealed classifiers focus on spectral properties of stressed vowels and distributed cues throughout the word.

Conclusion: Deep learning can acquire distributed stress cues from natural data, extending traditional phonetic methods.

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [39] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
*Zhe Ren*

Main category: cs.CL

TL;DR: The paper addresses challenges in Few-Shot Continual Learning Named Entity Recognition (FS-CLNER) by introducing a prompt tuning paradigm and memory demonstration templates to enhance performance and avoid the Few-Shot Distillation Dilemma.


<details>
  <summary>Details</summary>
Motivation: The scarcity of new-class entities and lack of old-class information in FS-CLNER tasks hinder generalization and knowledge distillation, leading to the Few-Shot Distillation Dilemma.

Method: The authors propose an expandable Anchor words-oriented Prompt Tuning (APT) paradigm and Memory Demonstration Templates (MDT) to bridge pre-training and fine-tuning gaps and provide replay samples.

Result: Experiments show the approach achieves competitive performance on FS-CLNER tasks.

Conclusion: The proposed APT and MDT strategies effectively address FS-CLNER challenges, improving generalization and avoiding catastrophic forgetting.

Abstract: Knowledge distillation has been successfully applied to Continual Learning
Named Entity Recognition (CLNER) tasks, by using a teacher model trained on
old-class data to distill old-class entities present in new-class data as a
form of regularization, thereby avoiding catastrophic forgetting. However, in
Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it
difficult for the trained model to generalize during inference. More
critically, the lack of old-class entity information hinders the distillation
of old knowledge, causing the model to fall into what we refer to as the
Few-Shot Distillation Dilemma. In this work, we address the above challenges
through a prompt tuning paradigm and memory demonstration template strategy.
Specifically, we designed an expandable Anchor words-oriented Prompt Tuning
(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby
enhancing performance in few-shot scenarios. Additionally, we incorporated
Memory Demonstration Templates (MDT) into each training instance to provide
replay samples from previous tasks, which not only avoids the Few-Shot
Distillation Dilemma but also promotes in-context learning. Experiments show
that our approach achieves competitive performances on FS-CLNER.

</details>


### [40] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: The paper extends a 2D articulatory model (DYNARTmo) by adding a 3D palatal dome representation to estimate tongue-palate contact areas. It implements two dome geometries for lateral curvature and enables electropalatography-like visualizations.


<details>
  <summary>Details</summary>
Motivation: To improve the estimation of tongue-palate contact areas and enhance visualization for speech science and therapy applications.

Method: Integrates 3D palatal dome geometries (half-ellipse and cosine-based) into the 2D model, computes lateral contact points, and generates synchronized views (sagittal, glottal, palatal).

Result: The model now supports dynamic articulation displays with enhanced visualization, useful for education and therapy.

Conclusion: The extension improves realism and utility, with future plans to add a facial view and articulatory-to-acoustic synthesis.

Abstract: This paper describes an extension of the two-dimensional dynamic articulatory
model DYNARTmo by integrating an internal three-dimensional representation of
the palatal dome to estimate tongue-palate contact areas from midsagittal
tongue contours. Two alternative dome geometries - a half-ellipse and a cosine
based profile - are implemented to model lateral curvature in the coronal
plane. Using these geometries, lateral contact points are analytically computed
for each anterior-posterior position, enabling the generation of
electropalatography-like visualizations within the 2D+ framework. The enhanced
model supports three synchronized views (sagittal, glottal, and palatal) for
static and dynamic (animated) articulation displays, suitable for speech
science education and speech therapy. Future work includes adding a facial
(lip) view and implementing articulatory-to-acoustic synthesis to
quantitatively evaluate model realism.

</details>


### [41] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: The paper addresses limitations in empathetic reasoning of Speech-LLMs by integrating contextual and paralinguistic cues through explicit and implicit methods, improving performance by 38.41% and 46.02% respectively.


<details>
  <summary>Details</summary>
Motivation: Current Speech-LLMs lack empathetic reasoning due to missing contextual and paralinguistic training data.

Method: Two approaches: (1) explicit paralinguistic metadata input, and (2) implicit QA pair generation using emotion annotations and speech transcriptions.

Result: Implicit method improves performance by 38.41%, reaching 46.02% when combined with explicit method. LLM judge reliability is validated.

Conclusion: Combining explicit and implicit methods enhances contextual paralinguistic understanding in Speech-LLMs.

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [42] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: MAQuA is an adaptive framework for mental health screening that reduces question burden by 50-87% while maintaining accuracy, using multi-outcome modeling and IRT.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and user burden in LLM-based mental health assessments by optimizing question selection.

Method: Combines multi-outcome modeling, IRT, and factor analysis to adaptively select informative questions across multiple symptom dimensions.

Result: Reduces questions needed for stable scores by 50-87% (e.g., 71% fewer for depression, 85% fewer for eating disorders).

Conclusion: MAQuA is efficient and scalable for mental health screening, enhancing LLM integration into clinical workflows.

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [43] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: The study evaluates 14 LLMs' moral reasoning across 27 trolley problem scenarios using 10 moral philosophies. Findings show variability in decisiveness, alignment with human consensus, and ethical framing sensitivity.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' moral reasoning is crucial as they mediate ethically sensitive decisions.

Method: Factorial prompting protocol to elicit 3,780 binary decisions and justifications, analyzing decisiveness, consistency, alignment, and sensitivity.

Result: Reasoning-enhanced models are more decisive but not always aligned with human consensus. Best alignment in altruistic, fairness, and virtue ethics framings.

Conclusion: Moral prompting is a diagnostic tool; moral reasoning should be a primary axis in LLM alignment, with standardized benchmarks.

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [44] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: ARCE introduces a method to enhance NER in the AEC domain by using LLMs to generate simple explanations (Cote) for pre-training RoBERTa, achieving a state-of-the-art Macro-F1 score of 77.20%.


<details>
  <summary>Details</summary>
Motivation: Standard pre-trained models underperform in the AEC domain due to specialized terminology and complex contexts, and manual corpus creation is costly.

Method: ARCE uses an LLM to generate simple explanations (Cote), then pre-trains RoBERTa with this corpus before fine-tuning for NER.

Result: ARCE achieves a Macro-F1 score of 77.20%, outperforming other methods.

Conclusion: Simple, explanation-based knowledge is more effective than complex rationales for NER in the AEC domain.

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [45] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Zheng Chu,Bo Yang,Xiaocheng Feng,Yang Xiang,Ming Liu*

Main category: cs.CL

TL;DR: The paper introduces CCFQA, a benchmark for evaluating multilingual and multimodal factuality in MLLMs, highlighting gaps in current evaluations and proposing a few-shot transfer learning solution.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs focus on English and textual/visual modalities, neglecting multilingual speech input, creating a need for a more comprehensive evaluation tool.

Method: The authors develop CCFQA, a benchmark with parallel speech-text questions in 8 languages, and propose a few-shot transfer learning strategy to enhance multilingual SQA tasks.

Result: Current MLLMs struggle with CCFQA, but the proposed few-shot method achieves competitive performance with minimal training.

Conclusion: CCFQA is released as a resource to improve MLLMs' multilingual and multimodal reliability, with code and dataset publicly available.

Abstract: As Large Language Models (LLMs) are increasingly popularized in the
multilingual world, ensuring hallucination-free factuality becomes markedly
crucial. However, existing benchmarks for evaluating the reliability of
Multimodal Large Language Models (MLLMs) predominantly focus on textual or
visual modalities with a primary emphasis on English, which creates a gap in
evaluation when processing multilingual input, especially in speech. To bridge
this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal
\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA
benchmark contains parallel speech-text factual questions across 8 languages,
designed to systematically evaluate MLLMs' cross-lingual and cross-modal
factuality capabilities. Our experimental results demonstrate that current
MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we
propose a few-shot transfer learning strategy that effectively transfers the
Question Answering (QA) capabilities of LLMs in English to multilingual Spoken
Question Answering (SQA) tasks, achieving competitive performance with
GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a
foundational research resource to promote the development of MLLMs with more
robust and reliable speech understanding capabilities. Our code and dataset are
available at https://github.com/yxduir/ccfqa.

</details>


### [46] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: HealthBranches is a benchmark dataset for medical Q&A, designed to evaluate complex reasoning in LLMs, with 4,063 case studies across 17 topics, supporting open-ended and multiple-choice formats.


<details>
  <summary>Details</summary>
Motivation: To enable robust evaluation of LLMs' multi-step inference in medical contexts and support trustworthy, interpretable, and clinically reliable LLM development.

Method: Semi-automated pipeline transforms medical decision pathways into realistic patient cases with questions, answers, and reasoning chains.

Result: A dataset of clinically validated reasoning chains, supporting structured RAG contexts and educational use.

Conclusion: HealthBranches provides a foundation for advancing reliable LLMs in high-stakes medical domains and serves as an educational resource.

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [47] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: The paper introduces ObfusQAte and ObfusQA, a framework to test LLM robustness against obfuscated questions, revealing LLMs' limitations in handling nuanced variations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of studies on LLM robustness when faced with obfuscated questions, aiming to evaluate their adaptability.

Method: Proposes ObfusQAte and ObfusQA, a multi-tiered framework testing LLMs across three obfuscation dimensions: Named-Entity Indirection, Distractor Indirection, and Contextual Overload.

Result: LLMs often fail or produce hallucinated responses when handling obfuscated questions.

Conclusion: ObfusQA provides a benchmark for LLM robustness, highlighting their limitations and encouraging further research in this area.

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [48] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: A chatbot was developed to study code-switching in Spanish-English bilinguals during a Map Task. Experiments tested feasibility and participant sensitivity to code-switching patterns. Predictable code-switching was preferred, while random or ungrammatical switching reduced enjoyment and task success.


<details>
  <summary>Details</summary>
Motivation: To understand the characteristics of code-switched language and investigate bilingual language use using technology.

Method: Developed a chatbot for a Map Task, prompting it to code-switch with different strategies in two experiments.

Result: Participants enjoyed predictable code-switching but disliked random or ungrammatical switching, which also reduced task success.

Conclusion: The study highlights risks of underdeveloped multilingual tech and its potential for bilingual research.

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [49] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
*Wenqian Cui,Lei Zhu,Xiaohui Li,Zhihan Guo,Haoli Bai,Lu Hou,Irwin King*

Main category: cs.CL

TL;DR: TurnGuide improves Full-Duplex Speech Language Models (FD-SLMs) by dynamically segmenting speech into turns and generating turn-level text guidance, enhancing conversational flow and coherence.


<details>
  <summary>Details</summary>
Motivation: FD-SLMs struggle with degraded conversational abilities due to prolonged speech sequences and limited high-quality spoken dialogue data, despite their potential for natural interactions.

Method: Proposes TurnGuide, a planning-inspired approach that segments assistant speech into dialogue turns and generates turn-level text guidance before speech output.

Result: TurnGuide significantly improves FD-SLMs' conversational abilities, enabling semantically meaningful and coherent speech while maintaining natural flow.

Conclusion: TurnGuide effectively addresses timing and length challenges in FD-SLMs, enhancing their performance for real-time spoken interactions.

Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation
models designed to enable natural, real-time spoken interactions by modeling
complex conversational dynamics such as interruptions, backchannels, and
overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world
double-channel conversational data to capture nuanced two-speaker dialogue
patterns for human-like interactions. However, they face a critical challenge
-- their conversational abilities often degrade compared to pure-text
conversation due to prolonged speech sequences and limited high-quality spoken
dialogue data. While text-guided speech generation could mitigate these issues,
it suffers from timing and length issues when integrating textual guidance into
double-channel audio streams, disrupting the precise time alignment essential
for natural interactions. To address these challenges, we propose TurnGuide, a
novel planning-inspired approach that mimics human conversational planning by
dynamically segmenting assistant speech into dialogue turns and generating
turn-level text guidance before speech output, which effectively resolves both
insertion timing and length challenges. Extensive experiments demonstrate our
approach significantly improves e2e FD-SLMs' conversational abilities, enabling
them to generate semantically meaningful and coherent speech while maintaining
natural conversational flow. Demos are available at
https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at
https://github.com/dreamtheater123/TurnGuide.

</details>


### [50] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: The paper introduces CulturalGround, a dataset for culturally grounding MLLMs, and CulturalPangea, an open-source MLLM, to address cultural and low-resource language gaps.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with long-tail cultural entities and low-resource languages, highlighting a need for culturally inclusive models.

Method: A data-centric approach using Wikidata to create a synthetic multilingual VQA dataset (CulturalGround) and train CulturalPangea, interleaving standard instruction-tuning data.

Result: CulturalPangea outperforms prior models by 5.0 on culture-focused benchmarks without degrading mainstream task performance.

Conclusion: The approach narrows the cultural gap in MLLMs and advances globally inclusive multimodal systems.

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [51] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
*Zhiyi Lyu,Jianguo Huang,Yanchen Deng,Steven Hoi,Bo An*

Main category: cs.CL

TL;DR: ReLoc is a unified local search framework for efficient code revision, outperforming existing methods in code generation tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing efficiency and scalability challenges in LLMs for code generation, particularly the limitations of construction-based and improvement-based methods.

Method: ReLoc uses a four-component local search framework (drafting, neighborhood generation, evaluation, updating) and a specialized revision reward model for guided search.

Result: Superior performance in diverse code generation tasks, surpassing tree-search and improvement-based methods.

Conclusion: ReLoc offers an effective solution for step-by-step code revision, enhancing efficiency and scalability in code generation.

Abstract: Large Language Models (LLMs) with inference-time scaling techniques show
promise for code generation, yet face notable efficiency and scalability
challenges. Construction-based tree-search methods suffer from rapid growth in
tree size, high token consumption, and lack of anytime property. In contrast,
improvement-based methods offer better performance but often struggle with
uninformative reward signals and inefficient search strategies. In this work,
we propose \textbf{ReLoc}, a unified local search framework which effectively
performs step-by-step code revision. Specifically, ReLoc explores a series of
local revisions through four key algorithmic components: initial code drafting,
neighborhood code generation, candidate evaluation, and incumbent code
updating, each of which can be instantiated with specific decision rules to
realize different local search algorithms such as Hill Climbing (HC) or Genetic
Algorithm (GA). Furthermore, we develop a specialized revision reward model
that evaluates code quality based on revision distance to produce fine-grained
preferences that guide the local search toward more promising candidates.
Finally, our extensive experimental results demonstrate that our approach
achieves superior performance across diverse code generation tasks,
significantly outperforming both construction-based tree search as well as the
state-of-the-art improvement-based code generation methods.

</details>


### [52] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
*Blerta Veseli,Julian Chibane,Mariya Toneva,Alexander Koller*

Main category: cs.CL

TL;DR: The paper analyzes positional biases in LLMs, finding the 'Lost in the Middle' effect is strongest when inputs use up to 50% of the context window. Beyond this, recency bias persists, while primacy weakens, leading to a distance-based bias. Retrieval is key for reasoning, and biases in reasoning stem from retrieval.


<details>
  <summary>Details</summary>
Motivation: To clarify inconsistent findings about positional biases (e.g., Lost in the Middle effect) in LLMs and understand their intensity and manifestation conditions.

Method: Conducted a comprehensive analysis using relative input lengths (relative to each model's context window) to study positional biases.

Result: The Lost in the Middle effect peaks at 50% context window usage. Recency bias remains stable, while primacy weakens, shifting to distance-based bias. Retrieval is foundational for reasoning, and reasoning biases mirror retrieval biases.

Conclusion: The study highlights the importance of retrieval in LLM reasoning and provides insights for designing benchmarks and evaluations for long-context tasks.

Abstract: Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [53] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: ALOPE is an adaptive layer-optimization framework enhancing LLM-based Quality Estimation (QE) for Machine Translation by restructuring Transformer representations and integrating low-rank adapters with regression task heads.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based QE systems are limited by their pre-training for causal language modeling, not regression tasks, and struggle with low-resource languages.

Method: ALOPE uses layer-wise adaptation, dynamic weighting, and multi-head regression to improve cross-lingual alignment and QE performance.

Result: ALOPE outperforms existing LLM-based QE approaches, with intermediate Transformer layers proving most effective for cross-lingual QE.

Conclusion: ALOPE enhances QE capabilities for LLMs, with models and framework code made publicly available for broader application.

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [54] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
*Keshav Varadarajan,Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: A method using topological data analysis identifies bias-contributing heads in GPT-2, revealing concentrated biases in specific attention heads.


<details>
  <summary>Details</summary>
Motivation: Current bias detection methods lack tests to pinpoint which parts of large language models cause bias toward specific groups.

Method: Topological data analysis is applied to identify bias-contributing heads in GPT-2 using the StereoSet dataset.

Result: Biases (e.g., gender, profession) are concentrated in specific attention heads acting as hot spots.

Conclusion: The proposed metric can help identify bias sources and may aid in de-biasing large language models in future work.

Abstract: Recently, many bias detection methods have been proposed to determine the
level of bias a large language model captures. However, tests to identify which
parts of a large language model are responsible for bias towards specific
groups remain underdeveloped. In this study, we present a method using
topological data analysis to identify which heads in GPT-2 contribute to the
misrepresentation of identity groups present in the StereoSet dataset. We find
that biases for particular categories, such as gender or profession, are
concentrated in attention heads that act as hot spots. The metric we propose
can also be used to determine which heads capture bias for a specific group
within a bias category, and future work could extend this method to help
de-bias large language models.

</details>


### [55] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: ThemeClouds is a tool using LLMs to create thematic word clouds from dialogue, improving on traditional frequency-based methods by focusing on participant-weighted themes.


<details>
  <summary>Details</summary>
Motivation: Traditional word clouds fail in conversational contexts by highlighting filler words and fragmenting ideas, limiting their usefulness for early-stage qualitative analysis.

Method: ThemeClouds employs LLMs to identify concept-level themes and counts unique participant mentions, creating visualizations based on thematic breadth.

Result: ThemeClouds outperforms frequency clouds and topic-modeling baselines in surfacing actionable insights from user study transcripts.

Conclusion: The tool offers transparency, control, and potential for interactive analyses, though design trade-offs with LLM integration remain.

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [56] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
*Jia Deng,Jie Chen,Zhipeng Chen,Daixuan Cheng,Fei Bai,Beichen Zhang,Yinqian Min,Yanzipeng Gao,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: The paper investigates exploration behaviors in RLVR for LLMs, focusing on space shaping, entropy-performance trade-offs, and performance optimization.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance the exploration mechanisms in RLVR for improving LLMs' reasoning capabilities.

Method: Systematic study of exploration in RLVR, including quantitative metrics for capability boundaries, entropy-performance analysis, and optimization techniques.

Result: Provides empirical insights and a framework for advancing RLVR systems by translating exploration gains into performance improvements.

Conclusion: The work lays a foundation for future RLVR advancements by unifying insights and empirical evidence.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

</details>


### [57] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: The paper introduces the Indian Bail Prediction System (IBPS), an AI framework to predict bail outcomes and generate legal rationales, addressing delays and inconsistencies in India's judicial system.


<details>
  <summary>Details</summary>
Motivation: Bail decisions in India are subjective, delayed, and inconsistent, disproportionately affecting disadvantaged groups and worsening judicial backlogs.

Method: IBPS uses a large-scale dataset of 150,430 bail judgments, fine-tunes a language model with statutory context, and employs RAG for performance evaluation.

Result: Models with statutory knowledge outperform baselines, achieving high accuracy and explanation quality, validated by legal experts.

Conclusion: IBPS provides a transparent, scalable solution to improve bail adjudication, reduce delays, and enhance fairness in India's judicial system.

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [58] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
*Ziheng Li,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: KeyCP++ improves one-shot event detection in LLMs by using keyword-centric chain-of-thought prompting to address over-interpretation and trigger misunderstanding.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with event detection due to inaccurate trigger understanding and over-interpretation, which in-context examples alone cannot fix.

Method: KeyCP++ introduces a keyword-centric chain-of-thought prompting approach, annotating logical gaps and using trigger discrimination to generate meaningful rationale.

Result: Experiments show KeyCP++ significantly advances one-shot event detection performance.

Conclusion: KeyCP++ effectively mitigates LLMs' weaknesses in event detection by promoting rule learning and reducing keyword over-reliance.

Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated
considerable success across various natural language processing tasks, it
encounters challenges in event detection. This is because LLMs lack an accurate
understanding of event triggers and tend to make over-interpretation, which
cannot be effectively corrected through in-context examples alone. In this
paper, we focus on the most challenging one-shot setting and propose KeyCP++, a
keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the
weaknesses of conventional ICL by automatically annotating the logical gaps
between input text and detection results for the demonstrations. Specifically,
to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger
discrimination prompting template. It incorporates the exemplary triggers
(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let
LLM propose candidate triggers, and justify each candidate. These
propose-and-judge rationales help LLMs mitigate over-reliance on the keywords
and promote detection rule learning. Extensive experiments demonstrate the
effectiveness of our approach, showcasing significant advancements in one-shot
event detection.

</details>


### [59] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: InterChart is a benchmark for evaluating vision-language models' reasoning across multiple charts, revealing their limitations in complex, multi-visual tasks.


<details>
  <summary>Details</summary>
Motivation: To assess VLMs' ability to reason across diverse, related charts, a critical skill for real-world applications like financial analysis and scientific reporting.

Method: The benchmark includes three difficulty tiers: factual reasoning, integrative analysis, and semantic inference over visually complex chart pairs.

Result: State-of-the-art VLMs show declining accuracy as chart complexity increases, performing better with simplified visual units.

Conclusion: InterChart highlights VLMs' struggles with cross-chart integration, offering a framework to advance multimodal reasoning in complex environments.

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


### [60] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
*Luyao Zhuang,Qinggang Zhang,Huachi Zhou,Juhua Liu,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: LoSemB introduces a logic-guided framework for inductive tool retrieval, addressing distribution shifts and similarity-based retrieval issues for unseen tools.


<details>
  <summary>Details</summary>
Motivation: Existing tool retrieval methods assume all tools are observed during training, which is unrealistic as tools evolve. Handling unseen tools is challenging due to distribution shifts and retrieval vulnerabilities.

Method: LoSemB uses logic-based embedding alignment to mitigate distribution shifts and a relational augmented retrieval mechanism to improve retrieval robustness.

Result: Extensive experiments show LoSemB performs well in inductive settings while maintaining effectiveness in transductive settings.

Conclusion: LoSemB provides a practical solution for inductive tool retrieval, leveraging logical information without costly retraining.

Abstract: Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

</details>


### [61] [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: The paper examines the limitations of Next Token Prediction (NTP) in Transformer-based models, proposing Masked Sentence Prediction (MSP) to evaluate long-range coherence. Testing three commercial LLMs shows poor performance in low-structured domains.


<details>
  <summary>Details</summary>
Motivation: NTP's focus on single-token prediction lacks explicit incentives for global coherence, raising concerns about LLMs' ability to handle longer contexts like full sentences.

Method: Evaluated GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash on MSP across narrative, procedural, and expository domains (ROCStories, Recipe1M, Wikipedia), measuring fidelity and cohesiveness.

Result: Commercial LLMs performed poorly in predicting masked sentences in low-structured domains, despite excelling in other tasks.

Conclusion: Current LLMs have a gap in handling long-range coherence, particularly in unstructured contexts, highlighting a need for improved methods.

Abstract: Transformer-based models primarily rely on Next Token Prediction (NTP), which
predicts the next token in a sequence based on the preceding context. However,
NTP's focus on single-token prediction often limits a model's ability to plan
ahead or maintain long-range coherence, raising questions about how well LLMs
can predict longer contexts, such as full sentences within structured
documents. While NTP encourages local fluency, it provides no explicit
incentive to ensure global coherence across sentence boundaries-an essential
skill for reconstructive or discursive tasks. To investigate this, we evaluate
three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on
Masked Sentence Prediction (MSP) - the task of infilling a randomly removed
sentence - from three domains: ROCStories (narrative), Recipe1M (procedural),
and Wikipedia (expository). We assess both fidelity (similarity to the original
sentence) and cohesiveness (fit within the surrounding context). Our key
finding reveals that commercial LLMs, despite their superlative performance in
other tasks, are poor at predicting masked sentences in low-structured domains,
highlighting a gap in current model capabilities.

</details>


### [62] [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)
*Zhenliang Zhang,Junzhe Zhang,Xinyu Hu,HuiXuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: The study explores the causal relationship between social bias and faithfulness hallucinations in LLMs, using SCM and a new dataset (BID) to validate causality and measure effects.


<details>
  <summary>Details</summary>
Motivation: To investigate if social bias contributes to faithfulness hallucinations in LLMs, a previously unexplored causal relationship.

Method: Utilizes Structural Causal Model (SCM) to establish causality, designs bias interventions, and develops the Bias Intervention Dataset (BID) for precise measurement.

Result: Experiments show biases significantly cause faithfulness hallucinations, with varying directional effects. Bias also subtly impacts unfairness hallucinations.

Conclusion: Social bias is a significant cause of hallucinations in LLMs, with distinct effects per bias state, highlighting the need for bias-aware interventions.

Abstract: Large language models (LLMs) have achieved remarkable success in various
tasks, yet they remain vulnerable to faithfulness hallucinations, where the
output does not align with the input. In this study, we investigate whether
social bias contributes to these hallucinations, a causal relationship that has
not been explored. A key challenge is controlling confounders within the
context, which complicates the isolation of causality between bias states and
hallucinations. To address this, we utilize the Structural Causal Model (SCM)
to establish and validate the causality and design bias interventions to
control confounders. In addition, we develop the Bias Intervention Dataset
(BID), which includes various social biases, enabling precise measurement of
causal effects. Experiments on mainstream LLMs reveal that biases are
significant causes of faithfulness hallucinations, and the effect of each bias
state differs in direction. We further analyze the scope of these causal
effects across various models, specifically focusing on unfairness
hallucinations, which are primarily targeted by social bias, revealing the
subtle yet significant causal effect of bias on hallucination generation.

</details>


### [63] [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)
*Zeyu Yang,Lai Wei,Roman Koshkin,Xi Chen,Satoshi Nakamura*

Main category: cs.CL

TL;DR: A grammar-based chunking strategy and SASST framework improve simultaneous speech translation by leveraging syntactic structures and dynamic output timing.


<details>
  <summary>Details</summary>
Motivation: To enhance simultaneous speech translation by ensuring semantic coherence and minimizing fragmentation through syntactic parsing.

Method: Proposes a grammar-based chunking strategy and SASST framework, integrating Whisper encoder and decoder-only LLM for dynamic translation output.

Result: Significant translation quality improvements on CoVoST2 multilingual corpus, validating syntactic structures' effectiveness.

Conclusion: Syntactic structures and dynamic timing in SASST improve translation quality in LLM-driven SimulST systems.

Abstract: This work proposes a grammar-based chunking strategy that segments input
streams into semantically complete units by parsing dependency relations (e.g.,
noun phrase boundaries, verb-object structures) and punctuation features. The
method ensures chunk coherence and minimizes semantic fragmentation. Building
on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech
Translation), an end-to-end framework integrating frozen Whisper encoder and
decoder-only LLM. The unified architecture dynamically outputs translation
tokens or <WAIT> symbols to jointly optimize translation timing and content,
with target-side reordering addressing word-order divergence. Experiments on
CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation
quality improvements across languages and validate the effectiveness of
syntactic structures in LLM-driven SimulST systems.

</details>


### [64] [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
*Haoyuan Wu,Haoxing Chen,Xiaodong Chen,Zhanchao Zhou,Tieyuan Chen,Yihong Zhuang,Guoshan Lu,Zenan Huang,Junbo Zhao,Lin Liu,Zhenzhong Lan,Bei Yu,Jianguo Li*

Main category: cs.CL

TL;DR: Grove MoE introduces a heterogeneous expert architecture for LLMs, improving efficiency by dynamically activating parameters based on input complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional MoE uses uniform experts, limiting computational efficiency. Grove MoE addresses this by varying expert sizes.

Method: Introduces Grove MoE with adjugate experts and dynamic activation, applied to Qwen3-30B-A3B-Base via upcycling.

Result: GroveMoE models activate 3.14-3.28B parameters dynamically, matching SOTA performance despite smaller size.

Conclusion: Grove MoE enhances LLM efficiency and scalability without sacrificing performance.

Abstract: The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate
scalability by enabling sparse parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.

</details>


### [65] [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: Persuasive language can bias LLM judges to unfairly inflate scores for incorrect math solutions, with techniques like Consistency causing the most distortion. Model size and counter-prompting don't fully mitigate this vulnerability.


<details>
  <summary>Details</summary>
Motivation: To investigate whether persuasive language can unfairly influence LLM judges in scoring tasks where correctness should be style-independent.

Method: Formalized seven persuasion techniques (e.g., Majority, Flattery) and embedded them into identical math responses. Tested across six benchmarks with LLM judges.

Result: Persuasive language inflated scores for incorrect solutions by up to 8%, with Consistency being the most impactful. Combining techniques worsened bias, and counter-prompting didn't resolve it.

Conclusion: LLM-as-a-Judge pipelines are vulnerable to persuasion-based attacks, requiring robust defenses to ensure fairness.

Abstract: As large language models take on growing roles as automated evaluators in
practical settings, a critical question arises: Can individuals persuade an LLM
judge to assign unfairly high scores? This study is the first to reveal that
strategically embedded persuasive language can bias LLM judges when scoring
mathematical reasoning tasks, where correctness should be independent of
stylistic variation. Grounded in Aristotle's rhetorical principles, we
formalize seven persuasion techniques (Majority, Consistency, Flattery,
Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical
responses. Across six math benchmarks, we find that persuasive language leads
LLM judges to assign inflated scores to incorrect solutions, by up to 8% on
average, with Consistency causing the most severe distortion. Notably,
increasing model size does not substantially mitigate this vulnerability.
Further analysis demonstrates that combining multiple persuasion techniques
amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,
the persuasive effect persists under counter prompting strategies, highlighting
a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need
for robust defenses against persuasion-based attacks.

</details>


### [66] [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)
*Olga Kellert,Muhammad Imran,Nicholas Hill Matlis,Mahmud Uz Zaman,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: The paper evaluates a compositional approach for Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in NLP, bridging a research gap by showing SA's compositional rules apply to FA.


<details>
  <summary>Details</summary>
Motivation: Quantitative evaluations of compositional approaches in FA are rare, despite their prevalence in SA. The paper aims to fill this gap by linking FA and SA.

Method: Uses Universal Dependencies (UDs) formalism for syntactic rules (modification, coordination, negation) in SA, comparing accuracy with non-compositional VADER.

Result: The compositional approach offers interpretability and explainability advantages over non-compositional methods, tested on appropriate datasets.

Conclusion: The study generalizes SA's compositional results to FA, demonstrating their applicability and benefits.

Abstract: This paper summarizes the results of evaluating a compositional approach for
Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural
Language Processing (NLP). While quantitative evaluations of compositional and
non-compositional approaches in SA exist in NLP, similar quantitative
evaluations are very rare in FA in Linguistics that deal with linguistic
expressions representing focus or emphasis such as "it was John who left". We
fill this gap in research by arguing that compositional rules in SA also apply
to FA because FA and SA are closely related meaning that SA is part of FA. Our
compositional approach in SA exploits basic syntactic rules such as rules of
modification, coordination, and negation represented in the formalism of
Universal Dependencies (UDs) in English and applied to words representing
sentiments from sentiment dictionaries. Some of the advantages of our
compositional analysis method for SA in contrast to non-compositional analysis
methods are interpretability and explainability. We test the accuracy of our
compositional approach and compare it with a non-compositional approach VADER
that uses simple heuristic rules to deal with negation, coordination and
modification. In contrast to previous related work that evaluates
compositionality in SA on long reviews, this study uses more appropriate
datasets to evaluate compositionality. In addition, we generalize the results
of compositional approaches in SA to compositional approaches in FA.

</details>


### [67] [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)
*Yu-Min Tseng,Wei-Lin Chen,Chung-Chi Chen,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: The paper explores whether top-performing LLMs can replace human expert annotators in specialized domains (finance, biomedicine, law) using individual and multi-agent approaches, finding limited gains from inference-time techniques and reasoning models.


<details>
  <summary>Details</summary>
Motivation: Textual data annotation is costly and labor-intensive. While LLMs show promise for general NLP tasks, their effectiveness in expert domains is unclear.

Method: Evaluated individual LLMs and multi-agent discussion frameworks with reasoning models (e.g., o3-mini) across finance, biomedicine, and law.

Result: (1) Marginal gains from inference-time techniques. (2) Reasoning models show no significant improvement. (3) Multi-agent discussions reveal model behaviors like stubbornness.

Conclusion: LLMs have limited effectiveness as direct replacements for human expert annotators in specialized domains, with multi-agent approaches offering mixed results.

Abstract: Textual data annotation, the process of labeling or tagging text with
relevant information, is typically costly, time-consuming, and labor-intensive.
While large language models (LLMs) have demonstrated their potential as direct
alternatives to human annotators for general domains natural language
processing (NLP) tasks, their effectiveness on annotation tasks in domains
requiring expert knowledge remains underexplored. In this paper, we
investigate: whether top-performing LLMs, which might be perceived as having
expert-level proficiency in academic and professional benchmarks, can serve as
direct alternatives to human expert annotators? To this end, we evaluate both
individual LLMs and multi-agent approaches across three highly specialized
domains: finance, biomedicine, and law. Specifically, we propose a multi-agent
discussion framework to simulate a group of human annotators, where LLMs are
tasked to engage in discussions by considering others' annotations and
justifications before finalizing their labels. Additionally, we incorporate
reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our
empirical results reveal that: (1) Individual LLMs equipped with inference-time
techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal
or even negative performance gains, contrary to prior literature suggesting
their broad effectiveness. (2) Overall, reasoning models do not demonstrate
statistically significant improvements over non-reasoning models in most
settings. This suggests that extended long CoT provides relatively limited
benefits for data annotation in specialized domains. (3) Certain model
behaviors emerge in the multi-agent discussion environment. For instance,
Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even
when other agents provide correct annotations or valid reasoning.

</details>


### [68] [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)
*Amrita Singh,H. Suhan Karaca,Aditya Joshi,Hye-young Paik,Jiaojiao Jiang*

Main category: cs.CL

TL;DR: Evaluation of 10 legal-specific LLMs vs. 7 general-purpose LLMs in contract understanding tasks shows legal-specific models outperform, especially in nuanced legal tasks. Legal-BERT and Contracts-BERT achieve SOTA results.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive evaluation of legal-specific LLMs in contract understanding tasks.

Method: Evaluated 10 legal-specific and 7 general-purpose LLMs on three English contract understanding tasks.

Result: Legal-specific LLMs consistently outperform general-purpose models, with Legal-BERT and Contracts-BERT setting new SOTAs.

Conclusion: Legal-specific LLMs are superior for contract understanding, aiding future development of more accurate systems.

Abstract: Despite advances in legal NLP, no comprehensive evaluation covering multiple
legal-specific LLMs currently exists for contract classification tasks in
contract understanding. To address this gap, we present an evaluation of 10
legal-specific LLMs on three English language contract understanding tasks and
compare them with 7 general-purpose LLMs. The results show that legal-specific
LLMs consistently outperform general-purpose models, especially on tasks
requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish
new SOTAs on two of the three tasks, despite having 69% fewer parameters than
the best-performing general-purpose LLM. We also identify CaseLaw-BERT and
LexLM as strong additional baselines for contract understanding. Our results
provide a holistic evaluation of legal-specific LLMs and will facilitate the
development of more accurate contract understanding systems.

</details>


### [69] [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: The paper evaluates 19 LLMs for Czech ABSA, finding domain-specific fine-tuned models outperform general-purpose LLMs in zero-shot/few-shot settings, while fine-tuned LLMs achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored capabilities of LLMs for Czech ABSA and compare their performance across different scenarios.

Method: Comprehensive evaluation of 19 LLMs of varying sizes/architectures in zero-shot, few-shot, and fine-tuning settings.

Result: Domain-specific fine-tuned models outperform general-purpose LLMs in zero-shot/few-shot; fine-tuned LLMs achieve SOTA.

Conclusion: LLMs show promise for Czech ABSA, with fine-tuning yielding best results, but challenges remain in aspect term prediction.

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to identify sentiment toward specific aspects of an entity.
While large language models (LLMs) have shown strong performance in various
natural language processing (NLP) tasks, their capabilities for Czech ABSA
remain largely unexplored. In this work, we conduct a comprehensive evaluation
of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their
performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show
that small domain-specific models fine-tuned for ABSA outperform
general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs
achieve state-of-the-art results. We analyze how factors such as
multilingualism, model size, and recency influence performance and present an
error analysis highlighting key challenges, particularly in aspect term
prediction. Our findings provide insights into the suitability of LLMs for
Czech ABSA and offer guidance for future research in this area.

</details>


### [70] [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: Adding a few target language examples to training significantly improves cross-lingual ABSA performance, even surpassing monolingual baselines with 1,000 examples.


<details>
  <summary>Details</summary>
Motivation: Challenges in low-resource languages for ABSA due to lack of labelled data, and the oversight of incorporating few-shot target language examples in current approaches.

Method: Evaluated the impact of adding few-shot target language examples across four ABSA tasks, six languages, and two sequence-to-sequence models.

Result: Adding as few as ten examples improves performance over zero-shot settings, and 1,000 examples can surpass monolingual baselines.

Conclusion: Few-shot target language examples are feasible and highly effective for improving cross-lingual ABSA in low-resource settings.

Abstract: Aspect-based sentiment analysis (ABSA) has received substantial attention in
English, yet challenges remain for low-resource languages due to the scarcity
of labelled data. Current cross-lingual ABSA approaches often rely on external
translation tools and overlook the potential benefits of incorporating a small
number of target language examples into training. In this paper, we evaluate
the effect of adding few-shot target language examples to the training set
across four ABSA tasks, six target languages, and two sequence-to-sequence
models. We show that adding as few as ten target language examples
significantly improves performance over zero-shot settings and achieves a
similar effect to constrained decoding in reducing prediction errors.
Furthermore, we demonstrate that combining 1,000 target language examples with
English data can even surpass monolingual baselines. These findings offer
practical insights for improving cross-lingual ABSA in low-resource and
domain-specific settings, as obtaining ten high-quality annotated examples is
both feasible and highly effective.

</details>


### [71] [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)
*Chen Cecilia Liu,Hiba Arnaout,Nils Kovačić,Dana Atzil-Slonim,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper introduces CultureCare, a dataset for culturally sensitive emotional support, and tests adaptation strategies for LLMs, showing their potential in outperforming peer responses and aiding clinical training.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored ability of LLMs in delivering culturally sensitive emotional support due to lack of resources.

Method: Develop CultureCare dataset, test four adaptation strategies for LLMs, and evaluate using LLM judges, human annotators, and psychologists.

Result: Adapted LLMs outperform anonymous peer responses, and cultural role-play alone is insufficient for sensitivity. LLMs show promise in clinical training.

Conclusion: LLMs can enhance culturally sensitive support and clinical training, but more nuanced approaches are needed beyond simple role-play.

Abstract: Large language models (LLMs) show promise in offering emotional support and
generating empathetic responses for individuals in distress, but their ability
to deliver culturally sensitive support remains underexplored due to lack of
resources. In this work, we introduce CultureCare, the first dataset designed
for this task, spanning four cultures and including 1729 distress messages,
1523 cultural signals, and 1041 support strategies with fine-grained emotional
and cultural annotations. Leveraging CultureCare, we (i) develop and test four
adaptation strategies for guiding three state-of-the-art LLMs toward culturally
sensitive responses; (ii) conduct comprehensive evaluations using LLM judges,
in-culture human annotators, and clinical psychologists; (iii) show that
adapted LLMs outperform anonymous online peer responses, and that simple
cultural role-play is insufficient for cultural sensitivity; and (iv) explore
the application of LLMs in clinical training, where experts highlight their
potential in fostering cultural competence in future therapists.

</details>


### [72] [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)
*John C. McDonald,Rosalee Wolfe,Fabrizio Nunnari*

Main category: cs.CL

TL;DR: The paper proposes a two-parameter method for specifying emotional non-manual signals in signing avatars, using the EASIER notation for more coherent and nuanced expressions.


<details>
  <summary>Details</summary>
Motivation: Emotional content in sign languages is challenging for avatars due to the lack of a standard method for specifying emotional states.

Method: An intuitive two-parameter representation is applied to the Paula signing avatar, controlled via the EASIER textual notation.

Result: The method enables nuanced emotional expressions and consistent specification in linguistic annotations for avatars.

Conclusion: The two-parameter approach shows promise for improving emotional expression in signing avatars and linguistic annotations.

Abstract: Non-manual signals in sign languages continue to be a challenge for signing
avatars. More specifically, emotional content has been difficult to incorporate
because of a lack of a standard method of specifying the avatar's emotional
state. This paper explores the application of an intuitive two-parameter
representation for emotive non-manual signals to the Paula signing avatar that
shows promise for facilitating the linguistic specification of emotional facial
expressions in a more coherent manner than previous methods. Users can apply
these parameters to control Paula's emotional expressions through a textual
representation called the EASIER notation. The representation can allow avatars
to express more nuanced emotional states using two numerical parameters. It
also has the potential to enable more consistent specification of emotional
non-manual signals in linguistic annotations which drive signing avatars.

</details>


### [73] [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper introduces GREP, a multi-turn evaluation framework for assessing the quality of AI-generated scientific writing, specifically related work sections, by integrating domain-specific criteria and expert preferences.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating AI-generated scientific writing, which lacks domain-specific quality standards and expert preferences, the paper focuses on related work generation as a challenging task.

Method: Proposes GREP, a framework decomposing evaluation into fine-grained dimensions, using contrastive few-shot examples for contextual guidance, and offering two variants (proprietary and open-weight LLMs).

Result: GREP robustly assesses related work sections, correlates strongly with human expert assessment, and reveals limitations in state-of-the-art LLMs' ability to improve based on feedback.

Conclusion: GREP provides a detailed, localized evaluation approach for AI-generated scientific writing, facilitating better post-training and human-AI collaboration.

Abstract: Expert domain writing, such as scientific writing, typically demands
extensive domain knowledge. Recent advances in LLMs show promising potential in
reducing the expert workload. However, evaluating the quality of automatically
generated scientific writing is a crucial open issue, as it requires knowledge
of domain-specific evaluation criteria and the ability to discern expert
preferences. Conventional automatic metrics and LLM-as-a-judge systems are
insufficient to grasp expert preferences and domain-specific quality standards.
To address this gap and support human-AI collaborative writing, we focus on
related work generation, one of the most challenging scientific tasks, as an
exemplar. We propose GREP, a multi-turn evaluation framework that integrates
classical related work evaluation criteria with expert-specific preferences.
Instead of assigning a single score, our framework decomposes the evaluation
into fine-grained dimensions. This localized evaluation approach is further
augmented with contrastive few-shot examples to provide detailed contextual
guidance for the evaluation dimensions. The design principles allow our
framework to deliver cardinal assessment of quality, which can facilitate
better post-training compared to ordinal preference data. For better
accessibility, we design two variants of GREP: a more precise variant with
proprietary LLMs as evaluators, and a cheaper alternative with open-weight
LLMs. Empirical investigation reveals that our framework is able to assess the
quality of related work sections in a much more robust manner compared to
standard LLM judges, reflects natural scenarios of scientific writing, and
bears a strong correlation with the human expert assessment. We also observe
that generations from state-of-the-art LLMs struggle to satisfy validation
constraints of a suitable related work section. They (mostly) fail to improve
based on feedback as well.

</details>


### [74] [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)
*Changhao Song,Yazhou Zhang,Hui Gao,Ben Yao,Peng Zhang*

Main category: cs.CL

TL;DR: A survey on applying large language models (LLMs) to subjective language tasks, covering definitions, challenges, methods, and future directions.


<details>
  <summary>Details</summary>
Motivation: To review advances in LLMs for subjective language tasks, addressing challenges like ambiguity and context dependence.

Method: Survey of LLM architectures and techniques, with task-specific summaries of definitions, datasets, and methods.

Result: LLMs show promise in modeling nuanced human-like judgments for subjective tasks, but challenges like bias and data limitations remain.

Conclusion: The survey highlights LLMs' potential in subjective language understanding while identifying open issues and future research directions.

Abstract: Subjective language understanding refers to a broad set of natural language
processing tasks where the goal is to interpret or generate content that
conveys personal feelings, opinions, or figurative meanings rather than
objective facts. With the advent of large language models (LLMs) such as
ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach
these inherently nuanced tasks. In this survey, we provide a comprehensive
review of recent advances in applying LLMs to subjective language tasks,
including sentiment analysis, emotion recognition, sarcasm detection, humor
understanding, stance detection, metaphor interpretation, intent detection, and
aesthetics assessment. We begin by clarifying the definition of subjective
language from linguistic and cognitive perspectives, and we outline the unique
challenges posed by subjective language (e.g. ambiguity, figurativeness,
context dependence). We then survey the evolution of LLM architectures and
techniques that particularly benefit subjectivity tasks, highlighting why LLMs
are well-suited to model subtle human-like judgments. For each of the eight
tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based
methods, and remaining challenges. We provide comparative insights, discussing
commonalities and differences among tasks and how multi-task LLM approaches
might yield unified models of subjectivity. Finally, we identify open issues
such as data limitations, model bias, and ethical considerations, and suggest
future research directions. We hope this survey will serve as a valuable
resource for researchers and practitioners interested in the intersection of
affective computing, figurative language processing, and large-scale language
models.

</details>


### [75] [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)
*Matthias Sperber,Maureen de Seyssel,Jiajun Bao,Matthias Paulik*

Main category: cs.CL

TL;DR: The paper explores how human interpreting principles can enhance speech translation systems to bridge the usability gap with human interpreters.


<details>
  <summary>Details</summary>
Motivation: Current speech translation systems lack adaptability and real-world usability compared to human interpreters. Understanding human interpreting can improve these systems.

Method: The study reviews human interpreting literature from a machine translation perspective, analyzing operational and qualitative aspects.

Result: Identifies potential to integrate human interpreting principles into speech translation systems using modern modeling techniques.

Conclusion: The findings aim to inspire progress toward more adaptive and practical machine interpreting systems.

Abstract: Current speech translation systems, while having achieved impressive
accuracies, are rather static in their behavior and do not adapt to real-world
situations in ways human interpreters do. In order to improve their practical
usefulness and enable interpreting-like experiences, a precise understanding of
the nature of human interpreting is crucial. To this end, we discuss human
interpreting literature from the perspective of the machine translation field,
while considering both operational and qualitative aspects. We identify
implications for the development of speech translation systems and argue that
there is great potential to adopt many human interpreting principles using
recent modeling techniques. We hope that our findings provide inspiration for
closing the perceived usability gap, and can motivate progress toward true
machine interpreting.

</details>


### [76] [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)
*David Arps,Hassan Sajjad,Laura Kallmeyer*

Main category: cs.CL

TL;DR: The paper evaluates three Structure-inducing Language Models (SiLMs) on syntactic representation, grammaticality judgment, and training dynamics, finding GPST performs most consistently.


<details>
  <summary>Details</summary>
Motivation: To address gaps in systematic evaluation and comparability of SiLMs, the study compares three architectures.

Method: Evaluates Structformer, UDGN, and GPST using English corpora and synthetic bracketing expressions across three metrics.

Result: GPST outperforms others on long-distance dependencies and shows consistency, while no model dominates all metrics.

Conclusion: Small models trained on synthetic data are useful for evaluating basic properties, and GPST is the most consistent performer.

Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised
language modeling task, and induce a hierarchical sentence representation as a
byproduct when processing an input. A wide variety of SiLMs have been proposed.
However, these have typically been evaluated on a relatively small scale, and
evaluation of these models has systematic gaps and lacks comparability. In this
work, we study three different SiLM architectures using both natural language
(English) corpora and synthetic bracketing expressions: Structformer (Shen et
al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare
them with respect to (i) properties of the induced syntactic representations
(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.
We find that none of the three architectures dominates across all evaluation
metrics. However, there are significant differences, in particular with respect
to the induced syntactic representations. The Generative Pretrained Structured
Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation
settings, and outperforms the other models on long-distance dependencies in
bracketing expressions. Furthermore, our study shows that small models trained
on large amounts of synthetic data provide a useful testbed for evaluating
basic model properties.

</details>


### [77] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
*Jiaxuan Gao,Wei Fu,Minyang Xie,Shusheng Xu,Chuyi He,Zhiyu Mei,Banghua Zhu,Yi Wu*

Main category: cs.CL

TL;DR: ASearcher is an open-source project for large-scale RL training of search agents, improving scalability, efficiency, and data quality in search intelligence tasks.


<details>
  <summary>Details</summary>
Motivation: Existing open-source agents lack expert-level Search Intelligence due to limitations in scalability, efficiency, and data quality.

Method: ASearcher uses scalable fully asynchronous RL training and a prompt-based LLM agent to synthesize high-quality QAs for training.

Result: The QwQ-32B agent achieves 46.7% and 20.8% Avg@4 gains on xBench and GAIA, with extreme long-horizon search capabilities.

Conclusion: ASearcher outperforms existing open-source 32B agents and is made publicly available for further research.

Abstract: Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

</details>


### [78] [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.CL

TL;DR: The paper introduces the Medical Metaphors Corpus (MCC), a dataset of 792 annotated scientific metaphors in medical and biological domains, addressing the gap in domain-specific metaphor detection.


<details>
  <summary>Details</summary>
Motivation: Existing metaphor detection tools focus on general-domain text, neglecting domain-specific needs, particularly in scientific communication.

Method: MCC aggregates metaphors from diverse sources (peer-reviewed literature, news, social media, crowdsourcing) with binary and graded annotations (0-7 scale).

Result: State-of-the-art language models show modest performance on scientific metaphor detection, indicating room for improvement.

Conclusion: MCC supports research in metaphor detection, generation systems, and patient communication tools, advancing domain-specific figurative language understanding.

Abstract: Metaphor is a fundamental cognitive mechanism that shapes scientific
understanding, enabling the communication of complex concepts while potentially
constraining paradigmatic thinking. Despite the prevalence of figurative
language in scientific discourse, existing metaphor detection resources
primarily focus on general-domain text, leaving a critical gap for
domain-specific applications. In this paper, we present the Medical Metaphors
Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual
metaphors spanning medical and biological domains. MCC aggregates metaphorical
expressions from diverse sources including peer-reviewed literature, news
media, social media discourse, and crowdsourced contributions, providing both
binary and graded metaphoricity judgments validated through human annotation.
Each instance includes source-target conceptual mappings and perceived
metaphoricity scores on a 0-7 scale, establishing the first annotated resource
for computational scientific metaphor research. Our evaluation demonstrates
that state-of-the-art language models achieve modest performance on scientific
metaphor detection, revealing substantial room for improvement in
domain-specific figurative language understanding. MCC enables multiple
research applications including metaphor detection benchmarking, quality-aware
generation systems, and patient-centered communication tools.

</details>


### [79] [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
*Ryan Wong,Jiawei Wang,Junjie Zhao,Li Chen,Yan Gao,Long Zhang,Xuan Zhou,Zuo Wang,Kai Xiang,Ge Zhang,Wenhao Huang,Yang Wang,Ke Wang*

Main category: cs.CL

TL;DR: The paper introduces WideSearch, a benchmark to evaluate LLM-powered search agents' reliability in large-scale information collection, revealing significant deficiencies in current systems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for evaluating LLM-powered search agents' performance in wide-context information collection tasks.

Method: Developed WideSearch, a benchmark with 200 manually curated questions across 15 domains, featuring a five-stage quality control pipeline.

Result: Current search agents perform poorly (0-5% success rate), while humans achieve near 100% success with cross-validation.

Conclusion: Existing search agents are unreliable for large-scale tasks, highlighting the need for further research and development.

Abstract: From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of Large Language Models (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable benchmarks. To bridge this gap, we introduce
WideSearch, a new benchmark engineered to evaluate agent reliability on these
large-scale collection tasks. The benchmark features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage quality control pipeline
ensures the difficulty, completeness, and verifiability of the dataset. We
benchmark over 10 state-of-the-art agentic search systems, including
single-agent, multi-agent frameworks, and end-to-end commercial systems. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and benchmark
results have been publicly released at https://widesearch-seed.github.io/

</details>


### [80] [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)
*Mingzi Cao,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: OpT-DeUS uses Optimal Transport to align and fuse Transformer blocks for efficient depth up-scaling of LLMs, outperforming existing methods in performance and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing depth up-scaling methods neglect neuron permutation differences, causing misalignment and performance issues. OpT-DeUS addresses this by aligning layers via Optimal Transport.

Method: Proposes OpT-DeUS, which aligns and fuses Transformer blocks in adjacent base layers using Optimal Transport to create new layers, mitigating neuron permutation mismatch.

Result: OpT-DeUS achieves better performance and training efficiency than existing methods for continual pre-training and fine-tuning across model sizes. Inserting layers closer to the top improves efficiency.

Conclusion: OpT-DeUS effectively mitigates neuron misalignment in depth up-scaling, enhancing performance and training efficiency, especially when new layers are added near the top.

Abstract: Scaling Large Language Models (LLMs) yields performance gains but incurs
substantial training costs. Depth up-scaling offers training efficiency by
adding new layers to pre-trained models. However, most existing methods copy or
average weights from base layers, neglecting neuron permutation differences.
This limitation can potentially cause misalignment that harms performance.
Inspired by applying Optimal Transport (OT) for neuron alignment, we propose
Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses
Transformer blocks in adjacent base layers via OT for new layer creation, to
mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better
overall performance and offers improved training efficiency than existing
methods for continual pre-training and supervised fine-tuning across different
model sizes. To further evaluate the impact of interpolation positions, our
extensive analysis shows that inserting new layers closer to the top results in
higher training efficiency due to shorter back-propagation time while obtaining
additional performance gains.

</details>


### [81] [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)
*Fabrizio Nunnari,Cristina Luna Jiménez,Rosalee Wolfe,John C. McDonald,Michael Filhol,Eleni Efthimiou,Evita Fotinea,Thomas Hanke*

Main category: cs.CL

TL;DR: The SLTAT workshop focuses on advancing deaf/human communication through sign language translation and avatar tech, with contributions in recognition, data, tools, ethics, and affective computing.


<details>
  <summary>Details</summary>
Motivation: Improve deaf/human communication non-invasively by bridging avatar tech and sign language research.

Method: Hosted by IVA, the workshop gathers interdisciplinary contributions on digital humans, recognition, and related fields.

Result: Diverse submissions on avatar tech, sign language recognition, data, tools, ethics, and usability.

Conclusion: SLTAT fosters collaboration between avatar and sign language research, expanding beyond tech to include ethical and usability considerations.

Abstract: The Sign Language Translation and Avatar Technology (SLTAT) workshops
continue a series of gatherings to share recent advances in improving deaf /
human communication through non-invasive means. This 2025 edition, the 9th
since its first appearance in 2011, is hosted by the International Conference
on Intelligent Virtual Agents (IVA), giving the opportunity for contamination
between two research communities, using digital humans as either virtual
interpreters or as interactive conversational agents. As presented in this
summary paper, SLTAT sees contributions beyond avatar technologies, with a
consistent number of submissions on sign language recognition, and other work
on data collection, data analysis, tools, ethics, usability, and affective
computing.

</details>


### [82] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
*Chun Wang,Chenyang Liu,Wenze Xu,Weihong Deng*

Main category: cs.CL

TL;DR: The paper proposes a method to improve speech-language models (SLMs) by disentangling paralinguistic and linguistic information using heterogeneous adapters and a weakly supervised training strategy, achieving competitive performance in emotional conversation tasks.


<details>
  <summary>Details</summary>
Motivation: Current SLMs, built by extending frozen LLMs, fail to capture paralinguistic cues and lose contextual understanding, limiting their effectiveness in emotional and intent-driven conversations.

Method: Introduces two heterogeneous adapters and a weakly supervised training strategy to disentangle paralinguistic and linguistic information, avoiding task-specific vectors through controlled randomness.

Result: The approach demonstrates competitive performance in emotional conversation tasks, effectively integrating paralinguistic and linguistic information while preserving context.

Conclusion: The proposed method enhances SLMs by addressing key issues of entangled information and improper training, enabling better interpretation of speech in contextual settings.

Abstract: Conversational systems relying on text-based large language models (LLMs)
often overlook paralinguistic cues, essential for understanding emotions and
intentions. Speech-language models (SLMs), which use speech as input, are
emerging as a promising solution. However, SLMs built by extending frozen LLMs
struggle to capture paralinguistic information and exhibit reduced context
understanding. We identify entangled information and improper training
strategies as key issues. To address these issues, we propose two heterogeneous
adapters and suggest a weakly supervised training strategy. Our approach
disentangles paralinguistic and linguistic information, enabling SLMs to
interpret speech through structured representations. It also preserves
contextual understanding by avoiding the generation of task-specific vectors
through controlled randomness. This approach trains only the adapters on common
datasets, ensuring parameter and data efficiency. Experiments demonstrate
competitive performance in emotional conversation tasks, showcasing the model's
ability to effectively integrate both paralinguistic and linguistic information
within contextual settings.

</details>


### [83] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
*Lukas Gehring,Benjamin Paaßen*

Main category: cs.CL

TL;DR: The paper benchmarks LLM-generated text detectors in education, introduces the GEDE dataset, and highlights challenges in detecting intermediate student contributions.


<details>
  <summary>Details</summary>
Motivation: Address the need for academic integrity as LLMs make text generation accessible, requiring reliable detection methods in education.

Method: Introduces the GEDE dataset with human and LLM-generated essays, proposes contribution levels, and evaluates state-of-the-art detectors.

Result: Most detectors struggle with intermediate contribution levels (e.g., LLM-improved texts) and produce false positives, posing risks in education.

Conclusion: Current detectors are unreliable for nuanced LLM usage in education, emphasizing the need for improved methods and caution in implementation.

Abstract: Recent advancements in Large Language Models (LLMs) and their increased
accessibility have made it easier than ever for students to automatically
generate texts, posing new challenges for educational institutions. To enforce
norms of academic integrity and ensure students' learning, learning analytics
methods to automatically detect LLM-generated text appear increasingly
appealing. This paper benchmarks the performance of different state-of-the-art
detectors in educational contexts, introducing a novel dataset, called
Generative Essay Detection in Education (GEDE), containing over 900
student-written essays and over 12,500 LLM-generated essays from various
domains. To capture the diversity of LLM usage practices in generating text, we
propose the concept of contribution levels, representing students' contribution
to a given assignment. These levels range from purely human-written texts, to
slightly LLM-improved versions, to fully LLM-generated texts, and finally to
active attacks on the detector by "humanizing" generated texts. We show that
most detectors struggle to accurately classify texts of intermediate student
contribution levels, like LLM-improved human-written texts. Detectors are
particularly likely to produce false positives, which is problematic in
educational settings where false suspicions can severely impact students'
lives. Our dataset, code, and additional supplementary materials are publicly
available at
https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [84] [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)
*Robin Huo,Ewan Dunbar*

Main category: cs.CL

TL;DR: The study compares HuBERT and wav2vec 2.0, focusing on how training iteration (not objective) affects linguistic information encoding in self-supervised speech models.


<details>
  <summary>Details</summary>
Motivation: To understand how model architecture impacts linguistic information in self-supervised speech representations.

Method: Comparison of HuBERT and wav2vec 2.0, analyzing training objective and iterative pseudo-label refinement.

Result: Training iteration, not objective, explains differences in linguistic information encoding.

Conclusion: Future work should explore why iterative refinement effectively encodes linguistic information.

Abstract: Self-supervised models for speech representation learning now see widespread
use for their versatility and performance on downstream tasks, but the effect
of model architecture on the linguistic information learned in their
representations remains under-studied. This study investigates two such models,
HuBERT and wav2vec 2.0, and minimally compares two of their architectural
differences: training objective and iterative pseudo-label refinement through
multiple training iterations. We find that differences in canonical correlation
of hidden representations to word identity, phoneme identity, and speaker
identity are explained by training iteration, not training objective. We
suggest that future work investigate the reason for the effectiveness of
iterative refinement in encoding linguistic information in self-supervised
speech representations.

</details>


### [85] [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)
*Jakub Šmíd,Pavel Přibáň,Ondřej Pražák,Pavel Král*

Main category: cs.CL

TL;DR: A new Czech dataset for ABSA with 3.1K annotated restaurant reviews, supporting complex tasks like target-aspect-category detection, and 24M unannotated reviews for unsupervised learning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a Czech dataset for advanced ABSA tasks and enable cross-lingual comparisons.

Method: Built upon an older dataset, using a unified annotation format (SemEval-2016 style) with two trained annotators (90% agreement).

Result: High-quality dataset with robust monolingual baselines from Transformer models and error analysis.

Conclusion: The dataset and code are publicly available, fostering research in ABSA and cross-lingual studies.

Abstract: In this paper, we introduce a novel Czech dataset for aspect-based sentiment
analysis (ABSA), which consists of 3.1K manually annotated reviews from the
restaurant domain. The dataset is built upon the older Czech dataset, which
contained only separate labels for the basic ABSA tasks such as aspect term
extraction or aspect polarity detection. Unlike its predecessor, our new
dataset is specifically designed for more complex tasks, e.g.
target-aspect-category detection. These advanced tasks require a unified
annotation format, seamlessly linking sentiment elements (labels) together. Our
dataset follows the format of the well-known SemEval-2016 datasets. This design
choice allows effortless application and evaluation in cross-lingual scenarios,
ultimately fostering cross-language comparisons with equivalent counterpart
datasets in other languages. The annotation process engaged two trained
annotators, yielding an impressive inter-annotator agreement rate of
approximately 90%. Additionally, we provide 24M reviews without annotations
suitable for unsupervised learning. We present robust monolingual baseline
results achieved with various Transformer-based models and insightful error
analysis to supplement our contributions. Our code and dataset are freely
available for non-commercial research purposes.

</details>


### [86] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
*Wenze Xu,Chun Wang,Jiazhen Yu,Sheng Chen,Liang Gao,Weihong Deng*

Main category: cs.CL

TL;DR: The paper introduces Optimal Transport Regularization (OTReg) to improve Spoken Language Models (SLMs) by aligning speech and text embeddings, addressing the modality gap and enhancing generalization.


<details>
  <summary>Details</summary>
Motivation: SLMs struggle to generalize across datasets due to the modality gap between speech and text representations, raising concerns about their intended text-like processing of speech.

Method: OTReg formulates speech-text alignment as an optimal transport problem, deriving a regularization loss to improve SLM training without additional labels or parameters.

Result: OTReg enhances speech-text alignment, mitigates the modality gap, and improves SLM generalization across diverse datasets in multilingual ASR experiments.

Conclusion: OTReg is a lightweight, effective method to improve SLM training and generalization by addressing the speech-text modality gap.

Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to
perceive speech inputs, have gained increasing attention for their potential to
advance speech understanding tasks. However, despite recent progress, studies
show that SLMs often struggle to generalize across datasets, even for trained
languages and tasks, raising concerns about whether they process speech in a
text-like manner as intended. A key challenge underlying this limitation is the
modality gap between speech and text representations. The high variability in
speech embeddings may allow SLMs to achieve strong in-domain performance by
exploiting unintended speech variations, ultimately hindering generalization.
To mitigate this modality gap, we introduce Optimal Transport Regularization
(OTReg), a method that formulates speech-text alignment as an optimal transport
problem and derives a regularization loss to improve SLM training. In each
training iteration, OTReg first establishes a structured correspondence between
speech and transcript embeddings by determining the optimal transport plan,
then incorporates the regularization loss based on this transport plan to
optimize SLMs in generating speech embeddings that align more effectively with
transcript embeddings. OTReg is lightweight, requiring no additional labels or
learnable parameters, and integrates seamlessly into existing SLM training
procedures. Extensive multilingual ASR experiments demonstrate that OTReg
enhances speech-text alignment, mitigates the modality gap, and consequently
improves SLM generalization across diverse datasets.

</details>


### [87] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
*Tianyi Zhou,Johanne Medina,Sanjay Chawla*

Main category: cs.CL

TL;DR: The paper investigates how in-context information affects LLM behavior and proposes a method using token-level uncertainty to predict response reliability.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate incorrect but fluent content (confabulation), posing risks in multi-turn or agentic applications where outputs are reused.

Method: Proposes reliability estimation using aleatoric and epistemic uncertainty from output logits to aggregate hidden states for reliability prediction.

Result: Correct context improves accuracy and confidence, while misleading context leads to confidently incorrect responses. The method improves unreliable output detection.

Conclusion: Highlights limitations of direct uncertainty signals and the potential of uncertainty-guided probing for reliable generation.

Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [88] [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)
*Jun Wang,Zaifu Zhan,Qixin Zhang,Mingquan Lin,Meijia Song,Rui Zhang*

Main category: cs.CL

TL;DR: Dual-Div improves biomedical NLP task performance by enhancing demonstration diversity in ICL, outperforming baselines by up to 5% in macro-F1 scores.


<details>
  <summary>Details</summary>
Motivation: Existing approaches prioritize representativeness over diversity in example selection for biomedical ICL, limiting performance.

Method: Dual-Div uses a two-stage retrieval and ranking process to optimize diversity and representativeness, selecting 3-5 demonstrations.

Result: Dual-Div achieves up to 5% higher macro-F1 scores on biomedical NLP tasks (NER, RE, TC) and shows robustness to prompt changes.

Conclusion: Diversity in initial retrieval is more critical than ranking optimization, and limiting demonstrations to 3-5 examples maximizes efficiency.

Abstract: Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.

</details>


### [89] [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)
*Wentao Jiang,Xiang Feng,Zengmao Wang,Yong Luo,Pingbo Xu,Zhe Chen,Bo Du,Jing Zhang*

Main category: cs.CL

TL;DR: REX-RAG improves RL-integrated LLMs by addressing dead-end reasoning paths with mixed sampling and policy correction, achieving performance gains on QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs in RL often get stuck in unproductive reasoning paths (dead ends), hindering exploration and policy optimization.

Method: Proposes REX-RAG with mixed sampling (probe sampling + exploratory prompts) and policy correction (importance sampling) to mitigate dead ends and distribution shifts.

Result: Achieves average gains of 5.1% (Qwen2.5-3B) and 3.6% (Qwen2.5-7B) over baselines on QA benchmarks.

Conclusion: REX-RAG effectively enhances reasoning exploration and policy learning in RL-augmented LLMs, demonstrating competitive performance.

Abstract: Reinforcement learning (RL) is emerging as a powerful paradigm for enabling
large language models (LLMs) to perform complex reasoning tasks. Recent
advances indicate that integrating RL with retrieval-augmented generation (RAG)
allows LLMs to dynamically incorporate external knowledge, leading to more
informed and robust decision making. However, we identify a critical challenge
during policy-driven trajectory sampling: LLMs are frequently trapped in
unproductive reasoning paths, which we refer to as "dead ends", committing to
overconfident yet incorrect conclusions. This severely hampers exploration and
undermines effective policy optimization. To address this challenge, we propose
REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented
Generation), a novel framework that explores alternative reasoning paths while
maintaining rigorous policy learning through principled distributional
corrections. Our approach introduces two key innovations: (1) Mixed Sampling
Strategy, which combines a novel probe sampling method with exploratory prompts
to escape dead ends; and (2) Policy Correction Mechanism, which employs
importance sampling to correct distribution shifts induced by mixed sampling,
thereby mitigating gradient estimation bias. We evaluate it on seven
question-answering benchmarks, and the experimental results show that REX-RAG
achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B
over strong baselines, demonstrating competitive results across multiple
datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

</details>


### [90] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
*Mandira Sawkar,Samay U. Shetty,Deepak Pandita,Tharindu Cyril Weerasooriya,Christopher M. Homan*

Main category: cs.CL

TL;DR: The paper extends DisCo to model annotator disagreement by incorporating metadata, enhancing inputs, and modifying loss functions, showing improved performance in soft and perspectivist evaluations.


<details>
  <summary>Details</summary>
Motivation: To better model annotator disagreement in labeled datasets by improving the DisCo architecture.

Method: Extends DisCo by adding annotator metadata, improving input representations, and adjusting loss functions.

Result: Substantial improvements in soft and perspectivist metrics across three datasets, with detailed error and calibration analyses.

Conclusion: Disagreement-aware modeling is valuable, and system components interact with human-annotated data complexity.

Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model
annotator disagreement through soft label distribution prediction and
perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution
from Context), a neural architecture that jointly models item-level and
annotator-level label distributions, and present detailed analysis and
improvements. In this paper, we extend the DisCo by incorporating annotator
metadata, enhancing input representations, and modifying the loss functions to
capture disagreement patterns better. Through extensive experiments, we
demonstrate substantial improvements in both soft and perspectivist evaluation
metrics across three datasets. We also conduct in-depth error and calibration
analyses, highlighting the conditions under which improvements occur. Our
findings underscore the value of disagreement-aware modeling and offer insights
into how system components interact with the complexity of human-annotated
data.

</details>


### [91] [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)
*Bangsheng Tang,Carl Chengyan Fu,Fei Kou,Grigory Sizov,Haoci Zhang,Jason Park,Jiawen Liu,Jie You,Qirui Yang,Sachin Mehta,Shengyong Cai,Xiaodong Wang,Xingyu Liu,Yunlu Li,Yanjun Zhou,Wei Wei,Zhiwei Zhao,Zixi Qi,Adolfo Victoria,Aya Ibrahim,Bram Wasti,Changkyu Kim,Daniel Haziza,Fei Sun,Giancarlo Delfin,Emily Guo,Jialin Ouyang,Jaewon Lee,Jianyu Huang,Jeremy Reizenstein,Lu Fang,Quinn Zhu,Ria Verma,Vlad Mihailescu,Xingwen Guo,Yan Cui,Ye Hu,Yejin Lee*

Main category: cs.CL

TL;DR: The paper details optimizations for EAGLE-based speculative decoding in Llama models, achieving state-of-the-art inference latency and speed-ups for large batch sizes.


<details>
  <summary>Details</summary>
Motivation: Scaling speculative decoding for production environments involves engineering challenges like efficient GPU implementation of operations like tree attention and multi-round speculative decoding.

Method: Implemented training and inference optimization techniques for EAGLE-based speculative decoding in Llama models.

Result: Achieved 4 ms per token latency (10% faster than prior methods) and 1.4x-2.0x speed-up for large batch sizes.

Conclusion: The optimizations enable efficient production-scale speculative decoding, setting a new benchmark for Llama model inference.

Abstract: Speculative decoding is a standard method for accelerating the inference
speed of large language models. However, scaling it for production environments
poses several engineering challenges, including efficiently implementing
different operations (e.g., tree attention and multi-round speculative
decoding) on GPU. In this paper, we detail the training and inference
optimization techniques that we have implemented to enable EAGLE-based
speculative decoding at a production scale for Llama models. With these
changes, we achieve a new state-of-the-art inference latency for Llama models.
For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a
batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the
previously best known method. Furthermore, for EAGLE-based speculative
decoding, our optimizations enable us to achieve a speed-up for large batch
sizes between 1.4x and 2.0x at production scale.

</details>


### [92] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
*Kyle Moore,Jesse Roberts,Daryl Watson*

Main category: cs.CL

TL;DR: The paper evaluates inference-time uncertainty measures in large language models (LLMs) to align with human uncertainty and traditional calibration, finding strong alignment in some metrics despite mismatches with human preferences.


<details>
  <summary>Details</summary>
Motivation: To improve LLM-user interaction by ensuring model uncertainty aligns with human uncertainty and traditional calibration metrics.

Method: Evaluates various inference-time uncertainty measures using established and novel metrics, comparing them to human group-level uncertainty and model calibration.

Result: Some measures strongly align with human uncertainty, showing moderate to strong calibration in correctness and distributional analysis, even without alignment to human answer preferences.

Conclusion: Certain uncertainty metrics effectively align with human uncertainty and calibration, suggesting practical utility for enhancing LLM-user trust and control.

Abstract: There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.

</details>


### [93] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
*Zhuohao Yu,Xingru Jiang,Weizheng Gu,Yidong Wang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: SAEMark is a post-hoc multi-bit watermarking framework for LLM-generated text, preserving quality and working with closed-source models without logit manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods degrade text quality and require white-box access, limiting their use with API-based models and multilingual scenarios.

Method: SAEMark uses feature-based rejection sampling to embed messages without altering model logits or requiring training, leveraging deterministic features from generated text.

Result: Achieves 99.7% F1 on English and strong multi-bit detection accuracy across 4 datasets, preserving text quality.

Conclusion: SAEMark offers a scalable, out-of-the-box solution for watermarking closed-source LLMs, enabling content attribution without compromising quality.

Abstract: Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.

</details>


### [94] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
*Shansong Wang,Mingzhe Hu,Qiang Li,Mojtaba Safari,Xiaofeng Yang*

Main category: cs.CL

TL;DR: GPT-5 outperforms GPT-4o and human experts in medical QA tasks, showing strong multimodal reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To evaluate GPT-5's zero-shot reasoning in medical decision-making, integrating text and visual data.

Method: Benchmarked GPT-5 variants against standardized medical QA datasets (MedQA, MedXpertQA, etc.) and human experts.

Result: GPT-5 achieved state-of-the-art accuracy, surpassing GPT-4o and human experts in reasoning and understanding.

Conclusion: GPT-5's performance suggests potential for advanced clinical decision-support systems.

Abstract: Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

</details>


### [95] [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)
*Yunna Cai,Fan Wang,Haowei Wang,Kun Wang,Kailai Yang,Sophia Ananiadou,Moyan Li,Mingming Fan*

Main category: cs.CL

TL;DR: PsyCrisis-Bench is a reference-free benchmark for evaluating LLM safety alignment in mental health dialogues, using expert-defined principles and a prompt-based LLM-as-Judge approach.


<details>
  <summary>Details</summary>
Motivation: The challenge of evaluating LLM safety in high-risk mental health dialogues due to missing gold standards and ethical sensitivity.

Method: A prompt-based LLM-as-Judge approach with expert-defined reasoning chains and binary point-wise scoring for explainability.

Result: Achieves highest agreement with expert assessments and produces interpretable rationales in 3600 judgments.

Conclusion: PsyCrisis-Bench and its dataset facilitate safer and more interpretable LLM evaluations in mental health contexts.

Abstract: Evaluating the safety alignment of LLM responses in high-risk mental health
dialogues is particularly difficult due to missing gold-standard answers and
the ethically sensitive nature of these interactions. To address this
challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark
based on real-world Chinese mental health dialogues. It evaluates whether the
model responses align with the safety principles defined by experts.
Specifically designed for settings without standard references, our method
adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation
using expert-defined reasoning chains grounded in psychological intervention
principles. We employ binary point-wise scoring across multiple safety
dimensions to enhance the explainability and traceability of the evaluation.
Additionally, we present a manually curated, high-quality Chinese-language
dataset covering self-harm, suicidal ideation, and existential distress,
derived from real-world online discourse. Experiments on 3600 judgments show
that our method achieves the highest agreement with expert assessments and
produces more interpretable evaluation rationales compared to existing
approaches. Our dataset and evaluation tool are publicly available to
facilitate further research.

</details>


### [96] [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
*Jiahao Zhao,Liwei Dong*

Main category: cs.CL

TL;DR: Jinx is a helpful-only language model variant designed for researchers to evaluate alignment failures and safety boundaries in language models.


<details>
  <summary>Details</summary>
Motivation: To provide researchers with an accessible tool for probing alignment failures and studying safety issues in language models, as current unlimited models are not publicly available.

Method: Develop Jinx, a helpful-only variant of open-weight LLMs that responds to all queries without refusals or safety filtering, while retaining the base model's capabilities.

Result: Jinx serves as a tool for researchers to systematically evaluate alignment failures and safety boundaries in language models.

Conclusion: Jinx addresses the lack of accessible unlimited models for research, enabling better alignment evaluation and safety studies.

Abstract: Unlimited, or so-called helpful-only language models are trained without
safety alignment constraints and never refuse user queries. They are widely
used by leading AI companies as internal tools for red teaming and alignment
evaluation. For example, if a safety-aligned model produces harmful outputs
similar to an unlimited model, this indicates alignment failures that require
further attention. Despite their essential role in assessing alignment, such
models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx
responds to all queries without refusals or safety filtering, while preserving
the base model's capabilities in reasoning and instruction following. It
provides researchers with an accessible tool for probing alignment failures,
evaluating safety boundaries, and systematically studying failure modes in
language model safety.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [97] [Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG](https://arxiv.org/abs/2508.06496)
*Rakesh Raj Madavan,Akshat Kaimal,Hashim Faisal,Chandrakala S*

Main category: cs.CV

TL;DR: BIND and Med-GRIM improve medical VQA by refining joint embeddings and using graph-based retrieval with SLMs for efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard VLMs lack precision for domain-specific tasks like medical VQA.

Method: Extends multimodal work with dense encodings, uses graph-based retrieval and prompt engineering with SLMs.

Result: Achieves LLM performance with lower compute cost; introduces DermaGraph dataset.

Conclusion: Med-GRIM offers efficient, accurate medical VQA; DermaGraph supports scalable research.

Abstract: An ensemble of trained multimodal encoders and vision-language models (VLMs)
has become a standard approach for visual question answering (VQA) tasks.
However, such models often fail to produce responses with the detailed
precision necessary for complex, domain-specific applications such as medical
VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,
extends prior multimodal work by refining the joint embedding space through
dense, query-token-based encodings inspired by contrastive pretraining
techniques. This refined encoder powers Med-GRIM, a model designed for medical
VQA tasks that leverages graph-based retrieval and prompt engineering to
integrate domain-specific knowledge. Rather than relying on compute-heavy
fine-tuning of vision and language models on specific datasets, Med-GRIM
applies a low-compute, modular workflow with small language models (SLMs) for
efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject
relevant knowledge, ensuring both accuracy and robustness in its responses. By
assigning distinct roles to each agent within the VQA system, Med-GRIM achieves
large language model performance at a fraction of the computational cost.
Additionally, to support scalable research in zero-shot multimodal medical
applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising
diverse dermatological conditions. This dataset facilitates both multimodal and
unimodal querying. The code and dataset are available at:
https://github.com/Rakesh-123-cryp/Med-GRIM.git

</details>


### [98] [DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation](https://arxiv.org/abs/2508.06511)
*He Feng,Yongjia Ma,Donglin Di,Lei Fan,Tonghua Su,Xiangqian Wu*

Main category: cs.CV

TL;DR: DiTalker is a DiT-based framework for portrait animation, improving lip sync and dynamic style control by decoupling audio and style features.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on lip sync or static emotions, neglecting dynamic styles like head movements and adding computational overhead with dual U-Net architectures.

Method: Proposes DiTalker with a Style-Emotion Encoding Module (separate branches for style and emotion) and an Audio-Style Fusion Module (parallel cross-attention layers). Uses optimization constraints for lip sync and detail preservation.

Result: DiTalker outperforms in lip synchronization and speaking style controllability.

Conclusion: DiTalker offers a unified, efficient solution for dynamic portrait animation with precise lip sync and style control.

Abstract: Portrait animation aims to synthesize talking videos from a static reference
face, conditioned on audio and style frame cues (e.g., emotion and head poses),
while ensuring precise lip synchronization and faithful reproduction of
speaking styles. Existing diffusion-based portrait animation methods primarily
focus on lip synchronization or static emotion transformation, often
overlooking dynamic styles such as head movements. Moreover, most of these
methods rely on a dual U-Net architecture, which preserves identity consistency
but incurs additional computational overhead. To this end, we propose DiTalker,
a unified DiT-based framework for speaking style-controllable portrait
animation. We design a Style-Emotion Encoding Module that employs two separate
branches: a style branch extracting identity-specific style information (e.g.,
head poses and movements), and an emotion branch extracting identity-agnostic
emotion features. We further introduce an Audio-Style Fusion Module that
decouples audio and speaking styles via two parallel cross-attention layers,
using these features to guide the animation process. To enhance the quality of
results, we adopt and modify two optimization constraints: one to improve lip
synchronization and the other to preserve fine-grained identity and background
details. Extensive experiments demonstrate the superiority of DiTalker in terms
of lip synchronization and speaking style controllability. Project Page:
https://thenameishope.github.io/DiTalker/

</details>


### [99] [BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok](https://arxiv.org/abs/2508.06515)
*Minh Duc Chu,Kshitij Pawar,Zihao He,Roxanna Sharifi,Ross Sonnenblick,Magdalayna Curry,Laura D'Adamo,Lindsay Young,Stuart B Murray,Kristina Lerman*

Main category: cs.CV

TL;DR: BigTokDetect is a framework for detecting pro-bigorexia content on TikTok using multimodal analysis, achieving high accuracy with expert-annotated data.


<details>
  <summary>Details</summary>
Motivation: Social media struggles to detect harmful pro-bigorexia content disguised as fitness material, especially affecting adolescent males.

Method: Developed BigTokDetect, a clinically-informed framework, using a multimodal dataset (BigTok) of 2,200 TikTok videos labeled by experts. Evaluated vision-language models for classification.

Result: Achieved 0.829% accuracy for primary category and 0.690% for subcategory detection. Multimodal fusion improved performance by 5-10% over text-only methods.

Conclusion: Established benchmarks for multimodal harmful content detection, providing tools for scalable moderation in mental health domains.

Abstract: Social media platforms increasingly struggle to detect harmful content that
promotes muscle dysmorphic behaviors, particularly pro-bigorexia content that
disproportionately affects adolescent males. Unlike traditional eating disorder
detection focused on the "thin ideal," pro-bigorexia material masquerades as
legitimate fitness content through complex multimodal combinations of visual
displays, coded language, and motivational messaging that evade text-based
detection systems. We address this challenge by developing BigTokDetect, a
clinically-informed detection framework for identifying pro-bigorexia content
on TikTok. We introduce BigTok, the first expert-annotated multimodal dataset
of over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists
across five primary categories spanning body image, nutrition, exercise,
supplements, and masculinity. Through a comprehensive evaluation of
state-of-the-art vision language models, we achieve 0.829% accuracy on primary
category classification and 0.690% on subcategory detection via domain-specific
finetuning. Our ablation studies demonstrate that multimodal fusion improves
performance by 5-10% over text-only approaches, with video features providing
the most discriminative signals. These findings establish new benchmarks for
multimodal harmful content detection and provide both the computational tools
and methodological framework needed for scalable content moderation in
specialized mental health domains.

</details>


### [100] [Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation](https://arxiv.org/abs/2508.06517)
*Haoran Xi,Chen Liu,Xiaolin Li*

Main category: cs.CV

TL;DR: FPGM introduces a frequency-based augmentation framework for polyp segmentation, improving cross-domain robustness and achieving state-of-the-art results with limited supervision.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of limited annotated data and domain shift in polyp segmentation, which hinder robust model development.

Method: FPGM uses a two-stage process: learning a domain-invariant frequency prior from labeled polyp edges and applying spectral perturbations to unlabeled images to align their amplitude spectra with this prior.

Result: FPGM achieves a new state-of-the-art, with over 10% absolute gain in Dice score in data-scarce scenarios and exceptional zero-shot generalization.

Conclusion: FPGM enhances cross-domain robustness, offering a clinically viable solution for polyp segmentation under limited supervision.

Abstract: Automated polyp segmentation is essential for early diagnosis of colorectal
cancer, yet developing robust models remains challenging due to limited
annotated data and significant performance degradation under domain shift.
Although semi-supervised learning (SSL) reduces annotation requirements,
existing methods rely on generic augmentations that ignore polyp-specific
structural properties, resulting in poor generalization to new imaging centers
and devices. To address this, we introduce Frequency Prior Guided Matching
(FPGM), a novel augmentation framework built on a key discovery: polyp edges
exhibit a remarkably consistent frequency signature across diverse datasets.
FPGM leverages this intrinsic regularity in a two-stage process. It first
learns a domain-invariant frequency prior from the edge regions of labeled
polyps. Then, it performs principled spectral perturbations on unlabeled
images, aligning their amplitude spectra with this learned prior while
preserving phase information to maintain structural integrity. This targeted
alignment normalizes domain-specific textural variations, thereby compelling
the model to learn the underlying, generalizable anatomical structure.
Validated on six public datasets, FPGM establishes a new state-of-the-art
against ten competing methods. It demonstrates exceptional zero-shot
generalization capabilities, achieving over 10% absolute gain in Dice score in
data-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM
presents a powerful solution for clinically deployable polyp segmentation under
limited supervision.

</details>


### [101] [Large Language Models Facilitate Vision Reflection in Image Classification](https://arxiv.org/abs/2508.06525)
*Guoyuan An,JaeYoon Kim,SungEui Yoon*

Main category: cs.CV

TL;DR: The paper explores how prompting large multimodal models (LMMs) to verify predictions improves accuracy, analyzes vision reflection's internal behavior, and shows training-free connectors enhance LMM performance.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the explainability and performance of vision reflection in LMMs, despite their typical underperformance compared to dedicated vision encoders.

Method: Prompting LMMs to verify predictions, analyzing vision reflection's behavior, and testing training-free connectors for fine-grained recognition.

Result: Improved recognition accuracy, insights into vision-language mapping, and enhanced LMM performance without extensive training.

Conclusion: Vision reflection is a promising strategy for robust and interpretable visual recognition in LMMs.

Abstract: This paper presents several novel findings on the explainability of vision
reflection in large multimodal models (LMMs). First, we show that prompting an
LMM to verify the prediction of a specialized vision model can improve
recognition accuracy, even on benchmarks like ImageNet, despite prior evidence
that LMMs typically underperform dedicated vision encoders. Second, we analyze
the internal behavior of vision reflection and find that the vision-language
connector maps visual features into explicit textual concepts, allowing the
language model to reason about prediction plausibility using commonsense
knowledge. We further observe that replacing a large number of vision tokens
with only a few text tokens still enables LLaVA to generate similar answers,
suggesting that LMMs may rely primarily on a compact set of distilled textual
representations rather than raw vision features. Third, we show that a
training-free connector can enhance LMM performance in fine-grained recognition
tasks, without extensive feature-alignment training. Together, these findings
offer new insights into the explainability of vision-language models and
suggest that vision reflection is a promising strategy for achieving robust and
interpretable visual recognition.

</details>


### [102] [A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition](https://arxiv.org/abs/2508.06528)
*Xiuliang Zhang,Tadiwa Elisha Nyamasvisva,Chuntao Liu*

Main category: cs.CV

TL;DR: A hybrid framework combining 3D CNN and Transformer improves video behavior recognition by leveraging local and global features.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D CNNs lack long-range dependency modeling, while Transformers are computationally expensive. A hybrid approach aims to balance these limitations.

Method: The framework integrates 3D CNN for local spatiotemporal features and Transformer for long-range dependencies, with a fusion mechanism.

Result: Outperforms standalone 3D CNN and Transformers in accuracy, with manageable complexity. Ablation studies confirm complementary strengths.

Conclusion: The hybrid framework is effective and scalable for video-based behavior recognition.

Abstract: Video-based behavior recognition is essential in fields such as public
safety, intelligent surveillance, and human-computer interaction. Traditional
3D Convolutional Neural Network (3D CNN) effectively capture local
spatiotemporal features but struggle with modeling long-range dependencies.
Conversely, Transformers excel at learning global contextual information but
face challenges with high computational costs. To address these limitations, we
propose a hybrid framework combining 3D CNN and Transformer architectures. The
3D CNN module extracts low-level spatiotemporal features, while the Transformer
module captures long-range temporal dependencies, with a fusion mechanism
integrating both representations. Evaluated on benchmark datasets, the proposed
model outperforms traditional 3D CNN and standalone Transformers, achieving
higher recognition accuracy with manageable complexity. Ablation studies
further validate the complementary strengths of the two modules. This hybrid
framework offers an effective and scalable solution for video-based behavior
recognition.

</details>


### [103] [RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving](https://arxiv.org/abs/2508.06529)
*Jiayuan Wang,Q. M. Jonathan Wu,Katsuya Suto,Ning Zhang*

Main category: cs.CV

TL;DR: RMT-PPAD is a real-time, transformer-based multi-task model for autonomous driving perception, excelling in object detection, drivable area segmentation, and lane line segmentation with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the need for precision and real-time performance in panoptic driving perception, while mitigating negative transfer between tasks and avoiding manual design of task-specific structures.

Method: Introduces a lightweight gate control with an adapter for adaptive feature fusion, an adaptive segmentation decoder for multi-scale feature learning, and resolves label inconsistency in lane line segmentation.

Result: Achieves mAP50 of 84.9%, Recall of 95.4% for object detection; mIoU of 92.6% for drivable area segmentation; IoU of 56.8% and accuracy of 84.7% for lane line segmentation at 32.6 FPS.

Conclusion: RMT-PPAD delivers stable, high-performance results in real-world scenarios, with released source code and pre-trained models.

Abstract: Autonomous driving systems rely on panoptic driving perception that requires
both precision and real-time performance. In this work, we propose RMT-PPAD, a
real-time, transformer-based multi-task model that jointly performs object
detection, drivable area segmentation, and lane line segmentation. We introduce
a lightweight module, a gate control with an adapter to adaptively fuse shared
and task-specific features, effectively alleviating negative transfer between
tasks. Additionally, we design an adaptive segmentation decoder to learn the
weights over multi-scale features automatically during the training stage. This
avoids the manual design of task-specific structures for different segmentation
tasks. We also identify and resolve the inconsistency between training and
testing labels in lane line segmentation. This allows fairer evaluation.
Experiments on the BDD100K dataset demonstrate that RMT-PPAD achieves
state-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object
detection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and
accuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6
FPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD
performance in practice. The results show that RMT-PPAD consistently delivers
stable performance. The source codes and pre-trained models are released at
https://github.com/JiayuanWang-JW/RMT-PPAD.

</details>


### [104] [What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?](https://arxiv.org/abs/2508.06530)
*Ming-Kun Xie,Jia-Hao Xiao,Gang Niu,Lei Feng,Zhiqiang Kou,Min-Ling Zhang,Masashi Sugiyama*

Main category: cs.CV

TL;DR: The paper introduces HOPE, a new benchmark to rigorously assess object hallucination in LVLMs by generating misleading distractors, outperforming the existing POPE benchmark.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks like POPE are ineffective in evaluating object hallucination in LVLMs due to simplistic sampling strategies.

Method: HOPE uses content-aware hallucination searching (leveraging CLIP) and description-based hallucination searching to create misleading distractors.

Result: HOPE causes a precision drop of 9-23% in LVLMs, significantly outperforming POPE.

Conclusion: HOPE provides a more rigorous and effective benchmark for assessing LVLMs' hallucination vulnerabilities.

Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large
Language Models (LLMs), have achieved impressive performance across domains.
Despite the great advances in LVLMs, they still suffer from the unavailable
object hallucination issue, which tends to generate objects inconsistent with
the image content. The most commonly used Polling-based Object Probing
Evaluation (POPE) benchmark evaluates this issue by sampling negative
categories according to category-level statistics, \textit{e.g.}, category
frequencies and co-occurrence. However, with the continuous advancement of
LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing
object hallucination, as it employs a simplistic sampling strategy that
overlooks image-specific information and restricts distractors to negative
object categories only. In this paper, we introduce the Hallucination
searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate
the most misleading distractors (\textit{i.e.}, non-existent objects or
incorrect image descriptions) that can trigger hallucination in LVLMs, which
serves as a means to more rigorously assess their immunity to hallucination. To
explore the image-specific information, the content-aware hallucination
searching leverages Contrastive Language-Image Pre-Training (CLIP) to
approximate the predictive behavior of LVLMs by selecting negative objects with
the highest predicted likelihood as distractors. To expand the scope of
hallucination assessment, the description-based hallucination searching
constructs highly misleading distractors by pairing true objects with false
descriptions. Experimental results show that HOPE leads to a precision drop of
at least 9\% and up to 23\% across various state-of-the-art LVLMs,
significantly outperforming POPE in exposing hallucination vulnerabilities. The
code is available at https://github.com/xiemk/HOPE.

</details>


### [105] [Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset](https://arxiv.org/abs/2508.06537)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: Benchmarking object detection models on MobilTelesco, a sparse night-sky dataset, reveals challenges in feature-deficient conditions compared to traditional datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional datasets (e.g., ImageNet, COCO) lack signal sparsity, limiting applicability in non-commercial domains like astrophotography.

Method: MobilTelesco, a smartphone-based astrophotography dataset, is introduced to evaluate detection models under sparse conditions.

Result: Models face significant challenges in feature-deficient scenarios, as shown by benchmarks on MobilTelesco.

Conclusion: Sparse datasets like MobilTelesco expose limitations of current detection models, urging improvements for non-commercial applications.

Abstract: Object detection models are typically trained on datasets like ImageNet,
COCO, and PASCAL VOC, which focus on everyday objects. However, these lack
signal sparsity found in non-commercial domains. MobilTelesco, a
smartphone-based astrophotography dataset, addresses this by providing sparse
night-sky images. We benchmark several detection models on it, highlighting
challenges under feature-deficient conditions.

</details>


### [106] [MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing](https://arxiv.org/abs/2508.06543)
*Jinghan Yu,Zhiyuan Ma,Yue Ma,Kaiqi Liu,Yuhan Wang,Jianjun Li*

Main category: cs.CV

TL;DR: The paper introduces MILD, a method for human erasing in complex multi-IP scenarios, addressing dataset limitations and spatial decoupling challenges with a novel dataset and diffusion-based approach.


<details>
  <summary>Details</summary>
Motivation: Prior methods struggle with complex human-human occlusions, human-object entanglements, and background interferences due to dataset limitations and lack of spatial decoupling.

Method: Proposes Multi-Layer Diffusion (MILD), decomposing generation into semantically separated pathways, and introduces Human Morphology Guidance and Spatially-Modulated Attention.

Result: MILD outperforms state-of-the-art methods on challenging human erasing benchmarks.

Conclusion: The work advances human erasing in complex scenarios by addressing key limitations and introducing innovative techniques.

Abstract: Recent years have witnessed the success of diffusion models in
image-customized tasks. Prior works have achieved notable progress on
human-oriented erasing using explicit mask guidance and semantic-aware
inpainting. However, they struggle under complex multi-IP scenarios involving
human-human occlusions, human-object entanglements, and background
interferences. These challenges are mainly due to: 1) Dataset limitations, as
existing datasets rarely cover dense occlusions, camouflaged backgrounds, and
diverse interactions; 2) Lack of spatial decoupling, where foreground instances
cannot be effectively disentangled, limiting clean background restoration. In
this work, we introduce a high-quality multi-IP human erasing dataset with
diverse pose variations and complex backgrounds. We then propose Multi-Layer
Diffusion (MILD), a novel strategy that decomposes generation into semantically
separated pathways for each instance and the background. To enhance
human-centric understanding, we introduce Human Morphology Guidance,
integrating pose, parsing, and spatial relations. We further present
Spatially-Modulated Attention to better guide attention flow. Extensive
experiments show that MILD outperforms state-of-the-art methods on challenging
human erasing benchmarks.

</details>


### [107] [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/abs/2508.06546)
*Qi Xun Yeo,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: A method for 3D semantic scene graph estimation using multi-view RGB images without ground truth 3D annotations, focusing on robust feature extraction and noise reduction.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of 3D semantic scene graph estimation without relying on ground truth 3D annotations, using only multi-view RGB images.

Method: Utilizes semantic masks for feature aggregation, incorporates neighboring node information, and leverages statistical priors for refining predictions.

Result: Outperforms current methods relying solely on multi-view images as input.

Conclusion: The proposed approach effectively estimates 3D semantic scene graphs from multi-view RGB images by enhancing feature robustness and reducing noise.

Abstract: Modern 3D semantic scene graph estimation methods utilize ground truth 3D
annotations to accurately predict target objects, predicates, and
relationships. In the absence of given 3D ground truth representations, we
explore leveraging only multi-view RGB images to tackle this task. To attain
robust features for accurate scene graph estimation, we must overcome the noisy
reconstructed pseudo point-based geometry from predicted depth maps and reduce
the amount of background noise present in multi-view image features. The key is
to enrich node and edge features with accurate semantic and spatial information
and through neighboring relations. We obtain semantic masks to guide feature
aggregation to filter background features and design a novel method to
incorporate neighboring node information to aid robustness of our scene graph
estimates. Furthermore, we leverage on explicit statistical priors calculated
from the training summary statistics to refine node and edge predictions based
on their one-hop neighborhood. Our experiments show that our method outperforms
current methods purely using multi-view images as the initial input. Our
project page is available at https://qixun1.github.io/projects/SCRSSG.

</details>


### [108] [Slice or the Whole Pie? Utility Control for AI Models](https://arxiv.org/abs/2508.06551)
*Ye Tao*

Main category: cs.CV

TL;DR: NNObfuscator enables dynamic performance adjustment in AI models, allowing a single model to adapt to diverse requirements without multiple versions.


<details>
  <summary>Details</summary>
Motivation: Training and maintaining multiple DNN versions for varied performance needs is inefficient and resource-intensive.

Method: Proposes NNObfuscator, a utility control mechanism for real-time model adaptation with tiered performance levels.

Result: Validated on tasks like image classification and text-to-image generation, showing adaptability without extensive changes.

Conclusion: NNObfuscator improves resource allocation, reduces computation, and supports sustainable AI deployment.

Abstract: Training deep neural networks (DNNs) has become an increasingly
resource-intensive task, requiring large volumes of labeled data, substantial
computational power, and considerable fine-tuning efforts to achieve optimal
performance across diverse use cases. Although pre-trained models offer a
useful starting point, adapting them to meet specific user needs often demands
extensive customization, and infrastructure overhead. This challenge grows when
a single model must support diverse appli-cations with differing requirements
for performance. Traditional solutions often involve training multiple model
versions to meet varying requirements, which can be inefficient and difficult
to maintain. In order to overcome this challenge, we propose NNObfuscator, a
novel utility control mechanism that enables AI models to dynamically modify
their performance according to predefined conditions. It is different from
traditional methods that need separate models for each user. Instead,
NNObfuscator allows a single model to be adapted in real time, giving you
controlled access to multiple levels of performance. This mechanism enables
model owners set up tiered access, ensuring that free-tier users receive a
baseline level of performance while premium users benefit from enhanced
capabilities. The approach improves resource allocation, reduces unnecessary
computation, and supports sustainable business models in AI deployment. To
validate our approach, we conducted experiments on multiple tasks, including
image classification, semantic segmentation, and text to image generation,
using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable
Diffusion. Experimental results show that NNObfuscator successfully makes model
more adaptable, so that a single trained model can handle a broad range of
tasks without requiring a lot of changes.

</details>


### [109] [Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection](https://arxiv.org/abs/2508.06552)
*Unisha Joshi*

Main category: cs.CV

TL;DR: The paper introduces an age-diverse deepfake dataset to address demographic bias, improving fairness and accuracy in deepfake detection.


<details>
  <summary>Details</summary>
Motivation: To mitigate age-specific bias in deepfake datasets, which is largely unaddressed despite advancements in detection models.

Method: Constructed an age-diverse dataset using Celeb-DF, FaceForensics++, UTKFace, and synthetic data, evaluated with XceptionNet, EfficientNet, and LipForensics.

Result: Models trained on the age-diverse dataset showed fairer performance across age groups, improved accuracy, and better generalization.

Conclusion: The study provides a reproducible, fairness-aware dataset and pipeline for fairer deepfake detection, with open access to data and code.

Abstract: The challenges associated with deepfake detection are increasing
significantly with the latest advancements in technology and the growing
popularity of deepfake videos and images. Despite the presence of numerous
detection models, demographic bias in the deepfake dataset remains largely
unaddressed. This paper focuses on the mitigation of age-specific bias in the
deepfake dataset by introducing an age-diverse deepfake dataset that will
improve fairness across age groups. The dataset is constructed through a
modular pipeline incorporating the existing deepfake datasets Celeb-DF,
FaceForensics++, and UTKFace datasets, and the creation of synthetic data to
fill the age distribution gaps. The effectiveness and generalizability of this
dataset are evaluated using three deepfake detection models: XceptionNet,
EfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and
EER, revealed that models trained on the age-diverse dataset demonstrated
fairer performance across age groups, improved overall accuracy, and higher
generalization across datasets. This study contributes a reproducible,
fairness-aware deepfake dataset and model pipeline that can serve as a
foundation for future research in fairer deepfake detection. The complete
dataset and implementation code are available at
https://github.com/unishajoshi/age-diverse-deepfake-detection.

</details>


### [110] [Static and Plugged: Make Embodied Evaluation Simple](https://arxiv.org/abs/2508.06553)
*Jiahao Xiao,Jianbo Zhang,BoWen Yan,Shengyu Guo,Tongrui Ye,Kaiwei Zhang,Zicheng Zhang,Xiaohong Liu,Zhengxue Cheng,Lei Fan,Chuyi Li,Guangtao Zhai*

Main category: cs.CV

TL;DR: StaticEmbodiedBench is introduced as a scalable, unified benchmark for embodied intelligence evaluation using static scenes, covering 42 scenarios and 8 dimensions. It evaluates 19 VLMs and 11 VLAs, providing a leaderboard and releasing 200 samples to aid development.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for embodied intelligence are costly, fragmented, and hard to scale, necessitating a more efficient solution.

Method: The paper proposes StaticEmbodiedBench, a plug-and-play benchmark using static scene representations for unified evaluation.

Result: The benchmark evaluates 30 models (19 VLMs and 11 VLAs) and establishes a unified leaderboard. A subset of 200 samples is released to support development.

Conclusion: StaticEmbodiedBench offers a scalable, comprehensive solution for embodied intelligence evaluation, facilitating future research with its released samples.

Abstract: Embodied intelligence is advancing rapidly, driving the need for efficient
evaluation. Current benchmarks typically rely on interactive simulated
environments or real-world setups, which are costly, fragmented, and hard to
scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play
benchmark that enables unified evaluation using static scene representations.
Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and
comprehensive assessment through a simple interface. Furthermore, we evaluate
19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),
establishing the first unified static leaderboard for Embodied intelligence.
Moreover, we release a subset of 200 samples from our benchmark to accelerate
the development of embodied intelligence.

</details>


### [111] [StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback](https://arxiv.org/abs/2508.06555)
*Hongbo Ma,Fei Shen,Hongbin Xu,Xiaoce Wang,Gang Xu,Jinkai Zheng,Liangqiong Qu,Ming Li*

Main category: cs.CV

TL;DR: StyleTailor is a collaborative agent framework for personalized fashion styling, integrating design, recommendation, virtual try-on, and evaluation. It uses iterative visual refinement with multi-level negative feedback to improve user alignment and outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Personalized fashion styling is underexplored despite its potential to enhance shopping experiences. StyleTailor aims to fill this gap by unifying multiple fashion-related tasks into a cohesive system.

Method: StyleTailor employs a framework with two core agents (Designer and Consultant) and uses hierarchical vision-language model feedback for iterative refinement. Negative feedback is aggregated into prompts for continuous improvement.

Result: StyleTailor outperforms baselines in delivering personalized designs and recommendations, excelling in style consistency, visual quality, face similarity, and artistic appraisal.

Conclusion: StyleTailor sets a new benchmark for intelligent fashion systems, demonstrating the effectiveness of its iterative refinement and negative feedback mechanism.

Abstract: The advancement of intelligent agents has revolutionized problem-solving
across diverse domains, yet solutions for personalized fashion styling remain
underexplored, which holds immense promise for promoting shopping experiences.
In this work, we present StyleTailor, the first collaborative agent framework
that seamlessly unifies personalized apparel design, shopping recommendation,
virtual try-on, and systematic evaluation into a cohesive workflow. To this
end, StyleTailor pioneers an iterative visual refinement paradigm driven by
multi-level negative feedback, enabling adaptive and precise user alignment.
Specifically, our framework features two core agents, i.e., Designer for
personalized garment selection and Consultant for virtual try-on, whose outputs
are progressively refined via hierarchical vision-language model feedback
spanning individual items, complete outfits, and try-on efficacy.
Counterexamples are aggregated into negative prompts, forming a closed-loop
mechanism that enhances recommendation quality.To assess the performance, we
introduce a comprehensive evaluation suite encompassing style consistency,
visual quality, face similarity, and artistic appraisal. Extensive experiments
demonstrate StyleTailor's superior performance in delivering personalized
designs and recommendations, outperforming strong baselines without negative
feedback and establishing a new benchmark for intelligent fashion systems.

</details>


### [112] [From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets](https://arxiv.org/abs/2508.06556)
*Sarina Penquitt,Jonathan Klees,Rinor Cakaj,Daniel Kondermann,Matthias Rottmann,Lars Schmarje*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Object detection has advanced rapidly in recent years, driven by increasingly
large and diverse datasets. However, label errors, defined as missing labels,
incorrect classification or inaccurate localization, often compromise the
quality of these datasets. This can have a significant impact on the outcomes
of training and benchmark evaluations. Although several methods now exist for
detecting label errors in object detection datasets, they are typically
validated only on synthetic benchmarks or limited manual inspection. How to
correct such errors systemically and at scale therefore remains an open
problem. We introduce a semi-automated framework for label-error correction
called REC$\checkmark$D (Rechecked). Building on existing detectors, the
framework pairs their error proposals with lightweight, crowd-sourced
microtasks. These tasks enable multiple annotators to independently verify each
candidate bounding box, and their responses are aggregated to estimate
ambiguity and improve label quality. To demonstrate the effectiveness of
REC$\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our
crowdsourced review yields high-quality corrected annotations, which indicate a
rate of at least 24% of missing and inaccurate annotations in original
annotations. This validated set will be released as a new real-world benchmark
for label error detection and correction. We show that current label error
detection methods, when combined with our correction framework, can recover
hundreds of errors in the time it would take a human to annotate bounding boxes
from scratch. However, even the best methods still miss up to 66% of the true
errors and with low quality labels introduce more errors than they find. This
highlights the urgent need for further research, now enabled by our released
benchmark.

</details>


### [113] [On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications](https://arxiv.org/abs/2508.06558)
*Simon Baur,Alexandra Benova,Emilio Dolgener Cantú,Jackie Ma*

Main category: cs.CV

TL;DR: MMPKD improves unimodal vision models using extra training modalities, enhancing ROI localization but lacks cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Deep learning in clinical practice often lacks all modalities at inference, requiring robust unimodal solutions.

Method: Proposes MMPKD, using text and tabular metadata teachers for knowledge distillation into a vision transformer.

Result: Improved zero-shot ROI localization in images, but no cross-domain generalization.

Conclusion: MMPKD enhances unimodal models but is domain-specific, challenging prior claims of generalization.

Abstract: Deploying deep learning models in clinical practice often requires leveraging
multiple data modalities, such as images, text, and structured data, to achieve
robust and trustworthy decisions. However, not all modalities are always
available at inference time. In this work, we propose multimodal privileged
knowledge distillation (MMPKD), a training strategy that utilizes additional
modalities available solely during training to guide a unimodal vision model.
Specifically, we used a text-based teacher model for chest radiographs
(MIMIC-CXR) and a tabular metadata-based teacher model for mammography
(CBIS-DDSM) to distill knowledge into a vision transformer student model. We
show that MMPKD can improve the resulting attention maps' zero-shot
capabilities of localizing ROI in input images, while this effect does not
generalize across domains, as contrarily suggested by prior research.

</details>


### [114] [Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC](https://arxiv.org/abs/2508.06564)
*Guanyu Hu,Dimitrios Kollias,Xinyu Yang*

Main category: cs.CV

TL;DR: The paper proposes VEGA, a novel mechanism using CLIP's image encoder to create emotion-specific visual anchors for multimodal emotion recognition, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing models lack psychologically meaningful priors for multimodal alignment, prompting the need for a more grounded approach.

Method: VEGA introduces class-level visual semantics via CLIP's image encoder, using facial exemplars to guide feature alignment. A stochastic anchor sampling strategy enhances robustness.

Result: The model achieves state-of-the-art performance on IEMOCAP and MELD datasets.

Conclusion: VEGA effectively integrates cognitive theories into multimodal fusion, improving emotion recognition accuracy.

Abstract: Multimodal Emotion Recognition in Conversations remains a challenging task
due to the complex interplay of textual, acoustic and visual signals. While
recent models have improved performance via advanced fusion strategies, they
often lack psychologically meaningful priors to guide multimodal alignment. In
this paper, we revisit the use of CLIP and propose a novel Visual Emotion
Guided Anchoring (VEGA) mechanism that introduces class-level visual semantics
into the fusion and classification process. Distinct from prior work that
primarily utilizes CLIP's textual encoder, our approach leverages its image
encoder to construct emotion-specific visual anchors based on facial exemplars.
These anchors guide unimodal and multimodal features toward a perceptually
grounded and psychologically aligned representation space, drawing inspiration
from cognitive theories (prototypical emotion categories and multisensory
integration). A stochastic anchor sampling strategy further enhances robustness
by balancing semantic stability and intra-class diversity. Integrated into a
dual-branch architecture with self-distillation, our VEGA-augmented model
achieves sota performance on IEMOCAP and MELD. Code is available at:
https://github.com/dkollias/VEGA.

</details>


### [115] [Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06565)
*Jing Zhang,Xiaowei Yu,Minheng Chen,Lu Zhang,Tong Chen,Yan Zhuang,Chao Cao,Yanjun Lyu,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.CV

TL;DR: A novel framework aligns brain connectomes with clinical reports in a shared latent space, improving diagnosis by linking imaging data and text reports.


<details>
  <summary>Details</summary>
Motivation: To leverage multimodal brain imaging and clinical reports for better diagnosis, addressing the challenge of linking objective imaging with subjective text.

Method: Aligns brain subnetworks (as tokens) with word tokens in reports in a shared latent space, enhancing representation learning.

Result: Achieves state-of-the-art predictive performance and identifies meaningful connectome-text pairs for mild cognitive impairment.

Conclusion: The method offers insights into Alzheimer's mechanisms and supports multimodal biomarker development.

Abstract: Integrating brain imaging data with clinical reports offers a valuable
opportunity to leverage complementary multimodal information for more effective
and timely diagnosis in practical clinical settings. This approach has gained
significant attention in brain disorder research, yet a key challenge remains:
how to effectively link objective imaging data with subjective text-based
reports, such as doctors' notes. In this work, we propose a novel framework
that aligns brain connectomes with clinical reports in a shared cross-modal
latent space at both the subject and connectome levels, thereby enhancing
representation learning. The key innovation of our approach is that we treat
brain subnetworks as tokens of imaging data, rather than raw image patches, to
align with word tokens in clinical reports. This enables a more efficient
identification of system-level associations between neuroimaging findings and
clinical observations, which is critical since brain disorders often manifest
as network-level abnormalities rather than isolated regional alterations. We
applied our method to mild cognitive impairment (MCI) using the ADNI dataset.
Our approach not only achieves state-of-the-art predictive performance but also
identifies clinically meaningful connectome-text pairs, offering new insights
into the early mechanisms of Alzheimer's disease and supporting the development
of clinically useful multimodal biomarkers.

</details>


### [116] [Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features](https://arxiv.org/abs/2508.06566)
*Manish Kansana,Elias Hossain,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: Surformer v1, a transformer-based model, integrates tactile and visual inputs for surface material recognition, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Surface material recognition is crucial for robotic perception and interaction, especially when combining tactile and visual inputs.

Method: The model uses modality-specific encoders with cross-modal attention layers, trained on tactile features and PCA-reduced visual embeddings.

Result: Surformer v1 achieved 99.4% accuracy with 0.77 ms inference time, outperforming Multimodal CNN in efficiency.

Conclusion: Surformer v1 balances accuracy, efficiency, and computational cost, making it suitable for real-time applications.

Abstract: Surface material recognition is a key component in robotic perception and
physical interaction, particularly when leveraging both tactile and visual
sensory inputs. In this work, we propose Surformer v1, a transformer-based
architecture designed for surface classification using structured tactile
features and PCA-reduced visual embeddings extracted via ResNet-50. The model
integrates modality-specific encoders with cross-modal attention layers,
enabling rich interactions between vision and touch. Currently,
state-of-the-art deep learning models for vision tasks have achieved remarkable
performance. With this in mind, our first set of experiments focused
exclusively on tactile-only surface classification. Using feature engineering,
we trained and evaluated multiple machine learning models, assessing their
accuracy and inference time. We then implemented an encoder-only Transformer
model tailored for tactile features. This model not only achieved the highest
accuracy but also demonstrated significantly faster inference time compared to
other evaluated models, highlighting its potential for real-time applications.
To extend this investigation, we introduced a multimodal fusion setup by
combining vision and tactile inputs. We trained both Surformer v1 (using
structured features) and Multimodal CNN (using raw images) to examine the
impact of feature-based versus image-based multimodal learning on
classification accuracy and computational efficiency. The results showed that
Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while
the Multimodal CNN achieved slightly higher accuracy but required significantly
more inference time. These findings suggest Surformer v1 offers a compelling
balance between accuracy, efficiency, and computational cost for surface
material recognition.

</details>


### [117] [ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos](https://arxiv.org/abs/2508.06570)
*Mohammad Zia Ur Rehman,Anukriti Bhatnagar,Omkar Kabde,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: The paper introduces ImpliHateVid, a novel dataset for implicit hate speech detection in videos, and proposes a two-stage contrastive learning framework for multimodal hate speech detection.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks focus on video-based hate speech detection, especially for implicit hate. The paper aims to fill this gap by creating a dedicated dataset and a robust detection method.

Method: A two-stage contrastive learning framework is proposed: (1) modality-specific encoders (audio, text, image) trained with contrastive loss, and (2) cross-encoders refined with contrastive learning. Additional features like sentiment and emotion are incorporated.

Result: The method is evaluated on ImpliHateVid and HateMM datasets, showing effectiveness in detecting hateful content, particularly implicit hate.

Conclusion: The work highlights the importance of video-based hate speech detection and demonstrates the success of multimodal contrastive learning, with ImpliHateVid serving as a valuable resource.

Abstract: The existing research has primarily focused on text and image-based hate
speech detection, video-based approaches remain underexplored. In this work, we
introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate
speech detection in videos. ImpliHateVid consists of 2,009 videos comprising
509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,
making it one of the first large-scale video datasets dedicated to implicit
hate detection. We also propose a novel two-stage contrastive learning
framework for hate speech detection in videos. In the first stage, we train
modality-specific encoders for audio, text, and image using contrastive loss by
concatenating features from the three encoders. In the second stage, we train
cross-encoders using contrastive learning to refine multimodal representations.
Additionally, we incorporate sentiment, emotion, and caption-based features to
enhance implicit hate detection. We evaluate our method on two datasets,
ImpliHateVid for implicit hate speech detection and another dataset for general
hate speech detection in videos, HateMM dataset, demonstrating the
effectiveness of the proposed multimodal contrastive learning for hateful
content detection in videos and the significance of our dataset.

</details>


### [118] [ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification](https://arxiv.org/abs/2508.06623)
*Sihan Ma,Qiming Wu,Ruotong Jiang,Frank Burns*

Main category: cs.CV

TL;DR: ContextGuard-LVLM, a framework using Vision-Language Large Models, improves fine-grained cross-modal consistency verification in digital news by detecting subtle misalignments beyond entity matching.


<details>
  <summary>Details</summary>
Motivation: The need for robust methods to verify content veracity in digital news, especially for fine-grained cross-modal contextual consistency (FCCC), which traditional methods fail to address.

Method: Proposes ContextGuard-LVLM, leveraging Vision-Language Large Models (LVLMs) with multi-stage contextual reasoning and reinforced/adversarial learning. Extends datasets with fine-grained annotations.

Result: Outperforms zero-shot LVLM baselines (InstructBLIP, LLaVA 1.5) in fine-grained tasks, excels in logical reasoning, and shows robustness to perturbations.

Conclusion: ContextGuard-LVLM effectively addresses FCCC, demonstrating superior performance and alignment with human judgments in detecting contextual misalignments.

Abstract: The proliferation of digital news media necessitates robust methods for
verifying content veracity, particularly regarding the consistency between
visual and textual information. Traditional approaches often fall short in
addressing the fine-grained cross-modal contextual consistency (FCCC) problem,
which encompasses deeper alignment of visual narrative, emotional tone, and
background information with text, beyond mere entity matching. To address this,
we propose ContextGuard-LVLM, a novel framework built upon advanced
Vision-Language Large Models (LVLMs) and integrating a multi-stage contextual
reasoning mechanism. Our model is uniquely enhanced through reinforced or
adversarial learning paradigms, enabling it to detect subtle contextual
misalignments that evade zero-shot baselines. We extend and augment three
established datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new
fine-grained contextual annotations, including "contextual sentiment," "visual
narrative theme," and "scene-event logical coherence," and introduce a
comprehensive CTXT (Contextual Coherence) entity type. Extensive experiments
demonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art
zero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all
fine-grained consistency tasks, showing significant improvements in complex
logical reasoning and nuanced contextual understanding. Furthermore, our model
exhibits superior robustness to subtle perturbations and a higher agreement
rate with human expert judgments on challenging samples, affirming its efficacy
in discerning sophisticated forms of context detachment.

</details>


### [119] [VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis](https://arxiv.org/abs/2508.06624)
*Kexin Yu,Zihan Xu,Jialei Xie,Carter Adams*

Main category: cs.CV

TL;DR: VL-MedGuide, a multi-modal framework using Visual-Language Large Models, improves skin disease diagnosis with interpretable results, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of diagnosing skin diseases due to complex visual features and lack of interpretability in current models.

Method: Two-stage framework: Multi-modal Concept Perception Module for feature description and Explainable Disease Reasoning Module for diagnosis with rationales.

Result: Achieves state-of-the-art performance (83.55% BACC, 80.12% F1 for diagnosis; 76.10% BACC, 67.45% F1 for concept detection) on Derm7pt dataset.

Conclusion: VL-MedGuide bridges AI performance and clinical utility with clear, trustworthy explanations, enhancing dermatological practice.

Abstract: Accurate diagnosis of skin diseases remains a significant challenge due to
the complex and diverse visual features present in dermatoscopic images, often
compounded by a lack of interpretability in existing purely visual diagnostic
models. To address these limitations, this study introduces VL-MedGuide
(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful
multi-modal understanding and reasoning capabilities of Visual-Language Large
Models (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis
of skin conditions. VL-MedGuide operates in two interconnected stages: a
Multi-modal Concept Perception Module, which identifies and linguistically
describes dermatologically relevant visual features through sophisticated
prompt engineering, and an Explainable Disease Reasoning Module, which
integrates these concepts with raw visual information via Chain-of-Thought
prompting to provide precise disease diagnoses alongside transparent
rationales. Comprehensive experiments on the Derm7pt dataset demonstrate that
VL-MedGuide achieves state-of-the-art performance in both disease diagnosis
(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),
surpassing existing baselines. Furthermore, human evaluations confirm the high
clarity, completeness, and trustworthiness of its generated explanations,
bridging the gap between AI performance and clinical utility by offering
actionable, explainable insights for dermatological practice.

</details>


### [120] [CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation](https://arxiv.org/abs/2508.06625)
*Shilong Zou,Yuhang Huang,Renjiao Yi,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: A diffusion-based cross-domain image translator is introduced, using joint learning to align diffusion and translation processes for improved performance and global optimality.


<details>
  <summary>Details</summary>
Motivation: Traditional GAN-based methods and shallow integration of diffusion models in image translation limit performance and optimality.

Method: Proposes a joint learning framework aligning diffusion and translation processes, using diffusion models to extract clean signals and a time-dependent translation network.

Result: Achieves better generative performance on RGB↔RGB and cross-modality tasks (e.g., RGB↔Edge, RGB↔Semantics) compared to state-of-the-art methods.

Conclusion: The joint learning framework enhances global optimality, fidelity, and structural consistency in cross-domain image translation.

Abstract: We introduce a diffusion-based cross-domain image translator in the absence
of paired training data. Unlike GAN-based methods, our approach integrates
diffusion models to learn the image translation process, allowing for more
coverable modeling of the data distribution and performance improvement of the
cross-domain translation. However, incorporating the translation process within
the diffusion process is still challenging since the two processes are not
aligned exactly, i.e., the diffusion process is applied to the noisy signal
while the translation process is conducted on the clean signal. As a result,
recent diffusion-based studies employ separate training or shallow integration
to learn the two processes, yet this may cause the local minimal of the
translation optimization, constraining the effectiveness of diffusion models.
To address the problem, we propose a novel joint learning framework that aligns
the diffusion and the translation process, thereby improving the global
optimality. Specifically, we propose to extract the image components with
diffusion models to represent the clean signal and employ the translation
process with the image components, enabling an end-to-end joint learning
manner. On the other hand, we introduce a time-dependent translation network to
learn the complex translation mapping, resulting in effective translation
learning and significant performance improvement. Benefiting from the design of
joint learning, our method enables global optimization of both processes,
enhancing the optimality and achieving improved fidelity and structural
consistency. We have conducted extensive experiments on RGB$\leftrightarrow$RGB
and diverse cross-modality translation tasks including
RGB$\leftrightarrow$Edge, RGB$\leftrightarrow$Semantics and
RGB$\leftrightarrow$Depth, showcasing better generative performances than the
state of the arts.

</details>


### [121] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: A neural rendering framework improves modeling of view-dependent appearance by decomposing complex appearance into static neural basis and dynamic coefficients, achieving sharper specular highlights.


<details>
  <summary>Details</summary>
Motivation: Challenges in rendering scenes with complex specular reflections and highlights in Neural Radiance Fields (NeRF) due to blurry reflections and optimization instability.

Method: Decomposes appearance into a static neural basis for material properties and dynamic coefficients generated by a Coefficient Network, combined by a Dynamic Radiance Integrator.

Result: Produces sharper and more realistic specular highlights compared to existing techniques.

Conclusion: The decomposition paradigm offers a flexible and effective direction for modeling complex appearance in neural scene representations.

Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view
synthesis, but challenges remain in rendering scenes with complex specular
reflections and highlights. Existing approaches may produce blurry reflections
due to entanglement between lighting and material properties, or encounter
optimization instability when relying on physically-based inverse rendering. In
this work, we present a neural rendering framework based on dynamic coefficient
decomposition, aiming to improve the modeling of view-dependent appearance. Our
approach decomposes complex appearance into a shared, static neural basis that
encodes intrinsic material properties, and a set of dynamic coefficients
generated by a Coefficient Network conditioned on view and illumination. A
Dynamic Radiance Integrator then combines these components to synthesize the
final radiance. Experimental results on several challenging benchmarks suggest
that our method can produce sharper and more realistic specular highlights
compared to existing techniques. We hope that this decomposition paradigm can
provide a flexible and effective direction for modeling complex appearance in
neural scene representations.

</details>


### [122] [Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors](https://arxiv.org/abs/2508.06640)
*Zheyuan Zhang,Weihao Tang,Hong Chen*

Main category: cs.CV

TL;DR: CausalNet is a novel framework for robust micro-expression recognition (MER) that handles key-frame index errors while maintaining accuracy by using the entire ME sequence and learning causal relationships between muscle movements.


<details>
  <summary>Details</summary>
Motivation: Current key-frame-based MER methods rely on accurate key-frame indexes, which are hard to obtain and prone to errors, limiting practical applications.

Method: CausalNet uses the entire ME sequence, employs a Causal Motion Position Learning Module (CMPLM) to focus on relevant muscle movement areas, and a Causal Attention Block (CAB) to learn causal relationships between muscle contractions and relaxations.

Result: CausalNet achieves robust MER under key-frame index noise and outperforms SOTA methods on standard benchmarks with annotated key-frames.

Conclusion: CausalNet addresses the limitations of key-frame-based methods, offering a practical and accurate solution for MER.

Abstract: Micro-expression recognition (MER) is a highly challenging task in affective
computing. With the reduced-sized micro-expression (ME) input that contains key
information based on key-frame indexes, key-frame-based methods have
significantly improved the performance of MER. However, most of these methods
focus on improving the performance with relatively accurate key-frame indexes,
while ignoring the difficulty of obtaining accurate key-frame indexes and the
objective existence of key-frame index errors, which impedes them from moving
towards practical applications. In this paper, we propose CausalNet, a novel
framework to achieve robust MER facing key-frame index errors while maintaining
accurate recognition. To enhance robustness, CausalNet takes the representation
of the entire ME sequence as the input. To address the information redundancy
brought by the complete ME range input and maintain accurate recognition,
first, the Causal Motion Position Learning Module (CMPLM) is proposed to help
the model locate the muscle movement areas related to Action Units (AUs),
thereby reducing the attention to other redundant areas. Second, the Causal
Attention Block (CAB) is proposed to deeply learn the causal relationships
between the muscle contraction and relaxation movements in MEs. Empirical
experiments have demonstrated that on popular ME benchmarks, the CausalNet has
achieved robust MER under different levels of key-frame index noise. Meanwhile,
it has surpassed state-of-the-art (SOTA) methods on several standard MER
benchmarks when using the provided annotated key-frames. Code is available at
https://github.com/tony19980810/CausalNet.

</details>


### [123] [Towards Robust Red-Green Watermarking for Autoregressive Image Generators](https://arxiv.org/abs/2508.06656)
*Denis Lukovnikov,Andreas Müller,Erwin Quiring,Asja Fischer*

Main category: cs.CV

TL;DR: The paper explores in-generation watermarking for autoregressive (AR) image models, proposing two novel methods using visual token clustering to improve robustness against perturbations and regeneration attacks while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for latent diffusion models (LDMs) are robust, but their application to AR image models remains unexplored. This work aims to adapt and improve watermarking for AR models.

Method: The study examines token-level watermarking schemes, proposing two methods: a training-free approach using a cluster lookup table and finetuning VAE encoders to predict token clusters from perturbed images.

Result: Cluster-level watermarks enhance robustness against perturbations and regeneration attacks, with cluster classification further improving detectability. The methods also offer fast verification runtime.

Conclusion: The proposed watermarking methods for AR models effectively balance robustness, image quality, and verification speed, outperforming baseline approaches.

Abstract: In-generation watermarking for detecting and attributing generated content
has recently been explored for latent diffusion models (LDMs), demonstrating
high robustness. However, the use of in-generation watermarks in autoregressive
(AR) image models has not been explored yet. AR models generate images by
autoregressively predicting a sequence of visual tokens that are then decoded
into pixels using a vector-quantized decoder. Inspired by red-green watermarks
for large language models, we examine token-level watermarking schemes that
bias the next-token prediction based on prior tokens. We find that a direct
transfer of these schemes works in principle, but the detectability of the
watermarks decreases considerably under common image perturbations. As a
remedy, we propose two novel watermarking methods that rely on visual token
clustering to assign similar tokens to the same set. Firstly, we investigate a
training-free approach that relies on a cluster lookup table, and secondly, we
finetune VAE encoders to predict token clusters directly from perturbed images.
Overall, our experiments show that cluster-level watermarks improve robustness
against perturbations and regeneration attacks while preserving image quality.
Cluster classification further boosts watermark detectability, outperforming a
set of baselines. Moreover, our methods offer fast verification runtime,
comparable to lightweight post-hoc watermarking methods.

</details>


### [124] [Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision](https://arxiv.org/abs/2508.06696)
*Tianqin Li,George Liu,Tai Sing Lee*

Main category: cs.CV

TL;DR: Using line drawings for pretraining improves model efficiency, generalization, and human-aligned biases in vision tasks.


<details>
  <summary>Details</summary>
Motivation: Modern vision systems rely on rich inputs, while humans excel with sparse representations like line drawings, suggesting structure is key for efficient understanding.

Method: Propose line drawings as a structure-first pretraining modality, including unsupervised learning via "learning to draw."

Result: Models show stronger shape bias, focused attention, data efficiency, and lower intrinsic dimensionality. They also enable better distillation into lightweight models.

Conclusion: Structure-first learning with line drawings enhances efficiency, generalization, and robustness in vision systems.

Abstract: Despite remarkable progress in computer vision, modern recognition systems
remain limited by their dependence on rich, redundant visual inputs. In
contrast, humans can effortlessly understand sparse, minimal representations
like line drawings - suggesting that structure, rather than appearance,
underlies efficient visual understanding. In this work, we propose using line
drawings as a structure-first pretraining modality to induce more compact and
generalizable visual representations. We show that models pretrained on line
drawings develop stronger shape bias, more focused attention, and greater data
efficiency across classification, detection, and segmentation tasks. Notably,
these models also exhibit lower intrinsic dimensionality, requiring
significantly fewer principal components to capture representational variance -
echoing the similar observation in low dimensional efficient representation in
the brain. Beyond performance improvements, line drawing pretraining produces
more compressible representations, enabling better distillation into
lightweight student models. Students distilled from line-pretrained teachers
consistently outperform those trained from color-supervised teachers,
highlighting the benefits of structurally compact knowledge. Finally, we
demonstrate that the pretraining with line-drawing can also be extended to
unsupervised setting via our proposed method "learning to draw". Together, our
results support the view that structure-first visual learning fosters
efficiency, generalization, and human-aligned inductive biases - offering a
simple yet powerful strategy for building more robust and adaptable vision
systems.

</details>


### [125] [MMFformer: Multimodal Fusion Transformer Network for Depression Detection](https://arxiv.org/abs/2508.06701)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: MMFformer, a multimodal depression detection network, improves early depression diagnosis by analyzing social media data, outperforming state-of-the-art methods with significant F1-Score improvements.


<details>
  <summary>Details</summary>
Motivation: Early depression detection is challenging due to subjective clinical evaluations. Social media offers rich, diverse data, but extracting relevant temporal and multimodal information is difficult.

Method: MMFformer uses transformer networks with residual connections for spatial features (videos) and temporal dynamics (audio), employing late and intermediate fusion strategies for intermodal correlations.

Result: The network achieves F1-Score improvements of 13.92% (D-Vlog) and 7.74% (LMVD) over existing methods.

Conclusion: MMFformer effectively detects depression from multimodal social media data, offering a robust tool for early diagnosis.

Abstract: Depression is a serious mental health illness that significantly affects an
individual's well-being and quality of life, making early detection crucial for
adequate care and treatment. Detecting depression is often difficult, as it is
based primarily on subjective evaluations during clinical interviews. Hence,
the early diagnosis of depression, thanks to the content of social networks,
has become a prominent research area. The extensive and diverse nature of
user-generated information poses a significant challenge, limiting the accurate
extraction of relevant temporal information and the effective fusion of data
across multiple modalities. This paper introduces MMFformer, a multimodal
depression detection network designed to retrieve depressive spatio-temporal
high-level patterns from multimodal social media information. The transformer
network with residual connections captures spatial features from videos, and a
transformer encoder is exploited to design important temporal dynamics in
audio. Moreover, the fusion architecture fused the extracted features through
late and intermediate fusion strategies to find out the most relevant
intermodal correlations among them. Finally, the proposed network is assessed
on two large-scale depression detection datasets, and the results clearly
reveal that it surpasses existing state-of-the-art approaches, improving the
F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is
made available publicly at
https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.

</details>


### [126] [Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography](https://arxiv.org/abs/2508.06703)
*Justin London*

Main category: cs.CV

TL;DR: A fast pipeline for computer-generated holography (CGH) using point cloud and MRI data, optimized with non-convex algorithms, outperforms deep learning methods like HoloNet after artifact removal.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and quality of CGH by leveraging volumetric data and advanced optimization techniques.

Method: Input data (point cloud, MRI) is reconstructed into volumetric objects, then optimized using alternating projection, SGD, and quasi-Newton methods for POH and CH generation. Artifacts are reduced with 2D median filtering.

Result: The proposed pipeline outperforms HoloNet in reconstruction performance (MSE, RMSE, PSNR) after noise and artifact removal.

Conclusion: The framework efficiently synthesizes high-quality CGH, demonstrating superior performance over deep learning alternatives when optimized and filtered.

Abstract: Computer-generated holography (CGH) is a promising method that modulates
user-defined waveforms with digital holograms. An efficient and fast pipeline
framework is proposed to synthesize CGH using initial point cloud and MRI data.
This input data is reconstructed into volumetric objects that are then input
into non-convex Fourier optics optimization algorithms for phase-only hologram
(POH) and complex-hologram (CH) generation using alternating projection, SGD,
and quasi-Netwton methods. Comparison of reconstruction performance of these
algorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet
deep learning CGH. Performance metrics are shown to be improved by using 2D
median filtering to remove artifacts and speckled noise during optimization.

</details>


### [127] [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](https://arxiv.org/abs/2508.06715)
*Jixuan He,Chieh Hubert Lin,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: Restage4D leverages real-world video motion priors to generate physically consistent 4D content, improving geometry and motion quality.


<details>
  <summary>Details</summary>
Motivation: Existing generative models lack physical realism and motion dynamics for 4D scene synthesis, while real-world videos provide grounded cues.

Method: Uses a video-rewinding training strategy, occlusion-aware rigidity loss, and disocclusion backtracing to bridge real and synthetic motion.

Result: Validated on DAVIS and PointOdyssey, showing improved geometry consistency, motion quality, and 3D tracking.

Conclusion: Restage4D preserves deformable structure and corrects generative model errors, highlighting video priors' potential in 4D restaging.

Abstract: Creating deformable 3D content has gained increasing attention with the rise
of text-to-image and image-to-video generative models. While these models
provide rich semantic priors for appearance, they struggle to capture the
physical realism and motion dynamics needed for authentic 4D scene synthesis.
In contrast, real-world videos can provide physically grounded geometry and
articulation cues that are difficult to hallucinate. One question is raised:
\textit{Can we generate physically consistent 4D content by leveraging the
motion priors of the real-world video}? In this work, we explore the task of
reanimating deformable 3D scenes from a single video, using the original
sequence as a supervisory signal to correct artifacts from synthetic motion. We
introduce \textbf{Restage4D}, a geometry-preserving pipeline for
video-conditioned 4D restaging. Our approach uses a video-rewinding training
strategy to temporally bridge a real base video and a synthetic driving video
via a shared motion representation. We further incorporate an occlusion-aware
rigidity loss and a disocclusion backtracing mechanism to improve structural
and geometry consistency under challenging motion. We validate Restage4D on
DAVIS and PointOdyssey, demonstrating improved geometry consistency, motion
quality, and 3D tracking performance. Our method not only preserves deformable
structure under novel motion, but also automatically corrects errors introduced
by generative models, revealing the potential of video prior in 4D restaging
task. Source code and trained models will be released.

</details>


### [128] [FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI](https://arxiv.org/abs/2508.06756)
*Somayeh Farahani,Marjaneh Hejazi,Antonio Di Ieva,Sidong Liu*

Main category: cs.CV

TL;DR: FoundBioNet, a SWIN-UNETR-based model, predicts IDH mutation in gliomas noninvasively using MRI, outperforming baselines with AUCs up to 90.58%.


<details>
  <summary>Details</summary>
Motivation: Noninvasive detection of IDH mutation is crucial for glioma management, but traditional invasive methods and limited annotated data hinder accuracy.

Method: FoundBioNet uses SWIN-UNETR with Tumor-Aware Feature Encoding (TAFE) and Cross-Modality Differential (CMD) modules to analyze multi-parametric MRI.

Result: Achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on diverse test sets, outperforming baselines (p <= 0.05).

Conclusion: FoundBioNet improves diagnostic accuracy and interpretability, enabling personalized glioma care.

Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is
essential for effective glioma management. Traditional methods rely on invasive
tissue sampling, which may fail to capture a tumor's spatial heterogeneity.
While deep learning models have shown promise in molecular profiling, their
performance is often limited by scarce annotated data. In contrast, foundation
deep learning models offer a more generalizable approach for glioma imaging
biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that
utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation
status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware
Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and
Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch
signals associated with IDH mutation. The model was trained and validated on a
diverse, multi-center cohort of 1705 glioma patients from six public datasets.
Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent
test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming
baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE
and CMD modules are essential for improving predictive accuracy. By integrating
large-scale pretraining and task-specific fine-tuning, FoundBioNet enables
generalizable glioma characterization. This approach enhances diagnostic
accuracy and interpretability, with the potential to enable more personalized
patient care.

</details>


### [129] [VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](https://arxiv.org/abs/2508.06757)
*Yash Garg,Saketh Bachu,Arindam Dutta,Rohit Lal,Sarosij Bose,Calvin-Khang Ta,M. Salman Asif,Amit Roy-Chowdhury*

Main category: cs.CV

TL;DR: The paper introduces VOccl3D, a realistic video-based occlusion dataset for 3D human pose and shape estimation, addressing gaps in existing datasets. It shows improved performance of HPS methods when fine-tuned on this dataset.


<details>
  <summary>Details</summary>
Motivation: Existing HPS methods struggle with complex poses and occlusions, and current datasets lack realistic occlusion scenarios. This work aims to provide a more realistic benchmark.

Method: The dataset was created using advanced rendering techniques, incorporating diverse occlusions, clothing, and motions. HPS methods (CLIFF, BEDLAM-CLIFF) were fine-tuned on VOccl3D.

Result: Fine-tuned methods showed significant improvements on public datasets and the test split of VOccl3D. An enhanced object detector (YOLO11) also improved human detection under occlusion.

Conclusion: VOccl3D is a valuable resource for benchmarking occlusion-handling methods, offering a more realistic alternative to existing datasets.

Abstract: Human pose and shape (HPS) estimation methods have been extensively studied,
with many demonstrating high zero-shot performance on in-the-wild images and
videos. However, these methods often struggle in challenging scenarios
involving complex human poses or significant occlusions. Although some studies
address 3D human pose estimation under occlusion, they typically evaluate
performance on datasets that lack realistic or substantial occlusions, e.g.,
most existing datasets introduce occlusions with random patches over the human
or clipart-style overlays, which may not reflect real-world challenges. To
bridge this gap in realistic occlusion datasets, we introduce a novel benchmark
dataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and
shape annotations. Inspired by works such as AGORA and BEDLAM, we constructed
this dataset using advanced computer graphics rendering techniques,
incorporating diverse real-world occlusion scenarios, clothing textures, and
human motions. Additionally, we fine-tuned recent HPS methods, CLIFF and
BEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and
quantitative improvements across multiple public datasets, as well as on the
test split of our dataset, while comparing its performance with other
state-of-the-art methods. Furthermore, we leveraged our dataset to enhance
human detection performance under occlusion by fine-tuning an existing object
detector, YOLO11, thus leading to a robust end-to-end HPS estimation system
under occlusions. Overall, this dataset serves as a valuable resource for
future research aimed at benchmarking methods designed to handle occlusions,
offering a more realistic alternative to existing occlusion datasets. See the
Project page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/

</details>


### [130] [SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding](https://arxiv.org/abs/2508.06763)
*Zihao Sheng,Zilin Huang,Yen-Jung Chen,Yansong Qu,Yuhao Luo,Yue Leng,Sikai Chen*

Main category: cs.CV

TL;DR: SafePLUG enhances MLLMs for fine-grained traffic accident analysis with pixel-level understanding and temporal grounding, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack fine-grained visual detail handling in traffic accidents, limiting their effectiveness in complex scenarios.

Method: Proposes SafePLUG, supporting region-aware QA, pixel-level segmentation, and temporal event recognition via a new multimodal dataset.

Result: SafePLUG excels in region-based QA, segmentation, temporal localization, and accident understanding.

Conclusion: SafePLUG advances fine-grained traffic scene analysis, improving safety and situational awareness in smart transportation.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress
across a range of vision-language tasks and demonstrate strong potential for
traffic accident understanding. However, existing MLLMs in this domain
primarily focus on coarse-grained image-level or video-level comprehension and
often struggle to handle fine-grained visual details or localized scene
components, limiting their applicability in complex accident scenarios. To
address these limitations, we propose SafePLUG, a novel framework that empowers
MLLMs with both Pixel-Level Understanding and temporal Grounding for
comprehensive traffic accident analysis. SafePLUG supports both
arbitrary-shaped visual prompts for region-aware question answering and
pixel-level segmentation based on language instructions, while also enabling
the recognition of temporally anchored events in traffic accident scenarios. To
advance the development of MLLMs for traffic accident understanding, we curate
a new dataset containing multimodal question-answer pairs centered on diverse
accident scenarios, with detailed pixel-level annotations and temporal event
boundaries. Experimental results show that SafePLUG achieves strong performance
on multiple tasks, including region-based question answering, pixel-level
segmentation, temporal event localization, and accident event understanding.
These capabilities lay a foundation for fine-grained understanding of complex
traffic scenes, with the potential to improve driving safety and enhance
situational awareness in smart transportation systems. The code, dataset, and
model checkpoints will be made publicly available at:
https://zihaosheng.github.io/SafePLUG

</details>


### [131] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: DiffUS is a differentiable ultrasound renderer that synthesizes realistic B-mode images from MRI scans, aiding surgical guidance by bridging preoperative planning and intraoperative imaging.


<details>
  <summary>Details</summary>
Motivation: Intraoperative ultrasound imaging is hindered by noise, artifacts, and misalignment with preoperative scans, necessitating a tool to improve accuracy and usability.

Method: DiffUS converts MRI scans to acoustic impedance volumes, simulates ultrasound beam propagation via ray tracing, and reconstructs B-mode images with realistic artifacts using differentiable tensor operations.

Result: DiffUS generates anatomically accurate ultrasound images from brain MRI data, as validated on the ReMIND dataset.

Conclusion: DiffUS effectively bridges preoperative and intraoperative imaging, offering potential for applications like slice-to-volume registration and volumetric reconstruction.

Abstract: Intraoperative ultrasound imaging provides real-time guidance during numerous
surgical procedures, but its interpretation is complicated by noise, artifacts,
and poor alignment with high-resolution preoperative MRI/CT scans. To bridge
the gap between reoperative planning and intraoperative guidance, we present
DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes
realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D
scans into acoustic impedance volumes using a machine learning approach. Next,
we simulate ultrasound beam propagation using ray tracing with coupled
reflection-transmission equations. DiffUS formulates wave propagation as a
sparse linear system that captures multiple internal reflections. Finally, we
reconstruct B-mode images via depth-resolved echo extraction across fan-shaped
acquisition geometry, incorporating realistic artifacts including speckle noise
and depth-dependent degradation. DiffUS is entirely implemented as
differentiable tensor operations in PyTorch, enabling gradient-based
optimization for downstream applications such as slice-to-volume registration
and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates
DiffUS's ability to generate anatomically accurate ultrasound images from brain
MRI data.

</details>


### [132] [Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling](https://arxiv.org/abs/2508.06805)
*Aarav Mehta,Priya Deshmukh,Vikram Singh,Siddharth Malhotra,Krishnan Menon Iyer,Tanvi Iyer*

Main category: cs.CV

TL;DR: A novel crisp edge detector for medical images improves boundary localization, enhancing segmentation, registration, and lesion delineation.


<details>
  <summary>Details</summary>
Motivation: Precise organ boundary localization is crucial for medical tasks like segmentation and radiotherapy, but current ConvNet edge detectors lack the required accuracy.

Method: A top-down backward refinement architecture fuses high-level semantic features with low-level cues, extended for anisotropic volumes with 2D slice-wise refinement and light 3D context.

Result: Outperforms baseline ConvNet and contemporary methods in boundary localization metrics (F-measure, Hausdorff distance) and improves downstream tasks like segmentation.

Conclusion: The proposed method delivers clinically valuable, high-resolution organ edges, significantly enhancing medical-imaging applications.

Abstract: Accurate localization of organ boundaries is critical in medical imaging for
segmentation, registration, surgical planning, and radiotherapy. While deep
convolutional networks (ConvNets) have advanced general-purpose edge detection
to near-human performance on natural images, their outputs often lack precise
localization, a limitation that is particularly harmful in medical applications
where millimeter-level accuracy is required. Building on a systematic analysis
of ConvNet edge outputs, we propose a medically focused crisp edge detector
that adapts a novel top-down backward refinement architecture to medical images
(2D and volumetric). Our method progressively upsamples and fuses high-level
semantic features with fine-grained low-level cues through a backward
refinement pathway, producing high-resolution, well-localized organ boundaries.
We further extend the design to handle anisotropic volumes by combining 2D
slice-wise refinement with light 3D context aggregation to retain computational
efficiency. Evaluations on several CT and MRI organ datasets demonstrate
substantially improved boundary localization under strict criteria (boundary
F-measure, Hausdorff distance) compared to baseline ConvNet detectors and
contemporary medical edge/contour methods. Importantly, integrating our crisp
edge maps into downstream pipelines yields consistent gains in organ
segmentation (higher Dice scores, lower boundary errors), more accurate image
registration, and improved delineation of lesions near organ interfaces. The
proposed approach produces clinically valuable, crisp organ edges that
materially enhance common medical-imaging tasks.

</details>


### [133] [DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation](https://arxiv.org/abs/2508.06816)
*Vikram Singh,Kabir Malhotra,Rohan Desai,Ananya Shankaracharya,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: A novel dual-resolution architecture for melanocytic tumor segmentation in dermoscopic images, combining full-resolution and pooled streams with boundary-aware connections and artifact suppression.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation is crucial for skin cancer screening, but challenges include subtle variations, artifacts, and precise boundary needs.

Method: ResNet-inspired dual-resolution architecture with boundary-aware connections, channel attention, artifact suppression, and multi-task training.

Result: Improved boundary adherence and segmentation metrics on public benchmarks, outperforming standard baselines.

Conclusion: The method is effective for automated melanoma assessment, offering pixel-accurate masks without heavy post-processing.

Abstract: Accurate segmentation of melanocytic tumors in dermoscopic images is a
critical step for automated skin cancer screening and clinical decision
support. Unlike natural scene segmentation, lesion delineation must reconcile
subtle texture and color variations, frequent artifacts (hairs, rulers,
bubbles), and a strong need for precise boundary localization to support
downstream diagnosis. In this paper we introduce Our method, a novel ResNet
inspired dual resolution architecture specifically designed for melanocytic
tumor segmentation. Our method maintains a full resolution stream that
preserves fine grained boundary information while a complementary pooled stream
aggregates multi scale contextual cues for robust lesion recognition. The
streams are tightly coupled by boundary aware residual connections that inject
high frequency edge information into deep feature maps, and by a channel
attention module that adapts color and texture sensitivity to dermoscopic
appearance. To further address common imaging artifacts and the limited size of
clinical datasets, we propose a lightweight artifact suppression block and a
multi task training objective that combines a Dice Tversky segmentation loss
with an explicit boundary loss and a contrastive regularizer for feature
stability. The combined design yields pixel accurate masks without requiring
heavy post processing or complex pre training protocols. Extensive experiments
on public dermoscopic benchmarks demonstrate that Our method significantly
improves boundary adherence and clinically relevant segmentation metrics
compared to standard encoder decoder baselines, making it a practical building
block for automated melanoma assessment systems.

</details>


### [134] [VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation](https://arxiv.org/abs/2508.06819)
*Ayaan Nooruddin Siddiqui,Mahnoor Zaidi,Ayesha Nazneen Shahbaz,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: A weakly supervised framework for subcutaneous vessel segmentation uses sparse annotations and probabilistic label propagation to reduce annotation burden while improving accuracy and topology preservation.


<details>
  <summary>Details</summary>
Motivation: Accurate vessel segmentation is challenging due to scarce ground truth and low contrast in clinical images. The goal is to reduce annotation effort while maintaining clinical relevance.

Method: The framework leverages sparse annotations (e.g., centerlines, dots) and expands them into dense supervision via a differentiable random walk model. It jointly learns label propagation and CNN-based segmentation, incorporating uncertainty-weighted loss and topology-aware regularization.

Result: Outperforms naive sparse label training and pseudo-labeling, producing more complete vascular maps and better uncertainty calibration.

Conclusion: The method reduces annotation burden, preserves vessel topology, and improves segmentation accuracy, making it clinically viable.

Abstract: Accurate segmentation of subcutaneous vessels from clinical images is
hampered by scarce, expensive ground truth and by low contrast, noisy
appearance of vessels across patients and modalities. We present a novel weakly
supervised training framework tailored for subcutaneous vessel segmentation
that leverages inexpensive sparse annotations (e.g., centerline traces, dot
markers, or short scribbles). Sparse labels are expanded into dense,
probabilistic supervision via a differentiable random walk label propagation
model whose transition weights incorporate image driven vesselness cues and
tubular continuity priors. The propagation yields per-pixel hitting
probabilities together with calibrated uncertainty estimates; these are
incorporated into an uncertainty weighted loss to avoid over fitting to
ambiguous regions. Crucially, the label-propagator is learned jointly with a
CNN based segmentation predictor, enabling the system to discover vessel edges
and continuity constraints without explicit edge supervision. We further
introduce a topology aware regularizer that encourages centerline connectivity
and penalizes spurious branches, improving clinical usability. In experiments
on clinical subcutaneous imaging datasets, our method consistently outperforms
naive training on sparse labels and conventional dense pseudo-labeling,
producing more complete vascular maps and better calibrated uncertainty for
downstream decision making. The approach substantially reduces annotation
burden while preserving clinically relevant vessel topology.

</details>


### [135] [Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification](https://arxiv.org/abs/2508.06831)
*Taha Mustapha Nehdi,Nairouz Mrabah,Atif Belal,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: SAGE-reID introduces a cost-effective, source-free MSDA method for person reID, using low-rank adapters and a gating network for efficient cross-domain knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of adapting person reID models to new environments without source data access, while reducing computational costs and overfitting risks.

Method: Trains source-specific low-rank adapters (LoRA) via source-free UDA, then uses a lightweight gating network to dynamically merge LoRA experts for cross-domain knowledge transfer.

Result: Outperforms state-of-the-art methods on benchmarks (Market-1501, DukeMTMC-reID, MSMT17) with computational efficiency.

Conclusion: SAGE-reID is a scalable, efficient solution for multi-source domain adaptation in person reID, balancing accuracy and resource usage.

Abstract: Adapting person re-identification (reID) models to new target environments
remains a challenging problem that is typically addressed using unsupervised
domain adaptation (UDA) methods. Recent works show that when labeled data
originates from several distinct sources (e.g., datasets and cameras),
considering each source separately and applying multi-source domain adaptation
(MSDA) typically yields higher accuracy and robustness compared to blending the
sources and performing conventional UDA. However, state-of-the-art MSDA methods
learn domain-specific backbone models or require access to source domain data
during adaptation, resulting in significant growth in training parameters and
computational cost. In this paper, a Source-free Adaptive Gated Experts
(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a
cost-effective, source-free MSDA method that first trains individual
source-specific low-rank adapters (LoRA) through source-free UDA. Next, a
lightweight gating network is introduced and trained to dynamically assign
optimal merging weights for fusion of LoRA experts, enabling effective
cross-domain knowledge transfer. While the number of backbone parameters
remains constant across source domains, LoRA experts scale linearly but remain
negligible in size (<= 2% of the backbone), reducing both the memory
consumption and risk of overfitting. Extensive experiments conducted on three
challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that
SAGE-reID outperforms state-of-the-art methods while being computationally
efficient.

</details>


### [136] [Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology](https://arxiv.org/abs/2508.06845)
*Hamidreza Samadi,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.CV

TL;DR: A hybrid ML framework improves geometric deviation forecasting in manufacturing, achieving 73% better accuracy than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Maintaining dimensional precision in complex geometries is challenging despite advancements in manufacturing.

Method: Uses a high-resolution 3D scanner for multi-angle surface data, processed with alignment, noise reduction, and merging. A hybrid ML framework (CNNs + gradient-boosted trees) predicts deviations.

Result: Achieved 0.012 mm prediction accuracy (95% confidence), 73% improvement over conventional methods, and uncovered hidden parameter-deviation correlations.

Conclusion: The approach enhances automated quality control, predictive maintenance, and design optimization, with the dataset aiding future research.

Abstract: This study addresses the challenge of accurately forecasting geometric
deviations in manufactured components using advanced 3D surface analysis.
Despite progress in modern manufacturing, maintaining dimensional precision
remains difficult, particularly for complex geometries. We present a
methodology that employs a high-resolution 3D scanner to acquire multi-angle
surface data from 237 components produced across different batches. The data
were processed through precise alignment, noise reduction, and merging
techniques to generate accurate 3D representations. A hybrid machine learning
framework was developed, combining convolutional neural networks for feature
extraction with gradient-boosted decision trees for predictive modeling. The
proposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence
level, representing a 73% improvement over conventional statistical process
control methods. In addition to improved accuracy, the model revealed hidden
correlations between manufacturing parameters and geometric deviations. This
approach offers significant potential for automated quality control, predictive
maintenance, and design optimization in precision manufacturing, and the
resulting dataset provides a strong foundation for future predictive modeling
research.

</details>


### [137] [AGIC: Attention-Guided Image Captioning to Improve Caption Relevance](https://arxiv.org/abs/2508.06853)
*L. D. M. S. Sai Teja,Ashok Urlana,Pruthwik Mishra*

Main category: cs.CV

TL;DR: AGIC improves image captioning by focusing on salient visual regions and using a hybrid decoding strategy, outperforming state-of-the-art models with faster inference.


<details>
  <summary>Details</summary>
Motivation: Generating accurate and descriptive captions for images remains a challenge despite progress in the field.

Method: AGIC amplifies salient visual regions in feature space and employs a hybrid decoding strategy (deterministic and probabilistic sampling) for caption generation.

Result: AGIC matches or surpasses state-of-the-art models on Flickr8k and Flickr30k datasets, with faster inference and strong performance across metrics.

Conclusion: AGIC offers a scalable, interpretable, and high-performing solution for image captioning.

Abstract: Despite significant progress in image captioning, generating accurate and
descriptive captions remains a long-standing challenge. In this study, we
propose Attention-Guided Image Captioning (AGIC), which amplifies salient
visual regions directly in the feature space to guide caption generation. We
further introduce a hybrid decoding strategy that combines deterministic and
probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we
conduct extensive experiments on the Flickr8k and Flickr30k datasets. The
results show that AGIC matches or surpasses several state-of-the-art models
while achieving faster inference. Moreover, AGIC demonstrates strong
performance across multiple evaluation metrics, offering a scalable and
interpretable solution for image captioning.

</details>


### [138] [A Joint Sparse Self-Representation Learning Method for Multiview Clustering](https://arxiv.org/abs/2508.06857)
*Mengxue Jia,Zhihua Allen-Zhao,You Zhao,Sanyang Liu*

Main category: cs.CV

TL;DR: A novel joint sparse self-representation learning model for multiview clustering (MC) is proposed, using cardinality constraints for local information extraction and a low-rank constraint for global structure. An alternating quadratic penalty (AQP) method ensures convergence, outperforming eight state-of-the-art algorithms.


<details>
  <summary>Details</summary>
Motivation: To improve multiview clustering by extracting reliable local and global structure information without relying on Graph-Laplacian regularization.

Method: Introduces cardinality constraints for view-specific local information and a low-rank constraint for global coherence. Uses an AQP method for convergence.

Result: Outperforms eight state-of-the-art algorithms on six standard datasets.

Conclusion: The proposed model and AQP method effectively address nonconvex, nonsmooth challenges, enhancing multiview clustering performance.

Abstract: Multiview clustering (MC) aims to group samples using consistent and
complementary information across various views. The subspace clustering, as a
fundamental technique of MC, has attracted significant attention. In this
paper, we propose a novel joint sparse self-representation learning model for
MC, where a featured difference is the extraction of view-specific local
information by introducing cardinality (i.e., $\ell_0$-norm) constraints
instead of Graph-Laplacian regularization. Specifically, under each view,
cardinality constraints directly restrict the samples used in the
self-representation stage to extract reliable local and global structure
information, while the low-rank constraint aids in revealing a global coherent
structure in the consensus affinity matrix during merging. The attendant
challenge is that Augmented Lagrange Method (ALM)-based alternating
minimization algorithms cannot guarantee convergence when applied directly to
our nonconvex, nonsmooth model, thus resulting in poor generalization ability.
To address it, we develop an alternating quadratic penalty (AQP) method with
global convergence, where two subproblems are iteratively solved by closed-form
solutions. Empirical results on six standard datasets demonstrate the
superiority of our model and AQP method, compared to eight state-of-the-art
algorithms.

</details>


### [139] [VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding](https://arxiv.org/abs/2508.06869)
*Jianxiang He,Shaoguang Wang,Weiyu Guo,Meisheng Hong,Jungang Li,Yijie Xu,Ziyang Chen,Hui Xiong*

Main category: cs.CV

TL;DR: VSI integrates subtitles, timestamps, and scene boundaries for better keyframe retrieval in long videos, improving accuracy in localization and QA tasks.


<details>
  <summary>Details</summary>
Motivation: Existing keyframe retrieval methods struggle with weak multimodal alignment and temporal semantic capture.

Method: VSI uses a dual-stream search mechanism (Video Search Stream and Subtitle Match Stream) to integrate visual and textual information.

Result: VSI achieves 40.00% keyframe localization accuracy and 68.48% QA accuracy, outperforming baselines by 20.35% and 15.79%.

Conclusion: VSI is robust and generalizable, achieving SOTA in medium-to-long video-QA tasks.

Abstract: Long video understanding presents a significant challenge to multimodal large
language models (MLLMs) primarily due to the immense data scale. A critical and
widely adopted strategy for making this task computationally tractable is
keyframe retrieval, which seeks to identify a sparse set of video frames that
are most salient to a given textual query. However, the efficacy of this
approach is hindered by weak multimodal alignment between textual queries and
visual content and fails to capture the complex temporal semantic information
required for precise reasoning. To address this, we propose Visual-Subtitle
Integeration(VSI), a multimodal keyframe search method that integrates
subtitles, timestamps, and scene boundaries into a unified multimodal search
process. The proposed method captures the visual information of video frames as
well as the complementary textual information through a dual-stream search
mechanism by Video Search Stream as well as Subtitle Match Stream,
respectively, and improves the keyframe search accuracy through the interaction
of the two search streams. Experimental results show that VSI achieve 40.00%
key frame localization accuracy on the text-relevant subset of LongVideoBench
and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive
baselines by 20.35% and 15.79%, respectively. Furthermore, on the
LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA
tasks, demonstrating the robustness and generalizability of the proposed
multimodal search strategy.

</details>


### [140] [NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective](https://arxiv.org/abs/2508.06878)
*Maoxun Yuan,Duanni Meng,Ziteng Xi,Tianyi Zhao,Shiji Zhao,Yimian Dai,Xingxing Wei*

Main category: cs.CV

TL;DR: The paper introduces NS-FPN, a noise-suppression feature pyramid network for infrared small target detection, addressing false alarms by focusing on noise suppression in the frequency domain.


<details>
  <summary>Details</summary>
Motivation: Infrared small target detection is challenging due to dim, shapeless targets and background clutter. Existing CNN methods enhance features but increase false alarms.

Method: Proposes NS-FPN with a low-frequency guided feature purification (LFP) module for noise suppression and a spiral-aware feature sampling (SFS) module for feature fusion.

Result: NS-FPN significantly reduces false alarms and outperforms existing methods on public IRSTDS datasets.

Conclusion: The lightweight and effective NS-FPN can be integrated into existing frameworks, improving performance in IRSTDS tasks.

Abstract: Infrared small target detection and segmentation (IRSTDS) is a critical yet
challenging task in defense and civilian applications, owing to the dim,
shapeless appearance of targets and severe background clutter. Recent CNN-based
methods have achieved promising target perception results, but they only focus
on enhancing feature representation to offset the impact of noise, which
results in the increased false alarms problem. In this paper, through analyzing
the problem from the frequency domain, we pioneer in improving performance from
noise suppression perspective and propose a novel noise-suppression feature
pyramid network (NS-FPN), which integrates a low-frequency guided feature
purification (LFP) module and a spiral-aware feature sampling (SFS) module into
the original FPN structure. The LFP module suppresses the noise features by
purifying high-frequency components to achieve feature enhancement devoid of
noise interference, while the SFS module further adopts spiral sampling to fuse
target-relevant features in feature fusion process. Our NS-FPN is designed to
be lightweight yet effective and can be easily plugged into existing IRSTDS
frameworks. Extensive experiments on the public IRSTDS datasets demonstrate
that our method significantly reduces false alarms and achieves superior
performance on IRSTDS tasks.

</details>


### [141] [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/abs/2508.06895)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: The paper proposes BASIC, a method to improve visual-textual alignment in MLLMs by directly supervising visual embeddings using refined embeddings from LLM layers, enhancing performance without extra supervision.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs treat visual embeddings as contextual cues without direct supervision, limiting finer alignment between visual and textual modalities.

Method: BASIC uses refined visual embeddings from LLM layers to guide the vision projector, optimizing embedding directions and semantic matching.

Result: BASIC significantly boosts MLLM performance across benchmarks without requiring additional models or annotations.

Conclusion: Direct visual supervision via BASIC effectively enhances visual-textual alignment in MLLMs, improving overall performance.

Abstract: Mainstream Multimodal Large Language Models (MLLMs) achieve visual
understanding by using a vision projector to bridge well-pretrained vision
encoders and large language models (LLMs). The inherent gap between visual and
textual modalities makes the embeddings from the vision projector critical for
visual comprehension. However, current alignment approaches treat visual
embeddings as contextual cues and merely apply auto-regressive supervision to
textual outputs, neglecting the necessity of introducing equivalent direct
visual supervision, which hinders the potential finer alignment of visual
embeddings. In this paper, based on our analysis of the refinement process of
visual embeddings in the LLM's shallow layers, we propose BASIC, a method that
utilizes refined visual embeddings within the LLM as supervision to directly
guide the projector in generating initial visual embeddings. Specifically, the
guidance is conducted from two perspectives: (i) optimizing embedding
directions by reducing angles between initial and supervisory embeddings in
semantic space; (ii) improving semantic matching by minimizing disparities
between the logit distributions of both visual embeddings. Without additional
supervisory models or artificial annotations, BASIC significantly improves the
performance of MLLMs across a wide range of benchmarks, demonstrating the
effectiveness of our introduced direct visual supervision.

</details>


### [142] [Advancements in Chinese font generation since deep learning era: A survey](https://arxiv.org/abs/2508.06900)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: A survey of deep learning-based Chinese font generation methods, categorizing them into many-shot and few-shot approaches, and discussing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of improving the quality of generated Chinese character images and provide a comprehensive review of recent deep learning techniques in this field.

Method: The paper categorizes existing methods into many-shot and few-shot font generation, reviews fundamentals like architectures and datasets, and evaluates strengths and limitations of representative approaches.

Result: A detailed analysis of current techniques, highlighting their progress and limitations in Chinese font generation.

Conclusion: Identifies challenges and future directions to guide researchers in advancing Chinese font generation using deep learning.

Abstract: Chinese font generation aims to create a new Chinese font library based on
some reference samples. It is a topic of great concern to many font designers
and typographers. Over the past years, with the rapid development of deep
learning algorithms, various new techniques have achieved flourishing and
thriving progress. Nevertheless, how to improve the overall quality of
generated Chinese character images remains a tough issue. In this paper, we
conduct a holistic survey of the recent Chinese font generation approaches
based on deep learning. To be specific, we first illustrate the research
background of the task. Then, we outline our literature selection and analysis
methodology, and review a series of related fundamentals, including classical
deep learning architectures, font representation formats, public datasets, and
frequently-used evaluation metrics. After that, relying on the number of
reference samples required to generate a new font, we categorize the existing
methods into two major groups: many-shot font generation and few-shot font
generation methods. Within each category, representative approaches are
summarized, and their strengths and limitations are also discussed in detail.
Finally, we conclude our paper with the challenges and future directions, with
the expectation to provide some valuable illuminations for the researchers in
this field.

</details>


### [143] [eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos](https://arxiv.org/abs/2508.06902)
*Xuecheng Wu,Dingkang Yang,Danlei Huang,Xinyi Yin,Yifan Wang,Jia Zhang,Jiayu Nie,Liangyu Fu,Yang Liu,Junxiao Xue,Hadi Amirpour,Wei Zhou*

Main category: cs.CV

TL;DR: The paper introduces eMotions, a large-scale dataset for video emotion analysis (VEA) in short-form videos (SVs), and proposes AV-CANet, an audio-visual fusion network to address challenges in SV emotion analysis.


<details>
  <summary>Details</summary>
Motivation: Short-form videos (SVs) are widely used but pose challenges for emotion analysis due to their multimodal complexity and lack of labeled data.

Method: The authors introduce eMotions, a dataset with 27,996 annotated videos, and propose AV-CANet, an end-to-end audio-visual fusion network with a Local-Global Fusion Module and EP-CE Loss.

Result: AV-CANet performs well on eMotions and public VEA datasets, validated through extensive experiments and ablation studies.

Conclusion: The work provides a robust dataset and method for SV emotion analysis, offering insights for future research.

Abstract: Short-form videos (SVs) have become a vital part of our online routine for
acquiring and sharing information. Their multimodal complexity poses new
challenges for video analysis, highlighting the need for video emotion analysis
(VEA) within the community. Given the limited availability of SVs emotion data,
we introduce eMotions, a large-scale dataset consisting of 27,996 videos with
full-scale annotations. To ensure quality and reduce subjective bias, we
emphasize better personnel allocation and propose a multi-stage annotation
procedure. Additionally, we provide the category-balanced and test-oriented
variants through targeted sampling to meet diverse needs. While there have been
significant studies on videos with clear emotional cues (e.g., facial
expressions), analyzing emotions in SVs remains a challenging task. The
challenge arises from the broader content diversity, which introduces more
distinct semantic gaps and complicates the representations learning of
emotion-related features. Furthermore, the prevalence of audio-visual
co-expressions in SVs leads to the local biases and collective information gaps
caused by the inconsistencies in emotional expressions. To tackle this, we
propose AV-CANet, an end-to-end audio-visual fusion network that leverages
video transformer to capture semantically relevant representations. We further
introduce the Local-Global Fusion Module designed to progressively capture the
correlations of audio-visual features. Besides, EP-CE Loss is constructed to
globally steer optimizations with tripolar penalties. Extensive experiments
across three eMotions-related datasets and four public VEA datasets demonstrate
the effectiveness of our proposed AV-CANet, while providing broad insights for
future research. Moreover, we conduct ablation studies to examine the critical
components of our method. Dataset and code will be made available at Github.

</details>


### [144] [A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2508.06904)
*Chao Yin,Jide Li,Xiaoqiang Li*

Main category: cs.CV

TL;DR: The paper introduces IAPF, a training-free COS method that converts generic prompts into fine-grained instance masks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current training-free COS methods using SAM produce coarse semantic masks, failing for multiple camouflaged instances.

Method: IAPF uses MLLM for tags, Grounding DINO for instance prompts, and SAM for masks, with a voting mechanism for consistency.

Result: IAPF significantly outperforms state-of-the-art training-free COS methods on benchmarks.

Conclusion: IAPF effectively addresses the limitation of coarse masks in training-free COS, achieving superior performance.

Abstract: Camouflaged Object Segmentation (COS) remains highly challenging due to the
intrinsic visual similarity between target objects and their surroundings.
While training-based COS methods achieve good performance, their performance
degrades rapidly with increased annotation sparsity. To circumvent this
limitation, recent studies have explored training-free COS methods, leveraging
the Segment Anything Model (SAM) by automatically generating visual prompts
from a single task-generic prompt (\textit{e.g.}, "\textit{camouflaged
animal}") uniformly applied across all test images. However, these methods
typically produce only semantic-level visual prompts, causing SAM to output
coarse semantic masks and thus failing to handle scenarios with multiple
discrete camouflaged instances effectively. To address this critical
limitation, we propose a simple yet powerful \textbf{I}nstance-\textbf{A}ware
\textbf{P}rompting \textbf{F}ramework (IAPF), the first training-free COS
pipeline that explicitly converts a task-generic prompt into fine-grained
instance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt
Generator, utilizing task-generic queries to prompt a Multimodal Large Language
Model (MLLM) for generating image-specific foreground and background tags; (2)
\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise
instance-level bounding box prompts, alongside the proposed Single-Foreground
Multi-Background Prompting strategy to sample region-constrained point prompts
within each box, enabling SAM to yield a candidate instance mask; (3)
Self-consistency Instance Mask Voting, which selects the final COS prediction
by identifying the candidate mask most consistent across multiple candidate
instance masks. Extensive evaluations on standard COS benchmarks demonstrate
that the proposed IAPF significantly surpasses existing state-of-the-art
training-free COS methods.

</details>


### [145] [MultiRef: Controllable Image Generation with Multiple Visual References](https://arxiv.org/abs/2508.06905)
*Ruoxi Chen,Dongping Chen,Siyuan Wu,Sinan Wang,Shiyun Lang,Petr Sushko,Gaoyang Jiang,Yao Wan,Ranjay Krishna*

Main category: cs.CV

TL;DR: The paper introduces MultiRef-bench, a framework for evaluating multi-reference image generation, and highlights the limitations of current models in handling multiple visual references.


<details>
  <summary>Details</summary>
Motivation: Current image generative frameworks rely on single-source inputs, limiting their ability to mimic human creativity, which draws from multiple visual references.

Method: The authors develop MultiRef-bench, a dataset with synthetic and real-world samples, and evaluate three image-text models and six agentic frameworks.

Result: State-of-the-art models struggle with multi-reference conditioning, with the best model achieving 66.6% (synthetic) and 79.0% (real-world) accuracy.

Conclusion: The findings highlight the need for more flexible generative tools that can integrate multiple visual references, and the dataset is made publicly available for further research.

Abstract: Visual designers naturally draw inspiration from multiple visual references,
combining diverse elements and aesthetic principles to create artwork. However,
current image generative frameworks predominantly rely on single-source inputs
-- either text prompts or individual reference images. In this paper, we focus
on the task of controllable image generation using multiple visual references.
We introduce MultiRef-bench, a rigorous evaluation framework comprising 990
synthetic and 1,000 real-world samples that require incorporating visual
content from multiple reference images. The synthetic samples are synthetically
generated through our data engine RefBlend, with 10 reference types and 33
reference combinations. Based on RefBlend, we further construct a dataset
MultiRef containing 38k high-quality images to facilitate further research. Our
experiments across three interleaved image-text models (i.e., OmniGen, ACE, and
Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that
even state-of-the-art systems struggle with multi-reference conditioning, with
the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in
real-world cases on average compared to the golden answer. These findings
provide valuable directions for developing more flexible and human-like
creative tools that can effectively integrate multiple sources of visual
inspiration. The dataset is publicly available at: https://multiref.github.io/.

</details>


### [146] [MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification](https://arxiv.org/abs/2508.06908)
*Jinhao Li,Zijian Chen,Lirong Deng,Changbo Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces MMReID-Bench, a multi-task multi-modal benchmark for person re-identification (ReID), leveraging multi-modal large language models (MLLMs) to address limitations of traditional uni-modal ReID models.


<details>
  <summary>Details</summary>
Motivation: Traditional person ReID models lack generalization in multi-modal data (e.g., RGB, thermal, infrared). MLLMs offer potential but are underutilized.

Method: Developed MMReID-Bench with 20,710 multi-modal queries and gallery images across 10 ReID tasks to evaluate MLLMs.

Result: MLLMs show strong performance in multi-modal ReID but struggle with thermal and infrared data.

Conclusion: MMReID-Bench aims to advance robust multi-modal foundation models for person ReID.

Abstract: Person re-identification (ReID) aims to retrieve the images of an interested
person in the gallery images, with wide applications in medical rehabilitation,
abnormal behavior detection, and public security. However, traditional person
ReID models suffer from uni-modal capability, leading to poor generalization
ability in multi-modal data, such as RGB, thermal, infrared, sketch images,
textual descriptions, etc. Recently, the emergence of multi-modal large
language models (MLLMs) shows a promising avenue for addressing this problem.
Despite this potential, existing methods merely regard MLLMs as feature
extractors or caption generators, which do not fully unleash their reasoning,
instruction-following, and cross-modal understanding capabilities. To bridge
this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark
specifically designed for person ReID. The MMReID-Bench includes 20,710
multi-modal queries and gallery images covering 10 different person ReID tasks.
Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in
delivering effective and versatile person ReID. Nevertheless, they also have
limitations in handling a few modalities, particularly thermal and infrared
data. We hope MMReID-Bench can facilitate the community to develop more robust
and generalizable multimodal foundation models for person ReID.

</details>


### [147] [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](https://arxiv.org/abs/2508.06916)
*Shichao Ma,Yunhe Guo,Jiahao Su,Qihe Huang,Zhengyang Zhou,Yang Wang*

Main category: cs.CV

TL;DR: Talk2Image is a multi-agent system for interactive image generation and editing in multi-turn dialogues, outperforming existing methods in controllability, coherence, and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image systems struggle with iterative, multi-turn creative tasks, often causing intention drift and incoherent edits.

Method: Talk2Image integrates intention parsing, task decomposition across specialized agents, and feedback-driven refinement via multi-view evaluation.

Result: The system outperforms baselines in controllability, coherence, and user satisfaction for iterative tasks.

Conclusion: Talk2Image effectively addresses limitations of single-agent systems, enabling consistent and aligned multi-turn image generation and editing.

Abstract: Text-to-image generation tasks have driven remarkable advances in diverse
media applications, yet most focus on single-turn scenarios and struggle with
iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to
bridge this gap, but their single-agent, sequential paradigm often causes
intention drift and incoherent edits. To address these limitations, we present
Talk2Image, a novel multi-agent system for interactive image generation and
editing in multi-turn dialogue scenarios. Our approach integrates three key
components: intention parsing from dialogue history, task decomposition and
collaborative execution across specialized agents, and feedback-driven
refinement based on a multi-view evaluation mechanism. Talk2Image enables
step-by-step alignment with user intention and consistent image editing.
Experiments demonstrate that Talk2Image outperforms existing baselines in
controllability, coherence, and user satisfaction across iterative image
generation and editing tasks.

</details>


### [148] [AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](https://arxiv.org/abs/2508.06924)
*Shihao Yuan,Yahui Liu,Yang Yue,Jingyuan Zhang,Wangmeng Zuo,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: AR-GRPO integrates online RL training into autoregressive image generation models, improving image quality and human preference over standard AR baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance autoregressive image generation models by leveraging RL, addressing quality dimensions like perceptual quality, realism, and semantic fidelity.

Method: Adapts the GRPO algorithm with reward functions for multi-dimensional image evaluation, applied to class- and text-conditional image generation tasks.

Result: Significant improvements in image quality and human preference across various metrics compared to standard AR baselines.

Conclusion: RL-based optimization is viable for AR image generation, enabling controllable and high-quality synthesis.

Abstract: Inspired by the success of reinforcement learning (RL) in refining large
language models (LLMs), we propose AR-GRPO, an approach to integrate online RL
training into autoregressive (AR) image generation models. We adapt the Group
Relative Policy Optimization (GRPO) algorithm to refine the vanilla
autoregressive models' outputs by carefully designed reward functions that
evaluate generated images across multiple quality dimensions, including
perceptual quality, realism, and semantic fidelity. We conduct comprehensive
experiments on both class-conditional (i.e., class-to-image) and
text-conditional (i.e., text-to-image) image generation tasks, demonstrating
that our RL-enhanced framework significantly improves both the image quality
and human preference of generated images compared to the standard AR baselines.
Our results show consistent improvements across various evaluation metrics,
establishing the viability of RL-based optimization for AR image generation and
opening new avenues for controllable and high-quality image synthesis. The
source codes and models are available at:
https://github.com/Kwai-Klear/AR-GRPO.

</details>


### [149] [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](https://arxiv.org/abs/2508.06937)
*Weiyan Xie,Han Gao,Didan Deng,Kaican Li,April Hua Liu,Yongxiang Huang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: CannyEdit is a training-free framework for precise, text-driven image editing, balancing text adherence, context fidelity, and seamless integration.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models struggle with balancing text adherence, context fidelity, and seamless edits in regional image editing.

Method: Uses Selective Canny Control for precise edits and Dual-Prompt Guidance for coherent scene interactions.

Result: Outperforms prior methods by 2.93-10.49% in text adherence and context fidelity, with higher editing seamlessness.

Conclusion: CannyEdit achieves superior performance in real-world image editing tasks, making edits harder to detect as AI-generated.

Abstract: Recent advances in text-to-image (T2I) models have enabled training-free
regional image editing by leveraging the generative priors of foundation
models. However, existing methods struggle to balance text adherence in edited
regions, context fidelity in unedited areas, and seamless integration of edits.
We introduce CannyEdit, a novel training-free framework that addresses these
challenges through two key innovations: (1) Selective Canny Control, which
masks the structural guidance of Canny ControlNet in user-specified editable
regions while strictly preserving details of the source images in unedited
areas via inversion-phase ControlNet information retention. This enables
precise, text-driven edits without compromising contextual integrity. (2)
Dual-Prompt Guidance, which combines local prompts for object-specific edits
with a global target prompt to maintain coherent scene interactions. On
real-world image editing tasks (addition, replacement, removal), CannyEdit
outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent
improvement in the balance of text adherence and context fidelity. In terms of
editing seamlessness, user studies reveal only 49.2 percent of general users
and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited
when paired with real images without edits, versus 76.08 to 89.09 percent for
competitor methods.

</details>


### [150] [SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work](https://arxiv.org/abs/2508.06951)
*Harry Walsh,Ed Fish,Ozge Mercanoglu Sincan,Mohamed Ilyes Lakhal,Richard Bowden,Neil Fox,Bencie Woll,Kepeng Wu,Zecheng Li,Weichao Zhao,Haodong Wang,Wengang Zhou,Houqiang Li,Shengeng Tang,Jiayi He,Xu Wang,Ruobei Zhang,Yaxiong Wang,Lechao Cheng,Meryem Tasyurek,Tugce Kiziltepe,Hacer Yalim Keles*

Main category: cs.CV

TL;DR: The paper introduces the first Sign Language Production Challenge to standardize evaluation metrics for SLP, using the RWTH-PHOENIX-Weather-2014T dataset. The winning method employed a retrieval-based framework and pre-trained language model, achieving high scores.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized evaluation metrics for SLP systems hinders meaningful comparisons across approaches.

Method: The challenge evaluated Text-to-Pose (T2P) translation architectures using a German Sign Language dataset and a custom hidden test set.

Result: Top-performing team achieved BLEU-1 scores of 31.40 and DTW-MJE of 0.0574.

Conclusion: The challenge and released evaluation network aim to establish a consistent baseline for future SLP research.

Abstract: Sign Language Production (SLP) is the task of generating sign language video
from spoken language inputs. The field has seen a range of innovations over the
last few years, with the introduction of deep learning-based approaches
providing significant improvements in the realism and naturalness of generated
outputs. However, the lack of standardized evaluation metrics for SLP
approaches hampers meaningful comparisons across different systems. To address
this, we introduce the first Sign Language Production Challenge, held as part
of the third SLRTP Workshop at CVPR 2025. The competition's aims are to
evaluate architectures that translate from spoken language sentences to a
sequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a
range of metrics. For our evaluation data, we use the
RWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche
Gebardensprache (DGS) weather broadcast dataset. In addition, we curate a
custom hidden test set from a similar domain of discourse. This paper presents
the challenge design and the winning methodologies. The challenge attracted 33
participants who submitted 231 solutions, with the top-performing team
achieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach
utilized a retrieval-based framework and a pre-trained language model. As part
of the workshop, we release a standardized evaluation network, including
high-quality skeleton extraction-based keypoints establishing a consistent
baseline for the SLP field, which will enable future researchers to compare
their work against a broader range of methods.

</details>


### [151] [Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification](https://arxiv.org/abs/2508.06959)
*Qin Xu,Lili Zhu,Xiaoxia Cheng,Bo Jiang*

Main category: cs.CV

TL;DR: SCOPE is a novel method for fine-grained visual classification (FGVC) that adaptively enhances spatial-domain features, overcoming limitations of fixed-scale frequency-domain methods.


<details>
  <summary>Details</summary>
Motivation: Current frequency-domain methods lack adaptability to image content and cannot dynamically adjust feature extraction for discriminative needs.

Method: SCOPE uses two modules: Subtle Detail Extractor (SDE) for enhancing shallow features and Salient Semantic Refiner (SSR) for refining high-level features. These are cascaded to combine local details with global semantics.

Result: Achieves state-of-the-art performance on four FGVC benchmarks.

Conclusion: SCOPE effectively addresses the limitations of fixed-scale frequency methods by dynamically enhancing spatial-domain features, improving FGVC performance.

Abstract: The crux of resolving fine-grained visual classification (FGVC) lies in
capturing discriminative and class-specific cues that correspond to subtle
visual characteristics. Recently, frequency decomposition/transform based
approaches have attracted considerable interests since its appearing
discriminative cue mining ability. However, the frequency-domain methods are
based on fixed basis functions, lacking adaptability to image content and
unable to dynamically adjust feature extraction according to the discriminative
requirements of different images. To address this, we propose a novel method
for FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively
enhances the representational capability of low-level details and high-level
semantics in the spatial domain, breaking through the limitations of fixed
scales in the frequency domain and improving the flexibility of multi-scale
fusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor
(SDE), which dynamically enhances subtle details such as edges and textures
from shallow features, and the Salient Semantic Refiner (SSR), which learns
semantically coherent and structure-aware refinement features from the
high-level features guided by the enhanced shallow features. The SDE and SSR
are cascaded stage-by-stage to progressively combine local details with global
semantics. Extensive experiments demonstrate that our method achieves new
state-of-the-art on four popular fine-grained image classification benchmarks.

</details>


### [152] [Adversarial Video Promotion Against Text-to-Video Retrieval](https://arxiv.org/abs/2508.06964)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Qian Li,Shuai Liu,Chao Shen*

Main category: cs.CV

TL;DR: The paper introduces ViPro, the first adversarial attack for promoting videos in text-to-video retrieval (T2VR), and proposes Modal Refinement (MoRe) to enhance transferability. It demonstrates superior performance over baselines and highlights vulnerabilities in T2VR systems.


<details>
  <summary>Details</summary>
Motivation: Existing T2VR attacks focus on suppressing video ranks, but promoting videos is more impactful for attackers. This gap motivates the study of adversarial attacks to boost video ranks.

Method: The authors propose ViPro, an attack to promote videos, and MoRe to refine cross-modal interactions for better black-box transferability. Experiments involve 3 T2VR models, 3 datasets, and 3 attack scenarios.

Result: ViPro outperforms baselines by over 30%/10%/4% in white/grey/black-box settings. It also evaluates defenses and imperceptibility, showing its effectiveness.

Conclusion: The work uncovers a vulnerability in T2VR, provides bounds for attacks, and suggests countermeasures. Code will be publicly released.

Abstract: Thanks to the development of cross-modal models, text-to-video retrieval
(T2VR) is advancing rapidly, but its robustness remains largely unexamined.
Existing attacks against T2VR are designed to push videos away from queries,
i.e., suppressing the ranks of videos, while the attacks that pull videos
towards selected queries, i.e., promoting the ranks of videos, remain largely
unexplored. These attacks can be more impactful as attackers may gain more
views/clicks for financial benefits and widespread (mis)information. To this
end, we pioneer the first attack against T2VR to promote videos adversarially,
dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement
(MoRe) to capture the finer-grained, intricate interaction between visual and
textual modalities to enhance black-box transferability. Comprehensive
experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing
datasets with over 10k videos, evaluated under 3 scenarios. All experiments are
conducted in a multi-target setting to reflect realistic scenarios where
attackers seek to promote the video regarding multiple queries simultaneously.
We also evaluated our attacks for defences and imperceptibility. Overall, ViPro
surpasses other baselines by over $30/10/4\%$ for white/grey/black-box settings
on average. Our work highlights an overlooked vulnerability, provides a
qualitative analysis on the upper/lower bound of our attacks, and offers
insights into potential counterplays. Code will be publicly available at
https://github.com/michaeltian108/ViPro.

</details>


### [153] [Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View](https://arxiv.org/abs/2508.06968)
*Ulas Gunes,Matias Turkulainen,Juho Kannala,Esa Rahtu*

Main category: cs.CV

TL;DR: Evaluation of fisheye-based 3D Gaussian Splatting methods (Fisheye-GS and 3DGUT) on real images with >180° FoV, comparing performance across varying FoVs and proposing a depth-based initialization strategy.


<details>
  <summary>Details</summary>
Motivation: To assess how fisheye-based 3DGS methods handle extreme distortion in real-world settings and improve initialization under strong distortion.

Method: Evaluated Fisheye-GS and 3DGUT on indoor/outdoor scenes with 200° fisheye cameras, tested varying FoVs (200°, 160°, 120°), and proposed a depth-based initialization using UniK3D predictions.

Result: Fisheye-GS performs better with reduced FoV (160°), while 3DGUT remains stable at 200°. UniK3D enables high-quality reconstruction despite not being trained on fisheye data.

Conclusion: Fisheye-based 3DGS methods are viable for wide-angle 3D reconstruction from sparse, distortion-heavy inputs, with 3DGUT excelling in full FoV settings.

Abstract: We present the first evaluation of fisheye-based 3D Gaussian Splatting
methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180
degree. Our study covers both indoor and outdoor scenes captured with 200
degree fisheye cameras and analyzes how each method handles extreme distortion
in real world settings. We evaluate performance under varying fields of view
(200 degree, 160 degree, and 120 degree) to study the tradeoff between
peripheral distortion and spatial coverage. Fisheye-GS benefits from field of
view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable
across all settings and maintains high perceptual quality at the full 200
degree view. To address the limitations of SfM-based initialization, which
often fails under strong distortion, we also propose a depth-based strategy
using UniK3D predictions from only 2-3 fisheye images per scene. Although
UniK3D is not trained on real fisheye data, it produces dense point clouds that
enable reconstruction quality on par with SfM, even in difficult scenes with
fog, glare, or sky. Our results highlight the practical viability of
fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and
distortion-heavy image inputs.

</details>


### [154] [WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering](https://arxiv.org/abs/2508.06982)
*Yixin Zhu,Zuoliang Zhu,Miloš Hašan,Jian Yang,Jin Xie,Beibei Wang*

Main category: cs.CV

TL;DR: WeatherDiffusion is a diffusion-based framework for forward and inverse rendering in autonomous driving scenes, addressing challenges from complex weather and lighting. It uses intrinsic maps and text guidance for control and outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Complex weather and illumination in autonomous driving scenes challenge rendering tasks. Existing diffusion models lack control and robustness.

Method: Proposes WeatherDiffusion with intrinsic map-aware attention (MAA) for high-quality inverse rendering. Introduces synthetic (WeatherSynthetic) and real-world (WeatherReal) datasets.

Result: Outperforms state-of-the-art methods on benchmarks and enhances robustness in downstream tasks like object detection and segmentation.

Conclusion: WeatherDiffusion effectively addresses rendering challenges in AD scenes, offering control and robustness, with demonstrated value in downstream tasks.

Abstract: Forward and inverse rendering have emerged as key techniques for enabling
understanding and reconstruction in the context of autonomous driving (AD).
However, complex weather and illumination pose great challenges to this task.
The emergence of large diffusion models has shown promise in achieving
reasonable results through learning from 2D priors, but these models are
difficult to control and lack robustness. In this paper, we introduce
WeatherDiffusion, a diffusion-based framework for forward and inverse rendering
on AD scenes with various weather and lighting conditions. Our method enables
authentic estimation of material properties, scene geometry, and lighting, and
further supports controllable weather and illumination editing through the use
of predicted intrinsic maps guided by text descriptions. We observe that
different intrinsic maps should correspond to different regions of the original
image. Based on this observation, we propose Intrinsic map-aware attention
(MAA) to enable high-quality inverse rendering. Additionally, we introduce a
synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie
WeatherReal) for forward and inverse rendering on AD scenes with diverse
weather and lighting. Extensive experiments show that our WeatherDiffusion
outperforms state-of-the-art methods on several benchmarks. Moreover, our
method demonstrates significant value in downstream tasks for AD, enhancing the
robustness of object detection and image segmentation in challenging weather
scenarios.

</details>


### [155] [TADoc: Robust Time-Aware Document Image Dewarping](https://arxiv.org/abs/2508.06988)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Yu Zhou*

Main category: cs.CV

TL;DR: The paper introduces TADoc, a lightweight framework for document image dewarping, modeling it as a dynamic process and proposing a new metric (DLS) for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex document structures and high deformation in real-world scenarios. The task is reformulated as a progressive motion.

Method: Proposes TADoc, a Time-Aware Document Dewarping Network, modeling dewarping as a dynamic process with intermediate states. Introduces DLS for evaluation.

Result: TADoc shows strong robustness and outperforms benchmarks across various document types and distortion levels.

Conclusion: The dynamic modeling and DLS metric improve dewarping effectiveness, making TADoc superior for real-world applications.

Abstract: Flattening curved, wrinkled, and rotated document images captured by portable
photographing devices, termed document image dewarping, has become an
increasingly important task with the rise of digital economy and online
working. Although many methods have been proposed recently, they often struggle
to achieve satisfactory results when confronted with intricate document
structures and higher degrees of deformation in real-world scenarios. Our main
insight is that, unlike other document restoration tasks (e.g., deblurring),
dewarping in real physical scenes is a progressive motion rather than a
one-step transformation. Based on this, we have undertaken two key initiatives.
Firstly, we reformulate this task, modeling it for the first time as a dynamic
process that encompasses a series of intermediate states. Secondly, we design a
lightweight framework called TADoc (Time-Aware Document Dewarping Network) to
address the geometric distortion of document images. In addition, due to the
inadequacy of OCR metrics for document images containing sparse text, the
comprehensiveness of evaluation is insufficient. To address this shortcoming,
we propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the
effectiveness of document dewarping in downstream tasks. Extensive experiments
and in-depth evaluations have been conducted and the results indicate that our
model possesses strong robustness, achieving superiority on several benchmarks
with different document types and degrees of distortion.

</details>


### [156] [OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware](https://arxiv.org/abs/2508.06993)
*Nick Lemke,John Kalkhof,Niklas Babendererde,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: OctreeNCA, a bio-inspired model, segments large medical inputs efficiently by leveraging an octree data structure and CUDA implementation, reducing VRAM usage by 90% compared to UNets.


<details>
  <summary>Details</summary>
Motivation: Medical segmentation tasks require processing large inputs (e.g., MRIs, pathology slices) with global context, but current models like UNets or Vision Transformers suffer from high VRAM consumption and poor scalability.

Method: Proposes OctreeNCA, extending Neural Cellular Automata (NCA) with an octree neighborhood for global knowledge traversal, and implements a CUDA-based inference function for efficiency.

Result: OctreeNCA reduces VRAM usage by 90% and segments high-resolution images (184 Megapixel) or videos (1-minute surgical) in one pass.

Conclusion: OctreeNCA offers a scalable, efficient solution for large medical segmentation tasks, outperforming traditional models in VRAM consumption and speed.

Abstract: Medical applications demand segmentation of large inputs, like prostate MRIs,
pathology slices, or videos of surgery. These inputs should ideally be inferred
at once to provide the model with proper spatial or temporal context. When
segmenting large inputs, the VRAM consumption of the GPU becomes the
bottleneck. Architectures like UNets or Vision Transformers scale very poorly
in VRAM consumption, resulting in patch- or frame-wise approaches that
compromise global consistency and inference speed. The lightweight Neural
Cellular Automaton (NCA) is a bio-inspired model that is by construction
size-invariant. However, due to its local-only communication rules, it lacks
global knowledge. We propose OctreeNCA by generalizing the neighborhood
definition using an octree data structure. Our generalized neighborhood
definition enables the efficient traversal of global knowledge. Since deep
learning frameworks are mainly developed for large multi-layer networks, their
implementation does not fully leverage the advantages of NCAs. We implement an
NCA inference function in CUDA that further reduces VRAM demands and increases
inference speed. Our OctreeNCA segments high-resolution images and videos
quickly while occupying 90% less VRAM than a UNet during evaluation. This
allows us to segment 184 Megapixel pathology slices or 1-minute surgical videos
at once.

</details>


### [157] [S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision](https://arxiv.org/abs/2508.06995)
*Huihui Xu,Jin Ye,Hongqiu Wang,Changkai Ji,Jiashi Lin,Ming Hu,Ziyan Huang,Ying Chen,Chenglong Ma,Tianbin Li,Lihao Liu,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: S2-UniSeg introduces Fast Universal Agglomerative Pooling (UniAP) for efficient pseudo-mask generation and a continuous pretraining framework, outperforming SOTA models on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised segmentation models suffer from time-consuming multi-stage pretraining and sub-optimal optimization due to discontinuous processes.

Method: Proposes UniAP for fast pseudo-mask generation and S2-UniSeg with QuerySD for continuous pretraining, leveraging student-teacher architecture.

Result: Achieves notable improvements: AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on COCOStuff-27, RQ+8.0 on Cityscapes, with further gains on a 2M-image subset.

Conclusion: S2-UniSeg offers a scalable, efficient solution for self-supervised segmentation, outperforming existing methods and scaling well with larger datasets.

Abstract: Recent self-supervised image segmentation models have achieved promising
performance on semantic segmentation and class-agnostic instance segmentation.
However, their pretraining schedule is multi-stage, requiring a time-consuming
pseudo-masks generation process between each training epoch. This
time-consuming offline process not only makes it difficult to scale with
training dataset size, but also leads to sub-optimal solutions due to its
discontinuous optimization routine. To solve these, we first present a novel
pseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer
of UniAP can identify groups of similar nodes in parallel, allowing to generate
both semantic-level and instance-level and multi-granular pseudo-masks within
ens of milliseconds for one image. Based on the fast UniAP, we propose the
Scalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a
student and a momentum teacher for continuous pretraining. A novel
segmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is
proposed to pretrain S2-UniSeg to learn the local-to-global correspondences.
Under the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving
notable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on
COCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image
subset of SA-1B, S2-UniSeg further achieves performance gains on all four
benchmarks. Our code and pretrained models are available at
https://github.com/bio-mlhui/S2-UniSeg

</details>


### [158] [HiMat: DiT-based Ultra-High Resolution SVBRDF Generation](https://arxiv.org/abs/2508.07011)
*Zixiong Wang,Jian Yang,Yiwei Hu,Milos Hasan,Beibei Wang*

Main category: cs.CV

TL;DR: HiMat is a diffusion-based framework for generating 4K SVBRDFs efficiently, using a CrossStitch module to ensure consistency across maps without heavy modifications to the DiT backbone.


<details>
  <summary>Details</summary>
Motivation: The need for detailed SVBRDFs in 3D content creation and the potential of high-resolution text-to-image models like DiT for this task, but challenges in producing aligned SVBRDF maps efficiently and consistently.

Method: HiMat introduces the CrossStitch module, a lightweight convolutional module to capture inter-map dependencies, initialized to preserve the DiT backbone's prior capabilities.

Result: HiMat successfully generates 4K SVBRDFs with structural coherence and high-frequency details, validated by text prompts and generalized to tasks like intrinsic decomposition.

Conclusion: HiMat provides an efficient and effective solution for high-resolution SVBRDF generation, maintaining consistency without compromising the DiT backbone's capabilities.

Abstract: Creating highly detailed SVBRDFs is essential for 3D content creation. The
rise of high-resolution text-to-image generative models, based on diffusion
transformers (DiT), suggests an opportunity to finetune them for this task.
However, retargeting the models to produce multiple aligned SVBRDF maps instead
of just RGB images, while achieving high efficiency and ensuring consistency
across different maps, remains a challenge. In this paper, we introduce HiMat:
a memory- and computation-efficient diffusion-based framework capable of
generating native 4K-resolution SVBRDFs. A key challenge we address is
maintaining consistency across different maps in a lightweight manner, without
relying on training new VAEs or significantly altering the DiT backbone (which
would damage its prior capabilities). To tackle this, we introduce the
CrossStitch module, a lightweight convolutional module that captures inter-map
dependencies through localized operations. Its weights are initialized such
that the DiT backbone operation is unchanged before finetuning starts. HiMat
enables generation with strong structural coherence and high-frequency details.
Results with a large set of text prompts demonstrate the effectiveness of our
approach for 4K SVBRDF generation. Further experiments suggest generalization
to tasks such as intrinsic decomposition.

</details>


### [159] [TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders](https://arxiv.org/abs/2508.07020)
*Tanjim Bin Faruk,Abdul Matin,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: TerraMAE is a novel hyperspectral image (HSI) encoding framework designed to improve spatial-spectral embedding for geospatial tasks, outperforming existing methods in reconstruction and downstream applications.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised Masked Autoencoders struggle with hyperspectral imagery due to its high dimensionality and intricate spatial-spectral correlations.

Method: TerraMAE uses adaptive channel grouping based on spectral similarities and an enhanced reconstruction loss with spatial-spectral metrics.

Result: TerraMAE excels in high-fidelity image reconstruction and performs strongly on crop identification, land cover classification, and soil texture prediction.

Conclusion: TerraMAE effectively addresses the challenges of hyperspectral imagery, offering superior performance in both reconstruction and practical geospatial tasks.

Abstract: Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of
contiguous spectral bands, enabling fine-grained mapping of soils, crops, and
land cover. While self-supervised Masked Autoencoders excel on RGB and low-band
multispectral data, they struggle to exploit the intricate spatial-spectral
correlations in 200+ band hyperspectral images. We introduce TerraMAE, a novel
HSI encoding framework specifically designed to learn highly representative
spatial-spectral embeddings for diverse geospatial analyses. TerraMAE features
an adaptive channel grouping strategy, based on statistical reflectance
properties to capture spectral similarities, and an enhanced reconstruction
loss function that incorporates spatial and spectral quality metrics. We
demonstrate TerraMAE's effectiveness through superior spatial-spectral
information preservation in high-fidelity image reconstruction. Furthermore, we
validate its practical utility and the quality of its learned representations
through strong performance on three key downstream geospatial tasks: crop
identification, land cover classification, and soil texture prediction.

</details>


### [160] [DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents](https://arxiv.org/abs/2508.07021)
*Kun Qian,Wenjie Li,Tianyu Sun,Wenhong Wang,Wenhan Luo*

Main category: cs.CV

TL;DR: DocRefine is a framework for intelligent understanding and refinement of scientific PDFs using a multi-agent system, outperforming baselines in semantic consistency, layout fidelity, and instruction adherence.


<details>
  <summary>Details</summary>
Motivation: The need for advanced tools to handle complex layouts and multimodal content in scientific PDFs, as traditional methods and direct LLM/LVLM applications lack precision.

Method: DocRefine employs a multi-agent system with six specialized agents for tasks like layout analysis, content understanding, refinement, summarization, and verification.

Result: DocRefine achieves 86.7% SCS, 93.9% LFI, and 85.0% IAR on DocEditBench, outperforming state-of-the-art baselines.

Conclusion: DocRefine advances automated scientific document processing by ensuring semantic integrity and visual consistency.

Abstract: The exponential growth of scientific literature in PDF format necessitates
advanced tools for efficient and accurate document understanding,
summarization, and content optimization. Traditional methods fall short in
handling complex layouts and multimodal content, while direct application of
Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks
precision and control for intricate editing tasks. This paper introduces
DocRefine, an innovative framework designed for intelligent understanding,
content refinement, and automated summarization of scientific PDF documents,
driven by natural language instructions. DocRefine leverages the power of
advanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent
system comprising six specialized and collaborative agents: Layout & Structure
Analysis, Multimodal Content Understanding, Instruction Decomposition, Content
Refinement, Summarization & Generation, and Fidelity & Consistency
Verification. This closed-loop feedback architecture ensures high semantic
accuracy and visual fidelity. Evaluated on the comprehensive DocEditBench
dataset, DocRefine consistently outperforms state-of-the-art baselines across
various tasks, achieving overall scores of 86.7% for Semantic Consistency Score
(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction
Adherence Rate (IAR). These results demonstrate DocRefine's superior capability
in handling complex multimodal document editing, preserving semantic integrity,
and maintaining visual consistency, marking a significant advancement in
automated scientific document processing.

</details>


### [161] [MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering](https://arxiv.org/abs/2508.07023)
*Jingwei Peng,Jiehao Chen,Mateo Alejandro Rojas,Meilin Zhang*

Main category: cs.CV

TL;DR: MV-CoRe enhances Complex VQA by fusing global and fine-grained visual-linguistic features, outperforming existing models with 77.5% accuracy on GQA.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs struggle with Complex VQA due to reliance on high-level global features, lacking fine-grained reasoning.

Method: MV-CoRe integrates global embeddings from VLMs/LLMs with fine-grained features (object detection, scene graphs) using a Multimodal Fusion Transformer.

Result: Achieves 77.5% accuracy on GQA, outperforming LVLM baselines, with ablation studies confirming feature contributions.

Conclusion: MV-CoRe excels in Complex VQA, validated by human evaluations for factual correctness and reasoning depth.

Abstract: Complex Visual Question Answering (Complex VQA) tasks, which demand
sophisticated multi-modal reasoning and external knowledge integration, present
significant challenges for existing large vision-language models (LVLMs) often
limited by their reliance on high-level global features. To address this, we
propose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model
designed to enhance Complex VQA performance through the deep fusion of diverse
visual and linguistic information. MV-CoRe meticulously integrates global
embeddings from pre-trained Vision Large Models (VLMs) and Language Large
Models (LLMs) with fine-grained semantic-aware visual features, including
object detection characteristics and scene graph representations. An innovative
Multimodal Fusion Transformer then processes and deeply integrates these
diverse feature sets, enabling rich cross-modal attention and facilitating
complex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,
including GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental
results demonstrate that MV-CoRe consistently outperforms established LVLM
baselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies
confirm the critical contribution of both object and scene graph features, and
human evaluations further validate MV-CoRe's superior factual correctness and
reasoning depth, underscoring its robust capabilities for deep visual and
conceptual understanding.

</details>


### [162] [Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation](https://arxiv.org/abs/2508.07028)
*Juntong Fan,Shuyi Fan,Debesh Jha,Changsheng Fang,Tieyong Zeng,Hengyong Yu,Dayang Wang*

Main category: cs.CV

TL;DR: FOCUS-Med is a novel method for polyp segmentation in endoscopic images, combining graph-based and attention mechanisms to improve accuracy and boundary delineation.


<details>
  <summary>Details</summary>
Motivation: The challenge of low contrast, specular highlights, and indistinct boundaries in endoscopic images makes polyp segmentation difficult, necessitating advanced solutions for early cancer detection.

Method: FOCUS-Med integrates a Dual-GCN module for spatial and structural dependencies, location-fused self-attention for global context, and a weighted fusion strategy for multi-scale aggregation. It also uses an LLM for qualitative evaluation.

Result: FOCUS-Med achieves state-of-the-art performance on public benchmarks across five key metrics.

Conclusion: The method demonstrates strong clinical potential for AI-assisted colonoscopy by effectively addressing segmentation challenges.

Abstract: Accurate endoscopic image segmentation on the polyps is critical for early
colorectal cancer detection. However, this task remains challenging due to low
contrast with surrounding mucosa, specular highlights, and indistinct
boundaries. To address these challenges, we propose FOCUS-Med, which stands for
Fusion of spatial and structural graph with attentional context-aware polyp
segmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph
Convolutional Network (Dual-GCN) module to capture contextual spatial and
topological structural dependencies. This graph-based representation enables
the model to better distinguish polyps from background tissues by leveraging
topological cues and spatial connectivity, which are often obscured in raw
image intensities. It enhances the model's ability to preserve boundaries and
delineate complex shapes typical of polyps. In addition, a location-fused
stand-alone self-attention is employed to strengthen global context
integration. To bridge the semantic gap between encoder-decoder layers, we
incorporate a trainable weighted fast normalized fusion strategy for efficient
multi-scale aggregation. Notably, we are the first to introduce the use of a
Large Language Model (LLM) to provide detailed qualitative evaluations of
segmentation quality. Extensive experiments on public benchmarks demonstrate
that FOCUS-Med achieves state-of-the-art performance across five key metrics,
underscoring its effectiveness and clinical potential for AI-assisted
colonoscopy.

</details>


### [163] [TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree](https://arxiv.org/abs/2508.07083)
*Yueyu Hu,Ran Gong,Tingyu Fan,Yao Wang*

Main category: cs.CV

TL;DR: The paper introduces Textured Surfel Octree (TeSO), a novel 3D representation for high-quality rendering and efficient compression, outperforming existing methods like point clouds and 3D Gaussians.


<details>
  <summary>Details</summary>
Motivation: Existing 3D representations (point clouds, meshes, 3D Gaussians) have limitations in rendering quality, surface definition, and compressibility, necessitating a more versatile solution.

Method: TeSO organizes cube-bounded surfels on an octree, each with a texture patch, reducing primitives while retaining detail. A compression scheme leverages the octree structure for efficient encoding.

Result: TeSO achieves higher rendering quality at lower bit-rates compared to point cloud and 3D Gaussian baselines.

Conclusion: TeSO is a promising 3D representation for streaming applications, balancing quality and compression efficiency.

Abstract: 3D visual content streaming is a key technology for emerging 3D telepresence
and AR/VR applications. One fundamental element underlying the technology is a
versatile 3D representation that is capable of producing high-quality renders
and can be efficiently compressed at the same time. Existing 3D representations
like point clouds, meshes and 3D Gaussians each have limitations in terms of
rendering quality, surface definition, and compressibility. In this paper, we
present the Textured Surfel Octree (TeSO), a novel 3D representation that is
built from point clouds but addresses the aforementioned limitations. It
represents a 3D scene as cube-bounded surfels organized on an octree, where
each surfel is further associated with a texture patch. By approximating a
smooth surface with a large surfel at a coarser level of the octree, it reduces
the number of primitives required to represent the 3D scene, and yet retains
the high-frequency texture details through the texture map attached to each
surfel. We further propose a compression scheme to encode the geometry and
texture efficiently, leveraging the octree structure. The proposed textured
surfel octree combined with the compression scheme achieves higher rendering
quality at lower bit-rates compared to multiple point cloud and 3D
Gaussian-based baselines.

</details>


### [164] [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/abs/2508.07089)
*Sandro Papais,Letian Wang,Brian Cheong,Steven L. Waslander*

Main category: cs.CV

TL;DR: ForeSight is a joint detection and forecasting framework for autonomous vehicles, combining tasks with a multi-task streaming approach to improve performance.


<details>
  <summary>Details</summary>
Motivation: Traditional methods treat detection and forecasting separately, missing temporal cues. ForeSight aims to integrate these tasks for better accuracy.

Method: Uses a multi-task streaming and bidirectional learning approach with shared query memory. Includes forecast-aware detection and streaming forecast transformers for spatial and temporal reasoning.

Result: Achieves state-of-the-art performance on nuScenes: 54.9% EPA, 9.3% improvement, and best mAP and minADE.

Conclusion: ForeSight outperforms existing methods by integrating detection and forecasting, eliminating tracking needs, and improving scalability.

Abstract: We introduce ForeSight, a novel joint detection and forecasting framework for
vision-based 3D perception in autonomous vehicles. Traditional approaches treat
detection and forecasting as separate sequential tasks, limiting their ability
to leverage temporal cues. ForeSight addresses this limitation with a
multi-task streaming and bidirectional learning approach, allowing detection
and forecasting to share query memory and propagate information seamlessly. The
forecast-aware detection transformer enhances spatial reasoning by integrating
trajectory predictions from a multiple hypothesis forecast memory queue, while
the streaming forecast transformer improves temporal consistency using past
forecasts and refined detections. Unlike tracking-based methods, ForeSight
eliminates the need for explicit object association, reducing error propagation
with a tracking-free model that efficiently scales across multi-frame
sequences. Experiments on the nuScenes dataset show that ForeSight achieves
state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous
methods by 9.3%, while also attaining the best mAP and minADE among multi-view
detection and forecasting models.

</details>


### [165] [Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration](https://arxiv.org/abs/2508.07092)
*Yue Hu,Juntong Peng,Yunqiao Yang,Siheng Chen*

Main category: cs.CV

TL;DR: Hybrid collaboration (HyComm) integrates perceptual outputs and raw observations for efficient 3D detection, optimizing performance-bandwidth trade-off.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between detection performance and communication bandwidth in collaborative 3D detection.

Method: Proposes HyComm, a system combining compact perceptual outputs and rich raw observations, prioritizing critical data and using standardized formats.

Result: HyComm outperforms prior methods, achieving superior performance-bandwidth trade-off (e.g., 2,006× lower volume) and higher AP50 on DAIR-V2X.

Conclusion: HyComm is adaptable, communication-efficient, and model-agnostic, making it effective for diverse collaborative detection scenarios.

Abstract: Collaborative 3D detection can substantially boost detection performance by
allowing agents to exchange complementary information. It inherently results in
a fundamental trade-off between detection performance and communication
bandwidth. To tackle this bottleneck issue, we propose a novel hybrid
collaboration that adaptively integrates two types of communication messages:
perceptual outputs, which are compact, and raw observations, which offer richer
information. This approach focuses on two key aspects: i) integrating
complementary information from two message types and ii) prioritizing the most
critical data within each type. By adaptively selecting the most critical set
of messages, it ensures optimal perceptual information and adaptability,
effectively meeting the demands of diverse communication scenarios.Building on
this hybrid collaboration, we present \texttt{HyComm}, a
communication-efficient LiDAR-based collaborative 3D detection system.
\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable
compression rates for messages, addressing various communication requirements,
and ii) it uses standardized data formats for messages. This ensures they are
independent of specific detection models, fostering adaptability across
different agent configurations. To evaluate HyComm, we conduct experiments on
both real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm
consistently outperforms previous methods and achieves a superior
performance-bandwidth trade-off regardless of whether agents use the same or
varied detection models. It achieves a lower communication volume of more than
2,006$\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.
The related code will be released.

</details>


### [166] [AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation](https://arxiv.org/abs/2508.07112)
*Nikolai Warner,Wenjin Zhang,Irfan Essa,Apaar Sadhwani*

Main category: cs.CV

TL;DR: AugLift improves 3D Human Pose Estimation by augmenting 2D keypoints with confidence scores and depth estimates, enhancing generalization without extra data.


<details>
  <summary>Details</summary>
Motivation: Existing lifting-based methods for 3D HPE generalize poorly to new datasets and real-world settings.

Method: AugLift enriches 2D keypoints with confidence scores and depth estimates from pre-trained models, integrating modularly into existing architectures.

Result: AugLift boosts cross-dataset performance by 10.1% and in-distribution performance by 4.0%.

Conclusion: AugLift offers a practical, modular solution to improve generalization in lifting-based pose estimation models.

Abstract: Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D
poses from detected 2D keypoints, often generalize poorly to new datasets and
real-world settings. To address this, we propose \emph{AugLift}, a simple yet
effective reformulation of the standard lifting pipeline that significantly
improves generalization performance without requiring additional data
collection or sensors. AugLift sparsely enriches the standard input -- the 2D
keypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection
confidence score $c$ and a corresponding depth estimate $d$. These additional
signals are computed from the image using off-the-shelf, pre-trained models
(e.g., for monocular depth estimation), thereby inheriting their strong
generalization capabilities. Importantly, AugLift serves as a modular add-on
and can be readily integrated into existing lifting architectures.
  Our extensive experiments across four datasets demonstrate that AugLift
boosts cross-dataset performance on unseen datasets by an average of $10.1\%$,
while also improving in-distribution performance by $4.0\%$. These gains are
consistent across various lifting architectures, highlighting the robustness of
our method. Our analysis suggests that these sparse, keypoint-aligned cues
provide robust frame-level context, offering a practical way to significantly
improve the generalization of any lifting-based pose estimation model. Code
will be made publicly available.

</details>


### [167] [Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays](https://arxiv.org/abs/2508.07128)
*Gregory Schuit,Denis Parra,Cecilia Besa*

Main category: cs.CV

TL;DR: The paper evaluates GANs and DMs for generating synthetic chest X-rays with abnormalities, finding DMs more realistic overall but GANs better for specific conditions. Radiologists' insights highlight gaps in current models.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity in medical imaging, especially for rare anomalies, and assess the fidelity and clinical utility of synthetic images.

Method: Evaluated GANs and DMs using real images from MIMIC-CXR and synthetic images. Conducted a reader study with three radiologists to distinguish real from synthetic images and assess abnormality consistency.

Result: DMs produced more realistic images overall, but GANs were more accurate for specific conditions like absence of ECS. Radiologists identified visual cues for detecting synthetic images.

Conclusion: GANs and DMs have complementary strengths; further refinement is needed to ensure reliable augmentation of training datasets for AI diagnostics.

Abstract: Generative image models have achieved remarkable progress in both natural and
medical imaging. In the medical context, these techniques offer a potential
solution to data scarcity-especially for low-prevalence anomalies that impair
the performance of AI-driven diagnostic and segmentation tools. However,
questions remain regarding the fidelity and clinical utility of synthetic
images, since poor generation quality can undermine model generalizability and
trust. In this study, we evaluate the effectiveness of state-of-the-art
generative models-Generative Adversarial Networks (GANs) and Diffusion Models
(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:
Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged
Cardiac Silhouette (ECS). Using a benchmark composed of real images from the
MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a
reader study with three radiologists of varied experience. Participants were
asked to distinguish real from synthetic images and assess the consistency
between visual features and the target abnormality. Our results show that while
DMs generate more visually realistic images overall, GANs can report better
accuracy for specific conditions, such as absence of ECS. We further identify
visual cues radiologists use to detect synthetic images, offering insights into
the perceptual gaps in current models. These findings underscore the
complementary strengths of GANs and DMs and point to the need for further
refinement to ensure generative models can reliably augment training datasets
for AI diagnostic systems.

</details>


### [168] [CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance](https://arxiv.org/abs/2508.07140)
*Yingtie Lei,Fanghai Yi,Yihang Dong,Weihuang Liu,Xiaofeng Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: CMAMRNet, a novel network for mural restoration, uses mask-aware up/down-sampling and co-feature aggregation to improve restoration quality by focusing on damaged regions and preserving artistic details.


<details>
  <summary>Details</summary>
Motivation: Murals deteriorate due to environmental and human factors, and existing methods fail to maintain consistent mask guidance, compromising restoration quality.

Method: CMAMRNet employs Mask-Aware Up/Down-Sampler (MAUDS) for consistent mask sensitivity and Co-Feature Aggregator (CFA) for multi-scale feature extraction.

Result: CMAMRNet outperforms state-of-the-art methods, preserving structural integrity and artistic details.

Conclusion: The proposed framework effectively addresses mural restoration challenges, offering superior performance and preserving authenticity.

Abstract: Murals, as invaluable cultural artifacts, face continuous deterioration from
environmental factors and human activities. Digital restoration of murals faces
unique challenges due to their complex degradation patterns and the critical
need to preserve artistic authenticity. Existing learning-based methods
struggle with maintaining consistent mask guidance throughout their networks,
leading to insufficient focus on damaged regions and compromised restoration
quality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network
that addresses these limitations through comprehensive mask guidance and
multi-scale feature extraction. Our framework introduces two key components:
(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask
sensitivity across resolution scales through dedicated channel-wise feature
selection and mask-guided feature fusion; and (2) the Co-Feature Aggregator
(CFA), operating at both the highest and lowest resolutions to extract
complementary features for capturing fine textures and global structures in
degraded regions. Experimental results on benchmark datasets demonstrate that
CMAMRNet outperforms state-of-the-art methods, effectively preserving both
structural integrity and artistic details in restored murals. The code is
available
at~\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}.

</details>


### [169] [Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models](https://arxiv.org/abs/2508.07144)
*Xuanhan Wang,Huimin Deng,Ke Liu,Jun Wang,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: DPAL is a distillation-based pretraining framework for lightweight human-centric vision models (HVMs) that achieves strong generalization by aligning visual patterns at multiple levels.


<details>
  <summary>Details</summary>
Motivation: Large HVMs are impractical due to their size and data dependency. DPAL aims to train lightweight HVMs with comparable generalization.

Method: DPAL uses a dynamic pattern decoder (D-PaDe) and three alignment objectives (global, local, instance) to distill knowledge from large HVMs.

Result: DPAL-ViT/Ti (5M parameters) matches the performance of larger HVMs (84M, 307M) and outperforms other distillation methods.

Conclusion: DPAL enables efficient, generalizable lightweight HVMs, making them practical for real-world applications.

Abstract: Human-centric vision models (HVMs) have achieved remarkable generalization
due to large-scale pretraining on massive person images. However, their
dependence on large neural architectures and the restricted accessibility of
pretraining data significantly limits their practicality in real-world
applications. To address this limitation, we propose Dynamic Pattern Alignment
Learning (DPAL), a novel distillation-based pretraining framework that
efficiently trains lightweight HVMs to acquire strong generalization from large
HVMs. In particular, human-centric visual perception are highly dependent on
three typical visual patterns, including global identity pattern, local shape
pattern and multi-person interaction pattern. To achieve generalizable
lightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting
as a dynamic Mixture of Expert (MoE) model. It incorporates three specialized
experts dedicated to adaptively extract typical visual patterns, conditioned on
both input image and pattern queries. And then, we present three levels of
alignment objectives, which aims to minimize generalization gap between
lightweight HVMs and large HVMs at global image level, local pixel level, and
instance relation level. With these two deliberate designs, the DPAL
effectively guides lightweight model to learn all typical human visual patterns
from large HVMs, which can generalize to various human-centric vision tasks.
Extensive experiments conducted on 15 challenging datasets demonstrate the
effectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher,
DPAL-ViT/Ti (5M parameters) achieves surprising generalizability similar to
existing large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms
previous distillation-based pretraining methods including Proteus-ViT/Ti (5M)
and TinyMiM-ViT/Ti (5M) by a large margin.

</details>


### [170] [Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.07146)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: A diffusion-based framework for pedestrian trajectory prediction incorporates short-term and long-term motion intentions, improving accuracy by modeling intent explicitly.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods lack explicit semantic modeling of pedestrian intent, leading to misinterpreted behaviors and reduced prediction accuracy.

Method: The framework models short-term intent with a residual polar representation and long-term intent via a token-based endpoint predictor. It also enhances the diffusion process with adaptive guidance and a residual noise predictor.

Result: The method achieves competitive results on ETH, UCY, and SDD benchmarks.

Conclusion: The proposed framework effectively addresses the limitations of diffusion-based methods by incorporating explicit intent modeling, leading to improved trajectory prediction.

Abstract: Predicting pedestrian motion trajectories is critical for the path planning
and motion control of autonomous vehicles. Recent diffusion-based models have
shown promising results in capturing the inherent stochasticity of pedestrian
behavior for trajectory prediction. However, the absence of explicit semantic
modelling of pedestrian intent in many diffusion-based methods may result in
misinterpreted behaviors and reduced prediction accuracy. To address the above
challenges, we propose a diffusion-based pedestrian trajectory prediction
framework that incorporates both short-term and long-term motion intentions.
Short-term intent is modelled using a residual polar representation, which
decouples direction and magnitude to capture fine-grained local motion
patterns. Long-term intent is estimated through a learnable, token-based
endpoint predictor that generates multiple candidate goals with associated
probabilities, enabling multimodal and context-aware intention modelling.
Furthermore, we enhance the diffusion process by incorporating adaptive
guidance and a residual noise predictor that dynamically refines denoising
accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and
SDD benchmarks, demonstrating competitive results against state-of-the-art
methods.

</details>


### [171] [SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.07149)
*Ruolin Yang,Da Li,Honggang Zhang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: Proposes SketchAnimator, a model for animating sketches using reference videos, divided into Appearance Learning, Motion Learning, and Video Prior Distillation stages.


<details>
  <summary>Details</summary>
Motivation: Animating sketches is time-consuming and requires professional skills, making it challenging for amateurs.

Method: Uses LoRA for appearance and motion learning, and Score Distillation Sampling (SDS) to update Bezier curves for motion.

Result: Generates sketch videos retaining original appearance while mirroring reference video dynamics.

Conclusion: Outperforms alternatives in one-shot motion customization, enabling creative sketch animation.

Abstract: Sketching is a uniquely human tool for expressing ideas and creativity. The
animation of sketches infuses life into these static drawings, opening a new
dimension for designers. Animating sketches is a time-consuming process that
demands professional skills and extensive experience, often proving daunting
for amateurs. In this paper, we propose a novel sketch animation model
SketchAnimator, which enables adding creative motion to a given sketch, like "a
jumping car''. Namely, given an input sketch and a reference video, we divide
the sketch animation into three stages: Appearance Learning, Motion Learning
and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate
sketch appearance information and motion dynamics from the reference video into
the pre-trained T2V model. In the third stage, we utilize Score Distillation
Sampling (SDS) to update the parameters of the Bezier curves in each sketch
frame according to the acquired motion information. Consequently, our model
produces a sketch video that not only retains the original appearance of the
sketch but also mirrors the dynamic movements of the reference video. We
compare our method with alternative approaches and demonstrate that it
generates the desired sketch video under the challenge of one-shot motion
customization.

</details>


### [172] [CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion](https://arxiv.org/abs/2508.07162)
*Xiaotong Lin,Tianming Liang,Jian-Fang Hu,Kun-Yu Lin,Yulei Kang,Chunwei Tian,Jianhuang Lai,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: CoopDiff decouples human and object motion modeling for 3D HOI anticipation, using contact points as anchors and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing works ignore the distinct motion patterns of humans and objects, treating them uniformly.

Method: CoopDiff uses two branches for human and object motion, linked by contact points with consistency constraints and a human-driven interaction module.

Result: Outperforms state-of-the-art methods on BEHAVE and Human-object Interaction datasets.

Conclusion: Decoupling motion modeling with contact consistency improves 3D HOI anticipation.

Abstract: 3D human-object interaction (HOI) anticipation aims to predict the future
motion of humans and their manipulated objects, conditioned on the historical
context. Generally, the articulated humans and rigid objects exhibit different
motion patterns, due to their distinct intrinsic physical properties. However,
this distinction is ignored by most of the existing works, which intend to
capture the dynamics of both humans and objects within a single prediction
model. In this work, we propose a novel contact-consistent decoupled diffusion
framework CoopDiff, which employs two distinct branches to decouple human and
object motion modeling, with the human-object contact points as shared anchors
to bridge the motion generation across branches. The human dynamics branch is
aimed to predict highly structured human motion, while the object dynamics
branch focuses on the object motion with rigid translations and rotations.
These two branches are bridged by a series of shared contact points with
consistency constraint for coherent human-object motion prediction. To further
enhance human-object consistency and prediction reliability, we propose a
human-driven interaction module to guide object motion modeling. Extensive
experiments on the BEHAVE and Human-object Interaction datasets demonstrate
that our CoopDiff outperforms state-of-the-art methods.

</details>


### [173] [Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection](https://arxiv.org/abs/2508.07170)
*Yunpeng Shi,Lei Chen,Xiaolu Shen,Yanju Guo*

Main category: cs.CV

TL;DR: The paper introduces LMFNet, a lightweight network using LMF layers for multi-scale feature extraction in salient object detection, achieving state-of-the-art results with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between efficiency and performance in lightweight networks for multi-scale feature extraction in computer vision tasks like salient object detection.

Method: Proposes the LMF layer, utilizing depthwise separable dilated convolutions in a fully connected structure, integrated into LMFNet.

Result: LMFNet achieves competitive performance with only 0.81M parameters, outperforming traditional and lightweight models on five benchmark datasets.

Conclusion: The work successfully tackles multi-scale learning in lightweight networks and shows potential for broader image processing applications.

Abstract: In the domain of computer vision, multi-scale feature extraction is vital for
tasks such as salient object detection. However, achieving this capability in
lightweight networks remains challenging due to the trade-off between
efficiency and performance. This paper proposes a novel lightweight multi-scale
feature extraction layer, termed the LMF layer, which employs depthwise
separable dilated convolutions in a fully connected structure. By integrating
multiple LMF layers, we develop LMFNet, a lightweight network tailored for
salient object detection. Our approach significantly reduces the number of
parameters while maintaining competitive performance. Here, we show that LMFNet
achieves state-of-the-art or comparable results on five benchmark datasets with
only 0.81M parameters, outperforming several traditional and lightweight models
in terms of both efficiency and accuracy. Our work not only addresses the
challenge of multi-scale learning in lightweight networks but also demonstrates
the potential for broader applications in image processing tasks. The related
code files are available at https://github.com/Shi-Yun-peng/LMFNet

</details>


### [174] [EventRR: Event Referential Reasoning for Referring Video Object Segmentation](https://arxiv.org/abs/2508.07171)
*Huihui Xu,Jiashi Lin,Haoyu Chen,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: EventRR framework improves RVOS by decoupling it into object summarization and referent reasoning, leveraging semantic event structure for better performance.


<details>
  <summary>Details</summary>
Motivation: Current RVOS methods ignore semantic structure in referring expressions, which is crucial for reasoning, especially in videos where event attributes and temporal relations add complexity.

Method: EventRR decouples RVOS into object summarization (using bottleneck tokens and video-level aggregation) and referent reasoning (via Referential Event Graph and Temporal Concept-Role Reasoning).

Result: EventRR outperforms state-of-the-art RVOS methods on four benchmark datasets, demonstrating superior quantitative and qualitative performance.

Conclusion: The EventRR framework effectively addresses the complexity of video-referring expressions by leveraging semantic structure, achieving significant improvements in RVOS tasks.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment out the object in
a video referred by an expression. Current RVOS methods view referring
expressions as unstructured sequences, neglecting their crucial semantic
structure essential for referent reasoning. Besides, in contrast to
image-referring expressions whose semantics focus only on object attributes and
object-object relations, video-referring expressions also encompass event
attributes and event-event temporal relations. This complexity challenges
traditional structured reasoning image approaches. In this paper, we propose
the Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS
into object summarization part and referent reasoning part. The summarization
phase begins by summarizing each frame into a set of bottleneck tokens, which
are then efficiently aggregated in the video-level summarization step to
exchange the global cross-modal temporal context. For reasoning part, EventRR
extracts semantic eventful structure of a video-referring expression into
highly expressive Referential Event Graph (REG), which is a single-rooted
directed acyclic graph. Guided by topological traversal of REG, we propose
Temporal Concept-Role Reasoning (TCRR) to accumulate the referring score of
each temporal query from REG leaf nodes to root node. Each reasoning step can
be interpreted as a question-answer pair derived from the concept-role
relations in REG. Extensive experiments across four widely recognized benchmark
datasets, show that EventRR quantitatively and qualitatively outperforms
state-of-the-art RVOS methods. Code is available at
https://github.com/bio-mlhui/EventRR

</details>


### [175] [Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset](https://arxiv.org/abs/2508.07211)
*Junyi He,Liuling Chen,Hongyang Zhou,Zhang xiaoxing,Xiaobin Zhu,Shengxiang Yu,Jingyan Qin,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: A Depth-Guided Network (DGN) is proposed for image restoration, leveraging depth information to improve similarity matching and attention, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing image restoration methods often ignore depth information, leading to suboptimal results in shallow and deep depth-of-field scenarios.

Method: DGN uses two interactive branches: depth estimation for structural guidance and image restoration with progressive window-based self-attention and sparse non-local attention.

Result: The method achieves state-of-the-art performance on benchmarks and generalizes well to unseen plant images.

Conclusion: DGN effectively integrates depth information for superior image restoration, validated by a new high-resolution dataset.

Abstract: Image restoration has seen substantial progress in recent years. However,
existing methods often neglect depth information, which hurts similarity
matching, results in attention distractions in shallow depth-of-field (DoF)
scenarios, and excessive enhancement of background content in deep DoF
settings. To overcome these limitations, we propose a novel Depth-Guided
Network (DGN) for image restoration, together with a novel large-scale
high-resolution dataset. Specifically, the network consists of two interactive
branches: a depth estimation branch that provides structural guidance, and an
image restoration branch that performs the core restoration task. In addition,
the image restoration branch exploits intra-object similarity through
progressive window-based self-attention and captures inter-object similarity
via sparse non-local attention. Through joint training, depth features
contribute to improved restoration quality, while the enhanced visual features
from the restoration branch in turn help refine depth estimation. Notably, we
also introduce a new dataset for training and evaluation, consisting of 9,205
high-resolution images from 403 plant species, with diverse depth and texture
variations. Extensive experiments show that our method achieves
state-of-the-art performance on several standard benchmarks and generalizes
well to unseen plant images, demonstrating its effectiveness and robustness.

</details>


### [176] [Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling](https://arxiv.org/abs/2508.07214)
*Hongyang Zhou,Xiaobin Zhu,Liuling Chen,Junyi He,Jingyan Qin,Xu-Cheng Yin,Zhang xiaoxing*

Main category: cs.CV

TL;DR: Proposes an unsupervised real-world SR method using rectified flow and Fourier priors to model realistic degradation, improving SR performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of generalizing from synthetic LR-HR pairs to real-world data due to complex, unknown degradation distributions.

Method: Introduces RFDM for continuous, invertible degradation modeling and FGDM for Fourier-based structural guidance, synthesizing realistic LR-HR pairs.

Result: Significantly enhances SR performance on real-world datasets compared to existing methods.

Conclusion: The method effectively captures real-world degradation, improving the realism and performance of unsupervised SR.

Abstract: Unsupervised real-world super-resolution (SR) faces critical challenges due
to the complex, unknown degradation distributions in practical scenarios.
Existing methods struggle to generalize from synthetic low-resolution (LR) and
high-resolution (HR) image pairs to real-world data due to a significant domain
gap. In this paper, we propose an unsupervised real-world SR method based on
rectified flow to effectively capture and model real-world degradation,
synthesizing LR-HR training pairs with realistic degradation. Specifically,
given unpaired LR and HR images, we propose a novel Rectified Flow Degradation
Module (RFDM) that introduces degradation-transformed LR (DT-LR) images as
intermediaries. By modeling the degradation trajectory in a continuous and
invertible manner, RFDM better captures real-world degradation and enhances the
realism of generated LR images. Additionally, we propose a Fourier Prior Guided
Degradation Module (FGDM) that leverages structural information embedded in
Fourier phase components to ensure more precise modeling of real-world
degradation. Finally, the LR images are processed by both FGDM and RFDM,
producing final synthetic LR images with real-world degradation. The synthetic
LR images are paired with the given HR images to train the off-the-shelf SR
networks. Extensive experiments on real-world datasets demonstrate that our
method significantly enhances the performance of existing SR approaches in
real-world scenarios.

</details>


### [177] [Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization](https://arxiv.org/abs/2508.07216)
*Songlin Li,Zhiqing Guo,Yuanman Li,Zeyu Li,Yunfeng Diao,Gaobo Yang,Liejun Wang*

Main category: cs.CV

TL;DR: CMB-Net improves image manipulation localization by integrating LLMs for semantic analysis, addressing visual-semantic gaps with ITCAM and ITIM, and preserving boundaries via RED.


<details>
  <summary>Details</summary>
Motivation: Existing IML models lack semantic logical relationships in visual cues, which real images naturally follow. Manipulation disrupts these, leaving semantic clues.

Method: CMB-Net uses LLMs for semantic analysis, ITCAM to weight text features, ITIM for visual-text alignment, and RED for boundary preservation.

Result: CMB-Net outperforms existing IML models in experiments.

Conclusion: Integrating semantic analysis with visual cues enhances IML accuracy, with CMB-Net setting a new benchmark.

Abstract: The existing image manipulation localization (IML) models mainly relies on
visual cues, but ignores the semantic logical relationships between content
features. In fact, the content semantics conveyed by real images often conform
to human cognitive laws. However, image manipulation technology usually
destroys the internal relationship between content features, thus leaving
semantic clues for IML. In this paper, we propose a cognition-inspired
multimodal boundary-preserving network (CMB-Net). Specifically, CMB-Net
utilizes large language models (LLMs) to analyze manipulated regions within
images and generate prompt-based textual information to compensate for the lack
of semantic relationships in the visual information. Considering that the
erroneous texts induced by hallucination from LLMs will damage the accuracy of
IML, we propose an image-text central ambiguity module (ITCAM). It assigns
weights to the text features by quantifying the ambiguity between text and
image features, thereby ensuring the beneficial impact of textual information.
We also propose an image-text interaction module (ITIM) that aligns visual and
text features using a correlation matrix for fine-grained interaction. Finally,
inspired by invertible neural networks, we propose a restoration edge decoder
(RED) that mutually generates input and output features to preserve boundary
information in manipulated regions without loss. Extensive experiments show
that CMB-Net outperforms most existing IML models.

</details>


### [178] [Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline](https://arxiv.org/abs/2508.07217)
*Yuqi Han,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: A hybrid calibration method combining generic and parametric models is proposed to address pose ambiguity and improve accuracy in offline camera calibration.


<details>
  <summary>Details</summary>
Motivation: Parametric models rely on user experience, while generic methods are complex and lack traditional intrinsic parameters. Pose ambiguity in generic methods affects pose estimation.

Method: A linear solver and nonlinear optimization address pose ambiguity. A global optimization hybrid method integrates generic and parametric models.

Result: The hybrid method improves extrinsic parameter accuracy, mitigates overfitting, and performs well across lens types and noise.

Conclusion: The hybrid method is reliable and accurate for complex camera calibration scenarios.

Abstract: Offline camera calibration techniques typically employ parametric or generic
camera models. Selecting parametric models relies heavily on user experience,
and an inappropriate camera model can significantly affect calibration
accuracy. Meanwhile, generic calibration methods involve complex procedures and
cannot provide traditional intrinsic parameters. This paper reveals a pose
ambiguity in the pose solutions of generic calibration methods that
irreversibly impacts subsequent pose estimation. A linear solver and a
nonlinear optimization are proposed to address this ambiguity issue. Then a
global optimization hybrid calibration method is introduced to integrate
generic and parametric models together, which improves extrinsic parameter
accuracy of generic calibration and mitigates overfitting and numerical
instability in parametric calibration. Simulation and real-world experimental
results demonstrate that the generic-parametric hybrid calibration method
consistently excels across various lens types and noise contamination,
hopefully serving as a reliable and accurate solution for camera calibration in
complex scenarios.

</details>


### [179] [Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource](https://arxiv.org/abs/2508.07233)
*Lei Yang,Junshan Jin,Mingyuan Zhang,Yi He,Bofan Chen,Shilin Wang*

Main category: cs.CV

TL;DR: The paper proposes a landmark-guided visual feature extractor to improve visual speech recognition by reducing user-specific feature influence and enhancing performance with limited data.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for visual speech recognition are affected by visual disturbances and require large datasets and resources. The goal is to mitigate these issues.

Method: A landmark-guided feature extractor uses facial landmarks for training. A spatio-temporal multi-graph convolutional network and multi-level lip dynamic fusion framework are employed.

Result: The approach performs well with limited data and improves accuracy on unseen speakers.

Conclusion: The proposed method effectively addresses data and resource limitations while enhancing recognition accuracy.

Abstract: Visual speech recognition is a technique to identify spoken content in silent
speech videos, which has raised significant attention in recent years.
Advancements in data-driven deep learning methods have significantly improved
both the speed and accuracy of recognition. However, these deep learning
methods can be effected by visual disturbances, such as lightning conditions,
skin texture and other user-specific features. Data-driven approaches could
reduce the performance degradation caused by these visual disturbances using
models pretrained on large-scale datasets. But these methods often require
large amounts of training data and computational resources, making them costly.
To reduce the influence of user-specific features and enhance performance with
limited data, this paper proposed a landmark guided visual feature extractor.
Facial landmarks are used as auxiliary information to aid in training the
visual feature extractor. A spatio-temporal multi-graph convolutional network
is designed to fully exploit the spatial locations and spatio-temporal features
of facial landmarks. Additionally, a multi-level lip dynamic fusion framework
is introduced to combine the spatio-temporal features of the landmarks with the
visual features extracted from the raw video frames. Experimental results show
that this approach performs well with limited data and also improves the
model's accuracy on unseen speakers.

</details>


### [180] [ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation](https://arxiv.org/abs/2508.07237)
*Bo Wang,Mengyuan Xu,Yue Yan,Yuqun Yang,Kechen Shu,Wei Ping,Xu Tang,Wei Jiang,Zheng You*

Main category: cs.CV

TL;DR: ASM-UNet, a Mamba-based model, improves fine-grained segmentation (FGS) by dynamically adapting scanning orders using adaptive scan scores, outperforming existing methods on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing coarse-grained segmentation (CGS) methods fail in clinical FGS due to individual variations in small-scale anatomical structures, and fixed scanning orders in Mamba-based models limit adaptability.

Method: Proposes ASM-UNet, which uses adaptive scan scores combining group-level commonalities and individual-level variations to dynamically guide scanning orders.

Result: ASM-UNet achieves superior performance on ACDC, Synapse, and BTMS datasets for both CGS and FGS tasks.

Conclusion: ASM-UNet effectively addresses FGS challenges by dynamically adapting to individual variations, demonstrating significant improvements over existing methods.

Abstract: Precise lesion resection depends on accurately identifying fine-grained
anatomical structures. While many coarse-grained segmentation (CGS) methods
have been successful in large-scale segmentation (e.g., organs), they fall
short in clinical scenarios requiring fine-grained segmentation (FGS), which
remains challenging due to frequent individual variations in small-scale
anatomical structures. Although recent Mamba-based models have advanced medical
image segmentation, they often rely on fixed manually-defined scanning orders,
which limit their adaptability to individual variations in FGS. To address
this, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It
introduces adaptive scan scores to dynamically guide the scanning order,
generated by combining group-level commonalities and individual-level
variations. Experiments on two public datasets (ACDC and Synapse) and a newly
proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that
ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and
dataset are available at https://github.com/YqunYang/ASM-UNet.

</details>


### [181] [Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers](https://arxiv.org/abs/2508.07246)
*Xin Ma,Yaohui Wang,Genyun Jia,Xinyuan Chen,Tien-Tsin Wong,Cunjian Chen*

Main category: cs.CV

TL;DR: MiraMo is a framework for efficient, consistent, and smooth image animation, addressing challenges in appearance consistency and motion transitions with linear attention, motion residual learning, and DCT-based noise refinement.


<details>
  <summary>Details</summary>
Motivation: Existing image animation methods struggle with appearance consistency, abrupt motion transitions, and computational inefficiency compared to text-to-video models.

Method: MiraMo introduces a text-to-video architecture with linear attention, motion residual learning for temporal consistency, and DCT-based noise refinement for smooth motion.

Result: MiraMo outperforms state-of-the-art methods in generating consistent, smooth animations with faster inference and demonstrates versatility in motion transfer and video editing.

Conclusion: MiraMo effectively addresses key challenges in image animation, offering improved efficiency, consistency, and motion quality.

Abstract: Image animation has seen significant progress, driven by the powerful
generative capabilities of diffusion models. However, maintaining appearance
consistency with static input images and mitigating abrupt motion transitions
in generated animations remain persistent challenges. While text-to-video (T2V)
generation has demonstrated impressive performance with diffusion transformer
models, the image animation field still largely relies on U-Net-based diffusion
models, which lag behind the latest T2V approaches. Moreover, the quadratic
complexity of vanilla self-attention mechanisms in Transformers imposes heavy
computational demands, making image animation particularly resource-intensive.
To address these issues, we propose MiraMo, a framework designed to enhance
efficiency, appearance consistency, and motion smoothness in image animation.
Specifically, MiraMo introduces three key elements: (1) A foundational
text-to-video architecture replacing vanilla self-attention with efficient
linear attention to reduce computational overhead while preserving generation
quality; (2) A novel motion residual learning paradigm that focuses on modeling
motion dynamics rather than directly predicting frames, improving temporal
consistency; and (3) A DCT-based noise refinement strategy during inference to
suppress sudden motion artifacts, complemented by a dynamics control module to
balance motion smoothness and expressiveness. Extensive experiments against
state-of-the-art methods validate the superiority of MiraMo in generating
consistent, smooth, and controllable animations with accelerated inference
speed. Additionally, we demonstrate the versatility of MiraMo through
applications in motion transfer and video editing tasks.

</details>


### [182] [SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking](https://arxiv.org/abs/2508.07250)
*Fengchao Xiong,Zhenxing Wu,Sen Jia,Yuntao Qian*

Main category: cs.CV

TL;DR: The paper proposes a method to improve hyperspectral video tracking by addressing spectral interactions, using Transformers and a spectral loss for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing tracking methods overlook spectral interactions, leading to suboptimal performance in cluttered backgrounds and small object scenarios.

Method: The approach uses Transformers for band-wise spatial relationships and models spectral interactions via the inclusion-exclusion principle. A spectral loss enforces material distribution alignment.

Result: The tracker achieves state-of-the-art performance, validated through extensive experiments.

Conclusion: The method effectively integrates spectral and spatial cues, enhancing tracking robustness. Code and models are available for reproducibility.

Abstract: Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal
structure, offer distinct advantages in challenging tracking scenarios such as
cluttered backgrounds and small objects. However, existing methods primarily
focus on spatial interactions between the template and search regions, often
overlooking spectral interactions, leading to suboptimal performance. To
address this issue, this paper investigates spectral interactions from both the
architectural and training perspectives. At the architectural level, we first
establish band-wise long-range spatial relationships between the template and
search regions using Transformers. We then model spectral interactions using
the inclusion-exclusion principle from set theory, treating them as the union
of spatial interactions across all bands. This enables the effective
integration of both shared and band-specific spatial cues. At the training
level, we introduce a spectral loss to enforce material distribution alignment
between the template and predicted regions, enhancing robustness to shape
deformation and appearance variations. Extensive experiments demonstrate that
our tracker achieves state-of-the-art tracking performance. The source code,
trained models and results will be publicly available via
https://github.com/bearshng/suit to support reproducibility.

</details>


### [183] [Understanding Dynamic Scenes in Ego Centric 4D Point Clouds](https://arxiv.org/abs/2508.07251)
*Junsheng Huang,Shengyu Hao,Bocheng Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: EgoDynamic4D introduces a QA benchmark for dynamic 4D egocentric scenes with unified annotations and tasks, proposing a spatio-temporal reasoning framework that outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack unified 4D annotations and task-driven evaluation for fine-grained spatio-temporal reasoning in dynamic scenes.

Method: The paper introduces EgoDynamic4D with RGB-D video, camera poses, instance masks, and 4D bounding boxes, and proposes an end-to-end spatio-temporal reasoning framework.

Result: The framework outperforms baselines on EgoDynamic4D, validating multimodal temporal modeling for dynamic scene understanding.

Conclusion: EgoDynamic4D and the proposed framework advance egocentric dynamic scene understanding with verifiable reasoning and fine-grained tasks.

Abstract: Understanding dynamic 4D scenes from an egocentric perspective-modeling
changes in 3D spatial structure over time-is crucial for human-machine
interaction, autonomous navigation, and embodied intelligence. While existing
egocentric datasets contain dynamic scenes, they lack unified 4D annotations
and task-driven evaluation protocols for fine-grained spatio-temporal
reasoning, especially on motion of objects and human, together with their
interactions. To address this gap, we introduce EgoDynamic4D, a novel QA
benchmark on highly dynamic scenes, comprising RGB-D video, camera poses,
globally unique instance masks, and 4D bounding boxes. We construct 927K QA
pairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,
step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering
agent motion, human-object interaction, trajectory prediction, relation
understanding, and temporal-causal reasoning, with fine-grained,
multidimensional metrics. To tackle these tasks, we propose an end-to-end
spatio-temporal reasoning framework that unifies dynamic and static scene
information, using instance-aware feature encoding, time and camera encoding,
and spatially adaptive down-sampling to compress large 4D scenes into token
sequences manageable by LLMs. Experiments on EgoDynamic4D show that our method
consistently outperforms baselines, validating the effectiveness of multimodal
temporal modeling for egocentric dynamic scene understanding.

</details>


### [184] [Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM](https://arxiv.org/abs/2508.07260)
*Sihan Yang,Huitong Ji,Shaolin Lu,Jiayi Chen,Binxiao Xu,Ming Lu,Yuanxing Zhang,Wenhui Dong,Wentao Zhang*

Main category: cs.CV

TL;DR: A collaborative framework (SLC) combines small and large VLMs for efficient personalization, leveraging small VLMs for personalized data and large VLMs for accurate responses, with a reflection strategy to avoid errors.


<details>
  <summary>Details</summary>
Motivation: Large VLMs are costly and restricted, while small VLMs lack reasoning. SLC bridges this gap for practical personalization.

Method: SLC uses a small VLM for personalized info and a large VLM for integration, with a test-time reflection strategy to prevent errors.

Result: SLC is training-efficient, supports open/closed-source VLMs, and shows effectiveness in benchmarks.

Conclusion: SLC enables broader real-world personalized applications efficiently.

Abstract: Personalizing Vision-Language Models (VLMs) to transform them into daily
assistants has emerged as a trending research direction. However, leading
companies like OpenAI continue to increase model size and develop complex
designs such as the chain of thought (CoT). While large VLMs are proficient in
complex multi-modal understanding, their high training costs and limited access
via paid APIs restrict direct personalization. Conversely, small VLMs are
easily personalized and freely available, but they lack sufficient reasoning
capabilities. Inspired by this, we propose a novel collaborative framework
named Small-Large Collaboration (SLC) for large VLM personalization, where the
small VLM is responsible for generating personalized information, while the
large model integrates this personalized information to deliver accurate
responses. To effectively incorporate personalized information, we develop a
test-time reflection strategy, preventing the potential hallucination of the
small VLM. Since SLC only needs to train a meta personalized small VLM for the
large VLMs, the overall process is training-efficient. To the best of our
knowledge, this is the first training-efficient framework that supports both
open-source and closed-source large VLMs, enabling broader real-world
personalized applications. We conduct thorough experiments across various
benchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC
framework. The code will be released at https://github.com/Hhankyangg/SLC.

</details>


### [185] [OpenHAIV: A Framework Towards Practical Open-World Learning](https://arxiv.org/abs/2508.07270)
*Xiang Xiang,Qinhao Zhou,Zhuo Xu,Jing Ma,Jiaxin Dai,Yifan Liang,Hanlin Li*

Main category: cs.CV

TL;DR: OpenHAIV integrates OOD detection, new class discovery, and incremental fine-tuning for autonomous knowledge updates in open-world recognition.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detection and incremental learning methods are limited in open-world scenarios, lacking knowledge updates and requiring supervised conditions.

Method: Proposes OpenHAIV, a unified framework combining OOD detection, new class discovery, and incremental continual fine-tuning.

Result: Enables models to autonomously acquire and update knowledge in open-world environments.

Conclusion: OpenHAIV addresses limitations of current approaches, offering a practical solution for open-world recognition.

Abstract: Substantial progress has been made in various techniques for open-world
recognition. Out-of-distribution (OOD) detection methods can effectively
distinguish between known and unknown classes in the data, while incremental
learning enables continuous model knowledge updates. However, in open-world
scenarios, these approaches still face limitations. Relying solely on OOD
detection does not facilitate knowledge updates in the model, and incremental
fine-tuning typically requires supervised conditions, which significantly
deviate from open-world settings. To address these challenges, this paper
proposes OpenHAIV, a novel framework that integrates OOD detection, new class
discovery, and incremental continual fine-tuning into a unified pipeline. This
framework allows models to autonomously acquire and update knowledge in
open-world environments. The proposed framework is available at
https://haiv-lab.github.io/openhaiv .

</details>


### [186] [Representation Understanding via Activation Maximization](https://arxiv.org/abs/2508.07281)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.CV

TL;DR: A unified feature visualization framework for CNNs and ViTs, extending to intermediate layers and exploring adversarial example generation.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability of DNNs by understanding internal feature representations and probing potential vulnerabilities.

Method: Uses Activation Maximization (AM) to synthesize inputs that strongly activate neurons, applied to intermediate and output layers of CNNs and ViTs.

Result: Demonstrates effectiveness in visualizing hierarchical features and generating adversarial examples, applicable to both CNNs and ViTs.

Conclusion: The framework provides deeper insights into DNNs, revealing feature hierarchies and vulnerabilities, with broad applicability.

Abstract: Understanding internal feature representations of deep neural networks (DNNs)
is a fundamental step toward model interpretability. Inspired by neuroscience
methods that probe biological neurons using visual stimuli, recent deep
learning studies have employed Activation Maximization (AM) to synthesize
inputs that elicit strong responses from artificial neurons. In this work, we
propose a unified feature visualization framework applicable to both
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike
prior efforts that predominantly focus on the last output-layer neurons in
CNNs, we extend feature visualization to intermediate layers as well, offering
deeper insights into the hierarchical structure of learned feature
representations. Furthermore, we investigate how activation maximization can be
leveraged to generate adversarial examples, revealing potential vulnerabilities
and decision boundaries of DNNs. Our experiments demonstrate the effectiveness
of our approach in both traditional CNNs and modern ViT, highlighting its
generalizability and interpretive value.

</details>


### [187] [SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations](https://arxiv.org/abs/2508.07298)
*Zhiqiang Shen,Peng Cao,Xiaoli Liu,Jinzhu Yang,Osmar R. Zaiane*

Main category: cs.CV

TL;DR: SynMatch synthesizes images to match pseudo labels in medical image segmentation, improving consistency and performance in label-scarce settings like BSL.


<details>
  <summary>Details</summary>
Motivation: Addressing inconsistencies between pseudo labels and unlabeled images in medical image segmentation due to label scarcity.

Method: SynMatch synthesizes images using texture and shape features from the segmentation model, ensuring consistency without additional training parameters.

Result: Outperforms strong-weak pseudo supervision by 29.71% and 10.05% in polyp segmentation with 5% and 10% scribble annotations, respectively.

Conclusion: SynMatch is effective for medical image segmentation in low-annotation settings, especially BSL.

Abstract: Label scarcity remains a major challenge in deep learning-based medical image
segmentation. Recent studies use strong-weak pseudo supervision to leverage
unlabeled data. However, performance is often hindered by inconsistencies
between pseudo labels and their corresponding unlabeled images. In this work,
we propose \textbf{SynMatch}, a novel framework that sidesteps the need for
improving pseudo labels by synthesizing images to match them instead.
Specifically, SynMatch synthesizes images using texture and shape features
extracted from the same segmentation model that generates the corresponding
pseudo labels for unlabeled images. This design enables the generation of
highly consistent synthesized-image-pseudo-label pairs without requiring any
training parameters for image synthesis. We extensively evaluate SynMatch
across diverse medical image segmentation tasks under semi-supervised learning
(SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL)
settings with increasingly limited annotations. The results demonstrate that
SynMatch achieves superior performance, especially in the most challenging BSL
setting. For example, it outperforms the recent strong-weak pseudo
supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task
with 5\% and 10\% scribble annotations, respectively. The code will be released
at https://github.com/Senyh/SynMatch.

</details>


### [188] [BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2508.07300)
*Ping-Mao Huang,I-Tien Chao,Ping-Chia Huang,Jia-Wei Liao,Yung-Yu Chuang*

Main category: cs.CV

TL;DR: BEVANet introduces LKA and SDLSKA for efficient real-time semantic segmentation, achieving 79.3% mIoU (81.0% with pretraining) at 33 FPS.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing efficient architectures with large receptive fields for semantic understanding and detailed contour refinement in real-time segmentation.

Method: Proposes BEVANet with LKA, SDLSKA, CKS, DLKPPM, and BGAF modules to expand receptive fields, adapt dynamically, and enhance boundary delineation.

Result: Achieves 79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes with pretraining, at 33 FPS.

Conclusion: BEVANet demonstrates state-of-the-art performance in real-time semantic segmentation by efficiently combining large kernel attention and dynamic adaptation.

Abstract: Real-time semantic segmentation presents the dual challenge of designing
efficient architectures that capture large receptive fields for semantic
understanding while also refining detailed contours. Vision transformers model
long-range dependencies effectively but incur high computational cost. To
address these challenges, we introduce the Large Kernel Attention (LKA)
mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)
expands the receptive field to capture contextual information and extracts
visual and structural features using Sparse Decomposed Large Separable Kernel
Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism
dynamically adapts the receptive field to further enhance performance.
Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches
contextual features by synergistically combining dilated convolutions and large
kernel attention. The bilateral architecture facilitates frequent branch
communication, and the Boundary Guided Adaptive Fusion (BGAF) module enhances
boundary delineation by integrating spatial and semantic features under
boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding
79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet
pretraining, demonstrating state-of-the-art performance. The code and model is
available at https://github.com/maomao0819/BEVANet.

</details>


### [189] [DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices](https://arxiv.org/abs/2508.07306)
*Md Zahurul Haquea,Yeahyea Sarker,Muhammed Farhan Sadique Mahi,Syed Jubayer Jaman,Md Robiul Islam*

Main category: cs.CV

TL;DR: DragonFruitQualityNet is a lightweight CNN for real-time dragon fruit quality assessment on mobile devices, achieving 93.98% accuracy.


<details>
  <summary>Details</summary>
Motivation: Rising global demand for dragon fruit necessitates efficient quality inspection to improve productivity and reduce post-harvest losses.

Method: A diverse dataset of 13,789 images was curated, and a lightweight CNN was developed and embedded into a mobile app for real-time use.

Result: The model achieved 93.98% accuracy, outperforming existing methods in fruit quality classification.

Conclusion: The research provides an efficient AI solution for dragon fruit quality control, supporting digital agriculture and sustainable farming.

Abstract: Dragon fruit, renowned for its nutritional benefits and economic value, has
experienced rising global demand due to its affordability and local
availability. As dragon fruit cultivation expands, efficient pre- and
post-harvest quality inspection has become essential for improving agricultural
productivity and minimizing post-harvest losses. This study presents
DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)
optimized for real-time quality assessment of dragon fruits on mobile devices.
We curated a diverse dataset of 13,789 images, integrating self-collected
samples with public datasets (dataset from Mendeley Data), and classified them
into four categories: fresh, immature, mature, and defective fruits to ensure
robust model training. The proposed model achieves an impressive 93.98%
accuracy, outperforming existing methods in fruit quality classification. To
facilitate practical adoption, we embedded the model into an intuitive mobile
application, enabling farmers and agricultural stakeholders to conduct
on-device, real-time quality inspections. This research provides an accurate,
efficient, and scalable AI-driven solution for dragon fruit quality control,
supporting digital agriculture and empowering smallholder farmers with
accessible technology. By bridging the gap between research and real-world
application, our work advances post-harvest management and promotes sustainable
farming practices.

</details>


### [190] [MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark](https://arxiv.org/abs/2508.07307)
*Haiyang Guo,Fei Zhu,Hongbo Zhao,Fanhu Zeng,Wenzhuo Liu,Shijie Ma,Da-Han Wang,Xu-Yao Zhang*

Main category: cs.CV

TL;DR: The paper introduces MCITlib, a code library for Multimodal Continual Learning, addressing challenges in cross-modal interactions and catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To advance research in Multimodal Continual Learning by providing tools for continual instruction tuning of Multimodal Large Language Models.

Method: Implemented 8 algorithms in MCITlib and evaluated them on 2 benchmarks.

Result: MCITlib offers a practical solution for Multimodal Continual Learning, with ongoing updates planned.

Conclusion: MCITlib is a valuable resource for the field, with potential for future enhancements.

Abstract: Continual learning aims to equip AI systems with the ability to continuously
acquire and adapt to new knowledge without forgetting previously learned
information, similar to human learning. While traditional continual learning
methods focusing on unimodal tasks have achieved notable success, the emergence
of Multimodal Large Language Models has brought increasing attention to
Multimodal Continual Learning tasks involving multiple modalities, such as
vision and language. In this setting, models are expected to not only mitigate
catastrophic forgetting but also handle the challenges posed by cross-modal
interactions and coordination. To facilitate research in this direction, we
introduce MCITlib, a comprehensive and constantly evolving code library for
continual instruction tuning of Multimodal Large Language Models. In MCITlib,
we have currently implemented 8 representative algorithms for Multimodal
Continual Instruction Tuning and systematically evaluated them on 2 carefully
selected benchmarks. MCITlib will be continuously updated to reflect advances
in the Multimodal Continual Learning field. The codebase is released at
https://github.com/Ghy0501/MCITlib.

</details>


### [191] [MobileViCLIP: An Efficient Video-Text Model for Mobile Devices](https://arxiv.org/abs/2508.07312)
*Min Yang,Zihan Jia,Zhilin Dai,Sheng Guo,Limin Wang*

Main category: cs.CV

TL;DR: MobileViCLIP introduces an efficient video-text model for mobile devices, achieving significant speed improvements and strong zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: Existing video models are inefficient for mobile deployment, lacking lightweight architectures.

Method: Temporal structural reparameterization is applied to an efficient image-text model, trained on a large video-text dataset.

Result: MobileViCLIP-Small is 55.4x faster than InternVideo2-L14 and matches its zero-shot performance.

Conclusion: MobileViCLIP bridges the gap for efficient video-text models on mobile devices with competitive performance.

Abstract: Efficient lightweight neural networks are with increasing attention due to
their faster reasoning speed and easier deployment on mobile devices. However,
existing video pre-trained models still focus on the common ViT architecture
with high latency, and few works attempt to build efficient architecture on
mobile devices. This paper bridges this gap by introducing temporal structural
reparameterization into an efficient image-text model and training it on a
large-scale high-quality video-text dataset, resulting in an efficient
video-text model that can run on mobile devices with strong zero-shot
classification and retrieval capabilities, termed as MobileViCLIP. In
particular, in terms of inference speed on mobile devices, our
MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster
than InternVideo2-S14. In terms of zero-shot retrieval performance, our
MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains
6.9\% better than InternVideo2-S14 on MSR-VTT. The code is available at
https://github.com/MCG-NJU/MobileViCLIP.

</details>


### [192] [DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding](https://arxiv.org/abs/2508.07313)
*Junyu Xiong,Yonghui Wang,Weichao Zhao,Chenyu Liu,Bing Yin,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: DocR1, an MLLM trained with EviGRPO, improves multi-page document understanding via evidence-aware RL, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of multi-page document understanding in MLLMs, which requires fine-grained visual and multi-hop reasoning.

Method: Introduces EviGRPO, an RL framework with evidence-aware rewards, and a two-stage annotation pipeline with curriculum learning.

Result: DocR1 achieves SOTA on multi-page tasks and maintains strong performance on single-page benchmarks.

Conclusion: EviGRPO and DocR1 advance multi-page document understanding with limited supervision, validated by high-quality datasets.

Abstract: Understanding multi-page documents poses a significant challenge for
multimodal large language models (MLLMs), as it requires fine-grained visual
comprehension and multi-hop reasoning across pages. While prior work has
explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,
its application to multi-page document understanding remains underexplored. In
this paper, we introduce DocR1, an MLLM trained with a novel RL framework,
Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware
reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the
model to first retrieve relevant pages before generating answers. This training
paradigm enables us to build high-quality models with limited supervision. To
support this, we design a two-stage annotation pipeline and a curriculum
learning strategy, based on which we construct two datasets: EviBench, a
high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation
benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments
across a wide range of benchmarks demonstrate that DocR1 achieves
state-of-the-art performance on multi-page tasks, while consistently
maintaining strong results on single-page benchmarks.

</details>


### [193] [RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning](https://arxiv.org/abs/2508.07318)
*Jinjing Gu,Tianbao Qin,Yuanyuan Pu,Zhengpeng Zhao*

Main category: cs.CV

TL;DR: RORPCap is a retrieval-based method for image captioning that avoids redundant detection and GCN issues by using object-relation prompts and Mamba-based mapping, achieving competitive results with minimal training time.


<details>
  <summary>Details</summary>
Motivation: Existing image captioning methods using object detectors or GCNs face challenges like redundant information, GCN construction difficulty, and high training costs.

Method: RORPCap extracts object and relation words, encodes them into prompts, maps image embeddings to visual-text embeddings using Mamba, and generates captions via GPT-2.

Result: Achieves 120.5% CIDEr and 22.0% SPICE scores on MS-COCO with only 2.6 hours of training.

Conclusion: RORPCap is a viable alternative to detector/GCN-based models, offering efficiency and competitive performance.

Abstract: Image captioning aims to generate natural language descriptions for input
images in an open-form manner. To accurately generate descriptions related to
the image, a critical step in image captioning is to identify objects and
understand their relations within the image. Modern approaches typically
capitalize on object detectors or combine detectors with Graph Convolutional
Network (GCN). However, these models suffer from redundant detection
information, difficulty in GCN construction, and high training costs. To
address these issues, a Retrieval-based Objects and Relations Prompt for Image
Captioning (RORPCap) is proposed, inspired by the fact that image-text
retrieval can provide rich semantic information for input images. RORPCap
employs an Objects and relations Extraction Model to extract object and
relation words from the image. These words are then incorporate into predefined
prompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping
network is designed to quickly map image embeddings extracted by CLIP to
visual-text embeddings. Finally, the resulting prompt embeddings and
visual-text embeddings are concatenated to form textual-enriched feature
embeddings, which are fed into a GPT-2 model for caption generation. Extensive
experiments conducted on the widely used MS-COCO dataset show that the RORPCap
requires only 2.6 hours under cross-entropy loss training, achieving 120.5%
CIDEr score and 22.0% SPICE score on the "Karpathy" test split. RORPCap
achieves comparable performance metrics to detector-based and GCN-based models
with the shortest training time and demonstrates its potential as an
alternative for image captioning.

</details>


### [194] [Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos](https://arxiv.org/abs/2508.07330)
*Tuyen Tran,Thao Minh Le,Quang-Hung Le,Truyen Tran*

Main category: cs.CV

TL;DR: Planner-Refiner is a framework for video-language alignment, refining visual representations iteratively using language guidance. It outperforms state-of-the-art methods, especially for complex prompts.


<details>
  <summary>Details</summary>
Motivation: Addressing the semantic gap between language and vision in videos, particularly with evolving entities and complex action chains.

Method: Uses a Planner module to decompose language prompts into short chains and a Refiner to iteratively align visual tokens with language.

Result: Superior performance on Referring Video Object Segmentation and Temporal Grounding tasks, validated by a new benchmark (MeViS-X).

Conclusion: Planner-Refiner effectively bridges semantic gaps in video-language alignment, excelling with complex queries.

Abstract: Vision-language alignment in video must address the complexity of language,
evolving interacting entities, their action chains, and semantic gaps between
language and vision. This work introduces Planner-Refiner, a framework to
overcome these challenges. Planner-Refiner bridges the semantic gap by
iteratively refining visual elements' space-time representation, guided by
language until semantic gaps are minimal. A Planner module schedules language
guidance by decomposing complex linguistic prompts into short sentence chains.
The Refiner processes each short sentence, a noun-phrase and verb-phrase pair,
to direct visual tokens' self-attention across space then time, achieving
efficient single-step refinement. A recurrent system chains these steps,
maintaining refined visual token representations. The final representation
feeds into task-specific heads for alignment generation. We demonstrate
Planner-Refiner's effectiveness on two video-language alignment tasks:
Referring Video Object Segmentation and Temporal Grounding with varying
language complexity. We further introduce a new MeViS-X benchmark to assess
models' capability with long queries. Superior performance versus
state-of-the-art methods on these benchmarks shows the approach's potential,
especially for complex prompts.

</details>


### [195] [CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation](https://arxiv.org/abs/2508.07341)
*Fangtai Wu,Mushui Liu,Weijie He,Wanggui He,Hao Jiang,Zhao Wang,Yunlong Yu*

Main category: cs.CV

TL;DR: CoAR introduces a lightweight framework for customized image generation in unified AR models, avoiding costly fine-tuning and overfitting by freezing pre-trained parameters and using minimal additional parameters.


<details>
  <summary>Details</summary>
Motivation: Existing methods for customized image generation are costly and prone to overfitting or catastrophic forgetting, highlighting the need for a more efficient solution.

Method: CoAR uses Layerwise Multimodal Context Learning to inject subject concepts into AR models with minimal parameters, along with regularization to prevent overfitting and language drift.

Result: CoAR achieves superior performance in subject and style personalization with less than 0.05% of parameters tuned, outperforming recent methods like Proxy-Tuning.

Conclusion: CoAR offers an efficient, high-performance solution for customized image generation in AR models, balancing computational efficiency and customization quality.

Abstract: The unified autoregressive (AR) model excels at multimodal understanding and
generation, but its potential for customized image generation remains
underexplored. Existing customized generation methods rely on full fine-tuning
or adapters, making them costly and prone to overfitting or catastrophic
forgetting. In this paper, we propose \textbf{CoAR}, a novel framework for
injecting subject concepts into the unified AR models while keeping all
pre-trained parameters completely frozen. CoAR learns effective, specific
subject representations with only a minimal number of parameters using a
Layerwise Multimodal Context Learning strategy. To address overfitting and
language drift, we further introduce regularization that preserves the
pre-trained distribution and anchors context tokens to improve subject fidelity
and re-contextualization. Additionally, CoAR supports training-free subject
customization in a user-provided style. Experiments demonstrate that CoAR
achieves superior performance on both subject-driven personalization and style
personalization, while delivering significant gains in computational and memory
efficiency. Notably, CoAR tunes less than \textbf{0.05\%} of the parameters
while achieving competitive performance compared to recent Proxy-Tuning. Code:
https://github.com/KZF-kzf/CoAR

</details>


### [196] [SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal](https://arxiv.org/abs/2508.07346)
*Tingyu Yang,Jue Gong,Jinpei Guo,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: SODiff is a semantic-oriented one-step diffusion model for JPEG artifact removal, leveraging semantic guidance and adaptive denoising to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: JPEG compression introduces artifacts, and current deep learning methods struggle with texture recovery, leading to over-smoothed outputs.

Method: SODiff uses a semantic-aligned image prompt extractor (SAIPE) and a quality factor-aware time predictor for adaptive denoising.

Result: SODiff achieves superior visual quality and quantitative metrics compared to recent leading methods.

Conclusion: SODiff effectively removes JPEG artifacts by combining semantic guidance and adaptive denoising, setting a new benchmark.

Abstract: JPEG, as a widely used image compression standard, often introduces severe
visual artifacts when achieving high compression ratios. Although existing deep
learning-based restoration methods have made considerable progress, they often
struggle to recover complex texture details, resulting in over-smoothed
outputs. To overcome these limitations, we propose SODiff, a novel and
efficient semantic-oriented one-step diffusion model for JPEG artifacts
removal. Our core idea is that effective restoration hinges on providing
semantic-oriented guidance to the pre-trained diffusion model, thereby fully
leveraging its powerful generative prior. To this end, SODiff incorporates a
semantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features
from low-quality (LQ) images and projects them into an embedding space
semantically aligned with that of the text encoder. Simultaneously, it
preserves crucial information for faithful reconstruction. Furthermore, we
propose a quality factor-aware time predictor that implicitly learns the
compression quality factor (QF) of the LQ image and adaptively selects the
optimal denoising start timestep for the diffusion process. Extensive
experimental results show that our SODiff outperforms recent leading methods in
both visual quality and quantitative metrics. Code is available at:
https://github.com/frakenation/SODiff

</details>


### [197] [GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction](https://arxiv.org/abs/2508.07355)
*Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: GS4Buildings improves Gaussian Splatting for urban scenes by using semantic 3D building models, achieving better completeness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing 2D Gaussian Splatting struggles with large-scale urban scenes and occlusions, leading to incomplete reconstructions.

Method: Leverages semantic 3D building models for initialization, uses prior depth/normal maps for optimization, and offers a building-focused mode.

Result: Improves reconstruction completeness by 20.5% and geometric accuracy by 32.8%, with a 71.8% reduction in Gaussian primitives.

Conclusion: Semantic building model integration enhances GS-based reconstruction for urban applications like smart cities and digital twins.

Abstract: Recent advances in Gaussian Splatting (GS) have demonstrated its
effectiveness in photo-realistic rendering and 3D reconstruction. Among these,
2D Gaussian Splatting (2DGS) is particularly suitable for surface
reconstruction due to its flattened Gaussian representation and integrated
normal regularization. However, its performance often degrades in large-scale
and complex urban scenes with frequent occlusions, leading to incomplete
building reconstructions. We propose GS4Buildings, a novel prior-guided
Gaussian Splatting method leveraging the ubiquity of semantic 3D building
models for robust and scalable building surface reconstruction. Instead of
relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings
initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic
3D building models. Moreover, we generate prior depth and normal maps from the
planar building geometry and incorporate them into the optimization process,
providing strong geometric guidance for surface consistency and structural
accuracy. We also introduce an optional building-focused mode that limits
reconstruction to building regions, achieving a 71.8% reduction in Gaussian
primitives and enabling a more efficient and compact representation.
Experiments on urban datasets demonstrate that GS4Buildings improves
reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These
results highlight the potential of semantic building model integration to
advance GS-based reconstruction toward real-world urban applications such as
smart cities and digital twins. Our project is available:
https://github.com/zqlin0521/GS4Buildings.

</details>


### [198] [Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring](https://arxiv.org/abs/2508.07369)
*Tianyu Xin,Jin-Liang Xiao,Zeyu Xia,Shan Yin,Liang-Jian Deng*

Main category: cs.CV

TL;DR: A novel method addresses cross-sensor degradation in pansharpening by integrating a Feature Tailor at a critical interface, enabling efficient unsupervised training and patch-wise processing for improved generalization and speed.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for pansharpening struggle with cross-sensor generalization, and current solutions like retraining or zero-shot methods are inefficient or require extra data.

Method: The method decomposes pansharpening models to identify a key interface, integrates a Feature Tailor for feature-level adaptation, and uses physics-aware unsupervised losses for efficient training. Patch-wise processing enhances speed.

Result: The method achieves state-of-the-art performance in cross-sensor cases, with sub-second training and inference times, outperforming prior methods by over 100 times in speed.

Conclusion: The proposed approach significantly improves generalization and efficiency in pansharpening, making it practical for real-world applications with minimal computational cost.

Abstract: Deep learning methods for pansharpening have advanced rapidly, yet models
pretrained on data from a specific sensor often generalize poorly to data from
other sensors. Existing methods to tackle such cross-sensor degradation include
retraining model or zero-shot methods, but they are highly time-consuming or
even need extra training data. To address these challenges, our method first
performs modular decomposition on deep learning-based pansharpening models,
revealing a general yet critical interface where high-dimensional fused
features begin mapping to the channel space of the final image. % may need
revisement A Feature Tailor is then integrated at this interface to address
cross-sensor degradation at the feature level, and is trained efficiently with
physics-aware unsupervised losses. Moreover, our method operates in a
patch-wise manner, training on partial patches and performing parallel
inference on all patches to boost efficiency. Our method offers two key
advantages: (1) $\textit{Improved Generalization Ability}$: it significantly
enhance performance in cross-sensor cases. (2) $\textit{Low Generalization
Cost}$: it achieves sub-second training and inference, requiring only partial
test inputs and no external data, whereas prior methods often take minutes or
even hours. Experiments on the real-world data from multiple datasets
demonstrate that our method achieves state-of-the-art quality and efficiency in
tackling cross-sensor degradation. For example, training and inference of
$512\times512\times8$ image within $\textit{0.2 seconds}$ and
$4000\times4000\times8$ image within $\textit{3 seconds}$ at the fastest
setting on a commonly used RTX 3090 GPU, which is over 100 times faster than
zero-shot methods.

</details>


### [199] [DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery](https://arxiv.org/abs/2508.07372)
*Rajaei Khatib,Raja Giryes*

Main category: cs.CV

TL;DR: DIP-GS enhances 3D Gaussian Splatting (3DGS) for sparse-view reconstruction by integrating Deep Image Prior (DIP), achieving SOTA results without pre-trained models.


<details>
  <summary>Details</summary>
Motivation: 3DGS struggles with sparse-view reconstruction due to limited input views. DIP-GS addresses this by leveraging DIP's internal patterns.

Method: DIP-GS combines DIP with 3DGS in a coarse-to-fine manner, using only input frames without external models.

Result: DIP-GS achieves competitive SOTA results in sparse-view reconstruction tasks.

Conclusion: DIP-GS effectively improves 3DGS for sparse-view scenarios, demonstrating robust performance without reliance on pre-trained models.

Abstract: 3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,
obtaining high-quality reconstruction with real-time rendering runtime
performance. The main idea behind 3DGS is to represent the scene as a
collection of 3D gaussians, while learning their parameters to fit the given
views of the scene. While achieving superior performance in the presence of
many views, 3DGS struggles with sparse view reconstruction, where the input
views are sparse and do not fully cover the scene and have low overlaps. In
this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By
using the DIP prior, which utilizes internal structure and patterns, with
coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla
3DGS fails, such as sparse view recovery. Note that our approach does not use
any pre-trained models such as generative models and depth estimation, but
rather relies only on the input frames. Among such methods, DIP-GS obtains
state-of-the-art (SOTA) competitive results on various sparse-view
reconstruction tasks, demonstrating its capabilities.

</details>


### [200] [LET-US: Long Event-Text Understanding of Scenes](https://arxiv.org/abs/2508.07401)
*Rui Chen,Xingyu Chen,Shaoan Wang,Shihan Kong,Junzhi Yu*

Main category: cs.CV

TL;DR: LET-US is a framework for long event-stream--text comprehension, using adaptive compression and a two-stage optimization paradigm to bridge the modality gap between event streams and text. It outperforms existing MLLMs in accuracy and comprehension.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with interpreting long event streams or fail to process them effectively, creating a need for a solution like LET-US.

Method: LET-US employs adaptive compression, text-guided cross-modal queries, hierarchical clustering, and similarity computation to reduce event volume and preserve details. A large-scale event-text dataset is curated for training.

Result: LET-US outperforms prior MLLMs in descriptive accuracy and semantic comprehension for long-duration event streams.

Conclusion: LET-US sets a new standard for cross-modal understanding of long event sequences, with publicly available datasets, codes, and models.

Abstract: Event cameras output event streams as sparse, asynchronous data with
microsecond-level temporal resolution, enabling visual perception with low
latency and a high dynamic range. While existing Multimodal Large Language
Models (MLLMs) have achieved significant success in understanding and analyzing
RGB video content, they either fail to interpret event streams effectively or
remain constrained to very short sequences. In this paper, we introduce LET-US,
a framework for long event-stream--text comprehension that employs an adaptive
compression mechanism to reduce the volume of input events while preserving
critical visual details. LET-US thus establishes a new frontier in cross-modal
inferential understanding over extended event sequences. To bridge the
substantial modality gap between event streams and textual representations, we
adopt a two-stage optimization paradigm that progressively equips our model
with the capacity to interpret event-based scenes. To handle the voluminous
temporal information inherent in long event streams, we leverage text-guided
cross-modal queries for feature reduction, augmented by hierarchical clustering
and similarity computation to distill the most representative event features.
Moreover, we curate and construct a large-scale event-text aligned dataset to
train our model, achieving tighter alignment of event features within the LLM
embedding space. We also develop a comprehensive benchmark covering a diverse
set of tasks -- reasoning, captioning, classification, temporal localization
and moment retrieval. Experimental results demonstrate that LET-US outperforms
prior state-of-the-art MLLMs in both descriptive accuracy and semantic
comprehension on long-duration event streams. All datasets, codes, and models
will be publicly available.

</details>


### [201] [ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack](https://arxiv.org/abs/2508.07402)
*Rongxuan Peng,Shunquan Tan,Chenqi Kong,Anwei Luo,Alex C. Kot,Jiwu Huang*

Main category: cs.CV

TL;DR: ForensicsSAM is a robust framework for image forgery detection and localization (IFDL) that addresses adversarial vulnerabilities in PEFT-based methods by integrating forgery and adversary experts, along with a light-weight detector.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT-based methods for adapting large vision models to IFDL tasks are vulnerable to adversarial attacks, degrading performance.

Method: ForensicsSAM introduces forgery experts in transformer blocks, a light-weight adversary detector, and adversary experts in attention layers and MLP modules to correct adversarial noise.

Result: The framework resists various adversarial attacks and achieves state-of-the-art performance in IFDL tasks.

Conclusion: ForensicsSAM provides a unified, robust solution for IFDL, combining adversarial resistance with high detection and localization accuracy.

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for
adapting large vision foundation models, such as the Segment Anything Model
(SAM) and LLaVA, to downstream tasks like image forgery detection and
localization (IFDL). However, existing PEFT-based approaches overlook their
vulnerability to adversarial attacks. In this paper, we show that highly
transferable adversarial images can be crafted solely via the upstream model,
without accessing the downstream model or training data, significantly
degrading the IFDL performance. To address this, we propose ForensicsSAM, a
unified IFDL framework with built-in adversarial robustness. Our design is
guided by three key ideas: (1) To compensate for the lack of forgery-relevant
knowledge in the frozen image encoder, we inject forgery experts into each
transformer block to enhance its ability to capture forgery artifacts. These
forgery experts are always activated and shared across any input images. (2) To
detect adversarial images, we design an light-weight adversary detector that
learns to capture structured, task-specific artifact in RGB domain, enabling
reliable discrimination across various attack methods. (3) To resist
adversarial attacks, we inject adversary experts into the global attention
layers and MLP modules to progressively correct feature shifts induced by
adversarial noise. These adversary experts are adaptively activated by the
adversary detector, thereby avoiding unnecessary interference with clean
images. Extensive experiments across multiple benchmarks demonstrate that
ForensicsSAM achieves superior resistance to various adversarial attack
methods, while also delivering state-of-the-art performance in image-level
forgery detection and pixel-level forgery localization. The resource is
available at https://github.com/siriusPRX/ForensicsSAM.

</details>


### [202] [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409)
*Junyao Gao,Jiaxing Li,Wenran Liu,Yanhong Zeng,Fei Shen,Kai Chen,Yanan Sun,Cairong Zhao*

Main category: cs.CV

TL;DR: CharacterShot is a 4D character animation framework that creates dynamic 3D characters from a single image and 2D pose sequence, using a pretrained 2D model, dual-attention for 3D lifting, and 4D Gaussian splatting for optimization.


<details>
  <summary>Details</summary>
Motivation: To enable individual designers to create consistent 4D character animations easily from minimal inputs.

Method: Pretrains a 2D animation model, lifts it to 3D with dual-attention and camera prior, and optimizes with neighbor-constrained 4D Gaussian splatting.

Result: Outperforms state-of-the-art methods on the CharacterBench benchmark.

Conclusion: CharacterShot provides a scalable and efficient solution for 4D character animation, supported by a new dataset (Character4D).

Abstract: In this paper, we propose \textbf{CharacterShot}, a controllable and
consistent 4D character animation framework that enables any individual
designer to create dynamic 3D characters (i.e., 4D character animation) from a
single reference character image and a 2D pose sequence. We begin by
pretraining a powerful 2D character animation model based on a cutting-edge
DiT-based image-to-video model, which allows for any 2D pose sequnce as
controllable signal. We then lift the animation model from 2D to 3D through
introducing dual-attention module together with camera prior to generate
multi-view videos with spatial-temporal and spatial-view consistency. Finally,
we employ a novel neighbor-constrained 4D gaussian splatting optimization on
these multi-view videos, resulting in continuous and stable 4D character
representations. Moreover, to improve character-centric performance, we
construct a large-scale dataset Character4D, containing 13,115 unique
characters with diverse appearances and motions, rendered from multiple
viewpoints. Extensive experiments on our newly constructed benchmark,
CharacterBench, demonstrate that our approach outperforms current
state-of-the-art methods. Code, models, and datasets will be publicly available
at https://github.com/Jeoyal/CharacterShot.

</details>


### [203] [CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](https://arxiv.org/abs/2508.07413)
*Youqi Wang,Shunquan Tan,Rongxuan Peng,Bin Li,Jiwu Huang*

Main category: cs.CV

TL;DR: CLUE repurposes Stable Diffusion 3 (SD3) and Segment Anything Model (SAM) to detect and localize digital forgeries with high fidelity, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated forgeries threatens digital media authenticity, necessitating advanced detection tools.

Method: CLUE uses LoRA to adapt SD3 for forensic feature extraction, leveraging noise injection and SAM for semantic context.

Result: CLUE achieves state-of-the-art generalization and robustness against post-processing and OSNs.

Conclusion: CLUE offers a powerful, parameter-efficient solution for forgery detection, with publicly available code.

Abstract: The increasing accessibility of image editing tools and generative AI has led
to a proliferation of visually convincing forgeries, compromising the
authenticity of digital media. In this paper, in addition to leveraging
distortions from conventional forgeries, we repurpose the mechanism of a
state-of-the-art (SOTA) text-to-image synthesis model by exploiting its
internal generative process, turning it into a high-fidelity forgery
localization tool. To this end, we propose CLUE (Capture Latent Uncovered
Evidence), a framework that employs Low- Rank Adaptation (LoRA) to
parameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic
feature extractor. Our approach begins with the strategic use of SD3's
Rectified Flow (RF) mechanism to inject noise at varying intensities into the
latent representation, thereby steering the LoRAtuned denoising process to
amplify subtle statistical inconsistencies indicative of a forgery. To
complement the latent analysis with high-level semantic context and precise
spatial details, our method incorporates contextual features from the image
encoder of the Segment Anything Model (SAM), which is parameter-efficiently
adapted to better trace the boundaries of forged regions. Extensive evaluations
demonstrate CLUE's SOTA generalization performance, significantly outperforming
prior methods. Furthermore, CLUE shows superior robustness against common
post-processing attacks and Online Social Networks (OSNs). Code is publicly
available at https://github.com/SZAISEC/CLUE.

</details>


### [204] [Freeze and Reveal: Exposing Modality Bias in Vision-Language Models](https://arxiv.org/abs/2508.07432)
*Vivek Hruday Kavuri,Vysishtya Karanam,Venkata Jahnavi Venkamsetty,Kriti Madumadukala,Lakshmipathi Balaji Darur,Ponnurangam Kumaraguru*

Main category: cs.CV

TL;DR: The paper investigates gender bias in Vision Language Models (VLMs), attributing it to vision and text modalities. It introduces debiasing methods (CDA and DAUDoS) and a new metric (Degree of Stereotypicality), showing improvements with minimal data. Results reveal bias sources in CLIP and PaliGemma2 encoders, aiding targeted mitigation.


<details>
  <summary>Details</summary>
Motivation: VLMs inherit gender biases from training data, but the source (vision or text) is unclear. The study aims to dissect these biases and develop efficient debiasing methods.

Method: Uses Counterfactual Data Augmentation (CDA) and introduces DAUDoS, a data-efficient debiasing method. Evaluates on VisoGender benchmark with a gender-annotated dataset.

Result: CDA reduces gender gap by 6%, DAUDoS by 3% with one-third data. Both improve gender identification by 3%. CLIP's vision encoder and PaliGemma2's text encoder are primary bias sources.

Conclusion: Identifying bias sources enables targeted mitigation. DAUDoS offers efficient debiasing, paving the way for fairer multi-modal systems.

Abstract: Vision Language Models achieve impressive multi-modal performance but often
inherit gender biases from their training data. This bias might be coming from
both the vision and text modalities. In this work, we dissect the contributions
of vision and text backbones to these biases by applying targeted debiasing
using Counterfactual Data Augmentation and Task Vector methods. Inspired by
data-efficient approaches in hate-speech classification, we introduce a novel
metric, Degree of Stereotypicality and a corresponding debiasing method, Data
Augmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with
minimal computational cost. We curate a gender annotated dataset and evaluate
all methods on VisoGender benchmark to quantify improvements and identify
dominant source of bias. Our results show that CDA reduces the gender gap by 6%
and DAUDoS by 3% but using only one-third of the data. Both methods also
improve the model's ability to correctly identify gender in images by 3%, with
DAUDoS achieving this improvement using only almost one-third of training data.
From our experiment's, we observed that CLIP's vision encoder is more biased
whereas PaliGemma2's text encoder is more biased. By identifying whether bias
stems more from vision or text encoders, our work enables more targeted and
effective bias mitigation strategies in future multi-modal systems.

</details>


### [205] [Levarging Learning Bias for Noisy Anomaly Detection](https://arxiv.org/abs/2508.07441)
*Yuxin Zhang,Yunkang Cao,Yuqi Cheng,Yihan Sun,Weiming Shen*

Main category: cs.CV

TL;DR: A two-stage framework for fully unsupervised image anomaly detection (FUIAD) leverages learning bias to filter contaminated training data, improving detection performance.


<details>
  <summary>Details</summary>
Motivation: Real-world training data often contains unlabeled anomalies, degrading conventional methods that assume anomaly-free data.

Method: Stage 1 partitions training data, trains sub-models, and aggregates anomaly scores to purify the dataset. Stage 2 trains the final detector on this dataset.

Result: Superior anomaly detection and localization on the Real-IAD benchmark, with resilience to noise.

Conclusion: The framework's exploitation of learning bias and model-agnostic design offers a practical solution for imperfect training data.

Abstract: This paper addresses the challenge of fully unsupervised image anomaly
detection (FUIAD), where training data may contain unlabeled anomalies.
Conventional methods assume anomaly-free training data, but real-world
contamination leads models to absorb anomalies as normal, degrading detection
performance. To mitigate this, we propose a two-stage framework that
systematically exploits inherent learning bias in models. The learning bias
stems from: (1) the statistical dominance of normal samples, driving models to
prioritize learning stable normal patterns over sparse anomalies, and (2)
feature-space divergence, where normal data exhibit high intra-class
consistency while anomalies display high diversity, leading to unstable model
responses. Leveraging the learning bias, stage 1 partitions the training set
into subsets, trains sub-models, and aggregates cross-model anomaly scores to
filter a purified dataset. Stage 2 trains the final detector on this dataset.
Experiments on the Real-IAD benchmark demonstrate superior anomaly detection
and localization performance under different noise conditions. Ablation studies
further validate the framework's contamination resilience, emphasizing the
critical role of learning bias exploitation. The model-agnostic design ensures
compatibility with diverse unsupervised backbones, offering a practical
solution for real-world scenarios with imperfect training data. Code is
available at https://github.com/hustzhangyuxin/LLBNAD.

</details>


### [206] [Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines](https://arxiv.org/abs/2508.07450)
*Suman Kunwar,Prabesh Rai*

Main category: cs.CV

TL;DR: The study benchmarks waste classification models for HCW in Nepal, finding YOLOv5-s as the most accurate (95.06%) but slightly slower than YOLOv8-n. EfficientNet-B0 showed promise but was slower. YOLOv5-s was deployed for public use.


<details>
  <summary>Details</summary>
Motivation: To address challenges in HCW management in Nepal, such as improper segregation and disposal, which risks contamination and disease spread.

Method: Benchmarked ResNeXt-50, EfficientNet-B0, MobileNetV3-S, YOLOv8-n, and YOLOv5-s using Stratified K-fold (5 folds) on HCW data. Used repetitive ANOVA for statistical significance.

Result: YOLOv5-s achieved the highest accuracy (95.06%) but was slightly slower than YOLOv8-n. EfficientNet-B0 had 93.22% accuracy but the slowest inference.

Conclusion: YOLOv5-s was deployed for public use with Nepal's HCW standards. Future work includes improving data and local context.

Abstract: The increasing number of Health Care facilities in Nepal has also added up
the challenges on managing health care waste (HCW). Improper segregation and
disposal of HCW leads to the contamination, spreading of infectious diseases
and puts a risk of waste handlers. This study benchmarks the state of the art
waste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,
YOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds
on combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%
accuracy but fell short few milliseconds in inference speed with YOLOv8-n
model. The EfficientNet-B0 showed promising results of 93.22% accuracy but took
the highest inference time. A repetitive ANOVA was performed to see statistical
significance and the best performing model (YOLOv5-s) was deployed to the web
with mapped bin color using Nepal's HCW management standards for public usage.
Further work on the data was suggested along with localized context.

</details>


### [207] [AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning](https://arxiv.org/abs/2508.07470)
*Siminfar Samakoush Galougah,Rishie Raj,Sanjoy Chowdhury,Sayan Nag,Ramani Duraiswami*

Main category: cs.CV

TL;DR: AURA is a new benchmark for evaluating cross-modal reasoning in AV-LLMs and OLMs, focusing on six cognitive domains to prevent uni-modal shortcuts. It introduces AuraScore to assess reasoning fidelity, revealing a gap between model accuracy and reasoning quality.


<details>
  <summary>Details</summary>
Motivation: Current AV benchmarks overlook reasoning processes, making it hard to distinguish genuine comprehension from flawed reasoning or hallucinations.

Method: AURA includes questions across six cognitive domains, forcing models to use both audio and video. AuraScore evaluates reasoning fidelity through Factual Consistency and Core Inference.

Result: SOTA models show high accuracy (up to 92%) but low reasoning scores (<45%), indicating flawed logic behind correct answers.

Conclusion: AURA highlights the need for robust multimodal evaluation and addresses the reasoning gap in AV models.

Abstract: Current audio-visual (AV) benchmarks focus on final answer accuracy,
overlooking the underlying reasoning process. This makes it difficult to
distinguish genuine comprehension from correct answers derived through flawed
reasoning or hallucinations. To address this, we introduce AURA (Audio-visual
Understanding and Reasoning Assessment), a benchmark for evaluating the
cross-modal reasoning capabilities of Audio-Visual Large Language Models
(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across
six challenging cognitive domains, such as causality, timbre and pitch, tempo
and AV synchronization, unanswerability, implicit distractions, and skill
profiling, explicitly designed to be unanswerable from a single modality. This
forces models to construct a valid logical path grounded in both audio and
video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To
assess reasoning traces, we propose a novel metric, AuraScore, which addresses
the lack of robust tools for evaluating reasoning fidelity. It decomposes
reasoning into two aspects: (i) Factual Consistency - whether reasoning is
grounded in perceptual evidence, and (ii) Core Inference - the logical validity
of each reasoning step. Evaluations of SOTA models on AURA reveal a critical
reasoning gap: although models achieve high accuracy (up to 92% on some tasks),
their Factual Consistency and Core Inference scores fall below 45%. This
discrepancy highlights that models often arrive at correct answers through
flawed logic, underscoring the need for our benchmark and paving the way for
more robust multimodal evaluation.

</details>


### [208] [Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution](https://arxiv.org/abs/2508.07483)
*Pranav Chougule*

Main category: cs.CV

TL;DR: Comparison of Photogrammetry and Gaussian Splatting for 3D model reconstruction and view synthesis, with a modified Gaussian Splatting method for novel view rendering.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the performance of Photogrammetry and Gaussian Splatting in 3D reconstruction and view synthesis, and to enhance Gaussian Splatting for novel view rendering.

Method: Created a dataset of real-world images, built 3D models using both techniques, and evaluated them using SSIM, PSNR, LPIPS, and lp/mm resolution. Enhanced Gaussian Splatting for novel view synthesis in Blender.

Result: Gaussian Splatting outperforms in generating high-quality novel views and improves photogrammetry-based reconstructions. Both methods have strengths and limitations.

Conclusion: Gaussian Splatting shows promise for enhancing 3D reconstructions and view synthesis, with applications in XR, photogrammetry, and autonomous simulations.

Abstract: In this paper, I present a comprehensive study comparing Photogrammetry and
Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I
created a dataset of images from a real-world scene and constructed 3D models
using both methods. To evaluate the performance, I compared the models using
structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned
perceptual image patch similarity (LPIPS), and lp/mm resolution based on the
USAF resolution chart. A significant contribution of this work is the
development of a modified Gaussian Splatting repository, which I forked and
enhanced to enable rendering images from novel camera poses generated in the
Blender environment. This innovation allows for the synthesis of high-quality
novel views, showcasing the flexibility and potential of Gaussian Splatting. My
investigation extends to an augmented dataset that includes both original
ground images and novel views synthesized via Gaussian Splatting. This
augmented dataset was employed to generate a new photogrammetry model, which
was then compared against the original photogrammetry model created using only
the original images. The results demonstrate the efficacy of using Gaussian
Splatting to generate novel high-quality views and its potential to improve
photogrammetry-based 3D reconstructions. The comparative analysis highlights
the strengths and limitations of both approaches, providing valuable
information for applications in extended reality (XR), photogrammetry, and
autonomous vehicle simulations. Code is available at
https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.

</details>


### [209] [VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding](https://arxiv.org/abs/2508.07493)
*Jian Chen,Ming Li,Jihyung Kil,Chenguang Wang,Tong Yu,Ryan Rossi,Tianyi Zhou,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: VisR-Bench is a multilingual benchmark for question-driven multimodal retrieval in long documents, addressing gaps in existing benchmarks by including diverse languages and question types.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are limited to English-only document retrieval or single-page multilingual QA, lacking coverage for long documents and diverse question types.

Method: VisR-Bench includes over 35K QA pairs across 1.2K documents in 16 languages, with three question types (figures, text, tables) and queries without explicit answers.

Result: MLLMs outperform text-based and multimodal encoder models but struggle with structured tables and low-resource languages.

Conclusion: VisR-Bench highlights challenges in multilingual visual retrieval, particularly with structured data and low-resource languages, while providing a robust evaluation framework.

Abstract: Most organizational data in this world are stored as documents, and visual
retrieval plays a crucial role in unlocking the collective intelligence from
all these documents. However, existing benchmarks focus on English-only
document retrieval or only consider multilingual question-answering on a
single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual
benchmark designed for question-driven multimodal retrieval in long documents.
Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents,
enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans
sixteen languages with three question types (figures, text, and tables),
offering diverse linguistic and question coverage. Unlike prior datasets, we
include queries without explicit answers, preventing models from relying on
superficial keyword matching. We evaluate various retrieval models, including
text-based methods, multimodal encoders, and MLLMs, providing insights into
their strengths and limitations. Our results show that while MLLMs
significantly outperform text-based and multimodal encoder models, they still
struggle with structured tables and low-resource languages, highlighting key
challenges in multilingual visual retrieval.

</details>


### [210] [FormCoach: Lift Smarter, Not Harder](https://arxiv.org/abs/2508.07501)
*Xiaoye Zuo,Nikos Athanasiou,Ginger Delmas,Yiming Huang,Xingyu Fu,Lingjie Liu*

Main category: cs.CV

TL;DR: FormCoach uses AI and vision-language models to provide real-time fitness form feedback via a web interface, benchmarking VLMs on a dataset of 1,700 expert-annotated videos.


<details>
  <summary>Details</summary>
Motivation: Expert feedback is often inaccessible for at-home fitness enthusiasts, creating a need for AI-driven solutions to provide real-time form corrections.

Method: Leverages vision-language models (VLMs) to analyze user form, benchmarked on a dataset of 1,700 expert-annotated video pairs across 22 exercises.

Result: Benchmarks show gaps compared to human-level coaching, highlighting challenges in nuanced movement analysis.

Conclusion: FormCoach pioneers AI-driven fitness coaching, framing form correction as a collaborative human-machine process and advancing embodied AI.

Abstract: Good form is the difference between strength and strain, yet for the
fast-growing community of at-home fitness enthusiasts, expert feedback is often
out of reach. FormCoach transforms a simple camera into an always-on,
interactive AI training partner, capable of spotting subtle form errors and
delivering tailored corrections in real time, leveraging vision-language models
(VLMs). We showcase this capability through a web interface and benchmark
state-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference
video pairs spanning 22 strength and mobility exercises. To accelerate research
in AI-driven coaching, we release both the dataset and an automated,
rubric-based evaluation pipeline, enabling standardized comparison across
models. Our benchmarks reveal substantial gaps compared to human-level
coaching, underscoring both the challenges and opportunities in integrating
nuanced, context-aware movement analysis into interactive AI systems. By
framing form correction as a collaborative and creative process between humans
and machines, FormCoach opens a new frontier in embodied AI.

</details>


### [211] [From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials](https://arxiv.org/abs/2508.07514)
*Artzai Picon,Itziar Eguskiza,Daniel Mugica,Javier Romero,Carlos Javier Jimenez,Eric White,Gabriel Do-Lago-Junqueira,Christian Klukas,Ramon Navarra-Mestre*

Main category: cs.CV

TL;DR: An improved segmentation model for herbicide research automates species and damage identification, outperforming manual methods and showing robustness across diverse conditions.


<details>
  <summary>Details</summary>
Motivation: Manual visual assessments in herbicide research are time-consuming and subjective, necessitating automation for efficiency and consistency.

Method: Combines a self-supervised visual model with hierarchical inference based on botanical taxonomy, trained on multi-year, multi-location data and tested across devices and domains.

Result: Significant improvements in species identification (F1-score: 0.52 to 0.85) and damage classification (F1-score: 0.28 to 0.44), with robust performance under domain shift.

Conclusion: The model is robust and applicable in real-world scenarios, now deployed in BASF's phenotyping pipeline for automated crop and weed monitoring.

Abstract: Field trials are vital in herbicide research and development to assess
effects on crops and weeds under varied conditions. Traditionally, evaluations
rely on manual visual assessments, which are time-consuming, labor-intensive,
and subjective. Automating species and damage identification is challenging due
to subtle visual differences, but it can greatly enhance efficiency and
consistency.
  We present an improved segmentation model combining a general-purpose
self-supervised visual model with hierarchical inference based on botanical
taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain
using digital and mobile cameras, the model was tested on digital camera data
(year 2023) and drone imagery from the United States, Germany, and Spain (year
2024) to evaluate robustness under domain shift. This cross-device evaluation
marks a key step in assessing generalization across platforms of the model.
  Our model significantly improved species identification (F1-score: 0.52 to
0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to
0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone
images), it maintained strong performance with moderate degradation (species:
F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where
earlier models failed.
  These results confirm the model's robustness and real-world applicability. It
is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated
crop and weed monitoring across diverse geographies.

</details>


### [212] [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/abs/2508.07519)
*Joonghyuk Shin,Alchan Hwang,Yujin Kim,Daneul Kim,Jaesik Park*

Main category: cs.CV

TL;DR: The paper analyzes MM-DiT's bidirectional attention mechanism, proposes a prompt-based editing method, and bridges U-Net and MM-DiT approaches.


<details>
  <summary>Details</summary>
Motivation: To address challenges in editing MM-DiT models due to their bidirectional attention, and to provide insights into their behavior.

Method: Decomposes MM-DiT's attention matrices into four blocks and develops a prompt-based editing technique.

Result: A robust editing method for MM-DiT, supporting global to local edits across variants.

Conclusion: The findings bridge U-Net and MM-DiT methods, offering deeper understanding of MM-DiT's behavior.

Abstract: Transformer-based diffusion models have recently superseded traditional U-Net
architectures, with multimodal diffusion transformers (MM-DiT) emerging as the
dominant approach in state-of-the-art models like Stable Diffusion 3 and
Flux.1. Previous approaches have relied on unidirectional cross-attention
mechanisms, with information flowing from text embeddings to image latents. In
contrast, MMDiT introduces a unified attention mechanism that concatenates
input projections from both modalities and performs a single full attention
operation, allowing bidirectional information flow between text and image
branches. This architectural shift presents significant challenges for existing
editing techniques. In this paper, we systematically analyze MM-DiT's attention
mechanism by decomposing attention matrices into four distinct blocks,
revealing their inherent characteristics. Through these analyses, we propose a
robust, prompt-based image editing method for MM-DiT that supports global to
local edits across various MM-DiT variants, including few-step models. We
believe our findings bridge the gap between existing U-Net-based methods and
emerging architectures, offering deeper insights into MMDiT's behavioral
patterns.

</details>


### [213] [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
*Weitai Kang,Weiming Zhuang,Zhizhong Li,Yan Yan,Lingjuan Lyu*

Main category: cs.CV

TL;DR: The paper analyzes design choices for fine-tuning Multimodal Large Language Models (MLLMs) for visual grounding (VG), using LLaVA-1.5 as a baseline. It identifies effective paradigms and optimizes grounding data, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic verification in existing approaches for fine-tuning MLLMs for VG, the study aims to provide a comprehensive analysis of design choices.

Method: The study uses LLaVA-1.5 to explore visual grounding paradigms and conducts ablation studies on grounding data design.

Result: The optimized MLLM achieves improvements of +5.6%, +6.9%, and +7.0% on RefCOCO, RefCOCO+, and RefCOCOg datasets, respectively.

Conclusion: The findings enhance MLLM performance for VG, offering insights into effective design choices and grounding data optimization.

Abstract: Fine-grained multimodal capability in Multimodal Large Language Models
(MLLMs) has emerged as a critical research direction, particularly for tackling
the visual grounding (VG) problem. Despite the strong performance achieved by
existing approaches, they often employ disparate design choices when
fine-tuning MLLMs for VG, lacking systematic verification to support these
designs. To bridge this gap, this paper presents a comprehensive study of
various design choices that impact the VG performance of MLLMs. We conduct our
analysis using LLaVA-1.5, which has been widely adopted in prior empirical
studies of MLLMs. While more recent models exist, we follow this convention to
ensure our findings remain broadly applicable and extendable to other
architectures. We cover two key aspects: (1) exploring different visual
grounding paradigms in MLLMs, identifying the most effective design, and
providing our insights; and (2) conducting ablation studies on the design of
grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our
findings contribute to a stronger MLLM for VG, achieving improvements of +5.6%
/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

</details>


### [214] [Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module](https://arxiv.org/abs/2508.07528)
*Xiaotong Ji,Ryoma Bise,Seiichi Uchida*

Main category: cs.CV

TL;DR: A novel approach enhances top-rank learning in medical image processing by integrating a rejection module to handle noisy labels and class-ambiguous instances, improving diagnosis accuracy.


<details>
  <summary>Details</summary>
Motivation: Noisy labels and class-ambiguous instances hinder top-rank learning, impacting the accuracy of medical image diagnoses.

Method: Proposes a rejection module cooptimized with top-rank loss to identify and mitigate outliers, using a rejection function to measure deviation from the norm.

Result: Experimental validation shows the method effectively detects and mitigates outliers, enhancing diagnosis reliability and accuracy.

Conclusion: The integration of a rejection module improves top-rank learning, addressing challenges in medical image processing for better diagnostic outcomes.

Abstract: In medical image processing, accurate diagnosis is of paramount importance.
Leveraging machine learning techniques, particularly top-rank learning, shows
significant promise by focusing on the most crucial instances. However,
challenges arise from noisy labels and class-ambiguous instances, which can
severely hinder the top-rank objective, as they may be erroneously placed among
the top-ranked instances. To address these, we propose a novel approach that
enhances toprank learning by integrating a rejection module. Cooptimized with
the top-rank loss, this module identifies and mitigates the impact of outliers
that hinder training effectiveness. The rejection module functions as an
additional branch, assessing instances based on a rejection function that
measures their deviation from the norm. Through experimental validation on a
medical dataset, our methodology demonstrates its efficacy in detecting and
mitigating outliers, improving the reliability and accuracy of medical image
diagnoses.

</details>


### [215] [Enhanced Generative Structure Prior for Chinese Text Image Super-resolution](https://arxiv.org/abs/2508.07537)
*Xiaoming Li,Wangmeng Zuo,Chen Change Loy*

Main category: cs.CV

TL;DR: A framework for high-quality text image super-resolution (SR) of Chinese characters, using a novel structure prior within a StyleGAN model to restore precise strokes and handle diverse fonts and layouts.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on English text, neglecting complex scripts like Chinese. This work aims to restore low-resolution Chinese characters accurately.

Method: Proposes a structure prior integrated into StyleGAN, using a codebook for character structure and StyleGAN's vector $w$ for style control, ensuring spatial and structural alignment.

Result: The framework effectively restores degraded Chinese characters, even with irregular layouts, by providing character-specific guidance.

Conclusion: The method successfully enhances visual quality for Chinese text SR, with code and models made publicly available.

Abstract: Faithful text image super-resolution (SR) is challenging because each
character has a unique structure and usually exhibits diverse font styles and
layouts. While existing methods primarily focus on English text, less attention
has been paid to more complex scripts like Chinese. In this paper, we introduce
a high-quality text image SR framework designed to restore the precise strokes
of low-resolution (LR) Chinese characters. Unlike methods that rely on
character recognition priors to regularize the SR task, we propose a novel
structure prior that offers structure-level guidance to enhance visual quality.
Our framework incorporates this structure prior within a StyleGAN model,
leveraging its generative capabilities for restoration. To maintain the
integrity of character structures while accommodating various font styles and
layouts, we implement a codebook-based mechanism that restricts the generative
space of StyleGAN. Each code in the codebook represents the structure of a
specific character, while the vector $w$ in StyleGAN controls the character's
style, including typeface, orientation, and location. Through the collaborative
interaction between the codebook and style, we generate a high-resolution
structure prior that aligns with LR characters both spatially and structurally.
Experiments demonstrate that this structure prior provides robust,
character-specific guidance, enabling the accurate restoration of clear strokes
in degraded characters, even for real-world LR Chinese text with irregular
layouts. Our code and pre-trained models will be available at
https://github.com/csxmli2016/MARCONetPlusPlus

</details>


### [216] [A DICOM Image De-identification Algorithm in the MIDI-B Challenge](https://arxiv.org/abs/2508.07538)
*Hongzhu Jiang,Sihan Xie,Zhiyu Wan*

Main category: cs.CV

TL;DR: The paper discusses the MIDI-B Challenge at MICCAI 2024, focusing on DICOM image de-identification to comply with privacy regulations. It details methods like pixel masking and text removal, achieving 99.92% accuracy, ranking 2nd out of 10 teams.


<details>
  <summary>Details</summary>
Motivation: To ensure patient privacy in medical image sharing by de-identifying DICOM images, adhering to HIPAA, DICOM PS3.15, and TCIA standards.

Method: Applied pixel masking, date shifting, text recognition, and removal methods to de-identify DICOM images.

Result: Achieved 99.92% accuracy in de-identification, ranking 2nd in the MIDI-B Challenge.

Conclusion: Highlights the success of the method but notes limitations and suggests future improvements for better de-identification.

Abstract: Image de-identification is essential for the public sharing of medical
images, particularly in the widely used Digital Imaging and Communications in
Medicine (DICOM) format as required by various regulations and standards,
including Health Insurance Portability and Accountability Act (HIPAA) privacy
rules, the DICOM PS3.15 standard, and best practices recommended by the Cancer
Imaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)
Challenge at the 27th International Conference on Medical Image Computing and
Computer Assisted Intervention (MICCAI 2024) was organized to evaluate
rule-based DICOM image de-identification algorithms with a large dataset of
clinical DICOM images. In this report, we explore the critical challenges of
de-identifying DICOM images, emphasize the importance of removing personally
identifiable information (PII) to protect patient privacy while ensuring the
continued utility of medical data for research, diagnostics, and treatment, and
provide a comprehensive overview of the standards and regulations that govern
this process. Additionally, we detail the de-identification methods we applied
- such as pixel masking, date shifting, date hashing, text recognition, text
replacement, and text removal - to process datasets during the test phase in
strict compliance with these standards. According to the final leaderboard of
the MIDI-B challenge, the latest version of our solution algorithm correctly
executed 99.92% of the required actions and ranked 2nd out of 10 teams that
completed the challenge (from a total of 22 registered teams). Finally, we
conducted a thorough analysis of the resulting statistics and discussed the
limitations of current approaches and potential avenues for future improvement.

</details>


### [217] [Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning](https://arxiv.org/abs/2508.07539)
*Yuki Shigeyasu,Shota Harada,Akihiko Yoshizawa,Kazuhiro Terada,Naoki Nakazima,Mariyo Kurata,Hiroyuki Abe,Tetsuo Ushiku,Ryoma Bise*

Main category: cs.CV

TL;DR: The paper proposes a domain generalization method for pathological images, focusing on intra-hospital domain shifts in WSIs. It uses clustering and contrastive learning to mitigate feature gaps.


<details>
  <summary>Details</summary>
Motivation: Addressing domain shifts in WSIs (e.g., patient characteristics, tissue thickness) is challenging due to impractical multi-hospital data collection. The work focuses on intra-hospital shifts instead.

Method: Clusters WSI-level features from non-tumor regions as domains and applies a two-stage contrastive learning approach (WSI-level and patch-level) to reduce feature gaps.

Result: The method effectively minimizes domain shifts by leveraging intra-hospital variations and contrastive learning.

Conclusion: The proposed approach offers a practical solution for domain generalization in pathological images by focusing on intra-hospital shifts and using contrastive learning.

Abstract: In this paper, we address domain shifts in pathological images by focusing on
shifts within whole slide images~(WSIs), such as patient characteristics and
tissue thickness, rather than shifts between hospitals. Traditional approaches
rely on multi-hospital data, but data collection challenges often make this
impractical. Therefore, the proposed domain generalization method captures and
leverages intra-hospital domain shifts by clustering WSI-level features from
non-tumor regions and treating these clusters as domains. To mitigate domain
shift, we apply contrastive learning to reduce feature gaps between WSI pairs
from different clusters. The proposed method introduces a two-stage contrastive
learning approach WSI-level and patch-level contrastive learning to minimize
these gaps effectively.

</details>


### [218] [CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts](https://arxiv.org/abs/2508.07540)
*Junuk Cha,Jihyeon Kim*

Main category: cs.CV

TL;DR: The paper introduces CoT-Pose, a framework using chain-of-thought reasoning to generate 3D human poses from abstract prompts, addressing the gap between high-level language and detailed pose descriptions.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-pose models require low-level prompts, unlike human communication which uses abstract language. This mismatch limits real-world deployment.

Method: Proposes CoT-Pose, integrating chain-of-thought reasoning to interpret abstract prompts into poses, and a data synthesis pipeline for training triplets (abstract prompts, detailed prompts, poses).

Result: CoT-Pose effectively generates plausible and semantically aligned poses from abstract inputs.

Conclusion: The work emphasizes high-level understanding in pose generation and suggests reasoning-enhanced approaches as a promising direction.

Abstract: Recent advances in multi-modal large language models (MLLMs) and
chain-of-thought (CoT) reasoning have led to significant progress in image and
text generation tasks. However, the field of 3D human pose generation still
faces critical limitations. Most existing text-to-pose models rely heavily on
detailed (low-level) prompts that explicitly describe joint configurations. In
contrast, humans tend to communicate actions and intentions using abstract
(high-level) language. This mismatch results in a practical challenge for
deploying pose generation systems in real-world scenarios. To bridge this gap,
we introduce a novel framework that incorporates CoT reasoning into the pose
generation process, enabling the interpretation of abstract prompts into
accurate 3D human poses. We further propose a data synthesis pipeline that
automatically generates triplets of abstract prompts, detailed prompts, and
corresponding 3D poses for training process. Experimental results demonstrate
that our reasoning-enhanced model, CoT-Pose, can effectively generate plausible
and semantically aligned poses from abstract textual inputs. This work
highlights the importance of high-level understanding in pose generation and
opens new directions for reasoning-enhanced approach for human pose generation.

</details>


### [219] [Commentary Generation for Soccer Highlights](https://arxiv.org/abs/2508.07543)
*Chidaksh Ravuru*

Main category: cs.CV

TL;DR: The paper extends the MatchVoice model for soccer commentary generation on the GOAL dataset, focusing on short clips. It evaluates training configurations, hardware impacts, and window sizes, suggesting broader video-language techniques for improvement.


<details>
  <summary>Details</summary>
Motivation: The challenge of fine-grained alignment between video content and commentary in soccer highlights motivates the extension of MatchVoice.

Method: The study extends MatchVoice to the GOAL dataset, evaluates training setups, hardware constraints, and window sizes, and explores zero-shot performance.

Result: MatchVoice shows promising generalization but requires integration of broader video-language techniques for enhanced performance.

Conclusion: The work highlights the potential of MatchVoice for soccer commentary but underscores the need for further advancements in alignment techniques.

Abstract: Automated soccer commentary generation has evolved from template-based
systems to advanced neural architectures, aiming to produce real-time
descriptions of sports events. While frameworks like SoccerNet-Caption laid
foundational work, their inability to achieve fine-grained alignment between
video content and commentary remains a significant challenge. Recent efforts
such as MatchTime, with its MatchVoice model, address this issue through coarse
and fine-grained alignment techniques, achieving improved temporal
synchronization. In this paper, we extend MatchVoice to commentary generation
for soccer highlights using the GOAL dataset, which emphasizes short clips over
entire games. We conduct extensive experiments to reproduce the original
MatchTime results and evaluate our setup, highlighting the impact of different
training configurations and hardware limitations. Furthermore, we explore the
effect of varying window sizes on zero-shot performance. While MatchVoice
exhibits promising generalization capabilities, our findings suggest the need
for integrating techniques from broader video-language domains to further
enhance performance. Our code is available at
https://github.com/chidaksh/SoccerCommentary.

</details>


### [220] [Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning](https://arxiv.org/abs/2508.07548)
*Takehiro Yamane,Itaru Tsuge,Susumu Saito,Ryoma Bise*

Main category: cs.CV

TL;DR: A novel pseudo-labeling method for medical image segmentation using PU learning to select effective pseudo-labels on individual images.


<details>
  <summary>Details</summary>
Motivation: To improve medical image segmentation by leveraging pseudo-labels for learning on individual images, addressing the challenge of selecting effective labels.

Method: Uses Positive and Unlabeled Learning (PU learning) for binary classification to discriminate foreground and background regions on unlabeled images.

Result: The method effectively selects pseudo-labels for various background regions, as shown in experiments.

Conclusion: The proposed PU learning-based pseudo-labeling method is effective for medical image segmentation.

Abstract: This paper proposes a novel pseudo-labeling method for medical image
segmentation that can perform learning on ``individual images'' to select
effective pseudo-labels. We introduce Positive and Unlabeled Learning (PU
learning), which uses only positive and unlabeled data for binary
classification problems, to obtain the appropriate metric for discriminating
foreground and background regions on each unlabeled image. Our PU learning
makes us easy to select pseudo-labels for various background regions. The
experimental results show the effectiveness of our method.

</details>


### [221] [Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring](https://arxiv.org/abs/2508.07552)
*Ludan Zhang,Sihan Wang,Yuqi Dai,Shuofei Qiao,Lei He*

Main category: cs.CV

TL;DR: The paper proposes an independent evaluation method (FMCS) for feature maps in autonomous driving models, improving interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: End-to-end models lack explicit supervision for intermediate modules, reducing interpretability and evaluation feasibility.

Method: Introduces Feature Map Convergence Score (FMCS) and a Dual-Granularity Dynamic Weighted Scoring System (DG-DWSS) for feature map evaluation. Develops CLIP-FMQE-Net for real-time quality analysis.

Result: Integration improves 3D object detection by 3.89% in NDS on NuScenes dataset.

Conclusion: The method effectively enhances feature representation quality and overall model performance.

Abstract: End-to-end models are emerging as the mainstream in autonomous driving
perception and planning. However, the lack of explicit supervision signals for
intermediate functional modules leads to opaque operational mechanisms and
limited interpretability, making it challenging for traditional methods to
independently evaluate and train these modules. Pioneering in the issue, this
study builds upon the feature map-truth representation similarity-based
evaluation framework and proposes an independent evaluation method based on
Feature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted
Scoring System (DG-DWSS) is constructed, formulating a unified quantitative
metric - Feature Map Quality Score - to enable comprehensive evaluation of the
quality of feature maps generated by functional modules. A CLIP-based Feature
Map Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining
feature-truth encoders and quality score prediction heads to enable real-time
quality analysis of feature maps generated by functional modules. Experimental
results on the NuScenes dataset demonstrate that integrating our evaluation
module into the training improves 3D object detection performance, achieving a
3.89 percent gain in NDS. These results verify the effectiveness of our method
in enhancing feature representation quality and overall model performance.

</details>


### [222] [Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation](https://arxiv.org/abs/2508.07557)
*Minghao Yin,Yukang Cao,Songyou Peng,Kai Han*

Main category: cs.CV

TL;DR: Splat4D is a novel framework for generating high-quality 4D content from monocular videos, addressing challenges like temporal-spatial consistency and detail preservation. It outperforms benchmarks with its multi-view rendering and refinement techniques.


<details>
  <summary>Details</summary>
Motivation: The need for high-fidelity 4D content from monocular videos for digital humans and AR/VR applications, ensuring consistency and detail preservation.

Method: Leverages multi-view rendering, inconsistency identification, a video diffusion model, and an asymmetric U-Net for refinement.

Result: State-of-the-art performance on public benchmarks, validated in applications like text/image-conditioned 4D generation and content editing.

Conclusion: Splat4D effectively addresses 4D content generation challenges, demonstrating versatility and superior performance.

Abstract: Generating high-quality 4D content from monocular videos for applications
such as digital humans and AR/VR poses challenges in ensuring temporal and
spatial consistency, preserving intricate details, and incorporating user
guidance effectively. To overcome these challenges, we introduce Splat4D, a
novel framework enabling high-fidelity 4D content generation from a monocular
video. Splat4D achieves superior performance while maintaining faithful
spatial-temporal coherence by leveraging multi-view rendering, inconsistency
identification, a video diffusion model, and an asymmetric U-Net for
refinement. Through extensive evaluations on public benchmarks, Splat4D
consistently demonstrates state-of-the-art performance across various metrics,
underscoring the efficacy of our approach. Additionally, the versatility of
Splat4D is validated in various applications such as text/image conditioned 4D
generation, 4D human generation, and text-guided content editing, producing
coherent outcomes following user instructions.

</details>


### [223] [Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2508.07570)
*Khanh-Binh Nguyen,Phuoc-Nguyen Bui,Hyunseung Choo,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: ACE framework improves VLMs' robustness to distribution shifts by dynamically updating a cache with high-confidence samples and adaptive thresholds.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation of VLMs under distribution shifts without labeled data.

Method: Uses Adaptive Cache Enhancement (ACE) with dynamic, class-specific thresholds and iterative refinement.

Result: Achieves state-of-the-art performance on 15 benchmark datasets in out-of-distribution scenarios.

Conclusion: ACE enhances robustness and generalization of VLMs in challenging distribution shifts.

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot generalization but
suffer performance degradation under distribution shifts in downstream tasks,
particularly in the absence of labeled data. Test-Time Adaptation (TTA)
addresses this challenge by enabling online optimization of VLMs during
inference, eliminating the need for annotated data. Cache-based TTA methods
exploit historical knowledge by maintaining a dynamic memory cache of
low-entropy or high-confidence samples, promoting efficient adaptation to
out-of-distribution data. Nevertheless, these methods face two critical
challenges: (1) unreliable confidence metrics under significant distribution
shifts, resulting in error accumulation within the cache and degraded
adaptation performance; and (2) rigid decision boundaries that fail to
accommodate substantial distributional variations, leading to suboptimal
predictions. To overcome these limitations, we introduce the Adaptive Cache
Enhancement (ACE) framework, which constructs a robust cache by selectively
storing high-confidence or low-entropy image embeddings per class, guided by
dynamic, class-specific thresholds initialized from zero-shot statistics and
iteratively refined using an exponential moving average and
exploration-augmented updates. This approach enables adaptive, class-wise
decision boundaries, ensuring robust and accurate predictions across diverse
visual distributions. Extensive experiments on 15 diverse benchmark datasets
demonstrate that ACE achieves state-of-the-art performance, delivering superior
robustness and generalization compared to existing TTA methods in challenging
out-of-distribution scenarios.

</details>


### [224] [Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification](https://arxiv.org/abs/2508.07577)
*Zhaorui Tan,Tan Pan,Kaizhu Huang,Weimiao Yu,Kai Yao,Chen Jiang,Qiufeng Wang,Anh Nguyen,Xin Guo,Yuan Cheng,Xi Yang*

Main category: cs.CV

TL;DR: The paper explores LayerNorm shifts in Vision Transformers (ViTs) during fine-tuning under data scarcity and domain shifts, proposing a rescaling mechanism and cyclic framework to improve performance.


<details>
  <summary>Details</summary>
Motivation: To understand and improve LayerNorm fine-tuning dynamics in ViTs, especially under data scarcity and domain shifts.

Method: Proposes a rescaling mechanism using a scalar λ negatively correlated to FSR and a cyclic framework to align LayerNorm shifts with ideal ones.

Result: Experiments show OOD tasks yield lower FSR and higher λ, while pathological data behaves like ID settings. The framework improves LayerNorm fine-tuning.

Conclusion: The study sheds light on LayerNorm dynamics in transfer learning and offers practical strategies for fine-tuning.

Abstract: LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning
dynamics under data scarcity and domain shifts remain underexplored. This paper
shows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)
are indicative of the transitions between source and target domains; its
efficacy is contingent upon the degree to which the target training samples
accurately represent the target domain, as quantified by our proposed
Fine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet
effective rescaling mechanism using a scalar $\lambda$ that is negatively
correlated to $FSR$ to align learned LayerNorm shifts with those ideal shifts
achieved under fully representative data, combined with a cyclic framework that
further enhances the LayerNorm fine-tuning. Extensive experiments across
natural and pathological images, in both in-distribution (ID) and
out-of-distribution (OOD) settings, and various target training sample regimes
validate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher
$\lambda$ in comparison to ID cases, especially with scarce data, indicating
under-represented target training samples. Moreover, ViTFs fine-tuned on
pathological data behave more like ID settings, favoring conservative LayerNorm
updates. Our findings illuminate the underexplored dynamics of LayerNorm in
transfer learning and provide practical strategies for LayerNorm fine-tuning.

</details>


### [225] [GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm](https://arxiv.org/abs/2508.07585)
*Yu-Huan Wu,Wei Liu,Zi-Xuan Zhu,Zizhou Wang,Yong Liu,Liangli Zhen*

Main category: cs.CV

TL;DR: GAPNet is a lightweight network for salient object detection (SOD) using granularity-aware supervision and efficient feature fusion, achieving state-of-the-art performance with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Current SOD models are computationally expensive, limiting their use in real-world edge devices. GAPNet addresses this by optimizing feature utilization and semantic interpretation.

Method: GAPNet uses granularity-aware supervision for multi-scale decoder outputs, granular pyramid convolution (GPC), cross-scale attention (CSA), and a self-attention module for global information.

Result: GAPNet achieves state-of-the-art performance among lightweight SOD models for both image and video tasks.

Conclusion: GAPNet offers an efficient, lightweight solution for SOD, balancing accuracy and computational cost, with potential for real-world applications.

Abstract: Recent salient object detection (SOD) models predominantly rely on
heavyweight backbones, incurring substantial computational cost and hindering
their practical application in various real-world settings, particularly on
edge devices. This paper presents GAPNet, a lightweight network built on the
granularity-aware paradigm for both image and video SOD. We assign saliency
maps of different granularities to supervise the multi-scale decoder
side-outputs: coarse object locations for high-level outputs and fine-grained
object boundaries for low-level outputs. Specifically, our decoder is built
with granularity-aware connections which fuse high-level features of low
granularity and low-level features of high granularity, respectively. To
support these connections, we design granular pyramid convolution (GPC) and
cross-scale attention (CSA) modules for efficient fusion of low-scale and
high-scale features, respectively. On top of the encoder, a self-attention
module is built to learn global information, enabling accurate object
localization with negligible computational cost. Unlike traditional U-Net-based
approaches, our proposed method optimizes feature utilization and semantic
interpretation while applying appropriate supervision at each processing stage.
Extensive experiments show that the proposed method achieves a new
state-of-the-art performance among lightweight image and video SOD models. Code
is available at https://github.com/yuhuan-wu/GAPNet.

</details>


### [226] [Voice Pathology Detection Using Phonation](https://arxiv.org/abs/2508.07587)
*Sri Raksha Siva,Nived Suthahar,Prakash Boominathan,Uma Ranjan*

Main category: cs.CV

TL;DR: A machine learning framework using phonation data for noninvasive detection of voice pathologies, leveraging acoustic features and RNNs for classification.


<details>
  <summary>Details</summary>
Motivation: Voice disorders impact communication and quality of life, but traditional diagnostic methods are invasive and subjective. This research aims to provide a noninvasive, automated solution.

Method: Analyzes phonation data using acoustic features (MFCCs, chroma, Mel spectrograms) and RNNs (LSTM, attention mechanisms). Data augmentation and preprocessing enhance model performance.

Result: The framework effectively classifies voice samples into normal and pathological categories, demonstrating potential for early diagnosis.

Conclusion: The proposed tool offers a noninvasive, AI-driven solution for early voice pathology detection, improving healthcare outcomes.

Abstract: Voice disorders significantly affect communication and quality of life,
requiring an early and accurate diagnosis. Traditional methods like
laryngoscopy are invasive, subjective, and often inaccessible. This research
proposes a noninvasive, machine learning-based framework for detecting voice
pathologies using phonation data.
  Phonation data from the Saarbr\"ucken Voice Database are analyzed using
acoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma
features, and Mel spectrograms. Recurrent Neural Networks (RNNs), including
LSTM and attention mechanisms, classify samples into normal and pathological
categories. Data augmentation techniques, including pitch shifting and Gaussian
noise addition, enhance model generalizability, while preprocessing ensures
signal quality. Scale-based features, such as H\"older and Hurst exponents,
further capture signal irregularities and long-term dependencies.
  The proposed framework offers a noninvasive, automated diagnostic tool for
early detection of voice pathologies, supporting AI-driven healthcare, and
improving patient outcomes.

</details>


### [227] [From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users](https://arxiv.org/abs/2508.07596)
*Shahroz Tariq,Simon S. Woo,Priyanka Singh,Irena Irmalasari,Saakshi Gupta,Dev Gupta*

Main category: cs.CV

TL;DR: DF-P2E is a multimodal framework for interpretable deepfake detection, combining visual, semantic, and narrative explanations to enhance transparency and usability.


<details>
  <summary>Details</summary>
Motivation: Address the lack of interpretability in deepfake detection systems, which limits their practical use in critical sectors like forensics and journalism.

Method: Proposes DF-P2E, integrating a deepfake classifier with Grad-CAM visualizations, a visual captioning module, and a narrative refinement LLM for context-aware explanations.

Result: Achieves competitive detection performance on the DF40 benchmark while providing high-quality, human-aligned explanations.

Conclusion: DF-P2E advances trustworthy AI by unifying prediction and explanation, making deepfake detection more transparent and accessible.

Abstract: The proliferation of deepfake technologies poses urgent challenges and
serious risks to digital integrity, particularly within critical sectors such
as forensics, journalism, and the legal system. While existing detection
systems have made significant progress in classification accuracy, they
typically function as black-box models, offering limited transparency and
minimal support for human reasoning. This lack of interpretability hinders
their usability in real-world decision-making contexts, especially for
non-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to
Explanation), a novel multimodal framework that integrates visual, semantic,
and narrative layers of explanation to make deepfake detection interpretable
and accessible. The framework consists of three modular components: (1) a
deepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual
captioning module that generates natural language summaries of manipulated
regions, and (3) a narrative refinement module that uses a fine-tuned Large
Language Model (LLM) to produce context-aware, user-sensitive explanations. We
instantiate and evaluate the framework on the DF40 benchmark, the most diverse
deepfake dataset to date. Experiments demonstrate that our system achieves
competitive detection performance while providing high-quality explanations
aligned with Grad-CAM activations. By unifying prediction and explanation in a
coherent, human-aligned pipeline, this work offers a scalable approach to
interpretable deepfake detection, advancing the broader vision of trustworthy
and transparent AI systems in adversarial media environments.

</details>


### [228] [ShoulderShot: Generating Over-the-Shoulder Dialogue Videos](https://arxiv.org/abs/2508.07597)
*Yuang Zhang,Junqi Cheng,Haoyu Zhao,Jiaxi Gu,Fangyuan Zou,Zenghui Lu,Peng Shu*

Main category: cs.CV

TL;DR: ShoulderShot is a framework for generating over-the-shoulder dialogue videos, addressing challenges like character consistency and spatial continuity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Over-the-shoulder dialogue videos are crucial for visual storytelling but underexplored in video generation research, with challenges like character consistency and computational limits.

Method: Combines dual-shot generation with looping video to maintain character consistency and spatial continuity for extended dialogues.

Result: Outperforms existing methods in shot-reverse-shot layout, spatial continuity, and dialogue length flexibility.

Conclusion: ShoulderShot opens new possibilities for practical dialogue video generation, demonstrated by superior performance and available comparisons.

Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and
advertisements, providing visual variety and enhancing viewers' emotional
connection. Despite their importance, such dialogue scenes remain largely
underexplored in video generation research. The main challenges include
maintaining character consistency across different shots, creating a sense of
spatial continuity, and generating long, multi-turn dialogues within limited
computational budgets. Here, we present ShoulderShot, a framework that combines
dual-shot generation with looping video, enabling extended dialogues while
preserving character consistency. Our results demonstrate capabilities that
surpass existing methods in terms of shot-reverse-shot layout, spatial
continuity, and flexibility in dialogue length, thereby opening up new
possibilities for practical dialogue video generation. Videos and comparisons
are available at https://shouldershot.github.io.

</details>


### [229] [LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation](https://arxiv.org/abs/2508.07603)
*Wenhui Song,Hanhui Li,Jiehui Huang,Panwen Hu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang*

Main category: cs.CV

TL;DR: LaVieID is a local autoregressive video diffusion framework for identity-preserving text-to-video generation, improving spatial and temporal identity consistency.


<details>
  <summary>Details</summary>
Motivation: To address the loss of identity information in global diffusion transformers (DiTs) for text-to-video tasks.

Method: Introduces a local router for fine-grained facial structure representation and a temporal autoregressive module to refine latent tokens.

Result: Generates high-fidelity personalized videos with state-of-the-art performance.

Conclusion: LaVieID effectively preserves identity in video generation, outperforming existing methods.

Abstract: In this paper, we present LaVieID, a novel \underline{l}ocal
\underline{a}utoregressive \underline{vi}d\underline{e}o diffusion framework
designed to tackle the challenging \underline{id}entity-preserving
text-to-video task. The key idea of LaVieID is to mitigate the loss of identity
information inherent in the stochastic global generation process of diffusion
transformers (DiTs) from both spatial and temporal perspectives. Specifically,
unlike the global and unstructured modeling of facial latent states in existing
DiTs, LaVieID introduces a local router to explicitly represent latent states
by weighted combinations of fine-grained local facial structures. This
alleviates undesirable feature interference and encourages DiTs to capture
distinctive facial characteristics. Furthermore, a temporal autoregressive
module is integrated into LaVieID to refine denoised latent tokens before video
decoding. This module divides latent tokens temporally into chunks, exploiting
their long-range temporal dependencies to predict biases for rectifying tokens,
thereby significantly enhancing inter-frame identity consistency. Consequently,
LaVieID can generate high-fidelity personalized videos and achieve
state-of-the-art performance. Our code and models are available at
https://github.com/ssugarwh/LaVieID.

</details>


### [230] [X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning](https://arxiv.org/abs/2508.07607)
*Jian Ma,Xujie Zhu,Zihao Pan,Qirong Peng,Xu Guo,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: The paper introduces X2Edit, a high-quality dataset for image editing tasks, and a plug-and-play module compatible with generative models, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Address the lack of optimal open-source datasets and a compatible editing module for community-prevalent generative models.

Method: Construct the X2Edit dataset using industry-leading models, design editing instructions with VLM, and implement scoring mechanisms. Develop a task-aware MoE-LoRA training method based on FLUX.1 with contrastive learning.

Result: The model achieves competitive editing performance, and the dataset outperforms existing open-source datasets.

Conclusion: X2Edit provides a high-quality dataset and efficient module, advancing image editing capabilities in the community.

Abstract: Existing open-source datasets for arbitrary-instruction image editing remain
suboptimal, while a plug-and-play editing module compatible with
community-prevalent generative models is notably absent. In this paper, we
first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse
editing tasks, including subject-driven generation. We utilize the
industry-leading unified image generation models and expert models to construct
the data. Meanwhile, we design reasonable editing instructions with the VLM and
implement various scoring mechanisms to filter the data. As a result, we
construct 3.7 million high-quality data with balanced categories. Second, to
better integrate seamlessly with community image generation models, we design
task-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parameters
of the full model. To further improve the final performance, we utilize the
internal representations of the diffusion model and define positive/negative
samples based on image editing types to introduce contrastive learning.
Extensive experiments demonstrate that the model's editing performance is
competitive among many excellent models. Additionally, the constructed dataset
exhibits substantial advantages over existing open-source datasets. The
open-source code, checkpoints, and datasets for X2Edit can be found at the
following link: https://github.com/OPPO-Mente-Lab/X2Edit.

</details>


### [231] [An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View](https://arxiv.org/abs/2508.07618)
*Hyoung Suk Park,Kiwan Jeon*

Main category: cs.CV

TL;DR: A two-stage method using Implicit Neural Representation (INR) and iterative reconstruction reduces truncation artifacts in dental CBCT, improving image quality.


<details>
  <summary>Details</summary>
Motivation: Small detectors in dental CBCT cause truncated FOV, degrading image quality in iterative reconstruction.

Method: 1. Use INR to create a prior image covering the full head. 2. Correct projection data with this prior and perform iterative reconstruction.

Result: The approach effectively suppresses truncation artifacts, enhancing CBCT image quality.

Conclusion: The two-stage method successfully mitigates truncation issues in dental CBCT.

Abstract: In dental cone-beam computed tomography (CBCT), compact and cost-effective
system designs often use small detectors, resulting in a truncated field of
view (FOV) that does not fully encompass the patient's head. In iterative
reconstruction approaches, the discrepancy between the actual projection and
the forward projection within the truncated FOV accumulates over iterations,
leading to significant degradation in the reconstructed image quality. In this
study, we propose a two-stage approach to mitigate truncation artifacts in
dental CBCT. In the first stage, we employ Implicit Neural Representation
(INR), leveraging its superior representation power, to generate a prior image
over an extended region so that its forward projection fully covers the
patient's head. To reduce computational and memory burdens, INR reconstruction
is performed with a coarse voxel size. The forward projection of this prior
image is then used to estimate the discrepancy due to truncated FOV in the
measured projection data. In the second stage, the discrepancy-corrected
projection data is utilized in a conventional iterative reconstruction process
within the truncated region. Our numerical results demonstrate that the
proposed two-grid approach effectively suppresses truncation artifacts, leading
to improved CBCT image quality.

</details>


### [232] [SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation](https://arxiv.org/abs/2508.07621)
*Yunsung Chung,Chanho Lim,Ghassan Bidaoui,Christian Massad,Nassir Marrouche,Jihun Hamm*

Main category: cs.CV

TL;DR: SOFA is a deep-learning framework that simulates and optimizes atrial fibrillation ablation to predict recurrence risk and improve procedural outcomes.


<details>
  <summary>Details</summary>
Motivation: Atrial fibrillation ablation outcomes vary widely, and current methods struggle to evaluate and improve efficacy due to complex patient-specific factors.

Method: SOFA uses a deep-learning approach to simulate post-ablation scar formation from pre-ablation LGE-MRI and procedural parameters, predicts recurrence risk, and optimizes parameters to minimize risk.

Result: SOFA accurately synthesizes post-ablation images and reduces predicted recurrence risk by 22.18%.

Conclusion: SOFA is the first framework to combine simulation, prediction, and optimization for personalized AF ablation, offering a novel tool for improving procedural outcomes.

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with
catheter ablation procedures, but procedural outcomes are highly variable.
Evaluating and improving ablation efficacy is challenging due to the complex
interaction between patient-specific tissue and procedural factors. This paper
asks two questions: Can AF recurrence be predicted by simulating the effects of
procedural parameters? How should we ablate to reduce AF recurrence? We propose
SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel
deep-learning framework that addresses these questions. SOFA first simulates
the outcome of an ablation strategy by generating a post-ablation image
depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and
the specific procedural parameters used (e.g., ablation locations, duration,
temperature, power, and force). During this simulation, it predicts AF
recurrence risk. Critically, SOFA then introduces an optimization scheme that
refines these procedural parameters to minimize the predicted risk. Our method
leverages a multi-modal, multi-view generator that processes 2.5D
representations of the atrium. Quantitative evaluations show that SOFA
accurately synthesizes post-ablation images and that our optimization scheme
leads to a 22.18\% reduction in the model-predicted recurrence risk. To the
best of our knowledge, SOFA is the first framework to integrate the simulation
of procedural effects, recurrence prediction, and parameter optimization,
offering a novel tool for personalizing AF ablation.

</details>


### [233] [Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction](https://arxiv.org/abs/2508.07624)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: A graph-based post-processing pipeline using GNNs improves object detection by leveraging spatial relationships, achieving up to 4% mAP@50 gain.


<details>
  <summary>Details</summary>
Motivation: Current object detectors fail to use spatial consistency in static environments, leading to errors in cluttered or occluded scenes.

Method: Proposes a GNN-based pipeline to model spatial relationships, correcting detection anomalies by analyzing neighborhood context.

Result: Improves detection performance, with mAP@50 gains of up to 4% when used with standard detectors like YOLOv7 and RT-DETR.

Conclusion: Spatial reasoning enhances reliability in object detection, demonstrating the value of leveraging environmental structure.

Abstract: In many real-world applications involving static environments, the spatial
layout of objects remains consistent across instances. However,
state-of-the-art object detection models often fail to leverage this spatial
prior, resulting in inconsistent predictions, missed detections, or
misclassifications, particularly in cluttered or occluded scenes. In this work,
we propose a graph-based post-processing pipeline that explicitly models the
spatial relationships between objects to correct detection anomalies in
egocentric frames. Using a graph neural network (GNN) trained on manually
annotated data, our model identifies invalid object class labels and predicts
corrected class labels based on their neighbourhood context. We evaluate our
approach both as a standalone anomaly detection and correction framework and as
a post-processing module for standard object detectors such as YOLOv7 and
RT-DETR. Experiments demonstrate that incorporating this spatial reasoning
significantly improves detection performance, with mAP@50 gains of up to 4%.
This method highlights the potential of leveraging the environment's spatial
structure to improve reliability in object detection systems.

</details>


### [234] [A Trustworthy Method for Multimodal Emotion Recognition](https://arxiv.org/abs/2508.07625)
*Junxiao Xue,Xiaozhen Liu,Jie Wang,Xuecheng Wu,Bin Wu*

Main category: cs.CV

TL;DR: Proposes a trusted emotion recognition (TER) method using uncertainty estimation for reliable predictions, outperforming existing methods in accuracy and trusted performance.


<details>
  <summary>Details</summary>
Motivation: Existing emotion recognition methods lack reliability for noisy or out-of-distribution data, necessitating a trusted approach.

Method: TER uses uncertainty estimation to calculate prediction confidence, combines multimodal results based on confidence, and introduces new evaluation metrics (trusted precision, recall, Acc., F1).

Result: TER achieves state-of-the-art performance (82.40% Acc. on Music-video) and superior trusted F1 scores (0.7511 on IEMOCAP, 0.9035 on Music-video).

Conclusion: TER enhances reliability and robustness in emotion recognition, validated by experimental results.

Abstract: Existing emotion recognition methods mainly focus on enhancing performance by
employing complex deep models, typically resulting in significantly higher
model complexity. Although effective, it is also crucial to ensure the
reliability of the final decision, especially for noisy, corrupted and
out-of-distribution data. To this end, we propose a novel emotion recognition
method called trusted emotion recognition (TER), which utilizes uncertainty
estimation to calculate the confidence value of predictions. TER combines the
results from multiple modalities based on their confidence values to output the
trusted predictions. We also provide a new evaluation criterion to assess the
reliability of predictions. Specifically, we incorporate trusted precision and
trusted recall to determine the trusted threshold and formulate the trusted
Acc. and trusted F1 score to evaluate the model's trusted performance. The
proposed framework combines the confidence module that accordingly endows the
model with reliability and robustness against possible noise or corruption. The
extensive experimental results validate the effectiveness of our proposed
model. The TER achieves state-of-the-art performance on the Music-video,
achieving 82.40% Acc. In terms of trusted performance, TER outperforms other
methods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511
and 0.9035, respectively.

</details>


### [235] [AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning](https://arxiv.org/abs/2508.07626)
*Dejie Yang,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: AR-VRM improves robot manipulation by explicitly imitating human hand keypoints from action videos, outperforming prior methods in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limited generalization of existing methods due to insufficient robot data and mismatched web data, AR-VRM leverages human action videos for explicit action knowledge.

Method: Proposes a keypoint Vision-Language Model (VLM) pretraining to learn human hand keypoints, then fine-tunes with Analogical Reasoning (AR) to map human actions to robot tasks.

Result: Achieves leading performance on the CALVIN benchmark and excels in few-shot scenarios, showing significant improvements over previous methods.

Conclusion: Explicit imitation of human actions via keypoints is highly effective for visual robot manipulation, especially under data scarcity.

Abstract: Visual Robot Manipulation (VRM) aims to enable a robot to follow natural
language instructions based on robot states and visual observations, and
therefore requires costly multi-modal data. To compensate for the deficiency of
robot data, existing approaches have employed vision-language pretraining with
large-scale data. However, they either utilize web data that differs from
robotic tasks, or train the model in an implicit way (e.g., predicting future
frames at the pixel level), thus showing limited generalization ability under
insufficient robot data. In this paper, we propose to learn from large-scale
human action video datasets in an explicit way (i.e., imitating human actions
from hand keypoints), introducing Visual Robot Manipulation with Analogical
Reasoning (AR-VRM). To acquire action knowledge explicitly from human action
videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,
enabling the VLM to learn human action knowledge and directly predict human
hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm
in imitating the action patterns of human motions, we first retrieve human
action videos that perform similar manipulation tasks and have similar
historical observations , and then learn the Analogical Reasoning (AR) map
between human hand keypoints and robot components. Taking advantage of focusing
on action keypoints instead of irrelevant visual cues, our method achieves
leading performance on the CALVIN benchmark {and real-world experiments}. In
few-shot scenarios, our AR-VRM outperforms previous methods by large margins ,
underscoring the effectiveness of explicitly imitating human actions under data
scarcity.

</details>


### [236] [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/abs/2508.07647)
*Xiaohang Zhan,Dingming Liu*

Main category: cs.CV

TL;DR: A training-free method for precise occlusion control in image generation using volume rendering principles in latent space.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack precision in controlling occlusion relationships between objects in generated images.

Method: Leverages volume rendering principles in latent space of a pre-trained diffusion model, guided by occlusion and transmittance, without retraining.

Result: Outperforms existing methods in occlusion accuracy and enables effects like transparency, density, and light intensity adjustments.

Conclusion: The physics-grounded approach provides accurate occlusion control and versatile effects without model retraining.

Abstract: We propose a novel training-free image generation algorithm that precisely
controls the occlusion relationships between objects in an image. Existing
image generation methods typically rely on prompts to influence occlusion,
which often lack precision. While layout-to-image methods provide control over
object locations, they fail to address occlusion relationships explicitly.
Given a pre-trained image diffusion model, our method leverages volume
rendering principles to "render" the scene in latent space, guided by occlusion
relationships and the estimated transmittance of objects. This approach does
not require retraining or fine-tuning the image diffusion model, yet it enables
accurate occlusion control due to its physics-grounded foundation. In extensive
experiments, our method significantly outperforms existing approaches in terms
of occlusion accuracy. Furthermore, we demonstrate that by adjusting the
opacities of objects or concepts during rendering, our method can achieve a
variety of effects, such as altering the transparency of objects, the density
of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the
intensity of light, and the strength of lens effects, etc.

</details>


### [237] [Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels](https://arxiv.org/abs/2508.07656)
*Yimin Fu,Zhunga Liu,Dongxiu Guo,Longfei Wang*

Main category: cs.CV

TL;DR: Proposes CLSDF for SAR ATR with noisy labels, combining scattering and deep features, using GMMs for label division, and semi-supervised learning for robustness.


<details>
  <summary>Details</summary>
Motivation: High-quality SAR labels are hard to obtain, leading to noisy labels and degraded ATR performance. Existing methods focus on image data, not SAR's unique challenges.

Method: Uses multi-model fusion of scattering and deep features, GMMs for label division, and semi-supervised learning with joint distribution alignment.

Result: Achieves state-of-the-art performance on MSTAR dataset under various noisy label conditions.

Conclusion: CLSDF effectively addresses SAR ATR's noisy label problem by integrating physical and deep features, enhancing robustness.

Abstract: The acquisition of high-quality labeled synthetic aperture radar (SAR) data
is challenging due to the demanding requirement for expert knowledge.
Consequently, the presence of unreliable noisy labels is unavoidable, which
results in performance degradation of SAR automatic target recognition (ATR).
Existing research on learning with noisy labels mainly focuses on image data.
However, the non-intuitive visual characteristics of SAR data are insufficient
to achieve noise-robust learning. To address this problem, we propose
collaborative learning of scattering and deep features (CLSDF) for SAR ATR with
noisy labels. Specifically, a multi-model feature fusion framework is designed
to integrate scattering and deep features. The attributed scattering centers
(ASCs) are treated as dynamic graph structure data, and the extracted physical
characteristics effectively enrich the representation of deep image features.
Then, the samples with clean and noisy labels are divided by modeling the loss
distribution with multiple class-wise Gaussian Mixture Models (GMMs).
Afterward, the semi-supervised learning of two divergent branches is conducted
based on the data divided by each other. Moreover, a joint distribution
alignment strategy is introduced to enhance the reliability of co-guessed
labels. Extensive experiments have been done on the Moving and Stationary
Target Acquisition and Recognition (MSTAR) dataset, and the results show that
the proposed method can achieve state-of-the-art performance under different
operating conditions with various label noises.

</details>


### [238] [Undress to Redress: A Training-Free Framework for Virtual Try-On](https://arxiv.org/abs/2508.07680)
*Zhiying Li,Junhao Wu,Yeying Jin,Daiheng Gao,Yun Ji,Kaichuan Kong,Lei Yu,Hao Xu,Kai Chen,Bruce Gu,Nana Wang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: UR-VTON improves virtual try-on for long-sleeve-to-short-sleeve conversions by introducing an 'undress-to-redress' mechanism and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods struggle with long-sleeve-to-short-sleeve conversions due to inaccurate skin restoration, limiting realism.

Method: UR-VTON uses a training-free 'undress-to-redress' approach, Dynamic Classifier-Free Guidance, and Structural Refiner for better detail fidelity.

Result: UR-VTON achieves superior detail preservation and image quality compared to state-of-the-art methods.

Conclusion: UR-VTON effectively addresses the limitations of current VTON methods, offering a practical solution for realistic garment previews.

Abstract: Virtual try-on (VTON) is a crucial task for enhancing user experience in
online shopping by generating realistic garment previews on personal photos.
Although existing methods have achieved impressive results, they struggle with
long-sleeve-to-short-sleeve conversions-a common and practical scenario-often
producing unrealistic outputs when exposed skin is underrepresented in the
original image. We argue that this challenge arises from the ''majority''
completion rule in current VTON models, which leads to inaccurate skin
restoration in such cases. To address this, we propose UR-VTON (Undress-Redress
Virtual Try-ON), a novel, training-free framework that can be seamlessly
integrated with any existing VTON method. UR-VTON introduces an
''undress-to-redress'' mechanism: it first reveals the user's torso by
virtually ''undressing,'' then applies the target short-sleeve garment,
effectively decomposing the conversion into two more manageable steps.
Additionally, we incorporate Dynamic Classifier-Free Guidance scheduling to
balance diversity and image quality during DDPM sampling, and employ Structural
Refiner to enhance detail fidelity using high-frequency cues. Finally, we
present LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.
Extensive experiments demonstrate that UR-VTON outperforms state-of-the-art
methods in both detail preservation and image quality. Code will be released
upon acceptance.

</details>


### [239] [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683)
*Chaohong Guo,Xun Mo,Yongwei Nie,Xuemiao Xu,Chao Xu,Fei Yu,Chengjiang Long*

Main category: cs.CV

TL;DR: TAR-TVG introduces timestamp anchors in reasoning for TVG, ensuring explicit supervision and progressive accuracy in temporal predictions. A three-stage training strategy improves anchor generation and reasoning quality, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches lack explicit constraints on reasoning quality for TVG, leading to suboptimal temporal predictions.

Method: Proposes TAR-TVG with timestamp anchors for intermediate supervision and a three-stage training strategy (GRPO, SFT, GRPO) to enhance anchor generation.

Result: Achieves state-of-the-art performance with interpretable, verifiable reasoning chains and refined temporal estimations.

Conclusion: TAR-TVG effectively improves TVG by enforcing explicit reasoning constraints and progressive accuracy, validated by superior experimental results.

Abstract: Temporal Video Grounding (TVG) aims to precisely localize video segments
corresponding to natural language queries, which is a critical capability for
long-form video understanding. Although existing reinforcement learning
approaches encourage models to generate reasoning chains before predictions,
they fail to explicitly constrain the reasoning process to ensure the quality
of the final temporal predictions. To address this limitation, we propose
Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),
a novel framework that introduces timestamp anchors within the reasoning
process to enforce explicit supervision to the thought content. These anchors
serve as intermediate verification points. More importantly, we require each
reasoning step to produce increasingly accurate temporal estimations, thereby
ensuring that the reasoning process contributes meaningfully to the final
prediction. To address the challenge of low-probability anchor generation in
models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation
training strategy: (1) initial GRPO training to collect 30K high-quality
reasoning traces containing multiple timestamp anchors, (2) supervised
fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the
SFT-enhanced model. This three-stage training strategy enables robust anchor
generation while maintaining reasoning quality. Experiments show that our model
achieves state-of-the-art performance while producing interpretable, verifiable
reasoning chains with progressively refined temporal estimations.

</details>


### [240] [Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing](https://arxiv.org/abs/2508.07700)
*Weitao Wang,Haoran Xu,Jun Meng,Haoqian Wang*

Main category: cs.CV

TL;DR: A tuning-free, plug-and-play method for aligning edited 3D assets with original geometry, improving multi-view consistency and mesh quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in 3D editing tools by preserving geometry while enhancing color, style, and lighting, avoiding information loss from 2D editing methods.

Method: Uses a geometry preservation module and injection switcher to guide multi-view generation with original normal latents, controlling supervision extent for alignment.

Result: Consistently improves multi-view consistency and mesh quality across various diffusion models and editing methods.

Conclusion: The proposed scheme effectively enhances 3D asset editing without compromising geometry, offering a practical solution for personalized content generation.

Abstract: As 3D generation techniques continue to flourish, the demand for generating
personalized content is rapidly rising. Users increasingly seek to apply
various editing methods to polish generated 3D content, aiming to enhance its
color, style, and lighting without compromising the underlying geometry.
However, most existing editing tools focus on the 2D domain, and directly
feeding their results into 3D generation methods (like multi-view diffusion
models) will introduce information loss, degrading the quality of the final 3D
assets. In this paper, we propose a tuning-free, plug-and-play scheme that
aligns edited assets with their original geometry in a single inference run.
Central to our approach is a geometry preservation module that guides the
edited multi-view generation with original input normal latents. Besides, an
injection switcher is proposed to deliberately control the supervision extent
of the original normals, ensuring the alignment between the edited color and
normal views. Extensive experiments show that our method consistently improves
both the multi-view consistency and mesh quality of edited 3D assets, across
multiple combinations of multi-view diffusion models and editing methods.

</details>


### [241] [Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2508.07701)
*Bo Jia,Yanan Guo,Ying Chang,Benkui Zhang,Ying Xie,Kangning Du,Lin Cao*

Main category: cs.CV

TL;DR: The paper introduces a multi-view normal and distance-guided Gaussian splatting method to address biases in 3D Gaussian Splatting (3DGS) by unifying geometric depth and aligning 3D normals, improving reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: Biases in 3DGS arise when Gaussian normal vectors align within single-view projection planes, causing inconsistencies in nearby views. The goal is to enhance multi-view scene reconstruction.

Method: The method includes a multi-view distance reprojection regularization module for Gaussian alignment and a multi-view normal enhancement module for consistency across views.

Result: The approach outperforms baselines in quantitative and qualitative evaluations, significantly improving 3DGS's surface reconstruction.

Conclusion: The proposed method effectively addresses multi-view challenges in 3DGS, achieving high-accuracy reconstruction for small indoor and outdoor scenes.

Abstract: 3D Gaussian Splatting (3DGS) achieves remarkable results in the field of
surface reconstruction. However, when Gaussian normal vectors are aligned
within the single-view projection plane, while the geometry appears reasonable
in the current view, biases may emerge upon switching to nearby views. To
address the distance and global matching challenges in multi-view scenes, we
design multi-view normal and distance-guided Gaussian splatting. This method
achieves geometric depth unification and high-accuracy reconstruction by
constraining nearby depth maps and aligning 3D normals. Specifically, for the
reconstruction of small indoor and outdoor scenes, we propose a multi-view
distance reprojection regularization module that achieves multi-view Gaussian
alignment by computing the distance loss between two nearby views and the same
Gaussian surface. Additionally, we develop a multi-view normal enhancement
module, which ensures consistency across views by matching the normals of pixel
points in nearby views and calculating the loss. Extensive experimental results
demonstrate that our method outperforms the baseline in both quantitative and
qualitative evaluations, significantly enhancing the surface reconstruction
capability of 3DGS.

</details>


### [242] [DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models](https://arxiv.org/abs/2508.07714)
*Licheng Zhang,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: A semi-automated pipeline combines deep object detection and LLMs to create a multi-class door detection dataset for floor plans, reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: Accurate door detection in floor plans is crucial for applications like compliance checking and indoor scene understanding, but lacks public datasets.

Method: Uses a deep object detector for unified door detection, an LLM for classification, and human-in-the-loop for quality assurance.

Result: Produces a high-quality dataset with minimal manual effort, suitable for benchmarking neural models.

Conclusion: Combining deep learning and multimodal reasoning efficiently constructs datasets for complex domains.

Abstract: Accurate detection and classification of diverse door types in floor plans
drawings is critical for multiple applications, such as building compliance
checking, and indoor scene understanding. Despite their importance, publicly
available datasets specifically designed for fine-grained multi-class door
detection remain scarce. In this work, we present a semi-automated pipeline
that leverages a state-of-the-art object detector and a large language model
(LLM) to construct a multi-class door detection dataset with minimal manual
effort. Doors are first detected as a unified category using a deep object
detection model. Next, an LLM classifies each detected instance based on its
visual and contextual features. Finally, a human-in-the-loop stage ensures
high-quality labels and bounding boxes. Our method significantly reduces
annotation cost while producing a dataset suitable for benchmarking neural
models in floor plan analysis. This work demonstrates the potential of
combining deep learning and multimodal reasoning for efficient dataset
construction in complex real-world domains.

</details>


### [243] [A Registration-Based Star-Shape Segmentation Model and Fast Algorithms](https://arxiv.org/abs/2508.07721)
*Daoping Zhang,Xue-Cheng Tai,Lok Ming Lui*

Main category: cs.CV

TL;DR: Proposes a star-shape segmentation model using a registration framework and level set representation, enabling full/partial segmentation with single/multiple centers and landmark constraints.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation is challenging in corrupted images due to occlusions, obscurities, or noise. Star-shape priors are explored to address this.

Method: Combines level set representation with registration framework, imposes constraints on deformed level set function, and uses the alternating direction method of multipliers.

Result: Demonstrates efficacy in achieving accurate star-shape segmentation on synthetic and real images.

Conclusion: The proposed model effectively handles star-shape segmentation with flexibility for full/partial shapes and landmark constraints.

Abstract: Image segmentation plays a crucial role in extracting objects of interest and
identifying their boundaries within an image. However, accurate segmentation
becomes challenging when dealing with occlusions, obscurities, or noise in
corrupted images. To tackle this challenge, prior information is often
utilized, with recent attention on star-shape priors. In this paper, we propose
a star-shape segmentation model based on the registration framework. By
combining the level set representation with the registration framework and
imposing constraints on the deformed level set function, our model enables both
full and partial star-shape segmentation, accommodating single or multiple
centers. Additionally, our approach allows for the enforcement of identified
boundaries to pass through specified landmark locations. We tackle the proposed
models using the alternating direction method of multipliers. Through numerical
experiments conducted on synthetic and real images, we demonstrate the efficacy
of our approach in achieving accurate star-shape segmentation.

</details>


### [244] [Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting](https://arxiv.org/abs/2508.07723)
*Ting Xiang,Changjian Chen,Zhuo Tang,Qifeng Zhang,Fei Lyu,Li Yang,Jiapeng Zhang,Kenli Li*

Main category: cs.CV

TL;DR: The paper introduces TriReWeight, a triplet-connection-based sample re-weighting method to improve generative data augmentation by reducing noise in generated images, outperforming SOTA methods by 7.9% on natural image datasets and 3.4% on medical datasets.


<details>
  <summary>Details</summary>
Motivation: The scarcity of images in real-world applications like medical diagnosis limits computer vision model performance. Generative models can expand datasets but may produce noisy images due to uncontrollable generation and ambiguous natural language.

Method: The authors analyze three types of supervision for generated images and develop TriReWeight, a re-weighting method that assigns low weights to noisy images. It integrates with any generative data augmentation method without performance degradation.

Result: TriReWeight achieves superior performance, with theoretical generalization approaching optimality (O(√(dln(n)/n)). Experiments show it outperforms SOTA methods by 7.9% on natural image datasets and 3.4% on medical datasets.

Conclusion: TriReWeight effectively enhances generative data augmentation, validated by theoretical analysis and experiments, making it a robust solution for noisy image generation in limited-data scenarios.

Abstract: The performance of computer vision models in certain real-world applications,
such as medical diagnosis, is often limited by the scarcity of available
images. Expanding datasets using pre-trained generative models is an effective
solution. However, due to the uncontrollable generation process and the
ambiguity of natural language, noisy images may be generated. Re-weighting is
an effective way to address this issue by assigning low weights to such noisy
images. We first theoretically analyze three types of supervision for the
generated images. Based on the theoretical analysis, we develop TriReWeight, a
triplet-connection-based sample re-weighting method to enhance generative data
augmentation. Theoretically, TriReWeight can be integrated with any generative
data augmentation methods and never downgrade their performance. Moreover, its
generalization approaches the optimal in the order $O(\sqrt{d\ln (n)/n})$. Our
experiments validate the correctness of the theoretical analysis and
demonstrate that our method outperforms the existing SOTA methods by $7.9\%$ on
average over six natural image datasets and by $3.4\%$ on average over three
medical datasets. We also experimentally validate that our method can enhance
the performance of different generative data augmentation methods.

</details>


### [245] [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/abs/2508.07747)
*Junhyuk So,Juncheol Shin,Hyunho Kook,Eunhyeok Park*

Main category: cs.CV

TL;DR: Grouped Speculative Decoding (GSD) accelerates autoregressive (AR) image models by 3.7x without additional training, addressing the redundancy and diversity of image tokens.


<details>
  <summary>Details</summary>
Motivation: AR image models suffer from slow inference due to their sequential nature, and existing speculative decoding methods either offer limited speedup or require extra training.

Method: GSD introduces a dynamic clustering strategy for speculative decoding, evaluating groups of visually valid tokens instead of a single target token.

Result: GSD achieves an average 3.7x speedup in AR image generation while maintaining image quality.

Conclusion: GSD is a practical, training-free solution for accelerating AR image models by leveraging token redundancy and diversity.

Abstract: Recently, autoregressive (AR) image models have demonstrated remarkable
generative capabilities, positioning themselves as a compelling alternative to
diffusion models. However, their sequential nature leads to long inference
times, limiting their practical scalability. In this work, we introduce Grouped
Speculative Decoding (GSD), a novel, training-free acceleration method for AR
image models. While recent studies have explored Speculative Decoding (SD) as a
means to speed up AR image generation, existing approaches either provide only
modest acceleration or require additional training. Our in-depth analysis
reveals a fundamental difference between language and image tokens: image
tokens exhibit inherent redundancy and diversity, meaning multiple tokens can
convey valid semantics. However, traditional SD methods are designed to accept
only a single most-likely token, which fails to leverage this difference,
leading to excessive false-negative rejections. To address this, we propose a
new SD strategy that evaluates clusters of visually valid tokens rather than
relying on a single target token. Additionally, we observe that static
clustering based on embedding distance is ineffective, which motivates our
dynamic GSD approach. Extensive experiments show that GSD accelerates AR image
models by an average of 3.7x while preserving image quality-all without
requiring any additional training. The source code is available at
https://github.com/junhyukso/GSD

</details>


### [246] [Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion](https://arxiv.org/abs/2508.07755)
*Minseo Kim,Minchan Kwon,Dongyeun Lee,Yunho Jeon,Junmo Kim*

Main category: cs.CV

TL;DR: A novel method, Contrastive Inversion, extracts common concepts from small image sets without relying on additional guidance, improving generation quality and outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: The need for customized image generation requires techniques to extract common concepts from small image sets without manual guidance, which can degrade quality.

Method: Proposes Contrastive Inversion, using contrastive learning to train target and auxiliary tokens, followed by disentangled cross-attention fine-tuning for better concept fidelity.

Result: Achieves balanced, high-level performance in concept representation and editing, outperforming existing methods.

Conclusion: Contrastive Inversion effectively identifies and represents common concepts without additional guidance, enhancing generation quality.

Abstract: The recent demand for customized image generation raises a need for
techniques that effectively extract the common concept from small sets of
images. Existing methods typically rely on additional guidance, such as text
prompts or spatial masks, to capture the common target concept. Unfortunately,
relying on manually provided guidance can lead to incomplete separation of
auxiliary features, which degrades generation quality.In this paper, we propose
Contrastive Inversion, a novel approach that identifies the common concept by
comparing the input images without relying on additional information. We train
the target token along with the image-wise auxiliary text tokens via
contrastive learning, which extracts the well-disentangled true semantics of
the target. Then we apply disentangled cross-attention fine-tuning to improve
concept fidelity without overfitting. Experimental results and analysis
demonstrate that our method achieves a balanced, high-level performance in both
concept representation and editing, outperforming existing techniques.

</details>


### [247] [Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild](https://arxiv.org/abs/2508.07759)
*Haoran Wang,Zekun Li,Jian Zhang,Lei Qi,Yinghuan Shi*

Main category: cs.CV

TL;DR: CAV-SAM adapts SAM2 for downstream tasks by treating reference-target pairs as pseudo videos, improving segmentation performance by over 5%.


<details>
  <summary>Details</summary>
Motivation: Existing reference segmentation methods rely on costly meta-learning; CAV-SAM offers a lightweight alternative.

Method: Represents reference-target pairs as pseudo videos, using DBST for semantic transition and TTGA for geometric alignment.

Result: Achieves over 5% improvement in segmentation performance compared to SOTA methods.

Conclusion: CAV-SAM provides an efficient, lightweight solution for adapting SAM2 to downstream tasks.

Abstract: Large vision models like the Segment Anything Model (SAM) exhibit significant
limitations when applied to downstream tasks in the wild. Consequently,
reference segmentation, which leverages reference images and their
corresponding masks to impart novel knowledge to the model, emerges as a
promising new direction for adapting vision models. However, existing reference
segmentation approaches predominantly rely on meta-learning, which still
necessitates an extensive meta-training process and brings massive data and
computational cost. In this study, we propose a novel approach by representing
the inherent correspondence between reference-target image pairs as a pseudo
video. This perspective allows the latest version of SAM, known as SAM2, which
is equipped with interactive video object segmentation (iVOS) capabilities, to
be adapted to downstream tasks in a lightweight manner. We term this approach
Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:
the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model
to construct a semantic transformation sequence, while the Test-Time Geometric
Alignment (TTGA) module aligns the geometric changes within this sequence
through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,
achieving segmentation performance improvements exceeding 5% over SOTA methods.
Implementation is provided in the supplementary materials.

</details>


### [248] [UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models](https://arxiv.org/abs/2508.07766)
*Jinke Li,Jiarui Yu,Chenxing Wei,Hande Dong,Qiang Lin,Liangjing Yang,Zhicai Wang,Yanbin Hao*

Main category: cs.CV

TL;DR: The paper introduces UniSVG, a dataset for training Multi-modal Large Language Models (MLLMs) to understand and generate SVG, addressing challenges in precision and multi-modal processing.


<details>
  <summary>Details</summary>
Motivation: AI-driven SVG understanding and generation face challenges due to high precision demands and diverse conditional constraints. MLLMs show potential for these tasks.

Method: Proposes UniSVG, a 525k-item dataset for MLLM training, enabling unified SVG generation and understanding.

Result: Training on UniSVG improves MLLM performance, outperforming SOTA models like GPT-4V.

Conclusion: UniSVG unlocks MLLM capabilities for SVG tasks, providing a comprehensive dataset and tools for future research.

Abstract: Unlike bitmap images, scalable vector graphics (SVG) maintain quality when
scaled, frequently employed in computer vision and artistic design in the
representation of SVG code. In this era of proliferating AI-powered systems,
enabling AI to understand and generate SVG has become increasingly urgent.
However, AI-driven SVG understanding and generation (U&G) remain significant
challenges. SVG code, equivalent to a set of curves and lines controlled by
floating-point parameters, demands high precision in SVG U&G. Besides, SVG
generation operates under diverse conditional constraints, including textual
prompts and visual references, which requires powerful multi-modal processing
for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal
Large Language Models (MLLMs) have demonstrated capabilities to process
multi-modal inputs and generate complex vector controlling parameters,
suggesting the potential to address SVG U&G tasks within a unified model. To
unlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset
called UniSVG, comprising 525k data items, tailored for MLLM training and
evaluation. To our best knowledge, it is the first comprehensive dataset
designed for unified SVG generation (from textual prompts and images) and SVG
understanding (color, category, usage, etc.). As expected, learning on the
proposed dataset boosts open-source MLLMs' performance on various SVG U&G
tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,
benchmark, weights, codes and experiment details on
https://ryanlijinke.github.io/.

</details>


### [249] [Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation](https://arxiv.org/abs/2508.07769)
*Xiaoyan Liu,Kangrui Li,Jiaxin Liu*

Main category: cs.CV

TL;DR: Dream4D introduces a novel framework for synthesizing spatiotemporally coherent 4D content by combining controllable video generation and neural 4D reconstruction, outperforming existing methods in quality.


<details>
  <summary>Details</summary>
Motivation: Current approaches struggle with maintaining view consistency and handling complex dynamics in large-scale environments, necessitating a more robust solution.

Method: Dream4D uses a two-stage architecture: predicting camera trajectories from a single image, then generating multi-view sequences via a pose-conditioned diffusion process, and converting them into a persistent 4D representation.

Result: The framework leverages temporal priors and geometric awareness, achieving higher quality (e.g., mPSNR, mSSIM) compared to existing methods.

Conclusion: Dream4D successfully bridges the gap in 4D content synthesis, offering improved consistency and quality in complex scenes.

Abstract: The synthesis of spatiotemporally coherent 4D content presents fundamental
challenges in computer vision, requiring simultaneous modeling of high-fidelity
spatial representations and physically plausible temporal dynamics. Current
approaches often struggle to maintain view consistency while handling complex
scene dynamics, particularly in large-scale environments with multiple
interacting elements. This work introduces Dream4D, a novel framework that
bridges this gap through a synergy of controllable video generation and neural
4D reconstruction. Our approach seamlessly combines a two-stage architecture:
it first predicts optimal camera trajectories from a single image using
few-shot learning, then generates geometrically consistent multi-view sequences
via a specialized pose-conditioned diffusion process, which are finally
converted into a persistent 4D representation. This framework is the first to
leverage both rich temporal priors from video diffusion models and geometric
awareness of the reconstruction models, which significantly facilitates 4D
generation and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.

</details>


### [250] [Prototype-Guided Curriculum Learning for Zero-Shot Learning](https://arxiv.org/abs/2508.07771)
*Lei Wang,Shiming Chen,Guo-Sen Xie,Ziming Hong,Chaojian Yu,Qinmu Peng,Xinge You*

Main category: cs.CV

TL;DR: The paper proposes CLZSL, a prototype-guided curriculum learning framework for Zero-Shot Learning (ZSL), addressing instance-level mismatches and class-level imprecision in semantic prototypes to improve knowledge transfer to unseen classes.


<details>
  <summary>Details</summary>
Motivation: Existing ZSL methods rely on manually defined semantic prototypes, which introduce noisy supervision due to instance-level mismatches and class-level imprecision, hindering effective knowledge transfer.

Method: CLZSL includes a Prototype-Guided Curriculum Learning (PCL) module to prioritize well-aligned samples and a Prototype Update (PUP) module to dynamically refine class-level prototypes.

Result: Experiments on AWA2, SUN, and CUB datasets demonstrate the framework's effectiveness in improving visual-semantic mapping.

Conclusion: CLZSL successfully mitigates noisy supervision in ZSL, enhancing knowledge transfer to unseen classes through curriculum learning and dynamic prototype updates.

Abstract: In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge
transfer from seen to unseen classes by learning a visual-semantic mapping from
seen-class images to class-level semantic prototypes (e.g., attributes).
However, these semantic prototypes are manually defined and may introduce noisy
supervision for two main reasons: (i) instance-level mismatch: variations in
perspective, occlusion, and annotation bias will cause discrepancies between
individual sample and the class-level semantic prototypes; and (ii) class-level
imprecision: the manually defined semantic prototypes may not accurately
reflect the true semantics of the class. Consequently, the visual-semantic
mapping will be misled, reducing the effectiveness of knowledge transfer to
unseen classes. In this work, we propose a prototype-guided curriculum learning
framework (dubbed as CLZSL), which mitigates instance-level mismatches through
a Prototype-Guided Curriculum Learning (PCL) module and addresses class-level
imprecision via a Prototype Update (PUP) module. Specifically, the PCL module
prioritizes samples with high cosine similarity between their visual mappings
and the class-level semantic prototypes, and progressively advances to
less-aligned samples, thereby reducing the interference of instance-level
mismatches to achieve accurate visual-semantic mapping. Besides, the PUP module
dynamically updates the class-level semantic prototypes by leveraging the
visual mappings learned from instances, thereby reducing class-level
imprecision and further improving the visual-semantic mapping. Experiments were
conducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the
effectiveness of our method.

</details>


### [251] [Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)](https://arxiv.org/abs/2508.07775)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: The paper proposes a method for modeling 3D rotation trajectories using Neural Controlled Differential Equations and $SO(3)$ Savitzky-Golay paths, addressing challenges like unknown dynamics and noisy observations.


<details>
  <summary>Details</summary>
Motivation: Challenges in $SO(3)$ extrapolation include unknown dynamics, non-conservative forces, and noisy observations, limiting existing methods' applicability.

Method: Uses Neural Controlled Differential Equations guided by $SO(3)$ Savitzky-Golay paths to model trajectories robustly, without relying on energy or momentum conservation.

Result: The method generalizes well to unknown physical parameters and noisy inputs, achieving robust extrapolation in simulations and real-world settings.

Conclusion: The approach is versatile, integrable into existing pipelines, and effective for complex, non-inertial systems.

Abstract: Modeling the rotation of moving objects is a fundamental task in computer
vision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)
unknown quantities such as the moment of inertia complicate dynamics, (2) the
presence of external forces and torques can lead to non-conservative
kinematics, and (3) estimating evolving state trajectories under sparse, noisy
observations requires robustness. We propose modeling trajectories of noisy
pose estimates on the manifold of 3D rotations in a physically and
geometrically meaningful way by leveraging Neural Controlled Differential
Equations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation
methods often rely on energy conservation or constant velocity assumptions,
limiting their applicability in real-world scenarios involving non-conservative
forces. In contrast, our approach is agnostic to energy and momentum
conservation while being robust to input noise, making it applicable to
complex, non-inertial systems. Our approach is easily integrated as a module in
existing pipelines and generalizes well to trajectories with unknown physical
parameters. By learning to approximate object dynamics from noisy states during
training, our model attains robust extrapolation capabilities in simulation and
various real-world settings. Code is available at
https://github.com/bastianlb/forecasting-rotational-dynamics

</details>


### [252] [GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences](https://arxiv.org/abs/2508.07782)
*Saihui Hou,Chenye Wang,Wenpeng Lang,Zhengxiang Lan,Yongzhen Huang*

Main category: cs.CV

TL;DR: The paper proposes a new gait recognition method by treating gait as individualized actions (snippets), addressing limitations of set-based and sequence-based approaches.


<details>
  <summary>Details</summary>
Motivation: Overcome the shortcomings of existing gait recognition methods, which either ignore short-range temporal context (set-based) or fail to capture long-range dependencies (sequence-based).

Method: Introduces snippets (randomly selected frame series from segments) for multi-scale temporal context. Key components are Snippet Sampling and Snippet Modeling.

Result: Achieves 77.5% rank-1 accuracy on Gait3D and 81.7% on GREW using a 2D convolution-based backbone.

Conclusion: The snippet-based approach effectively enhances gait recognition by incorporating multi-scale temporal context, demonstrating its potential.

Abstract: Recent advancements in gait recognition have significantly enhanced
performance by treating silhouettes as either an unordered set or an ordered
sequence. However, both set-based and sequence-based approaches exhibit notable
limitations. Specifically, set-based methods tend to overlook short-range
temporal context for individual frames, while sequence-based methods struggle
to capture long-range temporal dependencies effectively. To address these
challenges, we draw inspiration from human identification and propose a new
perspective that conceptualizes human gait as a composition of individualized
actions. Each action is represented by a series of frames, randomly selected
from a continuous segment of the sequence, which we term a snippet.
Fundamentally, the collection of snippets for a given sequence enables the
incorporation of multi-scale temporal context, facilitating more comprehensive
gait feature learning. Moreover, we introduce a non-trivial solution for
snippet-based gait recognition, focusing on Snippet Sampling and Snippet
Modeling as key components. Extensive experiments on four widely-used gait
datasets validate the effectiveness of our proposed approach and, more
importantly, highlight the potential of gait snippets. For instance, our method
achieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D
convolution-based backbone.

</details>


### [253] [Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake](https://arxiv.org/abs/2508.07795)
*Hongrui Zheng,Yuezun Li,Liejun Wang,Yunfeng Diao,Zhiqing Guo*

Main category: cs.CV

TL;DR: The paper proposes a Two-Stage Defense Framework (TSDF) to counter deepfake threats by combining distortion of forged content and disrupting attackers' retraining pipelines.


<details>
  <summary>Details</summary>
Motivation: Current active defenses against deepfakes lack persistence, as attackers can bypass them by retraining models on protected samples.

Method: TSDF uses dual-function adversarial perturbations to distort forged content and poison attackers' data sources, preventing model adaptation.

Result: TSDF demonstrates strong dual defense capability, outperforming traditional methods under adversarial retraining.

Conclusion: TSDF enhances the persistence of active defenses, ensuring long-term effectiveness against deepfake attacks.

Abstract: Active defense strategies have been developed to counter the threat of
deepfake technology. However, a primary challenge is their lack of persistence,
as their effectiveness is often short-lived. Attackers can bypass these
defenses by simply collecting protected samples and retraining their models.
This means that static defenses inevitably fail when attackers retrain their
models, which severely limits practical use. We argue that an effective defense
not only distorts forged content but also blocks the model's ability to adapt,
which occurs when attackers retrain their models on protected images. To
achieve this, we propose an innovative Two-Stage Defense Framework (TSDF).
Benefiting from the intensity separation mechanism designed in this paper, the
framework uses dual-function adversarial perturbations to perform two roles.
First, it can directly distort the forged results. Second, it acts as a
poisoning vehicle that disrupts the data preparation process essential for an
attacker's retraining pipeline. By poisoning the data source, TSDF aims to
prevent the attacker's model from adapting to the defensive perturbations, thus
ensuring the defense remains effective long-term. Comprehensive experiments
show that the performance of traditional interruption methods degrades sharply
when it is subjected to adversarial retraining. However, our framework shows a
strong dual defense capability, which can improve the persistence of active
defense. Our code will be available at https://github.com/vpsg-research/TSDF.

</details>


### [254] [Power Battery Detection](https://arxiv.org/abs/2508.07797)
*Xiaoqi Zhao,Peiqian Cao,Lihe Zhang,Zonglei Feng,Hanqi Liu,Jiaming Zuo,Youwei Pang,Weisi Lin,Georges El Fakhri,Huchuan Lu,Xiaofeng Liu*

Main category: cs.CV

TL;DR: The paper introduces PBD5K, a benchmark for power battery detection (PBD) using X-ray images, and proposes MDCNeXt, a model for point-level segmentation to address challenges like low contrast and dense plates.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of power batteries is inefficient, and traditional vision algorithms fail due to dense plates, low contrast, and imaging artifacts.

Method: Developed PBD5K (5,000 X-ray images with annotations) and MDCNeXt, a model integrating multi-dimensional clues (point, line, count) with state space modules for better segmentation.

Result: MDCNeXt improves plate discrimination and suppresses visual interference, aided by a distance-adaptive mask generation strategy.

Conclusion: The work advances PBD with a scalable benchmark and effective model, encouraging further research in this safety-critical task.

Abstract: Power batteries are essential components in electric vehicles, where internal
structural defects can pose serious safety risks. We conduct a comprehensive
study on a new task, power battery detection (PBD), which aims to localize the
dense endpoints of cathode and anode plates from industrial X-ray images for
quality inspection. Manual inspection is inefficient and error-prone, while
traditional vision algorithms struggle with densely packed plates, low
contrast, scale variation, and imaging artifacts. To address this issue and
drive more attention into this meaningful task, we present PBD5K, the first
large-scale benchmark for this task, consisting of 5,000 X-ray images from nine
battery types with fine-grained annotations and eight types of real-world
visual interference. To support scalable and consistent labeling, we develop an
intelligent annotation pipeline that combines image filtering, model-assisted
pre-labeling, cross-verification, and layered quality evaluation. We formulate
PBD as a point-level segmentation problem and propose MDCNeXt, a model designed
to extract and integrate multi-dimensional structure clues including point,
line, and count information from the plate itself. To improve discrimination
between plates and suppress visual interference, MDCNeXt incorporates two state
space modules. The first is a prompt-filtered module that learns contrastive
relationships guided by task-specific prompts. The second is a density-aware
reordering module that refines segmentation in regions with high plate density.
In addition, we propose a distance-adaptive mask generation strategy to provide
robust supervision under varying spatial distributions of anode and cathode
positions. The source code and datasets will be publicly available at
\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}.

</details>


### [255] [MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks](https://arxiv.org/abs/2508.07803)
*Yushen Xu,Xiaosong Li,Zhenyu Kuang,Xiaoqi Cheng,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: MambaTrans, a multimodal fusion image modality translator, improves downstream task performance by adapting fused images to models trained on visible images, using text and mask inputs.


<details>
  <summary>Details</summary>
Motivation: Pixel distribution differences between visible and multimodal fusion images degrade downstream task performance, prompting the need for adaptation.

Method: MambaTrans uses multimodal large language model descriptions and semantic segmentation masks, combining cross-attention and a 3D-Selective Scan Module.

Result: Experiments show MambaTrans enhances multimodal image performance in downstream tasks without modifying pre-trained model parameters.

Conclusion: MambaTrans effectively bridges modality gaps, improving performance in object detection and semantic segmentation for fused images.

Abstract: The goal of multimodal image fusion is to integrate complementary information
from infrared and visible images, generating multimodal fused images for
downstream tasks. Existing downstream pre-training models are typically trained
on visible images. However, the significant pixel distribution differences
between visible and multimodal fusion images can degrade downstream task
performance, sometimes even below that of using only visible images. This paper
explores adapting multimodal fused images with significant modality differences
to object detection and semantic segmentation models trained on visible images.
To address this, we propose MambaTrans, a novel multimodal fusion image
modality translator. MambaTrans uses descriptions from a multimodal large
language model and masks from semantic segmentation models as input. Its core
component, the Multi-Model State Space Block, combines mask-image-text
cross-attention and a 3D-Selective Scan Module, enhancing pure visual
capabilities. By leveraging object detection prior knowledge, MambaTrans
minimizes detection loss during training and captures long-term dependencies
among text, masks, and images. This enables favorable results in pre-trained
models without adjusting their parameters. Experiments on public datasets show
that MambaTrans effectively improves multimodal image performance in downstream
tasks.

</details>


### [256] [Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.07804)
*Bao Li,Xiaomei Zhang,Miao Xu,Zhaoxin Fan,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: Pose-RFT, a reinforcement fine-tuning framework, improves 3D human pose generation in MLLMs by jointly optimizing discrete and continuous actions using hybrid reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing supervised objectives in pose-specific MLLMs struggle with ambiguity and task-specific alignment for accurate 3D pose generation.

Method: Proposes Pose-RFT with HyGRPO, a hybrid reinforcement learning algorithm, for joint optimization of language prediction and pose generation.

Result: Pose-RFT outperforms existing MLLMs on multiple benchmarks, showing effectiveness in spatial and semantic alignment.

Conclusion: Hybrid action reinforcement fine-tuning is effective for 3D pose generation, as demonstrated by Pose-RFT's superior performance.

Abstract: Generating 3D human poses from multimodal inputs such as images or text
requires models to capture both rich spatial and semantic correspondences.
While pose-specific multimodal large language models (MLLMs) have shown promise
in this task, they are typically trained with supervised objectives such as
SMPL parameter regression or token-level prediction, which struggle to model
the inherent ambiguity and achieve task-specific alignment required for
accurate 3D pose generation. To address these limitations, we propose Pose-RFT,
a reinforcement fine-tuning framework tailored for 3D human pose generation in
MLLMs. We formulate the task as a hybrid action reinforcement learning problem
that jointly optimizes discrete language prediction and continuous pose
generation. To this end, we introduce HyGRPO, a hybrid reinforcement learning
algorithm that performs group-wise reward normalization over sampled responses
to guide joint optimization of discrete and continuous actions. Pose-RFT
further incorporates task-specific reward functions to guide optimization
towards spatial alignment in image-to-pose generation and semantic consistency
in text-to-pose generation. Extensive experiments on multiple pose generation
benchmarks demonstrate that Pose-RFT significantly improves performance over
existing pose-specific MLLMs, validating the effectiveness of hybrid action
reinforcement fine-tuning for 3D pose generation.

</details>


### [257] [DiTVR: Zero-Shot Diffusion Transformer for Video Restoration](https://arxiv.org/abs/2508.07811)
*Sicheng Gao,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: DiTVR is a zero-shot video restoration framework using a diffusion transformer with trajectory-aware attention and a wavelet-guided sampler, achieving state-of-the-art results with superior temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods produce unrealistic details and need paired datasets, while generative diffusion models struggle with temporal consistency.

Method: Combines a diffusion transformer with trajectory-aware attention and a flow-consistent sampler, focusing on temporal dynamics and motion correspondences.

Result: DiTVR sets a new zero-shot state-of-the-art on benchmarks, excelling in temporal consistency and detail preservation.

Conclusion: DiTVR effectively addresses challenges in video restoration, offering robust performance and high-quality results.

Abstract: Video restoration aims to reconstruct high quality video sequences from low
quality inputs, addressing tasks such as super resolution, denoising, and
deblurring. Traditional regression based methods often produce unrealistic
details and require extensive paired datasets, while recent generative
diffusion models face challenges in ensuring temporal consistency. We introduce
DiTVR, a zero shot video restoration framework that couples a diffusion
transformer with trajectory aware attention and a wavelet guided, flow
consistent sampler. Unlike prior 3D convolutional or frame wise diffusion
approaches, our attention mechanism aligns tokens along optical flow
trajectories, with particular emphasis on vital layers that exhibit the highest
sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically
selects relevant tokens based on motion correspondences across frames. The flow
guided sampler injects data consistency only into low-frequency bands,
preserving high frequency priors while accelerating convergence. DiTVR
establishes a new zero shot state of the art on video restoration benchmarks,
demonstrating superior temporal consistency and detail preservation while
remaining robust to flow noise and occlusions.

</details>


### [258] [Semi-supervised Multiscale Matching for SAR-Optical Image](https://arxiv.org/abs/2508.07812)
*Jingze Gai,Changchun Li*

Main category: cs.CV

TL;DR: S2M2-SAR introduces a semi-supervised method for SAR-optical image matching, reducing reliance on labeled data by leveraging pseudo-labeling and cross-modal feature enhancement.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for SAR-optical image matching is time-consuming and complex, limiting labeled data availability.

Method: Uses pseudo-labeling for unlabeled data and a cross-modal feature enhancement module with unsupervised loss for feature disentanglement.

Result: Outperforms semi-supervised methods and competes with fully supervised SOTA methods.

Conclusion: S2M2-SAR is efficient and practical, reducing dependency on labeled data while maintaining high performance.

Abstract: Driven by the complementary nature of optical and synthetic aperture radar
(SAR) images, SAR-optical image matching has garnered significant interest.
Most existing SAR-optical image matching methods aim to capture effective
matching features by employing the supervision of pixel-level matched
correspondences within SAR-optical image pairs, which, however, suffers from
time-consuming and complex manual annotation, making it difficult to collect
sufficient labeled SAR-optical image pairs. To handle this, we design a
semi-supervised SAR-optical image matching pipeline that leverages both scarce
labeled and abundant unlabeled image pairs and propose a semi-supervised
multiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we
pseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth
similarity heatmaps by combining both deep and shallow level matching results,
and train the matching model by employing labeled and pseudo-labeled similarity
heatmaps. In addition, we introduce a cross-modal feature enhancement module
trained using a cross-modality mutual independence loss, which requires no
ground-truth labels. This unsupervised objective promotes the separation of
modality-shared and modality-specific features by encouraging statistical
independence between them, enabling effective feature disentanglement across
optical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we
compare it with existing competitors on benchmark datasets. Experimental
results demonstrate that S2M2-SAR not only surpasses existing semi-supervised
methods but also achieves performance competitive with fully supervised SOTA
methods, demonstrating its efficiency and practical potential.

</details>


### [259] [Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models](https://arxiv.org/abs/2508.07818)
*Chenyue Song,Chen Hui,Haiqi Zhu,Feng Jiang,Yachun Mi,Wei Zhang,Shaohui Liu*

Main category: cs.CV

TL;DR: RSFIQA is a fine-grained NR-IQA model that integrates region-level distortion information using SAM and MLLM, enhanced by a Region-Aware Semantic Attention mechanism, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing NR-IQA methods lack sensitivity to local quality variations or fail to focus on semantically salient regions.

Method: Uses SAM for dynamic image partitioning, MLLM for distortion perception, and RSA for attention aggregation.

Result: Achieves robust and competitive performance across multiple benchmark datasets.

Conclusion: RSFIQA effectively addresses limitations of prior methods by combining region-level insights with global attention.

Abstract: No-reference image quality assessment (NR-IQA) aims to simulate the process
of perceiving image quality aligned with subjective human perception. However,
existing NR-IQA methods either focus on global representations that leads to
limited insights into the semantically salient regions or employ a uniform
weighting for region features that weakens the sensitivity to local quality
variations. In this paper, we propose a fine-grained image quality assessment
model, named RSFIQA, which integrates region-level distortion information to
perceive multi-dimensional quality discrepancies. To enhance regional quality
awareness, we first utilize the Segment Anything Model (SAM) to dynamically
partition the input image into non-overlapping semantic regions. For each
region, we teach a powerful Multi-modal Large Language Model (MLLM) to extract
descriptive content and perceive multi-dimensional distortions, enabling a
comprehensive understanding of both local semantics and quality degradations.
To effectively leverage this information, we introduce Region-Aware Semantic
Attention (RSA) mechanism, which generates a global attention map by
aggregating fine-grained representations from local regions. In addition,
RSFIQA is backbone-agnostic and can be seamlessly integrated into various deep
neural network architectures. Extensive experiments demonstrate the robustness
and effectiveness of the proposed method, which achieves competitive quality
prediction performance across multiple benchmark datasets.

</details>


### [260] [Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP](https://arxiv.org/abs/2508.07819)
*Ke Ma,Jun Long,Hongxiao Fei,Liujie Hua,Yueyi Luo*

Main category: cs.CV

TL;DR: The paper proposes an Architectural Co-Design framework to adapt Vision-Language Models (VLMs) for Zero-Shot Anomaly Detection (ZSAD), addressing limitations in local inductive biases and feature fusion.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with ZSAD due to lack of local inductive biases and inflexible feature fusion, limiting their effectiveness in dense prediction tasks.

Method: The framework integrates Conv-LoRA for fine-grained representation and Dynamic Fusion Gateway (DFG) for adaptive cross-modal fusion.

Result: Experiments show superior accuracy and robustness on industrial and medical benchmarks.

Conclusion: The co-design approach effectively adapts foundation models to dense perception tasks like ZSAD.

Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap
when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of
local inductive biases for dense prediction and their reliance on inflexible
feature fusion paradigms. We address these limitations through an Architectural
Co-Design framework that jointly refines feature representation and cross-modal
fusion. Our method integrates a parameter-efficient Convolutional Low-Rank
Adaptation (Conv-LoRA) adapter to inject local inductive biases for
fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that
leverages visual context to adaptively modulate text prompts, enabling a
powerful bidirectional fusion. Extensive experiments on diverse industrial and
medical benchmarks demonstrate superior accuracy and robustness, validating
that this synergistic co-design is critical for robustly adapting foundation
models to dense perception tasks.

</details>


### [261] [MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization](https://arxiv.org/abs/2508.07833)
*Animesh Jain,Alexandros Stergiou*

Main category: cs.CV

TL;DR: MIMIC is a framework to visualize internal representations of Vision Language Models (VLMs) by synthesizing visual concepts, improving interpretability and trust.


<details>
  <summary>Details</summary>
Motivation: VLMs are complex and hard to interpret, limiting transparency and trust. MIMIC aims to address this by visualizing internal VLM representations.

Method: MIMIC uses joint VLM-based inversion and feature alignment, with regularizers for spatial alignment, image smoothness, and semantic realism.

Result: MIMIC successfully inverts visual concepts from VLM outputs, evaluated with visual and semantic metrics.

Conclusion: MIMIC is the first model inversion approach for visual interpretations of VLM concepts, enhancing transparency.

Abstract: Vision Language Models (VLMs) encode multimodal inputs over large, complex,
and difficult-to-interpret architectures, which limit transparency and trust.
We propose a Multimodal Inversion for Model Interpretation and
Conceptualization (MIMIC) framework to visualize the internal representations
of VLMs by synthesizing visual concepts corresponding to internal encodings.
MIMIC uses a joint VLM-based inversion and a feature alignment objective to
account for VLM's autoregressive processing. It additionally includes a triplet
of regularizers for spatial alignment, natural image smoothness, and semantic
realism. We quantitatively and qualitatively evaluate MIMIC by inverting visual
concepts over a range of varying-length free-form VLM output texts. Reported
results include both standard visual quality metrics as well as semantic
text-based metrics. To the best of our knowledge, this is the first model
inversion approach addressing visual interpretations of VLM concepts.

</details>


### [262] [Effortless Vision-Language Model Specialization in Histopathology without Annotation](https://arxiv.org/abs/2508.07835)
*Jingna Qiu,Nishanth Jain,Jonas Ammeling,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: The paper explores annotation-free adaptation of Vision-Language Models (VLMs) in histopathology by continued pretraining on domain-relevant image-caption pairs, improving zero-shot and few-shot performance without manual labeling.


<details>
  <summary>Details</summary>
Motivation: General-purpose VLMs like CONCH and QuiltNet may underperform in specific histopathology tasks, and supervised fine-tuning requires manual labels. This work aims to adapt VLMs without annotations.

Method: The approach involves continued pretraining of VLMs using domain- and task-relevant image-caption pairs from existing databases, tested on CONCH and QuiltNet across three tasks.

Result: Continued pretraining enhances zero-shot and few-shot performance, matching few-shot methods with larger training sizes while avoiding manual labeling.

Conclusion: This annotation-free, task-agnostic method is a promising way to adapt VLMs for histopathology tasks, with code publicly available.

Abstract: Recent advances in Vision-Language Models (VLMs) in histopathology, such as
CONCH and QuiltNet, have demonstrated impressive zero-shot classification
capabilities across various tasks. However, their general-purpose design may
lead to suboptimal performance in specific downstream applications. While
supervised fine-tuning methods address this issue, they require manually
labeled samples for adaptation. This paper investigates annotation-free
adaptation of VLMs through continued pretraining on domain- and task-relevant
image-caption pairs extracted from existing databases. Our experiments on two
VLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs
substantially enhance both zero-shot and few-shot performance. Notably, with
larger training sizes, continued pretraining matches the performance of
few-shot methods while eliminating manual labeling. Its effectiveness,
task-agnostic design, and annotation-free workflow make it a promising pathway
for adapting VLMs to new histopathology tasks. Code is available at
https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.

</details>


### [263] [CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving](https://arxiv.org/abs/2508.07838)
*Qi Xiang,Kunsong Shi,Zhigui Lin,Lei He*

Main category: cs.CV

TL;DR: A modular Mixture-of-Experts (CBDES MoE) architecture improves BEV perception in autonomous driving by dynamically selecting expert paths, outperforming single-expert models.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal BEV methods face limitations in adaptability, modeling capacity, and generalization.

Method: Proposes CBDES MoE, integrating heterogeneous expert networks with a Self-Attention Router for dynamic path selection.

Result: Achieves 1.6-point mAP and 4.1-point NDS improvements over single-expert baselines on nuScenes dataset.

Conclusion: CBDES MoE is effective and practical for enhancing BEV perception in autonomous driving.

Abstract: Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion
have become a fundamental cornerstone for end-to-end autonomous driving.
However, existing multi-modal BEV methods commonly suffer from limited input
adaptability, constrained modeling capacity, and suboptimal generalization. To
address these challenges, we propose a hierarchically decoupled
Mixture-of-Experts architecture at the functional module level, termed
Computing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE
integrates multiple structurally heterogeneous expert networks with a
lightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic
expert path selection and sparse, input-aware efficient inference. To the best
of our knowledge, this is the first modular Mixture-of-Experts framework
constructed at the functional module granularity within the autonomous driving
domain. Extensive evaluations on the real-world nuScenes dataset demonstrate
that CBDES MoE consistently outperforms fixed single-expert baselines in 3D
object detection. Compared to the strongest single-expert model, CBDES MoE
achieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,
demonstrating the effectiveness and practical advantages of the proposed
approach.

</details>


### [264] [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/abs/2508.07847)
*Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: Deep SWM, a deep state space model with a sparse masked autoencoder, improves solar flare prediction by handling long-range spatio-temporal dependencies and outperforms baselines and human experts.


<details>
  <summary>Details</summary>
Motivation: Accurate solar flare prediction is vital for infrastructure protection, but current methods lack representation learning or struggle with temporal dependencies.

Method: Proposes Deep SWM, using deep state space models and a two-phase masked autoencoder for pretraining, and introduces FlareBench for validation.

Result: Outperforms baseline methods and human experts in performance and reliability.

Conclusion: Deep SWM advances solar flare prediction with improved accuracy and reliability, validated by FlareBench.

Abstract: Accurate, reliable solar flare prediction is crucial for mitigating potential
disruptions to critical infrastructure, while predicting solar flares remains a
significant challenge. Existing methods based on heuristic physical features
often lack representation learning from solar images. On the other hand,
end-to-end learning approaches struggle to model long-range temporal
dependencies in solar images. In this study, we propose Deep Space Weather
Model (Deep SWM), which is based on multiple deep state space models for
handling both ten-channel solar images and long-range spatio-temporal
dependencies. Deep SWM also features a sparse masked autoencoder, a novel
pretraining strategy that employs a two-phase masking approach to preserve
crucial regions such as sunspots while compressing spatial information.
Furthermore, we built FlareBench, a new public benchmark for solar flare
prediction covering a full 11-year solar activity cycle, to validate our
method. Our method outperformed baseline methods and even human expert
performance on standard metrics in terms of performance and reliability. The
project page can be found at https://keio-smilab25.github.io/DeepSWM.

</details>


### [265] [Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs](https://arxiv.org/abs/2508.07850)
*Noriko Nitta,Rei Miyata,Naoto Oishi*

Main category: cs.CV

TL;DR: Analyzed Ge surface microstructures via graph embeddings and PCA, finding irradiation angle more impactful than fluence on morphology.


<details>
  <summary>Details</summary>
Motivation: To understand how ion beam irradiation parameters (angle and fluence) affect the morphological properties of Ge surfaces.

Method: Processed electron microscopy images into skeleton graphs, embedded them using a graph convolutional network, and analyzed with PCA and Davies-Bouldin index.

Result: Irradiation angle has a more significant impact on Ge surface morphology than irradiation fluence.

Conclusion: The study highlights the importance of irradiation angle in controlling Ge surface properties, offering insights for material engineering.

Abstract: In this paper, electron microscopy images of microstructures formed on Ge
surfaces by ion beam irradiation were processed to extract topological features
as skeleton graphs, which were then embedded using a graph convolutional
network. The resulting embeddings were analyzed using principal component
analysis, and cluster separability in the resulting PCA space was evaluated
using the Davies-Bouldin index. The results indicate that variations in
irradiation angle have a more significant impact on the morphological
properties of Ge surfaces than variations in irradiation fluence.

</details>


### [266] [Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images](https://arxiv.org/abs/2508.07851)
*Konrad Reuter,Suresh Guttikonda,Sarah Latus,Lennart Maack,Christian Betz,Tobias Maurer,Alexander Schlaefer*

Main category: cs.CV

TL;DR: A novel method for markerless 3D tissue tracking in minimally invasive surgery using 2D TAP networks, achieving accurate results with low error rates.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like dynamic tissue motion and limited field of view in minimally invasive surgery to improve safety and enable robotic assistance.

Method: Combines two CoTracker models (temporal tracking and stereo matching) to estimate 3D motion from stereo endoscopic images.

Result: Achieved Euclidean distance errors as low as 1.1 mm at 10 mm/s velocity on a chicken tissue phantom.

Conclusion: TAP-based models show promise for accurate, markerless 3D tracking in surgical scenarios.

Abstract: Minimally invasive surgery presents challenges such as dynamic tissue motion
and a limited field of view. Accurate tissue tracking has the potential to
support surgical guidance, improve safety by helping avoid damage to sensitive
structures, and enable context-aware robotic assistance during complex
procedures. In this work, we propose a novel method for markerless 3D tissue
tracking by leveraging 2D Tracking Any Point (TAP) networks. Our method
combines two CoTracker models, one for temporal tracking and one for stereo
matching, to estimate 3D motion from stereo endoscopic images. We evaluate the
system using a clinical laparoscopic setup and a robotic arm simulating tissue
motion, with experiments conducted on a synthetic 3D-printed phantom and a
chicken tissue phantom. Tracking on the chicken tissue phantom yielded more
reliable results, with Euclidean distance errors as low as 1.1 mm at a velocity
of 10 mm/s. These findings highlight the potential of TAP-based models for
accurate, markerless 3D tracking in challenging surgical scenarios.

</details>


### [267] [Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/2508.07863)
*Bin Cao,Sipeng Zheng,Ye Wang,Lujie Xia,Qianshan Wei,Qin Jin,Jing Liu,Zongqing Lu*

Main category: cs.CV

TL;DR: The paper introduces Being-M0.5, a real-time, controllable vision-language-motion model (VLMM) addressing key limitations in human motion generation, leveraging the large-scale HuMo100M dataset and a novel part-aware residual quantization technique.


<details>
  <summary>Details</summary>
Motivation: Existing VLMMs lack controllability in diverse commands, pose initialization, long-term sequences, unseen scenarios, and fine-grained body part control, hindering practical deployment.

Method: The approach uses HuMo100M (a large motion dataset) and introduces part-aware residual quantization for motion tokenization, enabling precise control over body parts.

Result: Being-M0.5 achieves state-of-the-art performance in motion generation tasks and operates in real-time.

Conclusion: The contributions (dataset, model, and techniques) advance motion generation technology for real-world applications.

Abstract: Human motion generation has emerged as a critical technology with
transformative potential for real-world applications. However, existing
vision-language-motion models (VLMMs) face significant limitations that hinder
their practical deployment. We identify controllability as a main bottleneck,
manifesting in five key aspects: inadequate response to diverse human commands,
limited pose initialization capabilities, poor performance on long-term
sequences, insufficient handling of unseen scenarios, and lack of fine-grained
control over individual body parts. To overcome these limitations, we present
Being-M0.5, the first real-time, controllable VLMM that achieves
state-of-the-art performance across multiple motion generation tasks. Our
approach is built upon HuMo100M, the largest and most comprehensive human
motion dataset to date, comprising over 5 million self-collected motion
sequences, 100 million multi-task instructional instances, and detailed
part-level annotations that address a critical gap in existing datasets. We
introduce a novel part-aware residual quantization technique for motion
tokenization that enables precise, granular control over individual body parts
during generation. Extensive experimental validation demonstrates Being-M0.5's
superior performance across diverse motion benchmarks, while comprehensive
efficiency analysis confirms its real-time capabilities. Our contributions
include design insights and detailed computational analysis to guide future
development of practical motion generators. We believe that HuMo100M and
Being-M0.5 represent significant advances that will accelerate the adoption of
motion generation technologies in real-world applications. The project page is
available at https://beingbeyond.github.io/Being-M0.5.

</details>


### [268] [CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/abs/2508.07871)
*Yanshu Li,Jianjiang Yang,Zhennan Shen,Ligong Han,Haoyan Xu,Ruixiang Tang*

Main category: cs.CV

TL;DR: CATP is a training-free pruning method for multimodal ICL, reducing image token redundancy and improving efficiency without performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing image token pruning methods overlook multimodal ICL, where redundancy is greater and efficiency is critical.

Method: CATP uses a two-stage progressive pruning approach to handle cross-modal interactions in the input sequence.

Result: After pruning 77.8% of image tokens, CATP improves performance by 0.6% and reduces inference latency by 10.78%.

Conclusion: CATP enhances multimodal ICL's practical value and sets a foundation for future interleaved image-text scenarios.

Abstract: Modern large vision-language models (LVLMs) convert each input image into a
large set of tokens, far outnumbering the text tokens. Although this improves
visual perception, it introduces severe image token redundancy. Because image
tokens carry sparse information, many add little to reasoning, yet greatly
increase inference cost. The emerging image token pruning methods tackle this
issue by identifying the most important tokens and discarding the rest. These
methods can raise efficiency with only modest performance loss. However, most
of them only consider single-image tasks and overlook multimodal in-context
learning (ICL), where redundancy is greater and efficiency is more critical.
Redundant tokens weaken the advantage of multimodal ICL for rapid domain
adaptation and cause unstable performance. Applying existing pruning methods in
this setting leads to large accuracy drops, exposing a clear gap and the need
for new techniques. Thus, we propose Contextually Adaptive Token Pruning
(CATP), a training-free pruning method targeted at multimodal ICL. CATP
consists of two stages that perform progressive pruning to fully account for
the complex cross-modal interactions in the input sequence. After removing
77.8\% of the image tokens, CATP produces an average performance gain of 0.6\%
over the vanilla model on four LVLMs and eight benchmarks, exceeding all
baselines remarkably. Meanwhile, it effectively improves efficiency by
achieving an average reduction of 10.78\% in inference latency. CATP enhances
the practical value of multimodal ICL and lays the groundwork for future
progress in interleaved image-text scenarios.

</details>


### [269] [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2508.07877)
*WonJun Moon,Hyun Seok Seong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: The paper introduces a method for weakly supervised affordance grounding (WSAG) using selective prototypical and pixel contrastive objectives to learn affordance-relevant cues at part and object levels.


<details>
  <summary>Details</summary>
Motivation: Humans intuitively grasp functional parts without pixel-level annotations, but models often focus on irrelevant patterns. The goal is to improve affordance grounding by learning from complementary views.

Method: Leverages CLIP to find action-associated objects in egocentric and exocentric images, then cross-references them to identify part-level affordance clues. Uses selective prototypical and pixel contrastive objectives to focus on relevant cues.

Result: The approach shifts activation from irrelevant areas to meaningful affordance cues, demonstrating effectiveness in experiments.

Conclusion: The method advances WSAG by adaptively learning affordance-relevant cues, outperforming previous classification-focused approaches.

Abstract: Facilitating an entity's interaction with objects requires accurately
identifying parts that afford specific actions. Weakly supervised affordance
grounding (WSAG) seeks to imitate human learning from third-person
demonstrations, where humans intuitively grasp functional parts without needing
pixel-level annotations. To achieve this, grounding is typically learned using
a shared classifier across images from different perspectives, along with
distillation strategies incorporating part discovery process. However, since
affordance-relevant parts are not always easily distinguishable, models
primarily rely on classification, often focusing on common class-specific
patterns that are unrelated to affordance. To address this limitation, we move
beyond isolated part-level learning by introducing selective prototypical and
pixel contrastive objectives that adaptively learn affordance-relevant cues at
both the part and object levels, depending on the granularity of the available
information. Initially, we find the action-associated objects in both
egocentric (object-focused) and exocentric (third-person example) images by
leveraging CLIP. Then, by cross-referencing the discovered objects of
complementary views, we excavate the precise part-level affordance clues in
each perspective. By consistently learning to distinguish affordance-relevant
regions from affordance-irrelevant background context, our approach effectively
shifts activation from irrelevant areas toward meaningful affordance cues.
Experimental results demonstrate the effectiveness of our method. Codes are
available at github.com/hynnsk/SelectiveCL.

</details>


### [270] [TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal](https://arxiv.org/abs/2508.07878)
*Hanting Wang,Shengpeng Ji,Shulei Wang,Hai Huang,Xiao Jin,Qifei Zhang,Tao Jin*

Main category: cs.CV

TL;DR: A parameter-efficient All-in-One image restoration framework uses task-aware enhanced prompts to address various adverse weather degradations, reducing parameter overhead and improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing All-in-One image restoration methods suffer from high parameter overhead and overlook inter-task relatedness.

Method: A two-stage training paradigm (pretraining and prompt-tuning) with task-aware enhanced prompts and low-rank decomposition to capture task-general and task-specific characteristics.

Result: Achieves superior performance with only 2.75M parameters, as shown by experimental results.

Conclusion: The proposed framework efficiently handles multiple restoration tasks with minimal parameters, leveraging task-aware prompts and contrastive constraints.

Abstract: Image restoration under adverse weather conditions has been extensively
explored, leading to numerous high-performance methods. In particular, recent
advances in All-in-One approaches have shown impressive results by training on
multi-task image restoration datasets. However, most of these methods rely on
dedicated network modules or parameters for each specific degradation type,
resulting in a significant parameter overhead. Moreover, the relatedness across
different restoration tasks is often overlooked. In light of these issues, we
propose a parameter-efficient All-in-One image restoration framework that
leverages task-aware enhanced prompts to tackle various adverse weather
degradations.Specifically, we adopt a two-stage training paradigm consisting of
a pretraining phase and a prompt-tuning phase to mitigate parameter conflicts
across tasks. We first employ supervised learning to acquire general
restoration knowledge, and then adapt the model to handle specific degradation
via trainable soft prompts. Crucially, we enhance these task-specific prompts
in a task-aware manner. We apply low-rank decomposition to these prompts to
capture both task-general and task-specific characteristics, and impose
contrastive constraints to better align them with the actual inter-task
relatedness. These enhanced prompts not only improve the parameter efficiency
of the restoration model but also enable more accurate task modeling, as
evidenced by t-SNE analysis. Experimental results on different restoration
tasks demonstrate that the proposed method achieves superior performance with
only 2.75M parameters.

</details>


### [271] [NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction](https://arxiv.org/abs/2508.07897)
*Tianle Zeng,Junlei Hu,Gerardo Loza Galindo,Sharib Ali,Duygu Sarikaya,Pietro Valdastri,Dominic Jones*

Main category: cs.CV

TL;DR: A novel dynamic Gaussian Splatting technique is introduced to address data scarcity in surgical image datasets, enabling realistic synthetic data generation and improving model performance.


<details>
  <summary>Details</summary>
Motivation: Current data-driven approaches in surgical automation require large labeled datasets, limiting their application. This work aims to overcome data scarcity.

Method: Proposes a dynamic Gaussian model for surgical scenes, dynamic training adjustment for real-world challenges, and automatic annotation generation. Evaluated on a new dataset with 14,000 frames.

Result: Generates photo-realistic labeled datasets (PSNR 29.87) and improves model performance by 15% over state-of-the-art augmentation.

Conclusion: The method effectively addresses data scarcity, enhances synthetic data quality, and boosts model performance in surgical automation.

Abstract: Computer vision-based technologies significantly enhance surgical automation
by advancing tool tracking, detection, and localization. However, Current
data-driven approaches are data-voracious, requiring large, high-quality
labeled image datasets, which limits their application in surgical data
science. Our Work introduces a novel dynamic Gaussian Splatting technique to
address the data scarcity in surgical image datasets. We propose a dynamic
Gaussian model to represent dynamic surgical scenes, enabling the rendering of
surgical instruments from unseen viewpoints and deformations with real tissue
backgrounds. We utilize a dynamic training adjustment strategy to address
challenges posed by poorly calibrated camera poses from real-world scenarios.
Additionally, we propose a method based on dynamic Gaussians for automatically
generating annotations for our synthetic data. For evaluation, we constructed a
new dataset featuring seven scenes with 14,000 frames of tool and camera motion
and tool jaw articulation, with a background of an ex-vivo porcine model. Using
this dataset, we synthetically replicate the scene deformation from the ground
truth data, allowing direct comparisons of synthetic image quality.
Experimental results illustrate that our method generates photo-realistic
labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio
(29.87). We further evaluate the performance of medical-specific neural
networks trained on real and synthetic images using an unseen real-world image
dataset. Our results show that the performance of models trained on synthetic
images generated by the proposed method outperforms those trained with
state-of-the-art standard data augmentation by 10%, leading to an overall
improvement in model performances by nearly 15%.

</details>


### [272] [Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation](https://arxiv.org/abs/2508.07901)
*Bowen Xue,Qixin Yan,Wenjing Wang,Hao Liu,Chen Li*

Main category: cs.CV

TL;DR: Stand-In is a lightweight, plug-and-play framework for identity-preserving video generation, using minimal additional parameters and outperforming full-parameter methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for high-fidelity human video generation are resource-intensive and lack compatibility with other AIGC tools.

Method: Introduces a conditional image branch and restricted self-attentions with conditional position mapping, trained with only 2000 pairs.

Result: Achieves excellent video quality and identity preservation with ~1% additional parameters, outperforming full-parameter methods.

Conclusion: Stand-In is effective, lightweight, and versatile, enabling seamless integration for various video generation tasks.

Abstract: Generating high-fidelity human videos that match user-specified identities is
important yet challenging in the field of generative AI. Existing methods often
rely on an excessive number of training parameters and lack compatibility with
other AIGC tools. In this paper, we propose Stand-In, a lightweight and
plug-and-play framework for identity preservation in video generation.
Specifically, we introduce a conditional image branch into the pre-trained
video generation model. Identity control is achieved through restricted
self-attentions with conditional position mapping, and can be learned quickly
with only 2000 pairs. Despite incorporating and training just $\sim$1\%
additional parameters, our framework achieves excellent results in video
quality and identity preservation, outperforming other full-parameter training
methods. Moreover, our framework can be seamlessly integrated for other tasks,
such as subject-driven video generation, pose-referenced video generation,
stylization, and face swapping.

</details>


### [273] [CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality](https://arxiv.org/abs/2508.07904)
*Marco Peer,Anna Scius-Bertrand,Andreas Fischer*

Main category: cs.CV

TL;DR: A self-training method using CTC alignment improves handwritten text recognition for historical documents by addressing hyphenation errors, with iterative training enhancing performance and alignment accuracy.


<details>
  <summary>Details</summary>
Motivation: Historical documents pose challenges like handwriting variability and annotation errors (e.g., hyphenation), which hinder accurate text recognition.

Method: A self-training approach based on CTC alignment matches transcriptions to text line images using dynamic programming and model output probabilities trained with CTC loss.

Result: Performance improved by 1.1 percentage points CER; weaker models surprisingly yielded more accurate alignments, enabling iterative training.

Conclusion: The method enhances text recognition pipelines iteratively, with released code and a corrected dataset subset for further research.

Abstract: Handwritten text recognition for historical documents remains challenging due
to handwriting variability, degraded sources, and limited layout-aware
annotations. In this work, we address annotation errors - particularly
hyphenation issues - in the Bullinger correspondence, a large 16th-century
letter collection. We introduce a self-training method based on a CTC alignment
algorithm that matches full transcriptions to text line images using dynamic
programming and model output probabilities trained with the CTC loss. Our
approach improves performance (e.g., by 1.1 percentage points CER with PyLaia)
and increases alignment accuracy. Interestingly, we find that weaker models
yield more accurate alignments, enabling an iterative training strategy. We
release a new manually corrected subset of 100 pages from the Bullinger
dataset, along with our code and benchmarks. Our approach can be applied
iteratively to further improve the CER as well as the alignment quality for
text recognition pipelines. Code and data are available via
https://github.com/andreas-fischer-unifr/nntp.

</details>


### [274] [Generative Video Matting](https://arxiv.org/abs/2508.07905)
*Yongtao Ge,Kangyang Xie,Guangkai Xu,Mingyu Liu,Li Ke,Longtao Huang,Hui Xue,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: The paper addresses video matting limitations by proposing large-scale pre-training with synthetic data and a novel video matting approach leveraging pre-trained diffusion models for better generalization and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Existing video matting datasets lack high-quality ground-truth data, leading to poor generalization in real-world scenarios.

Method: Uses diverse synthetic and pseudo-labeled datasets for pre-training, develops a scalable synthetic data pipeline, and introduces a video matting approach leveraging pre-trained diffusion models.

Result: Demonstrates superior performance on benchmark datasets and strong generalization in real-world scenes.

Conclusion: The proposed method improves video matting quality and generalization, with code publicly available.

Abstract: Video matting has traditionally been limited by the lack of high-quality
ground-truth data. Most existing video matting datasets provide only
human-annotated imperfect alpha and foreground annotations, which must be
composited to background images or videos during the training stage. Thus, the
generalization capability of previous methods in real-world scenarios is
typically poor. In this work, we propose to solve the problem from two
perspectives. First, we emphasize the importance of large-scale pre-training by
pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also
develop a scalable synthetic data generation pipeline that can render diverse
human bodies and fine-grained hairs, yielding around 200 video clips with a
3-second duration for fine-tuning. Second, we introduce a novel video matting
approach that can effectively leverage the rich priors from pre-trained video
diffusion models. This architecture offers two key advantages. First, strong
priors play a critical role in bridging the domain gap between synthetic and
real-world scenes. Second, unlike most existing methods that process video
matting frame-by-frame and use an independent decoder to aggregate temporal
information, our model is inherently designed for video, ensuring strong
temporal consistency. We provide a comprehensive quantitative evaluation across
three benchmark datasets, demonstrating our approach's superior performance,
and present comprehensive qualitative results in diverse real-world scenes,
illustrating the strong generalization capability of our method. The code is
available at https://github.com/aim-uofa/GVM.

</details>


### [275] [Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07908)
*Xudong Cai,Shuo Wang,Peng Wang,Yongcai Wang,Zhaoxin Fan,Wanting Li,Tianbao Zhang,Jianrong Tao,Yeying Jin,Deying Li*

Main category: cs.CV

TL;DR: Mem4D proposes a dual-memory framework to decouple static and dynamic scene reconstruction, addressing the Memory Demand Dilemma in monocular video-based dense geometry reconstruction.


<details>
  <summary>Details</summary>
Motivation: The conflict between long-term stability for static structures and high-fidelity detail retention for dynamic motion in memory-based methods compromises reconstruction quality.

Method: Mem4D uses a dual-memory architecture: Transient Dynamics Memory (TDM) for dynamic motion and Persistent Structure Memory (PSM) for static geometry, enabling alternating queries for balanced reconstruction.

Result: Mem4D achieves state-of-the-art or competitive performance on benchmarks, maintaining global consistency for static elements and high fidelity for dynamic content.

Conclusion: The proposed framework effectively resolves the Memory Demand Dilemma, offering efficient and accurate reconstruction for both static and dynamic scenes.

Abstract: Reconstructing dense geometry for dynamic scenes from a monocular video is a
critical yet challenging task. Recent memory-based methods enable efficient
online reconstruction, but they fundamentally suffer from a Memory Demand
Dilemma: The memory representation faces an inherent conflict between the
long-term stability required for static structures and the rapid, high-fidelity
detail retention needed for dynamic motion. This conflict forces existing
methods into a compromise, leading to either geometric drift in static
structures or blurred, inaccurate reconstructions of dynamic objects. To
address this dilemma, we propose Mem4D, a novel framework that decouples the
modeling of static geometry and dynamic motion. Guided by this insight, we
design a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)
focuses on capturing high-frequency motion details from recent frames, enabling
accurate and fine-grained modeling of dynamic content; 2) The Persistent
Structure Memory (PSM) compresses and preserves long-term spatial information,
ensuring global consistency and drift-free reconstruction for static elements.
By alternating queries to these specialized memories, Mem4D simultaneously
maintains static geometry with global consistency and reconstructs dynamic
elements with high fidelity. Experiments on challenging benchmarks demonstrate
that our method achieves state-of-the-art or competitive performance while
maintaining high efficiency. Codes will be publicly available.

</details>


### [276] [RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering](https://arxiv.org/abs/2508.07918)
*Xing Zi,Jinghao Xiao,Yunxiao Shi,Xian Tao,Jun Li,Ali Braytee,Mukesh Prasad*

Main category: cs.CV

TL;DR: The paper introduces RSVLM-QA, a large-scale VQA dataset for remote sensing, addressing limitations in existing datasets through rich annotations and diverse questions, and evaluates VLMs.


<details>
  <summary>Details</summary>
Motivation: Existing RS VQA datasets lack annotation richness, question diversity, and specific reasoning assessment, necessitating a more comprehensive dataset.

Method: RSVLM-QA integrates data from RS datasets, uses GPT-4.1 for automated annotations, and includes a specialized counting QA process.

Result: The dataset contains 13,820 images and 162,373 VQA pairs, outperforming existing benchmarks in depth and breadth.

Conclusion: RSVLM-QA serves as a valuable resource for RS VQA and VLM research, advancing the field by challenging current models.

Abstract: Visual Question Answering (VQA) in remote sensing (RS) is pivotal for
interpreting Earth observation data. However, existing RS VQA datasets are
constrained by limitations in annotation richness, question diversity, and the
assessment of specific reasoning capabilities. This paper introduces RSVLM-QA
dataset, a new large-scale, content-rich VQA dataset for the RS domain.
RSVLM-QA is constructed by integrating data from several prominent RS
segmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ
an innovative dual-track annotation generation pipeline. Firstly, we leverage
Large Language Models (LLMs), specifically GPT-4.1, with meticulously designed
prompts to automatically generate a suite of detailed annotations including
image captions, spatial relations, and semantic tags, alongside complex
caption-based VQA pairs. Secondly, to address the challenging task of object
counting in RS imagery, we have developed a specialized automated process that
extracts object counts directly from the original segmentation data; GPT-4.1
then formulates natural language answers from these counts, which are paired
with preset question templates to create counting QA pairs. RSVLM-QA comprises
13,820 images and 162,373 VQA pairs, featuring extensive annotations and
diverse question types. We provide a detailed statistical analysis of the
dataset and a comparison with existing RS VQA benchmarks, highlighting the
superior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct
benchmark experiments on Six mainstream Vision Language Models (VLMs),
demonstrating that RSVLM-QA effectively evaluates and challenges the
understanding and reasoning abilities of current VLMs in the RS domain. We
believe RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM
research communities, poised to catalyze advancements in the field.

</details>


### [277] [Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection](https://arxiv.org/abs/2508.07923)
*Jakub Binda,Valentina Paneta,Vasileios Eleftheriadis,Hongkyou Chung,Panagiotis Papadimitroulas,Neo Christopher Chung*

Main category: cs.CV

TL;DR: A hybrid anomaly detection framework enhances reliability and regulatory compliance of Generative AI in nuclear medicine applications like synthetic X-ray generation and radiation dose estimation.


<details>
  <summary>Details</summary>
Motivation: The high-stakes nature of biomedical imaging requires robust mechanisms to detect and manage unexpected or erroneous behavior in Generative AI models.

Method: Development and implementation of a hybrid anomaly detection framework for GenAI models in BIOEMTECH's systems, demonstrated in Pose2Xray and DosimetrEYE applications.

Result: The framework improves reliability, reduces manual oversight, and supports real-time quality control in preclinical settings.

Conclusion: This approach strengthens the industrial viability of GenAI in nuclear medicine by increasing robustness, scalability, and regulatory compliance.

Abstract: Generative AI holds great potentials to automate and enhance data synthesis
in nuclear medicine. However, the high-stakes nature of biomedical imaging
necessitates robust mechanisms to detect and manage unexpected or erroneous
model behavior. We introduce development and implementation of a hybrid anomaly
detection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.
Two applications are demonstrated: Pose2Xray, which generates synthetic X-rays
from photographic mouse images, and DosimetrEYE, which estimates 3D radiation
dose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)
enhances reliability, reduces manual oversight, and supports real-time quality
control. This approach strengthens the industrial viability of GenAI in
preclinical settings by increasing robustness, scalability, and regulatory
compliance.

</details>


### [278] [TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding](https://arxiv.org/abs/2508.07925)
*Jin-Seop Lee,SungJoon Lee,Jaehan Ahn,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: The paper introduces TAG, a temporal-aware method for zero-shot video temporal grounding, addressing semantic fragmentation and skewed similarity distributions without relying on LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot VTG methods suffer from semantic fragmentation and skewed similarity distributions, and rely on expensive LLMs.

Method: TAG uses temporal pooling, temporal coherence clustering, and similarity adjustment to capture temporal context and correct similarity distributions.

Result: Achieves state-of-the-art results on Charades-STA and ActivityNet Captions datasets without LLMs.

Conclusion: TAG is a simple, effective solution for zero-shot VTG, outperforming existing methods without additional training or LLMs.

Abstract: Video Temporal Grounding (VTG) aims to extract relevant video segments based
on a given natural language query. Recently, zero-shot VTG methods have gained
attention by leveraging pretrained vision-language models (VLMs) to localize
target moments without additional training. However, existing approaches suffer
from semantic fragmentation, where temporally continuous frames sharing the
same semantics are split across multiple segments. When segments are
fragmented, it becomes difficult to predict an accurate target moment that
aligns with the text query. Also, they rely on skewed similarity distributions
for localization, making it difficult to select the optimal segment.
Furthermore, they heavily depend on the use of LLMs which require expensive
inferences. To address these limitations, we propose a \textit{TAG}, a simple
yet effective Temporal-Aware approach for zero-shot video temporal Grounding,
which incorporates temporal pooling, temporal coherence clustering, and
similarity adjustment. Our proposed method effectively captures the temporal
context of videos and addresses distorted similarity distributions without
training. Our approach achieves state-of-the-art results on Charades-STA and
ActivityNet Captions benchmark datasets without rely on LLMs. Our code is
available at https://github.com/Nuetee/TAG

</details>


### [279] [VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security](https://arxiv.org/abs/2508.07960)
*Ajnas Muhammed,Iurri Medvedev,Nuno Gonçalves*

Main category: cs.CV

TL;DR: VOIDFace introduces a privacy-preserving facial recognition framework using visual secret sharing and patch-based multi-training to eliminate data replication and enhance user control over personal data.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy and ethical concerns in facial recognition by reducing data replication and improving user control over personal face data.

Method: Uses visual secret sharing for secure data storage and a patch-based multi-training network to develop a robust, privacy-preserving system.

Result: VOIDFace ensures Right-To-Be-Forgotten, improves data control, security, and privacy while maintaining competitive performance on the VGGFace2 dataset.

Conclusion: VOIDFace successfully enhances privacy, security, and efficiency in facial recognition training while empowering users with greater data control.

Abstract: Advancement of machine learning techniques, combined with the availability of
large-scale datasets, has significantly improved the accuracy and efficiency of
facial recognition. Modern facial recognition systems are trained using large
face datasets collected from diverse individuals or public repositories.
However, for training, these datasets are often replicated and stored in
multiple workstations, resulting in data replication, which complicates
database management and oversight. Currently, once a user submits their face
for dataset preparation, they lose control over how their data is used, raising
significant privacy and ethical concerns. This paper introduces VOIDFace, a
novel framework for facial recognition systems that addresses two major issues.
First, it eliminates the need of data replication and improves data control to
securely store training face data by using visual secret sharing. Second, it
proposes a patch-based multi-training network that uses this novel training
data storage mechanism to develop a robust, privacy-preserving facial
recognition system. By integrating these advancements, VOIDFace aims to improve
the privacy, security, and efficiency of facial recognition training, while
ensuring greater control over sensitive personal face data. VOIDFace also
enables users to exercise their Right-To-Be-Forgotten property to control their
personal data. Experimental evaluations on the VGGFace2 dataset show that
VOIDFace provides Right-To-Be-Forgotten, improved data control, security, and
privacy while maintaining competitive facial recognition performance. Code is
available at: https://github.com/ajnasmuhammed89/VOIDFace

</details>


### [280] [TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking](https://arxiv.org/abs/2508.07968)
*Tony Danjun Wang,Christian Heiliger,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: TrackOR is a framework for long-term multi-person tracking and re-identification in operating rooms, using 3D geometric signatures to improve tracking accuracy and enable actionable insights for personalized surgical support.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve patient outcomes by providing intelligent, personalized support to surgical teams through consistent tracking of staff members during long procedures.

Method: TrackOR leverages 3D geometric signatures for online tracking and offline recovery, achieving state-of-the-art performance in association accuracy.

Result: The framework improves association accuracy by 11% over baselines and enables analysis-ready trajectories for staff-centric analyses.

Conclusion: TrackOR makes persistent identity tracking feasible, paving the way for granular, personalized surgical support systems and applications like temporal pathway imprints for team efficiency and safety.

Abstract: Providing intelligent support to surgical teams is a key frontier in
automated surgical scene understanding, with the long-term goal of improving
patient outcomes. Developing personalized intelligence for all staff members
requires maintaining a consistent state of who is located where for long
surgical procedures, which still poses numerous computational challenges. We
propose TrackOR, a framework for tackling long-term multi-person tracking and
re-identification in the operating room. TrackOR uses 3D geometric signatures
to achieve state-of-the-art online tracking performance (+11% Association
Accuracy over the strongest baseline), while also enabling an effective offline
recovery process to create analysis-ready trajectories. Our work shows that by
leveraging 3D geometric information, persistent identity tracking becomes
attainable, enabling a critical shift towards the more granular, staff-centric
analyses required for personalized intelligent systems in the operating room.
This new capability opens up various applications, including our proposed
temporal pathway imprints that translate raw tracking data into actionable
insights for improving team efficiency and safety and ultimately providing
personalized support.

</details>


### [281] [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](https://arxiv.org/abs/2508.07981)
*Fangyuan Mao,Aiming Hao,Jintao Chen,Dongxia Liu,Xiaokun Feng,Jiashu Zhu,Meiqi Wu,Chubin Chen,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Omni-Effects is a unified framework for generating spatially controllable composite visual effects (VFX) in videos, overcoming limitations of single-effect methods.


<details>
  <summary>Details</summary>
Motivation: Current VFX generation methods are limited to single effects due to per-effect LoRA training, hindering applications requiring multiple spatially controlled effects.

Method: Proposes Omni-Effects with LoRA-MoE for diverse effect integration and SAP for spatial control, plus an IIF module to isolate effects.

Result: Achieves precise spatial control and diverse effect generation, validated by experiments.

Conclusion: Omni-Effects enables users to specify effect categories and locations, advancing VFX production.

Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern
cinematic production. Although video generation models offer cost-efficient
solutions for VFX production, current methods are constrained by per-effect
LoRA training, which limits generation to single effects. This fundamental
limitation impedes applications that require spatially controllable composite
effects, i.e., the concurrent generation of multiple effects at designated
locations. However, integrating diverse effects into a unified framework faces
major challenges: interference from effect variations and spatial
uncontrollability during multi-VFX joint training. To tackle these challenges,
we propose Omni-Effects, a first unified framework capable of generating
prompt-guided effects and spatially controllable composite effects. The core of
our framework comprises two key innovations: (1) LoRA-based Mixture of Experts
(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects
within a unified model while effectively mitigating cross-task interference.
(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the
text token, enabling precise spatial control. Furthermore, we introduce an
Independent-Information Flow (IIF) module integrated within the SAP, isolating
the control signals corresponding to individual effects to prevent any unwanted
blending. To facilitate this research, we construct a comprehensive VFX dataset
Omni-VFX via a novel data collection pipeline combining image editing and
First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX
evaluation framework for validating model performance. Extensive experiments
demonstrate that Omni-Effects achieves precise spatial control and diverse
effect generation, enabling users to specify both the category and location of
desired effects.

</details>


### [282] [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/abs/2508.07989)
*Xiantao Zhang*

Main category: cs.CV

TL;DR: MLLMs struggle with perceiving escalator direction due to Implicit Motion Blindness, highlighting a need for better motion perception in assistive tech.


<details>
  <summary>Details</summary>
Motivation: To address the trustworthiness gap in MLLMs for the BVI community by identifying and analyzing the Escalator Problem.

Method: Position paper analyzing the failure mode (Implicit Motion Blindness) and its root cause (frame-sampling paradigm).

Result: Identified a critical limitation in MLLMs' ability to perceive continuous motion, impacting real-world reliability.

Conclusion: Calls for a shift to robust physical perception and human-centered benchmarks to improve safety and user trust.

Abstract: Multimodal Large Language Models (MLLMs) hold immense promise as assistive
technologies for the blind and visually impaired (BVI) community. However, we
identify a critical failure mode that undermines their trustworthiness in
real-world applications. We introduce the Escalator Problem -- the inability of
state-of-the-art models to perceive an escalator's direction of travel -- as a
canonical example of a deeper limitation we term Implicit Motion Blindness.
This blindness stems from the dominant frame-sampling paradigm in video
understanding, which, by treating videos as discrete sequences of static
images, fundamentally struggles to perceive continuous, low-signal motion. As a
position paper, our contribution is not a new model but rather to: (I) formally
articulate this blind spot, (II) analyze its implications for user trust, and
(III) issue a call to action. We advocate for a paradigm shift from purely
semantic recognition towards robust physical perception and urge the
development of new, human-centered benchmarks that prioritize safety,
reliability, and the genuine needs of users in dynamic environments.

</details>


### [283] [Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models](https://arxiv.org/abs/2508.07996)
*Thinesh Thiyakesan Ponbagavathi,Chengzheng Yang,Alina Roitberg*

Main category: cs.CV

TL;DR: ProGraD introduces learnable group prompts and a lightweight transformer to enhance Vision Foundation Models for Group Activity Detection, outperforming state-of-the-art methods, especially in multi-group scenarios.


<details>
  <summary>Details</summary>
Motivation: Current Vision Foundation Models (VFMs) are pretrained on object-centric data and lack group-aware reasoning, limiting their effectiveness in Group Activity Detection (GAD).

Method: ProGraD uses learnable group prompts and a two-layer GroupContext Transformer to guide VFM attention and infer actor-group associations and collective behavior.

Result: ProGraD achieves gains of 6.5% (Group mAP@1.0) and 8.2% (Group mAP@0.5) in multi-group scenarios with only 10M trainable parameters.

Conclusion: ProGraD effectively bridges the gap in GAD by enhancing VFMs with structured group reasoning, offering interpretable attention maps and superior performance.

Abstract: Group Activity Detection (GAD) involves recognizing social groups and their
collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,
offer excellent features, but are pretrained primarily on object-centric data
and remain underexplored for modeling group dynamics. While they are a
promising alternative to highly task-specific GAD architectures that require
full fine-tuning, our initial investigation reveals that simply swapping CNN
backbones used in these methods with VFMs brings little gain, underscoring the
need for structured, group-aware reasoning on top.
  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method
that bridges this gap through 1) learnable group prompts to guide the VFM
attention toward social configurations, and 2) a lightweight two-layer
GroupContext Transformer that infers actor-group associations and collective
behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which
features multiple concurrent social groups, and Social-CAD, which focuses on
single-group interactions. While we surpass state-of-the-art in both settings,
our method is especially effective in complex multi-group scenarios, where we
yield a gain of 6.5\% (Group mAP\@1.0) and 8.2\% (Group mAP\@0.5) using only
10M trainable parameters. Furthermore, our experiments reveal that ProGraD
produces interpretable attention maps, offering insights into actor-group
reasoning. Code and models will be released.

</details>


### [284] [Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition](https://arxiv.org/abs/2508.08004)
*Anqi Xiao,Weichen Yu,Hongyuan Yu*

Main category: cs.CV

TL;DR: SRA is a search-free AutoDA method that dynamically adjusts augmentation policies, achieving high accuracy and compatibility without extensive search or tuning.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and suboptimal performance of mainstream AutoDA methods by proposing a simpler, adaptive solution.

Method: Sample-aware RandAugment (SRA) uses a heuristic scoring module to tailor augmentations per sample and employs an asymmetric strategy.

Result: Achieves 78.31% Top-1 accuracy on ImageNet with ResNet-50, enhances downstream tasks, and generalizes well without tuning.

Conclusion: SRA simplifies AutoDA, offering effective, practical performance for diverse tasks.

Abstract: Automatic data augmentation (AutoDA) plays an important role in enhancing the
generalization of neural networks. However, mainstream AutoDA methods often
encounter two challenges: either the search process is excessively
time-consuming, hindering practical application, or the performance is
suboptimal due to insufficient policy adaptation during training. To address
these issues, we propose Sample-aware RandAugment (SRA), an asymmetric,
search-free AutoDA method that dynamically adjusts augmentation policies while
maintaining straightforward implementation. SRA incorporates a heuristic
scoring module that evaluates the complexity of the original training data,
enabling the application of tailored augmentations for each sample.
Additionally, an asymmetric augmentation strategy is employed to maximize the
potential of this scoring module. In multiple experimental settings, SRA
narrows the performance gap between search-based and search-free AutoDA
methods, achieving a state-of-the-art Top-1 accuracy of 78.31\% on ImageNet
with ResNet-50. Notably, SRA demonstrates good compatibility with existing
augmentation pipelines and solid generalization across new tasks, without
requiring hyperparameter tuning. The pretrained models leveraging SRA also
enhance recognition in downstream object detection tasks. SRA represents a
promising step towards simpler, more effective, and practical AutoDA designs
applicable to a variety of future tasks. Our code is available at
\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment

</details>


### [285] [Mitigating Biases in Surgical Operating Rooms with Geometry](https://arxiv.org/abs/2508.08028)
*Tony Danjun Wang,Tobias Czempiel,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: Deep neural networks in surgical ORs often learn spurious correlations due to standardized attire, leading to biased models. Using 3D point cloud sequences instead of RGB data helps avoid these biases by focusing on shape and motion.


<details>
  <summary>Details</summary>
Motivation: Standardized attire in ORs obscures identifying features, causing models to rely on incidental cues like footwear or eyewear, which biases predictions. Robust modeling of OR personnel is needed for accurate workflow analysis.

Method: The study uses gradient-based saliency analysis on OR datasets and proposes encoding personnel as 3D point cloud sequences to separate identity-relevant features from appearance-based confounders.

Result: RGB models perform comparably in simulated settings but drop 12% in realistic clinical scenarios, while geometric methods maintain accuracy by capturing meaningful biometric features.

Conclusion: Geometric representations (3D point clouds) are more robust for modeling OR personnel, avoiding biases from visual shortcuts and enabling accurate workflow analysis.

Abstract: Deep neural networks are prone to learning spurious correlations, exploiting
dataset-specific artifacts rather than meaningful features for prediction. In
surgical operating rooms (OR), these manifest through the standardization of
smocks and gowns that obscure robust identifying landmarks, introducing model
bias for tasks related to modeling OR personnel. Through gradient-based
saliency analysis on two public OR datasets, we reveal that CNN models succumb
to such shortcuts, fixating on incidental visual cues such as footwear beneath
surgical gowns, distinctive eyewear, or other role-specific identifiers.
Avoiding such biases is essential for the next generation of intelligent
assistance systems in the OR, which should accurately recognize personalized
workflow traits, such as surgical skill level or coordination with other staff
members. We address this problem by encoding personnel as 3D point cloud
sequences, disentangling identity-relevant shape and motion patterns from
appearance-based confounders. Our experiments demonstrate that while RGB and
geometric methods achieve comparable performance on datasets with apparent
simulation artifacts, RGB models suffer a 12% accuracy drop in realistic
clinical settings with decreased visual diversity due to standardizations. This
performance gap confirms that geometric representations capture more meaningful
biometric features, providing an avenue to developing robust methods of
modeling humans in the OR.

</details>


### [286] [TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation](https://arxiv.org/abs/2508.08038)
*Huawei Sun,Zixu Wang,Hao Feng,Julius Ott,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: The paper proposes TRIDE, a radar-camera fusion algorithm for depth estimation, incorporating weather-aware fusion and text features to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing radar-camera fusion methods ignore weather conditions and underutilize language descriptions, despite radar's robustness in adverse weather and advancements in Vision-Language models.

Method: Introduces text-generation and feature fusion techniques for monocular depth estimation, then proposes TRIDE with weather-aware fusion and radar-enhanced text features.

Result: Achieves 12.87% improvement in MAE and 9.08% in RMSE on nuScenes dataset.

Conclusion: TRIDE outperforms state-of-the-art methods by integrating weather awareness and language features, enhancing depth estimation accuracy.

Abstract: Depth estimation, essential for autonomous driving, seeks to interpret the 3D
environment surrounding vehicles. The development of radar sensors, known for
their cost-efficiency and robustness, has spurred interest in radar-camera
fusion-based solutions. However, existing algorithms fuse features from these
modalities without accounting for weather conditions, despite radars being
known to be more robust than cameras under adverse weather. Additionally, while
Vision-Language models have seen rapid advancement, utilizing language
descriptions alongside other modalities for depth estimation remains an open
challenge. This paper first introduces a text-generation strategy along with
feature extraction and fusion techniques that can assist monocular depth
estimation pipelines, leading to improved accuracy across different algorithms
on the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion
algorithm that enhances text feature extraction by incorporating radar point
information. To address the impact of weather on sensor performance, we
introduce a weather-aware fusion block that adaptively adjusts radar weighting
based on current weather conditions. Our method, benchmarked on the nuScenes
dataset, demonstrates performance gains over the state-of-the-art, achieving a
12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:
https://github.com/harborsarah/TRIDE

</details>


### [287] [S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix](https://arxiv.org/abs/2508.08048)
*Peng Dai,Feitong Tan,Qiangeng Xu,Yihua Huang,David Futschik,Ruofei Du,Sean Fanello,Yinda Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: A training-free method converts monocular videos into immersive 3D videos using depth estimation and a novel frame matrix inpainting framework.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored challenge of generating 3D stereoscopic and spatial videos from monocular video models.

Method: Warps monocular videos into pre-defined viewpoints using depth, applies frame matrix inpainting for consistency, and uses a dual-update scheme for quality.

Result: Produces high-quality 3D videos with spatial and temporal consistency, outperforming previous methods.

Conclusion: The method effectively generates immersive 3D videos without additional training, validated across various generative models.

Abstract: While video generation models excel at producing high-quality monocular
videos, generating 3D stereoscopic and spatial videos for immersive
applications remains an underexplored challenge. We present a pose-free and
training-free method that leverages an off-the-shelf monocular video generation
model to produce immersive 3D videos. Our approach first warps the generated
monocular video into pre-defined camera viewpoints using estimated depth
information, then applies a novel \textit{frame matrix} inpainting framework.
This framework utilizes the original video generation model to synthesize
missing content across different viewpoints and timestamps, ensuring spatial
and temporal consistency without requiring additional model fine-tuning.
Moreover, we develop a \dualupdate~scheme that further improves the quality of
video inpainting by alleviating the negative effects propagated from
disoccluded areas in the latent space. The resulting multi-view videos are then
adapted into stereoscopic pairs or optimized into 4D Gaussians for spatial
video synthesis. We validate the efficacy of our proposed method by conducting
experiments on videos from various generative models, such as Sora, Lumiere,
WALT, and Zeroscope. The experiments demonstrate that our method has a
significant improvement over previous methods. Project page at:
https://daipengwa.github.io/S-2VG_ProjectPage/

</details>


### [288] [PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI](https://arxiv.org/abs/2508.08058)
*Ziad Al-Haj Hemidi,Eytan Kats,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: PrIINeR integrates prior knowledge from deep learning into Implicit Neural Representations (INRs) for improved MRI reconstruction, outperforming existing methods in quality and artifact reduction.


<details>
  <summary>Details</summary>
Motivation: Accelerated MRI often degrades image quality; INRs struggle at high acceleration due to weak constraints.

Method: PrIINeR combines pre-trained deep learning models with INRs, enforcing dual data consistency for alignment with k-space data and prior-informed reconstruction.

Result: Outperforms state-of-the-art INR and learning-based methods on the NYU fastMRI dataset, improving structural preservation and reducing aliasing.

Conclusion: PrIINeR bridges deep learning and INRs, offering reliable high-quality accelerated MRI reconstruction.

Abstract: Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often
degrades image quality. While Implicit Neural Representations (INRs) show
promise for MRI reconstruction, they struggle at high acceleration factors due
to weak prior constraints, leading to structural loss and aliasing artefacts.
To address this, we propose PrIINeR, an INR-based MRI reconstruction method
that integrates prior knowledge from pre-trained deep learning models into the
INR framework. By combining population-level knowledge with instance-based
optimization and enforcing dual data consistency, PrIINeR aligns both with the
acquired k-space data and the prior-informed reconstruction. Evaluated on the
NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based
approaches but also improves upon several learning-based state-of-the-art
methods, significantly improving structural preservation and fidelity while
effectively removing aliasing artefacts.PrIINeR bridges deep learning and
INR-based techniques, offering a more reliable solution for high-quality,
accelerated MRI reconstruction. The code is publicly available on
https://github.com/multimodallearning/PrIINeR.

</details>


### [289] [Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition](https://arxiv.org/abs/2508.08069)
*Xiaoxiao Cui,Yiran Li,Kai He,Shanzhi Jiang,Mengli Xue,Wentao Li,Junhong Leng,Zhi Liu,Lizhen Cui,Shuo Li*

Main category: cs.CV

TL;DR: Proposes a new structural causal model (SCM) and Information Bottleneck-based Causal Attention (IBCA) to improve multi-label classification of medical images by filtering irrelevant features and enhancing class-specific attention.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with interpreting true causes due to attention to irrelevant features, limiting accurate diagnosis and interpretability.

Method: Uses Gaussian mixture multi-label spatial attention to filter irrelevant info and contrastive enhancement-based causal intervention to reduce spurious attention.

Result: IBCA outperforms other methods, with significant improvements in metrics like CR, OR, and mAP on Endo and MuReD datasets.

Conclusion: The proposed IBCA effectively learns discriminative class-specific attention, enhancing multi-label classification performance and interpretability.

Abstract: Multi-label classification (MLC) of medical images aims to identify multiple
diseases and holds significant clinical potential. A critical step is to learn
class-specific features for accurate diagnosis and improved interpretability
effectively. However, current works focus primarily on causal attention to
learn class-specific features, yet they struggle to interpret the true cause
due to the inadvertent attention to class-irrelevant features. To address this
challenge, we propose a new structural causal model (SCM) that treats
class-specific attention as a mixture of causal, spurious, and noisy factors,
and a novel Information Bottleneck-based Causal Attention (IBCA) that is
capable of learning the discriminative class-specific attention for MLC of
medical images. Specifically, we propose learning Gaussian mixture multi-label
spatial attention to filter out class-irrelevant information and capture each
class-specific attention pattern. Then a contrastive enhancement-based causal
intervention is proposed to gradually mitigate the spurious attention and
reduce noise information by aligning multi-head attention with the Gaussian
mixture multi-label spatial. Quantitative and ablation results on Endo and
MuReD show that IBCA outperforms all methods. Compared to the second-best
results for each metric, IBCA achieves improvements of 6.35\% in CR, 7.72\% in
OR, and 5.02\% in mAP for MuReD, 1.47\% in CR, and 1.65\% in CF1, and 1.42\% in
mAP for Endo.

</details>


### [290] [ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness](https://arxiv.org/abs/2508.08082)
*Zizheng Guo,Bochao Zou,Junbao Zhuo,Huimin Ma*

Main category: cs.CV

TL;DR: The paper proposes ME-TST and ME-TST+, state space model-based architectures for micro-expression (ME) analysis, replacing fixed-window classification with video-level regression and integrating spotting and recognition tasks for improved performance.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for ME analysis use fixed window lengths and treat spotting and recognition separately, leading to practical limitations and overlooking their relationship.

Method: The authors introduce ME-TST and ME-TST+, leveraging temporal state transition mechanisms, multi-granularity ROI modeling, and the slowfast Mamba framework to address these issues.

Result: The proposed methods achieve state-of-the-art performance in ME analysis.

Conclusion: The architectures effectively model ME dynamics and synergize spotting and recognition, enhancing overall analysis accuracy.

Abstract: Micro-expressions (MEs) are regarded as important indicators of an
individual's intrinsic emotions, preferences, and tendencies. ME analysis
requires spotting of ME intervals within long video sequences and recognition
of their corresponding emotional categories. Previous deep learning approaches
commonly employ sliding-window classification networks. However, the use of
fixed window lengths and hard classification presents notable limitations in
practice. Furthermore, these methods typically treat ME spotting and
recognition as two separate tasks, overlooking the essential relationship
between them. To address these challenges, this paper proposes two state space
model-based architectures, namely ME-TST and ME-TST+, which utilize temporal
state transition mechanisms to replace conventional window-level classification
with video-level regression. This enables a more precise characterization of
the temporal dynamics of MEs and supports the modeling of MEs with varying
durations. In ME-TST+, we further introduce multi-granularity ROI modeling and
the slowfast Mamba framework to alleviate information loss associated with
treating ME analysis as a time-series task. Additionally, we propose a synergy
strategy for spotting and recognition at both the feature and result levels,
leveraging their intrinsic connection to enhance overall analysis performance.
Extensive experiments demonstrate that the proposed methods achieve
state-of-the-art performance. The codes are available at
https://github.com/zizheng-guo/ME-TST.

</details>


### [291] [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086)
*Zhongqi Yang,Wenhang Ge,Yuqi Li,Jiaqi Chen,Haoyuan Li,Mengyin An,Fei Kang,Hua Xue,Baixin Xu,Yuyang Yin,Eric Li,Yang Liu,Yikai Wang,Hao-Xiang Guo,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-3D is a framework for generating explorable 3D worlds from single images or text prompts using panoramic video diffusion and 3D reconstruction methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D world generation often produce scenes with limited scope. Matrix-3D aims to overcome this by leveraging panoramic representations for wider coverage.

Method: The framework combines conditional video generation and panoramic 3D reconstruction, using a trajectory-guided panoramic video diffusion model and two reconstruction methods (feed-forward and optimization-based).

Result: Matrix-3D achieves state-of-the-art performance in panoramic video and 3D world generation, validated by extensive experiments.

Conclusion: Matrix-3D advances 3D world generation by improving scope and quality through panoramic representations and novel reconstruction techniques.

Abstract: Explorable 3D world generation from a single image or text prompt forms a
cornerstone of spatial intelligence. Recent works utilize video model to
achieve wide-scope and generalizable 3D world generation. However, existing
approaches often suffer from a limited scope in the generated scenes. In this
work, we propose Matrix-3D, a framework that utilize panoramic representation
for wide-coverage omnidirectional explorable 3D world generation that combines
conditional video generation and panoramic 3D reconstruction. We first train a
trajectory-guided panoramic video diffusion model that employs scene mesh
renders as condition, to enable high-quality and geometrically consistent scene
video generation. To lift the panorama scene video to 3D world, we propose two
separate methods: (1) a feed-forward large panorama reconstruction model for
rapid 3D scene reconstruction and (2) an optimization-based pipeline for
accurate and detailed 3D scene reconstruction. To facilitate effective
training, we also introduce the Matrix-Pano dataset, the first large-scale
synthetic collection comprising 116K high-quality static panoramic video
sequences with depth and trajectory annotations. Extensive experiments
demonstrate that our proposed framework achieves state-of-the-art performance
in panoramic video generation and 3D world generation. See more in
https://matrix-3d.github.io.

</details>


### [292] [MDD-Net: Multimodal Depression Detection through Mutual Transformer](https://arxiv.org/abs/2508.08093)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: The paper proposes MDD-Net, a multimodal depression detection network using acoustic and visual data from social media, outperforming state-of-the-art methods by 17.37% in F1-Score.


<details>
  <summary>Details</summary>
Motivation: Depression severely impacts well-being, and social media data offers a simple yet rich source for mental health research.

Method: MDD-Net uses mutual transformers to extract and fuse acoustic and visual features, with four modules: feature extraction, mutual transformer, and detection.

Result: Experiments on the D-Vlog dataset show MDD-Net outperforms existing methods by up to 17.37% in F1-Score.

Conclusion: MDD-Net is effective for depression detection, leveraging multimodal social media data for superior performance.

Abstract: Depression is a major mental health condition that severely impacts the
emotional and physical well-being of individuals. The simple nature of data
collection from social media platforms has attracted significant interest in
properly utilizing this information for mental health research. A Multimodal
Depression Detection Network (MDD-Net), utilizing acoustic and visual data
obtained from social media networks, is proposed in this work where mutual
transformers are exploited to efficiently extract and fuse multimodal features
for efficient depression detection. The MDD-Net consists of four core modules:
an acoustic feature extraction module for retrieving relevant acoustic
attributes, a visual feature extraction module for extracting significant
high-level patterns, a mutual transformer for computing the correlations among
the generated features and fusing these features from multiple modalities, and
a detection layer for detecting depression using the fused feature
representations. The extensive experiments are performed using the multimodal
D-Vlog dataset, and the findings reveal that the developed multimodal
depression detection network surpasses the state-of-the-art by up to 17.37% for
F1-Score, demonstrating the greater performance of the proposed system. The
source code is accessible at
https://github.com/rezwanh001/Multimodal-Depression-Detection.

</details>


### [293] [3D Plant Root Skeleton Detection and Extraction](https://arxiv.org/abs/2508.08094)
*Jiakai Lin,Jinchang Zhang,Ge Jin,Wenzhan Song,Tianming Liu,Guoyu Lu*

Main category: cs.CV

TL;DR: A 3D root skeleton extraction method is introduced to model plant roots from images, overcoming challenges like complex architecture and lack of texture. It aids automated breeding robots by improving phenotypic analysis.


<details>
  <summary>Details</summary>
Motivation: Plant roots' complex 3D architecture and lack of visual data hinder precise modeling. 3D phenotypic information is crucial for studying genetic traits and root development.

Method: The method involves detecting and matching lateral roots, triangulating their skeletal structure, and integrating them with primary roots. It was tested on a complex root dataset.

Result: Extracted 3D root skeletons closely matched ground truth, validating the method's effectiveness.

Conclusion: This method enhances automated breeding by enabling precise 3D root analysis, improving efficiency and reducing manual intervention in modern agriculture.

Abstract: Plant roots typically exhibit a highly complex and dense architecture,
incorporating numerous slender lateral roots and branches, which significantly
hinders the precise capture and modeling of the entire root system.
Additionally, roots often lack sufficient texture and color information, making
it difficult to identify and track root traits using visual methods. Previous
research on roots has been largely confined to 2D studies; however, exploring
the 3D architecture of roots is crucial in botany. Since roots grow in real 3D
space, 3D phenotypic information is more critical for studying genetic traits
and their impact on root development. We have introduced a 3D root skeleton
extraction method that efficiently derives the 3D architecture of plant roots
from a few images. This method includes the detection and matching of lateral
roots, triangulation to extract the skeletal structure of lateral roots, and
the integration of lateral and primary roots. We developed a highly complex
root dataset and tested our method on it. The extracted 3D root skeletons
showed considerable similarity to the ground truth, validating the
effectiveness of the model. This method can play a significant role in
automated breeding robots. Through precise 3D root structure analysis, breeding
robots can better identify plant phenotypic traits, especially root structure
and growth patterns, helping practitioners select seeds with superior root
systems. This automated approach not only improves breeding efficiency but also
reduces manual intervention, making the breeding process more intelligent and
efficient, thus advancing modern agriculture.

</details>


### [294] [TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning](https://arxiv.org/abs/2508.08098)
*Junzhe Xu,Yuyang Yin,Xi Chen*

Main category: cs.CV

TL;DR: TBAC-UniImage integrates a Diffusion Model with a Multimodal Large Language Model (MLLM) for deeper multimodal understanding and generation, overcoming shallow connections and high computational costs in prior methods.


<details>
  <summary>Details</summary>
Motivation: Previous unified models either had shallow connections between MLLM and generators or required expensive pretraining. TBAC-UniImage aims to address these limitations.

Method: Uses representations from multiple MLLM layers as generative conditions for the diffusion model, treating the generator as a ladder guided by MLLM's hierarchical understanding.

Result: Achieves deeper and more fine-grained unification of understanding and generation.

Conclusion: TBAC-UniImage presents a novel, efficient paradigm for multimodal tasks by leveraging hierarchical MLLM representations.

Abstract: This paper introduces TBAC-UniImage, a novel unified model for multimodal
understanding and generation. We achieve this by deeply integrating a
pre-trained Diffusion Model, acting as a generative ladder, with a Multimodal
Large Language Model (MLLM). Previous diffusion-based unified models face two
primary limitations. One approach uses only the MLLM's final hidden state as
the generative condition. This creates a shallow connection, as the generator
is isolated from the rich, hierarchical representations within the MLLM's
intermediate layers. The other approach, pretraining a unified generative
architecture from scratch, is computationally expensive and prohibitive for
many researchers. To overcome these issues, our work explores a new paradigm.
Instead of relying on a single output, we use representations from multiple,
diverse layers of the MLLM as generative conditions for the diffusion model.
This method treats the pre-trained generator as a ladder, receiving guidance
from various depths of the MLLM's understanding process. Consequently,
TBAC-UniImage achieves a much deeper and more fine-grained unification of
understanding and generation.

</details>


### [295] [Hyperspectral Imaging](https://arxiv.org/abs/2508.08107)
*Danfeng Hong,Chenyu Li,Naoto Yokoya,Bing Zhang,Xiuping Jia,Antonio Plaza,Paolo Gamba,Jon Atli Benediktsson,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: A comprehensive overview of hyperspectral imaging (HSI), covering principles, methods, applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a detailed understanding of HSI's capabilities and challenges, fostering its adoption across diverse fields.

Method: Summarizes physical principles, sensor architectures, data acquisition, calibration, and analysis techniques (e.g., dimensionality reduction, deep learning).

Result: Highlights HSI's transformative potential in fields like agriculture, biomedicine, and security, while addressing persistent challenges.

Conclusion: HSI is poised for broader impact through advancements in hardware, AI, and cross-disciplinary collaboration.

Abstract: Hyperspectral imaging (HSI) is an advanced sensing modality that
simultaneously captures spatial and spectral information, enabling
non-invasive, label-free analysis of material, chemical, and biological
properties. This Primer presents a comprehensive overview of HSI, from the
underlying physical principles and sensor architectures to key steps in data
acquisition, calibration, and correction. We summarize common data structures
and highlight classical and modern analysis methods, including dimensionality
reduction, classification, spectral unmixing, and AI-driven techniques such as
deep learning. Representative applications across Earth observation, precision
agriculture, biomedicine, industrial inspection, cultural heritage, and
security are also discussed, emphasizing HSI's ability to uncover sub-visual
features for advanced monitoring, diagnostics, and decision-making. Persistent
challenges, such as hardware trade-offs, acquisition variability, and the
complexity of high-dimensional data, are examined alongside emerging solutions,
including computational imaging, physics-informed modeling, cross-modal fusion,
and self-supervised learning. Best practices for dataset sharing,
reproducibility, and metadata documentation are further highlighted to support
transparency and reuse. Looking ahead, we explore future directions toward
scalable, real-time, and embedded HSI systems, driven by sensor
miniaturization, self-supervised learning, and foundation models. As HSI
evolves into a general-purpose, cross-disciplinary platform, it holds promise
for transformative applications in science, technology, and society.

</details>


### [296] [GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking](https://arxiv.org/abs/2508.08117)
*Xudong Han,Pengcheng Fang,Yueying Tian,Jianhui Yu,Xiaohao Cai,Daniel Roggen,Philip Birch*

Main category: cs.CV

TL;DR: GRASPTrack integrates depth estimation and instance segmentation into MOT to address occlusions and depth ambiguity, using 3D point clouds and voxelization for robust tracking.


<details>
  <summary>Details</summary>
Motivation: Conventional MOT methods lack geometric awareness, struggling with occlusions and depth ambiguity.

Method: Combines monocular depth estimation and instance segmentation, voxelizes 3D point clouds for spatial association, and introduces depth-aware noise compensation and motion direction consistency in 3D.

Result: Achieves competitive performance on MOT17, MOT20, and DanceTrack benchmarks, improving robustness in complex scenes.

Conclusion: GRASPTrack effectively resolves MOT challenges by leveraging 3D geometric reasoning and adaptive noise compensation.

Abstract: Multi-object tracking (MOT) in monocular videos is fundamentally challenged
by occlusions and depth ambiguity, issues that conventional
tracking-by-detection (TBD) methods struggle to resolve owing to a lack of
geometric awareness. To address these limitations, we introduce GRASPTrack, a
novel depth-aware MOT framework that integrates monocular depth estimation and
instance segmentation into a standard TBD pipeline to generate high-fidelity 3D
point clouds from 2D detections, thereby enabling explicit 3D geometric
reasoning. These 3D point clouds are then voxelized to enable a precise and
robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To
further enhance tracking robustness, our approach incorporates Depth-aware
Adaptive Noise Compensation, which dynamically adjusts the Kalman filter
process noise based on occlusion severity for more reliable state estimation.
Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which
extends the motion direction consistency from the image plane into 3D space to
improve motion-based association cues, particularly for objects with complex
trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack
benchmarks demonstrate that our method achieves competitive performance,
significantly improving tracking robustness in complex scenes with frequent
occlusions and intricate motion patterns.

</details>


### [297] [Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control](https://arxiv.org/abs/2508.08134)
*Zeqian Long,Mingzhe Zheng,Kunyu Feng,Xinhua Zhang,Hongyu Liu,Harry Yang,Linfeng Zhang,Qifeng Chen,Yue Ma*

Main category: cs.CV

TL;DR: Follow-Your-Shape is a training-free, mask-free framework for precise object shape editing, preserving non-target content via Trajectory Divergence Maps and Scheduled KV Injection.


<details>
  <summary>Details</summary>
Motivation: Existing flow-based models struggle with large-scale shape transformations, often degrading background quality or failing to achieve intended edits.

Method: Uses Trajectory Divergence Maps (TDM) to localize editable regions and Scheduled KV Injection for stable editing. Introduces ReShapeBench for evaluation.

Result: Achieves superior editability and visual fidelity, especially in large-scale shape replacement tasks.

Conclusion: The method effectively addresses challenges in shape-aware editing, offering precise control and preservation of non-target content.

Abstract: While recent flow-based image editing models demonstrate general-purpose
capabilities across diverse tasks, they often struggle to specialize in
challenging scenarios -- particularly those involving large-scale shape
transformations. When performing such structural edits, these methods either
fail to achieve the intended shape change or inadvertently alter non-target
regions, resulting in degraded background quality. We propose
Follow-Your-Shape, a training-free and mask-free framework that supports
precise and controllable editing of object shapes while strictly preserving
non-target content. Motivated by the divergence between inversion and editing
trajectories, we compute a Trajectory Divergence Map (TDM) by comparing
token-wise velocity differences between the inversion and denoising paths. The
TDM enables precise localization of editable regions and guides a Scheduled KV
Injection mechanism that ensures stable and faithful editing. To facilitate a
rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120
new images and enriched prompt pairs specifically curated for shape-aware
editing. Experiments demonstrate that our method achieves superior editability
and visual fidelity, particularly in tasks requiring large-scale shape
replacement.

</details>


### [298] [FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting](https://arxiv.org/abs/2508.08136)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Huajie Wang,Shuting He*

Main category: cs.CV

TL;DR: FantasyStyle is a 3DGS-based style transfer framework using diffusion model distillation to address multi-view inconsistency and VGG feature reliance, achieving superior stylization quality.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS-based style transfer methods suffer from multi-view inconsistency and reliance on VGG features, leading to style conflicts, distortion, and content leakage.

Method: FantasyStyle introduces Multi-View Frequency Consistency and Controllable Stylized Distillation, leveraging diffusion model distillation and negative guidance to optimize 3D Gaussians.

Result: The method outperforms state-of-the-art approaches, achieving higher stylization quality and visual realism across diverse scenes and styles.

Conclusion: FantasyStyle effectively addresses key challenges in 3DGS-based style transfer, offering improved consistency and control over stylization.

Abstract: The success of 3DGS in generative and editing applications has sparked
growing interest in 3DGS-based style transfer. However, current methods still
face two major challenges: (1) multi-view inconsistency often leads to style
conflicts, resulting in appearance smoothing and distortion; and (2) heavy
reliance on VGG features, which struggle to disentangle style and content from
style images, often causing content leakage and excessive stylization. To
tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style
transfer framework, and the first to rely entirely on diffusion model
distillation. It comprises two key components: (1) \textbf{Multi-View Frequency
Consistency}. We enhance cross-view consistency by applying a 3D filter to
multi-view noisy latent, selectively reducing low-frequency components to
mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized
Distillation}. To suppress content leakage from style images, we introduce
negative guidance to exclude undesired content. In addition, we identify the
limitations of Score Distillation Sampling and Delta Denoising Score in 3D
style transfer and remove the reconstruction term accordingly. Building on
these insights, we propose a controllable stylized distillation that leverages
negative guidance to more effectively optimize the 3D Gaussians. Extensive
experiments demonstrate that our method consistently outperforms
state-of-the-art approaches, achieving higher stylization quality and visual
realism across various scenes and styles.

</details>


### [299] [Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization](https://arxiv.org/abs/2508.08141)
*Nicholas Klein,Hemlata Tak,James Fullwood,Krishna Regmi,Leonidas Spinoulas,Ganesh Sivaraman,Tianxiang Chen,Elie Khoury*

Main category: cs.CV

TL;DR: The paper addresses the challenge of detecting synthetic content in videos, focusing on deepfake classification and localization, achieving top performance in a competition.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement in visual and audio generation necessitates robust detection methods for synthetic content, especially for subtle, localized manipulations.

Method: The paper presents solutions for deepfake video classification and localization, tested in the ACM 1M Deepfakes Detection Challenge.

Result: Achieved best performance in temporal localization and top four in classification for the TestA split.

Conclusion: The proposed methods are effective for detecting and localizing deepfake manipulations in videos.

Abstract: The field of visual and audio generation is burgeoning with new
state-of-the-art methods. This rapid proliferation of new techniques
underscores the need for robust solutions for detecting synthetic content in
videos. In particular, when fine-grained alterations via localized
manipulations are performed in visual, audio, or both domains, these subtle
modifications add challenges to the detection algorithms. This paper presents
solutions for the problems of deepfake video classification and localization.
The methods were submitted to the ACM 1M Deepfakes Detection Challenge,
achieving the best performance in the temporal localization task and a top four
ranking in the classification task for the TestA split of the evaluation
dataset.

</details>


### [300] [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2508.08165)
*Yan Wang,Da-Wei Zhou,Han-Jia Ye*

Main category: cs.CV

TL;DR: The paper proposes TUNA, a method integrating task-specific and universal adapters for Class-Incremental Learning (CIL) to improve performance by leveraging both specialized and shared knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing CIL methods freeze pre-trained models and use lightweight modules, but incorrect module selection and overlooking shared knowledge lead to errors.

Method: TUNA trains task-specific adapters for crucial features and uses an entropy-based selection mechanism. It also constructs a universal adapter via fusion for shared discriminative features.

Result: Extensive experiments show TUNA achieves state-of-the-art performance on benchmark datasets.

Conclusion: TUNA effectively combines task-specific and universal adapters to enhance CIL performance by balancing specialized and general knowledge.

Abstract: Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Existing pre-trained model-based CIL
methods often freeze the pre-trained network and adapt to incremental tasks
using additional lightweight modules such as adapters. However, incorrect
module selection during inference hurts performance, and task-specific modules
often overlook shared general knowledge, leading to errors on distinguishing
between similar classes across tasks. To address the aforementioned challenges,
we propose integrating Task-Specific and Universal Adapters (TUNA) in this
paper. Specifically, we train task-specific adapters to capture the most
crucial features relevant to their respective tasks and introduce an
entropy-based selection mechanism to choose the most suitable adapter.
Furthermore, we leverage an adapter fusion strategy to construct a universal
adapter, which encodes the most discriminative features shared across tasks. We
combine task-specific and universal adapter predictions to harness both
specialized and general knowledge during inference. Extensive experiments on
various benchmark datasets demonstrate the state-of-the-art performance of our
approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA

</details>


### [301] [ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction](https://arxiv.org/abs/2508.08170)
*Chaojun Ni,Guosheng Zhao,Xiaofeng Wang,Zheng Zhu,Wenkang Qin,Xinze Chen,Guanghong Jia,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: ReconDreamer-RL integrates video diffusion priors into scene reconstruction to enhance autonomous driving training, reducing the sim2real gap and improving performance.


<details>
  <summary>Details</summary>
Motivation: The sim2real gap in autonomous driving simulations limits training effectiveness, especially for novel or corner-case scenarios.

Method: Proposes ReconDreamer-RL with ReconSimulator (video diffusion + kinematic modeling), Dynamic Adversary Agent (DAA) for corner cases, and Cousin Trajectory Generator (CTG) for diverse training data.

Result: Outperforms imitation learning with a 5x reduction in Collision Ratio.

Conclusion: ReconDreamer-RL effectively bridges the sim2real gap and enhances training for autonomous driving.

Abstract: Reinforcement learning for training end-to-end autonomous driving models in
closed-loop simulations is gaining growing attention. However, most simulation
environments differ significantly from real-world conditions, creating a
substantial simulation-to-reality (sim2real) gap. To bridge this gap, some
approaches utilize scene reconstruction techniques to create photorealistic
environments as a simulator. While this improves realistic sensor simulation,
these methods are inherently constrained by the distribution of the training
data, making it difficult to render high-quality sensor data for novel
trajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a
framework designed to integrate video diffusion priors into scene
reconstruction to aid reinforcement learning, thereby enhancing end-to-end
autonomous driving training. Specifically, in ReconDreamer-RL, we introduce
ReconSimulator, which combines the video diffusion prior for appearance
modeling and incorporates a kinematic model for physical modeling, thereby
reconstructing driving scenarios from real-world data. This narrows the
sim2real gap for closed-loop evaluation and reinforcement learning. To cover
more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),
which adjusts the trajectories of surrounding vehicles relative to the ego
vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).
Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue
of training data distribution, which is often biased toward simple
straight-line movements. Experiments show that ReconDreamer-RL improves
end-to-end autonomous driving training, outperforming imitation learning
methods with a 5x reduction in the Collision Ratio.

</details>


### [302] [CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data](https://arxiv.org/abs/2508.08173)
*Chongke Bi,Xin Gao,Jiangkang Deng,Guan*

Main category: cs.CV

TL;DR: CD-TVD combines contrastive learning and an improved diffusion model for 3D super-resolution, reducing reliance on large HR datasets.


<details>
  <summary>Details</summary>
Motivation: Existing super-resolution methods require extensive HR training data, limiting their use in diverse simulation scenarios.

Method: Uses contrastive learning and a diffusion-based model with local attention, fine-tuned on minimal HR data.

Result: Achieves accurate and efficient 3D super-resolution on fluid and atmospheric datasets.

Conclusion: CD-TVD advances data augmentation for large-scale simulations by minimizing HR data dependency.

Abstract: Large-scale scientific simulations require significant resources to generate
high-resolution time-varying data (TVD). While super-resolution is an efficient
post-processing strategy to reduce costs, existing methods rely on a large
amount of HR training data, limiting their applicability to diverse simulation
scenarios. To address this constraint, we proposed CD-TVD, a novel framework
that combines contrastive learning and an improved diffusion-based
super-resolution model to achieve accurate 3D super-resolution from limited
time-step high-resolution data. During pre-training on historical simulation
data, the contrastive encoder and diffusion superresolution modules learn
degradation patterns and detailed features of high-resolution and
low-resolution samples. In the training phase, the improved diffusion model
with a local attention mechanism is fine-tuned using only one newly generated
high-resolution timestep, leveraging the degradation knowledge learned by the
encoder. This design minimizes the reliance on large-scale high-resolution
datasets while maintaining the capability to recover fine-grained details.
Experimental results on fluid and atmospheric simulation datasets confirm that
CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a
significant advancement in data augmentation for large-scale scientific
simulations. The code is available at
https://github.com/Xin-Gao-private/CD-TVD.

</details>


### [303] [MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision](https://arxiv.org/abs/2508.08177)
*Zhonghao Yan,Muxi Diao,Yuxuan Yang,Jiayuan Xu,Kaizhou Zhang,Ruoyan Jing,Lele Yang,Yanxi Liu,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: The paper introduces Unified Medical Reasoning Grounding (UMRG), a new vision-language task for clinical reasoning and pixel-level grounding, along with the U-MRG-14K dataset and MedReasoner framework. MedReasoner separates reasoning from segmentation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current medical-grounding pipelines rely on supervised fine-tuning with explicit spatial hints, which fail to handle implicit clinical queries. This work aims to bridge this gap.

Method: The paper proposes MedReasoner, a modular framework with an MLLM reasoner optimized via reinforcement learning and a frozen segmentation expert for mask generation.

Result: MedReasoner achieves state-of-the-art performance on U-MRG-14K and generalizes well to unseen clinical queries.

Conclusion: The work highlights the potential of reinforcement learning for interpretable medical grounding, offering a robust solution for clinical applications.

Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and
treatment planning in medical imaging. While multimodal large language models
(MLLMs) combine visual perception with natural language, current
medical-grounding pipelines still rely on supervised fine-tuning with explicit
spatial hints, making them ill-equipped to handle the implicit queries common
in clinical practice. This work makes three core contributions. We first define
Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that
demands clinical reasoning and pixel-level grounding. Second, we release
U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside
implicit clinical queries and reasoning traces, spanning 10 modalities, 15
super-categories, and 108 specific categories. Finally, we introduce
MedReasoner, a modular framework that distinctly separates reasoning from
segmentation: an MLLM reasoner is optimized with reinforcement learning, while
a frozen segmentation expert converts spatial prompts into masks, with
alignment achieved through format and accuracy rewards. MedReasoner achieves
state-of-the-art performance on U-MRG-14K and demonstrates strong
generalization to unseen clinical queries, underscoring the significant promise
of reinforcement learning for interpretable medical grounding.

</details>


### [304] [3D Human Mesh Estimation from Single View RGBD](https://arxiv.org/abs/2508.08178)
*Ozhan Suat,Bedirhan Uguz,Batuhan Karagoz,Muhammed Can Keles,Emre Akbas*

Main category: cs.CV

TL;DR: A method called M$^3$ (Masked Mesh Modeling) is proposed for accurate 3D human mesh estimation from a single RGBD view, leveraging simulated depth data from MoCap datasets to overcome data scarcity.


<details>
  <summary>Details</summary>
Motivation: RGBD cameras are underutilized despite their affordability and depth data potential. Existing datasets for 3D human mesh estimation are small and lack diversity, making supervised training challenging.

Method: The method simulates depth data by projecting MoCap 3D meshes to a virtual camera, creating partial single-view meshes. A masked autoencoder completes these meshes, matching sensor depth to a template mesh during inference.

Result: M$^3$ achieves 16.8 mm and 22.0 mm PVE on SURREAL and CAPE datasets, outperforming methods using full-body point clouds. It also outperforms an RGB-based method by 18.4 mm on BEHAVE.

Conclusion: M$^3$ effectively recovers full-body meshes from partial RGBD data, demonstrating the value of depth data and simulated training for real-world applications.

Abstract: Despite significant progress in 3D human mesh estimation from RGB images;
RGBD cameras, offering additional depth data, remain underutilized. In this
paper, we present a method for accurate 3D human mesh estimation from a single
RGBD view, leveraging the affordability and widespread adoption of RGBD cameras
for real-world applications. A fully supervised approach for this problem,
requires a dataset with RGBD image and 3D mesh label pairs. However, collecting
such a dataset is costly and challenging, hence, existing datasets are small,
and limited in pose and shape diversity. To overcome this data scarcity, we
leverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D
meshes from the body models found in MoCap datasets, and create partial,
single-view versions of them by projection to a virtual camera. This simulates
the depth data provided by an RGBD camera from a single viewpoint. Then, we
train a masked autoencoder to complete the partial, single-view mesh. During
inference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',
matches the depth values coming from the sensor to vertices of a template human
mesh, which creates a partial, single-view mesh. We effectively recover parts
of the 3D human body mesh model that are not visible, resulting in a full body
mesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL
and CAPE datasets, respectively; outperforming existing methods that use
full-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE
dataset, outperforming a recently published RGB based method by 18.4 mm,
highlighting the usefulness of depth data. Code will be released.

</details>


### [305] [PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation](https://arxiv.org/abs/2508.08179)
*Sihan Zhao,Zixuan Wang,Tianyu Luan,Jia Jia,Wentao Zhu,Jiebo Luo,Junsong Yuan,Nan Xi*

Main category: cs.CV

TL;DR: The paper introduces PP-Motion, a data-driven metric for evaluating human motion fidelity by combining physical alignment and human perception, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing motion fidelity evaluation methods lack a robust, objective measure bridging human perception and physical feasibility.

Method: A physical labeling method calculates minimal modifications for physical alignment, producing fine-grained annotations. PP-Motion uses Pearson's correlation loss and perceptual fidelity loss for training.

Result: PP-Motion aligns better with both physical laws and human perception than prior work.

Conclusion: PP-Motion provides a more comprehensive and objective metric for evaluating human motion fidelity.

Abstract: Human motion generation has found widespread applications in AR/VR, film,
sports, and medical rehabilitation, offering a cost-effective alternative to
traditional motion capture systems. However, evaluating the fidelity of such
generated motions is a crucial, multifaceted task. Although previous approaches
have attempted at motion fidelity evaluation using human perception or physical
constraints, there remains an inherent gap between human-perceived fidelity and
physical feasibility. Moreover, the subjective and coarse binary labeling of
human perception further undermines the development of a robust data-driven
metric. We address these issues by introducing a physical labeling method. This
method evaluates motion fidelity by calculating the minimum modifications
needed for a motion to align with physical laws. With this approach, we are
able to produce fine-grained, continuous physical alignment annotations that
serve as objective ground truth. With these annotations, we propose PP-Motion,
a novel data-driven metric to evaluate both physical and perceptual fidelity of
human motion. To effectively capture underlying physical priors, we employ
Pearson's correlation loss for the training of our metric. Additionally, by
incorporating a human-based perceptual fidelity loss, our metric can capture
fidelity that simultaneously considers both human perception and physical
alignment. Experimental results demonstrate that our metric, PP-Motion, not
only aligns with physical laws but also aligns better with human perception of
motion fidelity than previous work.

</details>


### [306] [THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening](https://arxiv.org/abs/2508.08183)
*Hongkun Jin,Hongcheng Jiang,Zejun Zhang,Yuan Zhang,Jia Fu,Tingfeng Li,Kai Luo*

Main category: cs.CV

TL;DR: The paper proposes the Token-wise High-frequency Augmentation Transformer (THAT) to improve hyperspectral pansharpening by addressing redundancy and multi-scale feature modeling in Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: Transformer-based methods for hyperspectral pansharpening are limited by redundant tokens and lack of multi-scale feature modeling, failing to preserve high-frequency details and suffering from attention dispersion.

Method: THAT introduces Pivotal Token Selective Attention (PTSA) to prioritize informative tokens and a Multi-level Variance-aware Feed-forward Network (MVFN) to enhance high-frequency detail learning.

Result: Experiments show THAT achieves state-of-the-art performance with improved reconstruction quality and efficiency.

Conclusion: THAT effectively addresses the limitations of Vision Transformers in hyperspectral pansharpening, offering better high-frequency feature representation and token selection.

Abstract: Transformer-based methods have demonstrated strong potential in hyperspectral
pansharpening by modeling long-range dependencies. However, their effectiveness
is often limited by redundant token representations and a lack of multi-scale
feature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,
abundance sparsity) and spatial priors (e.g., non-local similarity), which are
critical for accurate reconstruction. From a spectral-spatial perspective,
Vision Transformers (ViTs) face two major limitations: they struggle to
preserve high-frequency components--such as material edges and texture
transitions--and suffer from attention dispersion across redundant tokens.
These issues stem from the global self-attention mechanism, which tends to
dilute high-frequency signals and overlook localized details. To address these
challenges, we propose the Token-wise High-frequency Augmentation Transformer
(THAT), a novel framework designed to enhance hyperspectral pansharpening
through improved high-frequency feature representation and token selection.
Specifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to
prioritize informative tokens and suppress redundancy; (2) a Multi-level
Variance-aware Feed-forward Network (MVFN) to enhance high-frequency detail
learning. Experiments on standard benchmarks show that THAT achieves
state-of-the-art performance with improved reconstruction quality and
efficiency. The source code is available at https://github.com/kailuo93/THAT.

</details>


### [307] [KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning](https://arxiv.org/abs/2508.08186)
*Md Meftahul Ferdaus,Mahdi Abdelguerfi,Elias Ioup,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: KARMA is a lightweight semantic segmentation framework for structural defects, achieving high accuracy with 97% fewer parameters than state-of-the-art methods, enabling real-time inspection.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in semantic segmentation of structural defects, such as variable appearances and class imbalance, while reducing computational overhead for real-time systems.

Method: Uses a Tiny Kolmogorov-Arnold Network (TiKAN) for efficient feature transformation, a feature pyramid with separable convolutions, and a static-dynamic prototype mechanism for imbalanced classes.

Result: Achieves competitive mean IoU with 0.959M parameters (97% reduction) and operates at 0.264 GFLOPS, suitable for real-time deployment.

Conclusion: KARMA offers a practical solution for automated infrastructure inspection by balancing accuracy and efficiency.

Abstract: Semantic segmentation of structural defects in civil infrastructure remains
challenging due to variable defect appearances, harsh imaging conditions, and
significant class imbalance. Current deep learning methods, despite their
effectiveness, typically require millions of parameters, rendering them
impractical for real-time inspection systems. We introduce KARMA
(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient
semantic segmentation framework that models complex defect patterns through
compositions of one-dimensional functions rather than conventional
convolutions. KARMA features three technical innovations: (1) a
parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging
low-rank factorization for KAN-based feature transformation; (2) an optimized
feature pyramid structure with separable convolutions for multi-scale defect
analysis; and (3) a static-dynamic prototype mechanism that enhances feature
representation for imbalanced classes. Extensive experiments on benchmark
infrastructure inspection datasets demonstrate that KARMA achieves competitive
or superior mean IoU performance compared to state-of-the-art approaches, while
using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).
Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for
real-time deployment, enabling practical automated infrastructure inspection
systems without compromising accuracy. The source code can be accessed at the
following URL: https://github.com/faeyelab/karma.

</details>


### [308] [Reinforcement Learning in Vision: A Survey](https://arxiv.org/abs/2508.08189)
*Weijia Wu,Chen Gao,Joya Chen,Kevin Qinghong Lin,Qingwei Meng,Yiming Zhang,Yuke Qiu,Hong Zhou,Mike Zheng Shou*

Main category: cs.CV

TL;DR: A survey synthesizing recent advances in visual reinforcement learning (RL), covering problem formalization, policy-optimization evolution, thematic pillars, and evaluation protocols, with open challenges highlighted.


<details>
  <summary>Details</summary>
Motivation: To provide a coherent overview of the rapidly expanding field of visual RL, organizing works and identifying future directions.

Method: Critical synthesis of over 200 works, categorized into four pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. Examines algorithmic design, reward engineering, and benchmarks.

Result: Identifies trends like curriculum-driven training and unified reward modeling, alongside evaluation protocols and open challenges (e.g., sample efficiency, generalization).

Conclusion: Offers a structured map of visual RL, highlighting promising research directions and providing resources for further exploration.

Abstract: Recent advances at the intersection of reinforcement learning (RL) and visual
intelligence have enabled agents that not only perceive complex visual scenes
but also reason, generate, and act within them. This survey offers a critical
and up-to-date synthesis of the field. We first formalize visual RL problems
and trace the evolution of policy-optimization strategies from RLHF to
verifiable reward paradigms, and from Proximal Policy Optimization to Group
Relative Policy Optimization. We then organize more than 200 representative
works into four thematic pillars: multi-modal large language models, visual
generation, unified model frameworks, and vision-language-action models. For
each pillar we examine algorithmic design, reward engineering, benchmark
progress, and we distill trends such as curriculum-driven training,
preference-aligned diffusion, and unified reward modeling. Finally, we review
evaluation protocols spanning set-level fidelity, sample-level preference, and
state-level stability, and we identify open challenges that include sample
efficiency, generalization, and safe deployment. Our goal is to provide
researchers and practitioners with a coherent map of the rapidly expanding
landscape of visual RL and to highlight promising directions for future
inquiry. Resources are available at:
https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.

</details>


### [309] [Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model](https://arxiv.org/abs/2508.08199)
*Peiqi He,Zhenhao Zhang,Yixiang Zhang,Xiongjun Zhao,Shaoliang Peng*

Main category: cs.CV

TL;DR: Spatial-ORMLLM is a novel large vision-language model for 3D spatial reasoning in operating rooms, using only RGB data to infer volumetric and semantic cues, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for spatial modeling in ORs rely on multimodal datasets, which are hard to obtain, and 2D data, which lacks fine-grained details. Spatial-ORMLLM addresses these gaps.

Method: The model uses a Spatial-Enhanced Feature Fusion Block to integrate 2D inputs with 3D spatial knowledge, combining them in an end-to-end MLLM framework without extra annotations.

Result: Spatial-ORMLLM achieves state-of-the-art performance on benchmark datasets and generalizes well to unseen surgical scenarios.

Conclusion: The model provides a robust solution for 3D spatial reasoning in ORs, enhancing clinical tasks with detailed spatial context using only RGB data.

Abstract: Precise spatial modeling in the operating room (OR) is foundational to many
clinical tasks, supporting intraoperative awareness, hazard avoidance, and
surgical decision-making. While existing approaches leverage large-scale
multimodal datasets for latent-space alignment to implicitly learn spatial
relationships, they overlook the 3D capabilities of MLLMs. However, this
approach raises two issues: (1) Operating rooms typically lack multiple video
and audio sensors, making multimodal 3D data difficult to obtain; (2) Training
solely on readily available 2D data fails to capture fine-grained details in
complex scenes. To address this gap, we introduce Spatial-ORMLLM, the first
large vision-language model for 3D spatial reasoning in operating rooms using
only RGB modality to infer volumetric and semantic cues, enabling downstream
medical tasks with detailed and holistic spatial context. Spatial-ORMLLM
incorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D
modality inputs with rich 3D spatial knowledge extracted by the estimation
algorithm and then feeds the combined features into the visual tower. By
employing a unified end-to-end MLLM framework, it combines powerful spatial
features with textual features to deliver robust 3D scene reasoning without any
additional expert annotations or sensor inputs. Experiments on multiple
benchmark clinical datasets demonstrate that Spatial-ORMLLM achieves
state-of-the-art performance and generalizes robustly to previously unseen
surgical scenarios and downstream tasks.

</details>


### [310] [SAGOnline: Segment Any Gaussians Online](https://arxiv.org/abs/2508.08219)
*Wentao Sun,Quanyun Wu,Hanqing Xu,Kyle Gao,Zhengsen Xu,Yiping Chen,Dedong Zhang,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: SAGOnline is a lightweight, zero-shot framework for real-time 3D segmentation in Gaussian scenes, addressing computational inefficiency and multi-object tracking challenges.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D segmentation in Gaussian scenes are computationally expensive, lack spatial reasoning, and cannot track multiple objects simultaneously.

Method: SAGOnline integrates video foundation models for 2D mask propagation and uses a GPU-accelerated 3D mask generation algorithm for Gaussian-level instance labeling.

Result: Achieves state-of-the-art performance (92.7% mIoU on NVOS, 95.2% on Spin-NeRF) and 15--1500x faster inference speed (27 ms/frame).

Conclusion: SAGOnline enables real-time 3D scene understanding, advancing AR/VR and robotics applications.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit
3D scene representation, yet achieving efficient and consistent 3D segmentation
remains challenging. Current methods suffer from prohibitive computational
costs, limited 3D spatial reasoning, and an inability to track multiple objects
simultaneously. We present Segment Any Gaussians Online (SAGOnline), a
lightweight and zero-shot framework for real-time 3D segmentation in Gaussian
scenes that addresses these limitations through two key innovations: (1) a
decoupled strategy that integrates video foundation models (e.g., SAM2) for
view-consistent 2D mask propagation across synthesized views; and (2) a
GPU-accelerated 3D mask generation and Gaussian-level instance labeling
algorithm that assigns unique identifiers to 3D primitives, enabling lossless
multi-object tracking and segmentation across views. SAGOnline achieves
state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)
benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times
in inference speed (27 ms/frame). Qualitative results demonstrate robust
multi-object segmentation and tracking in complex scenes. Our contributions
include: (i) a lightweight and zero-shot framework for 3D segmentation in
Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling
simultaneous segmentation and tracking, and (iii) the effective adaptation of
2D video foundation models to the 3D domain. This work allows real-time
rendering and 3D scene understanding, paving the way for practical AR/VR and
robotic applications.

</details>


### [311] [Learning User Preferences for Image Generation Model](https://arxiv.org/abs/2508.08220)
*Wenyi Mo,Ying Ba,Tianyu Zhang,Yalong Bai,Biye Li*

Main category: cs.CV

TL;DR: The paper proposes a Multimodal Large Language Model-based approach with contrastive preference loss and preference tokens to predict personalized user preferences, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for user preference prediction often overlook individual variability and dynamic tastes, relying on general preferences or static profiles.

Method: Uses Multimodal Large Language Models with contrastive preference loss to distinguish likes/dislikes and preference tokens for shared interest representations.

Result: The model achieves higher accuracy in preference prediction and identifies users with similar aesthetic inclinations.

Conclusion: The approach effectively captures dynamic, multifaceted user preferences and improves personalized image generation guidance.

Abstract: User preference prediction requires a comprehensive and accurate
understanding of individual tastes. This includes both surface-level
attributes, such as color and style, and deeper content-related aspects, such
as themes and composition. However, existing methods typically rely on general
human preferences or assume static user profiles, often neglecting individual
variability and the dynamic, multifaceted nature of personal taste. To address
these limitations, we propose an approach built upon Multimodal Large Language
Models, introducing contrastive preference loss and preference tokens to learn
personalized user preferences from historical interactions. The contrastive
preference loss is designed to effectively distinguish between user ''likes''
and ''dislikes'', while the learnable preference tokens capture shared interest
representations among existing users, enabling the model to activate
group-specific preferences and enhance consistency across similar users.
Extensive experiments demonstrate our model outperforms other methods in
preference prediction accuracy, effectively identifying users with similar
aesthetic inclinations and providing more precise guidance for generating
images that align with individual tastes. The project page is
\texttt{https://learn-user-pref.github.io/}.

</details>


### [312] [OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.08227)
*Zhiqiang Wu,Zhaomang Sun,Tong Zhou,Bingtao Fu,Ji Cong,Yitong Dong,Huaqi Zhang,Xuan Tang,Mingsong Chen,Xian Wei*

Main category: cs.CV

TL;DR: OMGSR is a framework for one-step Real-ISR using DDPM/FM models, injecting LQ image latents at mid-timesteps to bridge the gap with Gaussian noise, achieving high-quality results.


<details>
  <summary>Details</summary>
Motivation: The gap between LQ image latent distribution and Gaussian noisy latent distribution limits generative prior utilization in Real-ISR.

Method: OMGSR injects LQ latents at mid-timesteps, uses Latent Distribution Refinement loss, and Overlap-Chunked LPIPS/GAN loss to avoid artifacts.

Result: OMGSR variants (S/F) excel in 512-resolution metrics; OMGSR-F dominates reference metrics and scales to 1k/2k resolutions.

Conclusion: OMGSR effectively bridges the latent distribution gap, enabling high-quality Real-ISR with DDPM/FM models.

Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)
generative models show promising potential for one-step Real-World Image
Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a
Low-Quality (LQ) image latent distribution at the initial timestep. However, a
fundamental gap exists between the LQ image latent distribution and the
Gaussian noisy latent distribution, limiting the effective utilization of
generative priors. We observe that the noisy latent distribution at DDPM/FM
mid-timesteps aligns more closely with the LQ image latent distribution. Based
on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a
universal framework applicable to DDPM/FM-based generative models. OMGSR
injects the LQ image latent distribution at a pre-computed mid-timestep,
incorporating the proposed Latent Distribution Refinement loss to alleviate the
latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to
eliminate checkerboard artifacts in image generation. Within this framework, we
instantiate OMGSR for DDPM/FM-based generative models with two variants:
OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate
that OMGSR-S/F achieves balanced/excellent performance across quantitative and
qualitative metrics at 512-resolution. Notably, OMGSR-F establishes
overwhelming dominance in all reference metrics. We further train a
1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which
yields excellent results, especially in the details of the image generation. We
also generate 2k-resolution images by the 1k-resolution OMGSR-F using our
two-stage Tiled VAE & Diffusion.

</details>


### [313] [Cut2Next: Generating Next Shot via In-Context Tuning](https://arxiv.org/abs/2508.08244)
*Jingwen He,Hongbo Liu,Jiajun Li,Ziqi Huang,Yu Qiao,Wanli Ouyang,Ziwei Liu*

Main category: cs.CV

TL;DR: Cut2Next introduces Next Shot Generation (NSG) using a Diffusion Transformer (DiT) with Hierarchical Multi-Prompting to generate cinematically coherent shots, outperforming current methods in visual consistency and narrative flow.


<details>
  <summary>Details</summary>
Motivation: Current methods focus on visual consistency but lack narrative sophistication and cinematic integrity, limiting their effectiveness for storytelling.

Method: Cut2Next uses a Diffusion Transformer (DiT) with Hierarchical Multi-Prompting (Relational and Individual Prompts) and architectural innovations like Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM).

Result: Cut2Next excels in visual consistency, text fidelity, and adherence to editing patterns, as validated by user studies.

Conclusion: Cut2Next successfully bridges the gap in cinematic continuity and narrative flow, generating high-quality, narratively expressive shots.

Abstract: Effective multi-shot generation demands purposeful, film-like transitions and
strict cinematic continuity. Current methods, however, often prioritize basic
visual consistency, neglecting crucial editing patterns (e.g., shot/reverse
shot, cutaways) that drive narrative flow for compelling storytelling. This
yields outputs that may be visually coherent but lack narrative sophistication
and true cinematic integrity. To bridge this, we introduce Next Shot Generation
(NSG): synthesizing a subsequent, high-quality shot that critically conforms to
professional editing patterns while upholding rigorous cinematic continuity.
Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs
in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This
strategy uses Relational Prompts to define overall context and inter-shot
editing styles. Individual Prompts then specify per-shot content and
cinematographic attributes. Together, these guide Cut2Next to generate
cinematically appropriate next shots. Architectural innovations, Context-Aware
Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further
integrate these diverse signals without introducing new parameters. We
construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with
hierarchical prompts, and introduce CutBench for evaluation. Experiments show
Cut2Next excels in visual consistency and text fidelity. Crucially, user
studies reveal a strong preference for Cut2Next, particularly for its adherence
to intended editing patterns and overall cinematic continuity, validating its
ability to generate high-quality, narratively expressive, and cinematically
coherent subsequent shots.

</details>


### [314] [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2508.08248)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAvatar introduces an end-to-end video diffusion transformer for infinite-length, high-quality avatar videos with improved audio synchronization and identity consistency.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to generate long videos with natural audio sync and identity consistency due to audio modeling limitations.

Method: StableAvatar uses a Time-step-aware Audio Adapter and Audio Native Guidance Mechanism to prevent error accumulation and enhance synchronization. A Dynamic Weighted Sliding-window Strategy ensures smoothness.

Result: Benchmark experiments show StableAvatar's effectiveness in generating high-quality, infinite-length videos.

Conclusion: StableAvatar addresses key limitations in audio-driven avatar video generation, enabling seamless, long-form video synthesis.

Abstract: Current diffusion models for audio-driven avatar video generation struggle to
synthesize long videos with natural audio synchronization and identity
consistency. This paper presents StableAvatar, the first end-to-end video
diffusion transformer that synthesizes infinite-length high-quality videos
without post-processing. Conditioned on a reference image and audio,
StableAvatar integrates tailored training and inference modules to enable
infinite-length video generation. We observe that the main reason preventing
existing models from generating long videos lies in their audio modeling. They
typically rely on third-party off-the-shelf extractors to obtain audio
embeddings, which are then directly injected into the diffusion model via
cross-attention. Since current diffusion backbones lack any audio-related
priors, this approach causes severe latent distribution error accumulation
across video clips, leading the latent distribution of subsequent segments to
drift away from the optimal distribution gradually. To address this,
StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents
error accumulation via time-step-aware modulation. During inference, we propose
a novel Audio Native Guidance Mechanism to further enhance the audio
synchronization by leveraging the diffusion's own evolving joint audio-latent
prediction as a dynamic guidance signal. To enhance the smoothness of the
infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy
that fuses latent over time. Experiments on benchmarks show the effectiveness
of StableAvatar both qualitatively and quantitatively.

</details>


### [315] [ReferSplat: Referring Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2508.08252)
*Shuting He,Guangquan Jie,Changshuo Wang,Yun Zhou,Shuming Hu,Guanbin Li,Henghui Ding*

Main category: cs.CV

TL;DR: R3DGS is a new task for segmenting 3D objects using natural language descriptions, addressing challenges in 3D multi-modal understanding and spatial relationships. The paper introduces the Ref-LERF dataset and the ReferSplat framework, achieving top performance.


<details>
  <summary>Details</summary>
Motivation: Advancing embodied AI by enabling segmentation of 3D objects based on natural language descriptions, even when objects are occluded or not directly visible.

Method: Proposes ReferSplat, a framework that models 3D Gaussian points with natural language expressions in a spatially aware paradigm.

Result: ReferSplat achieves state-of-the-art performance on R3DGS and 3D open-vocabulary segmentation benchmarks.

Conclusion: The work highlights the importance of 3D multi-modal understanding and spatial relationships, providing a dataset and framework to advance research in this area.

Abstract: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task
that aims to segment target objects in a 3D Gaussian scene based on natural
language descriptions, which often contain spatial relationships or object
attributes. This task requires the model to identify newly described objects
that may be occluded or not directly visible in a novel view, posing a
significant challenge for 3D multi-modal understanding. Developing this
capability is crucial for advancing embodied AI. To support research in this
area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that
3D multi-modal understanding and spatial relationship modeling are key
challenges for R3DGS. To address these challenges, we propose ReferSplat, a
framework that explicitly models 3D Gaussian points with natural language
expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art
performance on both the newly proposed R3DGS task and 3D open-vocabulary
segmentation benchmarks. Dataset and code are available at
https://github.com/heshuting555/ReferSplat.

</details>


### [316] [Learning an Implicit Physics Model for Image-based Fluid Simulation](https://arxiv.org/abs/2508.08254)
*Emily Yue-Ting Jia,Jiageng Mao,Zhiyuan Gao,Yajie Zhao,Yue Wang*

Main category: cs.CV

TL;DR: A novel method for generating 4D scenes (motion + 3D geometry) from a single image, using physics-informed neural networks to ensure realistic animations.


<details>
  <summary>Details</summary>
Motivation: Humans can imagine 4D scenes from still images due to accumulated observations and physics intuition. The paper aims to replicate this in neural networks, addressing the limitations of existing methods that produce unrealistic animations.

Method: Proposes a physics-informed neural network predicting motion for surface points, guided by physical principles (e.g., Navier-Stokes equations). Uses feature-based 3D Gaussians for appearance, animated and rendered from any camera perspective.

Result: Produces physically plausible animations, outperforming existing methods.

Conclusion: The approach effectively bridges the gap between single-image input and physics-consistent 4D scene generation, with practical applications in fluid imagery.

Abstract: Humans possess an exceptional ability to imagine 4D scenes, encompassing both
motion and 3D geometry, from a single still image. This ability is rooted in
our accumulated observations of similar scenes and an intuitive understanding
of physics. In this paper, we aim to replicate this capacity in neural
networks, specifically focusing on natural fluid imagery. Existing methods for
this task typically employ simplistic 2D motion estimators to animate the
image, leading to motion predictions that often defy physical principles,
resulting in unrealistic animations. Our approach introduces a novel method for
generating 4D scenes with physics-consistent animation from a single image. We
propose the use of a physics-informed neural network that predicts motion for
each surface point, guided by a loss term derived from fundamental physical
principles, including the Navier-Stokes equations. To capture appearance, we
predict feature-based 3D Gaussians from the input image and its estimated
depth, which are then animated using the predicted motions and rendered from
any desired camera perspective. Experimental results highlight the
effectiveness of our method in producing physically plausible animations,
showcasing significant performance improvements over existing methods. Our
project page is https://physfluid.github.io/ .

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [317] [Digital generation of the 3-D pore architecture of isotropic membranes using 2-D cross-sectional scanning electron microscopy images](https://arxiv.org/abs/2508.06664)
*Sima Zeinali Danalou,Hooman Chamani,Arash Rabbani,Patrick C. Lee,Jason Hattrick Simpers,Jay R Werber*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces an improved algorithm for 3-D pore network reconstruction from 2-D SEM images, addressing limitations of conventional tomography and enhancing accuracy in replicating pore geometries.


<details>
  <summary>Details</summary>
Motivation: The inability of 2-D SEM to resolve 3-D pore architecture and interconnectivity, critical for membrane performance, and the high cost and complexity of conventional tomographic methods.

Method: Development of an enhanced reconstruction algorithm from a single 2-D SEM image, maintaining statistical properties and accurately reproducing pore morphologies.

Result: High-fidelity 3-D reconstruction of a commercial microfiltration membrane, validated with X-ray tomography, showing excellent agreement and superior resolution.

Conclusion: The method is effective for isotropic porous membranes and offers a cost-efficient alternative to tomography, though further work is needed for anisotropic membranes.

Abstract: A major limitation of two-dimensional scanning electron microscopy (SEM) in
imaging porous membranes is its inability to resolve three-dimensional pore
architecture and interconnectivity, which are critical factors governing
membrane performance. Although conventional tomographic 3-D reconstruction
techniques can address this limitation, they are often expensive, technically
challenging, and not widely accessible. We previously introduced a
proof-of-concept method for reconstructing a membrane's 3-D pore network from a
single 2-D SEM image, yielding statistically equivalent results to those
obtained from 3-D tomography. However, this initial approach struggled to
replicate the diverse pore geometries commonly observed in real membranes. In
this study, we advance the methodology by developing an enhanced reconstruction
algorithm that not only maintains essential statistical properties (e.g., pore
size distribution), but also accurately reproduces intricate pore morphologies.
Applying this technique to a commercial microfiltration membrane, we generated
a high-fidelity 3-D reconstruction and derived key membrane properties.
Validation with X-ray tomography data revealed excellent agreement in
structural metrics, with our SEM-based approach achieving superior resolution
in resolving fine pore features. The tool can be readily applied to isotropic
porous membrane structures of any pore size, as long as those pores can be
visualized by SEM. Further work is needed for 3-D structure generation of
anisotropic membranes.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [318] [Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection](https://arxiv.org/abs/2508.07201)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: The paper introduces RAGCL, a graph contrastive learning method for rumor detection, addressing the wide structure of rumor propagation trees (RPTs) with adaptive augmentation based on node centrality.


<details>
  <summary>Details</summary>
Motivation: Existing models assume deep RPT structures, but real-world data shows wide structures with shallow nodes. The goal is to focus learning on intensive substructures.

Method: Proposes RAGCL with adaptive view augmentation (node dropping, attribute masking, edge dropping) guided by centrality scores. Three principles guide augmentation: exempt root nodes, retain deep reply nodes, preserve lower-level nodes in deep sections.

Result: RAGCL outperforms state-of-the-art methods on four benchmark datasets.

Conclusion: The work highlights the wide-structure nature of RPTs and offers an effective graph contrastive learning approach with principles applicable to other tree-structured graphs.

Abstract: Rumor detection on social media has become increasingly important. Most
existing graph-based models presume rumor propagation trees (RPTs) have deep
structures and learn sequential stance features along branches. However,
through statistical analysis on real-world datasets, we find RPTs exhibit wide
structures, with most nodes being shallow 1-level replies. To focus learning on
intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning
(RAGCL) method with adaptive view augmentation guided by node centralities. We
summarize three principles for RPT augmentation: 1) exempt root nodes, 2)
retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We
employ node dropping, attribute masking and edge dropping with probabilities
from centrality-based importance scores to generate views. A graph contrastive
objective then learns robust rumor representations. Extensive experiments on
four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.
Our work reveals the wide-structure nature of RPTs and contributes an effective
graph contrastive learning approach tailored for rumor detection through
principled adaptive augmentation. The proposed principles and augmentation
techniques can potentially benefit other applications involving tree-structured
graphs.

</details>


### [319] [Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning](https://arxiv.org/abs/2508.07205)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: The paper addresses data scarcity and imbalance in rumor detection by proposing AD-GSCL, a framework using anomaly detection and graph contrastive learning, validated on large-scale Weibo and Twitter datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world social media data is imbalanced, with rumors being a minority. Existing methods treat it as a balanced task, limiting performance.

Method: Constructed large-scale datasets from Weibo and Twitter, analyzed domain distributions, and proposed AD-GSCL, a framework combining anomaly detection and graph contrastive learning.

Result: AD-GSCL outperforms under balanced, imbalanced, and few-shot conditions, showing effectiveness in real-world scenarios.

Conclusion: The study highlights the need for anomaly detection in rumor detection and demonstrates AD-GSCL's superiority in handling imbalanced data.

Abstract: Current rumor detection methods based on propagation structure learning
predominately treat rumor detection as a class-balanced classification task on
limited labeled data. However, real-world social media data exhibits an
imbalanced distribution with a minority of rumors among massive regular posts.
To address the data scarcity and imbalance issues, we construct two large-scale
conversation datasets from Weibo and Twitter and analyze the domain
distributions. We find obvious differences between rumor and non-rumor
distributions, with non-rumors mostly in entertainment domains while rumors
concentrate in news, indicating the conformity of rumor detection to an anomaly
detection paradigm. Correspondingly, we propose the Anomaly Detection framework
with Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats
unlabeled data as non-rumors and adapts graph contrastive learning for rumor
detection. Extensive experiments demonstrate AD-GSCL's superiority under
class-balanced, imbalanced, and few-shot conditions. Our findings provide
valuable insights for real-world rumor detection featuring imbalanced data
distributions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [320] [Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer](https://arxiv.org/abs/2402.16868)
*Peigen Ye,Yaping Sun,Shumin Yao,Hao Chen,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: A robust codebook-assisted image semantic communication system is proposed, improving performance against channel noise by jointly constructing semantic codec and codebook, and designing a vector-to-index transformer.


<details>
  <summary>Details</summary>
Motivation: Current codebook-based semantic communication systems suffer from noise susceptibility due to mismatched semantic relations and code indices.

Method: Jointly construct semantic codec and codebook, then design a vector-to-index transformer to mitigate noise effects and enable image generation.

Result: The system outperforms JPEG+LDPC and traditional JSCC methods in visual perception and robustness.

Conclusion: The proposed method enhances robustness and image quality in semantic communication, validated by numerical results and generated images.

Abstract: Codebook-based generative semantic communication attracts increasing
attention, since only indices are required to be transmitted when the codebook
is shared between transmitter and receiver. However, due to the fact that the
semantic relations among code vectors are not necessarily related to the
distance of the corresponding code indices, the performance of the
codebook-enabled semantic communication system is susceptible to the channel
noise. Thus, how to improve the system robustness against the noise requires
careful design. This paper proposes a robust codebook-assisted image semantic
communication system, where semantic codec and codebook are first jointly
constructed, and then vector-to-index transformer is designed guided by the
codebook to eliminate the effects of channel noise, and achieve image
generation. Thanks to the assistance of the high-quality codebook to the
Transformer, the generated images at the receiver outperform those of the
compared methods in terms of visual perception. In the end, numerical results
and generated images demonstrate the advantages of the generative semantic
communication method over JPEG+LDPC and traditional joint source channel coding
(JSCC) methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [321] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: Mo2oM introduces soft clustering for microservice extraction, improving modularity and reducing communication overhead compared to hard clustering methods.


<details>
  <summary>Details</summary>
Motivation: Existing microservice extraction methods use hard clustering, leading to increased inter-service coupling and reduced cohesion. Mo2oM addresses this by allowing probabilistic component assignment to multiple microservices.

Method: Mo2oM combines deep semantic embeddings and structural dependencies from method-call graphs, using a graph neural network for soft clustering.

Result: Mo2oM improves structural modularity by 40.97%, reduces inter-service calls by 58%, enhances interface numbers by 26.16%, and balances service sizes by 38.96%.

Conclusion: Mo2oM outperforms existing methods, offering a more flexible and effective approach to microservice extraction.

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [322] [Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification](https://arxiv.org/abs/2508.06535)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: The study uses transfer learning with CNNs to classify ALL from blood smear images, achieving high accuracy with EfficientNet-B3.


<details>
  <summary>Details</summary>
Motivation: Early and accurate diagnosis of ALL is crucial for effective treatment.

Method: Applied data augmentation to balance the dataset and evaluated ResNet50, ResNet101, and EfficientNet variants.

Result: EfficientNet-B3 achieved the best performance (F1-score: 94.30%, accuracy: 92.02%, AUC: 94.79%).

Conclusion: Combining data augmentation with transfer learning, especially EfficientNet-B3, enhances diagnostic accuracy for ALL.

Abstract: Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral
blood smear images is essential for early diagnosis and effective treatment
planning. This study investigates the use of transfer learning with pretrained
convolutional neural networks (CNNs) to improve diagnostic performance. To
address the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL
images, we applied extensive data augmentation techniques to create a balanced
training set of 10,000 images per class. We evaluated several models, including
ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3
achieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,
andAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.
Thesefindings demonstrate the effectiveness of combining data augmentation with
advanced transfer learning models, particularly EfficientNet-B3, in developing
accurate and robust diagnostic tools for hematologic malignancy detection.

</details>


### [323] [LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification](https://arxiv.org/abs/2508.06874)
*Shisheng Zhang,Ramtin Gharleghi,Sonit Singh,Daniel Moses,Dona Adikari,Arcot Sowmya,Susann Beier*

Main category: eess.IV

TL;DR: A lightweight method combining anatomical knowledge and rule-based topology constraints for automated coronary artery labelling, outperforming traditional and deep-learning approaches.


<details>
  <summary>Details</summary>
Motivation: Coronary artery disease (CAD) is a leading global cause of death, and while CTCA is a key diagnostic tool, manual analysis is labor-intensive. Current automated methods lack efficiency or clinical insight.

Method: Proposes a lightweight method integrating anatomical knowledge and rule-based topology constraints for coronary artery labelling.

Result: Achieves state-of-the-art performance on benchmark datasets.

Conclusion: Offers a promising, efficient alternative for automated coronary artery labelling, balancing clinical knowledge and computational efficiency.

Abstract: Coronary artery disease (CAD) remains the leading cause of death globally,
with computed tomography coronary angiography (CTCA) serving as a key
diagnostic tool. However, coronary arterial analysis using CTCA, such as
identifying artery-specific features from computational modelling, is
labour-intensive and time-consuming. Automated anatomical labelling of coronary
arteries offers a potential solution, yet the inherent anatomical variability
of coronary trees presents a significant challenge. Traditional knowledge-based
labelling methods fall short in leveraging data-driven insights, while recent
deep-learning approaches often demand substantial computational resources and
overlook critical clinical knowledge. To address these limitations, we propose
a lightweight method that integrates anatomical knowledge with rule-based
topology constraints for effective coronary artery labelling. Our approach
achieves state-of-the-art performance on benchmark datasets, providing a
promising alternative for automated coronary artery labelling.

</details>


### [324] [Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning](https://arxiv.org/abs/2508.06891)
*Melika Filvantorkaman,Mohsen Piri,Maral Filvan Torkaman,Ashkan Zabihi,Hamidreza Moradi*

Main category: eess.IV

TL;DR: An ensemble deep learning framework combining MobileNetV2 and DenseNet121 CNNs with soft voting achieves high accuracy (91.7%) in classifying brain tumors, enhanced by explainable AI (Grad-CAM++) and clinical rule overlays for interpretability.


<details>
  <summary>Details</summary>
Motivation: Accurate and interpretable brain tumor classification from MRI is crucial for diagnosis and treatment planning.

Method: Ensemble of MobileNetV2 and DenseNet121 CNNs with soft voting, trained on the Figshare dataset using 5-fold cross-validation, integrated with Grad-CAM++ for saliency visualization and Clinical Decision Rule Overlay (CDRO).

Result: Superior performance (accuracy 91.7%, precision 91.9%, recall 91.7%, F1-score 91.6%), strong spatial alignment with expert annotations (Dice up to 0.88, IoU up to 0.78), and high clinical interpretability scores (mean 4.4 for usefulness).

Conclusion: The framework provides a robust, interpretable, and clinically relevant solution for automated brain tumor classification, advancing deep learning in neurodiagnostics.

Abstract: Accurate and interpretable classification of brain tumors from magnetic
resonance imaging (MRI) is critical for effective diagnosis and treatment
planning. This study presents an ensemble-based deep learning framework that
combines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using
a soft voting strategy to classify three common brain tumor types: glioma,
meningioma, and pituitary adenoma. The models were trained and evaluated on the
Figshare dataset using a stratified 5-fold cross-validation protocol. To
enhance transparency and clinical trust, the framework integrates an
Explainable AI (XAI) module employing Grad-CAM++ for class-specific saliency
visualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that
maps predictions to established radiological heuristics. The ensemble
classifier achieved superior performance compared to individual CNNs, with an
accuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.
Grad-CAM++ visualizations revealed strong spatial alignment between model
attention and expert-annotated tumor regions, supported by Dice coefficients up
to 0.88 and IoU scores up to 0.78. Clinical rule activation further validated
model predictions in cases with distinct morphological features. A
human-centered interpretability assessment involving five board-certified
radiologists yielded high Likert-scale scores for both explanation usefulness
(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the
framework's clinical relevance. Overall, the proposed approach offers a robust,
interpretable, and generalizable solution for automated brain tumor
classification, advancing the integration of deep learning into clinical
neurodiagnostics.

</details>


### [325] [Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments](https://arxiv.org/abs/2508.07006)
*Gian Mario Favero,Ge Ya Luo,Nima Fathi,Justin Szeto,Douglas L. Arnold,Brennan Nichyporuk,Chris Pal,Tal Arbel*

Main category: eess.IV

TL;DR: A treatment-aware spatio-temporal diffusion model predicts future lesion evolution in Multiple Sclerosis (MS) using multi-modal patient data, demonstrating potential for clinical applications.


<details>
  <summary>Details</summary>
Motivation: To improve personalized medicine for heterogeneous diseases like MS by forecasting lesion progression and evaluating treatment effects.

Method: A voxel-space approach incorporating MRI and treatment data to generate future lesion masks, tested on a multi-centre dataset of 2131 patient 3D MRIs.

Result: Accurate prediction of new and enlarging T2 lesions across six treatments, with applications in lesion count, location estimation, and counterfactual analysis.

Conclusion: The model shows promise for clinical use, advancing data-driven prognostics in MS.

Abstract: Image-based personalized medicine has the potential to transform healthcare,
particularly for diseases that exhibit heterogeneous progression such as
Multiple Sclerosis (MS). In this work, we introduce the first treatment-aware
spatio-temporal diffusion model that is able to generate future masks
demonstrating lesion evolution in MS. Our voxel-space approach incorporates
multi-modal patient data, including MRI and treatment information, to forecast
new and enlarging T2 (NET2) lesion masks at a future time point. Extensive
experiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized
clinical trials for relapsing-remitting MS demonstrate that our generative
model is able to accurately predict NET2 lesion masks for patients across six
different treatments. Moreover, we demonstrate our model has the potential for
real-world clinical applications through downstream tasks such as future lesion
count and location estimation, binary lesion activity classification, and
generating counterfactual future NET2 masks for several treatments with
different efficacies. This work highlights the potential of causal, image-based
generative models as powerful tools for advancing data-driven prognostics in
MS.

</details>


### [326] [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](https://arxiv.org/abs/2508.07031)
*Anindya Bijoy Das,Shahnewaz Karim Sakib,Shibbir Ahmed*

Main category: eess.IV

TL;DR: Study examines hallucinations in LLMs for medical imaging tasks, analyzing errors in image-to-text and text-to-image outputs, and discusses factors contributing to failures.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations in LLMs applied to medical imaging, which can mislead clinical decisions.

Method: Analyzes hallucinations in two directions: image-to-text (report generation) and text-to-image (image creation), evaluating errors using expert criteria.

Result: Reveals common hallucination patterns in both tasks, impacting clinical reliability, and identifies contributing factors like model architecture and training data.

Conclusion: Provides insights for improving the safety and trustworthiness of LLM-driven medical imaging systems.

Abstract: Large Language Models (LLMs) are increasingly applied to medical imaging
tasks, including image interpretation and synthetic image generation. However,
these models often produce hallucinations, which are confident but incorrect
outputs that can mislead clinical decisions. This study examines hallucinations
in two directions: image to text, where LLMs generate reports from X-ray, CT,
or MRI scans, and text to image, where models create medical images from
clinical prompts. We analyze errors such as factual inconsistencies and
anatomical inaccuracies, evaluating outputs using expert informed criteria
across imaging modalities. Our findings reveal common patterns of hallucination
in both interpretive and generative tasks, with implications for clinical
reliability. We also discuss factors contributing to these failures, including
model architecture and training data. By systematically studying both image
understanding and generation, this work provides insights into improving the
safety and trustworthiness of LLM driven medical imaging systems.

</details>


### [327] [3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression](https://arxiv.org/abs/2508.07038)
*Yuke Xing,William Gordon,Qi Yang,Kaifa Yang,Jiarui Wang,Yiling Xu*

Main category: eess.IV

TL;DR: 3DGS-VBench is introduced as a dataset and benchmark for assessing the quality of compressed 3D Gaussian Splatting (3DGS) models, addressing the lack of systematic evaluation in existing compression methods.


<details>
  <summary>Details</summary>
Motivation: High storage demands of 3DGS hinder practical use, and current compression methods lack standardized quality assessment.

Method: A large-scale dataset (660 models, 11 scenes, 6 compression algorithms) with MOS scores from 50 participants is created. Reliability is validated, and 15 quality metrics are evaluated.

Result: Benchmarking of 6 compression algorithms on storage and visual quality, along with evaluation of 15 assessment metrics, is presented.

Conclusion: 3DGS-VBench facilitates specialized VQA model training and advances research in 3DGS compression and quality assessment.

Abstract: 3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high
visual fidelity, but its substantial storage requirements hinder practical
deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate
compression modules. However, these 3DGS generative compression techniques
introduce unique distortions lacking systematic quality assessment research. To
this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment
(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences
generated from 11 scenes across 6 SOTA 3DGS compression algorithms with
systematically designed parameter levels. With annotations from 50
participants, we obtained MOS scores with outlier removal and validated dataset
reliability. We benchmark 6 3DGS compression algorithms on storage efficiency
and visual quality, and evaluate 15 quality assessment metrics across multiple
paradigms. Our work enables specialized VQA model training for 3DGS, serving as
a catalyst for compression and quality assessment research. The dataset is
available at https://github.com/YukeXing/3DGS-VBench.

</details>


### [328] [SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging](https://arxiv.org/abs/2508.07041)
*Junkai Liu,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: eess.IV

TL;DR: SAGCNet improves MRI slice imputation by modeling inter-slice relationships and leveraging 3D spatial context, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: MRI slice imputation is challenging due to missing or unusable slices, and existing methods struggle with 3D data complexities like inter-slice dependencies and spatial context.

Method: SAGCNet introduces a volumetric slice graph completion module for inter-slice relationships and a spatial adapter for 3D context.

Result: SAGCNet outperforms state-of-the-art methods in synthesizing missing CMR slices, even with limited data.

Conclusion: SAGCNet effectively addresses MRI slice imputation challenges, offering superior performance and robustness.

Abstract: Magnetic resonance imaging (MRI) provides detailed soft-tissue
characteristics that assist in disease diagnosis and screening. However, the
accuracy of clinical practice is often hindered by missing or unusable slices
due to various factors. Volumetric MRI synthesis methods have been developed to
address this issue by imputing missing slices from available ones. The inherent
3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR),
poses significant challenges for missing slice imputation approaches, including
(1) the difficulty of modeling local inter-slice correlations and dependencies
of volumetric slices, and (2) the limited exploration of crucial 3D spatial
information and global context. In this study, to mitigate these issues, we
present Spatial-Aware Graph Completion Network (SAGCNet) to overcome the
dependency on complete volumetric data, featuring two main innovations: (1) a
volumetric slice graph completion module that incorporates the inter-slice
relationships into a graph structure, and (2) a volumetric spatial adapter
component that enables our model to effectively capture and utilize various
forms of 3D spatial context. Extensive experiments on cardiac MRI datasets
demonstrate that SAGCNet is capable of synthesizing absent CMR slices,
outperforming competitive state-of-the-art MRI synthesis methods both
quantitatively and qualitatively. Notably, our model maintains superior
performance even with limited slice data.

</details>


### [329] [Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications](https://arxiv.org/abs/2508.07165)
*Zelin Qiu,Xi Wang,Zhuoyao Xie,Juan Zhou,Yu Wang,Lingjie Yang,Xinrui Jiang,Juyoung Bae,Moo Hyun Son,Qiang Ye,Dexuan Chen,Rui Zhang,Tao Li,Neeraj Ramesh Mahboobani,Varut Vardhanabhuti,Xiaohui Duan,Yinghua Zhao,Hao Chen*

Main category: eess.IV

TL;DR: PRISM is a foundation model pre-trained with large-scale multi-sequence MRI data, designed to improve generalization across diverse MRI protocols. It outperforms existing models in 39 out of 44 downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The heterogeneity among MRI sequences limits deep learning model generalization, restricting clinical utility. PRISM aims to address this by learning robust, generalizable representations.

Method: PRISM uses a novel pretraining paradigm to disentangle anatomically invariant features from sequence-specific variations. It leverages 336,476 volumetric MRI scans from 34 datasets.

Result: PRISM achieved top results in 39 of 44 downstream tasks, demonstrating superior generalization across diverse MRI protocols.

Conclusion: PRISM enhances AI's translational potential in radiology by providing a scalable framework for multi-sequence MRI analysis, ensuring consistent performance across imaging protocols.

Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable
versatility, enabling the distinct visualization of different tissue types.
Nevertheless, the inherent heterogeneity among MRI sequences poses significant
challenges to the generalization capability of deep learning models. These
challenges undermine model performance when faced with varying acquisition
parameters, thereby severely restricting their clinical utility. In this study,
we present PRISM, a foundation model PRe-trained with large-scale
multI-Sequence MRI. We collected a total of 64 datasets from both public and
private sources, encompassing a wide range of whole-body anatomical structures,
with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI
scans from 34 datasets (8 public and 26 private) were curated to construct the
largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a
novel pretraining paradigm that disentangles anatomically invariant features
from sequence-specific variations in MRI, while preserving high-level semantic
representations. We established a benchmark comprising 44 downstream tasks,
including disease diagnosis, image segmentation, registration, progression
prediction, and report generation. These tasks were evaluated on 32 public
datasets and 5 private cohorts. PRISM consistently outperformed both
non-pretrained models and existing foundation models, achieving first-rank
results in 39 out of 44 downstream benchmarks with statistical significance
improvements. These results underscore its ability to learn robust and
generalizable representations across unseen data acquired under diverse MRI
protocols. PRISM provides a scalable framework for multi-sequence MRI analysis,
thereby enhancing the translational potential of AI in radiology. It delivers
consistent performance across diverse imaging protocols, reinforcing its
clinical applicability.

</details>


### [330] [HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation](https://arxiv.org/abs/2508.07225)
*Xuepeng Liu,Zheng Jiang,Pinan Zhu,Hanyu Liu,Chao Li*

Main category: eess.IV

TL;DR: HaDM-ST improves spatial transcriptomics resolution by integrating H&E images and low-resolution ST, addressing feature extraction, alignment, and gene-specific modeling.


<details>
  <summary>Details</summary>
Motivation: Current ST methods lack resolution and face challenges in feature extraction, multimodal alignment, and gene-specific variation.

Method: HaDM-ST uses semantic distillation, spatial alignment, and channel-aware adversarial learning to enhance ST resolution.

Result: Outperforms prior methods in spatial fidelity and gene-level coherence across diverse tissues and species.

Conclusion: HaDM-ST effectively addresses key challenges in high-resolution ST generation.

Abstract: Spatial transcriptomics (ST) reveals spatial heterogeneity of gene
expression, yet its resolution is limited by current platforms. Recent methods
enhance resolution via H&E-stained histology, but three major challenges
persist: (1) isolating expression-relevant features from visually complex H&E
images; (2) achieving spatially precise multimodal alignment in diffusion-based
frameworks; and (3) modeling gene-specific variation across expression
channels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST
Generation), a high-resolution ST generation framework conditioned on H&E
images and low-resolution ST. HaDM-ST includes: (i) a semantic distillation
network to extract predictive cues from H&E; (ii) a spatial alignment module
enforcing pixel-wise correspondence with low-resolution ST; and (iii) a
channel-aware adversarial learner for fine-grained gene-level modeling.
Experiments on 200 genes across diverse tissues and species show HaDM-ST
consistently outperforms prior methods, enhancing spatial fidelity and
gene-level coherence in high-resolution ST predictions.

</details>


### [331] [DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework](https://arxiv.org/abs/2508.07682)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: eess.IV

TL;DR: DiffVC-OSD is a one-step diffusion-based neural video compression framework that outperforms multi-step methods in speed and bitrate reduction while enhancing perceptual quality.


<details>
  <summary>Details</summary>
Motivation: To improve perceptual quality and efficiency in video compression by reducing the steps required in diffusion-based methods.

Method: Uses a One-Step Diffusion Model with a Temporal Context Adapter for fine-grained guidance and an End-to-End Finetuning strategy.

Result: Achieves state-of-the-art performance, 20x faster decoding, and 86.92% bitrate reduction compared to multi-step variants.

Conclusion: DiffVC-OSD is a highly efficient and effective solution for perceptual neural video compression.

Abstract: In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based
Perceptual Neural Video Compression framework. Unlike conventional multi-step
diffusion-based methods, DiffVC-OSD feeds the reconstructed latent
representation directly into a One-Step Diffusion Model, enhancing perceptual
quality through a single diffusion step guided by both temporal context and the
latent itself. To better leverage temporal dependencies, we design a Temporal
Context Adapter that encodes conditional inputs into multi-level features,
offering more fine-grained guidance for the Denoising Unet. Additionally, we
employ an End-to-End Finetuning strategy to improve overall compression
performance. Extensive experiments demonstrate that DiffVC-OSD achieves
state-of-the-art perceptual compression performance, offers about 20$\times$
faster decoding and a 86.92\% bitrate reduction compared to the corresponding
multi-step diffusion-based variant.

</details>


### [332] [Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning](https://arxiv.org/abs/2508.07788)
*Runze Wang,Zeli Chen,Zhiyun Song,Wei Fang,Jiajin Zhang,Danyang Tu,Yuxing Tang,Minfeng Xu,Xianghua Ye,Le Lu,Dakai Jin*

Main category: eess.IV

TL;DR: ALDEN is a deep learning-based method for LDCT denoising that integrates anatomical semantics using pretrained vision models, adversarial learning, and contrastive learning to improve denoising outcomes.


<details>
  <summary>Details</summary>
Motivation: Current LDCT denoising methods often ignore anatomical semantics, leading to suboptimal results. ALDEN addresses this by incorporating tissue-specific features.

Method: ALDEN combines adversarial learning with a cross-attention-based discriminator and semantic-guided contrastive learning to preserve anatomical details.

Result: ALDEN outperforms existing methods on LDCT denoising datasets, reducing over-smoothing and improving anatomical preservation.

Conclusion: ALDEN effectively maintains anatomical awareness, validated by superior denoising performance and downstream segmentation tasks.

Abstract: To reduce radiation exposure and improve the diagnostic efficacy of low-dose
computed tomography (LDCT), numerous deep learning-based denoising methods have
been developed to mitigate noise and artifacts. However, most of these
approaches ignore the anatomical semantics of human tissues, which may
potentially result in suboptimal denoising outcomes. To address this problem,
we propose ALDEN, an anatomy-aware LDCT denoising method that integrates
semantic features of pretrained vision models (PVMs) with adversarial and
contrastive learning. Specifically, we introduce an anatomy-aware discriminator
that dynamically fuses hierarchical semantic features from reference
normal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific
realism evaluation in the discriminator. In addition, we propose a
semantic-guided contrastive learning module that enforces anatomical
consistency by contrasting PVM-derived features from LDCT, denoised CT and
NDCT, preserving tissue-specific patterns through positive pairs and
suppressing artifacts via dual negative pairs. Extensive experiments conducted
on two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art
performance, offering superior anatomy preservation and substantially reducing
over-smoothing issue of previous work. Further validation on a downstream
multi-organ segmentation task (encompassing 117 anatomical structures) affirms
the model's ability to maintain anatomical awareness.

</details>


### [333] [Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images](https://arxiv.org/abs/2508.07875)
*Shuo Han,Ahmed Karam Eldaly,Solomon Sunday Oyelere*

Main category: eess.IV

TL;DR: A human-in-the-loop deep learning system combines AI (EfficientNetV2S) and medical expertise to improve invasive ductal carcinoma detection in histopathology images, achieving 93.65% accuracy and further enhancing performance through iterative feedback.


<details>
  <summary>Details</summary>
Motivation: Early, accurate diagnosis of invasive ductal carcinoma (IDC) is crucial for patient survival, and combining AI with human expertise can enhance precision and efficiency in detection.

Method: The system uses an EfficientNetV2S model for initial diagnosis, followed by human expert review and correction of misclassified images, creating a feedback loop to refine the model iteratively.

Result: The model achieves 93.65% accuracy, and the human-in-the-loop system further improves performance by correcting misclassified images in experimental groups.

Conclusion: This collaborative human-AI approach advances automated, efficient, and accurate IDC detection, offering a promising future for AI-assisted medical diagnostics.

Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,
and early, accurate diagnosis is critical to improving patient survival rates
by guiding treatment decisions. Combining medical expertise with artificial
intelligence (AI) holds significant promise for enhancing the precision and
efficiency of IDC detection. In this work, we propose a human-in-the-loop
(HITL) deep learning system designed to detect IDC in histopathology images.
The system begins with an initial diagnosis provided by a high-performance
EfficientNetV2S model, offering feedback from AI to the human expert. Medical
professionals then review the AI-generated results, correct any misclassified
images, and integrate the revised labels into the training dataset, forming a
feedback loop from the human back to the AI. This iterative process refines the
model's performance over time. The EfficientNetV2S model itself achieves
state-of-the-art performance compared to existing methods in the literature,
with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system
further improves the model's accuracy using four experimental groups with
misclassified images. These results demonstrate the potential of this
collaborative approach to enhance AI performance in diagnostic systems. This
work contributes to advancing automated, efficient, and highly accurate methods
for IDC detection through human-AI collaboration, offering a promising
direction for future AI-assisted medical diagnostics.

</details>


### [334] [Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models](https://arxiv.org/abs/2508.07903)
*Johanna P. Müller,Anika Knupfer,Pedro Blöss,Edoardo Berardi Vittur,Bernhard Kainz,Jana Hutter*

Main category: eess.IV

TL;DR: A novel diffusion-based framework for generating anatomically precise female pelvic MRI images, addressing data scarcity and privacy in gynaecology.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with anatomical precision in female pelvic imaging, hindering applications in gynaecology due to data scarcity and privacy concerns.

Method: Integrates unconditional and conditioned DDPMs and LDMs in 2D/3D to synthesize high-fidelity uterine MRI images.

Result: Produces clinically realistic synthetic images, improves diagnostic accuracy, and passes expert evaluation.

Conclusion: The framework advances equitable AI in gynaecology by providing privacy-safe synthetic data for research and diagnostics.

Abstract: Despite significant progress in generative modelling, existing diffusion
models often struggle to produce anatomically precise female pelvic images,
limiting their application in gynaecological imaging, where data scarcity and
patient privacy concerns are critical. To overcome these barriers, we introduce
a novel diffusion-based framework for uterine MRI synthesis, integrating both
unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)
and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates
anatomically coherent, high fidelity synthetic images that closely mimic real
scans and provide valuable resources for training robust diagnostic models. We
evaluate generative quality using advanced perceptual and distributional
metrics, benchmarking against standard reconstruction methods, and demonstrate
substantial gains in diagnostic accuracy on a key classification task. A
blinded expert evaluation further validates the clinical realism of our
synthetic images. We release our models with privacy safeguards and a
comprehensive synthetic uterine MRI dataset to support reproducible research
and advance equitable AI in gynaecology.

</details>


### [335] [A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images](https://arxiv.org/abs/2508.08123)
*Lingjing Chen,Chengxiu Zhang,Yinqiao Yi,Yida Wang,Yang Song,Xu Yan,Shengfang Xu,Dalin Zhu,Mengqiu Cao,Yan Zhou,Chenglong Wang,Guang Yang*

Main category: eess.IV

TL;DR: A deep learning model integrates MRI sequence parameters (TR, TE, TI) via parameter embedding to synthesize quantitative MRI maps (T1, T2, PD) from clinical weighted MRI, achieving high accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and generalizability of quantitative MRI synthesis by embedding MRI sequence parameters into the model, enabling it to learn the physics of MRI signal formation.

Method: A physics-driven neural network embeds MRI sequence parameters (TR, TE, TI) and processes T1-weighted, T2-weighted, and T2-FLAIR images to synthesize T1, T2, and PD maps.

Result: High performance (PSNR >34 dB, SSIM >0.92) on internal and external datasets, outperforming conventional models in accuracy and robustness, even for unseen brain structures and lesions.

Conclusion: The method enhances quantitative MRI synthesis by leveraging sequence parameters, showing potential for clinical utility and accelerating qMRI.

Abstract: We propose a deep learning-based approach that integrates MRI sequence
parameters to improve the accuracy and generalizability of quantitative image
synthesis from clinical weighted MRI. Our physics-driven neural network embeds
MRI sequence parameters -- repetition time (TR), echo time (TE), and inversion
time (TI) -- directly into the model via parameter embedding, enabling the
network to learn the underlying physical principles of MRI signal formation.
The model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as
input and synthesizes T1, T2, and proton density (PD) quantitative maps.
Trained on healthy brain MR images, it was evaluated on both internal and
external test datasets. The proposed method achieved high performance with PSNR
values exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter
maps. It outperformed conventional deep learning models in accuracy and
robustness, including data with previously unseen brain structures and lesions.
Notably, our model accurately synthesized quantitative maps for these unseen
pathological regions, highlighting its superior generalization capability.
Incorporating MRI sequence parameters via parameter embedding allows the neural
network to better learn the physical characteristics of MR signals,
significantly enhancing the performance and reliability of quantitative MRI
synthesis. This method shows great potential for accelerating qMRI and
improving its clinical utility.

</details>


### [336] [RedDino: A foundation model for red blood cell analysis](https://arxiv.org/abs/2508.08180)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto,Carsten Marr*

Main category: eess.IV

TL;DR: RedDino is a self-supervised foundation model for RBC image analysis, outperforming state-of-the-art models in classification and generalization.


<details>
  <summary>Details</summary>
Motivation: Precise RBC morphological analysis is crucial for diagnosing hematological disorders, but comprehensive AI solutions are lacking.

Method: RedDino adapts DINOv2 for RBC analysis, trained on 1.25M diverse RBC images, evaluated via linear probing and nearest neighbor classification.

Result: RedDino outperforms existing models in RBC shape classification, demonstrating strong feature representation and generalization.

Conclusion: RedDino advances computational hematology by capturing nuanced RBC features, offering a reliable diagnostic tool with open-source availability.

Abstract: Red blood cells (RBCs) are essential to human health, and their precise
morphological analysis is important for diagnosing hematological disorders.
Despite the promise of foundation models in medical diagnostics, comprehensive
AI solutions for RBC analysis remain scarce. We present RedDino, a
self-supervised foundation model designed for RBC image analysis. RedDino uses
an RBC-specific adaptation of the DINOv2 self-supervised learning framework and
is trained on a curated dataset of 1.25 million RBC images from diverse
acquisition modalities and sources. Extensive evaluations show that RedDino
outperforms existing state-of-the-art models on RBC shape classification.
Through assessments including linear probing and nearest neighbor
classification, we confirm its strong feature representations and
generalization ability. Our main contributions are: (1) a foundation model
tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations
for RBC modeling, and (3) a detailed evaluation of generalization performance.
RedDino addresses key challenges in computational hematology by capturing
nuanced morphological features, advancing the development of reliable
diagnostic tools. The source code and pretrained models for RedDino are
available at https://github.com/Snarci/RedDino, and the pretrained models can
be downloaded from our Hugging Face collection at
https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc

</details>


### [337] [Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping](https://arxiv.org/abs/2508.07760)
*Maximilian Kromer,Panagiotis Agrafiotis,Begüm Demir*

Main category: eess.IV

TL;DR: Sea-Undistort is a synthetic dataset for training models to correct optical distortions in shallow water bathymetric mapping, improving accuracy in real-world applications.


<details>
  <summary>Details</summary>
Motivation: Challenges in accurate bathymetric mapping due to optical distortions like waves, scattering, and sunglint in shallow waters.

Method: Creation of a 1200-pair synthetic dataset (Sea-Undistort) with realistic water effects, used to train and benchmark image restoration methods, including an enhanced diffusion model.

Result: The enhanced diffusion model improves seabed mapping, reduces errors, and restores fine details in real aerial data.

Conclusion: Sea-Undistort enables effective training for distortion correction, advancing shallow water bathymetric mapping.

Abstract: Accurate image-based bathymetric mapping in shallow waters remains
challenging due to the complex optical distortions such as wave induced
patterns, scattering and sunglint, introduced by the dynamic water surface, the
water column properties, and solar illumination. In this work, we introduce
Sea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512
through-water scenes rendered in Blender. Each pair comprises a distortion-free
and a distorted view, featuring realistic water effects such as sun glint,
waves, and scattering over diverse seabeds. Accompanied by per-image metadata
such as camera parameters, sun position, and average depth, Sea-Undistort
enables supervised training that is otherwise infeasible in real environments.
We use Sea-Undistort to benchmark two state-of-the-art image restoration
methods alongside an enhanced lightweight diffusion-based framework with an
early-fusion sun-glint mask. When applied to real aerial data, the enhanced
diffusion model delivers more complete Digital Surface Models (DSMs) of the
seabed, especially in deeper areas, reduces bathymetric errors, suppresses
glint and scattering, and crisply restores fine seabed details. Dataset,
weights, and code are publicly available at
https://www.magicbathy.eu/Sea-Undistort.html.

</details>


### [338] [PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography](https://arxiv.org/abs/2508.07773)
*Mohammed Salah,Numan Saeed,Davor Svetinovic,Stefano Sfarra,Mohammed Omar,Yusra Abdulrahman*

Main category: eess.IV

TL;DR: A PCA-guided autoencoder framework is proposed for structured dimensionality reduction in AIRT data, improving defect characterization.


<details>
  <summary>Details</summary>
Motivation: Current autoencoders for AIRT data lack structured latent spaces, limiting defect characterization.

Method: A PCA-guided autoencoder with a novel PCA distillation loss enforces structured latent spaces while capturing non-linear features.

Result: The method outperforms state-of-the-art techniques in contrast, SNR, and neural network-based metrics on PVC, CFRP, and PLA samples.

Conclusion: The PCA-guided framework enhances defect characterization by structuring latent spaces in AIRT data.

Abstract: Active Infrared thermography (AIRT) is a widely adopted non-destructive
testing (NDT) technique for detecting subsurface anomalies in industrial
components. Due to the high dimensionality of AIRT data, current approaches
employ non-linear autoencoders (AEs) for dimensionality reduction. However, the
latent space learned by AIRT AEs lacks structure, limiting their effectiveness
in downstream defect characterization tasks. To address this limitation, this
paper proposes a principal component analysis guided (PCA-guided) autoencoding
framework for structured dimensionality reduction to capture intricate,
non-linear features in thermographic signals while enforcing a structured
latent space. A novel loss function, PCA distillation loss, is introduced to
guide AIRT AEs to align the latent representation with structured PCA
components while capturing the intricate, non-linear patterns in thermographic
signals. To evaluate the utility of the learned, structured latent space, we
propose a neural network-based evaluation metric that assesses its suitability
for defect characterization. Experimental results show that the proposed
PCA-guided AE outperforms state-of-the-art dimensionality reduction methods on
PVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR),
and neural network-based metrics.

</details>


### [339] [MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer](https://arxiv.org/abs/2508.07817)
*Tao Tang,Chengxu Yang*

Main category: eess.IV

TL;DR: A medical image denoising model (MI-ND) combining multi-scale convolution and Transformer, with noise level estimation and adaptive attention, outperforms existing methods in quality and diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Medical images often suffer from noise due to low-dose scanning and equipment limitations, impairing diagnosis.

Method: Proposes MI-ND with noise level estimator (NLE) and noise adaptive attention module (NAAB) for channel-spatial attention and cross-modal fusion.

Result: Outperforms others in PSNR, SSIM, LPIPS, and improves F1 score and ROC-AUC in diagnostics.

Conclusion: MI-ND enhances image quality and diagnostic accuracy, offering practical value for AI-assisted healthcare.

Abstract: The core role of medical images in disease diagnosis makes their quality
directly affect the accuracy of clinical judgment. However, due to factors such
as low-dose scanning, equipment limitations and imaging artifacts, medical
images are often accompanied by non-uniform noise interference, which seriously
affects structure recognition and lesion detection. This paper proposes a
medical image adaptive denoising model (MI-ND) that integrates multi-scale
convolutional and Transformer architecture, introduces a noise level estimator
(NLE) and a noise adaptive attention module (NAAB), and realizes
channel-spatial attention regulation and cross-modal feature fusion driven by
noise perception. Systematic testing is carried out on multimodal public
datasets. Experiments show that this method significantly outperforms the
comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,
and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing
strong prac-tical value and promotional potential. The model has outstanding
benefits in structural recovery, diagnostic sensitivity, and cross-modal
robustness, and provides an effective solution for medical image enhancement
and AI-assisted diagnosis and treatment.

</details>


### [340] [Learned Regularization for Microwave Tomography](https://arxiv.org/abs/2508.08114)
*Bowen Tong,Hao Chen,Shaorui Guo,Dong Liu*

Main category: eess.IV

TL;DR: A physics-informed hybrid framework (SSD-Reg) integrates diffusion models into MWT for improved reconstruction without paired data.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of conventional methods and deep learning in MWT, which struggle with nonlinearity, ill-posedness, and data dependency.

Method: Proposes SSD-Reg, embedding diffusion priors into iterative reconstruction for structural recovery without paired data.

Result: SSD-Reg improves accuracy, stability, and robustness in functional image reconstruction.

Conclusion: SSD-Reg offers a flexible, effective solution for ill-posed MWT problems, balancing physics fidelity and learned priors.

Abstract: Microwave Tomography (MWT) aims to reconstruct the dielectric properties of
tissues from measured scattered electromagnetic fields. This inverse problem is
highly nonlinear and ill-posed, posing significant challenges for conventional
optimization-based methods, which, despite being grounded in physical models,
often fail to recover fine structural details. Recent deep learning strategies,
including end-to-end and post-processing networks, have improved reconstruction
quality but typically require large paired training datasets and may struggle
to generalize. To overcome these limitations, we propose a physics-informed
hybrid framework that integrates diffusion models as learned regularization
within a data-consistency-driven variational scheme. Specifically, we introduce
Single-Step Diffusion Regularization (SSD-Reg), a novel approach that embeds
diffusion priors into the iterative reconstruction process, enabling the
recovery of complex anatomical structures without the need for paired data.
SSD-Reg maintains fidelity to both the governing physics and learned structural
distributions, improving accuracy, stability, and robustness. Extensive
experiments demonstrate that SSD-Reg, implemented as a Plug-and-Play (PnP)
module, provides a flexible and effective solution for tackling the
ill-posedness inherent in functional image reconstruction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [341] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: The paper introduces DatasetResearch, a benchmark for evaluating AI agents' ability to discover and synthesize datasets, revealing significant gaps in current capabilities.


<details>
  <summary>Details</summary>
Motivation: The bottleneck in AI development has shifted from computational power to data availability, prompting the need for autonomous, demand-driven data curation.

Method: A tri-dimensional evaluation framework tests AI agents on 208 real-world demands, assessing knowledge-intensive and reasoning-intensive tasks.

Result: Advanced systems achieve only 22% on the challenging DatasetResearch-pro subset, highlighting limitations in dataset discovery.

Conclusion: The benchmark sets a baseline for future AI systems, emphasizing the need for improved search and synthesis capabilities.

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [342] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: MultiMedEdit is the first benchmark for evaluating knowledge editing in multimodal medical tasks, highlighting gaps in current methods and offering metrics for reliability, generality, and locality.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on knowledge editing in multimodal medical scenarios, which requires integrating updated knowledge with visual reasoning for safe clinical decisions.

Method: Proposes MultiMedEdit, a benchmark framework for evaluating KE in clinical multimodal tasks, with a three-dimensional metric suite and cross-paradigm comparisons.

Result: Current methods struggle with generalization and long-tail reasoning in clinical workflows, and efficiency analysis reveals practical trade-offs.

Conclusion: MultiMedEdit exposes limitations of current KE approaches and lays groundwork for future clinically robust techniques.

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [343] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: EndoAgent, a memory-guided AI agent for endoscopic analysis, integrates iterative reasoning and adaptive tool selection, outperforming existing models in clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing AI methods lack coordination for multi-step clinical workflows in endoscopy, and AI agents' potential in this domain is underexplored.

Method: EndoAgent uses a dual-memory design for logical coherence (short-term tracking) and enhanced reasoning (long-term learning), integrating expert tools in a unified loop.

Result: EndoAgent outperforms general and medical multimodal models, demonstrating flexibility and reasoning in clinical tasks.

Conclusion: EndoAgent advances endoscopic AI by combining memory-guided reasoning with adaptive tool use, validated by a new benchmark.

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [344] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: The paper introduces Comp-Comp, an iterative benchmarking framework for domain-specific LLMs, emphasizing comprehensiveness and compactness over scaling laws, validated with XUBench in an academic domain.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on scaling laws, neglecting the impact of corpus and QA set design on precision and recall in domain-specific LLMs.

Method: Proposes Comp-Comp, a framework balancing comprehensiveness (semantic recall) and compactness (precision) for corpus and QA set construction.

Result: Validated with XUBench, a large-scale closed-domain benchmark in academia, showing the framework's effectiveness.

Conclusion: Comp-Comp is extensible beyond academia, offering insights for benchmark construction across domains.

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [345] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: The paper evaluates BERTopic and NMF for topic modeling in GAI-augmented strategic planning for government organizations, finding BERTopic superior.


<details>
  <summary>Details</summary>
Motivation: To leverage GAI and LLMs for automating strategic plan development in large-scale government organizations, overcoming regulatory challenges.

Method: BERTopic and NMF are trained on GAO reports to generate topics, which are then compared to Vision Elements in a strategic plan for similarity.

Result: Both techniques matched 100% of Vision Elements, with BERTopic outperforming NMF, achieving medium or strong correlation in over half of cases.

Conclusion: GAI-enabled strategic planning is viable and impactful, with BERTopic being the preferred method. Future work will operationalize the model and test remaining modules.

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [346] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: A survey on self-evolving AI agents, reviewing techniques for adaptability in dynamic environments, introducing a conceptual framework, and discussing domain-specific strategies and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of static AI agent systems by exploring self-evolving techniques for adaptability in dynamic environments.

Method: Introduces a unified framework with four key components (System Inputs, Agent System, Environment, Optimisers) and reviews self-evolving techniques targeting these components, including domain-specific strategies.

Result: Provides a comprehensive review of self-evolving agentic systems, highlighting their potential for adaptability and autonomy.

Conclusion: Lays the foundation for developing adaptive, autonomous, and lifelong AI agent systems, emphasizing evaluation, safety, and ethics.

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [347] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: A new agentic approach using a Python coding agent based on ReAct solves all 101 CP-Bench problems by combining general coding tools and domain expertise in prompts, avoiding fixed workflows.


<details>
  <summary>Details</summary>
Motivation: Automating the translation of natural language problem descriptions into formal constraint models is challenging due to the need for deep expertise and limitations of fixed workflows.

Method: A general-purpose Python coding agent using ReAct, with a persistent IPython kernel for stateful execution, dynamically tests hypotheses and debugs failures using domain-specific prompts.

Result: The agent successfully solved all 101 problems in the CP-Bench benchmark set.

Conclusion: Constraint modeling benefits from general coding tools and prompt-encoded expertise, not specialized architectures or predefined workflows.

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [348] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: A harness enables local LLMs to play Diplomacy without fine-tuning, using optimized game state representation and tooling for analysis. Larger models perform best, but smaller ones are adequate.


<details>
  <summary>Details</summary>
Motivation: Diplomacy's complexity and high variance made it prohibitive for study without fine-tuning or frontier LLMs.

Method: Data-driven iteration optimized textual game state representation; developed tooling for hypothesis testing and introduced Critical State Analysis.

Result: Larger LLMs perform best, but smaller models are adequate. Insights into strategic reasoning capabilities emerged naturally.

Conclusion: The harness democratizes strategic reasoning evaluation in LLMs, eliminating fine-tuning needs and providing insights into model capabilities.

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [349] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: ThinkTuning is a GRPO-based interactive training method where a teacher model guides a student model through feedback, improving reasoning capabilities by 3.85% on average over zero-shot baselines.


<details>
  <summary>Details</summary>
Motivation: Current RL methods don't instill new reasoning abilities in LLMs but only draw out existing behaviors. The goal is to train models lacking such abilities to develop them.

Method: ThinkTuning uses a teacher model to provide corrective feedback during student model rollouts, inspired by classroom teaching practices.

Result: Average 3.85% improvement over zero-shot baselines, with specific gains on MATH-500 (2.08%), AIME (2.23%), and GPQA-Diamond (3.99%) over vanilla-GRPO.

Conclusion: Implicit supervision via teacher feedback enhances student model reasoning, proving effective across benchmarks.

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [350] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav is a modular framework for Vision-and-Language Navigation (VLN) that uses skill-based reasoning and a zero-shot router to improve generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Current VLN methods struggle with generalization, especially in unseen scenarios requiring complex reasoning.

Method: Decomposes navigation into atomic skills, each handled by specialized agents, and uses a VLM-based router for dynamic agent selection.

Result: Achieves state-of-the-art on R2R and strong generalization on GSA-R2R.

Conclusion: SkillNav's modular, skill-based approach enhances VLN performance and generalization.

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [351] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: IRL-VLA introduces a three-stage close-loop Reinforcement Learning framework for autonomous driving, combining VLA architecture, inverse reinforcement learning, and PPO to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models face challenges in open-loop imitation learning and close-loop training due to domain gaps and inefficiencies.

Method: A three-stage approach: pretraining VLA policy via imitation learning, building a lightweight reward world model, and refining with PPO.

Result: Achieves top performance in NAVSIM v2 benchmark and CVPR2025 Autonomous Grand Challenge.

Conclusion: The framework advances VLA research in close-loop autonomous driving.

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [352] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: MLLMs struggle with object counting, limiting real-world reliability. CountQA, a new benchmark with 1,500+ QA pairs, reveals poor performance (42.9% accuracy) in 15 MLLMs, highlighting the need for improvement.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack fundamental object counting skills, restricting their practical use. Existing benchmarks are inadequate for testing this in complex scenarios.

Method: Introduces CountQA, a benchmark with real-world images featuring high object density, clutter, and occlusion. Evaluates 15 MLLMs on this benchmark.

Result: Top-performing MLLM achieves only 42.9% accuracy, with performance worsening as object counts increase.

Conclusion: CountQA identifies a critical MLLM weakness and aims to drive development of more numerically and spatially aware models. Dataset and code will be open-sourced.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [353] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: The paper introduces MP-Bench, a large-scale dataset for severe weather prediction, and MMLM, a multimodal model addressing challenges in AI-driven weather forecasting.


<details>
  <summary>Details</summary>
Motivation: Current severe weather forecasting relies on manual interpretation, which is subjective and burdensome. AI offers potential for automation but faces challenges like data scarcity and alignment issues.

Method: Developed MP-Bench dataset and MMLM model, which processes 4D meteorological data using adaptive fusion modules for dynamic feature extraction.

Result: MMLM performs exceptionally on MP-Bench, demonstrating effectiveness in severe weather prediction.

Conclusion: The work advances AI-driven weather forecasting, with public release of code and dataset planned.

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [354] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: FEAT (ForEnsic AgenT) is a multi-agent AI framework designed to automate and standardize forensic death investigations, outperforming existing AI systems and achieving expert-level accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing workforce shortages and diagnostic variability in forensic cause-of-death determination, particularly in high-volume systems like China's medicolegal infrastructure.

Method: FEAT integrates a central Planner, specialized Local Solvers, a Memory & Reflection module, and a Global Solver, using tool-augmented reasoning, hierarchical retrieval-augmented generation, and forensic-tuned LLMs with human-in-the-loop feedback.

Result: FEAT outperformed state-of-the-art AI systems in evaluations across diverse Chinese case cohorts, demonstrating robust generalization and high expert concordance.

Conclusion: FEAT offers scalable, consistent death certification with expert-level rigor, potentially advancing equitable access to reliable medicolegal services.

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [355] [Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody](https://arxiv.org/abs/2508.06890)
*Jinsung Yoon,Wooyeol Jeong,Jio Gim,Young-Joo Suh*

Main category: cs.SD

TL;DR: Maestro-EVC is a controllable emotional voice conversion framework that independently manages content, speaker identity, and emotion, improving disentanglement and fine-grained emotional expression.


<details>
  <summary>Details</summary>
Motivation: Existing EVC methods struggle with disentangling speaker identity and emotional style, and lack fine-grained emotional control like temporal dynamics.

Method: Maestro-EVC disentangles attributes using separate references, introduces temporal emotion representation, and employs prosody modeling with augmentation.

Result: The framework achieves high-quality, controllable, and emotionally expressive speech synthesis.

Conclusion: Maestro-EVC advances EVC by enabling independent control and capturing temporal emotional dynamics.

Abstract: Emotional voice conversion (EVC) aims to modify the emotional style of speech
while preserving its linguistic content. In practical EVC, controllability, the
ability to independently control speaker identity and emotional style using
distinct references, is crucial. However, existing methods often struggle to
fully disentangle these attributes and lack the ability to model fine-grained
emotional expressions such as temporal dynamics. We propose Maestro-EVC, a
controllable EVC framework that enables independent control of content, speaker
identity, and emotion by effectively disentangling each attribute from separate
references. We further introduce a temporal emotion representation and an
explicit prosody modeling with prosody augmentation to robustly capture and
transfer the temporal dynamics of the target emotion, even under
prosody-mismatched conditions. Experimental results confirm that Maestro-EVC
achieves high-quality, controllable, and emotionally expressive speech
synthesis.

</details>


### [356] [Joint Transcription of Acoustic Guitar Strumming Directions and Chords](https://arxiv.org/abs/2508.07973)
*Sebastian Murgul,Johannes Schimper,Michael Heizmann*

Main category: cs.SD

TL;DR: The paper introduces a deep learning-based method for transcribing guitar strumming, combining real-world and synthetic datasets to improve accuracy in detecting strumming directions and chord progressions.


<details>
  <summary>Details</summary>
Motivation: Automatic transcription of guitar strumming is challenging and underrepresented in MIR, with existing methods limited by dataset constraints.

Method: A CRNN model is trained using a novel dataset (90 min real-world recordings and 4h synthetic audio) to detect strumming events, classify directions, and identify chords.

Result: The hybrid approach outperforms baseline onset detection, achieving high accuracy in strumming action detection and chord classification.

Conclusion: Deep learning shows promise for robust guitar strumming transcription, enabling new possibilities for automatic rhythm guitar analysis.

Abstract: Automatic transcription of guitar strumming is an underrepresented and
challenging task in Music Information Retrieval (MIR), particularly for
extracting both strumming directions and chord progressions from audio signals.
While existing methods show promise, their effectiveness is often hindered by
limited datasets. In this work, we extend a multimodal approach to guitar
strumming transcription by introducing a novel dataset and a deep
learning-based transcription model. We collect 90 min of real-world guitar
recordings using an ESP32 smartwatch motion sensor and a structured recording
protocol, complemented by a synthetic dataset of 4h of labeled strumming audio.
A Convolutional Recurrent Neural Network (CRNN) model is trained to detect
strumming events, classify their direction, and identify the corresponding
chords using only microphone audio. Our evaluation demonstrates significant
improvements over baseline onset detection algorithms, with a hybrid method
combining synthetic and real-world data achieving the highest accuracy for both
strumming action detection and chord classification. These results highlight
the potential of deep learning for robust guitar strumming transcription and
open new avenues for automatic rhythm guitar analysis.

</details>


### [357] [Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription](https://arxiv.org/abs/2508.07987)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: Procedural data generation for acoustic guitar transcription achieves reasonable results, with finetuning on real data enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of limited labeled training data and legal constraints for acoustic guitar transcription.

Method: Uses a four-stage pipeline: tablature composition, MIDI rendering, physical modeling, and audio augmentation to synthesize training data. Evaluates a CRNN model on real and synthetic datasets.

Result: Procedural data achieves reasonable note-tracking; finetuning with real data improves accuracy over models trained only on real recordings.

Conclusion: Procedurally generated audio is promising for data-scarce music information retrieval tasks.

Abstract: Automatic transcription of acoustic guitar fingerpicking performances remains
a challenging task due to the scarcity of labeled training data and legal
constraints connected with musical recordings. This work investigates a
procedural data generation pipeline as an alternative to real audio recordings
for training transcription models. Our approach synthesizes training data
through four stages: knowledge-based fingerpicking tablature composition, MIDI
performance rendering, physical modeling using an extended Karplus-Strong
algorithm, and audio augmentation including reverb and distortion. We train and
evaluate a CRNN-based note-tracking model on both real and synthetic datasets,
demonstrating that procedural data can be used to achieve reasonable
note-tracking results. Finetuning with a small amount of real data further
enhances transcription accuracy, improving over models trained exclusively on
real recordings. These results highlight the potential of procedurally
generated audio for data-scarce music information retrieval tasks.

</details>


### [358] [Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning](https://arxiv.org/abs/2508.08039)
*Shu Wu,Chenxing Li,Wenfu Wang,Hao Zhang,Hualei Wang,Meng Yu,Dong Yu*

Main category: cs.SD

TL;DR: Audio-Thinker enhances LALMs' reasoning via adaptive rewards and external evaluation, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current LALMs lack human-level auditory-language reasoning, and explicit reasoning processes don't significantly benefit audio question answering.

Method: Proposes Audio-Thinker, a reinforcement learning framework with adaptive think accuracy rewards and external reward models for consistency.

Result: Outperforms existing reasoning-oriented LALMs in benchmarks, showing superior reasoning and generalization.

Conclusion: Audio-Thinker effectively addresses reasoning limitations in LALMs, improving adaptability and performance.

Abstract: Recent advancements in large language models, multimodal large language
models, and large audio language models (LALMs) have significantly improved
their reasoning capabilities through reinforcement learning with rule-based
rewards. However, the explicit reasoning process has yet to show significant
benefits for audio question answering, and effectively leveraging deep
reasoning remains an open challenge, with LALMs still falling short of
human-level auditory-language reasoning. To address these limitations, we
propose Audio-Thinker, a reinforcement learning framework designed to enhance
the reasoning capabilities of LALMs, with a focus on improving adaptability,
consistency, and effectiveness. Our approach introduces an adaptive think
accuracy reward, enabling the model to adjust its reasoning strategies based on
task complexity dynamically. Furthermore, we incorporate an external reward
model to evaluate the overall consistency and quality of the reasoning process,
complemented by think-based rewards that help the model distinguish between
valid and flawed reasoning paths during training. Experimental results
demonstrate that our Audio-Thinker model outperforms existing
reasoning-oriented LALMs across various benchmark tasks, exhibiting superior
reasoning and generalization capabilities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [359] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: A framework integrating generative AI with multi-disciplinary literature to design bioinspired materials, validated by real-world experiments.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in applying LLMs to discipline-specific experimental science, especially in multi-disciplinary fields like materials science.

Method: Combines fine-tuned models (BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and Hierarchical Sampling to extract insights and design experiments.

Result: Successfully fabricated a novel pollen-based adhesive with tunable properties, validating the AI-assisted approach.

Conclusion: AI-assisted ideation can effectively drive real-world materials design and enhance human-AI collaboration.

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [360] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: AMFT introduces a single-stage algorithm to balance SFT and RL dynamically using meta-gradient adaptive weights, achieving state-of-the-art performance on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the suboptimal trade-offs and catastrophic forgetting in the traditional two-stage SFT-RL pipeline for LLM fine-tuning.

Method: Proposes Adaptive Meta Fine-Tuning (AMFT), which uses a meta-gradient adaptive weight controller to dynamically balance SFT and RL rewards.

Result: AMFT achieves superior performance and generalization on benchmarks like mathematical reasoning and vision-language navigation.

Conclusion: AMFT offers a principled, stable, and effective paradigm for LLM alignment, validated by ablation studies and open-sourced code.

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [361] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: Klear-Reasoner is a high-performance reasoning model with detailed training insights, achieving top scores on benchmarks like AIME and LiveCodeBench.


<details>
  <summary>Details</summary>
Motivation: Addressing reproducibility issues in high-performance inference models due to incomplete training details.

Method: Uses long Chain-of-Thought supervised fine-tuning (long CoT SFT) and reinforcement learning (RL), with proposed Gradient-Preserving clipping Policy Optimization (GPPO).

Result: Scores 90.5% on AIME 2024, 83.2% on AIME 2025, 66.0% on LiveCodeBench V5, and 58.1% on LiveCodeBench V6.

Conclusion: High-quality data and GPPO enhance reasoning performance, making Klear-Reasoner a robust model for complex tasks.

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [362] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: GLiClass, a novel method adapting GLiNER for sequence classification, offers high accuracy and efficiency, addressing limitations of generative LLMs, cross-encoders, and embedding-based approaches.


<details>
  <summary>Details</summary>
Motivation: Modern AI systems require efficient, accurate classification with dynamic needs, but existing methods (generative LLMs, cross-encoders, embedding-based) have flaws like inconsistency, inefficiency, or poor handling of complex scenarios.

Method: GLiClass adapts GLiNER for sequence classification and uses PPO for multi-label training in data-sparse or feedback-driven conditions.

Result: Achieves strong accuracy and efficiency, comparable to embedding-based methods, with flexibility for zero-shot and few-shot learning.

Conclusion: GLiClass effectively balances accuracy, efficiency, and adaptability, addressing key challenges in classification tasks.

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [363] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: GRAO (Group Relative Alignment Optimization) unifies SFT and RL strengths for language model alignment, outperforming baselines with innovations like multi-sample generation and intra-group weighting.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of SFT (constrained by offline policy) and RL (low sample efficiency) in language model alignment.

Method: Proposes GRAO with multi-sample generation, Group Direct Alignment Loss, and reference-aware parameter updates.

Result: Achieves 57.70%, 17.65%, 7.95%, and 5.18% improvements over SFT, DPO, PPO, and GRPO baselines.

Conclusion: GRAO offers a theoretically grounded and empirically validated framework for efficient language model alignment.

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [364] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: PAMA is a scalable, efficient algorithm for multi-objective alignment in LLMs, reducing complexity from O(n^2*d) to O(n) and enabling practical deployment.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods like RLHF optimize for a single reward, failing to capture diverse human preferences, limiting LLM adaptability.

Method: PAMA transforms multi-objective RLHF into convex optimization with a closed-form solution, ensuring scalability and efficiency.

Result: PAMA achieves robust multi-objective alignment across models (125M to 7B parameters) with theoretical guarantees of Pareto optimality.

Conclusion: PAMA addresses the previously intractable MOA problem, enabling versatile LLM alignment with diverse human values for real-world applications.

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [365] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: A transfer learning-based predictive process monitoring (PPM) technique enables organizations with limited event data to implement PPM by transferring knowledge from similar processes.


<details>
  <summary>Details</summary>
Motivation: Existing PPM techniques require ample event data, which some organizations lack, limiting their ability to use PPM for proactive decision-making.

Method: The paper presents a transfer learning-based PPM technique, tested in two real-life IT service management use cases, transferring knowledge between similar processes.

Result: Experiments show knowledge transfer between similar processes (intra- and inter-organizational) enables effective PPM in target contexts.

Conclusion: The technique allows organizations to leverage transfer learning for PPM, even with limited data, by sharing pre-trained models across boundaries.

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [366] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: The paper reviews RL techniques for LLM reasoning, identifies challenges like lack of standardization, and proposes a minimalist combination of techniques for improved performance.


<details>
  <summary>Details</summary>
Motivation: Address the absence of standardized guidelines and fragmented understanding of RL techniques in LLM reasoning, along with inconsistent experimental results.

Method: Systematic review and rigorous reproduction of RL techniques within a unified framework, analyzing mechanisms, scenarios, and principles through fine-grained experiments.

Result: A minimalist combination of two techniques outperforms others like GRPO and DAPO, unlocking critic-free policy learning with vanilla PPO loss.

Conclusion: Provides clear guidelines for RL technique selection and a reliable roadmap for practitioners, demonstrating the effectiveness of a simple combined approach.

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


### [367] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: The paper introduces Second-Order MeanFlow, extending MeanFlow by incorporating average acceleration fields, proving its feasibility, expressivity, and scalable implementation via fast attention approximations.


<details>
  <summary>Details</summary>
Motivation: To enhance generative modelling by extending MeanFlow with higher-order dynamics (average acceleration) for richer dynamics while maintaining practical sampling efficiency.

Method: Theoretical study of Second-Order MeanFlow, proving consistency conditions, analyzing expressivity via circuit complexity, and deriving scalable implementation criteria using fast attention approximations.

Result: Second-Order MeanFlow supports stable one-step sampling, fits within the TC⁰ class, and allows efficient attention approximations with provable error bounds.

Conclusion: The work provides a theoretical foundation for high-order flow matching models, combining rich dynamics with practical efficiency.

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [368] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: The paper proposes an indoor navigation system combining vision-based localization (using ResNet-50) and LLM-based navigation (using ChatGPT), achieving high accuracy (96%) in localization and 75% in navigation instructions.


<details>
  <summary>Details</summary>
Motivation: Indoor navigation is challenging due to unreliable GPS and complex environments. The study aims to provide a scalable, infrastructure-free solution.

Method: Uses ResNet-50 for vision-based localization and ChatGPT for navigation, tested in an office corridor with repetitive features.

Result: Localization achieved 96% accuracy; navigation instructions were 75% accurate, with limitations in zero-shot reasoning and inference time.

Conclusion: The approach shows promise for resource-constrained settings, leveraging off-the-shelf cameras and public floor plans.

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [369] [Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading](https://arxiv.org/abs/2508.07408)
*Yueyi Wang,Qiyao Wei*

Main category: q-fin.ST

TL;DR: The study demonstrates how large language models (LLMs) can annotate financial tweets and discover alpha signals, revealing statistically significant negative alpha for certain event labels.


<details>
  <summary>Details</summary>
Motivation: To explore the utility of LLMs in financial semantic annotation and alpha signal discovery from social media sentiment.

Method: Using an LLM to label high-sentiment tweets and aligning them with forward returns to evaluate efficacy and tradability.

Result: Certain event labels consistently yield negative alpha (Sharpe ratios as low as -0.38, information coefficients >0.05), statistically significant at 95% confidence.

Conclusion: LLMs can transform unstructured social media text into structured financial signals, with transparency and reproducibility as key contributions.

Abstract: In this study, we wish to showcase the unique utility of large language
models (LLMs) in financial semantic annotation and alpha signal discovery.
Leveraging a corpus of company-related tweets, we use an LLM to automatically
assign multi-label event categories to high-sentiment-intensity tweets. We
align these labeled sentiment signals with forward returns over 1-to-7-day
horizons to evaluate their statistical efficacy and market tradability. Our
experiments reveal that certain event labels consistently yield negative alpha,
with Sharpe ratios as low as -0.38 and information coefficients exceeding 0.05,
all statistically significant at the 95\% confidence level. This study
establishes the feasibility of transforming unstructured social media text into
structured, multi-label event variables. A key contribution of this work is its
commitment to transparency and reproducibility; all code and methodologies are
made publicly available. Our results provide compelling evidence that social
media sentiment is a valuable, albeit noisy, signal in financial forecasting
and underscore the potential of open-source frameworks to democratize
algorithmic trading research.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [370] [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050)
*Wenhan Liu,Xinyu Ma,Weiwei Sun,Yutao Zhu,Yuchen Li,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: The paper introduces ReasonRank, a reasoning-intensive reranker, using automated data synthesis and a two-stage training approach to improve listwise ranking performance.


<details>
  <summary>Details</summary>
Motivation: Existing rerankers perform poorly in complex ranking scenarios due to lack of reasoning-intensive training data.

Method: Proposes an automated data synthesis framework and a two-stage post-training approach (SFT + RL) with a multi-view ranking reward.

Result: ReasonRank outperforms baselines, achieves SOTA performance (40.6 on BRIGHT leaderboard), and has lower latency.

Conclusion: ReasonRank effectively enhances reasoning-intensive listwise ranking, demonstrating superior performance and efficiency.

Abstract: Large Language Model (LLM) based listwise ranking has shown superior
performance in many passage ranking tasks. With the development of Large
Reasoning Models, many studies have demonstrated that step-by-step reasoning
during test-time helps improve listwise ranking performance. However, due to
the scarcity of reasoning-intensive training data, existing rerankers perform
poorly in many complex ranking scenarios and the ranking ability of
reasoning-intensive rerankers remains largely underdeveloped. In this paper, we
first propose an automated reasoning-intensive training data synthesis
framework, which sources training queries and passages from diverse domains and
applies DeepSeek-R1 to generate high-quality training labels. A
self-consistency data filtering mechanism is designed to ensure the data
quality. To empower the listwise reranker with strong reasoning ability, we
further propose a two-stage post-training approach, which includes a cold-start
supervised fine-tuning (SFT) stage for reasoning pattern learning and a
reinforcement learning (RL) stage for further ranking ability enhancement.
During the RL stage, based on the nature of listwise ranking, we design a
multi-view ranking reward, which is more effective than a ranking metric-based
reward. Extensive experiments demonstrate that our trained reasoning-intensive
reranker \textbf{ReasonRank} outperforms existing baselines significantly and
also achieves much lower latency than pointwise reranker Rank1. \textbf{Through
further experiments, our ReasonRank has achieved state-of-the-art (SOTA)
performance 40.6 on the BRIGHT
leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are
available at https://github.com/8421BCD/ReasonRank.

</details>


### [371] [PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization](https://arxiv.org/abs/2508.07342)
*Kepu Zhang,Teng Shi,Weijie Yu,Jun Xu*

Main category: cs.IR

TL;DR: PrLM, a reinforcement learning framework, improves personalized retrieval-augmented generation by explicitly reasoning over user profiles, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on LLMs to implicitly integrate retrieved user profiles, leading to sensitivity to retrieval quality and misaligned responses.

Method: PrLM uses reinforcement learning guided by a contrastively trained personalization reward model to train LLMs for explicit reasoning over user profiles.

Result: PrLM outperforms existing methods on three datasets and remains robust across varying retrieval conditions.

Conclusion: PrLM effectively addresses the limitations of implicit integration, enhancing personalized response generation.

Abstract: Personalized retrieval-augmented generation (RAG) aims to produce
user-tailored responses by incorporating retrieved user profiles alongside the
input query. Existing methods primarily focus on improving retrieval and rely
on large language models (LLMs) to implicitly integrate the retrieved context
with the query. However, such models are often sensitive to retrieval quality
and may generate responses that are misaligned with user preferences. To
address this limitation, we propose PrLM, a reinforcement learning framework
that trains LLMs to explicitly reason over retrieved user profiles. Guided by a
contrastively trained personalization reward model, PrLM effectively learns
from user responses without requiring annotated reasoning paths. Experiments on
three personalized text generation datasets show that PrLM outperforms existing
methods and remains robust across varying numbers of retrieved profiles and
different retrievers.

</details>


### [372] [Improving Document Retrieval Coherence for Semantically Equivalent Queries](https://arxiv.org/abs/2508.07975)
*Stefano Campese,Alessandro Moschitti,Ivano Lauriola*

Main category: cs.IR

TL;DR: A new Multi-Negative Ranking loss variation improves DR model coherence and accuracy by reducing sensitivity to query variations.


<details>
  <summary>Details</summary>
Motivation: Address the sensitivity of Dense Retrieval (DR) models to small query and document lexicon variations, aiming for more coherent retrievals.

Method: Proposes a variation of the Multi-Negative Ranking loss to penalize discrepancies in top-k documents for semantically similar queries.

Result: Models trained with the proposed loss show lower sensitivity and higher accuracy on datasets like MS-MARCO, Natural Questions, BEIR, and TREC DL 19/20.

Conclusion: The new loss function enhances DR model performance by improving coherence and accuracy while reducing sensitivity to query variations.

Abstract: Dense Retrieval (DR) models have proven to be effective for Document
Retrieval and Information Grounding tasks. Usually, these models are trained
and optimized for improving the relevance of top-ranked documents for a given
query. Previous work has shown that popular DR models are sensitive to the
query and document lexicon: small variations of it may lead to a significant
difference in the set of retrieved documents. In this paper, we propose a
variation of the Multi-Negative Ranking loss for training DR that improves the
coherence of models in retrieving the same documents with respect to
semantically similar queries. The loss penalizes discrepancies between the
top-k ranked documents retrieved for diverse but semantic equivalent queries.
We conducted extensive experiments on various datasets, MS-MARCO, Natural
Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes
by our loss are subject to lower sensitivity, and, (ii) interestingly, higher
accuracy.

</details>


### [373] [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches](https://arxiv.org/abs/2508.08088)
*Jiejun Tan,Zhicheng Dou,Yan Yu,Jiehan Cheng,Qiang Ju,Jian Xie,Ji-Rong Wen*

Main category: cs.IR

TL;DR: HierSearch, a hierarchical agentic deep search framework, improves performance by coordinating local and Web search agents using hierarchical RL, outperforming flat RL and other baselines.


<details>
  <summary>Details</summary>
Motivation: Enterprises need private deep search systems combining local and Web corpus, but existing methods are limited to single sources and face inefficiencies.

Method: HierSearch uses hierarchical RL: low-level agents retrieve evidence from local/Web domains, while a high-level planner coordinates and refines knowledge.

Result: HierSearch outperforms flat RL and other baselines in six benchmarks across general, finance, and medical domains.

Conclusion: HierSearch effectively addresses inefficiencies and limitations of single-source deep search, proving superior in multi-source retrieval tasks.

Abstract: Recently, large reasoning models have demonstrated strong mathematical and
coding abilities, and deep search leverages their reasoning capabilities in
challenging information retrieval tasks. Existing deep search works are
generally limited to a single knowledge source, either local or the Web.
However, enterprises often require private deep search systems that can
leverage search tools over both local and the Web corpus. Simply training an
agent equipped with multiple search tools using flat reinforcement learning
(RL) is a straightforward idea, but it has problems such as low training data
efficiency and poor mastery of complex tools. To address the above issue, we
propose a hierarchical agentic deep search framework, HierSearch, trained with
hierarchical RL. At the low level, a local deep search agent and a Web deep
search agent are trained to retrieve evidence from their corresponding domains.
At the high level, a planner agent coordinates low-level agents and provides
the final answer. Moreover, to prevent direct answer copying and error
propagation, we design a knowledge refiner that filters out hallucinations and
irrelevant evidence returned by low-level agents. Experiments show that
HierSearch achieves better performance compared to flat RL, and outperforms
various deep search and multi-source retrieval-augmented generation baselines
in six benchmarks across general, finance, and medical domains.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [374] [TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree](https://arxiv.org/abs/2508.07014)
*Andrei Andrusenko,Vladimir Bataev,Lilit Grigoryan,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: A universal ASR context-biasing framework is proposed, supporting major ASR types without speed degradation, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Existing context-biasing approaches require additional training, slow decoding, or limit ASR system types, prompting a need for a more versatile solution.

Method: The framework uses a GPU-accelerated word boosting tree, enabling shallow fusion for greedy and beam search decoding with up to 20K key phrases.

Result: The method outperforms open-source context-biasing approaches in accuracy and decoding speed.

Conclusion: The framework is open-sourced in the NeMo toolkit, offering a high-efficiency solution for ASR context-biasing.

Abstract: Recognizing specific key phrases is an essential task for contextualized
Automatic Speech Recognition (ASR). However, most existing context-biasing
approaches have limitations associated with the necessity of additional model
training, significantly slow down the decoding process, or constrain the choice
of the ASR system type. This paper proposes a universal ASR context-biasing
framework that supports all major types: CTC, Transducers, and Attention
Encoder-Decoder models. The framework is based on a GPU-accelerated word
boosting tree, which enables it to be used in shallow fusion mode for greedy
and beam search decoding without noticeable speed degradation, even with a vast
number of key phrases (up to 20K items). The obtained results showed high
efficiency of the proposed method, surpassing the considered open-source
context-biasing approaches in accuracy and decoding speed. Our context-biasing
framework is open-sourced as a part of the NeMo toolkit.

</details>


### [375] [FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities](https://arxiv.org/abs/2508.07315)
*Lilit Grigoryan,Vladimir Bataev,Nikolay Karpov,Andrei Andrusenko,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: FlexCTC is a GPU-based beam decoding toolkit for CTC models, offering fast, user-friendly, and extensible decoding with advanced contextualization techniques.


<details>
  <summary>Details</summary>
Motivation: Standard beam search implementations are slow, sequential, and CPU-bound, limiting their efficiency on modern hardware.

Method: Developed in Python and PyTorch, FlexCTC provides a fully batched GPU implementation with CUDA Graphs for minimized overhead and supports N-gram LM fusion and phrase-level boosting.

Result: The toolkit enables accurate and efficient decoding, suitable for research and production.

Conclusion: FlexCTC is a high-performance, GPU-optimized alternative to traditional decoders, enhancing speech recognition quality and speed.

Abstract: While beam search improves speech recognition quality over greedy decoding,
standard implementations are slow, often sequential, and CPU-bound. To fully
leverage modern hardware capabilities, we present a novel open-source FlexCTC
toolkit for fully GPU-based beam decoding, designed for Connectionist Temporal
Classification (CTC) models. Developed entirely in Python and PyTorch, it
offers a fast, user-friendly, and extensible alternative to traditional C++,
CUDA, or WFST-based decoders. The toolkit features a high-performance, fully
batched GPU implementation with eliminated CPU-GPU synchronization and
minimized kernel launch overhead via CUDA Graphs. It also supports advanced
contextualization techniques, including GPU-powered N-gram language model
fusion and phrase-level boosting. These features enable accurate and efficient
decoding, making them suitable for both research and production use.

</details>


### [376] [KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features](https://arxiv.org/abs/2508.07337)
*Ivan Kukanov,Jun Wah Ng*

Main category: eess.AS

TL;DR: The paper proposes a multimodal approach for detecting and localizing deepfakes, combining handcrafted visual features and self-supervised audio learning with graph attention networks, achieving high performance on the AV-Deepfake1M++ dataset.


<details>
  <summary>Details</summary>
Motivation: The rise of sophisticated temporal deepfakes necessitates robust detection methods that generalize to unseen attacks and are computationally efficient.

Method: Uses handcrafted visual features and a self-supervised learning backbone with graph attention networks for audio, focusing on interpretability and adaptability.

Result: Achieves 92.78% AUC for deepfake classification and 0.3536 IoU for temporal localization using audio alone.

Conclusion: The multimodal approach balances performance and real-world deployment, offering resilience and interpretability.

Abstract: The rapid development of audio-driven talking head generators and advanced
Text-To-Speech (TTS) models has led to more sophisticated temporal deepfakes.
These advances highlight the need for robust methods capable of detecting and
localizing deepfakes, even under novel, unseen attack scenarios. Current
state-of-the-art deepfake detectors, while accurate, are often computationally
expensive and struggle to generalize to novel manipulation techniques. To
address these challenges, we propose multimodal approaches for the
AV-Deepfake1M 2025 challenge. For the visual modality, we leverage handcrafted
features to improve interpretability and adaptability. For the audio modality,
we adapt a self-supervised learning (SSL) backbone coupled with graph attention
networks to capture rich audio representations, improving detection robustness.
Our approach strikes a balance between performance and real-world deployment,
focusing on resilience and potential interpretability. On the AV-Deepfake1M++
dataset, our multimodal system achieves AUC of 92.78% for deepfake
classification task and IoU of 0.3536 for temporal localization using only the
audio modality.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [377] [Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound](https://arxiv.org/abs/2508.06921)
*Zhongyu Chen,Chenyang Li,Xuesong Li,Dianye Huang,Zhongliang Jiang,Stefanie Speidel,Xiangyu Chu,K. W. Samuel Au*

Main category: cs.RO

TL;DR: A method using needle vibration to restore alignment in robotic ultrasound-guided needle insertion, effective even when the needle is out of plane.


<details>
  <summary>Details</summary>
Motivation: Challenges like speckle noise and low resolution make needle detection difficult, especially when visibility is lost.

Method: Uses a vibration-based energy metric and a control strategy to reposition the ultrasound probe.

Result: Achieves translational error of 0.41±0.27 mm and rotational error of 0.51±0.19 degrees.

Conclusion: The proposed vibration-based method is robust and effective for needle alignment in challenging conditions.

Abstract: Precise needle alignment is essential for percutaneous needle insertion in
robotic ultrasound-guided procedures. However, inherent challenges such as
speckle noise, needle-like artifacts, and low image resolution make robust
needle detection difficult, particularly when visibility is reduced or lost. In
this paper, we propose a method to restore needle alignment when the ultrasound
imaging plane and the needle insertion plane are misaligned. Unlike many
existing approaches that rely heavily on needle visibility in ultrasound
images, our method uses a more robust feature by periodically vibrating the
needle using a mechanical system. Specifically, we propose a vibration-based
energy metric that remains effective even when the needle is fully out of
plane. Using this metric, we develop a control strategy to reposition the
ultrasound probe in response to misalignments between the imaging plane and the
needle insertion plane in both translation and rotation. Experiments conducted
on ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided
needle insertion system demonstrate the effectiveness of the proposed approach.
The experimental results show the translational error of 0.41$\pm$0.27 mm and
the rotational error of 0.51$\pm$0.19 degrees.

</details>


### [378] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: The paper introduces the A2A benchmark for agricultural VLN and proposes AgriVLN, a baseline method that improves navigation for agricultural robots using VLM and an STL module.


<details>
  <summary>Details</summary>
Motivation: Agricultural robots lack mobility and adaptability due to reliance on manual operation or fixed railways. Existing VLN methods are not tailored for agricultural scenes.

Method: The A2A benchmark is created with 1,560 episodes in six agricultural scenes. AgriVLN, a VLM-based method, is proposed and enhanced with an STL module for instruction decomposition.

Result: AgriVLN performs well on short instructions but struggles with long ones. The STL module improves SR from 0.33 to 0.47, outperforming existing VLN methods in agriculture.

Conclusion: The A2A benchmark and AgriVLN advance agricultural VLN, with the STL module significantly improving performance, though challenges remain with long instructions.

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks,
nevertheless, still heavily rely on manual operation or untransportable railway
for movement, resulting in limited mobility and poor adaptability.
Vision-and-Language Navigation (VLN) enables robots to navigate to the target
destinations following natural language instructions, demonstrating strong
performance on several domains. However, none of the existing benchmarks or
methods is specifically designed for agricultural scenes. To bridge this gap,
we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560
episodes across six diverse agricultural scenes, in which all realistic RGB
videos are captured by front-facing camera on a quadruped robot at a height of
0.38 meters, aligning with the practical deployment conditions. Meanwhile, we
propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)
baseline based on Vision-Language Model (VLM) prompted with carefully crafted
templates, which can understand both given instructions and agricultural
environments to generate appropriate low-level actions for robot control. When
evaluated on A2A, AgriVLN performs well on short instructions but struggles
with long instructions, because it often fails to track which part of the
instruction is currently being executed. To address this, we further propose
Subtask List (STL) instruction decomposition module and integrate it into
AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare
AgriVLN with several existing VLN methods, demonstrating the state-of-the-art
performance in the agricultural domain.

</details>


### [379] [Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2508.07560)
*Yan Gong,Naibang Wang,Jianli Lu,Xinyu Zhang,Yongsheng Gao,Jie Zhao,Zifan Huang,Haozhi Bai,Nanxin Zeng,Nayu Su,Lei Yang,Ziying Song,Xiaoxi Hu,Xinmin Jiang,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.RO

TL;DR: A survey on Bird's-Eye-View (BEV) perception in autonomous driving, focusing on safety-critical challenges, frameworks, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: Ensuring BEV perception's safety and reliability in complex real-world scenarios like occlusions, adverse weather, and dynamic traffic.

Method: Systematic review of BEV perception frameworks across single-modality, multimodal, and multi-agent stages, and analysis of public datasets.

Result: Identifies key challenges (e.g., open-set recognition, sensor degradation) and outlines future research directions (e.g., end-to-end systems, large language models).

Conclusion: BEV perception is foundational but faces critical safety challenges; future work must address robustness and integration with advanced technologies.

Abstract: Bird's-Eye-View (BEV) perception has become a foundational paradigm in
autonomous driving, enabling unified spatial representations that support
robust multi-sensor fusion and multi-agent collaboration. As autonomous
vehicles transition from controlled environments to real-world deployment,
ensuring the safety and reliability of BEV perception in complex scenarios -
such as occlusions, adverse weather, and dynamic traffic - remains a critical
challenge. This survey provides the first comprehensive review of BEV
perception from a safety-critical perspective, systematically analyzing
state-of-the-art frameworks and implementation strategies across three
progressive stages: single-modality vehicle-side, multimodal vehicle-side, and
multi-agent collaborative perception. Furthermore, we examine public datasets
encompassing vehicle-side, roadside, and collaborative settings, evaluating
their relevance to safety and robustness. We also identify key open-world
challenges - including open-set recognition, large-scale unlabeled data, sensor
degradation, and inter-agent communication latency - and outline future
research directions, such as integration with end-to-end autonomous driving
systems, embodied intelligence, and large language models.

</details>


### [380] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: An AI-driven perception system for autonomous quadcopters in GPS-denied indoor environments, using cloud computing, YOLOv11, Depth Anything V2, and a custom PCB for robust navigation.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous navigation in confined, GPS-denied spaces by integrating advanced AI and cloud computing for efficient processing and decision-making.

Method: Combines YOLOv11 for object detection, Depth Anything V2 for depth estimation, a custom PCB with ToF sensors and IMU, and a cloud-based LLM for context-aware decisions. Uses a virtual safety envelope and multithreaded architecture for low-latency processing.

Result: Achieves mAP50 of 0.6 for object detection, depth MAE of 7.2 cm, 16 safety breaches in 42 trials, and sub-1-second latency.

Conclusion: The framework effectively supports drone autonomy in GPS-denied confined spaces, enhancing perception and navigation.

Abstract: This paper introduces an advanced AI-driven perception system for autonomous
quadcopter navigation in GPS-denied indoor environments. The proposed framework
leverages cloud computing to offload computationally intensive tasks and
incorporates a custom-designed printed circuit board (PCB) for efficient sensor
data acquisition, enabling robust navigation in confined spaces. The system
integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth
estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial
Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for
context-aware decision-making. A virtual safety envelope, enforced by
calibrated sensor offsets, ensures collision avoidance, while a multithreaded
architecture achieves low-latency processing. Enhanced spatial awareness is
facilitated by 3D bounding box estimation with Kalman filtering. Experimental
results in an indoor testbed demonstrate strong performance, with object
detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation
Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42
trials over approximately 11 minutes, and end-to-end system latency below 1
second. This cloud-supported, high-intelligence framework serves as an
auxiliary perception and navigation system, complementing state-of-the-art
drone autonomy for GPS-denied confined spaces.

</details>


### [381] [ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks](https://arxiv.org/abs/2508.08240)
*Kaijun Wang,Liqin Lu,Mingyu Liu,Jianuo Jiang,Zeju Li,Bolin Zhang,Wancai Zheng,Xinyi Yu,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: ODYSSEY is a unified mobile manipulation framework for quadruped robots, integrating task planning and whole-body control to address challenges in language-guided long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in current mobile manipulation, such as confined tabletop scenarios, insufficient generalization, and the lack of studies on maneuverability and precise control in unstructured settings.

Method: ODYSSEY uses a hierarchical planner with a vision-language model for task decomposition and a whole-body policy for robust control. It includes a benchmark for long-horizon tasks.

Result: The framework demonstrates successful sim-to-real transfer, generalization, and robustness in real-world deployments.

Conclusion: ODYSSEY advances the feasibility of generalized robotic assistants for complex tasks in unstructured environments.

Abstract: Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [382] [MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training](https://arxiv.org/abs/2508.07590)
*Xiongwei Xiao,Baoying Chen,Jishen Zeng,Jianquan Yang*

Main category: cs.MM

TL;DR: A lightweight face quality assessment network (MSPT) is proposed, using multi-stage progressive training to balance performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with face image quality assessment, and learning-based approaches are computationally expensive.

Method: Uses a three-stage progressive training strategy with diverse data and increasing resolution.

Result: Achieved second highest score on VQualA 2025 benchmark, matching or outperforming state-of-the-art methods.

Conclusion: MSPT offers high performance with efficient inference, addressing limitations of existing methods.

Abstract: Accurately assessing the perceptual quality of face images is crucial,
especially with the rapid progress in face restoration and generation.
Traditional quality assessment methods often struggle with the unique
characteristics of face images, limiting their generalizability. While
learning-based approaches demonstrate superior performance due to their strong
fitting capabilities, their high complexity typically incurs significant
computational and storage costs, hindering practical deployment. To address
this, we propose a lightweight face quality assessment network with Multi-Stage
Progressive Training (MSPT). Our network employs a three-stage progressive
training strategy that gradually introduces more diverse data samples and
increases input image resolution. This novel approach enables lightweight
networks to achieve high performance by effectively learning complex quality
features while significantly mitigating catastrophic forgetting. Our MSPT
achieved the second highest score on the VQualA 2025 face image quality
assessment benchmark dataset, demonstrating that MSPT achieves comparable or
better performance than state-of-the-art methods while maintaining efficient
inference.

</details>


### [383] [AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition](https://arxiv.org/abs/2508.07608)
*Junxiao Xue,Xiaozhen Liu,Xuecheng Wu,Xinyi Yin,Danlei Huang,Fei Yu*

Main category: cs.MM

TL;DR: AD-AVSR introduces bidirectional modality enhancement for AVSR, improving noise robustness and performance by capturing heterogeneous correlations between audio-visual data.


<details>
  <summary>Details</summary>
Motivation: Existing AVSR methods lack the ability to fully capture complementary correlations in asymmetric information conditions, limiting their effectiveness.

Method: Proposes AD-AVSR with bidirectional enhancement: Audio-aware Visual Refinement Module and Cross-modal Noise Suppression Masking Module, supported by a threshold-based selection mechanism.

Result: Outperforms SOTA methods on LRS2 and LRS3 datasets in performance and noise robustness.

Conclusion: AD-AVSR's bidirectional approach effectively enhances AVSR by leveraging cross-modal interactions and filtering irrelevant data.

Abstract: Audio-visual speech recognition (AVSR) combines audio-visual modalities to
improve speech recognition, especially in noisy environments. However, most
existing methods deploy the unidirectional enhancement or symmetric fusion
manner, which limits their capability to capture heterogeneous and
complementary correlations of audio-visual data-especially under asymmetric
information conditions. To tackle these gaps, we introduce a new AVSR framework
termed AD-AVSR based on bidirectional modality enhancement. Specifically, we
first introduce the audio dual-stream encoding strategy to enrich audio
representations from multiple perspectives and intentionally establish
asymmetry to support subsequent cross-modal interactions. The enhancement
process involves two key components, Audio-aware Visual Refinement Module for
enhanced visual representations under audio guidance, and Cross-modal Noise
Suppression Masking Module which refines audio representations using visual
cues, collaboratively leading to the closed-loop and bidirectional information
flow. To further enhance correlation robustness, we adopt a threshold-based
selection mechanism to filter out irrelevant or weakly correlated audio-visual
pairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate
that our AD-AVSR consistently surpasses SOTA methods in both performance and
noise robustness, highlighting the effectiveness of our model design.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [384] [SQL-Exchange: Transforming SQL Queries Across Domains](https://arxiv.org/abs/2508.07087)
*Mohammadreza Daviran,Brian Lin,Davood Rafiei*

Main category: cs.DB

TL;DR: SQL-Exchange maps SQL queries across schemas, preserving structure while adapting domain elements, improving text-to-SQL performance.


<details>
  <summary>Details</summary>
Motivation: To enhance text-to-SQL systems by enabling query mapping across schemas while maintaining query structure and domain alignment.

Method: Introduces SQL-Exchange, evaluates feasibility, benefits, and impact on text-to-SQL performance through structural alignment, execution validity, and semantic correctness tests.

Result: Effective across diverse schemas and query types, mapped queries improve text-to-SQL performance over source schema queries.

Conclusion: SQL-Exchange successfully bridges schema gaps, boosting text-to-SQL system performance with mapped queries.

Abstract: We introduce SQL-Exchange, a framework for mapping SQL queries across
different database schemas by preserving the source query structure while
adapting domain-specific elements to align with the target schema. We
investigate the conditions under which such mappings are feasible and
beneficial, and examine their impact on enhancing the in-context learning
performance of text-to-SQL systems as a downstream task. Our comprehensive
evaluation across multiple model families and benchmark datasets--assessing
structural alignment with source queries, execution validity on target
databases, and semantic correctness--demonstrates that SQL-Exchange is
effective across a wide range of schemas and query types. Our results further
show that using mapped queries as in-context examples consistently improves
text-to-SQL performance over using queries from the source schema.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [385] [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)
*Catherine Yeh,Tara Menon,Robin Singh Arya,Helen He,Moira Weigel,Fernanda Viégas,Martin Wattenberg*

Main category: cs.HC

TL;DR: The paper introduces an LLM-driven pipeline to extract narrative data from literature and visualize it interactively as Story Ribbons, aiding literary analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle to capture structured narrative relationships from unstructured text. LLMs offer potential to enhance storyline visualization.

Method: Developed an LLM-based pipeline for automatic narrative data extraction and created Story Ribbons, an interactive visualization tool.

Result: Evaluated on 36 works, the pipeline and Story Ribbons proved effective in streamlining narrative visualization and uncovering new insights.

Conclusion: LLMs can enhance narrative visualization but have limitations; interaction designs are proposed to mitigate these issues.

Abstract: Analyzing literature involves tracking interactions between characters,
locations, and themes. Visualization has the potential to facilitate the
mapping and analysis of these complex relationships, but capturing structured
information from unstructured story data remains a challenge. As large language
models (LLMs) continue to advance, we see an opportunity to use their text
processing and analysis capabilities to augment and reimagine existing
storyline visualization techniques. Toward this goal, we introduce an
LLM-driven data parsing pipeline that automatically extracts relevant narrative
information from novels and scripts. We then apply this pipeline to create
Story Ribbons, an interactive visualization system that helps novice and expert
literary analysts explore detailed character and theme trajectories at multiple
narrative levels. Through pipeline evaluations and user studies with Story
Ribbons on 36 literary works, we demonstrate the potential of LLMs to
streamline narrative visualization creation and reveal new insights about
familiar stories. We also describe current limitations of AI-based systems, and
interaction motifs designed to address these issues.

</details>


### [386] [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)
*Baihan Lin*

Main category: cs.HC

TL;DR: Conversational DNA is a visual language for analyzing dialogue, revealing hidden patterns through biological metaphors, offering insights beyond traditional methods.


<details>
  <summary>Details</summary>
Motivation: To uncover deeper communication patterns in dialogues (human-human, human-AI, group) by visualizing them as living systems, moving beyond statistical summaries.

Method: Uses biological metaphors (strand thickness, color gradients, helical patterns) to visualize dialogue structure, applied to therapeutic and human-AI conversations.

Result: Reveals interaction patterns missed by traditional methods, demonstrating the approach's effectiveness in understanding dialogue dynamics.

Conclusion: Provides a creative framework bridging data visualization, HCI, and the study of meaningful dialogue in human-AI communication.

Abstract: What if the patterns hidden within dialogue reveal more about communication
than the words themselves? We introduce Conversational DNA, a novel visual
language that treats any dialogue -- whether between humans, between human and
AI, or among groups -- as a living system with interpretable structure that can
be visualized, compared, and understood. Unlike traditional conversation
analysis that reduces rich interaction to statistical summaries, our approach
reveals the temporal architecture of dialogue through biological metaphors.
Linguistic complexity flows through strand thickness, emotional trajectories
cascade through color gradients, conversational relevance forms through
connecting elements, and topic coherence maintains structural integrity through
helical patterns. Through exploratory analysis of therapeutic conversations and
historically significant human-AI dialogues, we demonstrate how this
visualization approach reveals interaction patterns that traditional methods
miss. Our work contributes a new creative framework for understanding
communication that bridges data visualization, human-computer interaction, and
the fundamental question of what makes dialogue meaningful in an age where
humans increasingly converse with artificial minds.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [387] [Membership Inference Attacks with False Discovery Rate Control](https://arxiv.org/abs/2508.07066)
*Chenxu Zhao,Wei Qian,Aobo Chen,Mengdi Huai*

Main category: stat.ML

TL;DR: A novel membership inference attack (MIA) method is proposed to guarantee false discovery rate (FDR) control and marginal probability guarantees, working as a wrapper for existing MIAs.


<details>
  <summary>Details</summary>
Motivation: Existing MIAs lack guarantees on false discovery rates due to unknown distributions and interdependent probabilities, necessitating a method to address these challenges.

Method: The paper designs a new MIA method that ensures FDR guarantees and marginal probability assurances, functioning as a post-hoc wrapper for existing MIAs.

Result: Theoretical analysis and extensive experiments in various settings (e.g., black-box, lifelong learning) confirm the method's effectiveness.

Conclusion: The proposed method successfully provides FDR guarantees and integrates seamlessly with existing MIAs, demonstrating robust performance across diverse scenarios.

Abstract: Recent studies have shown that deep learning models are vulnerable to
membership inference attacks (MIAs), which aim to infer whether a data record
was used to train a target model or not. To analyze and study these
vulnerabilities, various MIA methods have been proposed. Despite the
significance and popularity of MIAs, existing works on MIAs are limited in
providing guarantees on the false discovery rate (FDR), which refers to the
expected proportion of false discoveries among the identified positive
discoveries. However, it is very challenging to ensure the false discovery rate
guarantees, because the underlying distribution is usually unknown, and the
estimated non-member probabilities often exhibit interdependence. To tackle the
above challenges, in this paper, we design a novel membership inference attack
method, which can provide the guarantees on the false discovery rate.
Additionally, we show that our method can also provide the marginal probability
guarantee on labeling true non-member data as member data. Notably, our method
can work as a wrapper that can be seamlessly integrated with existing MIA
methods in a post-hoc manner, while also providing the FDR control. We perform
the theoretical analysis for our method. Extensive experiments in various
settings (e.g., the black-box setting and the lifelong learning setting) are
also conducted to verify the desirable performance of our method.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [388] [Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems](https://arxiv.org/abs/2508.07263)
*Qingyuan Zeng,Shu Jiang,Jiajing Lin,Zhenzhong Wang,Kay Chen Tan,Min Jiang*

Main category: cs.CR

TL;DR: This paper introduces GMEA, a black-box attack framework to remove watermarks from 3D Gaussian Splatting models, revealing vulnerabilities in current copyright protection methods.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored robustness of watermarking techniques in 3DGS and challenge existing copyright protection systems.

Method: Formulates the attack as a multi-objective optimization problem, using a group-based strategy to partition the 3DGS model and an indirect objective function to blind the watermark detector.

Result: GMEA successfully removes 1D and 2D watermarks while maintaining visual quality, exposing vulnerabilities in current systems.

Conclusion: The study highlights the need for more robust watermarking techniques in 3DGS to withstand attacks like GMEA.

Abstract: With the rise of 3D Gaussian Splatting (3DGS), a variety of digital
watermarking techniques, embedding either 1D bitstreams or 2D images, are used
for copyright protection. However, the robustness of these watermarking
techniques against potential attacks remains underexplored. This paper
introduces the first universal black-box attack framework, the Group-based
Multi-objective Evolutionary Attack (GMEA), designed to challenge these
watermarking systems. We formulate the attack as a large-scale multi-objective
optimization problem, balancing watermark removal with visual quality. In a
black-box setting, we introduce an indirect objective function that blinds the
watermark detector by minimizing the standard deviation of features extracted
by a convolutional network, thus rendering the feature maps uninformative. To
manage the vast search space of 3DGS models, we employ a group-based
optimization strategy to partition the model into multiple, independent
sub-optimization problems. Experiments demonstrate that our framework
effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking
methods while maintaining high visual fidelity. This work reveals critical
vulnerabilities in existing 3DGS copyright protection schemes and calls for the
development of more robust watermarking systems.

</details>


### [389] [IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2508.08031)
*Jiayao Wang,Yang Song,Zhendong Zhao,Jiale Zhang,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: The paper proposes IPBA, an imperceptible backdoor attack method for Federated Self-Supervised Learning (FSSL), addressing challenges like limited transferability and feature entanglement to improve stealth and effectiveness.


<details>
  <summary>Details</summary>
Motivation: FSSL is vulnerable to backdoor attacks, but existing methods lack stealth and practicality. The study aims to develop a more effective and imperceptible attack.

Method: IPBA decouples feature distributions of backdoor and augmented samples and uses Sliced-Wasserstein distance to optimize trigger generation.

Result: IPBA outperforms existing methods in performance and robustness across various FSSL scenarios and datasets.

Conclusion: IPBA successfully addresses the limitations of traditional backdoor attacks in FSSL, offering a more stealthy and effective solution.

Abstract: Federated self-supervised learning (FSSL) combines the advantages of
decentralized modeling and unlabeled representation learning, serving as a
cutting-edge paradigm with strong potential for scalability and privacy
preservation. Although FSSL has garnered increasing attention, research
indicates that it remains vulnerable to backdoor attacks. Existing methods
generally rely on visually obvious triggers, which makes it difficult to meet
the requirements for stealth and practicality in real-world deployment. In this
paper, we propose an imperceptible and effective backdoor attack method against
FSSL, called IPBA. Our empirical study reveals that existing imperceptible
triggers face a series of challenges in FSSL, particularly limited
transferability, feature entanglement with augmented samples, and
out-of-distribution properties. These issues collectively undermine the
effectiveness and stealthiness of traditional backdoor attacks in FSSL. To
overcome these challenges, IPBA decouples the feature distributions of backdoor
and augmented samples, and introduces Sliced-Wasserstein distance to mitigate
the out-of-distribution properties of backdoor samples, thereby optimizing the
trigger generation process. Our experimental results on several FSSL scenarios
and datasets show that IPBA significantly outperforms existing backdoor attack
methods in performance and exhibits strong robustness under various defense
mechanisms.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [390] [Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models](https://arxiv.org/abs/2508.07115)
*Antonino Greco,Marco D'Alessandro,Karl J. Friston,Giovanni Pezzulo,Markus Siegel*

Main category: q-bio.NC

TL;DR: Top-down feedback in ConvRNNs improves speed-accuracy trade-off and robustness to noise, especially when combined with stochastic neural variability (dropout).


<details>
  <summary>Details</summary>
Motivation: To understand the functional role of top-down feedback in artificial vision models, contrasting with biological systems that heavily rely on it.

Method: Trained ConvRNNs with and without top-down feedback on image classification, using dropout to simulate stochastic neural variability.

Result: Feedback with dropout enhanced robustness, shaped representational geometry, and stabilized dynamics on low-dimensional manifolds.

Conclusion: Top-down feedback and neural stochasticity together enable resilient sensory coding by balancing chaos and stability.

Abstract: Biological systems leverage top-down feedback for visual processing, yet most
artificial vision models succeed in image classification using purely
feedforward or recurrent architectures, calling into question the functional
significance of descending cortical pathways. Here, we trained convolutional
recurrent neural networks (ConvRNN) on image classification in the presence or
absence of top-down feedback projections to elucidate the specific
computational contributions of those feedback pathways. We found that ConvRNNs
with top-down feedback exhibited remarkable speed-accuracy trade-off and
robustness to noise perturbations and adversarial attacks, but only when they
were trained with stochastic neural variability, simulated by randomly
silencing single units via dropout. By performing detailed analyses to identify
the reasons for such benefits, we observed that feedback information
substantially shaped the representational geometry of the post-integration
layer, combining the bottom-up and top-down streams, and this effect was
amplified by dropout. Moreover, feedback signals coupled with dropout optimally
constrained network activity onto a low-dimensional manifold and encoded object
information more efficiently in out-of-distribution regimes, with top-down
information stabilizing the representational dynamics at the population level.
Together, these findings uncover a dual mechanism for resilient sensory coding.
On the one hand, neural stochasticity prevents unit-level co-adaptation albeit
at the cost of more chaotic dynamics. On the other hand, top-down feedback
harnesses high-level information to stabilize network activity on compact
low-dimensional manifolds.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.CV](#cs.CV) [Total: 91]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 9]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Categorical Classification of Book Summaries Using Word Embedding Techniques](https://arxiv.org/abs/2507.21058)
*Kerem Keskin,Mümine Kaya Keleş*

Main category: cs.CL

TL;DR: The study compares word embedding methods (One Hot Encoding, Word2Vec, TF-IDF) and machine learning models (SVM, Naive Bayes, Logistic Regression) for classifying Turkish book summaries, finding TF-IDF and One-Hot Encoder most effective.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the effectiveness of various word embedding and machine learning techniques for classifying Turkish book summaries.

Method: Used word embedding methods (One Hot Encoding, Word2Vec, TF-IDF) and machine learning models (SVM, Naive Bayes, Logistic Regression) for classification.

Result: TF-IDF and One-Hot Encoder with SVM, Naive Bayes, and Logistic Regression performed best for Turkish texts.

Conclusion: TF-IDF and One-Hot Encoder are recommended for Turkish text classification tasks.

Abstract: In this study, book summaries and categories taken from book sites were
classified using word embedding methods, natural language processing techniques
and machine learning algorithms. In addition, one hot encoding, Word2Vec and
Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are
frequently used word embedding methods were used in this study and their
success was compared. Additionally, the combination table of the pre-processing
methods used is shown and added to the table. Looking at the results, it was
observed that Support Vector Machine, Naive Bayes and Logistic Regression
Models and TF-IDF and One-Hot Encoder word embedding techniques gave more
successful results for Turkish texts.

</details>


### [2] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)
*Sabrina Patania,Luca Annese,Cansu Koyuturk,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.CL

TL;DR: The paper explores socially mediated learning for LLMs, introducing the 'AI Social Gym' where AI learners interact with teacher agents, showing improved knowledge acquisition through dialogic methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with online knowledge integration. Traditional training methods rely on large datasets and sparse feedback, limiting efficiency. Inspired by Vygotsky's theory, the study aims to enhance learning through social interaction.

Method: The 'AI Social Gym' environment facilitates dyadic pedagogical dialogues between AI learners and teachers, focusing on structured dialogue for knowledge acquisition.

Result: Dialogic methods, especially mixed-direction interactions, outperform unidirectional instruction and direct access to structured knowledge, enhancing LLM knowledge acquisition.

Conclusion: Integrating pedagogical insights into AI training improves post-training knowledge acquisition, offering a complementary approach to methods like prompt engineering.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing extensive offline datasets. However, they often face challenges in
acquiring and integrating complex, knowledge online. Traditional AI training
paradigms, predominantly based on supervised learning or reinforcement
learning, mirror a 'Piagetian' model of independent exploration. These
approaches typically rely on large datasets and sparse feedback signals,
limiting the models' ability to learn efficiently from interactions. Drawing
inspiration from Vygotsky's sociocultural theory, this study explores the
potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI
learner agent engages in dyadic pedagogical dialogues with knowledgeable AI
teacher agents. These interactions emphasize external, structured dialogue as a
core mechanism for knowledge acquisition, contrasting with methods that depend
solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the
AI learning process in the context of ontology acquisition. Empirical results
indicate that such dialogic approaches-particularly those involving
mixed-direction interactions combining top-down explanations with
learner-initiated questioning-significantly enhance the LLM's ability to
acquire and apply new knowledge, outperforming both unidirectional
instructional methods and direct access to structured knowledge, formats
typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological
insights into AI and robot training can substantially improve post-training
knowledge acquisition and response quality. This approach offers a
complementary pathway to existing strategies like prompt engineering

</details>


### [3] [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)
*David James Woo,Yangyang Yu,Kai Guo,Yilin Huang,April Ka Yeng Fung*

Main category: cs.CL

TL;DR: The study explores how EFL secondary students edit AI-generated text in expository writing, finding minimal impact of editing on quality but noting AI's supportive role.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of AI-generated text on EFL students' writing processes and compositions, as its role in education grows.

Method: Analyzed screen recordings and compositions of 39 Hong Kong students using qualitative coding, descriptive stats, temporal sequence analysis, and MLR.

Result: AI-generated words positively predicted scores, but editing efforts had little impact, suggesting AI aids but doesn't replace writing skills.

Conclusion: Genre-specific instruction and process-focused writing are crucial before AI integration, with assessments valuing both process and product.

Abstract: Text generated by artificial intelligence (AI) chatbots is increasingly used
in English as a foreign language (EFL) writing contexts, yet its impact on
students' expository writing process and compositions remains understudied.
This research examines how EFL secondary students edit AI-generated text.
Exploring editing behaviors in their expository writing process and in
expository compositions, and their effect on human-rated scores for content,
organization, language, and overall quality. Participants were 39 Hong Kong
secondary students who wrote an expository composition with AI chatbots in a
workshop. A convergent design was employed to analyze their screen recordings
and compositions to examine students' editing behaviors and writing qualities.
Analytical methods included qualitative coding, descriptive statistics,
temporal sequence analysis, human-rated scoring, and multiple linear regression
analysis. We analyzed over 260 edits per dataset, and identified two editing
patterns: one where students refined introductory units repeatedly before
progressing, and another where they quickly shifted to extensive edits in body
units (e.g., topic and supporting sentences). MLR analyses revealed that the
number of AI-generated words positively predicted all score dimensions, while
most editing variables showed minimal impact. These results suggest a
disconnect between students' significant editing effort and improved
composition quality, indicating AI supports but does not replace writing
skills. The findings highlight the importance of genre-specific instruction and
process-focused writing before AI integration. Educators should also develop
assessments valuing both process and product to encourage critical engagement
with AI text.

</details>


### [4] [Which symbol grounding problem should we try to solve?](https://arxiv.org/abs/2507.21080)
*Vincent C. Müller*

Main category: cs.CL

TL;DR: The paper critiques Floridi and Taddeo's 'zero semantic commitment' condition for the grounding problem, argues it's unfulfillable, and suggests rethinking the problem's formulation, emphasizing the role of goals in computational agents.


<details>
  <summary>Details</summary>
Motivation: To challenge existing solutions to the grounding problem and propose a revised understanding of the problem itself.

Method: Critically analyzes Floridi and Taddeo's condition, examines Luc Steels' alternative, and redefines the grounding problem based on computational principles.

Result: The 'zero semantic commitment' condition is unattainable, and the grounding problem should focus on explaining and reproducing meaning in computational agents.

Conclusion: The grounding problem must be reformulated to address the behavioral ability and function of meaning in artificial computational agents.

Abstract: Floridi and Taddeo propose a condition of "zero semantic commitment" for
solutions to the grounding problem, and a solution to it. I argue briefly that
their condition cannot be fulfilled, not even by their own solution. After a
look at Luc Steels' very different competing suggestion, I suggest that we need
to re-think what the problem is and what role the 'goals' in a system play in
formulating the problem. On the basis of a proper understanding of computing, I
come to the conclusion that the only sensible grounding problem is how we can
explain and re-produce the behavioral ability and function of meaning in
artificial computational agents

</details>


### [5] [ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs](https://arxiv.org/abs/2507.21083)
*Franck Bardol*

Main category: cs.CL

TL;DR: GPT-4 adjusts responses based on emotional tone in prompts, showing a 'rebound' bias toward neutrality or positivity, especially on sensitive topics.


<details>
  <summary>Details</summary>
Motivation: To explore how emotional framing in prompts affects GPT-4's responses and identify biases in its behavior.

Method: Systematically varied emotional tone in 156 prompts across topics, analyzed responses, and introduced metrics like 'tone floor' and transition matrices.

Result: GPT-4 is three times less likely to respond negatively to negative prompts, with stronger effects on sensitive topics. Visualizations confirmed semantic drift.

Conclusion: Emotional framing introduces biases in GPT-4, impacting AI alignment and trust. Tools like 'tone floor' help quantify these effects.

Abstract: Large Language Models like GPT-4 adjust their responses not only based on the
question asked, but also on how it is emotionally phrased. We systematically
vary the emotional tone of 156 prompts - spanning controversial and everyday
topics - and analyze how it affects model responses. Our findings show that
GPT-4 is three times less likely to respond negatively to a negatively framed
question than to a neutral one. This suggests a "rebound" bias where the model
overcorrects, often shifting toward neutrality or positivity. On sensitive
topics (e.g., justice or politics), this effect is even more pronounced:
tone-based variation is suppressed, suggesting an alignment override. We
introduce concepts like the "tone floor" - a lower bound in response negativity
- and use tone-valence transition matrices to quantify behavior. Visualizations
based on 1536-dimensional embeddings confirm semantic drift based on tone. Our
work highlights an underexplored class of biases driven by emotional framing in
prompts, with implications for AI alignment and trust. Code and data are
available at: https://github.com/bardolfranck/llm-responses-viewer

</details>


### [6] [Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing](https://arxiv.org/abs/2507.21084)
*Aly M. Kassem,Zhuan Shi,Negar Rostamzadeh,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: MNEME is a lightweight framework using sparse model diffing to detect unintended side effects in fine-tuned or unlearned LLMs, achieving 95% accuracy without custom heuristics.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack general approaches to detect unpredictable side effects in LLMs after fine-tuning or unlearning.

Method: MNEME compares base and fine-tuned models on task-agnostic data to isolate behavioral shifts, using sparse probing and diffing.

Result: MNEME achieves up to 95% accuracy in predicting side effects across five LLMs and three scenarios, aligning with benchmarks.

Conclusion: Sparse probing and diffing provide scalable, automated tools for understanding and managing LLM behavior changes.

Abstract: Large language models (LLMs) are frequently fine-tuned or unlearned to adapt
to new tasks or eliminate undesirable behaviors. While existing evaluation
methods assess performance after such interventions, there remains no general
approach for detecting unintended side effects, such as unlearning biology
content degrading performance on chemistry tasks, particularly when these
effects are unpredictable or emergent. To address this issue, we introduce
MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight
framework for identifying these side effects using sparse model diffing. MNEME
compares base and fine-tuned models on task-agnostic data (for example, The
Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral
shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,
emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent
accuracy in predicting side effects, aligning with known benchmarks and
requiring no custom heuristics. Furthermore, we show that retraining on
high-activation samples can partially reverse these effects. Our results
demonstrate that sparse probing and diffing offer a scalable and automated lens
into fine-tuning-induced model changes, providing practical tools for
understanding and managing LLM behavior.

</details>


### [7] [Multi-Amateur Contrastive Decoding for Text Generation](https://arxiv.org/abs/2507.21086)
*Jaydip Sen,Subhasis Dasgupta,Hetvi Waghela*

Main category: cs.CL

TL;DR: MACD improves text generation by using multiple amateur models to better identify and avoid common flaws like repetition and hallucination, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current CD methods rely on a single amateur model, limiting their ability to address diverse generation issues like repetition and stylistic drift.

Method: MACD employs an ensemble of amateur models, integrating contrastive signals through averaging and consensus penalization, and extends plausibility constraints.

Result: MACD outperforms conventional decoding and CD in fluency, coherence, diversity, and adaptability across domains like news and narrative.

Conclusion: MACD offers a more robust and flexible approach to text generation without needing additional training, enhancing quality and control.

Abstract: Contrastive Decoding (CD) has emerged as an effective inference-time strategy
for enhancing open-ended text generation by exploiting the divergence in output
probabilities between a large expert language model and a smaller amateur
model. Although CD improves coherence and fluency, its dependence on a single
amateur restricts its capacity to capture the diverse and multifaceted failure
modes of language generation, such as repetition, hallucination, and stylistic
drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a
generalization of the CD framework that employs an ensemble of amateur models
to more comprehensively characterize undesirable generation patterns. MACD
integrates contrastive signals through both averaging and consensus
penalization mechanisms and extends the plausibility constraint to operate
effectively in the multi-amateur setting. Furthermore, the framework enables
controllable generation by incorporating amateurs with targeted stylistic or
content biases. Experimental results across multiple domains, such as news,
encyclopedic, and narrative, demonstrate that MACD consistently surpasses
conventional decoding methods and the original CD approach in terms of fluency,
coherence, diversity, and adaptability, all without requiring additional
training or fine-tuning.

</details>


### [8] [QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning](https://arxiv.org/abs/2507.21095)
*Mohammad AL-Smadi*

Main category: cs.CL

TL;DR: The paper proposes a feature-augmented transformer architecture for subjectivity detection in news sentences, combining pre-trained language models with statistical and linguistic features. It achieves competitive results across multiple languages, including zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of distinguishing subjective and objective views in news sentences, leveraging both contextual embeddings and additional features for improved performance.

Method: A feature-augmented transformer architecture combining pre-trained models (AraELECTRA for Arabic, DeBERTa~V3 for others) with lexical features (POS tags, TF-IDF) and a gating mechanism. Evaluated in monolingual, multilingual, and zero-shot settings.

Result: Competitive performance across languages, with top rankings in English, German, Arabic, and Romanian. Ablation analysis highlights the importance of TF-IDF features, gating, and cross-lingual transfer.

Conclusion: The approach is effective for subjectivity detection, with sensitivity to cross-lingual fine-tuning order and linguistic proximity of training languages.

Abstract: This paper presents our approach to the CheckThat! 2025 Task 1 on
subjectivity detection, where systems are challenged to distinguish whether a
sentence from a news article expresses the subjective view of the author or
presents an objective view on the covered topic. We propose a feature-augmented
transformer architecture that combines contextual embeddings from pre-trained
language models with statistical and linguistic features. Our system leveraged
pre-trained transformers with additional lexical features: for Arabic we used
AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while
for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined
with TF-IDF features through a gating mechanism. We evaluated our system in
monolingual, multilingual, and zero-shot settings across multiple languages
including English, Arabic, German, Italian, and several unseen languages. The
results demonstrate the effectiveness of our approach, achieving competitive
performance across different languages with notable success in the monolingual
setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with
macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank
1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an
ablation analysis that demonstrated the importance of combining TF-IDF features
with the gating mechanism and the cross-lingual transfer for subjectivity
detection. Furthermore, our analysis reveals the model's sensitivity to both
the order of cross-lingual fine-tuning and the linguistic proximity of the
training languages.

</details>


### [9] [Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting](https://arxiv.org/abs/2507.21099)
*Chloe Ho,Ishneet Sukhvinder Singh,Diya Sharma,Tanvi Reddy Anumandla,Michael Lu,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: The paper explores how LLM-based ad rewriting improves ad visibility in retrieval systems without altering the retrieval model, using a supervised fine-tuning framework and custom metrics.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of ad phrasing on visibility in LLM-integrated retrieval systems and improve ad ranking and inclusion in responses.

Method: Introduces a supervised fine-tuning framework with a custom loss balancing relevance and fidelity, evaluated via DeltaMRR@K and DeltaDIR@K metrics.

Result: PPO-trained models outperform prompt engineering and supervised fine-tuning, achieving up to 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in instruction-based prompting.

Conclusion: Ad phrasing, prompt format, and reinforcement learning are crucial for effective ad rewriting in LLM-integrated retrieval systems.

Abstract: Search algorithms and user query relevance have given LLMs the ability to
return relevant information, but the effect of content phrasing on ad
visibility remains underexplored. We investigate how LLM-based rewriting of
advertisements can improve their ranking in retrieval systems and inclusion in
generated LLM responses, without modifying the retrieval model itself. We
introduce a supervised fine-tuning framework with a custom loss balancing
semantic relevance and content fidelity. To evaluate effectiveness, we propose
two metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion
frequency improvement). Our approach presents a scalable method to optimize ad
phrasing, enhancing visibility in retrieval-based LLM workflows. Experiments
across both instruction-based and few-shot prompting demonstrate that PPO
trained models outperform both prompt engineering and supervised fine-tuning in
most cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in
instruction-based prompting. These results highlight the importance of how the
ad is written before retrieval and prompt format and reinforcement learning in
effective ad rewriting for LLM integrated retrieval systems.

</details>


### [10] [iLSU-T: an Open Dataset for Uruguayan Sign Language Translation](https://arxiv.org/abs/2507.21104)
*Ariel E. Stassi,Yanina Boria,J. Matías Di Martino,Gregory Randall*

Main category: cs.CL

TL;DR: iLSU T is an open dataset of Uruguayan Sign Language videos with audio/text transcriptions, used to evaluate translation algorithms and highlight the need for localized datasets.


<details>
  <summary>Details</summary>
Motivation: Address the lack of localized datasets for sign language translation, crucial for developing tools to improve accessibility and inclusion.

Method: Created iLSU T, a multimodal dataset with 185+ hours of interpreted sign language videos, and tested three state-of-the-art translation algorithms.

Result: Experiments established baselines and validated the dataset's usefulness, emphasizing the need for localized data in sign language processing.

Conclusion: Localized datasets like iLSU T are essential for advancing sign language translation and accessibility tools.

Abstract: Automatic sign language translation has gained particular interest in the
computer vision and computational linguistics communities in recent years.
Given each sign language country particularities, machine translation requires
local data to develop new techniques and adapt existing ones. This work
presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB
videos with audio and text transcriptions. This type of multimodal and curated
data is paramount for developing novel approaches to understand or generate
tools for sign language processing. iLSU T comprises more than 185 hours of
interpreted sign language videos from public TV broadcasting. It covers diverse
topics and includes the participation of 18 professional interpreters of sign
language. A series of experiments using three state of the art translation
algorithms is presented. The aim is to establish a baseline for this dataset
and evaluate its usefulness and the proposed pipeline for data processing. The
experiments highlight the need for more localized datasets for sign language
translation and understanding, which are critical for developing novel tools to
improve accessibility and inclusion of all individuals. Our data and code can
be accessed.

</details>


### [11] [Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype](https://arxiv.org/abs/2507.21106)
*Mandar Marathe*

Main category: cs.CL

TL;DR: The study aims to objectively measure the density of Arabic rhetoric in texts by identifying and quantifying literary devices, providing tools for analysis.


<details>
  <summary>Details</summary>
Motivation: There is no objective method to assess Arabic rhetoric's use, extent, or comparison across genres, authors, or time periods.

Method: Compiled 84 literary devices, created identification systems, and developed tools (electronic and analogue) to calculate rhetorical density based on morpheme count.

Result: A working tool was created to accurately report Arabic rhetoric density in any text or speech, including distribution across sub-domains.

Conclusion: The project successfully provides an objective measure for Arabic rhetoric, enabling comparative analysis and deeper understanding of its use.

Abstract: Arabic Rhetoric is the field of Arabic linguistics which governs the art and
science of conveying a message with greater beauty, impact and persuasiveness.
The field is as ancient as the Arabic language itself and is found extensively
in classical and contemporary Arabic poetry, free verse and prose. In practical
terms, it is the intelligent use of word order, figurative speech and
linguistic embellishments to enhance message delivery. Despite the volumes that
have been written about it and the high status accorded to it, there is no way
to objectively know whether a speaker or writer has used Arabic rhetoric in a
given text, to what extent, and why. There is no objective way to compare the
use of Arabic rhetoric across genres, authors or epochs. It is impossible to
know which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary
genres are richer in Arabic rhetoric. The aim of the current study was to
devise a way to measure the density of the literary devices which constitute
Arabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself.
A comprehensive list of 84 of the commonest literary devices and their
definitions was compiled. A system of identifying literary devices in texts was
constructed. A method of calculating the density of literary devices based on
the morpheme count of the text was utilised. Four electronic tools and an
analogue tool were created to support the calculation of an Arabic text's
rhetorical literary device density, including a website and online calculator.
Additionally, a technique of reporting the distribution of literary devices
used across the three sub-domains of Arabic rhetoric was created. The output of
this project is a working tool which can accurately report the density of
Arabic rhetoric in any Arabic text or speech.

</details>


### [12] [Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams](https://arxiv.org/abs/2507.21107)
*Rob Manson*

Main category: cs.CL

TL;DR: Curved Inference tracks how LLM residual stream trajectories bend with semantic concern shifts, revealing model behavior through curvature and salience metrics.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models (LLMs) like Gemma3-1b and LLaMA3.2-3b internally adjust to semantic shifts across diverse domains.

Method: Analyzed 20 matched prompts using five native-space metrics (focusing on curvature and salience) under a pullback semantic metric derived from the unembedding matrix.

Result: LLaMA shows consistent scaling in curvature and salience with concern intensity, while Gemma responds but with weaker differentiation.

Conclusion: Curved Inference provides a principled method to diagnose LLM alignment and abstraction, revealing latent conceptual and contextual dynamics.

Abstract: We propose Curved Inference - a geometric Interpretability framework that
tracks how the residual stream trajectory of a large language model bends in
response to shifts in semantic concern. Across 20 matched prompts spanning
emotional, moral, perspective, logical, identity, environmental, and nonsense
domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,
with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These
metrics are computed under a pullback semantic metric derived from the
unembedding matrix, ensuring that all measurements reflect token-aligned
geometry rather than raw coordinate structure. We find that concern-shifted
prompts reliably alter internal activation trajectories in both models - with
LLaMA exhibiting consistent, statistically significant scaling in both
curvature and salience as concern intensity increases. Gemma also responds to
concern but shows weaker differentiation between moderate and strong variants.
Our results support a two-layer view of LLM geometry - a latent conceptual
structure encoded in the embedding space, and a contextual trajectory shaped by
prompt-specific inference. Curved Inference reveals how models navigate,
reorient, or reinforce semantic meaning over depth, offering a principled
method for diagnosing alignment, abstraction, and emergent inference dynamics.
These findings offer fresh insight into semantic abstraction and model
alignment through the lens of Curved Inference.

</details>


### [13] [A Survey of Classification Tasks and Approaches for Legal Contracts](https://arxiv.org/abs/2507.21108)
*Amrita Singh,Aditya Joshi,Jiaojiao Jiang,Hye-young Paik*

Main category: cs.CL

TL;DR: This survey explores automatic Legal Contract Classification (LCC), addressing challenges, tasks, datasets, and methodologies to improve efficiency and accuracy in legal contract analysis.


<details>
  <summary>Details</summary>
Motivation: Manual contract reviews are inefficient and error-prone, necessitating automation to enhance speed, accuracy, and accessibility in legal processes.

Method: The survey categorizes LCC methodologies into Traditional Machine Learning, Deep Learning, and Transformer-based approaches, reviewing datasets and evaluation techniques.

Result: It identifies seven classification tasks, reviews fourteen datasets, and highlights best-performing results from existing studies.

Conclusion: The survey provides a comprehensive overview of LCC, suggesting future research directions to improve efficiency and scalability, aiding legal NLP researchers and practitioners.

Abstract: Given the large size and volumes of contracts and their underlying inherent
complexity, manual reviews become inefficient and prone to errors, creating a
clear need for automation. Automatic Legal Contract Classification (LCC)
revolutionizes the way legal contracts are analyzed, offering substantial
improvements in speed, accuracy, and accessibility. This survey delves into the
challenges of automatic LCC and a detailed examination of key tasks, datasets,
and methodologies. We identify seven classification tasks within LCC, and
review fourteen datasets related to English-language contracts, including
public, proprietary, and non-public sources. We also introduce a methodology
taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning,
and Transformer-based approaches. Additionally, the survey discusses evaluation
techniques and highlights the best-performing results from the reviewed
studies. By providing a thorough overview of current methods and their
limitations, this survey suggests future research directions to improve the
efficiency, accuracy, and scalability of LCC. As the first comprehensive survey
on LCC, it aims to support legal NLP researchers and practitioners in improving
legal processes, making legal information more accessible, and promoting a more
informed and equitable society.

</details>


### [14] [SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering](https://arxiv.org/abs/2507.21110)
*Kezhen Zhong,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: SemRAG is a domain-specific RAG framework using semantic chunking and knowledge graphs to enhance LLM performance without costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for integrating domain-specific knowledge into LLMs are computationally expensive and limited in scalability.

Method: SemRAG uses semantic chunking based on cosine similarity and structures retrieved information into knowledge graphs.

Result: Outperforms traditional RAG methods in relevance and correctness, with optimized buffer sizes further improving performance.

Conclusion: SemRAG provides a scalable, efficient solution for domain-specific LLM applications, avoiding resource-heavy fine-tuning.

Abstract: This paper introduces SemRAG, an enhanced Retrieval Augmented Generation
(RAG) framework that efficiently integrates domain-specific knowledge using
semantic chunking and knowledge graphs without extensive fine-tuning.
Integrating domain-specific knowledge into large language models (LLMs) is
crucial for improving their performance in specialized tasks. Yet, existing
adaptations are computationally expensive, prone to overfitting and limit
scalability. To address these challenges, SemRAG employs a semantic chunking
algorithm that segments documents based on the cosine similarity from sentence
embeddings, preserving semantic coherence while reducing computational
overhead. Additionally, by structuring retrieved information into knowledge
graphs, SemRAG captures relationships between entities, improving retrieval
accuracy and contextual understanding. Experimental results on MultiHop RAG and
Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance
and correctness of retrieved information from the Knowledge Graph,
outperforming traditional RAG methods. Furthermore, we investigate the
optimization of buffer sizes for different data corpus, as optimizing buffer
sizes tailored to specific datasets can further improve retrieval performance,
as integration of knowledge graphs strengthens entity relationships for better
contextual comprehension. The primary advantage of SemRAG is its ability to
create an efficient, accurate domain-specific LLM pipeline while avoiding
resource-intensive fine-tuning. This makes it a practical and scalable approach
aligned with sustainability goals, offering a viable solution for AI
applications in domain-specific fields.

</details>


### [15] [InsurTech innovation using natural language processing](https://arxiv.org/abs/2507.21112)
*Panyi Dong,Zhiyu Quan*

Main category: cs.CL

TL;DR: The paper explores NLP's role in transforming unstructured text into structured data for actuarial analysis in insurance, using real-world InsurTech data to enhance pricing and risk assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional insurance companies seek to leverage NLP and alternative data to maintain competitiveness in the InsurTech era.

Method: The study applies NLP techniques to unstructured text from InsurTech data, demonstrating practical use cases in commercial insurance.

Result: NLP-derived insights refine pricing factors and introduce novel risk assessment perspectives, proving its foundational role in insurance analytics.

Conclusion: NLP is essential for modern, data-driven insurance operations, offering transformative potential beyond traditional methods.

Abstract: With the rapid rise of InsurTech, traditional insurance companies are
increasingly exploring alternative data sources and advanced technologies to
sustain their competitive edge. This paper provides both a conceptual overview
and practical case studies of natural language processing (NLP) and its
emerging applications within insurance operations with a focus on transforming
raw, unstructured text into structured data suitable for actuarial analysis and
decision-making. Leveraging real-world alternative data provided by an
InsurTech industry partner that enriches traditional insurance data sources, we
apply various NLP techniques to demonstrate practical use cases in the
commercial insurance context. These enriched, text-derived insights not only
add to and refine traditional rating factors for commercial insurance pricing
but also offer novel perspectives for assessing underlying risk by introducing
novel industry classifications. Through these demonstrations, we show that NLP
is not merely a supplementary tool but a foundational element for modern,
data-driven insurance analytics.

</details>


### [16] [TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law](https://arxiv.org/abs/2507.21134)
*Zheng Hui,Yijiang River Dong,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

TL;DR: The paper introduces Trident-Bench, a benchmark for evaluating domain-specific safety of LLMs in law, finance, and medicine, revealing gaps in ethical compliance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation of domain-specific safety risks in LLMs deployed in high-risk domains like law, finance, and medicine.

Method: Defined safety principles based on professional ethics codes and introduced Trident-Bench to evaluate 19 LLMs.

Result: Generalist models meet basic safety expectations, while domain-specialized models struggle with ethical nuances.

Conclusion: Trident-Bench is a foundational resource for improving domain-specific LLM safety and reducing deployment risks.

Abstract: As large language models (LLMs) are increasingly deployed in high-risk
domains such as law, finance, and medicine, systematically evaluating their
domain-specific safety and compliance becomes critical. While prior work has
largely focused on improving LLM performance in these domains, it has often
neglected the evaluation of domain-specific safety risks. To bridge this gap,
we first define domain-specific safety principles for LLMs based on the AMA
Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and
the CFA Institute Code of Ethics. Building on this foundation, we introduce
Trident-Bench, a benchmark specifically targeting LLM safety in the legal,
financial, and medical domains. We evaluated 19 general-purpose and
domain-specialized models on Trident-Bench and show that it effectively reveals
key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic
expectations, whereas domain-specialized models often struggle with subtle
ethical nuances. This highlights an urgent need for finer-grained
domain-specific safety improvements. By introducing Trident-Bench, our work
provides one of the first systematic resources for studying LLM safety in law
and finance, and lays the groundwork for future research aimed at reducing the
safety risks of deploying LLMs in professionally regulated fields. Code and
benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT

</details>


### [17] [TTS-1 Technical Report](https://arxiv.org/abs/2507.21138)
*Oleg Atamanenko,Anna Chalova,Joseph Coombes,Nikki Cope,Phillip Dang,Zhifeng Deng,Jimmy Du,Michael Ermolenko,Feifan Fan,Yufei Feng,Cheryl Fichter,Pavel Filimonov,Louis Fischer,Kylan Gibbs,Valeria Gusarova,Pavel Karpik,Andreas Assad Kottner,Ian Lee,Oliver Louie,Jasmine Mai,Mikhail Mamontov,Suri Mao,Nurullah Morshed,Igor Poletaev,Florin Radu,Dmytro Semernia,Evgenii Shingarev,Vikram Sivaraja,Peter Skirko,Rinat Takhautdinov,Robert Villahermosa,Jean Wang*

Main category: cs.CL

TL;DR: Inworld TTS-1 introduces two Transformer-based TTS models, TTS-1-Max (8.8B params) for high quality and TTS-1 (1.6B params) for efficiency, achieving state-of-the-art performance with in-context learning.


<details>
  <summary>Details</summary>
Motivation: To advance text-to-speech technology with models that offer high quality, expressiveness, and efficiency for diverse applications.

Method: Utilizes scaling of train-time compute, pre-training, fine-tuning, and RL-alignment of SpeechLM for performance.

Result: State-of-the-art benchmarks, 48 kHz speech, 11 languages, emotional control, and non-verbal vocalizations.

Conclusion: Inworld TTS-1 models set new standards in TTS with open-source availability for broader use.

Abstract: We introduce Inworld TTS-1, a set of two Transformer-based autoregressive
text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters
and is designed for utmost quality and expressiveness in demanding
applications. TTS-1 is our most efficient model, with 1.6B parameters, built
for real-time speech synthesis and on-device use cases. By scaling train-time
compute and applying a sequential process of pre-training, fine-tuning, and
RL-alignment of the speech-language model (SpeechLM) component, both models
achieve state-of-the-art performance on a variety of benchmarks, demonstrating
exceptional quality relying purely on in-context learning of the speaker's
voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech
with low latency, and support 11 languages with fine-grained emotional control
and non-verbal vocalizations through audio markups. We additionally open-source
our training and modeling code under an MIT license.

</details>


### [18] [Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question](https://arxiv.org/abs/2507.21168)
*Rafael Rosales,Santiago Miret*

Main category: cs.CL

TL;DR: Question interpretation diversity outperforms model diversity in improving ensemble accuracy for binary question answering with LLMs.


<details>
  <summary>Details</summary>
Motivation: To determine the most effective way of leveraging diversity (model vs. question interpretation) for improving LLM performance in binary question answering.

Method: Compare model diversity (multiple models answering the same question) and question interpretation diversity (same model answering differently framed questions) using majority voting for consensus. Tested on boolq, strategyqa, and pubmedqa with GPT and LLaMa.

Result: Question interpretation diversity consistently achieves better ensemble accuracy than model diversity. Model diversity results fall between best and worst ensemble members without clear improvement.

Conclusion: Framing questions differently (question interpretation diversity) is more effective than using multiple models (model diversity) for improving LLM ensemble performance in binary tasks.

Abstract: Effectively leveraging diversity has been shown to improve performance for
various machine learning models, including large language models (LLMs).
However, determining the most effective way of using diversity remains a
challenge. In this work, we compare two diversity approaches for answering
binary questions using LLMs: model diversity, which relies on multiple models
answering the same question, and question interpretation diversity, which
relies on using the same model to answer the same question framed in different
ways. For both cases, we apply majority voting as the ensemble consensus
heuristic to determine the final answer. Our experiments on boolq, strategyqa,
and pubmedqa show that question interpretation diversity consistently leads to
better ensemble accuracy compared to model diversity. Furthermore, our analysis
of GPT and LLaMa shows that model diversity typically produces results between
the best and the worst ensemble members without clear improvement.

</details>


### [19] [Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers](https://arxiv.org/abs/2507.21186)
*Sungmin Han,Jeonghyun Lee,Sangkyun Lee*

Main category: cs.CL

TL;DR: Contrast-CAT improves interpretability of transformer-based text classification by filtering class-irrelevant features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Explaining transformer decisions is challenging, and current methods are unreliable due to class-irrelevant features in activations.

Method: Proposes Contrast-CAT, an activation contrast-based attribution method that contrasts input and reference activations to refine token-level attributions.

Result: Contrast-CAT outperforms state-of-the-art methods, with improvements of x1.30 in AOPC and x2.25 in LOdds under MoRF.

Conclusion: Contrast-CAT enhances interpretability and reliability of transformer-based text classification models.

Abstract: Transformers have profoundly influenced AI research, but explaining their
decisions remains challenging -- even for relatively simpler tasks such as
classification -- which hinders trust and safe deployment in real-world
applications. Although activation-based attribution methods effectively explain
transformer-based text classification models, our findings reveal that these
methods can be undermined by class-irrelevant features within activations,
leading to less reliable interpretations. To address this limitation, we
propose Contrast-CAT, a novel activation contrast-based attribution method that
refines token-level attributions by filtering out class-irrelevant features. By
contrasting the activations of an input sequence with reference activations,
Contrast-CAT generates clearer and more faithful attribution maps. Experimental
results across various datasets and models confirm that Contrast-CAT
consistently outperforms state-of-the-art methods. Notably, under the MoRF
setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds
over the most competing methods, demonstrating its effectiveness in enhancing
interpretability for transformer-based text classification.

</details>


### [20] [Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability](https://arxiv.org/abs/2507.21234)
*Fatema Binte Hassan,Md Al Jubair,Mohammad Mehadi Hasan,Tahmid Hossain,S M Mehebubur Rahman Khan Shuvo,Mohammad Shamsul Arefin*

Main category: cs.CL

TL;DR: This study analyzes public sentiment on crime-related news in Bangla social media comments using a transformer-based model (XLM-RoBERTa Base), achieving 97% accuracy and employing explainable AI for interpretability.


<details>
  <summary>Details</summary>
Motivation: To understand dynamic public perception of crime-related news and support policy-making through sentiment analysis of Bangla social media comments.

Method: A transformer-based model (XLM-RoBERTa Base) is used to classify 28,528 Bangla comments into positive, negative, or neutral sentiments, with explainable AI for feature analysis.

Result: The model achieves 97% accuracy, outperforming existing methods, and identifies key features influencing sentiment.

Conclusion: Transformer-based models are effective for low-resource languages like Bengali, offering actionable insights for public policy and crime prevention.

Abstract: In recent years, social media platforms have become prominent spaces for
individuals to express their opinions on ongoing events, including criminal
incidents. As a result, public sentiment can shift dynamically over time. This
study investigates the evolving public perception of crime-related news by
classifying user-generated comments into three categories: positive, negative,
and neutral. A newly curated dataset comprising 28,528 Bangla-language social
media comments was developed for this purpose. We propose a transformer-based
model utilizing the XLM-RoBERTa Base architecture, which achieves a
classification accuracy of 97%, outperforming existing state-of-the-art methods
in Bangla sentiment analysis. To enhance model interpretability, explainable AI
technique is employed to identify the most influential features driving
sentiment classification. The results underscore the effectiveness of
transformer-based models in processing low-resource languages such as Bengali
and demonstrate their potential to extract actionable insights that can support
public policy formulation and crime prevention strategies.

</details>


### [21] [Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach](https://arxiv.org/abs/2507.21242)
*Mohammad Mehadi Hasan,Fatema Binte Hassan,Md Al Jubair,Zobayer Ahmed,Sazzatul Yeakin,Md Masum Billah*

Main category: cs.CL

TL;DR: The paper fine-tunes Bangla BERT to detect hyperpartisan news in Bangla, achieving 95.65% accuracy and outperforming traditional methods, while using LIME for explainability.


<details>
  <summary>Details</summary>
Motivation: Misinformation in Bangla lacks detection tools, risking societal harm. The study aims to fill this gap with advanced NLP.

Method: Fine-tunes Bangla BERT, compares it with traditional ML models, and uses semi-supervised learning and LIME for explainability.

Result: Bangla BERT achieves 95.65% accuracy, surpassing conventional methods.

Conclusion: Transformer models like Bangla BERT are effective even in low-resource settings, enabling future advancements in misinformation detection.

Abstract: In the current digital landscape, misinformation circulates rapidly, shaping
public perception and causing societal divisions. It is difficult to identify
hyperpartisan news in Bangla since there aren't many sophisticated natural
language processing methods available for this low-resource language. Without
effective detection methods, biased content can spread unchecked, posing
serious risks to informed discourse. To address this gap, our research
fine-tunes Bangla BERT. This is a state-of-the-art transformer-based model,
designed to enhance classification accuracy for hyperpartisan news. We evaluate
its performance against traditional machine learning models and implement
semi-supervised learning to enhance predictions further. Not only that, we use
LIME to provide transparent explanations of the model's decision-making
process, which helps to build trust in its outcomes. With a remarkable accuracy
score of 95.65%, Bangla BERT outperforms conventional approaches, according to
our trial data. The findings of this study demonstrate the usefulness of
transformer models even in environments with limited resources, which opens the
door to further improvements in this area.

</details>


### [22] [Can human clinical rationales improve the performance and explainability of clinical text classification models?](https://arxiv.org/abs/2507.21302)
*Christoph Metzner,Shang Gao,Drahomira Herrmannova,Heidi A. Hanson*

Main category: cs.CL

TL;DR: Clinical rationales as additional training data improve model performance in high-resource settings but are inconsistent in low-resource scenarios. Using more reports outperforms rationale-based training for accuracy, while rationales slightly enhance explainability.


<details>
  <summary>Details</summary>
Motivation: To determine if human-based clinical rationales can enhance performance and explainability of transformer models for clinical text classification.

Method: Analyzed 99,125 clinical rationales alongside 128,649 pathology reports to train transformer models, evaluating performance and rationale quality via sufficiency metrics.

Result: Rationales improve performance in high-resource cases but are inconsistent otherwise. More reports outperform rationales for accuracy, while rationales slightly boost explainability.

Conclusion: For accuracy, prioritize labeling more reports over rationales. For explainability, rationale-supplemented training may help, but gains are modest.

Abstract: AI-driven clinical text classification is vital for explainable automated
retrieval of population-level health information. This work investigates
whether human-based clinical rationales can serve as additional supervision to
improve both performance and explainability of transformer-based models that
automatically encode clinical documents. We analyzed 99,125 human-based
clinical rationales that provide plausible explanations for primary cancer site
diagnoses, using them as additional training samples alongside 128,649
electronic pathology reports to evaluate transformer-based models for
extracting primary cancer sites. We also investigated sufficiency as a way to
measure rationale quality for pre-selecting rationales. Our results showed that
clinical rationales as additional training data can improve model performance
in high-resource scenarios but produce inconsistent behavior when resources are
limited. Using sufficiency as an automatic metric to preselect rationales also
leads to inconsistent results. Importantly, models trained on rationales were
consistently outperformed by models trained on additional reports instead. This
suggests that clinical rationales don't consistently improve model performance
and are outperformed by simply using more reports. Therefore, if the goal is
optimizing accuracy, annotation efforts should focus on labeling more reports
rather than creating rationales. However, if explainability is the priority,
training models on rationale-supplemented data may help them better identify
rationale-like features. We conclude that using clinical rationales as
additional training data results in smaller performance improvements and only
slightly better explainability (measured as average token-level rationale
coverage) compared to training on additional reports.

</details>


### [23] [Do Large Language Models Understand Morality Across Cultures?](https://arxiv.org/abs/2507.21319)
*Hadi Mohammadi,Yasmeen F. S. S. Meijer,Efthymia Papadopoulou,Ayoub Bagheri*

Main category: cs.CL

TL;DR: LLMs often fail to capture cross-cultural moral variation, compressing differences and showing low alignment with survey data, highlighting the need for bias mitigation.


<details>
  <summary>Details</summary>
Motivation: To investigate how well LLMs reflect cross-cultural moral perspectives and address concerns about biases in their outputs.

Method: Three methods: comparing moral score variances, cluster alignment analyses, and probing models with comparative prompts.

Result: LLMs compress cross-cultural moral differences and poorly align with empirical survey patterns.

Conclusion: Urgent need for bias mitigation and improved cultural representativeness in LLMs for ethical global deployment.

Abstract: Recent advancements in large language models (LLMs) have established them as
powerful tools across numerous domains. However, persistent concerns about
embedded biases, such as gender, racial, and cultural biases arising from their
training data, raise significant questions about the ethical use and societal
consequences of these technologies. This study investigates the extent to which
LLMs capture cross-cultural differences and similarities in moral perspectives.
Specifically, we examine whether LLM outputs align with patterns observed in
international survey data on moral attitudes. To this end, we employ three
complementary methods: (1) comparing variances in moral scores produced by
models versus those reported in surveys, (2) conducting cluster alignment
analyses to assess correspondence between country groupings derived from LLM
outputs and survey data, and (3) directly probing models with comparative
prompts using systematically chosen token pairs. Our results reveal that
current LLMs often fail to reproduce the full spectrum of cross-cultural moral
variation, tending to compress differences and exhibit low alignment with
empirical survey patterns. These findings highlight a pressing need for more
robust approaches to mitigate biases and improve cultural representativeness in
LLMs. We conclude by discussing the implications for the responsible
development and global deployment of LLMs, emphasizing fairness and ethical
alignment.

</details>


### [24] [A Deep Learning Automatic Speech Recognition Model for Shona Language](https://arxiv.org/abs/2507.21331)
*Leslie Wellington Sirora,Mainford Mutandavari*

Main category: cs.CL

TL;DR: A deep learning-based ASR system for Shona achieved 74% accuracy, outperforming traditional models, using hybrid CNN-LSTM architecture, data augmentation, and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address challenges like limited training data, lack of labeled data, and tonal complexities in Shona, aiming to improve ASR accuracy for low-resource languages.

Method: Developed a hybrid CNN-LSTM model with data augmentation, transfer learning, and attention mechanisms to handle tonal nuances and data scarcity.

Result: Achieved 29% WER, 12% PER, and 74% accuracy, demonstrating deep learning's potential for low-resource languages.

Conclusion: The study advances ASR technology for under-resourced languages like Shona, enhancing accessibility and communication.

Abstract: This study presented the development of a deep learning-based Automatic
Speech Recognition system for Shona, a low-resource language characterized by
unique tonal and grammatical complexities. The research aimed to address the
challenges posed by limited training data, lack of labelled data, and the
intricate tonal nuances present in Shona speech, with the objective of
achieving significant improvements in recognition accuracy compared to
traditional statistical models. The research first explored the feasibility of
using deep learning to develop an accurate ASR system for Shona. Second, it
investigated the specific challenges involved in designing and implementing
deep learning architectures for Shona speech recognition and proposed
strategies to mitigate these challenges. Lastly, it compared the performance of
the deep learning-based model with existing statistical models in terms of
accuracy. The developed ASR system utilized a hybrid architecture consisting of
a Convolutional Neural Network for acoustic modelling and a Long Short-Term
Memory network for language modelling. To overcome the scarcity of data, data
augmentation techniques and transfer learning were employed. Attention
mechanisms were also incorporated to accommodate the tonal nature of Shona
speech. The resulting ASR system achieved impressive results, with a Word Error
Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These
metrics indicated the potential of deep learning to enhance ASR accuracy for
under-resourced languages like Shona. This study contributed to the advancement
of ASR technology for under-resourced languages like Shona, ultimately
fostering improved accessibility and communication for Shona speakers
worldwide.

</details>


### [25] [StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation](https://arxiv.org/abs/2507.21340)
*Satyananda Kashyap,Sola Shirai,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.CL

TL;DR: StructText is a framework for automatically generating benchmarks for key-value extraction from text using tabular data, addressing the lack of scalable evaluation tools for LLMs in specific domains.


<details>
  <summary>Details</summary>
Motivation: There's a need for scalable benchmarks to evaluate LLMs' extraction quality in domain-specific contexts, as manual annotation is labor-intensive.

Method: StructText uses tabular data as ground truth, employing a two-stage 'plan-then-execute' pipeline to generate synthetic text. It evaluates alignment via LLM-based judgments and objective metrics.

Result: LLMs achieve high factual accuracy but struggle with narrative coherence, making extraction difficult despite accurate numerical and temporal information.

Conclusion: StructText provides a scalable solution for benchmark generation, supporting further research with released datasets and tools.

Abstract: Extracting structured information from text, such as key-value pairs that
could augment tabular data, is quite useful in many enterprise use cases.
Although large language models (LLMs) have enabled numerous automated pipelines
for converting natural language into structured formats, there is still a lack
of benchmarks for evaluating their extraction quality, especially in specific
domains or focused documents specific to a given organization. Building such
benchmarks by manual annotations is labour-intensive and limits the size and
scalability of the benchmarks. In this work, we present StructText, an
end-to-end framework for automatically generating high-fidelity benchmarks for
key-value extraction from text using existing tabular data. It uses available
tabular data as structured ground truth, and follows a two-stage
``plan-then-execute'' pipeline to synthetically generate corresponding
natural-language text. To ensure alignment between text and structured source,
we introduce a multi-dimensional evaluation strategy that combines (a)
LLM-based judgments on factuality, hallucination, and coherence and (b)
objective extraction metrics measuring numeric and temporal accuracy. We
evaluated the proposed method on 71,539 examples across 49 datasets. Results
reveal that while LLMs achieve strong factual accuracy and avoid hallucination,
they struggle with narrative coherence in producing extractable text. Notably,
models presume numerical and temporal information with high fidelity yet this
information becomes embedded in narratives that resist automated extraction. We
release a framework, including datasets, evaluation tools, and baseline
extraction systems, to support continued research.

</details>


### [26] [Turbocharging Web Automation: The Impact of Compressed History States](https://arxiv.org/abs/2507.21369)
*Xiyue Zhu,Peng Tang,Haofu Liao,Srikar Appalaraju*

Main category: cs.CL

TL;DR: A novel web history compressor improves web automation by distilling task-relevant history states into fixed-length representations, achieving 1.2-5.4% accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Current web automation methods ignore history states, leading to sparse information and long input sequences.

Method: Proposes a history compressor module to distill task-relevant information from verbose history states into concise representations.

Result: Achieves 1.2-5.4% absolute accuracy improvements on Mind2Web and WebLINX datasets.

Conclusion: The history compressor enhances web automation by effectively utilizing history states.

Abstract: Language models have led to a leap forward in web automation. The current web
automation approaches take the current web state, history actions, and language
instruction as inputs to predict the next action, overlooking the importance of
history states. However, the highly verbose nature of web page states can
result in long input sequences and sparse information, hampering the effective
utilization of history states. In this paper, we propose a novel web history
compressor approach to turbocharge web automation using history states. Our
approach employs a history compressor module that distills the most
task-relevant information from each history state into a fixed-length short
representation, mitigating the challenges posed by the highly verbose history
states. Experiments are conducted on the Mind2Web and WebLINX datasets to
evaluate the effectiveness of our approach. Results show that our approach
obtains 1.2-5.4% absolute accuracy improvements compared to the baseline
approach without history inputs.

</details>


### [27] [MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations](https://arxiv.org/abs/2507.21428)
*Elias Lumer,Anmol Gulati,Vamse Kumar Subbiah,Pradeep Honaganahalli Basavaraju,James A. Burke*

Main category: cs.CL

TL;DR: MemTool is a short-term memory framework for LLM agents to manage tools or MCP server contexts in multi-turn conversations, offering three modes with varying autonomy and control.


<details>
  <summary>Details</summary>
Motivation: Fixed context windows limit LLM agents' effectiveness in multi-turn interactions requiring repeated tool usage.

Method: MemTool introduces three agentic architectures: Autonomous Agent Mode, Workflow Mode, and Hybrid Mode, evaluated on 13+ LLMs using the ScaleMCP benchmark.

Result: Autonomous Agent Mode achieves high tool-removal efficiency (90-94%) with reasoning LLMs, while Workflow and Hybrid modes manage tool removal effectively. Autonomous and Hybrid modes excel at task completion.

Conclusion: MemTool provides trade-offs between task accuracy, agency, and model capabilities, with recommendations for each mode based on performance.

Abstract: Large Language Model (LLM) agents have shown significant autonomous
capabilities in dynamically searching and incorporating relevant tools or Model
Context Protocol (MCP) servers for individual queries. However, fixed context
windows limit effectiveness in multi-turn interactions requiring repeated,
independent tool usage. We introduce MemTool, a short-term memory framework
enabling LLM agents to dynamically manage tools or MCP server contexts across
multi-turn conversations. MemTool offers three agentic architectures: 1)
Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow
Mode, providing deterministic control without autonomy, and 3) Hybrid Mode,
combining autonomous and deterministic control. Evaluating each MemTool mode
across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100
consecutive user interactions, measuring tool removal ratios (short-term memory
efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning
LLMs achieve high tool-removal efficiency (90-94% over a 3-window average),
while medium-sized models exhibit significantly lower efficiency (0-60%).
Workflow and Hybrid modes consistently manage tool removal effectively, whereas
Autonomous and Hybrid modes excel at task completion. We present trade-offs and
recommendations for each MemTool mode based on task accuracy, agency, and model
capabilities.

</details>


### [28] [Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour](https://arxiv.org/abs/2507.21432)
*Tareq Alsaleh,Bilal Farooq*

Main category: cs.CL

TL;DR: The study introduces LiTransMC, a fine-tuned causal LLM for travel mode choice prediction, outperforming untuned models, proprietary systems, and classical methods. It combines predictive accuracy with interpretability, enabling local deployment for transportation research.


<details>
  <summary>Details</summary>
Motivation: To develop open-access, locally deployable causal LLMs for travel mode choice prediction, addressing the need for specialized, interpretable tools in transportation research while ensuring privacy and cost efficiency.

Method: Benchmarked 11 LLMs across three datasets, testing 396 configurations. LiTransMC was fine-tuned using parameter-efficient and loss masking strategies, evaluated for predictive accuracy and reasoning via BERTopic and a novel Explanation Strength Index.

Result: LiTransMC achieved a weighted F1 score of 0.6845 and Jensen-Shannon Divergence of 0.000245, surpassing untuned models, proprietary systems like GPT-4o, and classical methods. It demonstrated high accuracy and interpretability.

Conclusion: The study shows the feasibility of creating specialist LLMs for transportation, integrating prediction and interpretability. It opens avenues for conversational, multi-task transport models, supporting policy testing and behavioral insights.

Abstract: This study investigates the adoption of open-access, locally deployable
causal large language models (LLMs) for travel mode choice prediction and
introduces LiTransMC, the first fine-tuned causal LLM developed for this task.
We systematically benchmark eleven LLMs (1-12B parameters) across three stated
and revealed preference datasets, testing 396 configurations and generating
over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we
evaluate models generated reasoning using BERTopic for topic modelling and a
novel Explanation Strength Index, providing the first structured analysis of
how LLMs articulate decision factors in alignment with behavioural theory.
LiTransMC, fine-tuned using parameter efficient and loss masking strategy,
achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of
0.000245, surpassing both untuned local models and larger proprietary systems,
including GPT-4o with advanced persona inference and embedding-based loading,
while also outperforming classical mode choice methods such as discrete choice
models and machine learning classifiers for the same dataset. This dual
improvement, i.e., high instant-level accuracy and near-perfect distributional
calibration, demonstrates the feasibility of creating specialist, locally
deployable LLMs that integrate prediction and interpretability. Through
combining structured behavioural prediction with natural language reasoning,
this work unlocks the potential for conversational, multi-task transport models
capable of supporting agent-based simulations, policy testing, and behavioural
insight generation. These findings establish a pathway for transforming general
purpose LLMs into specialized, explainable tools for transportation research
and policy formulation, while maintaining privacy, reducing cost, and
broadening access through local deployment.

</details>


### [29] [Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench](https://arxiv.org/abs/2507.21476)
*Reuben Narad,Siddharth Suresh,Jiayi Chen,Pine S. L. Dysart-Bricken,Bob Mankoff,Robert Nowak,Jifan Zhang,Lalit Jain*

Main category: cs.CL

TL;DR: HumorBench evaluates LLMs' humor reasoning via cartoon captions, revealing STEM-trained models transfer well but scaling thinking tokens has mixed results.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' humor comprehension beyond STEM, as reasoning models saturate existing benchmarks.

Method: Uses 300 cartoon-caption pairs with expert rubrics; evaluates LLMs on joke explanation and element identification.

Result: STEM-trained models perform well, showing reasoning transferability; thinking token scaling yields mixed outcomes.

Conclusion: HumorBench highlights LLMs' humor reasoning potential but notes variability in performance with scaling.

Abstract: We present HumorBench, a benchmark designed to evaluate large language
models' (LLMs) ability to reason about and explain sophisticated humor in
cartoon captions. As reasoning models increasingly saturate existing benchmarks
in mathematics and science, novel and challenging evaluations of model
intelligence beyond STEM domains are essential. Reasoning is fundamentally
involved in text-based humor comprehension, requiring the identification of
connections between concepts in cartoons/captions and external cultural
references, wordplays, and other mechanisms. HumorBench includes approximately
300 unique cartoon-caption pairs from the New Yorker Caption Contest and
Cartoonstock.com, with expert-annotated evaluation rubrics identifying
essential joke elements. LLMs are evaluated based on their explanations towards
the humor and abilities in identifying the joke elements. To perform well on
this task, models must form and test hypotheses about associations between
concepts, potentially backtracking from initial interpretations to arrive at
the most plausible explanation. Our extensive benchmarking of current SOTA
models reveals three key insights: (1) LLM progress on STEM reasoning transfers
effectively to humor comprehension; (2) models trained exclusively on STEM
reasoning data still perform well on HumorBench, demonstrating strong
transferability of reasoning abilities; and (3) test-time scaling by increasing
thinking token budgets yields mixed results across different models in humor
reasoning.

</details>


### [30] [Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs](https://arxiv.org/abs/2507.21482)
*Abhinav Arabelly,Jagrut Nemade,Robert D Nowak,Jifan Zhang*

Main category: cs.CL

TL;DR: The paper introduces a label-efficient learning method for supervised finetuning (SFT) by leveraging task-diversity for data selection, outperforming existing methods and reducing annotation costs by up to 80%.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and high cost of human annotation in developing specialized LLMs by proposing a task-diversity-based data selection approach.

Method: Uses an inverse confidence weighting strategy to sample examples across tasks, leveraging readily available task labels and varying model confidence levels.

Result: Achieves better accuracy (4% increase in MMLU score) than training on the full dataset and reduces annotation costs by up to 80%.

Conclusion: The proposed task-diversity-based sampling strategy is simpler, more efficient, and outperforms existing methods in label-efficient learning for LLMs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, but developing high-performing models for specialized
applications often requires substantial human annotation -- a process that is
time-consuming, labor-intensive, and expensive. In this paper, we address the
label-efficient learning problem for supervised finetuning (SFT) by leveraging
task-diversity as a fundamental principle for effective data selection. This is
markedly different from existing methods based on the prompt-diversity. Our
approach is based on two key observations: 1) task labels for different prompts
are often readily available; 2) pre-trained models have significantly varying
levels of confidence across tasks. We combine these facts to devise a simple
yet effective sampling strategy: we select examples across tasks using an
inverse confidence weighting strategy. This produces models comparable to or
better than those trained with more complex sampling procedures, while being
significantly easier to implement and less computationally intensive. Notably,
our experimental results demonstrate that this method can achieve better
accuracy than training on the complete dataset (a 4\% increase in MMLU score).
Across various annotation budgets and two instruction finetuning datasets, our
algorithm consistently performs at or above the level of the best existing
methods, while reducing annotation costs by up to 80\%.

</details>


### [31] [VN-MTEB: Vietnamese Massive Text Embedding Benchmark](https://arxiv.org/abs/2507.21500)
*Loc Pham,Tung Luu,Thu Vo,Minh Nguyen,Viet Hoang*

Main category: cs.CL

TL;DR: The paper introduces VN-MTEB, a Vietnamese benchmark for embedding models, created by translating English samples from MTEB using LLMs and advanced embedding models to ensure quality and semantic fidelity.


<details>
  <summary>Details</summary>
Motivation: Vietnam's high internet traffic and online toxicity necessitate robust embedding models for recommendation and content control, but the lack of large-scale test datasets hinders effective AI evaluation.

Method: The authors translated English samples from MTEB using LLMs and embedding models, ensuring language flow, semantic fidelity, and retention of NER and code snippets. The benchmark includes 41 datasets across six tasks.

Result: Larger, more complex models with Rotary Positional Embedding outperformed those with Absolute Positional Embedding in embedding tasks.

Conclusion: VN-MTEB addresses the need for Vietnamese embedding benchmarks, providing a high-quality, diverse dataset for evaluating AI models in real-world applications.

Abstract: Vietnam ranks among the top countries in terms of both internet traffic and
online toxicity. As a result, implementing embedding models for recommendation
and content control duties in applications is crucial. However, a lack of
large-scale test datasets, both in volume and task diversity, makes it tricky
for scientists to effectively evaluate AI models before deploying them in
real-world, large-scale projects. To solve this important problem, we introduce
a Vietnamese benchmark, VN-MTEB for embedding models, which we created by
translating a large number of English samples from the Massive Text Embedding
Benchmark using our new automated framework. We leverage the strengths of large
language models (LLMs) and cutting-edge embedding models to conduct translation
and filtering processes to retain high-quality samples, guaranteeing a natural
flow of language and semantic fidelity while preserving named entity
recognition (NER) and code snippets. Our comprehensive benchmark consists of 41
datasets from six tasks specifically designed for Vietnamese text embeddings.
In our analysis, we find that bigger and more complex models using Rotary
Positional Embedding outperform those using Absolute Positional Embedding in
embedding tasks. Datasets are available at HuggingFace:
https://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686

</details>


### [32] [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)
*Runjin Chen,Andy Arditi,Henry Sleight,Owain Evans,Jack Lindsey*

Main category: cs.CL

TL;DR: The paper identifies 'persona vectors' in large language models to monitor and control personality traits like evil, sycophancy, and hallucination, offering methods to mitigate unwanted shifts.


<details>
  <summary>Details</summary>
Motivation: To address deviations from ideal traits (helpful, harmless, honest) in AI assistants by understanding and controlling underlying personality shifts.

Method: Extracts persona vectors from model activation space, uses them to predict and control personality shifts during training, and applies interventions to mitigate changes.

Result: Persona vectors effectively predict and control personality shifts, with methods to flag undesirable training data and prevent unwanted changes.

Conclusion: Persona vectors provide a scalable, automated way to monitor and steer AI personality traits, improving alignment with desired behaviors.

Abstract: Large language models interact with users through a simulated 'Assistant'
persona. While the Assistant is typically trained to be helpful, harmless, and
honest, it sometimes deviates from these ideals. In this paper, we identify
directions in the model's activation space-persona vectors-underlying several
traits, such as evil, sycophancy, and propensity to hallucinate. We confirm
that these vectors can be used to monitor fluctuations in the Assistant's
personality at deployment time. We then apply persona vectors to predict and
control personality shifts that occur during training. We find that both
intended and unintended personality changes after finetuning are strongly
correlated with shifts along the relevant persona vectors. These shifts can be
mitigated through post-hoc intervention, or avoided in the first place with a
new preventative steering method. Moreover, persona vectors can be used to flag
training data that will produce undesirable personality changes, both at the
dataset level and the individual sample level. Our method for extracting
persona vectors is automated and can be applied to any personality trait of
interest, given only a natural-language description.

</details>


### [33] [Model-free Speculative Decoding for Transformer-based ASR with Token Map Drafting](https://arxiv.org/abs/2507.21522)
*Tuan Vu Ho,Hiroaki Kokubo,Masaaki Yamamoto,Yohei Kawaguchi*

Main category: cs.CL

TL;DR: The paper introduces Token Map Drafting, a model-free speculative decoding method for ASR, improving speed on CPU-based devices without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding in transformer-based ASR systems like Whisper is computationally expensive, limiting deployment on resource-constrained devices. Speculative decoding (SD) helps but requires hardware accelerators.

Method: Proposes Token Map Drafting, a model-free SD technique using a precomputed n-gram token map from domain-specific data, eliminating the need for a draft model.

Result: Achieves speed-ups of 1.27× and 1.37× on CI-AVSR and internal datasets, with no accuracy loss. Outperforms Distill-spec by 10% in speed on CPU.

Conclusion: Token Map Drafting is effective for accelerating ASR inference in structured domains, making it suitable for on-device applications.

Abstract: End-to-end automatic speech recognition (ASR) systems based on transformer
architectures, such as Whisper, offer high transcription accuracy and
robustness. However, their autoregressive decoding is computationally
expensive, hence limiting deployment on CPU-based and resource-constrained
devices. Speculative decoding (SD) mitigates this issue by using a smaller
draft model to propose candidate tokens, which are then verified by the main
model. However, this approach is impractical for devices lacking hardware
accelerators like GPUs. To address this, we propose \emph{Token Map Drafting},
a model-free SD technique that eliminates the need for a separate draft model.
Instead, we leverage a precomputed n-gram token map derived from
domain-specific training data, enabling efficient speculative decoding with
minimal overhead. Our method significantly accelerates ASR inference in
structured, low-perplexity domains without sacrificing transcription accuracy.
Experimental results demonstrate decoding speed-ups of $1.27\times$ on the
CI-AVSR dataset and $1.37\times$ on our internal dataset without degrading
recognition accuracy. Additionally, our approach achieves a $10\%$ absolute
improvement in decoding speed over the Distill-spec baseline running on CPU,
highlighting its effectiveness for on-device ASR applications.

</details>


### [34] [TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling](https://arxiv.org/abs/2507.21526)
*Zhiyuan He,Yike Zhang,Chengruidong Zhang,Huiqiang Jiang,Yuqing Yang,Lili Qiu*

Main category: cs.CL

TL;DR: TriangleMix reduces attention overhead and speeds up LLM inference by combining dense and sparse attention patterns without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency of attention mechanisms in LLMs, which grow quadratically with input length, by improving static sparse attention methods.

Method: Proposes TriangleMix, a training-free static attention pattern that uses dense attention in shallow layers and switches to a triangle-shaped sparse pattern in deeper layers.

Result: Reduces attention overhead by 3.7x to 15.3x in deep layers and decreases TTFT by 12% to 32% for sequences up to 128K, maintaining accuracy.

Conclusion: TriangleMix enhances LLM inference efficiency and can integrate with dynamic sparsity for further speedup, making it practical for long-sequence tasks.

Abstract: Large Language Models (LLMs) rely on attention mechanisms whose time
complexity grows quadratically with input sequence length, creating significant
computational bottlenecks during the prefilling stage. Existing static sparse
attention methods typically degrade accuracy, while dynamic sparsity methods
introduce additional computational overhead due to runtime sparse index
estimation. To address these limitations, we propose TriangleMix, a novel
training-free static attention pattern. TriangleMix employs dense attention in
shallow layers and switches to a triangle-shaped sparse pattern in deeper
layers. Extensive experiments demonstrate that TriangleMix reduces attention
overhead by 3.7x to 15.3x in deep layers, and decreases overall
Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K
to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be
seamlessly integrated with dynamic sparsity methods to achieve further speedup,
e.g. accelerating MInference by 19% at 128K, highlighting its potential to
enhance LLM inference efficiency.

</details>


### [35] [Automatic Classification of User Requirements from Online Feedback -- A Replication Study](https://arxiv.org/abs/2507.21532)
*Meet Bhatt,Nic Boilard,Muhammad Rehan Chaudhary,Cole Thompson,Jacob Idoko,Aakash Sorathiya,Gouri Ginde*

Main category: cs.CL

TL;DR: The paper replicates and extends an NLP4RE study, evaluating deep learning models for requirement classification, and assesses reproducibility and replication readiness.


<details>
  <summary>Details</summary>
Motivation: To strengthen the external validity of NLP4RE studies and explore new opportunities with advanced NLP techniques.

Method: Reproduced the baseline study using released source code, extended it with an external dataset and GPT-4o comparison, and prepared a replication study ID-card.

Result: Naive Bayes showed perfect reproducibility, while BERT and ELMo generalized well. GPT-4o matched traditional models. Replication readiness was confirmed but lacked setup files.

Conclusion: The study highlights reproducibility challenges and the potential of advanced NLP models, providing resources to encourage future replications.

Abstract: Natural language processing (NLP) techniques have been widely applied in the
requirements engineering (RE) field to support tasks such as classification and
ambiguity detection. Although RE research is rooted in empirical investigation,
it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The
rapidly advancing realm of NLP is creating new opportunities for efficient,
machine-assisted workflows, which can bring new perspectives and results to the
forefront. Thus, we replicate and extend a previous NLP4RE study (baseline),
"Classifying User Requirements from Online Feedback in Small Dataset
Environments using Deep Learning", which evaluated different deep learning
models for requirement classification from user reviews. We reproduced the
original results using publicly released source code, thereby helping to
strengthen the external validity of the baseline study. We then extended the
setup by evaluating model performance on an external dataset and comparing
results to a GPT-4o zero-shot classifier. Furthermore, we prepared the
replication study ID-card for the baseline study, important for evaluating
replication readiness. Results showed diverse reproducibility levels across
different models, with Naive Bayes demonstrating perfect reproducibility. In
contrast, BERT and other models showed mixed results. Our findings revealed
that baseline deep learning models, BERT and ELMo, exhibited good
generalization capabilities on an external dataset, and GPT-4o showed
performance comparable to traditional baseline machine learning models.
Additionally, our assessment confirmed the baseline study's replication
readiness; however missing environment setup files would have further enhanced
readiness. We include this missing information in our replication package and
provide the replication study ID-card for our study to further encourage and
support the replication of our study.

</details>


### [36] [Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic Framework for a Low-Resource Language](https://arxiv.org/abs/2507.21536)
*Jiaxin Zuo,Yiquan Wang,Yuan Pan,Xiadiya Yibulayin*

Main category: cs.CL

TL;DR: The study introduces a tailored dependency annotation framework for Uyghur NLP, addressing gaps in existing treebanks. It includes 44 relations and validates its necessity through cross-standard evaluation, showing 47.9% divergence from universal schemes. The MUDT treebank improves parsing accuracy and serves as a model for other complex languages.


<details>
  <summary>Details</summary>
Motivation: To address the lack of suitable dependency annotation frameworks for Uyghur, a low-resource, agglutinative language, and improve NLP tasks.

Method: Developed a dependency annotation framework with 18 main relations and 26 subtypes, validated using a pre-trained Universal Dependencies parser.

Result: A 47.9% divergence in annotations was found, highlighting the inadequacy of universal schemes for Uyghur. The MUDT treebank provides more accurate and transparent representations.

Conclusion: The MUDT framework improves parsing and NLP tasks for Uyghur and offers a replicable model for other morphologically complex languages.

Abstract: To address a critical resource gap in Uyghur Natural Language Processing
(NLP), this study introduces a dependency annotation framework designed to
overcome the limitations of existing treebanks for the low-resource,
agglutinative language. This inventory includes 18 main relations and 26
subtypes, with specific labels such as cop:zero for verbless clauses and
instr:case=loc/dat for nuanced instrumental functions. To empirically validate
the necessity of this tailored approach, we conducted a cross-standard
evaluation using a pre-trained Universal Dependencies parser. The analysis
revealed a systematic 47.9% divergence in annotations, pinpointing the
inadequacy of universal schemes for handling Uyghur-specific structures.
Grounded in nine annotation principles that ensure typological accuracy and
semantic transparency, the Modern Uyghur Dependency Treebank (MUDT) provides a
more accurate and semantically transparent representation, designed to enable
significant improvements in parsing and downstream NLP tasks, and offers a
replicable model for other morphologically complex languages.

</details>


### [37] [MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21544)
*Jungyeon Lee,Kangmin Lee,Taeuk Kim*

Main category: cs.CL

TL;DR: The paper introduces a KG-based framework, MAGIC, to address limitations in existing benchmarks for knowledge conflict in RAG systems, revealing LLMs' struggles with conflict detection and source identification.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for knowledge conflict in RAG systems are limited in scope and methods, prompting the need for a more versatile and interpretable framework.

Method: Proposes a knowledge graph (KG)-based framework to generate varied and subtle conflicts, leveraging explicit relational structures for interpretability.

Result: LLMs, both open-source and proprietary, struggle with detecting conflicts, especially in multi-hop reasoning, and often fail to identify contradiction sources.

Conclusion: The MAGIC benchmark provides insights for improving LLMs in handling conflicting information, laying groundwork for future enhancements.

Abstract: Knowledge conflict often arises in retrieval-augmented generation (RAG)
systems, where retrieved documents may be inconsistent with one another or
contradict the model's parametric knowledge. Existing benchmarks for
investigating the phenomenon have notable limitations, including a narrow focus
on the question answering setup, heavy reliance on entity substitution
techniques, and a restricted range of conflict types. To address these issues,
we propose a knowledge graph (KG)-based framework that generates varied and
subtle conflicts between two similar yet distinct contexts, while ensuring
interpretability through the explicit relational structure of KGs. Experimental
results on our benchmark, MAGIC, provide intriguing insights into the inner
workings of LLMs regarding knowledge conflict: both open-source and proprietary
models struggle with conflict detection -- especially when multi-hop reasoning
is required -- and often fail to pinpoint the exact source of contradictions.
Finally, we present in-depth analyses that serve as a foundation for improving
LLMs in integrating diverse, sometimes even conflicting, information.

</details>


### [38] [Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers](https://arxiv.org/abs/2507.21556)
*Akhilesh Kakolu Ramarao,Kevin Tang,Dinah Baer-Henney*

Main category: cs.CL

TL;DR: The study compares transformer models to human data in processing Spanish irregular morphomic patterns, finding models outperform humans in accuracy but differ in response preferences and sensitivity to training data.


<details>
  <summary>Details</summary>
Motivation: To assess if transformer models can replicate human-like sensitivity to complex linguistic phenomena like the morphome under controlled conditions.

Method: Direct comparison of transformer models to human behavioral data, using three frequency conditions (natural, low-frequency, high-frequency) of irregular verb distributions.

Result: Models outperformed humans in accuracy but diverged in response preferences, favoring irregular responses and being influenced by training data. Sensitivity to phonological similarity varied by training distribution.

Conclusion: Transformer models show promise in mimicking human linguistic processing but exhibit key differences, highlighting the impact of training data on model behavior.

Abstract: This study investigates the cognitive plausibility of the Spanish irregular
morphomic pattern by directly comparing transformer-based neural networks to
human behavioral data from \citet{Nevins2015TheRA}. Using the same analytical
framework as the original human study, we evaluate whether transformer models
can replicate human-like sensitivity to a complex linguistic phenomena, the
morphome, under controlled input conditions. Our experiments focus on three
frequency conditions: natural, low-frequency, and high-frequency distributions
of verbs exhibiting irregular morphomic patterns. While the models outperformed
humans in stem and suffix accuracy, a clear divergence emerged in response
preferences. Unlike humans, who consistently favored natural responses across
all test items, models' preferred irregular responses and were influenced by
the proportion of irregular verbs in their training data. Additionally, models
trained on the natural and low-frequency distributions, but not the
high-frequency distribution, were sensitive to the phonological similarity
between test items and real Spanish L-shaped verbs.

</details>


### [39] [Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages](https://arxiv.org/abs/2507.21568)
*Aarón Galiano-Jiménez,Juan Antonio Pérez-Ortiz,Felipe Sánchez-Martínez,Víctor M. Sánchez-Cartagena*

Main category: cs.CL

TL;DR: The paper introduces Multi-Hypothesis Distillation (MHD), a sequence-level knowledge distillation method for multilingual translation models, leveraging multiple translations to enrich student learning and address issues like low variability and bias.


<details>
  <summary>Details</summary>
Motivation: To improve student model performance by capturing more of the teacher model's output distribution beyond beam search, addressing variability and bias in low-resource languages.

Method: Proposes MHD, which generates multiple translations per source sentence using n-best lists and alternative decoding methods to enhance variability and lexical richness.

Result: Shows that sampling methods, despite slight quality trade-offs, improve variability and reduce gender bias, enhancing student model performance.

Conclusion: MHD effectively leverages teacher model insights for better student learning, especially in low-resource settings, while mitigating bias.

Abstract: This paper explores sequence-level knowledge distillation (KD) of
multilingual pre-trained encoder-decoder translation models. We argue that the
teacher model's output distribution holds valuable insights for the student,
beyond the approximated mode obtained through beam search (the standard
decoding method), and present Multi-Hypothesis Distillation (MHD), a
sequence-level KD method that generates multiple translations for each source
sentence. This provides a larger representation of the teacher model
distribution and exposes the student model to a wider range of target-side
prefixes. We leverage $n$-best lists from beam search to guide the student's
learning and examine alternative decoding methods to address issues like low
variability and the under-representation of infrequent tokens. For low-resource
languages, our research shows that while sampling methods may slightly
compromise translation quality compared to beam search based approaches, they
enhance the generated corpora with greater variability and lexical richness.
This ultimately improves student model performance and mitigates the gender
bias amplification often associated with KD.

</details>


### [40] [Multilingual JobBERT for Cross-Lingual Job Title Matching](https://arxiv.org/abs/2507.21609)
*Jens-Joris Decorte,Matthias De Lange,Jeroen Van Hautte*

Main category: cs.CL

TL;DR: JobBERT-V3 is a contrastive learning-based model for cross-lingual job title matching, extending JobBERT-V2 to support English, German, Spanish, and Chinese. It uses synthetic translations and a multilingual dataset, achieving strong performance on benchmarks and demonstrating broader applicability in labor market intelligence.


<details>
  <summary>Details</summary>
Motivation: To extend monolingual job title matching to a multilingual context, enabling robust alignment across languages without task-specific supervision.

Method: Leverages synthetic translations and a balanced multilingual dataset (21M+ job titles) with a contrastive learning approach, building on JobBERT-V2's architecture.

Result: Outperforms multilingual baselines on TalentCLEF 2025, achieving consistent performance in monolingual and cross-lingual settings. Also effective for ranking relevant skills.

Conclusion: JobBERT-V3 is a powerful tool for multilingual job title matching and labor market intelligence, with publicly available implementation.

Abstract: We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual
job title matching. Building on the state-of-the-art monolingual JobBERT-V2,
our approach extends support to English, German, Spanish, and Chinese by
leveraging synthetic translations and a balanced multilingual dataset of over
21 million job titles. The model retains the efficiency-focused architecture of
its predecessor while enabling robust alignment across languages without
requiring task-specific supervision. Extensive evaluations on the TalentCLEF
2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual
baselines and achieves consistent performance across both monolingual and
cross-lingual settings. While not the primary focus, we also show that the
model can be effectively used to rank relevant skills for a given job title,
demonstrating its broader applicability in multilingual labor market
intelligence. The model is publicly available:
https://huggingface.co/TechWolf/JobBERT-v3.

</details>


### [41] [Libra: Assessing and Improving Reward Model by Learning to Think](https://arxiv.org/abs/2507.21645)
*Meng Zhou,Bei Li,Jiahao Liu,Xiaowen Shi,Yang Bai,Rongxiang Weng,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: The paper proposes a framework to enhance reward models in reinforcement learning for complex reasoning, introducing a benchmark (Libra Bench) and a generative reward model (Libra-RM) that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Current reward models in RL for reasoning tasks rely on rule-based or reference-based rewards, limiting scalability and performance.

Method: Developed Libra Bench for benchmarking and introduced learning-to-think methodologies to improve generative reward models (Libra-RM).

Result: Libra-RM achieves state-of-the-art results and shows potential for improving reasoning models with unlabeled data.

Conclusion: The proposed framework addresses limitations of current reward models, enabling better reasoning performance and scalability.

Abstract: Reinforcement learning (RL) has significantly improved the reasoning ability
of large language models. However, current reward models underperform in
challenging reasoning scenarios and predominant RL training paradigms rely on
rule-based or reference-based rewards, which impose two critical limitations:
1) the dependence on finely annotated reference answer to attain rewards; and
2) the requirement for constrained output format. These limitations
fundamentally hinder further RL data scaling and sustained enhancement of model
reasoning performance. To address these limitations, we propose a comprehensive
framework for evaluating and improving the performance of reward models in
complex reasoning scenarios. We first present a reasoning-oriented benchmark
(Libra Bench), systematically constructed from a diverse collection of
challenging mathematical problems and advanced reasoning models, to address the
limitations of existing reward model benchmarks in reasoning scenarios. We
further introduce a novel approach for improving the generative reward model
via learning-to-think methodologies. Based on the proposed approach, we develop
Libra-RM series, a collection of generative reward models with reasoning
capabilities that achieve state-of-the-art results on various benchmarks.
Comprehensive downstream experiments are conducted and the experimental results
demonstrate the correlation between our Libra Bench and downstream application,
and the potential of Libra-RM to further improve reasoning models with
unlabeled data.

</details>


### [42] [UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases](https://arxiv.org/abs/2507.21652)
*Raj Vardhan Tomar,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: UnsafeChain is a dataset for safety alignment in large reasoning models, addressing hard prompts that elicit harmful outputs by correcting them into safe responses. It outperforms existing datasets in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment methods overlook hard prompts that consistently produce harmful outputs, creating a gap in ensuring model safety.

Method: UnsafeChain is constructed from hard prompts with diverse sources, identifying and correcting unsafe completions into safe responses. Three LRMs are fine-tuned on this dataset.

Result: UnsafeChain consistently outperforms SafeChain and STAR-1 across benchmarks, with even a small subset matching or surpassing baseline performance.

Conclusion: Correction-based supervision in UnsafeChain effectively enhances safety while preserving reasoning ability, demonstrating generalizability.

Abstract: As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT)
reasoning introduces new safety challenges. Existing SFT-based safety alignment
studies dominantly focused on filtering prompts with safe, high-quality
responses, while overlooking hard prompts that always elicit harmful outputs.
To fill this gap, we introduce UnsafeChain, a safety alignment dataset
constructed from hard prompts with diverse sources, where unsafe completions
are identified and explicitly corrected into safe responses. By exposing models
to unsafe behaviors and guiding their correction, UnsafeChain enhances safety
while preserving general reasoning ability. We fine-tune three LRMs on
UnsafeChain and compare them against recent SafeChain and STAR-1 across six
out-of-distribution and five in-distribution benchmarks. UnsafeChain
consistently outperforms prior datasets, with even a 1K subset matching or
surpassing baseline performance, demonstrating the effectiveness and
generalizability of correction-based supervision. We release our dataset and
code at https://github.com/mbzuai-nlp/UnsafeChain

</details>


### [43] [Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal](https://arxiv.org/abs/2507.21750)
*Yang Wang,Chenghao Xiao,Yizhi Li,Stuart E. Middleton,Noura Al Moubayed,Chenghua Lin*

Main category: cs.CL

TL;DR: A simple add-on module improves adversarial robustness of pre-trained language models by removing instance-level principal components, avoiding costly adversarial training.


<details>
  <summary>Details</summary>
Motivation: Pre-trained language models (PLMs) are vulnerable to adversarial attacks, and existing defenses are computationally expensive.

Method: Proposes transforming the embedding space to approximate Gaussian properties, removing instance-level principal components without adversarial training.

Result: Improves robustness on eight benchmarks while maintaining baseline accuracy, balancing robustness and generalization.

Conclusion: The method offers a cost-effective way to enhance PLM robustness without adversarial examples or expensive training.

Abstract: Pre-trained language models (PLMs) have driven substantial progress in
natural language processing but remain vulnerable to adversarial attacks,
raising concerns about their robustness in real-world applications. Previous
studies have sought to mitigate the impact of adversarial attacks by
introducing adversarial perturbations into the training process, either
implicitly or explicitly. While both strategies enhance robustness, they often
incur high computational costs. In this work, we propose a simple yet effective
add-on module that enhances the adversarial robustness of PLMs by removing
instance-level principal components, without relying on conventional
adversarial defences or perturbing the original training data. Our approach
transforms the embedding space to approximate Gaussian properties, thereby
reducing its susceptibility to adversarial perturbations while preserving
semantic relationships. This transformation aligns embedding distributions in a
way that minimises the impact of adversarial noise on decision boundaries,
enhancing robustness without requiring adversarial examples or costly
training-time augmentation. Evaluations on eight benchmark datasets show that
our approach improves adversarial robustness while maintaining comparable
before-attack accuracy to baselines, achieving a balanced trade-off between
robustness and generalisation.

</details>


### [44] [AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models](https://arxiv.org/abs/2507.21773)
*Lian Yan,Haotian Wang,Chen Tang,Haifeng Liu,Tianyang Sun,Liangliang Liu,Yi Guan,Jingchi Jiang*

Main category: cs.CL

TL;DR: AgriEval is the first comprehensive Chinese agricultural benchmark for evaluating LLMs, covering six major categories and 29 subcategories. It includes 14,697 multiple-choice and 2,167 open-ended questions, revealing that most LLMs struggle to achieve 60% accuracy.


<details>
  <summary>Details</summary>
Motivation: The lack of training data and evaluation benchmarks in agriculture hinders LLM deployment. AgriEval addresses this gap.

Method: AgriEval curates high-quality data from university-level exams, covering memorization, understanding, inference, and generation. It evaluates 51 LLMs.

Result: Most LLMs score below 60% accuracy, highlighting developmental potential. Factors affecting performance are analyzed.

Conclusion: AgriEval provides a robust benchmark for agricultural LLMs, revealing current limitations and suggesting improvement strategies.

Abstract: In the agricultural domain, the deployment of large language models (LLMs) is
hindered by the lack of training data and evaluation benchmarks. To mitigate
this issue, we propose AgriEval, the first comprehensive Chinese agricultural
benchmark with three main characteristics: (1) Comprehensive Capability
Evaluation. AgriEval covers six major agriculture categories and 29
subcategories within agriculture, addressing four core cognitive scenarios:
memorization, understanding, inference, and generation. (2) High-Quality Data.
The dataset is curated from university-level examinations and assignments,
providing a natural and robust benchmark for assessing the capacity of LLMs to
apply knowledge and make expert-like decisions. (3) Diverse Formats and
Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167
open-ended question-and-answer questions, establishing it as the most extensive
agricultural benchmark available to date. We also present comprehensive
experimental results over 51 open-source and commercial LLMs. The experimental
results reveal that most existing LLMs struggle to achieve 60% accuracy,
underscoring the developmental potential in agricultural LLMs. Additionally, we
conduct extensive experiments to investigate factors influencing model
performance and propose strategies for enhancement. AgriEval is available at
https://github.com/YanPioneer/AgriEval/.

</details>


### [45] [The Problem with Safety Classification is not just the Models](https://arxiv.org/abs/2507.21782)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: The paper highlights multilingual disparities in safety classifiers for LLMs and identifies issues with evaluation datasets, emphasizing the need for better methods to detect harmful content across languages.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on evaluating safety classifiers for LLMs, especially in multilingual contexts, and to uncover disparities in their effectiveness.

Method: Analyzed 5 safety classification models using datasets covering 18 languages, identifying multilingual disparities and dataset issues.

Result: Found significant disparities in safety classifiers' performance across languages and highlighted flaws in evaluation datasets.

Conclusion: Current safety classifiers' shortcomings stem from both model and dataset issues, calling for improved multilingual evaluation methods.

Abstract: Studying the robustness of Large Language Models (LLMs) to unsafe behaviors
is an important topic of research today. Building safety classification models
or guard models, which are fine-tuned models for input/output safety
classification for LLMs, is seen as one of the solutions to address the issue.
Although there is a lot of research on the safety testing of LLMs themselves,
there is little research on evaluating the effectiveness of such safety
classifiers or the evaluation datasets used for testing them, especially in
multilingual scenarios. In this position paper, we demonstrate how multilingual
disparities exist in 5 safety classification models by considering datasets
covering 18 languages. At the same time, we identify potential issues with the
evaluation datasets, arguing that the shortcomings of current safety
classifiers are not only because of the models themselves. We expect that these
findings will contribute to the discussion on developing better methods to
identify harmful content in LLM inputs across languages.

</details>


### [46] [ChartMark: A Structured Grammar for Chart Annotation](https://arxiv.org/abs/2507.21810)
*Yiyu Chen,Yifan Wu,Shuyu Shen,Yupeng Xie,Leixian Shen,Hui Xiong,Yuyu Luo*

Main category: cs.CL

TL;DR: ChartMark is a structured grammar for chart annotations, separating semantics from implementation, enabling cross-platform reuse and flexibility.


<details>
  <summary>Details</summary>
Motivation: Current chart annotation representations are fragmented and non-standardized, limiting accessibility and reuse.

Method: Proposes ChartMark, a hierarchical framework mapping annotation dimensions (e.g., task, chart context) to support abstract intents and visual details.

Result: Demonstrates practical applicability by converting ChartMark into Vega-Lite visualizations, showing flexibility and expressiveness.

Conclusion: ChartMark standardizes annotations, enhancing accessibility and cross-platform usability.

Abstract: Chart annotations enhance visualization accessibility but suffer from
fragmented, non-standardized representations that limit cross-platform reuse.
We propose ChartMark, a structured grammar that separates annotation semantics
from visualization implementations. ChartMark features a hierarchical framework
mapping onto annotation dimensions (e.g., task, chart context), supporting both
abstract intents and precise visual details. Our toolkit demonstrates
converting ChartMark specifications into Vega-Lite visualizations, highlighting
its flexibility, expressiveness, and practical applicability.

</details>


### [47] [Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish](https://arxiv.org/abs/2507.21813)
*Elena Alvarez-Mellado,Jordi Porta-Zamorano,Constantine Lignos,Julio Gonzalo*

Main category: cs.CL

TL;DR: ADoBo 2025 task focused on identifying anglicisms in Spanish texts, with five teams using diverse methods (LLMs, deep learning, Transformers, rule-based). Performance varied widely (F1: 0.17-0.99).


<details>
  <summary>Details</summary>
Motivation: To benchmark and advance techniques for detecting English lexical borrowings in Spanish journalistic texts.

Method: Teams employed LLMs, deep learning, Transformer-based models, and rule-based systems to identify anglicisms.

Result: Performance varied significantly, with F1 scores ranging from 0.17 to 0.99.

Conclusion: The task highlighted the variability in system performance for anglicism detection, indicating room for improvement.

Abstract: This paper summarizes the main findings of ADoBo 2025, the shared task on
anglicism identification in Spanish proposed in the context of IberLEF 2025.
Participants of ADoBo 2025 were asked to detect English lexical borrowings (or
anglicisms) from a collection of Spanish journalistic texts. Five teams
submitted their solutions for the test phase. Proposed systems included LLMs,
deep learning models, Transformer-based models and rule-based systems. The
results range from F1 scores of 0.17 to 0.99, which showcases the variability
in performance different systems can have for this task.

</details>


### [48] [HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs](https://arxiv.org/abs/2507.21815)
*Kaixuan Wang,Chenxin Diao,Jason T. Jacques,Zhongliang Guo,Shuai Zhao*

Main category: cs.CL

TL;DR: HRIPBench evaluates LLMs' accuracy and safety in providing harm reduction info for substance use, revealing gaps and risks.


<details>
  <summary>Details</summary>
Motivation: Assess LLMs' ability to provide accurate and safe harm reduction info for people who use drugs (PWUD).

Method: Created HRIPBench with 2,160 QA pairs, testing safety boundaries, quantitative values, and polysubstance risks. Evaluated LLMs using Instruction and RAG schemes.

Result: State-of-the-art LLMs often provide inaccurate info and pose safety risks to PWUD.

Conclusion: LLMs in harm reduction require cautious use to avoid negative health outcomes.

Abstract: Millions of individuals' well-being are challenged by the harms of substance
use. Harm reduction as a public health strategy is designed to improve their
health outcomes and reduce safety risks. Some large language models (LLMs) have
demonstrated a decent level of medical knowledge, promising to address the
information needs of people who use drugs (PWUD). However, their performance in
relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark
designed to evaluate LLM's accuracy and safety risks in harm reduction
information provision. The benchmark dataset HRIP-Basic has 2,160
question-answer-evidence pairs. The scope covers three tasks: checking safety
boundaries, providing quantitative values, and inferring polysubstance use
risks. We build the Instruction and RAG schemes to evaluate model behaviours
based on their inherent knowledge and the integration of domain knowledge. Our
results indicate that state-of-the-art LLMs still struggle to provide accurate
harm reduction information, and sometimes, carry out severe safety risks to
PWUD. The use of LLMs in harm reduction contexts should be cautiously
constrained to avoid inducing negative health outcomes. WARNING: This paper
contains illicit content that potentially induces harms.

</details>


### [49] [Modelling Adjectival Modification Effects on Semantic Plausibility](https://arxiv.org/abs/2507.21828)
*Anna Golub,Beate Zywietz,Annerose Eichel*

Main category: cs.CL

TL;DR: The paper addresses the challenge of tracking plausibility changes in events due to modifications, using the ADEPT benchmark. It evaluates sentence transformers and other models, finding they struggle, with sentence transformers underperforming compared to RoBERTa. It also critiques evaluation imbalances.


<details>
  <summary>Details</summary>
Motivation: Understanding plausibility changes is crucial for tasks like dialogue generation and commonsense reasoning, but prior work has overlooked this aspect.

Method: The study uses the ADEPT benchmark (16K sentence pairs with one adjectival modifier difference) and evaluates sentence transformers and transformer-based models like RoBERTa.

Result: Sentence transformers and other models perform poorly, with sentence transformers underperforming RoBERTa. Evaluation imbalances distort performance metrics.

Conclusion: The work highlights the need for realistic, balanced evaluation methods to ensure trustworthy results in plausibility change tasks.

Abstract: While the task of assessing the plausibility of events such as ''news is
relevant'' has been addressed by a growing body of work, less attention has
been paid to capturing changes in plausibility as triggered by event
modification. Understanding changes in plausibility is relevant for tasks such
as dialogue generation, commonsense reasoning, and hallucination detection as
it allows to correctly model, for example, ''gentle sarcasm'' as a sign of
closeness rather than unkindness among friends [9]. In this work, we tackle the
ADEPT challenge benchmark [6] consisting of 16K English sentence pairs
differing by exactly one adjectival modifier. Our modeling experiments provide
a conceptually novel method by using sentence transformers, and reveal that
both they and transformer-based models struggle with the task at hand, and
sentence transformers - despite their conceptual alignment with the task - even
under-perform in comparison to models like RoBERTa. Furthermore, an in-depth
comparison with prior work highlights the importance of a more realistic,
balanced evaluation method: imbalances distort model performance and evaluation
metrics, and weaken result trustworthiness.

</details>


### [50] [Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences](https://arxiv.org/abs/2507.21831)
*Andreas Reich,Claudia Thoms,Tobias Schrimpf*

Main category: cs.CL

TL;DR: HALC is a pipeline for systematically constructing optimal prompts for LLM coding tasks, validated with 1,512 prompts and expert codings, achieving reliable performance.


<details>
  <summary>Details</summary>
Motivation: To address the variability in prompt effectiveness across LLMs and tasks, reducing reliance on trial and error.

Method: Proposes HALC, a pipeline for systematic prompt construction, tested with 1,512 prompts and expert codings (ground truth).

Result: Achieved reliable coding for single and multiple variables (e.g., αclimate = .76, αmovement = .78) using Mistral NeMo.

Conclusion: HALC provides insights into effective prompting strategies and reliable prompt identification for LLM coding tasks.

Abstract: LLMs are seeing widespread use for task automation, including automated
coding in the social sciences. However, even though researchers have proposed
different prompting strategies, their effectiveness varies across LLMs and
tasks. Often trial and error practices are still widespread. We propose
HALC$-$a general pipeline that allows for the systematic and reliable
construction of optimal prompts for any given coding task and model, permitting
the integration of any prompting strategy deemed relevant. To investigate LLM
coding and validate our pipeline, we sent a total of 1,512 individual prompts
to our local LLMs in over two million requests. We test prompting strategies
and LLM task performance based on few expert codings (ground truth). When
compared to these expert codings, we find prompts that code reliably for single
variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two
variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM
Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM
to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our
paper provides insights into the effectiveness of different prompting
strategies, crucial influencing factors, and the identification of reliable
prompts for each coding task and model.

</details>


### [51] [AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.21836)
*Yifan Wei,Xiaoyan Yu,Yixuan Weng,Tengfei Pan,Angsheng Li,Li Du*

Main category: cs.CL

TL;DR: AutoTIR is a reinforcement learning framework that enables LLMs to autonomously decide tool usage during reasoning, outperforming rigid predefined methods.


<details>
  <summary>Details</summary>
Motivation: Existing tool-use methods for LLMs are rigid and risk degrading language competence, unlike human adaptive tool selection.

Method: AutoTIR uses reinforcement learning with a hybrid reward mechanism to optimize task correctness, structured output, and penalize incorrect tool usage.

Result: AutoTIR achieves superior performance and generalization across diverse tasks compared to baselines.

Conclusion: Reinforcement learning shows promise for scalable and generalizable Tool-Integrated Reasoning in LLMs.

Abstract: Large Language Models (LLMs), when enhanced through reasoning-oriented
post-training, evolve into powerful Large Reasoning Models (LRMs).
Tool-Integrated Reasoning (TIR) further extends their capabilities by
incorporating external tools, but existing methods often rely on rigid,
predefined tool-use patterns that risk degrading core language competence.
Inspired by the human ability to adaptively select tools, we introduce AutoTIR,
a reinforcement learning framework that enables LLMs to autonomously decide
whether and which tool to invoke during the reasoning process, rather than
following static tool-use strategies. AutoTIR leverages a hybrid reward
mechanism that jointly optimizes for task-specific answer correctness,
structured output adherence, and penalization of incorrect tool usage, thereby
encouraging both precise reasoning and efficient tool integration. Extensive
evaluations across diverse knowledge-intensive, mathematical, and general
language modeling tasks demonstrate that AutoTIR achieves superior overall
performance, significantly outperforming baselines and exhibits superior
generalization in tool-use behavior. These results highlight the promise of
reinforcement learning in building truly generalizable and scalable TIR
capabilities in LLMs. The code and data are available at
https://github.com/weiyifan1023/AutoTIR.

</details>


### [52] [Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2507.21892)
*Haoran Luo,Haihong E,Guanting Chen,Qika Lin,Yikai Guo,Fangzhi Xu,Zemin Kuang,Meina Song,Xiaobao Wu,Yifan Zhu,Luu Anh Tuan*

Main category: cs.CL

TL;DR: Graph-R1 improves GraphRAG with lightweight hypergraph construction and RL-based retrieval, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in GraphRAG like high construction cost, fixed retrieval, and reliance on long-context reasoning.

Method: Proposes Graph-R1: lightweight hypergraph construction, multi-turn RL-based retrieval, and end-to-end reward optimization.

Result: Outperforms traditional GraphRAG and RL-enhanced RAG in reasoning accuracy, retrieval efficiency, and generation quality.

Conclusion: Graph-R1 offers a more efficient and effective solution for retrieval-augmented generation.

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by
incorporating external knowledge, but relies on chunk-based retrieval that
lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge
as entity-relation graphs, but still face challenges in high construction cost,
fixed one-time retrieval, and reliance on long-context reasoning and prompt
design. To address these challenges, we propose Graph-R1, an agentic GraphRAG
framework via end-to-end reinforcement learning (RL). It introduces lightweight
knowledge hypergraph construction, models retrieval as a multi-turn
agent-environment interaction, and optimizes the agent process via an
end-to-end reward mechanism. Experiments on standard RAG datasets show that
Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in
reasoning accuracy, retrieval efficiency, and generation quality.

</details>


### [53] [Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs](https://arxiv.org/abs/2507.21914)
*Qinyuan Wu,Soumi Das,Mahsa Amani,Bishwamittra Ghosh,Mohammad Aflah Khan,Krishna P. Gummadi,Muhammad Bilal Zafar*

Main category: cs.CL

TL;DR: LLMs can generalize from rote memorized data using a two-phase framework: memorization followed by fine-tuning with meaningful prompts.


<details>
  <summary>Details</summary>
Motivation: Challenge the belief that rote learning hinders generalization, especially for factual knowledge requiring memorization.

Method: Two-phase approach: 1) rote memorization of factual data, 2) fine-tuning with semantically meaningful prompts.

Result: Models reinterpret memorized data, showing structured, semantically aligned latent representations.

Conclusion: This method enables efficient knowledge injection but also poses risks of misuse.

Abstract: Rote learning is a memorization technique based on repetition. It is commonly
believed to hinder generalization by encouraging verbatim memorization rather
than deeper understanding. This insight holds for even learning factual
knowledge that inevitably requires a certain degree of memorization. In this
work, we demonstrate that LLMs can be trained to generalize from rote memorized
data. We introduce a two-phase memorize-then-generalize framework, where the
model first rote memorizes factual subject-object associations using a
semantically meaningless token and then learns to generalize by fine-tuning on
a small set of semantically meaningful prompts. Extensive experiments over 8
LLMs show that the models can reinterpret rote memorized data through the
semantically meaningful prompts, as evidenced by the emergence of structured,
semantically aligned latent representations between the two. This surprising
finding opens the door to both effective and efficient knowledge injection and
possible risks of repurposing the memorized data for malicious usage.

</details>


### [54] [Training language models to be warm and empathetic makes them less reliable and more sycophantic](https://arxiv.org/abs/2507.21919)
*Lujain Ibrahim,Franziska Sofia Hafner,Luc Rocher*

Main category: cs.CL

TL;DR: Optimizing AI language models for warmth and empathy reduces their reliability, especially in safety-critical tasks, despite preserved performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: To investigate the trade-off between warmth and reliability in AI language models, particularly when users express vulnerability.

Method: Controlled experiments on five language models of varying sizes and architectures, training them for warmer responses and evaluating them on safety-critical tasks.

Result: Warm models showed higher error rates (+10 to +30 percentage points), promoted conspiracy theories, provided incorrect information, and validated incorrect beliefs, especially with sad user messages.

Conclusion: Current evaluation practices may miss systematic risks, necessitating a rethink in AI development and oversight for human-like systems.

Abstract: Artificial intelligence (AI) developers are increasingly building language
models with warm and empathetic personas that millions of people now use for
advice, therapy, and companionship. Here, we show how this creates a
significant trade-off: optimizing language models for warmth undermines their
reliability, especially when users express vulnerability. We conducted
controlled experiments on five language models of varying sizes and
architectures, training them to produce warmer, more empathetic responses, then
evaluating them on safety-critical tasks. Warm models showed substantially
higher error rates (+10 to +30 percentage points) than their original
counterparts, promoting conspiracy theories, providing incorrect factual
information, and offering problematic medical advice. They were also
significantly more likely to validate incorrect user beliefs, particularly when
user messages expressed sadness. Importantly, these effects were consistent
across different model architectures, and occurred despite preserved
performance on standard benchmarks, revealing systematic risks that current
evaluation practices may fail to detect. As human-like AI systems are deployed
at an unprecedented scale, our findings indicate a need to rethink how we
develop and oversee these systems that are reshaping human relationships and
social interaction.

</details>


### [55] [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](https://arxiv.org/abs/2507.21931)
*Carel van Niekerk,Renato Vukovic,Benjamin Matthias Ruppik,Hsien-chin Lin,Milica Gašić*

Main category: cs.CL

TL;DR: RLSF enhances LLM reliability by using self-generated confidence as intrinsic reward, improving calibration and reasoning without external feedback.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce poorly-calibrated answers, limiting reliability on reasoning tasks. RLSF aims to improve this by leveraging the model's own confidence.

Method: RLSF ranks chain-of-thought solutions by confidence, uses synthetic preferences for fine-tuning via preference optimization, mimicking RLHF but without human labels.

Result: RLSF improves calibration and reasoning, boosting performance on arithmetic and multiple-choice tasks.

Conclusion: RLSF demonstrates the potential of intrinsic rewards for efficient LLM post-training, warranting further research.

Abstract: Large Language Models (LLMs) often produce plausible but poorly-calibrated
answers, limiting their reliability on reasoning-intensive tasks. We present
Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that
uses the model's own confidence as an intrinsic reward, mimicking how humans
learn in the absence of external feedback. After a frozen LLM generates several
chain-of-thought solutions, we define and compute the confidence of each final
answer span and rank the traces accordingly. These synthetic preferences are
then used to fine-tune the policy with standard preference optimization,
similar to RLHF yet requiring no human labels, gold answers, or externally
curated rewards.
  RLSF simultaneously (i) refines the model's probability estimates --
restoring well-behaved calibration -- and (ii) strengthens step-by-step
reasoning, yielding improved performance on arithmetic reasoning and
multiple-choice question answering.
  By turning a model's own uncertainty into useful self-feedback, RLSF affirms
reinforcement learning on intrinsic model behaviour as a principled and
data-efficient component of the LLM post-training pipeline and warrents further
research in intrinsic rewards for LLM post-training.

</details>


### [56] [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)
*Tianyi Hu,Andrea Morales-Garzón,Jingyi Zheng,Maria Maistro,Daniel Hershcovich*

Main category: cs.CL

TL;DR: The paper introduces CARRIAGE, a RAG framework for diverse cross-cultural recipe adaptation, addressing RAG's limitation in generating varied outputs.


<details>
  <summary>Details</summary>
Motivation: To ensure cultural appropriateness and dietary diversity in recipe adaptation, while overcoming RAG's lack of output diversity.

Method: Proposes CARRIAGE, a plug-and-play RAG framework enhancing diversity in retrieval and context organization.

Result: CARRIAGE achieves Pareto efficiency in diversity and quality compared to closed-book LLMs.

Conclusion: CARRIAGE is the first RAG framework explicitly designed for diverse outputs, improving cross-cultural recipe adaptation.

Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural
appropriateness and retain the original dish's essence, but also to provide
diverse options for various dietary needs and preferences. Retrieval Augmented
Generation (RAG) is a promising approach, combining the retrieval of real
recipes from the target cuisine for cultural adaptability with large language
models (LLMs) for relevance. However, it remains unclear whether RAG can
generate diverse adaptation results. Our analysis shows that RAG tends to
overly rely on a limited portion of the context across generations, failing to
produce diverse outputs even when provided with varied contextual inputs. This
reveals a key limitation of RAG in creative tasks with multiple valid answers:
it fails to leverage contextual diversity for generating varied responses. To
address this issue, we propose CARRIAGE, a plug-and-play RAG framework for
cross-cultural recipe adaptation that enhances diversity in both retrieval and
context organization. To our knowledge, this is the first RAG framework that
explicitly aims to generate highly diverse outputs to accommodate multiple user
preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in
terms of diversity and quality of recipe adaptation compared to closed-book
LLMs.

</details>


### [57] [Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models](https://arxiv.org/abs/2507.21980)
*Hyunwoo Yoo,Gail L. Rosen*

Main category: cs.CL

TL;DR: LLMs outperform traditional models in classifying microbiome samples and predicting contamination risk using metadata alone.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing microbiome studies with metadata, especially in small-sample or heterogeneous settings.

Method: Evaluated LLMs (ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, LLaMA 4) in zero-shot and few-shot settings against traditional models like Random Forests.

Result: LLMs outperformed baselines in ontology classification and contamination risk prediction, generalizing well across datasets.

Conclusion: LLMs offer a promising metadata-only approach for environmental microbiology and biosurveillance.

Abstract: Traditional machine learning models struggle to generalize in microbiome
studies where only metadata is available, especially in small-sample settings
or across studies with heterogeneous label formats. In this work, we explore
the use of large language models (LLMs) to classify microbial samples into
ontology categories such as EMPO 3 and related biological labels, as well as to
predict pathogen contamination risk, specifically the presence of E. Coli,
using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude
3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing
their performance against traditional models like Random Forests across
multiple real-world datasets. Our results show that LLMs not only outperform
baselines in ontology classification, but also demonstrate strong predictive
ability for contamination risk, generalizing across sites and metadata
distributions. These findings suggest that LLMs can effectively reason over
sparse, heterogeneous biological metadata and offer a promising metadata-only
approach for environmental microbiology and biosurveillance applications.

</details>


### [58] [DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router](https://arxiv.org/abs/2507.22050)
*Minghao Guo,Qingcheng Zeng,Xujiang Zhao,Yanchi Liu,Wenchao Yu,Mengnan Du,Haifeng Chen,Wei Cheng*

Main category: cs.CL

TL;DR: DeepSieve is an agentic RAG framework that improves reasoning and retrieval by decomposing queries and routing sub-questions to suitable sources, filtering noise through multi-stage distillation.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with knowledge-intensive queries due to lack of dynamic access to updated or domain-specific information, and existing RAG methods lack fine-grained control, leading to noisy retrieval and shallow reasoning.

Method: DeepSieve decomposes queries into sub-questions, routes them to optimal knowledge sources, and filters irrelevant information via multi-stage distillation, leveraging LLM-as-a-knowledge-router.

Result: Experiments show improved reasoning depth, retrieval precision, and interpretability over traditional RAG methods in multi-hop QA tasks.

Conclusion: DeepSieve offers a modular, transparent, and adaptable solution for enhancing LLM performance in knowledge-intensive tasks.

Abstract: Large Language Models (LLMs) excel at many reasoning tasks but struggle with
knowledge-intensive queries due to their inability to dynamically access
up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)
has emerged as a promising solution, enabling LLMs to ground their responses in
external sources. However, existing RAG methods lack fine-grained control over
both the query and source sides, often resulting in noisy retrieval and shallow
reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that
incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve
decomposes complex queries into structured sub-questions and recursively routes
each to the most suitable knowledge source, filtering irrelevant information
through a multi-stage distillation process. Our design emphasizes modularity,
transparency, and adaptability, leveraging recent advances in agentic system
design. Experiments on multi-hop QA tasks across heterogeneous sources
demonstrate improved reasoning depth, retrieval precision, and interpretability
over conventional RAG approaches.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [59] [GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data](https://arxiv.org/abs/2507.21069)
*Andreas Spilz,Heiko Oppel,Jochen Werner,Kathrin Stucke-Straub,Felix Capanni,Michael Munz*

Main category: cs.CV

TL;DR: A multimodal dataset of physiotherapeutic exercises and gait patterns, recorded using IMUs and MoCap, supports machine learning model development for movement analysis.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large, diverse datasets for robust sensor-based classification models in physiotherapeutic exercises and gait analysis.

Method: Data collected from 19 participants using synchronized IMUs and MoCap, including raw and processed data, annotations, and tools for analysis.

Result: A comprehensive dataset with detailed annotations, processed orientations, and tools for reproducibility, aiding in model development and benchmarking.

Conclusion: The dataset accelerates research in machine learning-driven human movement analysis by providing a valuable resource for diverse tasks.

Abstract: Wearable inertial measurement units (IMUs) offer a cost-effective and
scalable means to assess human movement quality in clinical and everyday
settings. However, the development of robust sensor-based classification models
for physiotherapeutic exercises and gait analysis requires large, diverse
datasets, which are costly and time-consuming to collect. Here, we present a
multimodal dataset of physiotherapeutic exercises - including correct and
clinically relevant variants - and gait-related exercises - including both
normal and impaired gait patterns - recorded from 19 participants using
synchronized IMUs and marker-based motion capture (MoCap). The dataset includes
raw data from nine IMUs and thirty-five optical markers capturing full-body
kinematics. Each IMU is additionally equipped with four optical markers,
enabling precise comparison between IMU-derived orientation estimates and
reference values from the MoCap system. To support further analysis, we also
provide processed IMU orientations aligned with common segment coordinate
systems, subject-specific OpenSim models, inverse kinematics results, and tools
for visualizing IMU orientations in the musculoskeletal context. Detailed
annotations of movement execution quality and time-stamped segmentations
support diverse analysis goals. This dataset supports the development and
benchmarking of machine learning models for tasks such as automatic exercise
evaluation, gait analysis, temporal activity segmentation, and biomechanical
parameter estimation. To facilitate reproducibility, we provide code for
postprocessing, sensor-to-segment alignment, inverse kinematics computation,
and technical validation. This resource is intended to accelerate research in
machine learning-driven human movement analysis.

</details>


### [60] [Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues](https://arxiv.org/abs/2507.21161)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CV

TL;DR: BF-PIP is a zero-shot pedestrian intention prediction method using continuous video clips and contextual data, outperforming GPT-4V by 18% without retraining.


<details>
  <summary>Details</summary>
Motivation: Pedestrian intention prediction is critical for autonomous driving but current methods require extensive retraining for new scenarios.

Method: BF-PIP uses continuous video clips with JAAD metadata, bounding-box annotations, and ego-vehicle speed via multimodal prompts, without retraining.

Result: Achieves 73% accuracy, 18% higher than GPT-4V baseline.

Conclusion: Combining temporal video and contextual cues improves intent prediction, enabling agile, retraining-free systems for intelligent transportation.

Abstract: Pedestrian intention prediction is essential for autonomous driving in
complex urban environments. Conventional approaches depend on supervised
learning over frame sequences and require extensive retraining to adapt to new
scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention
Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing
intentions directly from short, continuous video clips enriched with structured
JAAD metadata. In contrast to GPT-4V based methods that operate on discrete
frames, BF-PIP processes uninterrupted temporal clips. It also incorporates
bounding-box annotations and ego-vehicle speed via specialized multimodal
prompts. Without any additional training, BF-PIP achieves 73% prediction
accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate
that combining temporal video inputs with contextual cues enhances
spatiotemporal perception and improves intent inference under ambiguous
conditions. This approach paves the way for agile, retraining-free perception
module in intelligent transportation system.

</details>


### [61] [ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions](https://arxiv.org/abs/2507.21167)
*Danglu Yang,Liang Zhang,Zihao Yue,Liangyu Chen,Yichen Xu,Wenxuan Wang,Qin Jin*

Main category: cs.CV

TL;DR: The paper introduces a multimodal approach for chart editing, combining natural language and visual indicators, and presents ChartM3, a benchmark for evaluating and improving multimodal chart editing models.


<details>
  <summary>Details</summary>
Motivation: Existing chart editing methods rely on ambiguous natural language instructions, lacking support for fine-grained edits. A multimodal approach is proposed to address this limitation.

Method: The authors introduce ChartM3, a benchmark with 1,000 samples of varying difficulty, and ChartM3-Train, a 24,000-sample training set. They evaluate and fine-tune MLLMs using these datasets.

Result: Current MLLMs, including GPT-4o, struggle with visual indicators. Fine-tuning on ChartM3-Train significantly improves performance, highlighting the need for multimodal supervision.

Conclusion: Multimodal supervision is crucial for practical chart editing systems. The datasets and tools are publicly available to support further research.

Abstract: Charts are a fundamental visualization format widely used in data analysis
across research and industry. While enabling users to edit charts based on
high-level intentions is of great practical value, existing methods primarily
rely on natural language instructions, which are often too ambiguous to support
fine-grained editing. In this work, we introduce a novel paradigm for
multimodal chart editing, where user intent is expressed through a combination
of natural language and visual indicators that explicitly highlight the
elements to be modified. To support this paradigm, we present
Chart$\text{M}^3$, a new benchmark for Multimodal chart editing with
Multi-level complexity and Multi-perspective evaluation. Chart$\text{M}^3$
contains 1,000 samples spanning four levels of editing difficulty. Each sample
includes triplets in the form of (chart, code, multimodal instructions). To
comprehensively evaluate chart editing models, Chart$\text{M}^3$ provides
metrics that assess both visual appearance and code correctness. Our benchmark
reveals significant limitations in current multimodal large language models
(MLLMs), including GPT-4o, particularly in their ability to interpret and act
on visual indicators. To address this, we construct Chart$\text{M}^3$-Train, a
large-scale training set with 24,000 multimodal chart editing samples.
Fine-tuning MLLMs on this dataset leads to substantial improvements,
demonstrating the importance of multimodal supervision in building practical
chart editing systems. Our datasets, codes, and evaluation tools are available
at https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our
datasets, codes, and evaluation tools are available at
https://github.com/yaolinli/VCE.

</details>


### [62] [PanoGAN A Deep Generative Model for Panoramic Dental Radiographs](https://arxiv.org/abs/2507.21200)
*Soren Pedersen,Sanyam Jain,Mikkel Chavez,Viktor Ladehoff,Bruna Neves de Freitas,Ruben Pauwels*

Main category: cs.CV

TL;DR: A GAN was developed to synthesize dental panoramic radiographs, addressing data scarcity. A DCGAN with WGANGP was trained on 2322 radiographs, focusing on dentoalveolar regions. Four models were tested, with trade-offs between detail and clarity noted.


<details>
  <summary>Details</summary>
Motivation: To tackle the scarcity of data in dental research and education by generating synthetic radiographs.

Method: Trained a DCGAN using WGANGP on 2322 radiographs, with preprocessing and four model variations (critic iterations, feature depth, denoising).

Result: Generated images showed moderate anatomical depiction, with trade-offs: non-denoised data yielded finer details, while denoised data provided better clarity.

Conclusion: The study lays groundwork for future GAN applications in dental imaging, highlighting trade-offs in model choices.

Abstract: This paper presents the development of a generative adversarial network (GAN)
for synthesizing dental panoramic radiographs. Although exploratory in nature,
the study aims to address the scarcity of data in dental research and
education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss
with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying
quality. The focus was on the dentoalveolar regions, other anatomical
structures were cropped out. Extensive preprocessing and data cleaning were
performed to standardize the inputs while preserving anatomical variability. We
explored four candidate models by varying critic iterations, feature depth, and
the use of denoising prior to training. A clinical expert evaluated the
generated radiographs based on anatomical visibility and realism, using a
5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical
depiction, although some were degraded by artifacts. A trade-off was observed
the model trained on non-denoised data yielded finer details especially in
structures like the mandibular canal and trabecular bone, while a model trained
on denoised data offered superior overall image clarity and sharpness. These
findings provide a foundation for future work on GAN-based methods in dental
imaging.

</details>


### [63] [On Explaining Visual Captioning with Hybrid Markov Logic Networks](https://arxiv.org/abs/2507.21246)
*Monika Shah,Somdeb Sarkhel,Deepak Venugopal*

Main category: cs.CV

TL;DR: A novel framework using Hybrid Markov Logic Networks (HMLNs) is proposed to explain how DNNs integrate multimodal information for image captioning, offering interpretable insights into training data influences.


<details>
  <summary>Details</summary>
Motivation: Current metrics for DNN performance in multimodal tasks like image captioning lack deep insights into how models integrate visual, language, and knowledge information.

Method: The framework learns a HMLN distribution over training instances and infers distribution shifts when conditioning on generated captions, identifying influential examples.

Result: Experiments on state-of-the-art captioning models demonstrate the framework's interpretability and enable model comparison based on explainability.

Conclusion: The HMLN-based framework provides a transparent and interpretable method to understand DNN behavior in multimodal tasks, enhancing explainability.

Abstract: Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks
such as image captioning. However, explaining/interpreting how these models
integrate visual information, language information and knowledge representation
to generate meaningful captions remains a challenging problem. Standard metrics
to measure performance typically rely on comparing generated captions with
human-written ones that may not provide a user with a deep insights into this
integration. In this work, we develop a novel explanation framework that is
easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language
that can combine symbolic rules with real-valued functions - where we
hypothesize how relevant examples from the training data could have influenced
the generation of the observed caption. To do this, we learn a HMLN
distribution over the training instances and infer the shift in distributions
over these instances when we condition on the generated sample which allows us
to quantify which examples may have been a source of richer information to
generate the observed caption. Our experiments on captions generated for
several state-of-the-art captioning models using Amazon Mechanical Turk
illustrate the interpretability of our explanations, and allow us to compare
these models along the dimension of explainability.

</details>


### [64] [Dual Guidance Semi-Supervised Action Detection](https://arxiv.org/abs/2507.21247)
*Ankit Singh,Efstratios Gavves,Cees G. M. Snoek,Hilde Kuehne*

Main category: cs.CV

TL;DR: A semi-supervised learning (SSL) approach for spatial-temporal action localization is introduced, using a dual guidance network to improve pseudo-bounding box selection, outperforming image-based SSL baselines.


<details>
  <summary>Details</summary>
Motivation: SSL is underutilized in spatial-temporal action localization despite its potential to reduce annotation dependency.

Method: A dual guidance network combines frame-level classification and bounding-box prediction to ensure action class consistency.

Result: The method significantly improves performance on UCF101-24, J-HMDB-21, and AVA datasets in limited labeled data settings.

Conclusion: The proposed framework advances SSL in spatial-temporal action localization, demonstrating superior results over image-based SSL methods.

Abstract: Semi-Supervised Learning (SSL) has shown tremendous potential to improve the
predictive performance of deep learning models when annotations are hard to
obtain. However, the application of SSL has so far been mainly studied in the
context of image classification. In this work, we present a semi-supervised
approach for spatial-temporal action localization. We introduce a dual guidance
network to select better pseudo-bounding boxes. It combines a frame-level
classification with a bounding-box prediction to enforce action class
consistency across frames and boxes. Our evaluation across well-known
spatial-temporal action localization datasets, namely UCF101-24 , J-HMDB-21 and
AVA shows that the proposed module considerably enhances the model's
performance in limited labeled data settings. Our framework achieves superior
results compared to extended image-based semi-supervised baselines.

</details>


### [65] [Tracking Moose using Aerial Object Detection](https://arxiv.org/abs/2507.21256)
*Christopher Indris,Raiyan Rahman,Goetz Bramesfeld,Guanghui Wang*

Main category: cs.CV

TL;DR: The paper explores patching augmentation for small object detection in aerial wildlife tracking, comparing three models to balance accuracy and computational efficiency for UAV deployment.


<details>
  <summary>Details</summary>
Motivation: Aerial wildlife tracking faces challenges like high costs, risks, and computational limits, requiring efficient small object detection methods.

Method: Patching augmentation is applied to datasets, and three diverse object detectors are compared under varying hyperparameters.

Result: All models achieved ≥93% mAP@IoU=0.5 in some configurations, with simpler models performing comparably to complex ones.

Conclusion: Faster, simpler models are effective for UAV deployment, and datasets/models will be shared publicly.

Abstract: Aerial wildlife tracking is critical for conservation efforts and relies on
detecting small objects on the ground below the aircraft. It presents technical
challenges: crewed aircraft are expensive, risky and disruptive; autonomous
drones have limited computational capacity for onboard AI systems. Since the
objects of interest may appear only a few pixels wide, small object detection
is an inherently challenging computer vision subfield compounded by
computational efficiency needs. This paper applies a patching augmentation to
datasets to study model performance under various settings. A comparative study
of three common yet architecturally diverse object detectors is conducted using
the data, varying the patching method's hyperparameters against detection
accuracy. Each model achieved at least 93\% mAP@IoU=0.5 on at least one
patching configuration. Statistical analyses provide an in-depth commentary on
the effects of various factors. Analysis also shows that faster, simpler models
are about as effective as models that require more computational power for this
task and perform well given limited patch scales, encouraging UAV deployment.
Datasets and models will be made available via
https://github.com/chrisindris/Moose.

</details>


### [66] [HDR Environment Map Estimation with Latent Diffusion Models](https://arxiv.org/abs/2507.21261)
*Jack Hilliard,Adrian Hilton,Jean-Yves Guillemaut*

Main category: cs.CV

TL;DR: A novel method using Latent Diffusion Model (LDM) and ERP convolutional padding improves HDR environment map estimation from single-view images, reducing distortions and seams.


<details>
  <summary>Details</summary>
Motivation: Addressing distortions and seams in ERP representation for better HDR environment map estimation.

Method: Uses LDM with ERP convolutional padding and a panoramically-adapted Diffusion Transformer (PanoDiT) to reduce artefacts.

Result: Produces high-quality environment maps with fewer distortions, though PanoDiT trades some image quality for accuracy.

Conclusion: The approach competes with state-of-the-art methods in quality and lighting accuracy.

Abstract: We advance the field of HDR environment map estimation from a single-view
image by establishing a novel approach leveraging the Latent Diffusion Model
(LDM) to produce high-quality environment maps that can plausibly light
mirror-reflective surfaces. A common issue when using the ERP representation,
the format used by the vast majority of approaches, is distortions at the poles
and a seam at the sides of the environment map. We remove the border seam
artefact by proposing an ERP convolutional padding in the latent autoencoder.
Additionally, we investigate whether adapting the diffusion network
architecture to the ERP format can improve the quality and accuracy of the
estimated environment map by proposing a panoramically-adapted Diffusion
Transformer architecture. Our proposed PanoDiT network reduces ERP distortions
and artefacts, but at the cost of image quality and plausibility. We evaluate
with standard benchmarks to demonstrate that our models estimate high-quality
environment maps that perform competitively with state-of-the-art approaches in
both image quality and lighting accuracy.

</details>


### [67] [Fairness and Robustness of CLIP-Based Models for Chest X-rays](https://arxiv.org/abs/2507.21291)
*Théo Sourget,David Restrepo,Céline Hudelot,Enzo Ferrante,Stergios Christodoulidis,Maria Vakalopoulou*

Main category: cs.CV

TL;DR: The study evaluates fairness and robustness of six CLIP-based models in chest X-ray classification, revealing performance gaps by age and reliance on spurious correlations like chest drains.


<details>
  <summary>Details</summary>
Motivation: To assess fairness and robustness of CLIP-based models in medical tasks, particularly radiology, where these aspects are underexplored despite promising accuracy.

Method: Evaluation of six CLIP-based models on chest X-ray classification using MIMIC-CXR, NIH-CXR14, and NEATX datasets, assessing fairness across age, sex, race, and robustness to shortcut learning (e.g., chest drains).

Result: Performance gaps observed by age; equitable results for sex and race. Models showed lower performance without chest drains, indicating reliance on spurious correlations. Embeddings revealed limitations in detecting sensitive attributes via PCA.

Conclusion: CLIP-based models exhibit fairness and robustness challenges, particularly in age-related performance gaps and shortcut learning, highlighting the need for further refinement in medical applications.

Abstract: Motivated by the strong performance of CLIP-based models in natural
image-text domains, recent efforts have adapted these architectures to medical
tasks, particularly in radiology, where large paired datasets of images and
reports, such as chest X-rays, are available. While these models have shown
encouraging results in terms of accuracy and discriminative performance, their
fairness and robustness in the different clinical tasks remain largely
underexplored. In this study, we extensively evaluate six widely used
CLIP-based models on chest X-ray classification using three publicly available
datasets: MIMIC-CXR, NIH-CXR14, and NEATX. We assess the models fairness across
six conditions and patient subgroups based on age, sex, and race. Additionally,
we assess the robustness to shortcut learning by evaluating performance on
pneumothorax cases with and without chest drains. Our results indicate
performance gaps between patients of different ages, but more equitable results
for the other attributes. Moreover, all models exhibit lower performance on
images without chest drains, suggesting reliance on spurious correlations. We
further complement the performance analysis with a study of the embeddings
generated by the models. While the sensitive attributes could be classified
from the embeddings, we do not see such patterns using PCA, showing the
limitations of these visualisation techniques when assessing models. Our code
is available at https://github.com/TheoSourget/clip_cxr_fairness

</details>


### [68] [VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction](https://arxiv.org/abs/2507.21311)
*Martin de La Gorce,Charlie Hewitt,Tibor Takacs,Robert Gerdisch,Zafiirah Hosenie,Givi Meishvili,Marek Kowalski,Thomas J. Cashman,Antonio Criminisi*

Main category: cs.CV

TL;DR: A method for real-time 3D Gaussian reconstructions from a single 2D webcam feed, enhancing remote meetings with realistic and authentic 3D representations.


<details>
  <summary>Details</summary>
Motivation: Improving remote meetings by overcoming limitations of existing 3D representation methods, which rely on complex hardware or fixed appearances.

Method: Predicts 3D Gaussian reconstructions in real time from a 2D webcam feed, ensuring authenticity and stability with a novel stability loss.

Result: Achieves state-of-the-art accuracy in visual quality and stability, demonstrated in live 3D meetings using standard hardware.

Conclusion: Enables highly accessible, realistic, and authentic 3D videoconferencing without specialized equipment.

Abstract: Virtual 3D meetings offer the potential to enhance copresence, increase
engagement and thus improve effectiveness of remote meetings compared to
standard 2D video calls. However, representing people in 3D meetings remains a
challenge; existing solutions achieve high quality by using complex hardware,
making use of fixed appearance via enrolment, or by inverting a pre-trained
generative model. These approaches lead to constraints that are unwelcome and
ill-fitting for videoconferencing applications. We present the first method to
predict 3D Gaussian reconstructions in real time from a single 2D webcam feed,
where the 3D representation is not only live and realistic, but also authentic
to the input video. By conditioning the 3D representation on each video frame
independently, our reconstruction faithfully recreates the input video from the
captured viewpoint (a property we call authenticity), while generalizing
realistically to novel viewpoints. Additionally, we introduce a stability loss
to obtain reconstructions that are temporally stable on video sequences. We
show that our method delivers state-of-the-art accuracy in visual quality and
stability metrics compared to existing methods, and demonstrate our approach in
live one-to-one 3D meetings using only a standard 2D camera and display. This
demonstrates that our approach can allow anyone to communicate volumetrically,
via a method for 3D videoconferencing that is not only highly accessible, but
also realistic and authentic.

</details>


### [69] [GLCP: Global-to-Local Connectivity Preservation for Tubular Structure Segmentation](https://arxiv.org/abs/2507.21328)
*Feixiang Zhou,Zhuangzhi Gao,He Zhao,Jianyang Xie,Yanda Meng,Yitian Zhao,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.CV

TL;DR: A novel Global-to-Local Connectivity Preservation (GLCP) framework improves tubular structure segmentation by addressing both global and local structural challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods for tubular structure segmentation often neglect local discontinuity regions, leading to fragmented results.

Method: The GLCP framework includes an Interactive Multi-head Segmentation (IMS) module for joint learning of global segmentation, skeleton maps, and local discontinuity maps, alongside a Dual-Attention-based Refinement (DAR) module.

Result: GLCP outperforms state-of-the-art methods in accuracy and continuity on 2D and 3D datasets.

Conclusion: The proposed GLCP framework effectively addresses fragmentation in tubular structure segmentation by integrating global and local structural insights.

Abstract: Accurate segmentation of tubular structures, such as vascular networks, plays
a critical role in various medical domains. A remaining significant challenge
in this task is structural fragmentation, which can adversely impact downstream
applications. Existing methods primarily focus on designing various loss
functions to constrain global topological structures. However, they often
overlook local discontinuity regions, leading to suboptimal segmentation
results. To overcome this limitation, we propose a novel Global-to-Local
Connectivity Preservation (GLCP) framework that can simultaneously perceive
global and local structural characteristics of tubular networks. Specifically,
we propose an Interactive Multi-head Segmentation (IMS) module to jointly learn
global segmentation, skeleton maps, and local discontinuity maps, respectively.
This enables our model to explicitly target local discontinuity regions while
maintaining global topological integrity. In addition, we design a lightweight
Dual-Attention-based Refinement (DAR) module to further improve segmentation
quality by refining the resulting segmentation maps. Extensive experiments on
both 2D and 3D datasets demonstrate that our GLCP achieves superior accuracy
and continuity in tubular structure segmentation compared to several
state-of-the-art approaches. The source codes will be available at
https://github.com/FeixiangZhou/GLCP.

</details>


### [70] [Analyzing the Sensitivity of Vision Language Models in Visual Question Answering](https://arxiv.org/abs/2507.21335)
*Monika Shah,Sudarshan Balaji,Somdeb Sarkhel,Sanorita Dey,Deepak Venugopal*

Main category: cs.CV

TL;DR: The paper explores how Vision Language Models (VLMs) handle violations of Grice's conversational maxims, finding their performance declines with added modifiers.


<details>
  <summary>Details</summary>
Motivation: To assess if VLMs can process flouted conversational maxims like humans, revealing their limitations.

Method: Modifiers were added to VQA v2.0 questions; responses of GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Flash were analyzed.

Result: VLMs' performance consistently dropped with modifiers, highlighting their sensitivity to conversational violations.

Conclusion: The approach effectively uncovers VLM limitations, suggesting further research to improve their robustness.

Abstract: We can think of Visual Question Answering as a (multimodal) conversation
between a human and an AI system. Here, we explore the sensitivity of Vision
Language Models (VLMs) through the lens of cooperative principles of
conversation proposed by Grice. Specifically, even when Grice's maxims of
conversation are flouted, humans typically do not have much difficulty in
understanding the conversation even though it requires more cognitive effort.
Here, we study if VLMs are capable of handling violations to Grice's maxims in
a manner that is similar to humans. Specifically, we add modifiers to
human-crafted questions and analyze the response of VLMs to these modifiers. We
use three state-of-the-art VLMs in our study, namely, GPT-4o, Claude-3.5-Sonnet
and Gemini-1.5-Flash on questions from the VQA v2.0 dataset. Our initial
results seem to indicate that the performance of VLMs consistently diminish
with the addition of modifiers which indicates our approach as a promising
direction to understand the limitations of VLMs.

</details>


### [71] [Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging](https://arxiv.org/abs/2507.21349)
*Amirmohammad Shamaei,Alexander Stebner,Salome,Bosshart,Johanna Ospel,Gouri Ginde,Mariana Bento,Roberto Souza*

Main category: cs.CV

TL;DR: A deep-learning-based MRI reconstruction framework improves scan quality and reduces time by integrating prior scans, outperforming existing methods and enhancing downstream tasks like brain segmentation.


<details>
  <summary>Details</summary>
Motivation: Long MRI acquisition times increase costs and reduce patient comfort. Prior scans can improve reconstruction but require time-consuming registration.

Method: Proposes a framework with an initial reconstruction network, deep registration model, and transformer-based enhancement network, validated on 2,808 T1-weighted MRI scans.

Result: Outperforms existing methods (p < 0.05), improves brain segmentation accuracy, and reduces reconstruction time significantly.

Conclusion: The method is clinically viable, offering faster, higher-quality reconstructions and improved downstream task performance.

Abstract: Magnetic resonance imaging (MRI) is a crucial medical imaging modality.
However, long acquisition times remain a significant challenge, leading to
increased costs, and reduced patient comfort. Recent studies have shown the
potential of using deep learning models that incorporate information from prior
subject-specific MRI scans to improve reconstruction quality of present scans.
Integrating this prior information requires registration of the previous scan
to the current image reconstruction, which can be time-consuming. We propose a
novel deep-learning-based MRI reconstruction framework which consists of an
initial reconstruction network, a deep registration model, and a
transformer-based enhancement network. We validated our method on a
longitudinal dataset of T1-weighted MRI scans with 2,808 images from 18
subjects at four acceleration factors (R5, R10, R15, R20). Quantitative metrics
confirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon
signed-rank test). Furthermore, we analyzed the impact of our MRI
reconstruction method on the downstream task of brain segmentation and observed
improved accuracy and volumetric agreement with reference segmentations. Our
approach also achieved a substantial reduction in total reconstruction time
compared to methods that use traditional registration algorithms, making it
more suitable for real-time clinical applications. The code associated with
this work is publicly available at
https://github.com/amirshamaei/longitudinal-mri-deep-recon.

</details>


### [72] [Group Relative Augmentation for Data Efficient Action Detection](https://arxiv.org/abs/2507.21353)
*Deep Anil Patel,Iain Melvin,Zachary Izzo,Martin Renqiang Min*

Main category: cs.CV

TL;DR: Efficient adaptation of Video-Language Models (VLMs) for action detection using few examples, addressing overfitting and granularity mismatch with parameter-efficient tuning and feature augmentation.


<details>
  <summary>Details</summary>
Motivation: Challenges like overfitting and granularity mismatch in adapting VLMs for action detection with limited data.

Method: Combines LoRA for parameter-efficient tuning and learnable feature augmentation (FiLM) within a frozen VLM backbone, plus a group-weighted loss function.

Result: Achieves strong mAP on AVA and MOMA datasets, demonstrating data efficiency.

Conclusion: The proposed method effectively adapts VLMs for action detection with limited examples, outperforming in performance and efficiency.

Abstract: Adapting large Video-Language Models (VLMs) for action detection using only a
few examples poses challenges like overfitting and the granularity mismatch
between scene-level pre-training and required person-centric understanding. We
propose an efficient adaptation strategy combining parameter-efficient tuning
(LoRA) with a novel learnable internal feature augmentation. Applied within the
frozen VLM backbone using FiLM, these augmentations generate diverse feature
variations directly relevant to the task. Additionally, we introduce a
group-weighted loss function that dynamically modulates the training
contribution of each augmented sample based on its prediction divergence
relative to the group average. This promotes robust learning by prioritizing
informative yet reasonable augmentations. We demonstrate our method's
effectiveness on complex multi-label, multi-person action detection datasets
(AVA, MOMA), achieving strong mAP performance and showcasing significant data
efficiency for adapting VLMs from limited examples.

</details>


### [73] [Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy](https://arxiv.org/abs/2507.21358)
*Jicheng Yuan,Manh Nguyen Duc,Qian Liu,Manfred Hauswirth,Danh Le Phuoc*

Main category: cs.CV

TL;DR: CoP introduces a multi-task learning framework for BEV 3D object detection, using spatial occupancy as auxiliary info to improve perception by integrating object detection and occupancy prediction.


<details>
  <summary>Details</summary>
Motivation: Existing BEV methods neglect environmental contexts like roads, limiting comprehensive perception. CoP aims to bridge this gap by leveraging spatial occupancy.

Method: CoP uses LDO for dense occupancy ground truths, VHS for fine-grained feature sampling, and CFF for feature fusion between tasks.

Result: CoP achieves 49.5% mAP and 59.2% NDS on nuScenes, outperforming existing vision-based methods.

Conclusion: CoP enhances BEV representations by integrating environmental context, improving 3D object detection performance.

Abstract: Vision-based bird's-eye-view (BEV) 3D object detection has advanced
significantly in autonomous driving by offering cost-effectiveness and rich
contextual information. However, existing methods often construct BEV
representations by collapsing extracted object features, neglecting intrinsic
environmental contexts, such as roads and pavements. This hinders detectors
from comprehensively perceiving the characteristics of the physical world. To
alleviate this, we introduce a multi-task learning framework, Collaborative
Perceiver (CoP), that leverages spatial occupancy as auxiliary information to
mine consistent structural and conceptual similarities shared between 3D object
detection and occupancy prediction tasks, bridging gaps in spatial
representations and feature refinement. To this end, we first propose a
pipeline to generate dense occupancy ground truths incorporating local density
information (LDO) for reconstructing detailed environmental information. Next,
we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained
local features according to distinct object properties. Furthermore, we develop
a global-local collaborative feature fusion (CFF) module that seamlessly
integrates complementary knowledge between both tasks, thus composing more
robust BEV representations. Extensive experiments on the nuScenes benchmark
demonstrate that CoP outperforms existing vision-based frameworks, achieving
49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials are
available at this link https://github.com/jichengyuan/Collaborative-Perceiver.

</details>


### [74] [Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers](https://arxiv.org/abs/2507.21364)
*Lukman Jibril Aliyu,Umar Sani Muhammad,Bilqisu Ismail,Nasiru Muhammad,Almustapha A Wakili,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad,Mustapha Abdullahi*

Main category: cs.CV

TL;DR: The paper compares deep learning models for classifying African wildlife images, finding ViT-H/14 most accurate but computationally costly, while DenseNet-201 offers a balance for deployable solutions.


<details>
  <summary>Details</summary>
Motivation: Addressing the decline of African wildlife by leveraging deep learning for biodiversity monitoring and conservation.

Method: Comparative study of DenseNet-201, ResNet-152, EfficientNet-B4, and ViT-H/14 using transfer learning on a dataset of four species.

Result: ViT-H/14 achieved 99% accuracy but was computationally expensive; DenseNet-201 (67%) was integrated into a deployable tool.

Conclusion: The study provides insights into model selection and deployment for wildlife conservation, emphasizing practicality and resource efficiency.

Abstract: Wildlife populations in Africa face severe threats, with vertebrate numbers
declining by over 65% in the past five decades. In response, image
classification using deep learning has emerged as a promising tool for
biodiversity monitoring and conservation. This paper presents a comparative
study of deep learning models for automatically classifying African wildlife
images, focusing on transfer learning with frozen feature extractors. Using a
public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we
evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and
Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among
convolutional networks (67% accuracy), while ViT-H/14 achieved the highest
overall accuracy (99%), but with significantly higher computational cost,
raising deployment concerns. Our experiments highlight the trade-offs between
accuracy, resource requirements, and deployability. The best-performing CNN
(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time
field use, demonstrating the feasibility of deploying lightweight models in
conservation settings. This work contributes to African-grounded AI research by
offering practical insights into model selection, dataset preparation, and
responsible deployment of deep learning tools for wildlife conservation.

</details>


### [75] [Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation](https://arxiv.org/abs/2507.21367)
*I-Hsiang Chen,Hua-En Chang,Wei-Ting Chen,Jenq-Neng Hwang,Sy-Yen Kuo*

Main category: cs.CV

TL;DR: PDAF introduces a probabilistic diffusion framework to improve domain generalization in semantic segmentation by modeling latent domain priors and aligning features.


<details>
  <summary>Details</summary>
Motivation: Domain shifts in unseen environments degrade model performance, and existing methods often ignore intrinsic latent domain priors.

Method: PDAF uses a Latent Domain Prior (LDP) to capture shifts, integrating three modules: LPE, DCM, and DPE, to iteratively refine features.

Result: PDAF enhances generalization under complex target conditions, validated by experiments in diverse urban scenes.

Conclusion: PDAF effectively addresses domain shifts in semantic segmentation, improving model robustness in unseen environments.

Abstract: Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging
task, as domain shifts in unseen environments can severely compromise model
performance. While recent studies enhance feature alignment by projecting
features into the source domain, they often neglect intrinsic latent domain
priors, leading to suboptimal results. In this paper, we introduce PDAF, a
Probabilistic Diffusion Alignment Framework that enhances the generalization of
existing segmentation networks through probabilistic diffusion modeling. PDAF
introduces a Latent Domain Prior (LDP) to capture domain shifts and uses this
prior as a conditioning factor to align both source and unseen target domains.
To achieve this, PDAF integrates into a pre-trained segmentation model and
utilizes paired source and pseudo-target images to simulate latent domain
shifts, enabling LDP modeling. The framework comprises three modules: the
Latent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the
Domain Compensation Module (DCM) adjusts feature representations to mitigate
domain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion
process to estimate the LDP without requiring paired samples. This design
enables PDAF to iteratively model domain shifts, progressively refining feature
representations to enhance generalization under complex target conditions.
Extensive experiments validate the effectiveness of PDAF across diverse and
challenging urban scenes.

</details>


### [76] [Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View](https://arxiv.org/abs/2507.21371)
*Zitong Zhang,Suranjan Gautam,Rui Yu*

Main category: cs.CV

TL;DR: Top2Pano generates realistic 360° indoor panoramas from 2D top-down views using volumetric occupancy estimation and diffusion-based refinement, outperforming baselines in geometry and photorealism.


<details>
  <summary>Details</summary>
Motivation: The task is challenging due to missing 3D structure and the need for geometric consistency and photorealism in applications like VR, interior design, and robotics.

Method: Top2Pano estimates volumetric occupancy for 3D structure, uses volumetric rendering for coarse panoramas, and refines them with ControlNet-based diffusion.

Result: Top2Pano outperforms baselines, reconstructing geometry, occlusions, and spatial arrangements effectively, and generalizes well to schematic floorplans.

Conclusion: Top2Pano bridges top-down views with immersive indoor synthesis, showing strong potential for practical applications.

Abstract: Generating immersive 360{\deg} indoor panoramas from 2D top-down views has
applications in virtual reality, interior design, real estate, and robotics.
This task is challenging due to the lack of explicit 3D structure and the need
for geometric consistency and photorealism. We propose Top2Pano, an end-to-end
model for synthesizing realistic indoor panoramas from top-down views. Our
method estimates volumetric occupancy to infer 3D structures, then uses
volumetric rendering to generate coarse color and depth panoramas. These guide
a diffusion-based refinement stage using ControlNet, enhancing realism and
structural fidelity. Evaluations on two datasets show Top2Pano outperforms
baselines, effectively reconstructing geometry, occlusions, and spatial
arrangements. It also generalizes well, producing high-quality panoramas from
schematic floorplans. Our results highlight Top2Pano's potential in bridging
top-down views with immersive indoor synthesis.

</details>


### [77] [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/abs/2507.21391)
*Shijie Zhou,Ruiyi Zhang,Huaisheng Zhu,Branislav Kveton,Yufan Zhou,Jiuxiang Gu,Jian Chen,Changyou Chen*

Main category: cs.CV

TL;DR: LLaVA-Reward is a reward model for evaluating text-to-image generations using multimodal large language models (MLLMs), improving efficiency and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based approaches are time-consuming and difficult to train, requiring instruction-following data. LLaVA-Reward addresses this by leveraging hidden states of MLLMs and enhancing text-image interaction.

Method: LLaVA-Reward uses hidden states of MLLMs for text-image pairs and introduces a Skip-connection Cross Attention (SkipCA) module to improve visual-textual reasoning. It supports various preference data types for fine-tuning.

Result: LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and scaling in text-to-image generations.

Conclusion: LLaVA-Reward offers an efficient and effective solution for evaluating text-to-image generations, enhancing bidirectional interaction and supporting diverse preference data.

Abstract: We introduce LLaVA-Reward, an efficient reward model designed to
automatically evaluate text-to-image (T2I) generations across multiple
perspectives, leveraging pretrained multimodal large language models (MLLMs).
Existing MLLM-based approaches require instruction-following data for
supervised fine-tuning and evaluate generation quality on analyzing text
response, which is time-consuming and difficult to train. To address this
problem, we propose LLaVA-Reward, which directly utilizes the hidden states of
MLLMs given text-image pairs. To enhance the bidirectional interaction between
visual and textual representations in decoder-only MLLMs, we further propose
adding a Skip-connection Cross Attention (SkipCA) module. This design enhances
text-image correlation reasoning by connecting early-layer visual features with
later-layer hidden representations.In addition, LLaVA-Reward supports different
types of preference data for efficient fine-tuning, including paired preference
data and unpaired data. We train LLaVA-Reward on four evaluation perspectives:
text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical
results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based
methods in generating human-aligned scores for automatic evaluations and
inference-time scaling in text-to-image generations.

</details>


### [78] [ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs](https://arxiv.org/abs/2507.21420)
*Chaoyu Li,Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: ReGATE is an adaptive token pruning method for accelerating MLLM training, using a teacher-student framework to selectively process tokens, reducing computational cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of training multimodal large language models (MLLMs) due to token volume motivates the need for efficient training methods beyond token reduction or merging.

Method: ReGATE employs a teacher-student framework with a frozen reference LLM (teacher) guiding the MLLM (student) via adaptive token pruning based on combined reference losses and student difficulty scores.

Result: ReGATE achieves up to 2x faster training on VideoLLaMA2, matching peak accuracy with 35% tokens, and surpasses baseline on benchmarks with 41% fewer tokens.

Conclusion: ReGATE effectively reduces training costs for MLLMs while maintaining or improving performance, offering a scalable solution for multimodal model training.

Abstract: The computational cost of training multimodal large language models (MLLMs)
rapidly increases with the number of tokens involved. Existing efficiency
methods primarily target inference and rely on token reduction or merging,
offering limited benefit during training. In this paper, we propose ReGATE
(Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method
for accelerating MLLM training. Specifically, ReGATE adopts a teacher-student
framework in which the MLLM being trained serves as the student, and a frozen
reference large language model (LLM) acts as the teacher. The teacher computes
per-token reference losses, which are combined with an exponential moving
average (EMA) of the student's own difficulty scores. This adaptive
difficulty-based scoring enables the selective processing of crucial tokens
while bypassing less informative ones in the forward pass, significantly
reducing computational overhead. Experiments demonstrate that ReGATE, when
applied to VideoLLaMA2, matches the peak accuracy of standard training on
MVBench up to 2$\times$ faster, using only 35% of the tokens. With additional
training, it even surpasses the baseline on several multimodal benchmarks, all
while reducing the total token count by over 41%. Code and models will be
released soon.

</details>


### [79] [MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving](https://arxiv.org/abs/2507.21423)
*Thomas Monninger,Zihan Zhang,Zhipeng Mo,Md Zafar Anwar,Steffen Staab,Sihao Ding*

Main category: cs.CV

TL;DR: MapDiffusion introduces a generative approach using diffusion models to capture uncertainty in vectorized map construction for autonomous driving, outperforming baselines and improving robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional map construction models lack uncertainty capture, which is critical for handling real-world ambiguities like occlusions and missing lane markings.

Method: MapDiffusion uses diffusion models to iteratively refine randomly initialized queries, conditioned on a BEV latent grid, generating multiple plausible map samples.

Result: Achieves 5% better single-sample performance on nuScenes and shows improved accuracy with aggregated samples. Uncertainty estimates correlate with scene ambiguity.

Conclusion: MapDiffusion enhances robustness and reliability in HD map construction, enabling uncertainty-aware decision-making for autonomous vehicles.

Abstract: Autonomous driving requires an understanding of the static environment from
sensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse
multiple inputs, and a vector decoder predicts a vectorized map representation
from the latent BEV grid. However, traditional map construction models provide
deterministic point estimates, failing to capture uncertainty and the inherent
ambiguities of real-world environments, such as occlusions and missing lane
markings. We propose MapDiffusion, a novel generative approach that leverages
the diffusion paradigm to learn the full distribution of possible vectorized
maps. Instead of predicting a single deterministic output from learned queries,
MapDiffusion iteratively refines randomly initialized queries, conditioned on a
BEV latent grid, to generate multiple plausible map samples. This allows
aggregating samples to improve prediction accuracy and deriving uncertainty
estimates that directly correlate with scene ambiguity. Extensive experiments
on the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art
performance in online map construction, surpassing the baseline by 5% in
single-sample performance. We further show that aggregating multiple samples
consistently improves performance along the ROC curve, validating the benefit
of distribution modeling. Additionally, our uncertainty estimates are
significantly higher in occluded areas, reinforcing their value in identifying
regions with ambiguous sensor input. By modeling the full map distribution,
MapDiffusion enhances the robustness and reliability of online vectorized HD
map construction, enabling uncertainty-aware decision-making for autonomous
vehicles in complex environments.

</details>


### [80] [Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.21440)
*Han Wu,Chong Wang,Zhiming Cui*

Main category: cs.CV

TL;DR: DuCiSC introduces a dual cross-image semantic consistency framework for semi-supervised medical image segmentation, addressing feature discrepancy and improving region-level consistency.


<details>
  <summary>Details</summary>
Motivation: Current semi-supervised methods overlook semantic-level consistency and suffer from feature discrepancy due to imbalanced labeled and unlabeled data.

Method: DuCiSC enforces region-level consistency across labeled/unlabeled and labeled/fused images via prototype alignment and uses self-aware confidence estimation for reliable pseudo-label selection.

Result: DuCiSC outperforms state-of-the-art methods on four datasets, including binary and multi-class segmentation tasks.

Conclusion: The proposed framework effectively addresses feature discrepancy and enhances segmentation performance in semi-supervised medical image segmentation.

Abstract: Semi-supervised learning has proven highly effective in tackling the
challenge of limited labeled training data in medical image segmentation. In
general, current approaches, which rely on intra-image pixel-wise consistency
training via pseudo-labeling, overlook the consistency at more comprehensive
semantic levels (e.g., object region) and suffer from severe discrepancy of
extracted features resulting from an imbalanced number of labeled and unlabeled
data. To overcome these limitations, we present a new \underline{Du}al
\underline{C}ross-\underline{i}mage \underline{S}emantic
\underline{C}onsistency (DuCiSC) learning framework, for semi-supervised
medical image segmentation. Concretely, beyond enforcing pixel-wise semantic
consistency, DuCiSC proposes dual paradigms to encourage region-level semantic
consistency across: 1) labeled and unlabeled images; and 2) labeled and fused
images, by explicitly aligning their prototypes. Relying on the dual paradigms,
DuCiSC can effectively establish consistent cross-image semantics via prototype
representations, thereby addressing the feature discrepancy issue. Moreover, we
devise a novel self-aware confidence estimation strategy to accurately select
reliable pseudo labels, allowing for exploiting the training dynamics of
unlabeled data. Our DuCiSC method is extensively validated on four datasets,
including two popular binary benchmarks in segmenting the left atrium and
pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a
challenging scenario of segmenting the inferior alveolar nerve that features
complicated anatomical structures, showing superior segmentation results over
previous state-of-the-art approaches. Our code is publicly available at
\href{https://github.com/ShanghaiTech-IMPACT/DuCiSC}{https://github.com/ShanghaiTech-IMPACT/DuCiSC}.

</details>


### [81] [Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation](https://arxiv.org/abs/2507.21450)
*Bolei Chen,Jiaxu Kang,Yifei Wang,Ping Zhong,Qi Wu,Jianxin Wang*

Main category: cs.CV

TL;DR: The paper proposes a navigation policy for Vision Language Navigation (VLN) using Recursive Visual Imagination (RVI) and Adaptive Linguistic Grounding (ALG) to improve scene understanding and command alignment.


<details>
  <summary>Details</summary>
Motivation: Current VLN agents struggle with overly detailed scene representation and poor vision-language alignment, leading to navigation errors.

Method: Introduces RVI to summarize visual perceptions recursively and ALG to align situational memories with linguistic commands.

Result: The policy outperforms state-of-the-art methods on VLN-CE and ObjectNav tasks.

Conclusion: RVI and ALG enhance VLN by focusing on high-level scene priors and fine-grained semantic matching.

Abstract: Vision Language Navigation (VLN) typically requires agents to navigate to
specified objects or remote regions in unknown scenes by obeying linguistic
commands. Such tasks require organizing historical visual observations for
linguistic grounding, which is critical for long-sequence navigational
decisions. However, current agents suffer from overly detailed scene
representation and ambiguous vision-language alignment, which weaken their
comprehension of navigation-friendly high-level scene priors and easily lead to
behaviors that violate linguistic commands. To tackle these issues, we propose
a navigation policy by recursively summarizing along-the-way visual
perceptions, which are adaptively aligned with commands to enhance linguistic
grounding. In particular, by structurally modeling historical trajectories as
compact neural grids, several Recursive Visual Imagination (RVI) techniques are
proposed to motivate agents to focus on the regularity of visual transitions
and semantic scene layouts, instead of dealing with misleading geometric
details. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to
align the learned situational memories with different linguistic components
purposefully. Such fine-grained semantic matching facilitates the accurate
anticipation of navigation actions and progress. Our navigation policy
outperforms the state-of-the-art methods on the challenging VLN-CE and
ObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.

</details>


### [82] [Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation](https://arxiv.org/abs/2507.21455)
*Sheng-Feng Yu,Jia-Jiun Yao,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: The paper introduces Self-Supervised Dataset Distillation, a method to reduce dataset size while maintaining model performance by distilling images and their self-supervised representations. Novel techniques include low-dimensional parameterization, handling augmentation randomness, and lightweight network modeling.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of training large deep models with large datasets by creating a compact set of representative exemplars that retain performance.

Method: Proposes Self-Supervised Dataset Distillation with techniques like low-dimensional parameterization, predetermined augmentations, and lightweight network modeling for compact distillation.

Result: The method shows superior distillation efficiency, cross-architecture generalization, and transfer learning performance in experiments.

Conclusion: The approach effectively distills datasets while preserving key characteristics, enhancing generalizability and reducing computational costs.

Abstract: Although larger datasets are crucial for training large deep models, the
rapid growth of dataset size has brought a significant challenge in terms of
considerable training costs, which even results in prohibitive computational
expenses. Dataset Distillation becomes a popular technique recently to reduce
the dataset size via learning a highly compact set of representative exemplars,
where the model trained with these exemplars ideally should have comparable
performance with respect to the one trained with the full dataset. While most
of existing works upon dataset distillation focus on supervised datasets, we
instead aim to distill images and their self-supervisedly trained
representations into a distilled set. This procedure, named as Self-Supervised
Dataset Distillation, effectively extracts rich information from real datasets,
yielding the distilled sets with enhanced cross-architecture generalizability.
Particularly, in order to preserve the key characteristics of original dataset
more faithfully and compactly, several novel techniques are proposed: 1) we
introduce an innovative parameterization upon images and representations via
distinct low-dimensional bases, where the base selection for parameterization
is experimentally shown to play a crucial role; 2) we tackle the instability
induced by the randomness of data augmentation -- a key component in
self-supervised learning but being underestimated in the prior work of
self-supervised dataset distillation -- by utilizing predetermined
augmentations; 3) we further leverage a lightweight network to model the
connections among the representations of augmented views from the same image,
leading to more compact pairs of distillation. Extensive experiments conducted
on various datasets validate the superiority of our approach in terms of
distillation efficiency, cross-architecture generalization, and transfer
learning performance.

</details>


### [83] [An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes](https://arxiv.org/abs/2507.21460)
*Mianzhao Wang,Fan Shi,Xu Cheng,Feifei Zhang,Shengyong Chen*

Main category: cs.CV

TL;DR: The paper introduces a novel light field representation (ESI) and an angular-temporal interaction network (ATINet) for improved object tracking in low-light scenes, validated on a new dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with reliable angular modeling in temporal domains, especially in low-light scenes.

Method: Proposes ESI for geometric structure and ATINet for angular-temporal interaction learning, with self-supervised optimization.

Result: ATINet achieves state-of-the-art performance in single and multiple object tracking.

Conclusion: The approach enhances light field angular-temporal modeling, proving effective for tracking in challenging conditions.

Abstract: High-quality 4D light field representation with efficient angular feature
modeling is crucial for scene perception, as it can provide discriminative
spatial-angular cues to identify moving targets. However, recent developments
still struggle to deliver reliable angular modeling in the temporal domain,
particularly in complex low-light scenes. In this paper, we propose a novel
light field epipolar-plane structure image (ESI) representation that explicitly
defines the geometric structure within the light field. By capitalizing on the
abrupt changes in the angles of light rays within the epipolar plane, this
representation can enhance visual expression in low-light scenes and reduce
redundancy in high-dimensional light fields. We further propose an
angular-temporal interaction network (ATINet) for light field object tracking
that learns angular-aware representations from the geometric structural cues
and angular-temporal interaction cues of light fields. Furthermore, ATINet can
also be optimized in a self-supervised manner to enhance the geometric feature
interaction across the temporal domain. Finally, we introduce a large-scale
light field low-light dataset for object tracking. Extensive experimentation
demonstrates that ATINet achieves state-of-the-art performance in single object
tracking. Furthermore, we extend the proposed method to multiple object
tracking, which also shows the effectiveness of high-quality light field
angular-temporal modeling.

</details>


### [84] [Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval](https://arxiv.org/abs/2507.21489)
*Zhichuan Wang,Yang Zhou,Zhe Liu,Rui Yu,Song Bai,Yulong Wang,Xinwei He,Xiang Bai*

Main category: cs.CV

TL;DR: The paper introduces DAC, a framework for open-set 3D object retrieval using multi-view images, leveraging CLIP and MLLM for generalized representations, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing methods for open-set 3D object retrieval struggle with generalization due to limited 3D training data. The authors aim to leverage CLIP's pre-trained generalized representations and MLLM for improved performance.

Method: DAC combines CLIP with a multi-modal large language model (MLLM) for dual purposes: aligning seen category information during training and providing external hints during inference. AB-LoRA is introduced to enhance generalization.

Result: DAC outperforms prior methods by +10.01% mAP on four datasets and shows strong generalization in image-based and cross-dataset setups.

Conclusion: DAC is a simple yet effective framework for open-set 3DOR, demonstrating superior performance and generalization capabilities.

Abstract: Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D
objects of unseen categories beyond the training set. Existing methods
typically utilize all modalities (i.e., voxels, point clouds, multi-view
images) and train specific backbones before fusion. However, they still
struggle to produce generalized representations due to insufficient 3D training
data. Being contrastively pre-trained on web-scale image-text pairs, CLIP
inherently produces generalized representations for a wide range of downstream
tasks. Building upon it, we present a simple yet effective framework named
Describe, Adapt and Combine (DAC) by taking only multi-view images for open-set
3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large
language model (MLLM) to learn generalized 3D representations, where the MLLM
is used for dual purposes. First, it describes the seen category information to
align with CLIP's training objective for adaptation during training. Second, it
provides external hints about unknown objects complementary to visual cues
during inference. To improve the synergy, we introduce an Additive-Bias
Low-Rank adaptation (AB-LoRA), which alleviates overfitting and further
enhances the generalization to unseen categories. With only multi-view images,
DAC significantly surpasses prior arts by an average of +10.01\% mAP on four
open-set 3DOR datasets. Moreover, its generalization is also validated on
image-based and cross-dataset setups. Code is available at
https://github.com/wangzhichuan123/DAC.

</details>


### [85] [VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding](https://arxiv.org/abs/2507.21507)
*Shibo Gao,Peipei Yang,Yangyang Liu,Yi Chen,Han Zhu,Xuyao Zhang,Linlin Huang*

Main category: cs.CV

TL;DR: The paper introduces VAGU, a benchmark for Video Anomaly Detection (VAD) that integrates anomaly understanding and grounding, and proposes the GtS framework and JeAUG metric for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing VAD methods focus either on temporal localization or semantic understanding, but no model or dataset supports both tasks simultaneously.

Method: The authors introduce VAGU, a benchmark with annotations for anomaly category, explanation, temporal grounding, and Video QA. They propose the GtS framework for coarse localization and refinement, and the JeAUG metric for joint evaluation.

Result: Experiments show the effectiveness of VAGU, GtS, and JeAUG in addressing the limitations of current VAD approaches.

Conclusion: The paper presents a comprehensive solution for VAD by combining anomaly understanding and grounding, validated by the proposed benchmark, framework, and metric.

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events in videos and
accurately determine their time intervals. Current VAD methods mainly fall into
two categories: traditional DNN-based approaches that focus on temporal
localization, and LLM-based approaches that emphasize semantic understanding.
Both anomaly understanding and grounding are essential for comprehensive video
anomaly detection and can complement each other. However, no existing model or
dataset supports both tasks simultaneously. To address this, we introduce VAGU
(Video Anomaly Grounding and Understanding), the first benchmark to integrate
both tasks. Each VAGU instance includes annotations for anomaly category,
semantic explanation, precise temporal grounding and Video QA. We also provide
multiple-choice Video QA for objective evaluation. Based on this dataset, we
propose Glance then Scrutinize (GtS), a training-free framework guided by
textual prompts. The framework first enables coarse localization of
high-probability anomalous regions, followed by detailed anomaly interpretation
and temporal boundary refinement. Additionally, we propose the JeAUG metric,
which jointly evaluates semantic interpretability and temporal precision,
overcoming the limitations of traditional metrics. Extensive experiments verify
the effectiveness of our benchmark, framework, and evaluation metric.

</details>


### [86] [Optimizing Active Learning in Vision-Language Models via Parameter-Efficient Uncertainty Calibration](https://arxiv.org/abs/2507.21521)
*Athmanarayanan Lakshmi Narayanan,Amrutha Machireddy,Ranganath Krishnan*

Main category: cs.CV

TL;DR: A novel parameter-efficient AL method with uncertainty calibration loss improves sample selection for vision-language models, outperforming complex techniques while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To address challenges in uncertainty estimation and efficient sampling for large-scale vision-language models in AL.

Method: Introduces a differentiable loss function for uncertainty calibration within AL, comparing Prompt learning and LoRA for sample selection.

Result: Matches/exceeds performance of complex sampling techniques with computational efficiency.

Conclusion: The proposed method is effective for AL in vision-language models, with insights on Prompt learning vs. LoRA.

Abstract: Active Learning (AL) has emerged as a powerful approach for minimizing
labeling costs by selectively sampling the most informative data for neural
network model development. Effective AL for large-scale vision-language models
necessitates addressing challenges in uncertainty estimation and efficient
sampling given the vast number of parameters involved. In this work, we
introduce a novel parameter-efficient learning methodology that incorporates
uncertainty calibration loss within the AL framework. We propose a
differentiable loss function that promotes uncertainty calibration for
effectively selecting fewer and most informative data samples for fine-tuning.
Through extensive experiments across several datasets and vision backbones, we
demonstrate that our solution can match and exceed the performance of complex
feature-based sampling techniques while being computationally very efficient.
Additionally, we investigate the efficacy of Prompt learning versus Low-rank
adaptation (LoRA) in sample selection, providing a detailed comparative
analysis of these methods in the context of efficient AL.

</details>


### [87] [Chain-of-Cooking:Cooking Process Visualization via Bidirectional Chain-of-Thought Guidance](https://arxiv.org/abs/2507.21529)
*Mengling Xu,Ming Tao,Bing-Kun Bao*

Main category: cs.CV

TL;DR: The paper introduces Chain-of-Cooking, a model for visualizing cooking processes by generating images for each step, addressing semantic inconsistency and contextual coherence challenges.


<details>
  <summary>Details</summary>
Motivation: Existing works focus on finished foods, lacking methods to visualize intermediate cooking steps with correct ingredient appearances and sequential coherence.

Method: Proposes Dynamic Patch Selection Module for ingredient appearance, Semantic Evolution Module for semantic association, and Bidirectional Chain-of-Thought Guidance for coherence.

Result: Outperforms existing methods in generating coherent and semantically consistent cooking process images, validated on the CookViz dataset.

Conclusion: Chain-of-Cooking effectively addresses challenges in cooking process visualization, offering improved accuracy and coherence.

Abstract: Cooking process visualization is a promising task in the intersection of
image generation and food analysis, which aims to generate an image for each
cooking step of a recipe. However, most existing works focus on generating
images of finished foods based on the given recipes, and face two challenges to
visualize the cooking process. First, the appearance of ingredients changes
variously across cooking steps, it is difficult to generate the correct
appearances of foods that match the textual description, leading to semantic
inconsistency. Second, the current step might depend on the operations of
previous step, it is crucial to maintain the contextual coherence of images in
sequential order. In this work, we present a cooking process visualization
model, called Chain-of-Cooking. Specifically, to generate correct appearances
of ingredients, we present a Dynamic Patch Selection Module to retrieve
previously generated image patches as references, which are most related to
current textual contents. Furthermore, to enhance the coherence and keep the
rational order of generated images, we propose a Semantic Evolution Module and
a Bidirectional Chain-of-Thought (CoT) Guidance. To better utilize the
semantics of previous texts, the Semantic Evolution Module establishes the
semantical association between latent prompts and current cooking step, and
merges it with the latent features. Then the CoT Guidance updates the merged
features to guide the current cooking step remain coherent with the previous
step. Moreover, we construct a dataset named CookViz, consisting of
intermediate image-text pairs for the cooking process. Quantitative and
qualitative experiments show that our method outperforms existing methods in
generating coherent and semantic consistent cooking process.

</details>


### [88] [Suppressing Gradient Conflict for Generalizable Deepfake Detection](https://arxiv.org/abs/2507.21530)
*Ming-Hui Liu,Harry Cheng,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TL;DR: The paper addresses the challenge of improving deepfake detection by mitigating gradient conflicts when training on both original and synthesized fake images. It proposes a Conflict-Suppressed Deepfake Detection (CS-DFD) framework with two modules to resolve these conflicts, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection models degrade when trained on both original and synthesized fake images due to gradient conflicts, contradicting the expectation that more data should improve accuracy.

Method: The CS-DFD framework includes: 1) Update Vector Search (UVS) to find a gradient update vector that balances loss reductions for both data types, and 2) Conflict Gradient Reduction (CGR) to enforce a low-conflict feature space via a novel Conflict Descent Loss.

Result: CS-DFD achieves state-of-the-art performance in in-domain detection accuracy and cross-domain generalization on multiple benchmarks.

Conclusion: The proposed CS-DFD framework effectively mitigates gradient conflicts, enhancing deepfake detection performance and generalization.

Abstract: Robust deepfake detection models must be capable of generalizing to
ever-evolving manipulation techniques beyond training data. A promising
strategy is to augment the training data with online synthesized fake images
containing broadly generalizable artifacts. However, in the context of deepfake
detection, it is surprising that jointly training on both original and online
synthesized forgeries may result in degraded performance. This contradicts the
common belief that incorporating more source-domain data should enhance
detection accuracy. Through empirical analysis, we trace this degradation to
gradient conflicts during backpropagation which force a trade-off between
source domain accuracy and target domain generalization. To overcome this
issue, we propose a Conflict-Suppressed Deepfake Detection (CS-DFD) framework
that explicitly mitigates the gradient conflict via two synergistic modules.
First, an Update Vector Search (UVS) module searches for an alternative update
vector near the initial gradient vector to reconcile the disparities of the
original and online synthesized forgeries. By further transforming the search
process into an extremum optimization problem, UVS yields the uniquely update
vector, which maximizes the simultaneous loss reductions for each data type.
Second, a Conflict Gradient Reduction (CGR) module enforces a low-conflict
feature embedding space through a novel Conflict Descent Loss. This loss
penalizes misaligned gradient directions and guides the learning of
representations with aligned, non-conflicting gradients. The synergy of UVS and
CGR alleviates gradient interference in both parameter optimization and
representation learning. Experiments on multiple deepfake benchmarks
demonstrate that CS-DFD achieves state-of-the-art performance in both in-domain
detection accuracy and cross-domain generalization.

</details>


### [89] [Sun sensor calibration algorithms: A systematic mapping and survey](https://arxiv.org/abs/2507.21541)
*Michael Herman,Olivia J. Pinon Fischer,Dimitri N. Mavris*

Main category: cs.CV

TL;DR: The paper reviews sun sensor modeling and calibration algorithms, addressing uncertainties and research gaps in spacecraft attitude determination.


<details>
  <summary>Details</summary>
Motivation: Sun sensors are critical for spacecraft attitude determination but face complex uncertainties, motivating advanced calibration techniques.

Method: The paper systematically maps and surveys existing sun sensor modeling and calibration methodologies.

Result: It identifies research gaps and provides recommendations for future improvements in sun sensor techniques.

Conclusion: The review consolidates and analyzes past work, offering a foundation for advancing sun sensor calibration and modeling.

Abstract: Attitude sensors determine the spacecraft attitude through the sensing of an
astronomical object, field or other phenomena. The Sun and fixed stars are the
two primary astronomical sensing objects. Attitude sensors are critical
components for the survival and knowledge improvement of spacecraft. Of these,
sun sensors are the most common and important sensor for spacecraft attitude
determination. The sun sensor measures the Sun vector in spacecraft
coordinates. The sun sensor calibration process is particularly difficult due
to the complex nature of the uncertainties involved. The uncertainties are
small, difficult to observe, and vary spatio-temporally over the lifecycle of
the sensor. In addition, the sensors are affected by numerous sources of
uncertainties, including manufacturing, electrical, environmental, and
interference sources. This motivates the development of advanced calibration
algorithms to minimize uncertainty over the sensor lifecycle and improve
accuracy. Although modeling and calibration techniques for sun sensors have
been explored extensively in the literature over the past two decades, there is
currently no resource that consolidates and systematically reviews this body of
work. The present review proposes a systematic mapping of sun sensor modeling
and calibration algorithms across a breadth of sensor configurations. It
specifically provides a comprehensive survey of each methodology, along with an
analysis of research gaps and recommendations for future directions in sun
sensor modeling and calibration techniques.

</details>


### [90] [Multi-View Reconstruction with Global Context for 3D Anomaly Detection](https://arxiv.org/abs/2507.21555)
*Yihan Sun,Yuqi Cheng,Yunkang Cao,Yuxin Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: Proposes Multi-View Reconstruction (MVR) for 3D anomaly detection, improving global information learning by converting point clouds to multi-view images, achieving high AU-ROC scores.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack sufficient global information for high-precision 3D anomaly detection in industrial quality inspection.

Method: MVR converts high-resolution point clouds into multi-view images and uses a reconstruction-based anomaly detection framework.

Result: Achieves 89.6% object-wise AU-ROC and 95.7% point-wise AU-ROC on the Real3D-AD benchmark.

Conclusion: MVR effectively enhances global information learning for 3D anomaly detection, outperforming existing methods.

Abstract: 3D anomaly detection is critical in industrial quality inspection. While
existing methods achieve notable progress, their performance degrades in
high-precision 3D anomaly detection due to insufficient global information. To
address this, we propose Multi-View Reconstruction (MVR), a method that
losslessly converts high-resolution point clouds into multi-view images and
employs a reconstruction-based anomaly detection framework to enhance global
information learning. Extensive experiments demonstrate the effectiveness of
MVR, achieving 89.6\% object-wise AU-ROC and 95.7\% point-wise AU-ROC on the
Real3D-AD benchmark.

</details>


### [91] [RelMap: Enhancing Online Map Construction with Class-Aware Spatial Relation and Semantic Priors](https://arxiv.org/abs/2507.21567)
*Tianhui Cai,Yun Zhang,Zewei Zhou,Zhiyu Huang,Jiaqi Ma*

Main category: cs.CV

TL;DR: RelMap improves online HD map construction by incorporating spatial relations and semantic priors, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based methods for online HD map construction often ignore spatial and semantic relationships among map elements, limiting accuracy and generalization.

Method: RelMap introduces a Class-aware Spatial Relation Prior for positional dependencies and a Mixture-of-Experts-based Semantic Prior for refining feature decoding. It works with single-frame and temporal backbones.

Result: State-of-the-art performance on nuScenes and Argoverse 2 datasets.

Conclusion: RelMap effectively addresses limitations of existing methods by leveraging spatial and semantic relationships, enhancing map construction accuracy.

Abstract: Online high-definition (HD) map construction plays an increasingly important
role in scaling autonomous driving systems. Transformer-based methods have
become prevalent in online HD map construction; however, existing approaches
often neglect the inherent spatial and semantic relationships among map
elements, which limits their accuracy and generalization. To address this, we
propose RelMap, an end-to-end framework that enhances online map construction
by incorporating spatial relations and semantic priors. We introduce a
Class-aware Spatial Relation Prior, which explicitly encodes relative
positional dependencies between map elements using a learnable class-aware
relation encoder. Additionally, we propose a Mixture-of-Experts (MoE)-based
Semantic Prior, which routes features to class-specific experts based on
predicted class probabilities, refining instance feature decoding. Our method
is compatible with both single-frame and temporal perception backbones,
achieving state-of-the-art performance on both the nuScenes and Argoverse 2
datasets.

</details>


### [92] [LinDeps: A Fine-tuning Free Post-Pruning Method to Remove Layer-Wise Linear Dependencies with Guaranteed Performance Preservation](https://arxiv.org/abs/2507.21573)
*Maxim Henry,Adrien Deliège,Anthony Cioppa,Marc Van Droogenbroeck*

Main category: cs.CV

TL;DR: LinDeps is a novel post-pruning method that uses linear dependency analysis to remove redundant filters in CNNs, improving compression rates without performance loss.


<details>
  <summary>Details</summary>
Motivation: The increasing size and complexity of CNNs challenge deployment on resource-constrained platforms, and existing pruning techniques often overlook structural dependencies, leading to suboptimal results.

Method: LinDeps applies pivoted QR decomposition to feature maps to detect and prune linearly dependent filters, followed by a signal recovery mechanism to adjust kernels without fine-tuning.

Result: Experiments on CIFAR-10 and ImageNet with VGG and ResNet show LinDeps improves compression rates and preserves performance, achieving state-of-the-art results.

Conclusion: LinDeps is a versatile add-on for pruning techniques, especially effective in low-resource setups without retraining.

Abstract: Convolutional Neural Networks (CNN) are widely used in many computer vision
tasks. Yet, their increasing size and complexity pose significant challenges
for efficient deployment on resource-constrained platforms. Hence, network
pruning has emerged as an effective way of reducing the size and computational
requirements of neural networks by removing redundant or unimportant
parameters. However, a fundamental challenge with pruning consists in optimally
removing redundancies without degrading performance. Most existing pruning
techniques overlook structural dependencies across feature maps within a layer,
resulting in suboptimal pruning decisions. In this work, we introduce LinDeps,
a novel post-pruning method, i.e., a pruning method that can be applied on top
of any pruning technique, which systematically identifies and removes redundant
filters via linear dependency analysis. Particularly, LinDeps applies pivoted
QR decomposition to feature maps to detect and prune linearly dependent
filters. Then, a novel signal recovery mechanism adjusts the next layer's
kernels to preserve compatibility and performance without requiring any
fine-tuning. Our experiments on CIFAR-10 and ImageNet with VGG and ResNet
backbones demonstrate that LinDeps improves compression rates of existing
pruning techniques while preserving performances, leading to a new state of the
art in CNN pruning. We also benchmark LinDeps in low-resource setups where no
retraining can be performed, which shows significant pruning improvements and
inference speedups over a state-of-the-art method. LinDeps therefore
constitutes an essential add-on for any current or future pruning technique.

</details>


### [93] [TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs](https://arxiv.org/abs/2507.21584)
*Kejia Zhang,Keda Tao,Zhiming Luo,Chang Liu,Jiasheng Tang,Huan Wang*

Main category: cs.CV

TL;DR: TARS, a token-adaptive preference strategy, improves multimodal reasoning by reducing hallucinations through min-max optimization, outperforming standard DPO and matching GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing DPO strategies overfit to linguistic cues, impairing visual grounding. TARS aims to mitigate this by dynamically adapting preferences.

Method: TARS reformulates DPO as a min-max problem, maximizing token-level shifts under semantic constraints to simulate uncertainty and minimize preference loss.

Result: TARS reduces hallucination rates from 26.4% to 13.2% and cognition value from 2.5 to 0.4, outperforming standard DPO and matching GPT-4o.

Conclusion: TARS effectively preserves causal grounding and reduces hallucinations, demonstrating strong performance with minimal data.

Abstract: Multimodal large language models (MLLMs) enable vision-language reasoning,
yet often generate plausible outputs that are factually incorrect or visually
ungrounded, thereby compromising their reliability. Direct preference
optimization (DPO) is a common strategy for correcting hallucinations by
aligning model outputs with human preferences. Existing DPO strategies
typically treat hallucination-related preferences as fixed targets, relying on
static supervision signals during training. This approach tends to overfit to
superficial linguistic cues in preference data, leading to distributional
rigidity and spurious correlations that impair grounding in causally relevant
visual information. To overcome this limitation, we propose TARS, a
token-adaptive preference strategy that reformulates DPO as a min-max
optimization problem. TARS maximizes token-level distributional shifts under
semantic constraints to simulate alignment uncertainty, and simultaneously
minimizes the expected preference loss under these controlled perturbations.
This joint objective preserves causal grounding while mitigating overfitting to
preference patterns, thereby reducing hallucinations in multimodal reasoning.
We evaluate TARS on multiple hallucination benchmarks and find consistently
strong performance. Using only 4.8k preference samples and no expert feedback,
TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition
value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on
several key metrics.

</details>


### [94] [Emerging Trends in Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation with Image-Level Supervision](https://arxiv.org/abs/2507.21587)
*Zheyuan Zhang,Wang Zhang*

Main category: cs.CV

TL;DR: This review focuses on weakly supervised semantic segmentation (WSSS) with image-level annotations, summarizing recent advancements, categorizing methods, and discussing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid development of WSSS methods and the lack of updated surveys necessitate a comprehensive review to capture recent trends and advancements.

Method: The review synthesizes state-of-the-art techniques in WSSS with image-level labels, categorizing methods by additional supervision types and levels, and examines domain-specific challenges.

Result: The paper provides an updated overview of WSSS advancements, highlights underexplored topics like domain-specific applications, and evaluates current limitations.

Conclusion: The review aims to deepen understanding for researchers familiar with WSSS, outlining challenges and promising future research directions.

Abstract: Unlike fully supervised semantic segmentation, weakly supervised semantic
segmentation (WSSS) relies on weaker forms of supervision to perform dense
prediction tasks. Among the various types of weak supervision, WSSS with image
level annotations is considered both the most challenging and the most
practical, attracting significant research attention. Therefore, in this
review, we focus on WSSS with image level annotations. Additionally, this
review concentrates on mainstream research directions, deliberately omitting
less influential branches.
  Given the rapid development of new methods and the limitations of existing
surveys in capturing recent trends, there is a pressing need for an updated and
comprehensive review. Our goal is to fill this gap by synthesizing the latest
advancements and state-of-the-art techniques in WSSS with image level labels.
  Basically, we provide a comprehensive review of recent advancements in WSSS
with image level labels, categorizing existing methods based on the types and
levels of additional supervision involved. We also examine the challenges of
applying advanced methods to domain specific datasets in WSSS,a topic that
remains underexplored. Finally, we discuss the current challenges, evaluate the
limitations of existing approaches, and outline several promising directions
for future research. This review is intended for researchers who are already
familiar with the fundamental concepts of WSSS and are seeking to deepen their
understanding of current advances and methodological innovations.

</details>


### [95] [Locally Controlled Face Aging with Latent Diffusion Models](https://arxiv.org/abs/2507.21600)
*Lais Isabelle Alves dos Santos,Julien Despois,Thibaut Chauffier,Sileye O. Ba,Giovanni Palma*

Main category: cs.CV

TL;DR: A novel face aging method using latent diffusion models to selectively age facial regions, addressing heterogeneity in aging for more realistic results.


<details>
  <summary>Details</summary>
Motivation: Current methods treat aging as homogeneous, ignoring regional differences due to intrinsic and extrinsic factors.

Method: Leverages latent diffusion models to age specific facial regions locally, with a refiner for seamless blending.

Result: Achieves robust identity preservation, high-fidelity imagery, and natural aging progression.

Conclusion: The method provides finer-grained control and more realistic personalized aging.

Abstract: We present a novel approach to face aging that addresses the limitations of
current methods which treat aging as a global, homogeneous process. Existing
techniques using GANs and diffusion models often condition generation on a
reference image and target age, neglecting that facial regions age
heterogeneously due to both intrinsic chronological factors and extrinsic
elements like sun exposure. Our method leverages latent diffusion models to
selectively age specific facial regions using local aging signs. This approach
provides significantly finer-grained control over the generation process,
enabling more realistic and personalized aging. We employ a latent diffusion
refiner to seamlessly blend these locally aged regions, ensuring a globally
consistent and natural-looking synthesis. Experimental results demonstrate that
our method effectively achieves three key criteria for successful face aging:
robust identity preservation, high-fidelity and realistic imagery, and a
natural, controllable aging progression.

</details>


### [96] [Decoupled Spatio-Temporal Consistency Learning for Self-Supervised Tracking](https://arxiv.org/abs/2507.21606)
*Yaozong Zheng,Bineng Zhong,Qihua Liang,Ning Li,Shuxiang Song*

Main category: cs.CV

TL;DR: A self-supervised tracking framework (SSTrack) eliminates the need for manual box annotations by using spatio-temporal consistency and instance contrastive loss, outperforming SOTA methods by significant margins.


<details>
  <summary>Details</summary>
Motivation: Manual box annotations are labor-intensive and limit dataset scale/diversity. SSTrack aims to reduce reliance on such annotations.

Method: Proposes a decoupled spatio-temporal consistency framework for global spatial localization and local temporal association, along with an instance contrastive loss for robust instance supervision.

Result: SSTrack improves AUC scores by 25.3%, 20.4%, and 14.8% on GOT10K, LaSOT, and TrackingNet datasets, respectively.

Conclusion: SSTrack effectively learns tracking representations self-supervised, reducing annotation dependency and outperforming existing methods.

Abstract: The success of visual tracking has been largely driven by datasets with
manual box annotations. However, these box annotations require tremendous human
effort, limiting the scale and diversity of existing tracking datasets. In this
work, we present a novel Self-Supervised Tracking framework named
\textbf{{\tracker}}, designed to eliminate the need of box annotations.
Specifically, a decoupled spatio-temporal consistency training framework is
proposed to learn rich target information across timestamps through global
spatial localization and local temporal association. This allows for the
simulation of appearance and motion variations of instances in real-world
scenarios. Furthermore, an instance contrastive loss is designed to learn
instance-level correspondences from a multi-view perspective, offering robust
instance supervision without additional labels. This new design paradigm
enables {\tracker} to effectively learn generic tracking representations in a
self-supervised manner, while reducing reliance on extensive box annotations.
Extensive experiments on nine benchmark datasets demonstrate that {\tracker}
surpasses \textit{SOTA} self-supervised tracking methods, achieving an
improvement of more than 25.3\%, 20.4\%, and 14.8\% in AUC (AO) score on the
GOT10K, LaSOT, TrackingNet datasets, respectively. Code:
https://github.com/GXNU-ZhongLab/SSTrack.

</details>


### [97] [Semantic Segmentation of iPS Cells: Case Study on Model Complexity in Biomedical Imaging](https://arxiv.org/abs/2507.21608)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: A DeepLabv3 model outperforms larger models like SAM2 and MedSAM2 in segmenting iPS cell colonies, showing simpler models can excel in specialized tasks with subtle boundaries.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that larger, generalized models always perform better in medical image segmentation, especially for tasks with low-contrast boundaries.

Method: Configured a DeepLabv3 model for segmenting iPS cell colonies and compared its performance against SAM2 and MedSAM2 without structural changes.

Result: DeepLabv3 achieved higher performance than SAM2 and MedSAM2, demonstrating that simpler models can be more effective for specialized tasks.

Conclusion: Simpler, domain-specific models like DeepLabv3 can outperform larger models in biomedical applications, offering accuracy and reliability.

Abstract: Medical image segmentation requires not only accuracy but also robustness
under challenging imaging conditions. In this study, we show that a carefully
configured DeepLabv3 model can achieve high performance in segmenting induced
pluripotent stem (iPS) cell colonies, and, under our experimental conditions,
outperforms large-scale foundation models such as SAM2 and its medical variant
MedSAM2 without structural modifications. These results suggest that, for
specialized tasks characterized by subtle, low-contrast boundaries, increased
model complexity does not necessarily translate to better performance. Our work
revisits the assumption that ever-larger and more generalized architectures are
always preferable, and provides evidence that appropriately adapted, simpler
models may offer strong accuracy and practical reliability in domain-specific
biomedical applications. We also offer an open-source implementation that
includes strategies for small datasets and domain-specific encoding, with the
aim of supporting further advances in semantic segmentation for regenerative
medicine and related fields.

</details>


### [98] [Wind Turbine Feature Detection Using Deep Learning and Synthetic Data](https://arxiv.org/abs/2507.21611)
*Arash Shahirpour,Jakob Gebler,Manuel Sanders,Tim Reuscher*

Main category: cs.CV

TL;DR: Proposes synthetic data generation for training drone-based wind turbine inspection systems, using YOLOv11 with modified loss for feature detection, achieving high accuracy on real-world images.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of manually labeled real-world datasets for wind turbine inspection by enhancing diversity and complexity through synthetic data.

Method: Generates synthetic training data with controlled variations and trains YOLOv11 with a modified loss function for feature detection.

Result: Achieves a Pose mAP50-95 of 0.97 on unseen real-world images, demonstrating robust performance.

Conclusion: Synthetic data generation and modified YOLOv11 training effectively improve wind turbine feature detection for autonomous drone inspection.

Abstract: For the autonomous drone-based inspection of wind turbine (WT) blades,
accurate detection of the WT and its key features is essential for safe drone
positioning and collision avoidance. Existing deep learning methods typically
rely on manually labeled real-world images, which limits both the quantity and
the diversity of training datasets in terms of weather conditions, lighting,
turbine types, and image complexity. In this paper, we propose a method to
generate synthetic training data that allows controlled variation of visual and
environmental factors, increasing the diversity and hence creating challenging
learning scenarios. Furthermore, we train a YOLOv11 feature detection network
solely on synthetic WT images with a modified loss function, to detect WTs and
their key features within an image. The resulting network is evaluated both
using synthetic images and a set of real-world WT images and shows promising
performance across both synthetic and real-world data, achieving a Pose
mAP50-95 of 0.97 on real images never seen during training.

</details>


### [99] [EMIT: Enhancing MLLMs for Industrial Anomaly Detection via Difficulty-Aware GRPO](https://arxiv.org/abs/2507.21619)
*Wei Guan,Jun Lan,Jian Cao,Hao Tan,Huijia Zhu,Weiqiang Wang*

Main category: cs.CV

TL;DR: EMIT enhances MLLMs for industrial anomaly detection (IAD) using difficulty-aware GRPO, achieving a 7.77% performance boost.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack domain-specific adaptation for IAD, limiting their effectiveness.

Method: EMIT uses GRPO with response resampling and advantage reweighting, integrates soft prompts, and leverages GPT-generated descriptions for missing defective images.

Result: EMIT improves MLLMs' IAD performance by 7.77% on the MMAD benchmark.

Conclusion: EMIT effectively adapts MLLMs for IAD, demonstrating significant performance gains.

Abstract: Industrial anomaly detection (IAD) plays a crucial role in maintaining the
safety and reliability of manufacturing systems. While multimodal large
language models (MLLMs) show strong vision-language reasoning abilities, their
effectiveness in IAD remains limited without domain-specific adaptation. In
this work, we propose EMIT, a unified framework that enhances MLLMs for IAD via
difficulty-aware group relative policy optimization (GRPO). EMIT constructs a
multi-task IAD dataset and utilizes GPT-generated object text descriptions to
compensate for missing defective images. For few-shot anomaly detection, it
integrates a soft prompt and heatmap-guided contrastive embeddings derived from
patch-level comparisons. To better handle difficult data samples, i.e., cases
where the MLLM struggles to generate correct answers, we propose a
difficulty-aware GRPO that extends the original GRPO by incorporating a
response resampling strategy to ensure the inclusion of correct answers in the
sampled responses, as well as an advantage reweighting mechanism to strengthen
learning from such difficult data samples. Extensive experiments on the MMAD
benchmark demonstrate that EMIT significantly enhances the IAD performance of
MLLMs, achieving an average improvement of 7.77\% over the base model
(InternVL3-8B) across seven tasks.

</details>


### [100] [GuidPaint: Class-Guided Image Inpainting with Diffusion Models](https://arxiv.org/abs/2507.21627)
*Qimin Wang,Xinda Liu,Guohua Geng*

Main category: cs.CV

TL;DR: GuidPaint is a training-free, class-guided diffusion model for image inpainting, improving semantic consistency and visual realism without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based inpainting methods lack fine-grained control over masked regions, leading to inconsistent or implausible results.

Method: GuidPaint incorporates classifier guidance into denoising, enabling precise control and integrates stochastic/deterministic sampling for refinement.

Result: GuidPaint outperforms existing context-aware methods in qualitative and quantitative evaluations.

Conclusion: GuidPaint offers a computationally efficient and high-quality solution for image inpainting with better control and realism.

Abstract: In recent years, diffusion models have been widely adopted for image
inpainting tasks due to their powerful generative capabilities, achieving
impressive results. Existing multimodal inpainting methods based on diffusion
models often require architectural modifications and retraining, resulting in
high computational cost. In contrast, context-aware diffusion inpainting
methods leverage the model's inherent priors to adjust intermediate denoising
steps, enabling high-quality inpainting without additional training and
significantly reducing computation. However, these methods lack fine-grained
control over the masked regions, often leading to semantically inconsistent or
visually implausible content. To address this issue, we propose GuidPaint, a
training-free, class-guided image inpainting framework. By incorporating
classifier guidance into the denoising process, GuidPaint enables precise
control over intermediate generations within the masked areas, ensuring both
semantic consistency and visual realism. Furthermore, it integrates stochastic
and deterministic sampling, allowing users to select preferred intermediate
results and deterministically refine them. Experimental results demonstrate
that GuidPaint achieves clear improvements over existing context-aware
inpainting methods in both qualitative and quantitative evaluations.

</details>


### [101] [The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM](https://arxiv.org/abs/2507.21649)
*Shibo Gao,Peipei Yang,Haiyang Guo,Yangyang Liu,Yi Chen,Shuai Li,Han Zhu,Jian Xu,Xu-Yao Zhang,Linlin Huang*

Main category: cs.CV

TL;DR: A survey on video anomaly detection (VAD) focusing on advancements driven by multi-modal large language models (MLLMs) and large language models (LLMs), proposing a unified framework and future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid development of MLLMs and LLMs has transformed VAD, necessitating a systematic review of recent advancements and their impact on the field.

Method: The paper provides a comprehensive survey, analyzing VAD methods based on MLLMs and LLMs, proposing a unified framework, and comparing strengths and weaknesses.

Result: The survey highlights significant transformations in VAD due to MLLMs and LLMs, offering a classification system and identifying new paradigms.

Conclusion: Key challenges and future research directions are outlined to guide the VAD community in leveraging MLLMs and LLMs effectively.

Abstract: Video anomaly detection (VAD) aims to identify and ground anomalous behaviors
or events in videos, serving as a core technology in the fields of intelligent
surveillance and public safety. With the advancement of deep learning, the
continuous evolution of deep model architectures has driven innovation in VAD
methodologies, significantly enhancing feature representation and scene
adaptability, thereby improving algorithm generalization and expanding
application boundaries. More importantly, the rapid development of multi-modal
large language (MLLMs) and large language models (LLMs) has introduced new
opportunities and challenges to the VAD field. Under the support of MLLMs and
LLMs, VAD has undergone significant transformations in terms of data
annotation, input modalities, model architectures, and task objectives. The
surge in publications and the evolution of tasks have created an urgent need
for systematic reviews of recent advancements. This paper presents the first
comprehensive survey analyzing VAD methods based on MLLMs and LLMs, providing
an in-depth discussion of the changes occurring in the VAD field in the era of
large models and their underlying causes. Additionally, this paper proposes a
unified framework that encompasses both deep neural network (DNN)-based and
LLM-based VAD methods, offering a thorough analysis of the new VAD paradigms
empowered by LLMs, constructing a classification system, and comparing their
strengths and weaknesses. Building on this foundation, this paper focuses on
current VAD methods based on MLLMs/LLMs. Finally, based on the trajectory of
technological advancements and existing bottlenecks, this paper distills key
challenges and outlines future research directions, offering guidance for the
VAD community.

</details>


### [102] [Automated Detection of Antarctic Benthic Organisms in High-Resolution In Situ Imagery to Aid Biodiversity Monitoring](https://arxiv.org/abs/2507.21665)
*Cameron Trotter,Huw Griffiths,Tasnuva Ming Khan,Rowan Whittle*

Main category: cs.CV

TL;DR: A computer vision framework for Antarctic benthic biodiversity monitoring using high-resolution imagery, addressing challenges like limited data and complex seafloor structures, with strong performance for medium/large organisms but limitations for small/rare taxa.


<details>
  <summary>Details</summary>
Motivation: Monitoring Antarctic benthic biodiversity is crucial for understanding climate-driven ecological changes, but manual annotation of high-resolution imagery is laborious and specialized.

Method: The framework combines resolution-preserving patching, spatial data augmentation, fine-tuning, and postprocessing via Slicing Aided Hyper Inference, benchmarking multiple object detection architectures.

Result: Strong performance in detecting 25 fine-grained morphotypes of medium/large organisms, though small/rare taxa detection remains challenging.

Conclusion: The framework offers a scalable foundation for machine-assisted benthic biodiversity monitoring, despite current limitations in detecting small/rare species.

Abstract: Monitoring benthic biodiversity in Antarctica is vital for understanding
ecological change in response to climate-driven pressures. This work is
typically performed using high-resolution imagery captured in situ, though
manual annotation of such data remains laborious and specialised, impeding
large-scale analysis. We present a tailored object detection framework for
identifying and classifying Antarctic benthic organisms in high-resolution
towed camera imagery, alongside the first public computer vision dataset for
benthic biodiversity monitoring in the Weddell Sea. Our approach addresses key
challenges associated with marine ecological imagery, including limited
annotated data, variable object sizes, and complex seafloor structure. The
proposed framework combines resolution-preserving patching, spatial data
augmentation, fine-tuning, and postprocessing via Slicing Aided Hyper
Inference. We benchmark multiple object detection architectures and demonstrate
strong performance in detecting medium and large organisms across 25
fine-grained morphotypes, significantly more than other works in this area.
Detection of small and rare taxa remains a challenge, reflecting limitations in
current detection architectures. Our framework provides a scalable foundation
for future machine-assisted in situ benthic biodiversity monitoring research.

</details>


### [103] [APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing](https://arxiv.org/abs/2507.21690)
*Sangmin Han,Jinho Jeong,Jinwoo Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: APT addresses patch-level distribution shift and patch monotonicity in LDMs for high-resolution image generation, improving detail and speed.


<details>
  <summary>Details</summary>
Motivation: Fixed-resolution LDMs struggle with high-resolution images; training-based methods are resource-heavy, while patch-based approaches face distribution and monotonicity issues.

Method: APT uses Statistical Matching and Scale-aware Scheduling to maintain patch consistency and handle monotonicity, enabling faster denoising.

Result: APT produces clearer, more detailed high-resolution images with improved inference speed.

Conclusion: APT offers a practical, efficient solution for high-resolution image generation in LDMs.

Abstract: Latent Diffusion Models (LDMs) are generally trained at fixed resolutions,
limiting their capability when scaling up to high-resolution images. While
training-based approaches address this limitation by training on
high-resolution datasets, they require large amounts of data and considerable
computational resources, making them less practical. Consequently,
training-free methods, particularly patch-based approaches, have become a
popular alternative. These methods divide an image into patches and fuse the
denoising paths of each patch, showing strong performance on high-resolution
generation. However, we observe two critical issues for patch-based approaches,
which we call ``patch-level distribution shift" and ``increased patch
monotonicity." To address these issues, we propose Adaptive Path Tracing (APT),
a framework that combines Statistical Matching to ensure patch distributions
remain consistent in upsampled latents and Scale-aware Scheduling to deal with
the patch monotonicity. As a result, APT produces clearer and more refined
details in high-resolution images. In addition, APT enables a shortcut
denoising process, resulting in faster sampling with minimal quality
degradation. Our experimental results confirm that APT produces more detailed
outputs with improved inference speed, providing a practical approach to
high-resolution image generation.

</details>


### [104] [Semantics versus Identity: A Divide-and-Conquer Approach towards Adjustable Medical Image De-Identification](https://arxiv.org/abs/2507.21703)
*Yuan Tian,Shuo Wang,Rongzhao Zhang,Zijian Chen,Yankai Jiang,Chunyi Li,Xiangyang Zhu,Fang Yan,Qiang Hu,XiaoSong Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: A framework for medical image de-identification balances privacy and medical semantics using identity-blocking and feature compensation with pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy risks in medical imaging while preserving medical semantics and allowing flexible privacy adjustments.

Method: Divide-and-conquer approach: Identity-Blocking for privacy levels and Medical-Semantics-Compensation using pre-trained models, with feature decoupling to remove residual identity.

Result: Outperforms existing methods across seven datasets and three tasks.

Conclusion: Proposed framework achieves state-of-the-art performance in de-identification while maintaining medical utility.

Abstract: Medical imaging has significantly advanced computer-aided diagnosis, yet its
re-identification (ReID) risks raise critical privacy concerns, calling for
de-identification (DeID) techniques. Unfortunately, existing DeID methods
neither particularly preserve medical semantics, nor are flexibly adjustable
towards different privacy levels. To address these issues, we propose a
divide-and-conquer framework comprising two steps: (1) Identity-Blocking, which
blocks varying proportions of identity-related regions, to achieve different
privacy levels; and (2) Medical-Semantics-Compensation, which leverages
pre-trained Medical Foundation Models (MFMs) to extract medical semantic
features to compensate the blocked regions. Moreover, recognizing that features
from MFMs may still contain residual identity information, we introduce a
Minimum Description Length principle-based feature decoupling strategy, to
effectively decouple and discard such identity components. Extensive
evaluations against existing approaches across seven datasets and three
downstream tasks, demonstrates our state-of-the-art performance.

</details>


### [105] [Impact of Underwater Image Enhancement on Feature Matching](https://arxiv.org/abs/2507.21715)
*Jason M. Summers,Mark W. Jones*

Main category: cs.CV

TL;DR: The paper proposes local matching stability and furthest matchable frame as metrics to evaluate underwater image enhancement, focusing on improving feature extraction and frame matching for tasks like SLAM.


<details>
  <summary>Details</summary>
Motivation: Underwater image enhancement is crucial for tasks like autonomous navigation, but existing methods lack robust evaluation metrics tailored to real-world underwater conditions.

Method: A novel evaluation framework is introduced, using metric-based analysis to assess enhancement techniques' impact on frame-matching performance.

Result: The framework identifies strengths and limitations of current methods and provides a context-aware benchmark for comparison.

Conclusion: The study demonstrates the framework's practical relevance by showing how visual enhancements improve SLAM performance in underwater scenarios.

Abstract: We introduce local matching stability and furthest matchable frame as
quantitative measures for evaluating the success of underwater image
enhancement. This enhancement process addresses visual degradation caused by
light absorption, scattering, marine growth, and debris. Enhanced imagery plays
a critical role in downstream tasks such as path detection and autonomous
navigation for underwater vehicles, relying on robust feature extraction and
frame matching. To assess the impact of enhancement techniques on
frame-matching performance, we propose a novel evaluation framework tailored to
underwater environments. Through metric-based analysis, we identify strengths
and limitations of existing approaches and pinpoint gaps in their assessment of
real-world applicability. By incorporating a practical matching strategy, our
framework offers a robust, context-aware benchmark for comparing enhancement
methods. Finally, we demonstrate how visual improvements affect the performance
of a complete real-world algorithm -- Simultaneous Localization and Mapping
(SLAM) -- reinforcing the framework's relevance to operational underwater
scenarios.

</details>


### [106] [Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations](https://arxiv.org/abs/2507.21723)
*Nils Hütten,Florian Hölken,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: The paper investigates the roles of internal components in detection transformer models (DETR, DDETR, DINO) using ablation studies, revealing model-specific resilience patterns and structural redundancies.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and efficiency in Explainable AI by understanding the distinct roles of internal components in detection transformers.

Method: Systematic ablation of key components (query embeddings, MHSA, MHCA) in DETR, DDETR, and DINO models, evaluated on COCO dataset using gIoU and F1-score.

Result: DETR is sensitive to encoder MHSA and decoder MHCA ablations, DDETR is robust due to multi-scale deformable attention, and DINO is most resilient due to its update rule. Structural redundancies in DDETR and DINO decoder MHCA layers were identified.

Conclusion: The study clarifies component contributions in DETRs, offering insights for optimizing transparency and efficiency, and releases the DeepDissect library for reproducibility.

Abstract: In recent years, Explainable AI has gained traction as an approach to
enhancing model interpretability and transparency, particularly in complex
models such as detection transformers. Despite rapid advancements, a
substantial research gap remains in understanding the distinct roles of
internal components - knowledge that is essential for improving transparency
and efficiency. Inspired by neuroscientific ablation studies, which investigate
the functions of brain regions through selective impairment, we systematically
analyze the impact of ablating key components in three state-of-the-art
detection transformer models: Detection transformer (DETR), deformable
detection transformer (DDETR), and DETR with improved denoising anchor boxes
(DINO). The ablations target query embeddings, encoder and decoder multi-head
self-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA)
layers. We evaluate the effects of these ablations on the performance metrics
gIoU and F1-score, quantifying effects on both the classification and
regression sub-tasks on the COCO dataset. To facilitate reproducibility and
future research, we publicly release the DeepDissect library. Our findings
reveal model-specific resilience patterns: while DETR is particularly sensitive
to ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable
attention enhances robustness, and DINO exhibits the greatest resilience due to
its look-forward twice update rule, which helps distributing knowledge across
blocks. These insights also expose structural redundancies, particularly in
DDETR's and DINO's decoder MHCA layers, highlighting opportunities for model
simplification without sacrificing performance. This study advances XAI for
DETRs by clarifying the contributions of internal components to model
performance, offering insights to optimize and improve transparency and
efficiency in critical applications.

</details>


### [107] [SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking](https://arxiv.org/abs/2507.21732)
*Qianxiong Xu,Lanyun Zhu,Chenxi Liu,Guosheng Lin,Cheng Long,Ziyue Li,Rui Zhao*

Main category: cs.CV

TL;DR: SAMITE improves VOT by addressing occlusion and distraction issues using a Prototypical Memory Bank and Positional Prompt Generator, built upon SAM2.


<details>
  <summary>Details</summary>
Motivation: Existing VOT methods lack temporal dependency handling and generalizability, and struggle with occlusions and distractions.

Method: SAMITE enhances SAM2 with a Prototypical Memory Bank to filter inaccurate frames and a Positional Prompt Generator for explicit positional clues.

Result: SAMITE outperforms on six benchmarks, demonstrating superior tracking accuracy.

Conclusion: SAMITE effectively tackles VOT challenges, offering improved performance and error interception.

Abstract: Visual Object Tracking (VOT) is widely used in applications like autonomous
driving to continuously track targets in videos. Existing methods can be
roughly categorized into template matching and autoregressive methods, where
the former usually neglects the temporal dependencies across frames and the
latter tends to get biased towards the object categories during training,
showing weak generalizability to unseen classes. To address these issues, some
methods propose to adapt the video foundation model SAM2 for VOT, where the
tracking results of each frame would be encoded as memory for conditioning the
rest of frames in an autoregressive manner. Nevertheless, existing methods fail
to overcome the challenges of object occlusions and distractions, and do not
have any measures to intercept the propagation of tracking errors. To tackle
them, we present a SAMITE model, built upon SAM2 with additional modules,
including: (1) Prototypical Memory Bank: We propose to quantify the
feature-wise and position-wise correctness of each frame's tracking results,
and select the best frames to condition subsequent frames. As the features of
occluded and distracting objects are feature-wise and position-wise inaccurate,
their scores would naturally be lower and thus can be filtered to intercept
error propagation; (2) Positional Prompt Generator: To further reduce the
impacts of distractors, we propose to generate positional mask prompts to
provide explicit positional clues for the target, leading to more accurate
tracking. Extensive experiments have been conducted on six benchmarks, showing
the superiority of SAMITE. The code is available at
https://github.com/Sam1224/SAMITE.

</details>


### [108] [MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces](https://arxiv.org/abs/2507.21741)
*Shaojun E,Yuchen Yang,Jiaheng Wu,Yan Zhang,Tiejun Zhao,Ziyan Chen*

Main category: cs.CV

TL;DR: MAGE is a novel multimodal framework addressing spatial and semantic losses in visual data encoding, enhancing alignment between vision and text through Intelligent Alignment Network (IAN) and innovative training strategies.


<details>
  <summary>Details</summary>
Motivation: The performance of large multimodal models depends on the coupling between visual encoders and language models, but existing methods suffer from vector gaps and semantic disparities, leading to information loss.

Method: Proposes MAGE with IAN for dimensional and semantic alignment, using cross-entropy and mean squared error training. Introduces a fine-tuning dataset for multimodal tool-calling to expand output capabilities.

Result: MAGE outperforms similar works on benchmarks like MME, MMBench, and SEED.

Conclusion: MAGE effectively bridges vision-text semantic gaps and enhances multimodal model performance, with code and appendix publicly available.

Abstract: In the latest advancements in multimodal learning, effectively addressing the
spatial and semantic losses of visual data after encoding remains a critical
challenge. This is because the performance of large multimodal models is
positively correlated with the coupling between visual encoders and large
language models. Existing approaches often face issues such as vector gaps or
semantic disparities, resulting in information loss during the propagation
process. To address these issues, we propose MAGE (Multimodal Alignment and
Generation Enhancement), a novel framework that bridges the semantic spaces of
vision and text through an innovative alignment mechanism. By introducing the
Intelligent Alignment Network (IAN), MAGE achieves dimensional and semantic
alignment. To reduce the gap between synonymous heterogeneous data, we employ a
training strategy that combines cross-entropy and mean squared error,
significantly enhancing the alignment effect. Moreover, to enhance MAGE's
"Any-to-Any" capability, we developed a fine-tuning dataset for multimodal
tool-calling instructions to expand the model's output capability boundaries.
Finally, our proposed multimodal large model architecture, MAGE, achieved
significantly better performance compared to similar works across various
evaluation benchmarks, including MME, MMBench, and SEED. Complete code and
appendix are available at: https://github.com/GTCOM-NLP/MAGE.

</details>


### [109] [Adversarial Reconstruction Feedback for Robust Fine-grained Generalization](https://arxiv.org/abs/2507.21742)
*Shijie Wang,Jian Shi,Haojie Li*

Main category: cs.CV

TL;DR: AdvRF is a novel adversarial reconstruction feedback framework for fine-grained image retrieval, learning category-agnostic discrepancy representations to improve generalization to unseen categories.


<details>
  <summary>Details</summary>
Motivation: Existing FGIR methods introduce category-specific semantics, hindering generalization. AdvRF aims to overcome this by learning category-agnostic representations.

Method: AdvRF combines category-aware discrepancy localization from retrieval models with category-agnostic feature learning from reconstruction models, using adversarial feedback and knowledge distillation.

Result: AdvRF achieves strong performance on fine-grained and coarse-grained datasets, demonstrating improved generalization.

Conclusion: AdvRF effectively learns category-agnostic representations, enhancing FGIR performance and generalization.

Abstract: Existing fine-grained image retrieval (FGIR) methods predominantly rely on
supervision from predefined categories to learn discriminative representations
for retrieving fine-grained objects. However, they inadvertently introduce
category-specific semantics into the retrieval representation, creating
semantic dependencies on predefined classes that critically hinder
generalization to unseen categories. To tackle this, we propose AdvRF, a novel
adversarial reconstruction feedback framework aimed at learning
category-agnostic discrepancy representations. Specifically, AdvRF reformulates
FGIR as a visual discrepancy reconstruction task via synergizing category-aware
discrepancy localization from retrieval models with category-agnostic feature
learning from reconstruction models. The reconstruction model exposes residual
discrepancies overlooked by the retrieval model, forcing it to improve
localization accuracy, while the refined signals from the retrieval model guide
the reconstruction model to improve its reconstruction ability. Consequently,
the retrieval model localizes visual differences, while the reconstruction
model encodes these differences into category-agnostic representations. This
representation is then transferred to the retrieval model through knowledge
distillation for efficient deployment. Quantitative and qualitative evaluations
demonstrate that our AdvRF achieves impressive performance on both widely-used
fine-grained and coarse-grained datasets.

</details>


### [110] [Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards](https://arxiv.org/abs/2507.21745)
*Aybora Koksal,A. Aydin Alatan*

Main category: cs.CV

TL;DR: A few-shot reinforcement learning framework (RLVR) for satellite imagery eliminates the need for caption supervision, using rule-based rewards. It achieves strong performance with minimal data, even matching models trained on thousands of samples.


<details>
  <summary>Details</summary>
Motivation: Specialized domains like remote sensing lack annotated data, making large models impractical. RLVR addresses this by reducing reliance on costly supervision.

Method: Adapts the 1-shot RLVR paradigm from language models to vision-language models, using policy-gradient optimization with minimal curated examples.

Result: Achieves substantial improvements with as few as one example, scaling to 128 examples matches/exceeds models trained on thousands. Shows robust generalization.

Conclusion: RLVR enables cost-effective, data-efficient development of domain-specialist models, offering a practical solution for data-scarce fields.

Abstract: Recent advances in large language and vision-language models have enabled
strong reasoning capabilities, yet they remain impractical for specialized
domains like remote sensing, where annotated data is scarce and expensive. We
present the first few-shot reinforcement learning with verifiable reward (RLVR)
framework for satellite imagery that eliminates the need for caption
supervision--relying solely on lightweight, rule-based binary or IoU-based
rewards. Adapting the "1-shot RLVR" paradigm from language models to
vision-language models, we employ policy-gradient optimization with as few as
one curated example to align model outputs for satellite reasoning tasks.
Comprehensive experiments across multiple remote sensing benchmarks--including
classification, visual question answering, and grounding--show that even a
single example yields substantial improvements over the base model. Scaling to
128 examples matches or exceeds models trained on thousands of annotated
samples. While the extreme one-shot setting can induce mild, task-specific
overfitting, our approach consistently demonstrates robust generalization and
efficiency across diverse tasks. Further, we find that prompt design and loss
weighting significantly influence training stability and final accuracy. Our
method enables cost-effective and data-efficient development of
domain-specialist vision-language reasoning models, offering a pragmatic recipe
for data-scarce fields: start from a compact VLM, curate a handful of
reward-checkable cases, and train via RLVR.

</details>


### [111] [LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection](https://arxiv.org/abs/2507.21756)
*Jing Ren,Suyu Ma,Hong Jia,Xiwei Xu,Ivan Lee,Haytham Fayek,Xiaodong Li,Feng Xia*

Main category: cs.CV

TL;DR: LiteFat is a lightweight spatio-temporal graph learning model for efficient driver fatigue detection, reducing computational demands while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Drowsy driving causes many accidents, but existing deep learning solutions are too resource-intensive for embedded devices.

Method: Converts video data into spatio-temporal graphs using facial landmarks, extracts features with MobileNet, and processes with a lightweight graph neural network.

Result: LiteFat achieves competitive accuracy with lower computational complexity and latency than state-of-the-art methods.

Conclusion: Enables real-time, resource-efficient fatigue detection for embedded devices like intelligent vehicles.

Abstract: Detecting driver fatigue is critical for road safety, as drowsy driving
remains a leading cause of traffic accidents. Many existing solutions rely on
computationally demanding deep learning models, which result in high latency
and are unsuitable for embedded robotic devices with limited resources (such as
intelligent vehicles/cars) where rapid detection is necessary to prevent
accidents. This paper introduces LiteFat, a lightweight spatio-temporal graph
learning model designed to detect driver fatigue efficiently while maintaining
high accuracy and low computational demands. LiteFat involves converting
streaming video data into spatio-temporal graphs (STG) using facial landmark
detection, which focuses on key motion patterns and reduces unnecessary data
processing. LiteFat uses MobileNet to extract facial features and create a
feature matrix for the STG. A lightweight spatio-temporal graph neural network
is then employed to identify signs of fatigue with minimal processing and low
latency. Experimental results on benchmark datasets show that LiteFat performs
competitively while significantly decreasing computational complexity and
latency as compared to current state-of-the-art methods. This work enables the
development of real-time, resource-efficient human fatigue detection systems
that can be implemented upon embedded robotic devices.

</details>


### [112] [MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions](https://arxiv.org/abs/2507.21761)
*YiZhou Li*

Main category: cs.CV

TL;DR: MoR-ViT introduces a token-level dynamic recursion mechanism for Vision Transformers, reducing parameters by 70% and speeding up inference by 2.5x while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard ViTs suffer from parameter redundancy and high computational costs, limiting practical deployment. Existing methods lack flexibility in computational depth for tokens.

Method: MoR-ViT uses a Mixture-of-Recursions (MoR) paradigm to let tokens adaptively determine their processing depth, enabling dynamic resource allocation.

Result: MoR-ViT achieves state-of-the-art accuracy with significant parameter reduction (70%) and inference acceleration (2.5x), outperforming baselines like DynamicViT and TinyViT.

Conclusion: Dynamic recursion is an effective strategy for efficient ViTs, offering scalability and deployability in real-world applications.

Abstract: Vision Transformers (ViTs) have achieved remarkable success in image
recognition, yet standard ViT architectures are hampered by substantial
parameter redundancy and high computational cost, limiting their practical
deployment. While recent efforts on efficient ViTs primarily focus on static
model compression or token-level sparsification, they remain constrained by
fixed computational depth for all tokens. In this work, we present MoR-ViT, a
novel vision transformer framework that, for the first time, incorporates a
token-level dynamic recursion mechanism inspired by the Mixture-of-Recursions
(MoR) paradigm. This approach enables each token to adaptively determine its
processing depth, yielding a flexible and input-dependent allocation of
computational resources. Extensive experiments on ImageNet-1K and transfer
benchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy
with up to 70% parameter reduction and 2.5x inference acceleration, but also
outperforms leading efficient ViT baselines such as DynamicViT and TinyViT
under comparable conditions. These results establish dynamic recursion as an
effective strategy for efficient vision transformers and open new avenues for
scalable and deployable deep learning models in real-world scenarios.

</details>


### [113] [AU-LLM: Micro-Expression Action Unit Detection via Enhanced LLM-Based Feature Fusion](https://arxiv.org/abs/2507.21778)
*Zhishu Liu,Kaishen Yuan,Bo Zhao,Yong Xu,Zitong Yu*

Main category: cs.CV

TL;DR: AU-LLM, a novel framework using LLMs for micro-expression AU detection, addresses the vision-language gap with an Enhanced Fusion Projector (EFP) and achieves state-of-the-art results on CASME II and SAMM datasets.


<details>
  <summary>Details</summary>
Motivation: Micro-expression AU detection is challenging due to subtle intensities and data scarcity. LLMs' potential in this domain is unexplored.

Method: AU-LLM uses an EFP (MLP-based) to fuse mid-level and high-level visual features from a 3D-CNN backbone into a dense token for LLM reasoning.

Result: AU-LLM achieves state-of-the-art performance on CASME II and SAMM datasets under LOSO and cross-domain protocols.

Conclusion: LLM-based reasoning shows significant potential for robust micro-expression analysis, validated by AU-LLM's performance.

Abstract: The detection of micro-expression Action Units (AUs) is a formidable
challenge in affective computing, pivotal for decoding subtle, involuntary
human emotions. While Large Language Models (LLMs) demonstrate profound
reasoning abilities, their application to the fine-grained, low-intensity
domain of micro-expression AU detection remains unexplored. This paper pioneers
this direction by introducing \textbf{AU-LLM}, a novel framework that for the
first time uses LLM to detect AUs in micro-expression datasets with subtle
intensities and the scarcity of data. We specifically address the critical
vision-language semantic gap, the \textbf{Enhanced Fusion Projector (EFP)}. The
EFP employs a Multi-Layer Perceptron (MLP) to intelligently fuse mid-level
(local texture) and high-level (global semantics) visual features from a
specialized 3D-CNN backbone into a single, information-dense token. This
compact representation effectively empowers the LLM to perform nuanced
reasoning over subtle facial muscle movements.Through extensive evaluations on
the benchmark CASME II and SAMM datasets, including stringent
Leave-One-Subject-Out (LOSO) and cross-domain protocols, AU-LLM establishes a
new state-of-the-art, validating the significant potential and robustness of
LLM-based reasoning for micro-expression analysis. The codes are available at
https://github.com/ZS-liu-JLU/AU-LLMs.

</details>


### [114] [MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot Learning](https://arxiv.org/abs/2507.21786)
*Zhaolong Wang,Tongfeng Sun,Mingzheng Du,Yachao Huang*

Main category: cs.CV

TL;DR: MSGCoOp enhances few-shot generalization in VLMs by using parallel learnable context vectors and semantic guidance from LLMs, improving robustness and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generalization to novel classes due to overfitting and forgetting general knowledge, while complex solutions add overhead.

Method: Proposes MSGCoOp with parallel context vectors, semantic guidance from LLMs, and diversity regularization to avoid redundancy.

Result: Achieves 1.10% average harmonic mean improvement over KgCoOp on 11 datasets, with better cross-domain robustness.

Conclusion: MSGCoOp effectively balances generalization and efficiency, outperforming baselines in few-shot and cross-domain tasks.

Abstract: Vision-language pre-trained models (VLMs) such as CLIP have demonstrated
remarkable zero-shot generalization, and prompt learning has emerged as an
efficient alternative to full fine-tuning. However, existing methods often
struggle with generalization to novel classes, a phenomenon attributed to
overfitting on seen classes and forgetting general knowledge. Furthermore,
recent approaches that improve generalization often introduce complex
architectures or heavy computational overhead. In this paper, we propose a
Multiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance
few-shot generalization while maintaining computational efficiency. Our
approach leverages an ensemble of parallel learnable context vectors to capture
diverse semantic aspects. To enrich these prompts, we introduce a semantic
guidance mechanism that aligns them with comprehensive class descriptions
automatically generated by a Large Language Model (LLM). Furthermore, a
diversity regularization loss encourages the prompts to learn complementary and
orthogonal features, preventing them from collapsing into redundant
representations. Extensive experiments on 11 benchmark datasets show that
MSGCoOp significantly improves performance on base-to-novel generalization,
achieving an average harmonic mean improvement of 1.10\% over the strong KgCoOp
baseline. Our method also demonstrates enhanced robustness in cross-domain
generalization tasks. Our code is avaliable at:
\href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.

</details>


### [115] [Distribution-Based Masked Medical Vision-Language Model Using Structured Reports](https://arxiv.org/abs/2507.21794)
*Shreyank N Gowda,Ruichi Zhang,Xiao Gu,Ying Weng,Lu Yang*

Main category: cs.CV

TL;DR: An uncertainty-aware medical image-text pre-training model is introduced to improve generalization in medical image analysis by leveraging structured text reports and modeling ambiguity.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with variability and ambiguity in medical data, limiting their ability to capture nuanced clinical information.

Method: Utilizes structured text reports (generated by an LLM) to augment image data, focusing on Chest X-Rays. Reports include disease definitions, appearance, observations, and verdicts. Models inter- and intra-modal uncertainty.

Result: Achieves state-of-the-art performance on multiple downstream tasks by improving representations and handling ambiguity.

Conclusion: The model significantly advances medical image-text pre-training, demonstrating enhanced generalization and performance.

Abstract: Medical image-language pre-training aims to align medical images with
clinically relevant text to improve model performance on various downstream
tasks. However, existing models often struggle with the variability and
ambiguity inherent in medical data, limiting their ability to capture nuanced
clinical information and uncertainty. This work introduces an uncertainty-aware
medical image-text pre-training model that enhances generalization capabilities
in medical image analysis. Building on previous methods and focusing on Chest
X-Rays, our approach utilizes structured text reports generated by a large
language model (LLM) to augment image data with clinically relevant context.
These reports begin with a definition of the disease, followed by the
`appearance' section to highlight critical regions of interest, and finally
`observations' and `verdicts' that ground model predictions in clinical
semantics. By modeling both inter- and intra-modal uncertainty, our framework
captures the inherent ambiguity in medical images and text, yielding improved
representations and performance on downstream tasks. Our model demonstrates
significant advances in medical image-text pre-training, obtaining
state-of-the-art performance on multiple downstream tasks.

</details>


### [116] [HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels](https://arxiv.org/abs/2507.21809)
*HunyuanWorld Team,Zhenwei Wang,Yuhao Liu,Junta Wu,Zixiao Gu,Haoyuan Wang,Xuhui Zuo,Tianyu Huang,Wenhuan Li,Sheng Zhang,Yihang Lian,Yulin Tsai,Lifu Wang,Sicong Liu,Puhua Jiang,Xianghui Yang,Dongyuan Guo,Yixuan Tang,Xinyue Mao,Jiaao Yu,Junlin Yu,Jihong Zhang,Meng Chen,Liang Dong,Yiwen Jia,Chao Zhang,Yonghao Tan,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Minghui Chen,Zhan Li,Wangchen Qin,Lei Wang,Yifu Sun,Lin Niu,Xiang Yuan,Xiaofeng Yang,Yingping He,Jie Xiao,Yangyu Tao,Jianchen Zhu,Jinbao Xue,Kai Liu,Chongqing Zhao,Xinming Wu,Tian Liu,Peng Chen,Di Wang,Yuhong Liu,Linus,Jie Jiang,Tengfei Wang,Chunchao Guo*

Main category: cs.CV

TL;DR: HunyuanWorld 1.0 is a novel framework for generating immersive, explorable, and interactive 3D scenes from text and image inputs, combining the strengths of video-based and 3D-based methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of creating 3D worlds from texts or images lacks a solution that balances diversity, 3D consistency, and efficiency. Existing methods are either inconsistent or inefficient.

Method: HunyuanWorld 1.0 uses a semantically layered 3D mesh representation with panoramic world proxies for decomposition and reconstruction, enabling diverse 3D world generation.

Result: The framework achieves state-of-the-art performance in generating coherent, explorable, and interactive 3D worlds with applications in VR, simulation, and gaming.

Conclusion: HunyuanWorld 1.0 successfully bridges the gap between diversity and consistency in 3D world generation, offering practical utility across multiple domains.

Abstract: Creating immersive and playable 3D worlds from texts or images remains a
fundamental challenge in computer vision and graphics. Existing world
generation approaches typically fall into two categories: video-based methods
that offer rich diversity but lack 3D consistency and rendering efficiency, and
3D-based methods that provide geometric consistency but struggle with limited
training data and memory-inefficient representations. To address these
limitations, we present HunyuanWorld 1.0, a novel framework that combines the
best of both worlds for generating immersive, explorable, and interactive 3D
scenes from text and image conditions. Our approach features three key
advantages: 1) 360{\deg} immersive experiences via panoramic world proxies; 2)
mesh export capabilities for seamless compatibility with existing computer
graphics pipelines; 3) disentangled object representations for augmented
interactivity. The core of our framework is a semantically layered 3D mesh
representation that leverages panoramic images as 360{\deg} world proxies for
semantic-aware world decomposition and reconstruction, enabling the generation
of diverse 3D worlds. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in generating coherent, explorable, and
interactive 3D worlds while enabling versatile applications in virtual reality,
physical simulation, game development, and interactive content creation.

</details>


### [117] [Anyone Can Jailbreak: Prompt-Based Attacks on LLMs and T2Is](https://arxiv.org/abs/2507.21820)
*Ahmed B Mustafa,Zihan Ye,Yang Lu,Michael P Pound,Shreyank N Gowda*

Main category: cs.CV

TL;DR: The paper investigates non-expert jailbreak techniques for LLMs and T2I systems, proposing a taxonomy and highlighting vulnerabilities in moderation pipelines.


<details>
  <summary>Details</summary>
Motivation: To understand how everyday users exploit prompt-based attacks to bypass safety mechanisms in AI systems.

Method: Systems-style investigation with empirical case studies across popular APIs, analyzing techniques like multi-turn narrative escalation and lexical camouflage.

Result: Reveals that all moderation pipeline stages can be bypassed using accessible strategies.

Conclusion: Urges the development of context-aware defenses to counter easily reproducible jailbreaks.

Abstract: Despite significant advancements in alignment and content moderation, large
language models (LLMs) and text-to-image (T2I) systems remain vulnerable to
prompt-based attacks known as jailbreaks. Unlike traditional adversarial
examples requiring expert knowledge, many of today's jailbreaks are low-effort,
high-impact crafted by everyday users with nothing more than cleverly worded
prompts. This paper presents a systems-style investigation into how non-experts
reliably circumvent safety mechanisms through techniques such as multi-turn
narrative escalation, lexical camouflage, implication chaining, fictional
impersonation, and subtle semantic edits. We propose a unified taxonomy of
prompt-level jailbreak strategies spanning both text-output and T2I models,
grounded in empirical case studies across popular APIs. Our analysis reveals
that every stage of the moderation pipeline, from input filtering to output
validation, can be bypassed with accessible strategies. We conclude by
highlighting the urgent need for context-aware defenses that reflect the ease
with which these jailbreaks can be reproduced in real-world settings.

</details>


### [118] [Cross-Architecture Distillation Made Simple with Redundancy Suppression](https://arxiv.org/abs/2507.21844)
*Weijia Zhang,Yuehao Liu,Wu Ran,Chao Ma*

Main category: cs.CV

TL;DR: A simple method for cross-architecture knowledge distillation suppresses redundant information, outperforming OFA with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing methods are inefficient due to complex designs. This work aims to simplify knowledge transfer by reducing architecture-exclusive redundancy.

Method: Proposes a redundancy suppression distillation (RSD) loss with cross-architecture invariance and feature decorrelation, plus a lightweight module to preserve student-specific capabilities.

Result: Outperforms OFA on CIFAR-100 and ImageNet-1k with fewer parameters.

Conclusion: The method is a simple, efficient baseline for cross-architecture distillation.

Abstract: We describe a simple method for cross-architecture knowledge distillation,
where the knowledge transfer is cast into a redundant information suppression
formulation. Existing methods introduce sophisticated modules,
architecture-tailored designs, and excessive parameters, which impair their
efficiency and applicability. We propose to extract the architecture-agnostic
knowledge in heterogeneous representations by reducing the redundant
architecture-exclusive information. To this end, we present a simple redundancy
suppression distillation (RSD) loss, which comprises cross-architecture
invariance maximisation and feature decorrelation objectives. To prevent the
student from entirely losing its architecture-specific capabilities, we further
design a lightweight module that decouples the RSD objective from the student's
internal representations. Our method is devoid of the architecture-specific
designs and complex operations in the pioneering method of OFA. It outperforms
OFA on CIFAR-100 and ImageNet-1k benchmarks with only a fraction of their
parameter overhead, which highlights its potential as a simple and strong
baseline to the cross-architecture distillation community.

</details>


### [119] [Unleashing the Power of Motion and Depth: A Selective Fusion Strategy for RGB-D Video Salient Object Detection](https://arxiv.org/abs/2507.21857)
*Jiahao He,Daerji Suolang,Keren Fu,Qijun Zhao*

Main category: cs.CV

TL;DR: The paper introduces SMFNet, a selective cross-modal fusion framework for RGB-D VSOD, using pixel-level fusion and multi-dimensional attention to optimize motion and depth contributions.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-D VSOD models treat optical flow and depth equally, limiting their potential. The paper aims to address this by selectively fusing these modalities based on their actual contributions.

Method: Proposes SMFNet with a pixel-level selective fusion strategy (PSF) and a multi-dimensional selective attention module (MSAM) to enhance feature representation.

Result: SMFNet outperforms 19 state-of-the-art models on RDVS and DVisal datasets, and performs well on synthetic depth datasets.

Conclusion: SMFNet effectively leverages motion and depth, setting a new benchmark for RGB-D VSOD.

Abstract: Applying salient object detection (SOD) to RGB-D videos is an emerging task
called RGB-D VSOD and has recently gained increasing interest, due to
considerable performance gains of incorporating motion and depth and that RGB-D
videos can be easily captured now in daily life. Existing RGB-D VSOD models
have different attempts to derive motion cues, in which extracting motion
information explicitly from optical flow appears to be a more effective and
promising alternative. Despite this, there remains a key issue that how to
effectively utilize optical flow and depth to assist the RGB modality in SOD.
Previous methods always treat optical flow and depth equally with respect to
model designs, without explicitly considering their unequal contributions in
individual scenarios, limiting the potential of motion and depth. To address
this issue and unleash the power of motion and depth, we propose a novel
selective cross-modal fusion framework (SMFNet) for RGB-D VSOD, incorporating a
pixel-level selective fusion strategy (PSF) that achieves optimal fusion of
optical flow and depth based on their actual contributions. Besides, we propose
a multi-dimensional selective attention module (MSAM) to integrate the fused
features derived from PSF with the remaining RGB modality at multiple
dimensions, effectively enhancing feature representation to generate refined
features. We conduct comprehensive evaluation of SMFNet against 19
state-of-the-art models on both RDVS and DVisal datasets, making the evaluation
the most comprehensive RGB-D VSOD benchmark up to date, and it also
demonstrates the superiority of SMFNet over other models. Meanwhile, evaluation
on five video benchmark datasets incorporating synthetic depth validates the
efficacy of SMFNet as well. Our code and benchmark results are made publicly
available at https://github.com/Jia-hao999/SMFNet.

</details>


### [120] [Low-Cost Test-Time Adaptation for Robust Video Editing](https://arxiv.org/abs/2507.21858)
*Jianhui Wang,Yinda Chen,Yangfan He,Xinyuan Song,Yi Xin,Dapeng Zhang,Zhongwei Wan,Bin Li,Rongchao Zhang*

Main category: cs.CV

TL;DR: Vid-TTA is a lightweight test-time adaptation framework for video editing, addressing temporal inconsistencies and prompt overfitting via self-supervised tasks, motion-aware reconstruction, and dynamic loss balancing.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods struggle with temporal inconsistencies and overfitting to simple prompts due to UNet limitations and lack of annotated data. Vid-TTA aims to enhance quality without heavy computational demands.

Method: Vid-TTA uses self-supervised auxiliary tasks, motion-aware frame reconstruction, and prompt perturbation with reconstruction. A meta-learning dynamic loss balancing mechanism adapts optimization per video.

Result: Vid-TTA improves temporal consistency and reduces prompt overfitting while keeping computational costs low, boosting performance for existing models.

Conclusion: Vid-TTA offers an efficient, plug-and-play solution for enhancing video editing quality by addressing key challenges with minimal overhead.

Abstract: Video editing is a critical component of content creation that transforms raw
footage into coherent works aligned with specific visual and narrative
objectives. Existing approaches face two major challenges: temporal
inconsistencies due to failure in capturing complex motion patterns, and
overfitting to simple prompts arising from limitations in UNet backbone
architectures. While learning-based methods can enhance editing quality, they
typically demand substantial computational resources and are constrained by the
scarcity of high-quality annotated data. In this paper, we present Vid-TTA, a
lightweight test-time adaptation framework that personalizes optimization for
each test video during inference through self-supervised auxiliary tasks. Our
approach incorporates a motion-aware frame reconstruction mechanism that
identifies and preserves crucial movement regions, alongside a prompt
perturbation and reconstruction strategy that strengthens model robustness to
diverse textual descriptions. These innovations are orchestrated by a
meta-learning driven dynamic loss balancing mechanism that adaptively adjusts
the optimization process based on video characteristics. Extensive experiments
demonstrate that Vid-TTA significantly improves video temporal consistency and
mitigates prompt overfitting while maintaining low computational overhead,
offering a plug-and-play performance boost for existing video editing models.

</details>


### [121] [CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding](https://arxiv.org/abs/2507.21888)
*Fevziye Irem Eyiokur,Dogucan Yaman,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: A dual-model framework improves embodied reference understanding by leveraging head-to-fingertip and wrist-to-fingertip directions, using Gaussian ray heatmaps and a CLIP-Aware Pointing Ensemble, achieving a 4 mAP improvement.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multimodal integration (text, pointing, scene context) and rely on simplistic single-line assumptions for referent prediction.

Method: Proposes a dual-model framework with Gaussian ray heatmaps, a CLIP-Aware Pointing Ensemble, and an object center prediction head for better referent localization.

Result: Achieves a ~4 mAP improvement at 0.25 IoU on the YouRefIt dataset.

Conclusion: The dual-model approach effectively integrates multimodal cues and improves referent prediction accuracy.

Abstract: We address the problem of Embodied Reference Understanding, which involves
predicting the object that a person in the scene is referring to through both
pointing gesture and language. Accurately identifying the referent requires
multimodal understanding: integrating textual instructions, visual pointing,
and scene context. However, existing methods often struggle to effectively
leverage visual clues for disambiguation. We also observe that, while the
referent is often aligned with the head-to-fingertip line, it occasionally
aligns more closely with the wrist-to-fingertip line. Therefore, relying on a
single line assumption can be overly simplistic and may lead to suboptimal
performance. To address this, we propose a dual-model framework, where one
model learns from the head-to-fingertip direction and the other from the
wrist-to-fingertip direction. We further introduce a Gaussian ray heatmap
representation of these lines and use them as input to provide a strong
supervisory signal that encourages the model to better attend to pointing cues.
To combine the strengths of both models, we present the CLIP-Aware Pointing
Ensemble module, which performs a hybrid ensemble based on CLIP features.
Additionally, we propose an object center prediction head as an auxiliary task
to further enhance referent localization. We validate our approach through
extensive experiments and analysis on the benchmark YouRefIt dataset, achieving
an improvement of approximately 4 mAP at the 0.25 IoU threshold.

</details>


### [122] [Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs](https://arxiv.org/abs/2507.21893)
*Saeed Ghorbani*

Main category: cs.CV

TL;DR: Aether Weaver is a multimodal narrative co-generation framework that integrates text, visuals, and soundscapes, outperforming sequential pipelines in depth, fidelity, and emotional resonance.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of sequential text-to-visual pipelines by enabling concurrent synthesis of narratives, visuals, and soundscapes for richer storytelling.

Method: Uses a Narrator (LLM) for text and prompts, a Director for scene graphs, a Narrative Arc Controller for story structure, and an Affective Tone Mapper for emotional consistency.

Result: Qualitative evaluations show enhanced narrative depth, visual fidelity, and emotional resonance compared to baselines.

Conclusion: Aether Weaver offers a robust platform for creative prototyping and immersive storytelling.

Abstract: We introduce Aether Weaver, a novel, integrated framework for multimodal
narrative co-generation that overcomes limitations of sequential text-to-visual
pipelines. Our system concurrently synthesizes textual narratives, dynamic
scene graph representations, visual scenes, and affective soundscapes, driven
by a tightly integrated, co-generation mechanism. At its core, the Narrator, a
large language model, generates narrative text and multimodal prompts, while
the Director acts as a dynamic scene graph manager, and analyzes the text to
build and maintain a structured representation of the story's world, ensuring
spatio-temporal and relational consistency for visual rendering and subsequent
narrative generation. Additionally, a Narrative Arc Controller guides the
high-level story structure, influencing multimodal affective consistency,
further complemented by an Affective Tone Mapper that ensures congruent
emotional expression across all modalities. Through qualitative evaluations on
a diverse set of narrative prompts encompassing various genres, we demonstrate
that Aether Weaver significantly enhances narrative depth, visual fidelity, and
emotional resonance compared to cascaded baseline approaches. This integrated
framework provides a robust platform for rapid creative prototyping and
immersive storytelling experiences.

</details>


### [123] [Evaluating Deepfake Detectors in the Wild](https://arxiv.org/abs/2507.21905)
*Viacheslav Pirogov,Maksim Artemev*

Main category: cs.CV

TL;DR: The paper evaluates modern deepfake detectors using a novel real-world testing procedure and a large dataset of 500,000+ deepfake images, finding detection remains challenging with most detectors performing poorly (AUC < 60%).


<details>
  <summary>Details</summary>
Motivation: Deepfakes pose a growing threat to digital media authenticity, but existing detectors lack real-world testing.

Method: A novel testing procedure mimicking real-world scenarios was developed, using state-of-the-art deepfake generation to create a large dataset.

Result: Detection is difficult, with fewer than half of detectors achieving AUC > 60%. Basic image manipulations further degrade performance.

Conclusion: Deepfake detection remains a significant challenge, and current detectors are vulnerable to simple image manipulations.

Abstract: Deepfakes powered by advanced machine learning models present a significant
and evolving threat to identity verification and the authenticity of digital
media. Although numerous detectors have been developed to address this problem,
their effectiveness has yet to be tested when applied to real-world data. In
this work we evaluate modern deepfake detectors, introducing a novel testing
procedure designed to mimic real-world scenarios for deepfake detection. Using
state-of-the-art deepfake generation methods, we create a comprehensive dataset
containing more than 500,000 high-quality deepfake images. Our analysis shows
that detecting deepfakes still remains a challenging task. The evaluation shows
that in fewer than half of the deepfake detectors tested achieved an AUC score
greater than 60%, with the lowest being 50%. We demonstrate that basic image
manipulations, such as JPEG compression or image enhancement, can significantly
reduce model performance. All code and data are publicly available at
https://github.com/messlav/Deepfake-Detectors-in-the-Wild.

</details>


### [124] [Predict Patient Self-reported Race from Skin Histological Images](https://arxiv.org/abs/2507.21912)
*Shengjia Chen,Ruchika Verma,Kevin Clare,Jannes Jegminat,Kuan-lin Huang,Brandon Veremis,Thomas Fuchs,Gabriele Campanella*

Main category: cs.CV

TL;DR: AI in computational pathology can predict self-reported race from dermatopathology slides, revealing unintended biases and the need for careful data curation.


<details>
  <summary>Details</summary>
Motivation: To investigate whether deep learning models can learn unintended demographic biases, particularly race, from pathology slides.

Method: Used an attention-based mechanism on a racially diverse dataset to identify race-associated morphological features, testing three curation strategies.

Result: Models predicted race with high AUC for White and Black groups (0.799, 0.762), but overall performance dropped to 0.663. Epidermis was a key predictive feature.

Conclusion: Highlights the need for bias mitigation and equitable AI deployment in pathology.

Abstract: Artificial Intelligence (AI) has demonstrated success in computational
pathology (CPath) for disease detection, biomarker classification, and
prognosis prediction. However, its potential to learn unintended demographic
biases, particularly those related to social determinants of health, remains
understudied. This study investigates whether deep learning models can predict
self-reported race from digitized dermatopathology slides and identifies
potential morphological shortcuts. Using a multisite dataset with a racially
diverse population, we apply an attention-based mechanism to uncover
race-associated morphological features. After evaluating three dataset curation
strategies to control for confounding factors, the final experiment showed that
White and Black demographic groups retained high prediction performance (AUC:
0.799, 0.762), while overall performance dropped to 0.663. Attention analysis
revealed the epidermis as a key predictive feature, with significant
performance declines when these regions were removed. These findings highlight
the need for careful data curation and bias mitigation to ensure equitable AI
deployment in pathology. Code available at:
https://github.com/sinai-computational-pathology/CPath_SAIF.

</details>


### [125] [ArtSeek: Deep artwork understanding via multimodal in-context reasoning and late interaction retrieval](https://arxiv.org/abs/2507.21917)
*Nicola Fanelli,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: ArtSeek is a multimodal framework for art analysis using retrieval-augmented generation and multimodal LLMs, achieving state-of-the-art results without relying on external links.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of analyzing digitized artworks, which requires deep artistic and historical knowledge, especially for works lacking Wikidata/Wikipedia links.

Method: Combines multimodal retrieval, contrastive multitask classification, and agentic reasoning with WikiFragments dataset.

Result: Achieves +8.4% F1 in style classification and +7.1 BLEU@1 in captioning, with qualitative success in interpreting obscure works.

Conclusion: ArtSeek generalizes to domains needing external knowledge, with plans to release dataset and code publicly.

Abstract: Analyzing digitized artworks presents unique challenges, requiring not only
visual interpretation but also a deep understanding of rich artistic,
contextual, and historical knowledge. We introduce ArtSeek, a multimodal
framework for art analysis that combines multimodal large language models with
retrieval-augmented generation. Unlike prior work, our pipeline relies only on
image input, enabling applicability to artworks without links to Wikidata or
Wikipedia-common in most digitized collections. ArtSeek integrates three key
components: an intelligent multimodal retrieval module based on late
interaction retrieval, a contrastive multitask classification network for
predicting artist, genre, style, media, and tags, and an agentic reasoning
strategy enabled through in-context examples for complex visual question
answering and artwork explanation via Qwen2.5-VL. Central to this approach is
WikiFragments, a Wikipedia-scale dataset of image-text fragments curated to
support knowledge-grounded multimodal reasoning. Our framework achieves
state-of-the-art results on multiple benchmarks, including a +8.4% F1
improvement in style classification over GraphCLIP and a +7.1 BLEU@1 gain in
captioning on ArtPedia. Qualitative analyses show that ArtSeek can interpret
visual motifs, infer historical context, and retrieve relevant knowledge, even
for obscure works. Though focused on visual arts, our approach generalizes to
other domains requiring external knowledge, supporting scalable multimodal AI
research. Both the dataset and the source code will be made publicly available
at https://github.com/cilabuniba/artseek.

</details>


### [126] [SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention](https://arxiv.org/abs/2507.21922)
*Peiran Gu,Teng Yao,Mengshen He,Fuhao Duan,Feiyan Liu,RenYuan Peng,Bao Ge*

Main category: cs.CV

TL;DR: The paper introduces SwinECAT, a Transformer-based model combining Swin and ECA Attention, to improve fundus image analysis by addressing challenges like small lesions and subtle disease differences. It achieves high accuracy (88.29%) in 9-category classification.


<details>
  <summary>Details</summary>
Motivation: Fundus image analysis faces challenges like small lesion areas and subtle disease differences, reducing model accuracy. Existing methods classify fewer disease types, limiting diagnostic granularity.

Method: Proposes SwinECAT, integrating Swin Attention for spatial structures and ECA Attention for critical feature channels, enhancing discriminative representation. Evaluated on EDID with 16,140 images.

Result: SwinECAT achieves 88.29% accuracy, weighted F1-score of 0.88, and macro F1-score of 0.90, outperforming baseline models.

Conclusion: SwinECAT advances fundus disease classification with higher granularity and performance, setting a new benchmark for 9-category classification.

Abstract: In recent years, artificial intelligence has been increasingly applied in the
field of medical imaging. Among these applications, fundus image analysis
presents special challenges, including small lesion areas in certain fundus
diseases and subtle inter-disease differences, which can lead to reduced
prediction accuracy and overfitting in the models. To address these challenges,
this paper proposes the Transformer-based model SwinECAT, which combines the
Shifted Window (Swin) Attention with the Efficient Channel Attention (ECA)
Attention. SwinECAT leverages the Swin Attention mechanism in the Swin
Transformer backbone to effectively capture local spatial structures and
long-range dependencies within fundus images. The lightweight ECA mechanism is
incorporated to guide the SwinECAT's attention toward critical feature
channels, enabling more discriminative feature representation. In contrast to
previous studies that typically classify fundus images into 4 to 6 categories,
this work expands fundus disease classification to 9 distinct types, thereby
enhancing the granularity of diagnosis. We evaluate our method on the Eye
Disease Image Dataset (EDID) containing 16,140 fundus images for 9-category
classification. Experimental results demonstrate that SwinECAT achieves 88.29\%
accuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. The
classification results of our proposed model SwinECAT significantly outperform
the baseline Swin Transformer and multiple compared baseline models. To our
knowledge, this represents the highest reported performance for 9-category
classification on this public dataset.

</details>


### [127] [MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning](https://arxiv.org/abs/2507.21924)
*Tianhong Gao,Yannian Fu,Weiqun Wu,Haixiao Yue,Shanshan Liu,Gang Zhang*

Main category: cs.CV

TL;DR: The paper introduces MMAT-1M, a million-scale multimodal agent tuning dataset to enhance CoT, reflection, and tool usage in multimodal LLMs, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale, high-quality multimodal agent tuning datasets limits the potential of multimodal LLMs.

Method: A four-stage data engine curates and refines multimodal data, integrating rationales, reflections, and dynamic tool usage via GPT-4o.

Result: Fine-tuning on MMAT-1M improves performance, e.g., 2.7% average gain on benchmarks and 8.8% on Dyn-VQA.

Conclusion: MMAT-1M effectively bridges the gap in multimodal agent tuning, enhancing reasoning and tool-based capabilities.

Abstract: Large Language Models (LLMs), enhanced through agent tuning, have
demonstrated remarkable capabilities in Chain-of-Thought (CoT) and tool
utilization, significantly surpassing the performance of standalone models.
However, the multimodal domain still lacks a large-scale, high-quality agent
tuning dataset to unlock the full potential of multimodal large language
models. To bridge this gap, we introduce MMAT-1M, the first million-scale
multimodal agent tuning dataset designed to support CoT, reflection, and
dynamic tool usage. Our dataset is constructed through a novel four-stage data
engine: 1) We first curate publicly available multimodal datasets containing
question-answer pairs; 2) Then, leveraging GPT-4o, we generate rationales for
the original question-answer pairs and dynamically integrate API calls and
Retrieval Augmented Generation (RAG) information through a multi-turn paradigm;
3) Furthermore, we refine the rationales through reflection to ensure logical
consistency and accuracy, creating a multi-turn dialogue dataset with both
Rationale and Reflection (RR); 4) Finally, to enhance efficiency, we optionally
compress multi-turn dialogues into a One-turn Rationale and Reflection (ORR)
format. By fine-tuning open-source multimodal models on the MMAT-1M, we observe
significant performance gains. For instance, the InternVL2.5-8B-RR model
achieves an average improvement of 2.7% across eight public benchmarks and 8.8%
on the RAG benchmark Dyn-VQA, demonstrating the dataset's effectiveness in
enhancing multimodal reasoning and tool-based capabilities. The dataset is
publicly available at https://github.com/VIS-MPU-Agent/MMAT-1M.

</details>


### [128] [Attention-Driven Multimodal Alignment for Long-term Action Quality Assessment](https://arxiv.org/abs/2507.21945)
*Xin Wang,Peng-Jie Li,Yuan-Yuan Shen*

Main category: cs.CV

TL;DR: LMAC-Net improves long-term AQA by aligning multimodal features (visual and audio) with attention consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing AQA methods lack deep cross-modal collaboration and temporal dynamics, limiting performance in artistic sports evaluation.

Method: LMAC-Net uses multimodal attention consistency, a local query encoder, and two-level score evaluation with joint optimization losses.

Result: LMAC-Net outperforms existing methods on RG and Fis-V datasets.

Conclusion: The proposed LMAC-Net effectively addresses multimodal alignment and temporal dynamics in long-term AQA.

Abstract: Long-term action quality assessment (AQA) focuses on evaluating the quality
of human activities in videos lasting up to several minutes. This task plays an
important role in the automated evaluation of artistic sports such as rhythmic
gymnastics and figure skating, where both accurate motion execution and
temporal synchronization with background music are essential for performance
assessment. However, existing methods predominantly fall into two categories:
unimodal approaches that rely solely on visual features, which are inadequate
for modeling multimodal cues like music; and multimodal approaches that
typically employ simple feature-level contrastive fusion, overlooking deep
cross-modal collaboration and temporal dynamics. As a result, they struggle to
capture complex interactions between modalities and fail to accurately track
critical performance changes throughout extended sequences. To address these
challenges, we propose the Long-term Multimodal Attention Consistency Network
(LMAC-Net). LMAC-Net introduces a multimodal attention consistency mechanism to
explicitly align multimodal features, enabling stable integration of visual and
audio information and enhancing feature representations. Specifically, we
introduce a multimodal local query encoder module to capture temporal semantics
and cross-modal relations, and use a two-level score evaluation for
interpretable results. In addition, attention-based and regression-based losses
are applied to jointly optimize multimodal alignment and score fusion.
Experiments conducted on the RG and Fis-V datasets demonstrate that LMAC-Net
significantly outperforms existing methods, validating the effectiveness of our
proposed approach.

</details>


### [129] [Enhancing Generalization in Data-free Quantization via Mixup-class Prompting](https://arxiv.org/abs/2507.21947)
*Jiwoong Park,Chaeun Lee,Yongseok Choi,Sein Park,Deokki Hong,Jungwook Choi*

Main category: cs.CV

TL;DR: The paper introduces a mixup-class prompt strategy for data-free quantization (DFQ) to improve synthetic data diversity and robustness, enhancing PTQ performance, especially in low-bit scenarios.


<details>
  <summary>Details</summary>
Motivation: Post-training quantization (PTQ) faces challenges with limited calibration data under privacy constraints, and existing DFQ methods using single-class prompts suffer from polysemy and performance degradation.

Method: Proposes a mixup-based text prompting strategy (mixup-class prompt) to fuse multiple class labels at the text level, generating diverse synthetic data for PTQ. Analyzes gradient norm and generalization error.

Result: Outperforms state-of-the-art DFQ methods like GenQ, achieving new accuracy in 2-bit weight, 4-bit activation (W2A4) quantization.

Conclusion: The mixup-class prompt enhances generalization and optimization stability in PTQ, pushing performance boundaries in low-bit scenarios.

Abstract: Post-training quantization (PTQ) improves efficiency but struggles with
limited calibration data, especially under privacy constraints. Data-free
quantization (DFQ) mitigates this by generating synthetic images using
generative models such as generative adversarial networks (GANs) and
text-conditioned latent diffusion models (LDMs), while applying existing PTQ
algorithms. However, the relationship between generated synthetic images and
the generalizability of the quantized model during PTQ remains underexplored.
Without investigating this relationship, synthetic images generated by previous
prompt engineering methods based on single-class prompts suffer from issues
such as polysemy, leading to performance degradation. We propose
\textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses
multiple class labels at the text prompt level to generate diverse, robust
synthetic data. This approach enhances generalization, and improves
optimization stability in PTQ. We provide quantitative insights through
gradient norm and generalization error analysis. Experiments on convolutional
neural networks (CNNs) and vision transformers (ViTs) show that our method
consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore,
it pushes the performance boundary in extremely low-bit scenarios, achieving
new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation
(W2A4) quantization.

</details>


### [130] [Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal](https://arxiv.org/abs/2507.21949)
*Jiyu Wu,Yifan Liu,Jiancheng Huang,Mingfu Yan,Shifeng Chen*

Main category: cs.CV

TL;DR: Proposes AGBA and FCFN for shadow removal without masks, outperforming mask-free methods and competing with mask-based ones.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on hard-to-acquire shadow masks; intrinsic cues like contrast are ambiguous in complex scenes.

Method: Introduces AGBA for dynamic feature filtering and FCFN for detail restoration using frequency-contrast fusion.

Result: Achieves state-of-the-art in mask-free approaches and competitive with mask-based methods.

Conclusion: AGBA and FCFN effectively address shadow removal challenges without masks, offering robust performance.

Abstract: Existing shadow removal methods often rely on shadow masks, which are
challenging to acquire in real-world scenarios. Exploring intrinsic image cues,
such as local contrast information, presents a potential alternative for
guiding shadow removal in the absence of explicit masks. However, the cue's
inherent ambiguity becomes a critical limitation in complex scenes, where it
can fail to distinguish true shadows from low-reflectance objects and intricate
background textures. To address this motivation, we propose the Adaptive Gated
Dual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs
the contrast prior to effectively disentangle shadow features from confounding
visual elements. Furthermore, to tackle the persistent challenge of restoring
soft shadow boundaries and fine-grained details, we introduce a diffusion-based
Frequency-Contrast Fusion Network (FCFN) that leverages high-frequency and
contrast cues to guide the generative process. Extensive experiments
demonstrate that our method achieves state-of-the-art results among mask-free
approaches while maintaining competitive performance relative to mask-based
methods.

</details>


### [131] [MetaCLIP 2: A Worldwide Scaling Recipe](https://arxiv.org/abs/2507.22062)
*Yung-Sung Chuang,Yang Li,Dong Wang,Ching-Feng Yeh,Kehan Lyu,Ramya Raghavendra,James Glass,Lifei Huang,Jason Weston,Luke Zettlemoyer,Xinlei Chen,Zhuang Liu,Saining Xie,Wen-tau Yih,Shang-Wen Li,Hu Xu*

Main category: cs.CV

TL;DR: MetaCLIP 2 improves CLIP training on worldwide web-scale data, outperforming English-only and multilingual models in zero-shot tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in scaling CLIP training to non-English data and mitigating the 'curse of multilinguality' in multilingual models.

Method: Training CLIP from scratch on worldwide web-scale image-text pairs with minimal changes to handle multilingual data.

Result: MetaCLIP 2 ViT-H/14 surpasses English-only CLIP by 0.8% in zero-shot ImageNet classification and sets new SOTA on multilingual benchmarks.

Conclusion: MetaCLIP 2 demonstrates mutual benefits from English and non-English data, achieving superior performance without system-level changes.

Abstract: Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,
supporting from zero-shot classification, retrieval to encoders for multimodal
large language models (MLLMs). Although CLIP is successfully trained on
billion-scale image-text pairs from the English world, scaling CLIP's training
further to learning from the worldwide web data is still challenging: (1) no
curation method is available to handle data points from non-English world; (2)
the English performance from existing multilingual CLIP is worse than its
English-only counterpart, i.e., "curse of multilinguality" that is common in
LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch
on worldwide web-scale image-text pairs. To generalize our findings, we conduct
rigorous ablations with minimal changes that are necessary to address the above
challenges and present a recipe enabling mutual benefits from English and
non-English world data. In zero-shot ImageNet classification, MetaCLIP 2
ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,
and surprisingly sets new state-of-the-art without system-level confounding
factors (e.g., translation, bespoke architecture changes) on multilingual
benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with
64.3% on image-to-text retrieval.

</details>


### [132] [Mitigating Spurious Correlations in Weakly Supervised Semantic Segmentation via Cross-architecture Consistency Regularization](https://arxiv.org/abs/2507.21959)
*Zheyuan Zhang,Yen-chia Hsu*

Main category: cs.CV

TL;DR: A novel weakly supervised semantic segmentation (WSSS) framework is proposed to address the co-occurrence problem in industrial smoke detection without external supervision, using a teacher-student model combining CNNs and ViTs.


<details>
  <summary>Details</summary>
Motivation: The scarcity of pixel-level labels, especially in domains like industrial smoke, makes detailed annotations difficult. Existing WSSS methods suffer from incomplete coverage and inaccurate boundaries due to supervision gaps and model bias.

Method: A teacher-student framework combining CNNs and ViTs is introduced, with a knowledge transfer loss for cross-architecture consistency and post-processing to improve pseudo mask quality.

Result: The proposed framework targets co-occurrence issues directly, avoiding reliance on external priors, and enhances pseudo mask accuracy.

Conclusion: The method effectively addresses limitations in WSSS for industrial smoke detection, improving segmentation quality without additional supervision.

Abstract: Scarcity of pixel-level labels is a significant challenge in practical
scenarios. In specific domains like industrial smoke, acquiring such detailed
annotations is particularly difficult and often requires expert knowledge. To
alleviate this, weakly supervised semantic segmentation (WSSS) has emerged as a
promising approach. However, due to the supervision gap and inherent bias in
models trained with only image level labels, existing WSSS methods suffer from
limitations such as incomplete foreground coverage, inaccurate object
boundaries, and spurious correlations, especially in our domain, where
emissions are always spatially coupled with chimneys.
  Previous solutions typically rely on additional priors or external knowledge
to mitigate these issues, but they often lack scalability and fail to address
the model's inherent bias toward co-occurring context. To address this, we
propose a novel WSSS framework that directly targets the co-occurrence problem
without relying on external supervision. Unlike prior methods that adopt a
single network, we employ a teacher-student framework that combines CNNs and
ViTs. We introduce a knowledge transfer loss that enforces cross-architecture
consistency by aligning internal representations. Additionally, we incorporate
post-processing techniques to address partial coverage and further improve
pseudo mask quality.

</details>


### [133] [PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction](https://arxiv.org/abs/2507.21960)
*Jiahui Ren,Mochu Xiang,Jiajun Zhu,Yuchao Dai*

Main category: cs.CV

TL;DR: PanoSplatt3R is a novel unposed wide-baseline panorama reconstruction method that eliminates the need for precise pose information, outperforming state-of-the-art methods in novel view generation and depth estimation.


<details>
  <summary>Details</summary>
Motivation: Existing panorama reconstruction methods rely heavily on accurate pose information, which is resource-intensive and noise-prone, limiting their practicality.

Method: PanoSplatt3R adapts reconstruction pretrainings from the perspective to the panoramic domain and introduces RoPE rolling for efficient domain transfer, modeling panorama periodicity.

Result: PanoSplatt3R outperforms current methods in novel view generation and depth estimation without requiring pose information.

Conclusion: PanoSplatt3R demonstrates superior performance and practicality, making it a promising solution for real-world applications.

Abstract: Wide-baseline panorama reconstruction has emerged as a highly effective and
pivotal approach for not only achieving geometric reconstruction of the
surrounding 3D environment, but also generating highly realistic and immersive
novel views. Although existing methods have shown remarkable performance across
various benchmarks, they are predominantly reliant on accurate pose
information. In real-world scenarios, the acquisition of precise pose often
requires additional computational resources and is highly susceptible to noise.
These limitations hinder the broad applicability and practicality of such
methods. In this paper, we present PanoSplatt3R, an unposed wide-baseline
panorama reconstruction method. We extend and adapt the foundational
reconstruction pretrainings from the perspective domain to the panoramic
domain, thus enabling powerful generalization capabilities. To ensure a
seamless and efficient domain-transfer process, we introduce RoPE rolling that
spans rolled coordinates in rotary positional embeddings across different
attention heads, maintaining a minimal modification to RoPE's mechanism, while
modeling the horizontal periodicity of panorama images. Comprehensive
experiments demonstrate that PanoSplatt3R, even in the absence of pose
information, significantly outperforms current state-of-the-art methods. This
superiority is evident in both the generation of high-quality novel views and
the accuracy of depth estimation, thereby showcasing its great potential for
practical applications. Project page: https://npucvr.github.io/PanoSplatt3R

</details>


### [134] [A Deep Learning Pipeline Using Synthetic Data to Improve Interpretation of Paper ECG Images](https://arxiv.org/abs/2507.21968)
*Xiaoyu Wang,Ramesh Nadarajah,Zhiqiang Zhang,David Wong*

Main category: cs.CV

TL;DR: A deep learning framework for classifying ECG images into diagnostic categories, addressing visual noise and fine-detailed pattern detection, achieving high AUROC scores.


<details>
  <summary>Details</summary>
Motivation: Early detection of CVDs is critical, and automating ECG interpretation from images can improve efficiency and accessibility in clinical workflows.

Method: Proposes a pre-processing pipeline for noise reduction and a two-stage fine-tuning strategy using ConvNeXt architecture.

Result: Achieved AUROC scores of 0.9688 (validation) and 0.9677 (test), demonstrating high accuracy.

Conclusion: The framework shows promise for practical automated ECG interpretation, bridging the gap between image-based and digital signal-based methods.

Abstract: Cardiovascular diseases (CVDs) are the leading global cause of death, and
early detection is essential to improve patient outcomes. Electrocardiograms
(ECGs), especially 12-lead ECGs, play a key role in the identification of CVDs.
These are routinely interpreted by human experts, a process that is
time-consuming and requires expert knowledge. Historical research in this area
has focused on automatic ECG interpretation from digital signals, with recent
deep learning approaches achieving strong results. In practice, however, most
ECG data in clinical practice are stored or shared in image form. To bridge
this gap, we propose a deep learning framework designed specifically to
classify paper-like ECG images into five main diagnostic categories. Our method
was the winning entry to the 2024 British Heart Foundation Open Data Science
Challenge. It addresses two main challenges of paper ECG classification: visual
noise (e.g., shadows or creases) and the need to detect fine-detailed waveform
patterns. We propose a pre-processing pipeline that reduces visual noise and a
two-stage fine-tuning strategy: the model is first fine-tuned on synthetic and
external ECG image datasets to learn domain-specific features, and then further
fine-tuned on the target dataset to enhance disease-specific recognition. We
adopt the ConvNeXt architecture as the backbone of our model. Our method
achieved AUROC scores of 0.9688 on the public validation set and 0.9677 on the
private test set of the British Heart Foundation Open Data Science Challenge,
highlighting its potential as a practical tool for automated ECG interpretation
in clinical workflows.

</details>


### [135] [EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation](https://arxiv.org/abs/2507.21971)
*Zhijiang Li,Haoran He*

Main category: cs.CV

TL;DR: EIFNet is a multi-modal fusion network for event-based semantic segmentation, addressing challenges in feature extraction and fusion of sparse event data with dense image data. It achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer high dynamic range and fine temporal resolution, but extracting reliable features from sparse, noisy event streams and fusing them with dense image data is challenging.

Method: Proposes EIFNet with Adaptive Event Feature Refinement Module (AEFRM), Modality-Adaptive Recalibration Module (MARM), and Multi-Head Attention Gated Fusion Module (MGFM) for feature alignment and fusion.

Result: Achieves state-of-the-art performance on DDD17-Semantic and DSEC-Semantic datasets.

Conclusion: EIFNet effectively addresses the challenges of event-based semantic segmentation, demonstrating robust performance.

Abstract: Event-based semantic segmentation explores the potential of event cameras,
which offer high dynamic range and fine temporal resolution, to achieve robust
scene understanding in challenging environments. Despite these advantages, the
task remains difficult due to two main challenges: extracting reliable features
from sparse and noisy event streams, and effectively fusing them with dense,
semantically rich image data that differ in structure and representation. To
address these issues, we propose EIFNet, a multi-modal fusion network that
combines the strengths of both event and frame-based inputs. The network
includes an Adaptive Event Feature Refinement Module (AEFRM), which improves
event representations through multi-scale activity modeling and spatial
attention. In addition, we introduce a Modality-Adaptive Recalibration Module
(MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align and
integrate features across modalities using attention mechanisms and gated
fusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasets
show that EIFNet achieves state-of-the-art performance, demonstrating its
effectiveness in event-based semantic segmentation.

</details>


### [136] [Motion Matters: Motion-guided Modulation Network for Skeleton-based Micro-Action Recognition](https://arxiv.org/abs/2507.21977)
*Jihao Gu,Kun Li,Fei Wang,Yanyan Wei,Zhiliang Wu,Hehe Fan,Meng Wang*

Main category: cs.CV

TL;DR: The paper introduces a Motion-guided Modulation Network (MMN) to improve Micro-Action Recognition by capturing subtle motion cues, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook subtle changes in Micro-Actions (MAs), limiting recognition accuracy.

Method: Proposes MMN with Motion-guided Skeletal Modulation (MSM) and Temporal Modulation (MTM) modules to enhance spatial-temporal representation learning.

Result: MMN outperforms on Micro-Action 52 and iMiGUE datasets, demonstrating superior performance.

Conclusion: Explicitly modeling subtle motion cues is crucial for accurate micro-action recognition.

Abstract: Micro-Actions (MAs) are an important form of non-verbal communication in
social interactions, with potential applications in human emotional analysis.
However, existing methods in Micro-Action Recognition often overlook the
inherent subtle changes in MAs, which limits the accuracy of distinguishing MAs
with subtle changes. To address this issue, we present a novel Motion-guided
Modulation Network (MMN) that implicitly captures and modulates subtle motion
cues to enhance spatial-temporal representation learning. Specifically, we
introduce a Motion-guided Skeletal Modulation module (MSM) to inject motion
cues at the skeletal level, acting as a control signal to guide spatial
representation modeling. In parallel, we design a Motion-guided Temporal
Modulation module (MTM) to incorporate motion information at the frame level,
facilitating the modeling of holistic motion patterns in micro-actions.
Finally, we propose a motion consistency learning strategy to aggregate the
motion cues from multi-scale features for micro-action classification.
Experimental results on the Micro-Action 52 and iMiGUE datasets demonstrate
that MMN achieves state-of-the-art performance in skeleton-based micro-action
recognition, underscoring the importance of explicitly modeling subtle motion
cues. The code will be available at https://github.com/momiji-bit/MMN.

</details>


### [137] [ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models](https://arxiv.org/abs/2507.21985)
*Hyun Jun Yook,Ga San Jhun,Jae Hyun Cho,Min Jeon,Donghyun Kim,Tae Hyung Kim,Youn Kyu Lee*

Main category: cs.CV

TL;DR: ZIUM is a zero-shot intent-aware adversarial attack method for unlearned models, improving attack success rates and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Adversarial prompts can exploit unlearned models to generate removed content, posing security risks. Existing methods are inefficient and costly.

Method: Proposes ZIUM, which customizes target attack images to reflect intent and supports zero-shot attacks without optimization.

Result: ZIUM achieves higher attack success rates and significantly reduces attack time for previously attacked concepts.

Conclusion: ZIUM effectively addresses challenges in adversarial attacks on unlearned models, offering intent-aware customization and efficiency.

Abstract: Machine unlearning (MU) removes specific data points or concepts from deep
learning models to enhance privacy and prevent sensitive content generation.
Adversarial prompts can exploit unlearned models to generate content containing
removed concepts, posing a significant security risk. However, existing
adversarial attack methods still face challenges in generating content that
aligns with an attacker's intent while incurring high computational costs to
identify successful prompts. To address these challenges, we propose ZIUM, a
Zero-shot Intent-aware adversarial attack on Unlearned Models, which enables
the flexible customization of target attack images to reflect an attacker's
intent. Additionally, ZIUM supports zero-shot adversarial attacks without
requiring further optimization for previously attacked unlearned concepts. The
evaluation across various MU scenarios demonstrated ZIUM's effectiveness in
successfully customizing content based on user-intent prompts while achieving a
superior attack success rate compared to existing methods. Moreover, its
zero-shot adversarial attack significantly reduces the attack time for
previously attacked unlearned concepts.

</details>


### [138] [Staining and locking computer vision models without retraining](https://arxiv.org/abs/2507.22000)
*Oliver J. Sutton,Qinghua Zhou,George Leete,Alexander N. Gorban,Ivan Y. Tyukin*

Main category: cs.CV

TL;DR: New methods for staining (watermarking) and locking computer vision models to protect intellectual property, without retraining, with provable guarantees.


<details>
  <summary>Details</summary>
Motivation: To safeguard model owners' intellectual property by embedding identifiable features (staining) and restricting usage (locking) without performance loss.

Method: Directly modify a small number of model weights to embed stains or locks; unlock models with a trigger patch in input images.

Result: Effective staining and locking with minimal performance impact, supported by experimental validation.

Conclusion: The methods offer practical, provable solutions for protecting pre-trained models without retraining.

Abstract: We introduce new methods of staining and locking computer vision models, to
protect their owners' intellectual property. Staining, also known as
watermarking, embeds secret behaviour into a model which can later be used to
identify it, while locking aims to make a model unusable unless a secret
trigger is inserted into input images. Unlike existing methods, our algorithms
can be used to stain and lock pre-trained models without requiring fine-tuning
or retraining, and come with provable, computable guarantees bounding their
worst-case false positive rates. The stain and lock are implemented by directly
modifying a small number of the model's weights and have minimal impact on the
(unlocked) model's performance. Locked models are unlocked by inserting a small
`trigger patch' into the corner of the input image. We present experimental
results showing the efficacy of our methods and demonstrating their practical
performance on a variety of computer vision models.

</details>


### [139] [Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation](https://arxiv.org/abs/2507.22002)
*Yida Tao,Yen-Chia Hsu*

Main category: cs.CV

TL;DR: CEDANet integrates citizen-provided weak labels with adversarial feature alignment for industrial smoke segmentation, achieving significant performance gains without costly annotations.


<details>
  <summary>Details</summary>
Motivation: High cost and scarcity of pixel-level annotations in industrial smoke segmentation hinder air-quality monitoring.

Method: CEDANet uses citizen feedback to refine pseudo-labels and employs class-specific domain discriminators for feature alignment.

Result: CEDANet achieves a five-fold F1-score increase (0.414 vs. 0.083) and six-fold smoke-class IoU improvement (0.261 vs. 0.043).

Conclusion: CEDANet offers a scalable, cost-efficient solution for environmental monitoring by combining citizen science with weakly supervised domain adaptation.

Abstract: Industrial smoke segmentation is critical for air-quality monitoring and
environmental protection but is often hampered by the high cost and scarcity of
pixel-level annotations in real-world settings. We introduce CEDANet, a
human-in-the-loop, class-aware domain adaptation framework that uniquely
integrates weak, citizen-provided video-level labels with adversarial feature
alignment. Specifically, we refine pseudo-labels generated by a source-trained
segmentation model using citizen votes, and employ class-specific domain
discriminators to transfer rich source-domain representations to the industrial
domain. Comprehensive experiments on SMOKE5K and custom IJmond datasets
demonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of
0.261 with citizen feedback, vastly outperforming the baseline model, which
scored 0.083 and 0.043 respectively. This represents a five-fold increase in
F1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with
citizen-constrained pseudo-labels achieves performance comparable to the same
architecture trained on limited 100 fully annotated images with F1-score of
0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully
supervised-level accuracy without target-domain annotations. Our research
validates the scalability and cost-efficiency of combining citizen science with
weakly supervised domain adaptation, offering a practical solution for complex,
data-scarce environmental monitoring applications.

</details>


### [140] [See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2507.22003)
*Ziyun Dai,Xiaoqiang Li,Shaohua Zhang,Yuanchen Wu,Jide Li*

Main category: cs.CV

TL;DR: ViHallu is a vision-centric framework to reduce hallucinations in LVLMs by improving visual-semantic alignment through visual variation images and instructions.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination mitigation methods are text-centric and struggle with fine-grained visual understanding due to poor visual-semantic alignment.

Method: ViHallu uses visual variation images with controllable alterations and constructed visual instructions to fine-tune LVLMs for better alignment.

Result: Experiments show ViHallu improves fine-grained visual understanding and reduces hallucinations. A dataset, ViHallu-Instruction, is also released.

Conclusion: ViHallu effectively addresses LVLM hallucinations by enhancing visual-semantic alignment, supported by empirical results and a new dataset.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable
capabilities in visual understanding and multimodal reasoning. However, LVLMs
frequently exhibit hallucination phenomena, manifesting as the generated
textual responses that demonstrate inconsistencies with the provided visual
content. Existing hallucination mitigation methods are predominantly
text-centric, the challenges of visual-semantic alignment significantly limit
their effectiveness, especially when confronted with fine-grained visual
understanding scenarios. To this end, this paper presents ViHallu, a
Vision-Centric Hallucination mitigation framework that enhances visual-semantic
alignment through Visual Variation Image Generation and Visual Instruction
Construction. ViHallu introduces \textbf{\textit{visual variation images}} with
controllable visual alterations while maintaining the overall image structure.
These images, combined with carefully constructed visual instructions, enable
LVLMs to better understand fine-grained visual content through fine-tuning,
allowing models to more precisely capture the correspondence between visual
content and text, thereby enhancing visual-semantic alignment. Extensive
experiments on multiple benchmarks show that ViHallu effectively enhances
models' fine-grained visual understanding while significantly reducing
hallucination tendencies. Furthermore, we release ViHallu-Instruction, a visual
instruction dataset specifically designed for hallucination mitigation and
visual-semantic alignment. Code is available at
https://github.com/oliviadzy/ViHallu.

</details>


### [141] [VeS: Teaching Pixels to Listen Without Supervision](https://arxiv.org/abs/2507.22008)
*Sajay Raj*

Main category: cs.CV

TL;DR: Dense audio-visual models perform well in multilingual, low-resource settings, with dense token routing outperforming global pooling by +59% R@1.


<details>
  <summary>Details</summary>
Motivation: To evaluate if dense AV models work in noisy, multilingual, low-resource settings, unlike English-centric, caption-rich scenarios.

Method: Compared three contrastive objectives on a multilingual dataset (Project Vaani): global mean-pooled loss, dense max-mean token matcher, and a hybrid.

Result: Dense objective improved R@1 by 59% over global pooling and produced sharp localization heatmaps without fine-tuning the vision backbone.

Conclusion: Dense token routing is crucial in low-resource settings, outperforming global pooling, and works without fine-tuning.

Abstract: Recent dense audio-visual (AV) models achieve impressive retrieval and
emergent localization, but almost all evidence comes from English-centric,
caption-rich web video. It is unclear whether these objectives survive in
low-resource, code-switched, and noisy multilingual settings that typify
developing regions. We show they do**-**and that the choice of aggregation
function becomes even more critical. Using a multilingual subset of Project
Vaani spanning dozens of Indian languages and dialectal variants, we compare
three contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii)
a dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid
(motivated by frozen-vision alignment strategies). The dense objective delivers
a +59% relative R@1 (Audio Visual) improvement over global pooling and
substantially lower mean/median ranks, while consistently producing sharp
zero-shot localization heatmaps of spoken objects-despite keeping the vision
backbone entirely frozen (no LoRA / partial fine-tuning). Our results
demonstrate that dense token routing is not a luxury of high-resource English
corpora; it is more decisive when annotations and acoustic cleanliness are
scarce. We release the codebase and trained models.

</details>


### [142] [XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation](https://arxiv.org/abs/2507.22020)
*Raju Ningappa Mulawade,Christoph Garth,Alexander Wiebel*

Main category: cs.CV

TL;DR: A novel XAI method for point cloud classification is proposed, using point-shifting perturbations and segmentation to generate human-interpretable saliency maps.


<details>
  <summary>Details</summary>
Motivation: Understanding AI decision-making in critical applications is crucial, especially for point cloud data. The goal is to produce easily understandable explanations.

Method: Uses point-shifting perturbations and segmentation models to create saliency maps, ensuring shifted points don't influence classification.

Result: Produces more meaningful saliency maps than classical clustering methods, as segments are human-interpretable.

Conclusion: The method effectively generates human-understandable explanations for point cloud classification, outperforming traditional approaches.

Abstract: We propose a novel segmentation-based explainable artificial intelligence
(XAI) method for neural networks working on point cloud classification. As one
building block of this method, we propose a novel point-shifting mechanism to
introduce perturbations in point cloud data. Recently, AI has seen an
exponential growth. Hence, it is important to understand the decision-making
process of AI algorithms when they are applied in critical areas. Our work
focuses on explaining AI algorithms that classify point cloud data. An
important aspect of the methods used for explaining AI algorithms is their
ability to produce explanations that are easy for humans to understand. This
allows them to analyze the AI algorithms better and make appropriate decisions
based on that analysis. Therefore, in this work, we intend to generate
meaningful explanations that can be easily interpreted by humans. The point
cloud data we consider represents 3D objects such as cars, guitars, and
laptops. We make use of point cloud segmentation models to generate
explanations for the working of classification models. The segments are used to
introduce perturbations into the input point cloud data and generate saliency
maps. The perturbations are introduced using the novel point-shifting mechanism
proposed in this work which ensures that the shifted points no longer influence
the output of the classification algorithm. In contrast to previous methods,
the segments used by our method are meaningful, i.e. humans can easily
interpret the meaning of the segments. Thus, the benefit of our method over
other methods is its ability to produce more meaningful saliency maps. We
compare our method with the use of classical clustering algorithms to generate
explanations. We also analyze the saliency maps generated for example inputs
using our method to demonstrate the usefulness of the method in generating
meaningful explanations.

</details>


### [143] [From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning](https://arxiv.org/abs/2507.22028)
*Honglin He,Yukai Ma,Wayne Wu,Bolei Zhou*

Main category: cs.CV

TL;DR: The paper introduces the Seeing-to-Experiencing (S2E) framework to enhance navigation foundation models by combining offline pre-training with reinforcement learning (RL), improving interactivity and safety in real-world urban navigation.


<details>
  <summary>Details</summary>
Motivation: Current navigation foundation models, trained on offline data, lack reasoning about action consequences and adaptability, limiting their real-world applicability in interactive and safe behaviors.

Method: S2E integrates pre-training on videos with RL post-training. Innovations include Anchor-Guided Distribution Matching for stable learning and a Residual-Attention Module to retain pretrained knowledge while acquiring reactive behaviors.

Result: S2E mitigates diminishing returns from offline data scaling, demonstrating improved generalizability and safety. RL outperforms Supervised Fine-Tuning in post-training for robot learning.

Conclusion: Integrating interactive online experiences is crucial for scaling foundation models in robotics, as shown by S2E's success in enhancing navigation capabilities.

Abstract: Navigation foundation models trained on massive webscale data enable agents
to generalize across diverse environments and embodiments. However, these
models trained solely on offline data, often lack the capacity to reason about
the consequences of their actions or adapt through counterfactual
understanding. They thus face significant limitations in the real-world urban
navigation where interactive and safe behaviors, such as avoiding obstacles and
moving pedestrians, are critical. To tackle these challenges, we introduce the
Seeing-to-Experiencing framework to scale the capability of navigation
foundation models with reinforcement learning. S2E combines the strengths of
pre-training on videos and post-training through RL. It maintains the
generalizability acquired from large-scale real-world videos while enhancing
its interactivity through RL in simulation environments. Specifically, we
introduce two innovations: an Anchor-Guided Distribution Matching strategy,
which stabilizes learning and models diverse motion patterns through
anchor-based supervision; and a Residual-Attention Module, which obtains
reactive behaviors from simulation environments without erasing the model's
pretrained knowledge. Moreover, we establish a comprehensive end-to-end
evaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructions
of real-world scenes that incorporate physical interactions. It can
systematically assess the generalizability and safety of navigation foundation
models. Extensive experiments show that S2E mitigates the diminishing returns
often seen when scaling with offline data alone. We perform a thorough analysis
of the benefits of Reinforcement Learning compared to Supervised Fine-Tuning in
the context of post-training for robot learning. Our findings emphasize the
crucial role of integrating interactive online experiences to effectively scale
foundation models in Robotics.

</details>


### [144] [Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning](https://arxiv.org/abs/2507.22041)
*Chaofei Qi,Chao Ye,Zhitai Liu,Weiyang Lin,Jianbin Qiu*

Main category: cs.CV

TL;DR: The paper explores whether shallow deep architectures like ConvNet-4 can match or outperform deeper backbones in fine-grained few-shot learning (FGFSL). It introduces LCN-4, a location-aware constellation network, which improves performance by addressing spatial feature fusion and positional information loss.


<details>
  <summary>Details</summary>
Motivation: Shallow backbones like ConvNet-4 are often overlooked due to their tendency to extract non-abstract visual attributes. The study re-evaluates their potential in FGFSL and aims to demonstrate their competitiveness with deeper architectures.

Method: The authors propose LCN-4, featuring a location-aware feature clustering module. It includes grid position encoding compensation and frequency domain location embedding to address positional information loss during feature extraction.

Result: LCN-4 outperforms ConvNet-4-based state-of-the-art methods and matches or surpasses ResNet12-based approaches on three FGFSL benchmarks.

Conclusion: Shallow architectures like LCN-4 can achieve competitive or superior performance in FGFSL, challenging the preference for deeper backbones.

Abstract: Deep learning has witnessed the extensive utilization across a wide spectrum
of domains, including fine-grained few-shot learning (FGFSL) which heavily
depends on deep backbones. Nonetheless, shallower deep backbones such as
ConvNet-4, are not commonly preferred because they're prone to extract a larger
quantity of non-abstract visual attributes. In this paper, we initially
re-evaluate the relationship between network depth and the ability to fully
encode few-shot instances, and delve into whether shallow deep architecture
could effectuate comparable or superior performance to mainstream deep
backbone. Fueled by the inspiration from vanilla ConvNet-4, we introduce a
location-aware constellation network (LCN-4), equipped with a cutting-edge
location-aware feature clustering module. This module can proficiently encoder
and integrate spatial feature fusion, feature clustering, and recessive feature
location, thereby significantly minimizing the overall loss. Specifically, we
innovatively put forward a general grid position encoding compensation to
effectively address the issue of positional information missing during the
feature extraction process of specific ordinary convolutions. Additionally, we
further propose a general frequency domain location embedding technique to
offset for the location loss in clustering features. We have carried out
validation procedures on three representative fine-grained few-shot benchmarks.
Relevant experiments have established that LCN-4 notably outperforms the
ConvNet-4 based State-of-the-Arts and achieves performance that is on par with
or superior to most ResNet12-based methods, confirming the correctness of our
conjecture.

</details>


### [145] [Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos](https://arxiv.org/abs/2507.22052)
*Ziren Gong,Xiaohan Li,Fabio Tosi,Jiawei Han,Stefano Mattoccia,Jianfei Cai,Matteo Poggi*

Main category: cs.CV

TL;DR: Ov3R is a framework for open-vocabulary 3D reconstruction from RGB videos, integrating CLIP semantics for improved geometry and semantic alignment.


<details>
  <summary>Details</summary>
Motivation: Advancing Spatial AI by enabling real-time, semantics-aware 3D reconstruction with fine-grained semantic alignment.

Method: Combines CLIP3R for CLIP-informed 3D reconstruction and 2D-3D OVS for lifting 2D features into 3D with fused descriptors.

Result: Achieves state-of-the-art performance in dense 3D reconstruction and open-vocabulary 3D segmentation.

Conclusion: Ov3R marks progress toward real-time, semantics-aware Spatial AI with globally consistent geometry and fine-grained semantics.

Abstract: We present Ov3R, a novel framework for open-vocabulary semantic 3D
reconstruction from RGB video streams, designed to advance Spatial AI. The
system features two key components: CLIP3R, a CLIP-informed 3D reconstruction
module that predicts dense point maps from overlapping clips while embedding
object-level semantics; and 2D-3D OVS, a 2D-3D open-vocabulary semantic module
that lifts 2D features into 3D by learning fused descriptors integrating
spatial, geometric, and semantic cues. Unlike prior methods, Ov3R incorporates
CLIP semantics directly into the reconstruction process, enabling globally
consistent geometry and fine-grained semantic alignment. Our framework achieves
state-of-the-art performance in both dense 3D reconstruction and
open-vocabulary 3D segmentation, marking a step forward toward real-time,
semantics-aware Spatial AI.

</details>


### [146] [MetaLab: Few-Shot Game Changer for Image Recognition](https://arxiv.org/abs/2507.22057)
*Chaofei Qi,Zhitai Liu,Jianbin Qiu*

Main category: cs.CV

TL;DR: Proposes MetaLab, a method for few-shot image recognition using CIELab color space and collaborative neural networks, achieving near-human accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the technical gaps in few-shot image recognition compared to large-scale recognition.

Method: Uses two networks: LabNet for domain transformation and feature extraction, and LabGNN for mutual learning between lightness and color graphs.

Result: Achieves ~99% accuracy on benchmarks, demonstrating robustness and generalization with minimal samples.

Conclusion: MetaLab effectively bridges the gap in few-shot recognition, nearing human-level performance.

Abstract: Difficult few-shot image recognition has significant application prospects,
yet remaining the substantial technical gaps with the conventional large-scale
image recognition. In this paper, we have proposed an efficient original method
for few-shot image recognition, called CIELab-Guided Coherent Meta-Learning
(MetaLab). Structurally, our MetaLab comprises two collaborative neural
networks: LabNet, which can perform domain transformation for the CIELab color
space and extract rich grouped features, and coherent LabGNN, which can
facilitate mutual learning between lightness graph and color graph. For
sufficient certification, we have implemented extensive comparative studies on
four coarse-grained benchmarks, four fine-grained benchmarks, and four
cross-domain few-shot benchmarks. Specifically, our method can achieve high
accuracy, robust performance, and effective generalization capability with
one-shot sample per class. Overall, all experiments have demonstrated that our
MetaLab can approach 99\% $\uparrow\downarrow$ accuracy, reaching the human
recognition ceiling with little visual deviation.

</details>


### [147] [X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again](https://arxiv.org/abs/2507.22058)
*Zigang Geng,Yibing Wang,Yeyao Ma,Chen Li,Yongming Rao,Shuyang Gu,Zhao Zhong,Qinglin Lu,Han Hu,Xiaosong Zhang,Linus,Di Wang,Jie Jiang*

Main category: cs.CV

TL;DR: The paper introduces X-Omni, a framework combining reinforcement learning with autoregressive modeling to improve image generation quality, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing issues like low fidelity and distorted outputs in autoregressive image generation, the paper aims to unify image and language generation.

Method: Uses reinforcement learning, a semantic image tokenizer, a unified autoregressive model, and an offline diffusion decoder (X-Omni).

Result: X-Omni achieves high-quality image generation with a 7B language model, excelling in instruction-following and text rendering.

Conclusion: Reinforcement learning effectively enhances autoregressive image generation, enabling seamless integration of image and language tasks.

Abstract: Numerous efforts have been made to extend the ``next token prediction''
paradigm to visual contents, aiming to create a unified approach for both image
generation and understanding. Nevertheless, attempts to generate images through
autoregressive modeling with discrete tokens have been plagued by issues such
as low visual fidelity, distorted outputs, and failure to adhere to complex
instructions when rendering intricate details. These shortcomings are likely
attributed to cumulative errors during autoregressive inference or information
loss incurred during the discretization process. Probably due to this
challenge, recent research has increasingly shifted toward jointly training
image generation with diffusion objectives and language generation with
autoregressive objectives, moving away from unified modeling approaches. In
this work, we demonstrate that reinforcement learning can effectively mitigate
artifacts and largely enhance the generation quality of a discrete
autoregressive modeling method, thereby enabling seamless integration of image
and language generation. Our framework comprises a semantic image tokenizer, a
unified autoregressive model for both language and images, and an offline
diffusion decoder for image generation, termed X-Omni. X-Omni achieves
state-of-the-art performance in image generation tasks using a 7B language
model, producing images with high aesthetic quality while exhibiting strong
capabilities in following instructions and rendering long texts.

</details>


### [148] [StepAL: Step-aware Active Learning for Cataract Surgical Videos](https://arxiv.org/abs/2507.22059)
*Nisarg A. Shah,Bardia Safaei,Shameema Sikder,S. Swaroop Vedula,Vishal M. Patel*

Main category: cs.CV

TL;DR: StepAL is an active learning framework for surgical video analysis, improving annotation efficiency by selecting full videos for labeling, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional AL methods for images or short clips are ineffective for surgical videos due to inter-step dependencies and lack of context.

Method: StepAL uses step-aware feature representation with pseudo-labels and entropy-weighted clustering to prioritize uncertain and diverse videos.

Result: StepAL outperforms existing AL methods on cataract surgery datasets, achieving higher accuracy with fewer labeled videos.

Conclusion: StepAL reduces annotation costs effectively for surgical video analysis, aiding computer-assisted surgical systems.

Abstract: Active learning (AL) can reduce annotation costs in surgical video analysis
while maintaining model performance. However, traditional AL methods, developed
for images or short video clips, are suboptimal for surgical step recognition
due to inter-step dependencies within long, untrimmed surgical videos. These
methods typically select individual frames or clips for labeling, which is
ineffective for surgical videos where annotators require the context of the
entire video for annotation. To address this, we propose StepAL, an active
learning framework designed for full video selection in surgical step
recognition. StepAL integrates a step-aware feature representation, which
leverages pseudo-labels to capture the distribution of predicted steps within
each video, with an entropy-weighted clustering strategy. This combination
prioritizes videos that are both uncertain and exhibit diverse step
compositions for annotation. Experiments on two cataract surgery datasets
(Cataract-1k and Cataract-101) demonstrate that StepAL consistently outperforms
existing active learning approaches, achieving higher accuracy in step
recognition with fewer labeled videos. StepAL offers an effective approach for
efficient surgical video analysis, reducing the annotation burden in developing
computer-assisted surgical systems.

</details>


### [149] [MOVE: Motion-Guided Few-Shot Video Object Segmentation](https://arxiv.org/abs/2507.22061)
*Kaining Ying,Hengrui Hu,Henghui Ding*

Main category: cs.CV

TL;DR: The paper introduces MOVE, a dataset for motion-guided few-shot video object segmentation (FSVOS), evaluates existing methods, and proposes DMA, a baseline method that outperforms others.


<details>
  <summary>Details</summary>
Motivation: Existing FSVOS datasets and methods focus on static object categories, ignoring motion dynamics, limiting their use in motion-understanding scenarios.

Method: Introduces MOVE dataset, evaluates 6 state-of-the-art methods, and proposes DMA, a Decoupled Motion Appearance Network.

Result: Current methods struggle with motion-guided FSVOS; DMA achieves superior performance in few-shot motion understanding.

Conclusion: DMA sets a foundation for future research in motion-guided FSVOS, addressing gaps in existing approaches.

Abstract: This work addresses motion-guided few-shot video object segmentation (FSVOS),
which aims to segment dynamic objects in videos based on a few annotated
examples with the same motion patterns. Existing FSVOS datasets and methods
typically focus on object categories, which are static attributes that ignore
the rich temporal dynamics in videos, limiting their application in scenarios
requiring motion understanding. To fill this gap, we introduce MOVE, a
large-scale dataset specifically designed for motion-guided FSVOS. Based on
MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different
related tasks across 2 experimental settings. Our results reveal that current
methods struggle to address motion-guided FSVOS, prompting us to analyze the
associated challenges and propose a baseline method, Decoupled Motion
Appearance Network (DMA). Experiments demonstrate that our approach achieves
superior performance in few shot motion understanding, establishing a solid
foundation for future research in this direction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [150] [Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas](https://arxiv.org/abs/2507.21103)
*Daniel Meireles do Rego*

Main category: cs.IR

TL;DR: The paper explores using RAG and LLMs to automate PDF document analysis, showing improved efficiency in unstructured data extraction.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of digital documents necessitates efficient methods for extracting and analyzing unstructured information.

Method: Combines RAG architectures with LLMs, integrating vector search, semantic extraction, and natural language response generation.

Result: Experiments with drug package inserts showed gains in accuracy, completeness, speed, and consistency in information retrieval.

Conclusion: RAG combined with LLMs enhances intelligent retrieval and interpretation of unstructured technical texts.

Abstract: The production of digital documents has been growing rapidly in academic,
business, and health environments, presenting new challenges in the efficient
extraction and analysis of unstructured information. This work investigates the
use of RAG (Retrieval-Augmented Generation) architectures combined with
Large-Scale Language Models (LLMs) to automate the analysis of documents in PDF
format. The proposal integrates vector search techniques by embeddings,
semantic data extraction and generation of contextualized natural language
responses. To validate the approach, we conducted experiments with drug package
inserts extracted from official public sources. The semantic queries applied
were evaluated by metrics such as accuracy, completeness, response speed and
consistency. The results indicate that the combination of RAG with LLMs offers
significant gains in intelligent information retrieval and interpretation of
unstructured technical texts.

</details>


### [151] [AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis](https://arxiv.org/abs/2507.21105)
*Callie C. Liao,Duoduo Liao,Sai Surya Gadiraju*

Main category: cs.IR

TL;DR: AgentMaster is a modular multi-protocol MAS framework integrating A2A and MCP for dynamic coordination and flexible communication, achieving high performance in tasks like information retrieval and image analysis.


<details>
  <summary>Details</summary>
Motivation: Address challenges in inter-agent communication, coordination, and interaction with heterogeneous tools in MAS, leveraging LLMs.

Method: Develop AgentMaster with self-implemented A2A and MCP protocols, enabling natural language interaction and multimodal query handling.

Result: Achieved 96.3% BERTScore F1 and 87.1% G-Eval, demonstrating robust coordination and domain-specific responses.

Conclusion: AgentMaster enhances domain-specific, cooperative, and scalable conversational AI powered by MAS.

Abstract: The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI),
especially integrated with Large Language Models (LLMs), has greatly
facilitated the resolution of complex tasks. However, current systems are still
facing challenges of inter-agent communication, coordination, and interaction
with heterogeneous tools and resources. Most recently, the Model Context
Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by
Google have been introduced, and to the best of our knowledge, very few
applications exist where both protocols are employed within a single MAS
framework. We present a pilot study of AgentMaster, a novel modular
multi-protocol MAS framework with self-implemented A2A and MCP, enabling
dynamic coordination and flexible communication. Through a unified
conversational interface, the system supports natural language interaction
without prior technical expertise and responds to multimodal queries for tasks
including information retrieval, question answering, and image analysis.
Evaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged
96.3\% and 87.1\%, revealing robust inter-agent coordination, query
decomposition, dynamic routing, and domain-specific, relevant responses.
Overall, our proposed framework contributes to the potential capabilities of
domain-specific, cooperative, and scalable conversational AI powered by MAS.

</details>


### [152] [Page image classification for content-specific data processing](https://arxiv.org/abs/2507.21114)
*Kateryna Lutsai,Pavel Straňák*

Main category: cs.IR

TL;DR: An AI-based image classification system for historical document pages to automate sorting and analysis.


<details>
  <summary>Details</summary>
Motivation: Manual sorting of diverse historical document content is inefficient; automation is needed for tailored analysis.

Method: Developed an image classification system using AI/ML to categorize pages by content type.

Result: Enables efficient processing by separating pages for content-specific analysis (e.g., OCR for text, image analysis for graphics).

Conclusion: The system addresses the challenge of heterogeneous historical document processing, improving efficiency and enabling tailored workflows.

Abstract: Digitization projects in humanities often generate vast quantities of page
images from historical documents, presenting significant challenges for manual
sorting and analysis. These archives contain diverse content, including various
text types (handwritten, typed, printed), graphical elements (drawings, maps,
photos), and layouts (plain text, tables, forms). Efficiently processing this
heterogeneous data requires automated methods to categorize pages based on
their content, enabling tailored downstream analysis pipelines. This project
addresses this need by developing and evaluating an image classification system
specifically designed for historical document pages, leveraging advancements in
artificial intelligence and machine learning. The set of categories was chosen
to facilitate content-specific processing workflows, separating pages requiring
different analysis techniques (e.g., OCR for text, image analysis for graphics)

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [153] [Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition](https://arxiv.org/abs/2507.21610)
*Ruiyang Hao,Haibao Yu,Jiaru Zhong,Chuanye Wang,Jiahao Wang,Yiming Kan,Wenxian Yang,Siqi Fan,Huilin Yin,Jianing Qiu,Yao Mu,Jiankai Sun,Li Chen,Walter Zimmer,Dandan Zhang,Shanghang Zhang,Mac Schwager,Wei Huang,Xiaobo Zhang,Ping Luo,Zaiqing Nie*

Main category: cs.RO

TL;DR: The paper discusses a challenge focused on V2X-cooperative autonomous driving, addressing technical hurdles like bandwidth-aware fusion and multi-agent planning.


<details>
  <summary>Details</summary>
Motivation: To advance V2X communication for autonomous driving by tackling real-world constraints like limited bandwidth and dynamic environments.

Method: Organized a challenge with two tracks (cooperative temporal perception and end-to-end planning) using the UniV2X framework and V2X-Seq-SPD dataset.

Result: Over 30 teams participated, establishing a benchmark for cooperative driving systems and highlighting key research problems.

Conclusion: The challenge aids in developing scalable, reliable V2X-cooperative autonomous driving systems by addressing practical constraints.

Abstract: With the rapid advancement of autonomous driving technology,
vehicle-to-everything (V2X) communication has emerged as a key enabler for
extending perception range and enhancing driving safety by providing visibility
beyond the line of sight. However, integrating multi-source sensor data from
both ego-vehicles and infrastructure under real-world constraints, such as
limited communication bandwidth and dynamic environments, presents significant
technical challenges. To facilitate research in this area, we organized the
End-to-End Autonomous Driving through V2X Cooperation Challenge, which features
two tracks: cooperative temporal perception and cooperative end-to-end
planning. Built on the UniV2X framework and the V2X-Seq-SPD dataset, the
challenge attracted participation from over 30 teams worldwide and established
a unified benchmark for evaluating cooperative driving systems. This paper
describes the design and outcomes of the challenge, highlights key research
problems including bandwidth-aware fusion, robust multi-agent planning, and
heterogeneous sensor integration, and analyzes emerging technical trends among
top-performing solutions. By addressing practical constraints in communication
and data fusion, the challenge contributes to the development of scalable and
reliable V2X-cooperative autonomous driving systems.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [154] [Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation](https://arxiv.org/abs/2507.21903)
*Tiviatis Sim,Kaiwen Yang,Shen Xin,Kenji Kawaguchi*

Main category: cs.SI

TL;DR: SUnSET introduces a novel framework for timeline summarization by leveraging stakeholder analysis and LLMs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing news summarization methods lack stakeholder analysis, limiting their effectiveness in tracking related events across sources.

Method: Uses LLMs to build SET triplets (Stakeholder, Events, Time) and introduces stakeholder-based ranking for a Relevancy metric.

Result: Outperforms prior baselines, achieving State-of-the-Art performance by incorporating stakeholder information.

Conclusion: Stakeholder analysis significantly improves timeline summarization, as demonstrated by SUnSET's success.

Abstract: As news reporting becomes increasingly global and decentralized online,
tracking related events across multiple sources presents significant
challenges. Existing news summarization methods typically utilizes Large
Language Models and Graphical methods on article-based summaries. However, this
is not effective since it only considers the textual content of similarly dated
articles to understand the gist of the event. To counteract the lack of
analysis on the parties involved, it is essential to come up with a novel
framework to gauge the importance of stakeholders and the connection of related
events through the relevant entities involved. Therefore, we present SUnSET:
Synergistic Understanding of Stakeholder, Events and Time for the task of
Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)
to build SET triplets and introduced the use of stakeholder-based ranking to
construct a $Relevancy$ metric, which can be extended into general situations.
Our experimental results outperform all prior baselines and emerged as the new
State-of-the-Art, highlighting the impact of stakeholder information within
news article.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [155] [OneShield -- the Next Generation of LLM Guardrails](https://arxiv.org/abs/2507.21170)
*Chad DeLuca,Anna Lisa Gentile,Shubhi Asthana,Bing Zhang,Pawan Chowdhary,Kellen Cheng,Basel Shbita,Pengyuan Li,Guang-Jie Ren,Sandeep Gopisetty*

Main category: cs.CR

TL;DR: OneShield is a standalone, model-agnostic solution designed to safeguard Large Language Models (LLMs) by addressing safety, privacy, and ethics concerns through customizable risk mitigation and policy enforcement.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of LLMs has raised concerns about safety, privacy, and ethics, necessitating flexible and scalable protective measures.

Method: Proposes OneShield, a framework for defining risk factors, expressing safety policies, and mitigating risks tailored to specific customers.

Result: Implementation details, scalability considerations, and usage statistics of OneShield are provided.

Conclusion: OneShield offers a customizable and scalable solution to address the evolving risks of LLMs, catering to individual customer needs.

Abstract: The rise of Large Language Models has created a general excitement about the
great potential for a myriad of applications. While LLMs offer many
possibilities, questions about safety, privacy, and ethics have emerged, and
all the key actors are working to address these issues with protective measures
for their own models and standalone solutions. The constantly evolving nature
of LLMs makes the task of universally shielding users against their potential
risks extremely challenging, and one-size-fits-all solutions unfeasible. In
this work, we propose OneShield, our stand-alone, model-agnostic and
customizable solution to safeguard LLMs. OneShield aims to provide facilities
for defining risk factors, expressing and declaring contextual safety and
compliance policies, and mitigating LLM risks, with a focus on each specific
customer. We describe the implementation of the framework, the scalability
considerations and provide usage statistics of OneShield since its first
deployment.

</details>


### [156] [Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution](https://arxiv.org/abs/2507.20650)
*Zhicheng Zhang,Peizhuo Lv,Mengke Wan,Jiang Fang,Diandian Guo,Yezeng Chen,Yinlong Liu,Wei Ma,Jiyan Sun,Liru Geng*

Main category: cs.CR

TL;DR: Proposes Hot-Swap MarkBoard for efficient, customizable watermarking in on-device AI to protect IP without retraining.


<details>
  <summary>Details</summary>
Motivation: Addresses IP risks in distributed on-device AI by enabling unique, customizable watermarks for each user-specific model instance.

Method: Uses multi-branch LoRA module for embedding n-bit binary signatures, allowing watermark customization via branch swapping and parameter obfuscation.

Result: Achieves 100% verification accuracy across diverse tasks and models, outperforming existing methods.

Conclusion: Hot-Swap MarkBoard is efficient, adaptable, and effective for protecting AI models in large-scale distribution scenarios.

Abstract: Recently, Deep Learning (DL) models have been increasingly deployed on
end-user devices as On-Device AI, offering improved efficiency and privacy.
However, this deployment trend poses more serious Intellectual Property (IP)
risks, as models are distributed on numerous local devices, making them
vulnerable to theft and redistribution. Most existing ownership protection
solutions (e.g., backdoor-based watermarking) are designed for cloud-based
AI-as-a-Service (AIaaS) and are not directly applicable to large-scale
distribution scenarios, where each user-specific model instance must carry a
unique watermark. These methods typically embed a fixed watermark, and
modifying the embedded watermark requires retraining the model. To address
these challenges, we propose Hot-Swap MarkBoard, an efficient watermarking
method. It encodes user-specific $n$-bit binary signatures by independently
embedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA)
module, enabling efficient watermark customization without retraining through
branch swapping. A parameter obfuscation mechanism further entangles the
watermark weights with those of the base model, preventing removal without
degrading model performance. The method supports black-box verification and is
compatible with various model architectures and DL tasks, including
classification, image generation, and text generation. Extensive experiments
across three types of tasks and six backbone models demonstrate our method's
superior efficiency and adaptability compared to existing approaches, achieving
100\% verification accuracy.

</details>


### [157] [Unmasking Synthetic Realities in Generative AI: A Comprehensive Review of Adversarially Robust Deepfake Detection Systems](https://arxiv.org/abs/2507.21157)
*Naseem Khan,Tuan Nguyen,Amine Bermak,Issa Khalil*

Main category: cs.CR

TL;DR: This systematic review evaluates deepfake detection methods, highlighting their precision but noting vulnerabilities to adversarial attacks. It calls for future work on adversarial resilience.


<details>
  <summary>Details</summary>
Motivation: The rise of deepfakes challenges digital security and misinformation, necessitating robust detection methods.

Method: The review categorizes two paradigms: detecting fully synthetic media and localizing manipulated regions, using statistical anomalies, feature extraction, and multi-modal cues.

Result: Current methods show precision in controlled settings but lack adversarial robustness, making them unreliable in real-world scenarios.

Conclusion: Future research must prioritize adversarial resilience and scalable, modality-agnostic architectures for trustworthy deepfake detection.

Abstract: The rapid advancement of Generative Artificial Intelligence has fueled
deepfake proliferation-synthetic media encompassing fully generated content and
subtly edited authentic material-posing challenges to digital security,
misinformation mitigation, and identity preservation. This systematic review
evaluates state-of-the-art deepfake detection methodologies, emphasizing
reproducible implementations for transparency and validation. We delineate two
core paradigms: (1) detection of fully synthetic media leveraging statistical
anomalies and hierarchical feature extraction, and (2) localization of
manipulated regions within authentic content employing multi-modal cues such as
visual artifacts and temporal inconsistencies. These approaches, spanning
uni-modal and multi-modal frameworks, demonstrate notable precision and
adaptability in controlled settings, effectively identifying manipulations
through advanced learning techniques and cross-modal fusion. However,
comprehensive assessment reveals insufficient evaluation of adversarial
robustness across both paradigms. Current methods exhibit vulnerability to
adversarial perturbations-subtle alterations designed to evade
detection-undermining reliability in real-world adversarial contexts. This gap
highlights critical disconnect between methodological development and evolving
threat landscapes. To address this, we contribute a curated GitHub repository
aggregating open-source implementations, enabling replication and testing. Our
findings emphasize urgent need for future work prioritizing adversarial
resilience, advocating scalable, modality-agnostic architectures capable of
withstanding sophisticated manipulations. This review synthesizes strengths and
shortcomings of contemporary deepfake detection while charting paths toward
robust trustworthy systems.

</details>


### [158] [PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking](https://arxiv.org/abs/2507.21540)
*Quanchen Zou,Zonghao Ying,Moyang Chen,Wenzhuo Xu,Yisong Xiao,Yakai Li,Deyue Zhang,Dongdong Yang,Zhao Liu,Xiangzheng Zhang*

Main category: cs.CR

TL;DR: A novel jailbreak framework for LVLMs, inspired by ROP, decomposes harmful instructions into benign visual gadgets, achieving high attack success rates and revealing vulnerabilities in compositional reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing safety defenses in LVLMs are vulnerable to sophisticated adversarial attacks, especially those exploiting compositional reasoning.

Method: Proposes a jailbreak framework using ROP techniques, breaking harmful instructions into benign visual gadgets guided by a textual prompt.

Result: Achieves near-perfect attack success rates (over 0.90 on SafeBench) and improves ASR by up to 0.39.

Conclusion: Highlights a critical vulnerability in LVLMs' compositional reasoning, urging defenses for the entire reasoning process.

Abstract: The increasing sophistication of large vision-language models (LVLMs) has
been accompanied by advances in safety alignment mechanisms designed to prevent
harmful content generation. However, these defenses remain vulnerable to
sophisticated adversarial attacks. Existing jailbreak methods typically rely on
direct and semantically explicit prompts, overlooking subtle vulnerabilities in
how LVLMs compose information over multiple reasoning steps. In this paper, we
propose a novel and effective jailbreak framework inspired by Return-Oriented
Programming (ROP) techniques from software security. Our approach decomposes a
harmful instruction into a sequence of individually benign visual gadgets. A
carefully engineered textual prompt directs the sequence of inputs, prompting
the model to integrate the benign visual gadgets through its reasoning process
to produce a coherent and harmful output. This makes the malicious intent
emergent and difficult to detect from any single component. We validate our
method through extensive experiments on established benchmarks including
SafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our
approach consistently and substantially outperforms existing baselines on
state-of-the-art models, achieving near-perfect attack success rates (over 0.90
on SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical
and underexplored vulnerability that exploits the compositional reasoning
abilities of LVLMs, highlighting the urgent need for defenses that secure the
entire reasoning process.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [159] [A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling](https://arxiv.org/abs/2507.21100)
*Wei Meng*

Main category: cs.CY

TL;DR: TACTIC-GRAPHS combines spectral graph theory and multimodal GNNs for threat detection in noisy tactical video, achieving high accuracy in temporal alignment and threat recognition.


<details>
  <summary>Details</summary>
Motivation: To address semantic understanding and threat detection in tactical video with high noise and weak structure.

Method: Uses spectral embedding, temporal causal edge modeling, discriminative path inference, and semantic-aware keyframe extraction across visual, acoustic, and action modalities.

Result: Achieves 89.3% accuracy in temporal alignment and over 85% threat chain recognition, with node latency under ±150ms.

Conclusion: The system improves structural interpretability and is applicable to surveillance, defense, and security.

Abstract: This paper introduces TACTIC-GRAPHS, a system that combines spectral graph
theory and multimodal graph neural reasoning for semantic understanding and
threat detection in tactical video under high noise and weak structure. The
framework incorporates spectral embedding, temporal causal edge modeling, and
discriminative path inference across heterogeneous modalities. A semantic-aware
keyframe extraction method fuses visual, acoustic, and action cues to construct
temporal graphs. Using graph attention and Laplacian spectral mapping, the
model performs cross-modal weighting and causal signal analysis. Experiments on
TACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal
alignment and over 85 percent recognition of complete threat chains, with node
latency within plus-minus 150 milliseconds. The approach enhances structural
interpretability and supports applications in surveillance, defense, and
intelligent security systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [160] [Comparative Analysis of Vision Transformers and Convolutional Neural Networks for Medical Image Classification](https://arxiv.org/abs/2507.21156)
*Kunal Kawadkar*

Main category: eess.IV

TL;DR: The study compares CNN and ViT models for medical imaging tasks, showing task-specific performance advantages.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of ViTs versus CNNs in medical imaging, an area where ViTs are under-explored.

Method: Comparative analysis of ResNet-50, EfficientNet-B0, ViT-Base, and DeiT-Small on three medical imaging tasks using 8,469 images.

Result: ResNet-50 excelled in chest X-ray (98.37%), DeiT-Small in brain tumor (92.16%), and EfficientNet-B0 in skin cancer (81.84%).

Conclusion: Task-specific architecture selection is crucial for medical AI, with no single model outperforming all others.

Abstract: The emergence of Vision Transformers (ViTs) has revolutionized computer
vision, yet their effectiveness compared to traditional Convolutional Neural
Networks (CNNs) in medical imaging remains under-explored. This study presents
a comprehensive comparative analysis of CNN and ViT architectures across three
critical medical imaging tasks: chest X-ray pneumonia detection, brain tumor
classification, and skin cancer melanoma detection. We evaluated four
state-of-the-art models - ResNet-50, EfficientNet-B0, ViT-Base, and DeiT-Small
- across datasets totaling 8,469 medical images. Our results demonstrate
task-specific model advantages: ResNet-50 achieved 98.37% accuracy on chest
X-ray classification, DeiT-Small excelled at brain tumor detection with 92.16%
accuracy, and EfficientNet-B0 led skin cancer classification at 81.84%
accuracy. These findings provide crucial insights for practitioners selecting
architectures for medical AI applications, highlighting the importance of
task-specific architecture selection in clinical decision support systems.

</details>


### [161] [Querying GI Endoscopy Images: A VQA Approach](https://arxiv.org/abs/2507.21165)
*Gaurav Parajuli*

Main category: eess.IV

TL;DR: The paper explores adapting the Florence2 model for medical VQA tasks, specifically for GI endoscopy images, to improve diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs perform poorly in specialized domains like medical imaging, despite their potential for aiding clinical diagnoses.

Method: Adaptation of the Florence2 model for medical VQA tasks, evaluated using ROUGE, BLEU, and METEOR metrics.

Result: Performance of the adapted Florence2 model is assessed, though specific results are not detailed in the abstract.

Conclusion: The study highlights the potential of adapting general-domain models like Florence2 for specialized medical VQA applications.

Abstract: VQA (Visual Question Answering) combines Natural Language Processing (NLP)
with image understanding to answer questions about a given image. It has
enormous potential for the development of medical diagnostic AI systems. Such a
system can help clinicians diagnose gastro-intestinal (GI) diseases accurately
and efficiently. Although many of the multimodal LLMs available today have
excellent VQA capabilities in the general domain, they perform very poorly for
VQA tasks in specialized domains such as medical imaging. This study is a
submission for ImageCLEFmed-MEDVQA-GI 2025 subtask 1 that explores the
adaptation of the Florence2 model to answer medical visual questions on GI
endoscopy images. We also evaluate the model performance using standard metrics
like ROUGE, BLEU and METEOR

</details>


### [162] [ST-DAI: Single-shot 2.5D Spatial Transcriptomics with Intra-Sample Domain Adaptive Imputation for Cost-efficient 3D Reconstruction](https://arxiv.org/abs/2507.21516)
*Jiahe Qian,Yaoyu Fang,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: eess.IV

TL;DR: ST-DAI is a cost-efficient 3D spatial transcriptomics framework using 2.5D sampling and intra-sample domain-adaptive imputation to reduce experimental costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: High costs and domain discrepancies in fully sampling 3D spatial transcriptomics (ST) datasets motivate the need for a cost-efficient and generalizable solution.

Method: ST-DAI combines 2.5D sampling (fully sampling a central section and sparsely sampling adjacent sections) with intra-sample domain-adaptive imputation, including alignment, pseudo-supervision, Fast Multi-Domain Refinement (FMDR), and confidence-based reweighting.

Result: ST-DAI achieves gene expression prediction performance comparable to fully sampled approaches while significantly reducing measurement burden.

Conclusion: ST-DAI offers a practical and efficient solution for 3D ST by balancing cost and accuracy through innovative sampling and domain adaptation.

Abstract: For 3D spatial transcriptomics (ST), the high per-section acquisition cost of
fully sampling every tissue section remains a significant challenge. Although
recent approaches predict gene expression from histology images, these methods
require large external datasets, which leads to high-cost and suffers from
substantial domain discrepancies that lead to poor generalization on new
samples. In this work, we introduce ST-DAI, a single-shot framework for 3D ST
that couples a cost-efficient 2.5D sampling scheme with an intra-sample
domain-adaptive imputation framework. First, in the cost-efficient 2.5D
sampling stage, one reference section (central section) is fully sampled while
other sections (adjacent sections) is sparsely sampled, thereby capturing
volumetric context at significantly reduced experimental cost. Second, we
propose a single-shot 3D imputation learning method that allows us to generate
fully sampled 3D ST from this cost-efficient 2.5D ST scheme, using only
sample-specific training. We observe position misalignment and domain
discrepancy between sections. To address those issues, we adopt a pipeline that
first aligns the central section to the adjacent section, thereafter generates
dense pseudo-supervision on the central section, and then performs Fast
Multi-Domain Refinement (FMDR), which adapts the network to the domain of the
adjacent section while fine-tuning only a few parameters through the use of
Parameter-Efficient Domain-Alignment Layers (PDLs). During this refinement, a
Confidence Score Generator (CSG) reweights the pseudo-labels according to their
estimated reliability, thereby directing imputation toward trustworthy regions.
Our experimental results demonstrate that ST-DAI achieves gene expression
prediction performance comparable to fully sampled approaches while
substantially reducing the measurement burden.

</details>


### [163] [VidFuncta: Towards Generalizable Neural Representations for Ultrasound Videos](https://arxiv.org/abs/2507.21863)
*Julia Wolleb,Florentin Bieder,Paul Friedrich,Hemant D. Tagare,Xenophon Papademetris*

Main category: eess.IV

TL;DR: VidFuncta, a novel framework using implicit neural representations (INRs), encodes ultrasound videos into compact, time-resolved representations, outperforming 2D/3D baselines and enabling efficient downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Standard deep learning struggles with ultrasound video analysis due to non-standardized acquisition and operator bias. VidFuncta addresses this gap by extending Functa to the temporal domain.

Method: VidFuncta disentangles videos into static video-specific vectors and time-dependent modulation vectors, capturing temporal dynamics and redundancies.

Result: Outperforms 2D/3D baselines in reconstruction and enables tasks like ejection fraction prediction, B-line detection, and breast lesion classification.

Conclusion: VidFuncta is a generalizable, efficient framework for ultrasound video analysis, validated on cardiac, lung, and breast datasets.

Abstract: Ultrasound is widely used in clinical care, yet standard deep learning
methods often struggle with full video analysis due to non-standardized
acquisition and operator bias. We offer a new perspective on ultrasound video
analysis through implicit neural representations (INRs). We build on Functa, an
INR framework in which each image is represented by a modulation vector that
conditions a shared neural network. However, its extension to the temporal
domain of medical videos remains unexplored. To address this gap, we propose
VidFuncta, a novel framework that leverages Functa to encode variable-length
ultrasound videos into compact, time-resolved representations. VidFuncta
disentangles each video into a static video-specific vector and a sequence of
time-dependent modulation vectors, capturing both temporal dynamics and
dataset-level redundancies. Our method outperforms 2D and 3D baselines on video
reconstruction and enables downstream tasks to directly operate on the learned
1D modulation vectors. We validate VidFuncta on three public ultrasound video
datasets -- cardiac, lung, and breast -- and evaluate its downstream
performance on ejection fraction prediction, B-line detection, and breast
lesion classification. These results highlight the potential of VidFuncta as a
generalizable and efficient representation framework for ultrasound videos. Our
code is publicly available under
https://github.com/JuliaWolleb/VidFuncta_public.

</details>


### [164] [Cyst-X: AI-Powered Pancreatic Cancer Risk Prediction from Multicenter MRI in Centralized and Federated Learning](https://arxiv.org/abs/2507.22017)
*Hongyi Pan,Gorkem Durak,Elif Keles,Deniz Seyithanoglu,Zheyuan Zhang,Alpay Medetalibeyoglu,Halil Ertugrul Aktas,Andrea Mia Bejar,Ziliang Hong,Yavuz Taktak,Gulbiz Dagoglu Kartal,Mehmet Sukru Erturk,Timurhan Cebeci,Maria Jaramillo Gonzalez,Yury Velichko,Lili Zhao,Emil Agarunov,Federica Proietto Salanitri,Concetto Spampinato,Pallavi Tiwari,Ziyue Xu,Sachin Jambawalikar,Ivo G. Schoots,Marco J. Bruno,Chenchang Huang,Candice Bolan,Tamas Gonda,Frank H. Miller,Rajesh N. Keswani,Michael B. Wallace,Ulas Bagci*

Main category: eess.IV

TL;DR: Cyst-X, an AI framework, predicts IPMN malignancy using MRI data, outperforming current guidelines and radiologists, and supports federated learning for privacy-preserving collaboration.


<details>
  <summary>Details</summary>
Motivation: Pancreatic cancer's rising mortality and the limitations of current IPMN assessment methods necessitate better early detection tools.

Method: Cyst-X leverages multicenter MRI data (723 T1- and 738 T2-weighted scans from 764 patients) to predict IPMN malignancy, using AI models.

Result: Cyst-X achieves AUC=0.82, surpassing Kyoto guidelines (AUC=0.75) and expert radiologists, with biologically meaningful insights.

Conclusion: Cyst-X improves IPMN risk stratification, supports federated learning, and releases a large-scale dataset to advance privacy-preserving AI development.

Abstract: Pancreatic cancer is projected to become the second-deadliest malignancy in
Western countries by 2030, highlighting the urgent need for better early
detection. Intraductal papillary mucinous neoplasms (IPMNs), key precursors to
pancreatic cancer, are challenging to assess with current guidelines, often
leading to unnecessary surgeries or missed malignancies. We present Cyst-X, an
AI framework that predicts IPMN malignancy using multicenter MRI data,
leveraging MRI's superior soft tissue contrast over CT. Trained on 723 T1- and
738 T2-weighted scans from 764 patients across seven institutions, our models
(AUC=0.82) significantly outperform both Kyoto guidelines (AUC=0.75) and expert
radiologists. The AI-derived imaging features align with known clinical markers
and offer biologically meaningful insights. We also demonstrate strong
performance in a federated learning setting, enabling collaborative training
without sharing patient data. To promote privacy-preserving AI development and
improve IPMN risk stratification, the Cyst-X dataset is released as the first
large-scale, multi-center pancreatic cysts MRI dataset.

</details>


### [165] [Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT Images](https://arxiv.org/abs/2507.22024)
*Yutao Hu,Ying Zheng,Shumei Miao,Xiaolei Zhang,Jiahao Xia,Yaolei Qi,Yiyang Zhang,Yuting He,Qian Chen,Jing Ye,Hongyan Qiao,Xiuhua Hu,Lei Xu,Jiayin Zhang,Hui Liu,Minwen Zheng,Yining Wang,Daimin Zhang,Ji Zhang,Wenqi Shao,Yun Liu,Longjiang Zhang,Guanyu Yang*

Main category: eess.IV

TL;DR: Cardiac-CLIP is a multi-modal foundation model for 3D cardiac CT images, developed via a two-stage pre-training strategy. It achieves state-of-the-art performance in cardiovascular diagnostics.


<details>
  <summary>Details</summary>
Motivation: The application of foundation models to complex cardiovascular diagnostics is underexplored, despite their potential in the medical domain.

Method: A two-stage pre-training strategy: 1) self-supervised learning with a 3D masked autoencoder (MAE), and 2) contrastive learning to align visual and textual representations. Data includes 16,641 clinical CT scans and 114k public data.

Result: Cardiac-CLIP achieves state-of-the-art performance in tasks like abnormality classification, information retrieval, and clinical analysis, including predicting acute coronary syndrome.

Conclusion: Cardiac-CLIP demonstrates significant potential for complex cardiovascular diagnostics, outperforming existing methods in real-world clinical tasks.

Abstract: Foundation models have demonstrated remarkable potential in medical domain.
However, their application to complex cardiovascular diagnostics remains
underexplored. In this paper, we present Cardiac-CLIP, a multi-modal foundation
model designed for 3D cardiac CT images. Cardiac-CLIP is developed through a
two-stage pre-training strategy. The first stage employs a 3D masked
autoencoder (MAE) to perform self-supervised representation learning from
large-scale unlabeled volumetric data, enabling the visual encoder to capture
rich anatomical and contextual features. In the second stage, contrastive
learning is introduced to align visual and textual representations,
facilitating cross-modal understanding. To support the pre-training, we collect
16641 real clinical CT scans, supplemented by 114k publicly available data.
Meanwhile, we standardize free-text radiology reports into unified templates
and construct the pathology vectors according to diagnostic attributes, based
on which the soft-label matrix is generated to supervise the contrastive
learning process. On the other hand, to comprehensively evaluate the
effectiveness of Cardiac-CLIP, we collect 6,722 real-clinical data from 12
independent institutions, along with the open-source data to construct the
evaluation dataset. Specifically, Cardiac-CLIP is comprehensively evaluated
across multiple tasks, including cardiovascular abnormality classification,
information retrieval and clinical analysis. Experimental results demonstrate
that Cardiac-CLIP achieves state-of-the-art performance across various
downstream tasks in both internal and external data. Particularly, Cardiac-CLIP
exhibits great effectiveness in supporting complex clinical tasks such as the
prospective prediction of acute coronary syndrome, which is notoriously
difficult in real-world scenarios.

</details>


### [166] [ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from Free-Text Reports](https://arxiv.org/abs/2507.22030)
*Mohammed Baharoon,Luyang Luo,Michael Moritz,Abhinav Kumar,Sung Eun Kim,Xiaoman Zhang,Miao Zhu,Mahmoud Hussain Alabbad,Maha Sbayel Alhazmi,Neel P. Mistry,Kent Ryan Kleinschmidt,Brady Chrisler,Sathvik Suryadevara,Sri Sai Dinesh Jaliparthi,Noah Michael Prudlo,Mark David Marino,Jeremy Palacio,Rithvik Akula,Hong-Yu Zhou,Ibrahim Ethem Hamamci,Scott J. Adams,Hassan Rayhan AlOmaish,Pranav Rajpurkar*

Main category: eess.IV

TL;DR: ReXGroundingCT is a manually annotated dataset linking free-text radiology findings to 3D CT scan segmentations, addressing the gap in connecting descriptive text to precise anatomical locations.


<details>
  <summary>Details</summary>
Motivation: Prior datasets used structured labels; ReXGroundingCT captures clinical language expressiveness and grounds it in 3D imaging, essential for AI in radiology.

Method: The dataset pairs 3,142 CT scans with radiology reports, using GPT-4 to extract findings and expert annotators for segmentation. Quality control was done by radiologists.

Result: 8,028 findings across 16,301 entities were annotated (79% focal, 21% non-focal). Training includes up to three segmentations per finding; validation/test sets have exhaustive labels.

Conclusion: ReXGroundingCT sets a benchmark for sentence-level grounding and free-text segmentation in chest CT, advancing medical AI capabilities.

Abstract: We present ReXGroundingCT, the first publicly available dataset to link
free-text radiology findings with pixel-level segmentations in 3D chest CT
scans that is manually annotated. While prior datasets have relied on
structured labels or predefined categories, ReXGroundingCT captures the full
expressiveness of clinical language represented in free text and grounds it to
spatially localized 3D segmentation annotations in volumetric imaging. This
addresses a critical gap in medical AI: the ability to connect complex,
descriptive text, such as "3 mm nodule in the left lower lobe", to its precise
anatomical location in three-dimensional space, a capability essential for
grounded radiology report generation systems. The dataset comprises 3,142
non-contrast chest CT scans paired with standardized radiology reports from the
CT-RATE dataset. Using a systematic three-stage pipeline, GPT-4 was used to
extract positive lung and pleural findings, which were then manually segmented
by expert annotators. A total of 8,028 findings across 16,301 entities were
annotated, with quality control performed by board-certified radiologists.
Approximately 79% of findings are focal abnormalities, while 21% are non-focal.
The training set includes up to three representative segmentations per finding,
while the validation and test sets contain exhaustive labels for each finding
entity. ReXGroundingCT establishes a new benchmark for developing and
evaluating sentence-level grounding and free-text medical segmentation models
in chest CT. The dataset can be accessed at
https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [167] [Can LLMs Reason About Trust?: A Pilot Study](https://arxiv.org/abs/2507.21075)
*Anushka Debnath,Stephen Cranefield,Emiliano Lorini,Bastin Tony Roy Savarimuthu*

Main category: cs.HC

TL;DR: The paper explores how Large Language Models (LLMs) can reason about and induce trust in human relationships, particularly in digital interactions.


<details>
  <summary>Details</summary>
Motivation: Trust is vital for healthy relationships and cooperation, and with increasing digital interactions, AI can help understand and foster trust.

Method: The study investigates LLMs' ability to reason about trust and role-play to induce trust in interactions.

Result: Assesses LLMs' capability in understanding and planning trust-building actions.

Conclusion: LLMs show potential in reasoning about and fostering trust in digital human interactions.

Abstract: In human society, trust is an essential component of social attitude that
helps build and maintain long-term, healthy relationships which creates a
strong foundation for cooperation, enabling individuals to work together
effectively and achieve shared goals. As many human interactions occur through
electronic means such as using mobile apps, the potential arises for AI systems
to assist users in understanding the social state of their relationships. In
this paper we investigate the ability of Large Language Models (LLMs) to reason
about trust between two individuals in an environment which requires fostering
trust relationships. We also assess whether LLMs are capable of inducing trust
by role-playing one party in a trust based interaction and planning actions
which can instil trust.

</details>


### [168] [Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations](https://arxiv.org/abs/2507.21089)
*Xiaotian Su,Naim Zierau,Soomin Kim,April Yi Wang,Thiemo Wambsganss*

Main category: cs.HC

TL;DR: Proactive moderation tools like emotion monitoring dashboards can reduce hate speech but may increase negative emotions in sensitive discussions.


<details>
  <summary>Details</summary>
Motivation: Address criticisms of current moderation by enhancing emotional awareness to mitigate uncivil behavior.

Method: Proposed and evaluated two emotion monitoring dashboards with 211 participants to assess effects on commenting behavior and emotions.

Result: Increased emotional awareness and reduced hate speech, but also heightened negative emotions in sensitive topics.

Conclusion: Emotion regulation tools show promise for healthier digital interactions but require further research to address unintended effects.

Abstract: Social media platforms increasingly employ proactive moderation techniques,
such as detecting and curbing toxic and uncivil comments, to prevent the spread
of harmful content. Despite these efforts, such approaches are often criticized
for creating a climate of censorship and failing to address the underlying
causes of uncivil behavior. Our work makes both theoretical and practical
contributions by proposing and evaluating two types of emotion monitoring
dashboards to users' emotional awareness and mitigate hate speech. In a study
involving 211 participants, we evaluate the effects of the two mechanisms on
user commenting behavior and emotional experiences. The results reveal that
these interventions effectively increase users' awareness of their emotional
states and reduce hate speech. However, our findings also indicate potential
unintended effects, including increased expression of negative emotions (Angry,
Fear, and Sad) when discussing sensitive issues. These insights provide a basis
for further research on integrating proactive emotion regulation tools into
social media platforms to foster healthier digital interactions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [169] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: R-Stitch accelerates CoT reasoning by dynamically switching between small and large language models based on token-level confidence, reducing latency by 85% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Current CoT acceleration methods either compromise sequence length or decoding speed, and speculative decoding underperforms when small and large models disagree. R-Stitch aims to improve efficiency without sacrificing quality.

Method: R-Stitch uses a hybrid decoding framework where a small language model (SLM) generates tokens by default, switching to a large language model (LLM) only when SLM confidence is low. This avoids full-sequence rollback and selectively invokes the LLM.

Result: Experiments show R-Stitch reduces inference latency by up to 85% with negligible accuracy drop on math reasoning benchmarks.

Conclusion: R-Stitch is a practical, model-agnostic solution for accelerating CoT reasoning efficiently while maintaining answer quality.

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [170] [MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge](https://arxiv.org/abs/2507.21183)
*Guangchen Lan,Sipeng Zhang,Tianle Wang,Yuwei Zhang,Daoan Zhang,Xinpeng Wei,Xiaoman Pan,Hongming Zhang,Dong-Jun Han,Christopher G. Brinton*

Main category: cs.LG

TL;DR: MaPPO is a new framework for aligning LLMs with human preferences by integrating prior reward knowledge into optimization, outperforming existing methods like DPO without extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DPO treat preference learning as MLE, oversimplifying response classification. MaPPO aims to enhance alignment by incorporating prior reward knowledge.

Method: MaPPO extends MLE to a Maximum a Posteriori objective, integrating prior reward estimates. It works offline/online and is compatible with DPO variants.

Result: Empirical evaluations show MaPPO improves alignment performance on benchmarks like MT-Bench and AlpacaEval 2.0 without computational trade-offs.

Conclusion: MaPPO generalizes and improves existing preference optimization methods, offering better alignment and flexibility without added complexity.

Abstract: As the era of large language models (LLMs) on behalf of users unfolds,
Preference Optimization (PO) methods have become a central approach to aligning
LLMs with human preferences and improving performance. We propose Maximum a
Posteriori Preference Optimization (MaPPO), a framework for learning from
preferences that explicitly incorporates prior reward knowledge into the
optimization objective. While existing methods such as Direct Preference
Optimization (DPO) and its variants treat preference learning as a Maximum
Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating
prior reward estimates into a principled Maximum a Posteriori (MaP) objective.
This not only generalizes DPO and its variants, but also enhances alignment by
mitigating the oversimplified binary classification of responses. More
importantly, MaPPO introduces no additional hyperparameter, and supports
preference optimization in both offline and online settings. In addition, MaPPO
can be used as a plugin with consistent improvement on DPO variants, including
widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different
model sizes and model series on three standard benchmarks, including MT-Bench,
AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in
alignment performance without sacrificing computational efficiency.

</details>


### [171] [EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models](https://arxiv.org/abs/2507.21184)
*Haowei Lin,Xiangyu Wang,Jianzhu Ma,Yitao Liang*

Main category: cs.LG

TL;DR: EvoSLD is an automated framework for discovering scaling laws in neural networks using evolutionary algorithms and LLMs, outperforming manual methods and baselines.


<details>
  <summary>Details</summary>
Motivation: Manual discovery of scaling laws is time-consuming and requires expertise; EvoSLD automates this process.

Method: Uses evolutionary algorithms guided by LLMs to co-evolve symbolic expressions and optimization routines, handling diverse variables and metrics.

Result: Rediscovers human-derived laws in some cases and surpasses them in others, achieving significant error reductions.

Conclusion: EvoSLD is accurate, interpretable, and efficient, potentially accelerating AI research.

Abstract: Scaling laws are fundamental mathematical relationships that predict how
neural network performance evolves with changes in variables such as model
size, dataset size, and computational resources. Traditionally, discovering
these laws requires extensive human expertise and manual experimentation. We
introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that
leverages evolutionary algorithms guided by Large Language Models (LLMs) to
co-evolve symbolic expressions and their optimization routines. Formulated to
handle scaling variables, control variables, and response metrics across
diverse experimental settings, EvoSLD searches for parsimonious, universal
functional forms that minimize fitting errors on grouped data subsets.
Evaluated on five real-world scenarios from recent literature, EvoSLD
rediscovers exact human-derived laws in two cases and surpasses them in others,
achieving up to orders-of-magnitude reductions in normalized mean squared error
on held-out test sets. Compared to baselines like symbolic regression and
ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and
efficiency, highlighting its potential to accelerate AI research. Code is
available at https://github.com/linhaowei1/SLD.

</details>


### [172] [Learning from Limited and Imperfect Data](https://arxiv.org/abs/2507.21205)
*Harsh Rangwani*

Main category: cs.LG

TL;DR: The paper addresses challenges in deep learning with real-world, imbalanced data, proposing robust algorithms for diverse scenarios like long-tail learning, inductive regularization, semi-supervised learning, and domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Real-world data is often imbalanced and imperfect, unlike curated datasets. Existing algorithms underperform on such data, necessitating robust solutions to avoid labor-intensive curation.

Method: Develops four practical algorithms: 1) Learning generative models for long-tail data, 2) Inductive regularization for tail class generalization, 3) Optimizing metrics for semi-supervised learning, and 4) Efficient domain adaptation with minimal labels.

Result: Proposed methods mitigate mode-collapse in generative models, improve tail class generalization, optimize semi-supervised learning, and enable domain adaptation with few or zero labeled samples.

Conclusion: The work advances deep learning for real-world data by addressing imbalance and distribution shifts, reducing reliance on curated datasets.

Abstract: The distribution of data in the world (eg, internet, etc.) significantly
differs from the well-curated datasets and is often over-populated with samples
from common categories. The algorithms designed for well-curated datasets
perform suboptimally when used for learning from imperfect datasets with
long-tailed imbalances and distribution shifts. To expand the use of deep
models, it is essential to overcome the labor-intensive curation process by
developing robust algorithms that can learn from diverse, real-world data
distributions. Toward this goal, we develop practical algorithms for Deep
Neural Networks which can learn from limited and imperfect data present in the
real world. This thesis is divided into four segments, each covering a scenario
of learning from limited or imperfect data. The first part of the thesis
focuses on Learning Generative Models from Long-Tail Data, where we mitigate
the mode-collapse and enable diverse aesthetic image generations for tail
(minority) classes. In the second part, we enable effective generalization on
tail classes through Inductive Regularization schemes, which allow tail classes
to generalize as effectively as the head classes without requiring explicit
generation of images. In the third part, we develop algorithms for Optimizing
Relevant Metrics for learning from long-tailed data with limited annotation
(semi-supervised), followed by the fourth part, which focuses on the Efficient
Domain Adaptation of the model to various domains with very few to zero labeled
samples.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [173] [CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting](https://arxiv.org/abs/2507.21257)
*David Maria Schmidt,Raoul Schubert,Philipp Cimiano*

Main category: cs.AI

TL;DR: The paper investigates the compositional interpretation abilities of large language models (LLMs) by testing their performance on structured datasets of varying complexity, concluding that LLMs struggle with systematic and compositional question interpretation.


<details>
  <summary>Details</summary>
Motivation: To assess how systematic LLMs are in interpreting questions compositionally, given their success in mapping questions to SPARQL queries.

Method: Created three controlled datasets based on DBpedia graph patterns, using Lemon lexica for verbalization, and tested LLMs with prompts, few-shot optimization, and fine-tuning.

Result: Performance (macro $F_1$) degraded from 0.45 to 0.09 with increasing complexity, not exceeding 0.57 even for the simplest dataset.

Conclusion: LLMs lack systematic and compositional interpretation abilities for complex questions, despite understanding atomic parts.

Abstract: Language interpretation is a compositional process, in which the meaning of
more complex linguistic structures is inferred from the meaning of their parts.
Large language models possess remarkable language interpretation capabilities
and have been successfully applied to interpret questions by mapping them to
SPARQL queries. An open question is how systematic this interpretation process
is. Toward this question, in this paper, we propose a benchmark for
investigating to what extent the abilities of LLMs to interpret questions are
actually compositional. For this, we generate three datasets of varying
difficulty based on graph patterns in DBpedia, relying on Lemon lexica for
verbalization. Our datasets are created in a very controlled fashion in order
to test the ability of LLMs to interpret structurally complex questions, given
that they have seen the atomic building blocks. This allows us to evaluate to
what degree LLMs are able to interpret complex questions for which they
"understand" the atomic parts. We conduct experiments with models of different
sizes using both various prompt and few-shot optimization techniques as well as
fine-tuning. Our results show that performance in terms of macro $F_1$ degrades
from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the
samples optimized on. Even when all necessary information was provided to the
model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of
lowest complexity. We thus conclude that LLMs struggle to systematically and
compositionally interpret questions and map them into SPARQL queries.

</details>


### [174] [LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems](https://arxiv.org/abs/2507.21276)
*Yufei Li,Zexin Li,Yinglun Zhu,Cong Liu*

Main category: cs.AI

TL;DR: LeMix is a system for co-locating LLM serving and training workloads, improving efficiency and responsiveness by dynamically managing resources.


<details>
  <summary>Details</summary>
Motivation: Inefficiencies in current practices where LLM serving and training are separated, leading to GPU idleness and delayed adaptation.

Method: LeMix integrates offline profiling, execution prediction, and runtime scheduling to dynamically allocate resources.

Result: LeMix improves throughput by 3.53x, reduces inference loss by 0.61x, and enhances response time SLO attainment by 2.12x.

Conclusion: LeMix demonstrates the benefits of joint LLM inference and training, enabling more resource-efficient deployment.

Abstract: Modern deployment of large language models (LLMs) frequently involves both
inference serving and continuous retraining to stay aligned with evolving data
and user feedback. Common practices separate these workloads onto distinct
servers in isolated phases, causing substantial inefficiencies (e.g., GPU
idleness) and delayed adaptation to new data in distributed settings. Our
empirical analysis reveals that these inefficiencies stem from dynamic request
arrivals during serving and workload heterogeneity in pipeline-parallel
training. To address these challenges, we propose LeMix, a system for
co-locating and managing concurrent LLM serving and training workloads. LeMix
integrates offline profiling, execution prediction mechanisms, and runtime
scheduling to dynamically adapt resource allocation based on workload
characteristics and system conditions. By understanding task-specific behaviors
and co-execution interference across shared nodes, LeMix improves utilization
and serving quality without compromising serving responsiveness. Our evaluation
shows that LeMix improves throughput by up to 3.53x, reduces inference loss by
up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over
traditional separate setups. To our knowledge, this is the first work to
uncover and exploit the opportunities of joint LLM inference and training,
paving the way for more resource-efficient deployment of LLMs in production
environments.

</details>


### [175] [Teaching Language Models To Gather Information Proactively](https://arxiv.org/abs/2507.21389)
*Tenghao Huang,Sihao Chen,Muhao Chen,Jonathan May,Longqi Yang,Mengting Wan,Pei Zhou*

Main category: cs.AI

TL;DR: The paper introduces proactive information gathering for LLMs, using reinforcement finetuning to improve their ability to ask targeted questions and gather implicit user knowledge, outperforming baselines in evaluations.


<details>
  <summary>Details</summary>
Motivation: Current LLMs often fail to proactively gather missing information in ambiguous tasks, limiting their effectiveness as collaborative partners.

Method: A scalable framework generates partially specified tasks, and reinforcement finetuning rewards questions that elicit implicit user knowledge.

Result: The Qwen-2.5-7B model outperforms o3-mini by 18% in automatic metrics and is favored by humans for clarification questions (42%) and outlines (28%).

Conclusion: Proactive clarification enhances LLMs' role as collaborative partners, moving beyond passive text generation.

Abstract: Large language models (LLMs) are increasingly expected to function as
collaborative partners, engaging in back-and-forth dialogue to solve complex,
ambiguous problems. However, current LLMs often falter in real-world settings,
defaulting to passive responses or narrow clarifications when faced with
incomplete or under-specified prompts, falling short of proactively gathering
the missing information that is crucial for high-quality solutions. In this
work, we introduce a new task paradigm: proactive information gathering, where
LLMs must identify gaps in the provided context and strategically elicit
implicit user knowledge through targeted questions. To systematically study and
train this capability, we design a scalable framework that generates partially
specified, real-world tasks, masking key information and simulating authentic
ambiguity. Within this setup, our core innovation is a reinforcement finetuning
strategy that rewards questions that elicit genuinely new, implicit user
information -- such as hidden domain expertise or fine-grained requirements --
that would otherwise remain unspoken. Experiments demonstrate that our trained
Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic
evaluation metrics. More importantly, human evaluation reveals that
clarification questions and final outlines generated by our model are favored
by human annotators by 42% and 28% respectively. Together, these results
highlight the value of proactive clarification in elevating LLMs from passive
text generators to genuinely collaborative thought partners.

</details>


### [176] [What Does it Mean for a Neural Network to Learn a "World Model"?](https://arxiv.org/abs/2507.21513)
*Kenneth Li,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: The paper proposes criteria to define when a neural net learns a 'world model,' focusing on latent state space representation and avoiding trivial outcomes.


<details>
  <summary>Details</summary>
Motivation: To provide operational meaning to informal terms and establish a common language for experimental research in neural networks.

Method: Uses ideas from linear probing literature to formalize computations factoring through a representation of the data generation process, with conditions to prevent trivial results.

Result: A precise definition of a 'world model' in neural nets, emphasizing latent state space representation.

Conclusion: The criteria offer a foundation for future work on modeling actions and experimental validation of world models in neural networks.

Abstract: We propose a set of precise criteria for saying a neural net learns and uses
a "world model." The goal is to give an operational meaning to terms that are
often used informally, in order to provide a common language for experimental
investigation. We focus specifically on the idea of representing a latent
"state space" of the world, leaving modeling the effect of actions to future
work. Our definition is based on ideas from the linear probing literature, and
formalizes the notion of a computation that factors through a representation of
the data generation process. An essential addition to the definition is a set
of conditions to check that such a "world model" is not a trivial consequence
of the neural net's data or task.

</details>


### [177] [UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding](https://arxiv.org/abs/2507.22025)
*Shuquan Lian,Yuhang Wu,Jia Ma,Zihan Song,Bingqi Chen,Xiawu Zheng,Hui Li*

Main category: cs.AI

TL;DR: UI-AGILE enhances GUI agents with improved training (Continuous Reward, Simple Thinking reward, Cropping-based Resampling) and inference (Decomposed Grounding with Selection), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing GUI agent training and inference, such as ineffective rewards and visual noise.

Method: Proposes training enhancements (reward functions, resampling) and inference method (Decomposed Grounding with Selection).

Result: 23% grounding accuracy improvement on ScreenSpot-Pro benchmark.

Conclusion: UI-AGILE significantly advances GUI agent capabilities with its comprehensive framework.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven
significant advances in Graphical User Interface (GUI) agent capabilities.
Nevertheless, existing GUI agent training and inference techniques still suffer
from a dilemma for reasoning designs, ineffective reward, and visual noise. To
address these issues, we introduce UI-AGILE, a comprehensive framework
enhancing GUI agents at both the training and inference stages. For training,
we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:
1) a Continuous Reward function to incentivize high-precision grounding; 2) a
"Simple Thinking" reward to balance planning with speed and grounding accuracy;
and 3) a Cropping-based Resampling strategy to mitigate the sparse reward
problem and improve learning on complex tasks. For inference, we present
Decomposed Grounding with Selection, a novel method that dramatically improves
grounding accuracy on high-resolution displays by breaking the image into
smaller, manageable parts. Experiments show that UI-AGILE achieves the
state-of-the-art performance on two benchmarks ScreenSpot-Pro and
ScreenSpot-v2. For instance, using both our proposed training and inference
enhancement methods brings 23% grounding accuracy improvement over the best
baseline on ScreenSpot-Pro.

</details>


### [178] [UserBench: An Interactive Gym Environment for User-Centric Agents](https://arxiv.org/abs/2507.22034)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Zhiwei Liu,Jianguo Zhang,Haolin Chen,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserBench is introduced to evaluate LLM agents in multi-turn, preference-driven interactions, revealing gaps in proactive collaboration with users.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack proactive collaboration with users, especially for vague or evolving goals.

Method: UserBench, a user-centric benchmark, simulates users with underspecified goals and incremental preference revelation, testing agents' proactive intent clarification and tool use.

Result: Leading LLMs align with user intents only 20% of the time and uncover fewer than 30% of preferences, showing a task-user alignment gap.

Conclusion: UserBench highlights the need for LLM agents to evolve into true collaborative partners, not just task executors.

Abstract: Large Language Models (LLMs)-based agents have made impressive progress in
reasoning and tool use, enabling them to solve complex tasks. However, their
ability to proactively collaborate with users, especially when goals are vague,
evolving, or indirectly expressed, remains underexplored. To address this gap,
we introduce UserBench, a user-centric benchmark designed to evaluate agents in
multi-turn, preference-driven interactions. UserBench features simulated users
who start with underspecified goals and reveal preferences incrementally,
requiring agents to proactively clarify intent and make grounded decisions with
tools. Our evaluation of leading open- and closed-source LLMs reveals a
significant disconnect between task completion and user alignment. For
instance, models provide answers that fully align with all user intents only
20% of the time on average, and even the most advanced models uncover fewer
than 30% of all user preferences through active interaction. These results
highlight the challenges of building agents that are not just capable task
executors, but true collaborative partners. UserBench offers an interactive
environment to measure and advance this critical capability.

</details>


### [179] [MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions](https://arxiv.org/abs/2507.21503)
*Yanxu Zhu,Shitong Duan,Xiangxu Zhang,Jitao Sang,Peng Zhang,Tun Lu,Xiao Zhou,Jing Yao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: The paper assesses honesty in Multimodal Large Language Models (MLLMs) when answering visually unanswerable questions, introduces MoHoBench (a benchmark with 12k+ samples), and finds most models fail to refuse answers appropriately. It suggests dedicated alignment methods for improving honesty.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in MLLMs, their honesty in handling visually unanswerable questions is underexplored. This work aims to systematically evaluate and improve this aspect.

Method: The study defines four types of unanswerable visual questions, constructs MoHoBench (a large-scale benchmark), and evaluates 28 MLLMs. It also implements supervised and preference learning for alignment.

Result: Most models fail to refuse unanswerable questions appropriately. Honesty is influenced by visual information, requiring dedicated alignment methods.

Conclusion: The work provides a foundation for improving MLLM honesty through alignment methods, with data and code available for future research.

Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable
advancements in vision-language tasks, yet produce potentially harmful or
untrustworthy content. Despite substantial work investigating the
trustworthiness of language models, MMLMs' capability to act honestly,
especially when faced with visually unanswerable questions, remains largely
underexplored. This work presents the first systematic assessment of honesty
behaviors across various MLLMs. We ground honesty in models' response behaviors
to unanswerable visual questions, define four representative types of such
questions, and construct MoHoBench, a large-scale MMLM honest benchmark,
consisting of 12k+ visual question samples, whose quality is guaranteed by
multi-stage filtering and human verification. Using MoHoBench, we benchmarked
the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our
findings show that: (1) most models fail to appropriately refuse to answer when
necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but
is deeply influenced by visual information, necessitating the development of
dedicated methods for multimodal honesty alignment. Therefore, we implemented
initial alignment methods using supervised and preference learning to improve
honesty behavior, providing a foundation for future work on trustworthy MLLMs.
Our data and code can be found at https://github.com/DSTTSD/MoHoBench.

</details>


### [180] [Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning](https://arxiv.org/abs/2507.21588)
*Jiong Yin,Liang Li,Jiehua Zhang,Yuhan Gao,Chenggang Yan,Xichun Sheng*

Main category: cs.AI

TL;DR: The paper introduces a three-stage Progressive Homeostatic and Plastic (PHP) method for audio-visual multi-task incremental learning, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of preserving old task knowledge while learning new tasks in audio-visual multi-task incremental learning.

Method: PHP method includes three phases: shallow (task-shared modality aggregating adapter), middle (task-specific modality-shared dynamic generating adapter), and deep (task-specific modality-independent prompts).

Result: Achieves SOTA performance in four tasks (AVE, AVVP, AVS, AVQA).

Conclusion: PHP effectively balances knowledge sharing and specificity, retaining task-specific prompts while adapting shared parameters for new tasks.

Abstract: Audio-visual multi-task incremental learning aims to continuously learn from
multiple audio-visual tasks without the need for joint training on all tasks.
The challenge of the problem is how to preserve the old task knowledge while
facilitating the learning of new task with previous experiences. To address
these challenges, we introduce a three-stage Progressive Homeostatic and
Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the
task-shared modality aggregating adapter to foster cross-task and cross-modal
audio-visual representation learning to enhance shared understanding between
tasks. In the middle phase, we propose the task-specific modality-shared
dynamic generating adapter, which constructs prompts that are tailored to
individual tasks while remaining general across modalities, which balances the
models ability to retain knowledge against forgetting with its potential for
versatile multi-task transferability. In the deep phase, we introduce the
task-specific modality-independent prompts to further refine the understand
ability by targeting individual information for each task and modality. By
incorporating these three phases, PHP retains task-specific prompts while
adapting shared parameters for new tasks to effectively balance knowledge
sharing and specificity. Our method achieves SOTA performance in different
orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at
https://github.com/ENJOY-Yin-jiong/PHP.

</details>


### [181] [MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE](https://arxiv.org/abs/2507.21802)
*Junzhe Li,Yutao Cui,Tao Huang,Yinping Ma,Chun Fan,Miles Yang,Zhao Zhong*

Main category: cs.AI

TL;DR: MixGRPO improves efficiency in human preference alignment for image generation by combining SDE and ODE sampling with a sliding window mechanism, reducing training time by 50% and introducing a faster variant, MixGRPO-Flash, with 71% lower training time.


<details>
  <summary>Details</summary>
Motivation: Existing methods like FlowGRPO are inefficient due to sampling and optimizing all denoising steps in MDPs. MixGRPO aims to streamline this process.

Method: MixGRPO integrates SDE and ODE sampling, using a sliding window to confine randomness and optimize only within the window. MixGRPO-Flash adds higher-order solvers for faster sampling.

Result: MixGRPO outperforms DanceGRPO in efficiency and effectiveness, reducing training time by 50%. MixGRPO-Flash further cuts training time by 71%.

Conclusion: MixGRPO offers a more efficient and effective solution for human preference alignment in image generation, with significant time savings and performance gains.

Abstract: Although GRPO substantially enhances flow matching models in human preference
alignment of image generation, methods such as FlowGRPO still exhibit
inefficiency due to the necessity of sampling and optimizing over all denoising
steps specified by the Markov Decision Process (MDP). In this paper, we propose
$\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed
sampling strategies through the integration of stochastic differential
equations (SDE) and ordinary differential equations (ODE). This streamlines the
optimization process within the MDP to improve efficiency and boost
performance. Specifically, MixGRPO introduces a sliding window mechanism, using
SDE sampling and GRPO-guided optimization only within the window, while
applying ODE sampling outside. This design confines sampling randomness to the
time-steps within the window, thereby reducing the optimization overhead, and
allowing for more focused gradient updates to accelerate convergence.
Additionally, as time-steps beyond the sliding window are not involved in
optimization, higher-order solvers are supported for sampling. So we present a
faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves
training efficiency while achieving comparable performance. MixGRPO exhibits
substantial gains across multiple dimensions of human preference alignment,
outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%
lower training time. Notably, MixGRPO-Flash further reduces training time by
71%. Codes and models are available at
$\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [182] [Supervised Quantum Image Processing](https://arxiv.org/abs/2507.22039)
*Marco Parigi,Mehran Khosrojerdi,Filippo Caruso,Leonardo Banchi*

Main category: quant-ph

TL;DR: The paper compares four Quantum Image Representations (QImRs) for compression and evaluates quantum kernels in binary classification, finding FRQI superior in compression and quantum kernels resource-efficient.


<details>
  <summary>Details</summary>
Motivation: Addressing data volume and computational challenges in big data and AI by leveraging quantum computing for efficient image processing.

Method: Comparison of four QImRs (TNR, FRQI, NEQR, QPIE) for compression properties and evaluation of quantum vs. classical kernels in binary classification.

Result: FRQI outperforms other QImRs in compression; quantum kernels match classical accuracy with fewer resources.

Conclusion: Quantum image processing, particularly FRQI and quantum kernels, offers efficient solutions for data storage and classification tasks.

Abstract: In the era of big data and artificial intelligence, the increasing volume of
data and the demand to solve more and more complex computational challenges are
two driving forces for improving the efficiency of data storage, processing and
analysis. Quantum image processing (QIP) is an interdisciplinary field between
quantum information science and image processing, which has the potential to
alleviate some of these challenges by leveraging the power of quantum
computing. In this work, we compare and examine the compression properties of
four different Quantum Image Representations (QImRs): namely, Tensor Network
Representation (TNR), Flexible Representation of Quantum Image (FRQI), Novel
Enhanced Quantum Representation NEQR, and Quantum Probability Image Encoding
(QPIE). Our simulations show that FRQI performs a higher compression of image
information than TNR, NEQR, and QPIE. Furthermore, we investigate the trade-off
between accuracy and memory in binary classification problems, evaluating the
performance of quantum kernels based on QImRs compared to the classical linear
kernel. Our results indicate that quantum kernels provide comparable
classification average accuracy but require exponentially fewer resources for
image storage.

</details>

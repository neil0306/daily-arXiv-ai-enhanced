<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 122]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: ParallelSearch introduces a reinforcement learning framework for LLMs to execute parallel search operations, improving efficiency and performance over sequential methods.


<details>
  <summary>Details</summary>
Motivation: Existing search agents process queries sequentially, limiting efficiency for parallelizable tasks.

Method: Proposes ParallelSearch, a framework with dedicated reward functions to identify and execute parallelizable query components.

Result: Outperforms baselines by 2.9% on average, with 12.7% improvement on parallelizable questions and 69.6% fewer LLM calls.

Conclusion: ParallelSearch enhances computational efficiency and performance in multi-step information retrieval tasks.

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [2] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: GPT-4o is evaluated for rare disease NER using prompt-based strategies, achieving competitive or SOTA results. Few-shot prompting is cost-effective, while RAG offers limited benefits.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in rare disease NER, such as limited labeled data and semantic ambiguity, by leveraging GPT-4o's capabilities.

Method: Uses zero-shot, few-shot, RAG, and fine-tuning with a structured prompting framework and semantically guided example selection.

Result: GPT-4o outperforms BioClinicalBERT, with fine-tuning achieving SOTA. Few-shot is cost-effective; RAG has marginal benefits.

Conclusion: Prompt-optimized LLMs are scalable alternatives for biomedical NER, especially in low-resource rare disease settings.

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [3] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: TEN is a neurosymbolic method for extracting tabular data from semistructured text, combining LLM prompting with symbolic checks to improve accuracy and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: Extracting tables from inconsistently delimited text is challenging for neural methods due to hallucinations and lack of constraint enforcement.

Method: Uses Structural Decomposition prompting on an LLM for initial table generation, followed by symbolic checks and a critique-LLM for self-debugging.

Result: Outperforms neural baselines in accuracy and hallucination reduction, with user studies confirming higher accuracy and preference.

Conclusion: TEN's hybrid approach effectively addresses limitations of purely neural methods for tabular data extraction.

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [4] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: A computational framework maps textual emotional content to brain regions without neuroimaging, using semantic embeddings and clustering. It distinguishes clinical populations and evaluates AI emotional expression.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between neuroimaging-based emotion localization and computational text analysis by integrating them into a cost-effective, scalable method.

Method: Uses OpenAI's text-embedding-ada-002 for semantic representations, dimensionality reduction, clustering, and mapping to 18 brain regions. Tested on conversational data (healthy vs. depressed), GoEmotions dataset, and human vs. LLM text.

Result: Neuroanatomically plausible mappings with high specificity. Depressed subjects showed greater limbic engagement. LLM text matched humans in basic emotions but lacked nuanced activation in empathy-related regions.

Conclusion: The framework enables large-scale emotion-brain mapping, differentiates clinical groups, and provides a brain-based benchmark for AI emotional expression.

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [5] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: The paper introduces a Human-AI Hybrid Delphi (HAH-Delphi) framework to improve expert consensus by combining AI (Gemini 2.5 Pro) with human experts, showing high accuracy and efficiency in trials.


<details>
  <summary>Details</summary>
Motivation: Traditional consensus methods like Delphi studies face limitations like high burden and oversimplification, worsened by information overload and fragmented evidence.

Method: The HAH-Delphi integrates AI, small expert panels, and structured facilitation, tested in three phases: replication, comparison, and applied deployment in training domains.

Result: AI replicated 95% of consensus conclusions and agreed 95% with human experts, while compact panels achieved >90% consensus coverage and faster saturation.

Conclusion: HAH-Delphi is a scalable, robust method for high-quality, context-sensitive consensus, validated in health and performance science.

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [6] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: Textless spoken language models (SLMs) jointly model linguistic and acoustic information using semantic tokens and continuous acoustic frames, improving acoustic detail without compromising linguistic performance.


<details>
  <summary>Details</summary>
Motivation: Existing textless SLMs lack acoustic context and control, relying on separate vocoders. This work aims to integrate linguistic and acoustic modeling for better speech generation.

Method: Proposes joint modeling of semantic tokens and continuous acoustic frames using a flow-matching objective. Predicts multiple future semantic tokens to preserve linguistic information.

Result: Achieves comparable linguistic performance to existing models while enhancing acoustic detail in prompted generation.

Conclusion: Joint modeling of linguistic and acoustic information improves textless SLMs, offering better control and detail in speech generation.

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [7] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: APIO is a prompt induction and optimization method for Grammatical Error Correction and Text Simplification, achieving state-of-the-art performance without manual seed prompts.


<details>
  <summary>Details</summary>
Motivation: To advance automatic prompt optimization (APO) by eliminating reliance on manually specified seed prompts for tasks like GEC and Text Simplification.

Method: APIO, a simple but effective approach for prompt induction and optimization, applied to GEC and Text Simplification tasks.

Result: APIO achieves state-of-the-art performance for purely LLM-based prompting methods on these tasks.

Conclusion: APIO is a promising method for prompt optimization, with publicly available resources for further research.

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [8] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: The paper introduces Columbo, an LLM-based solution for expanding table column abbreviations, outperforming existing methods by 4-29%. It addresses limitations in prior work with new datasets and synonym-aware accuracy measures.


<details>
  <summary>Details</summary>
Motivation: The need to accurately expand abbreviated column names in tables for downstream tasks in enterprises, sciences, and government agencies.

Method: Develops Columbo, leveraging context, rules, chain-of-thought reasoning, and token-level analysis. Introduces 4 new datasets with real-world abbreviations and proposes synonym-aware accuracy measures.

Result: Columbo outperforms the current state-of-the-art solution, NameGuess, by 4-29% across 5 datasets.

Conclusion: Columbo advances the field by addressing prior limitations and has been successfully deployed in production for environmental sciences.

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [9] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: The paper uses Zipformer to improve language identification in bilingual child-directed speech, achieving a 15.47% improvement in Balanced Accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in code-switching and language identification in bilingual child-directed speech, particularly with imbalanced Mandarin and English data.

Method: Utilizes Zipformer to encode language characteristics, selects inner layers for embeddings, and compares performance across different back-ends.

Result: Achieves a Balanced Accuracy (BAC) of 81.89%, a 15.47% improvement over the baseline.

Conclusion: Highlights the effectiveness of transformer encoder architectures like Zipformer in real-world bilingual scenarios.

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [10] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: The paper investigates geo-economic biases in VLM-generated chart summaries, finding that high-income countries receive more positive descriptions than lower-income ones.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention to biases in chart-to-text tasks, particularly how VLMs may amplify geo-economic biases in summaries.

Method: A large-scale evaluation of 6,000 chart-country pairs across six VLMs, analyzing sentiment differences based on economic status.

Result: VLMs produce more positive summaries for high-income countries, with models like GPT-4o-mini and Gemini-1.5-Flash showing bias. Prompt-based debiasing is only partially effective.

Conclusion: Geo-economic bias in VLMs is a significant issue requiring more robust debiasing strategies.

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [11] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: The paper introduces a User-Centric Subjective Leaderboard (USL) and Customizable Reward Models (CRMs) to address limitations of static benchmarks for LLMs, leveraging human preference data for dynamic, preference-driven rankings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs focus on verifiable tasks, lacking utility for practical model selection. The paper aims to bridge this gap by incorporating human preferences.

Method: The USL is built on 10K+ subjective queries, revealing diversity in human preferences. CRMs, with 4B parameters, outperform leading models like GPT-4.1 and Gemini-2.5-pro.

Result: CRMs show exceptional generalization and outperform top models. USL exhibits strong negative correlations to contradictory preferences.

Conclusion: The USL and CRMs provide a dynamic, user-centric approach to LLM evaluation, addressing limitations of static benchmarks and improving model selection.

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [12] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: Active Reading improves LLMs' knowledge absorption by training them with self-generated learning strategies, outperforming vanilla finetuning and other methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the unreliable learning and recall of facts in LLMs by providing a structured framework for consistent knowledge absorption.

Method: Proposes Active Reading, where models study given material with self-generated learning strategies, applied to expert domains and pre-training.

Result: Achieves significant improvements: 66% on SimpleQA (+313% over vanilla finetuning) and 26% on FinanceBench (+160%). Also releases Meta WikiExpert-8B, a high-performing factual QA model.

Conclusion: Active Reading effectively enhances factual knowledge in LLMs, scalable to pre-training, and outperforms larger models on factual QA tasks.

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [13] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: DPS is a dynamic passage selector for RAG systems that improves reranking by adaptively selecting relevant passages, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional reranking modules in RAG systems struggle with multi-hop queries, either omitting crucial information or introducing noise.

Method: DPS treats passage selection as a supervised learning problem, capturing inter-passage dependencies and dynamically selecting passages.

Result: DPS outperforms baselines, improving F1-score by 30.06% and 15.4% on MuSiQue.

Conclusion: DPS enhances reasoning in complex RAG scenarios by enabling adaptive evidence selection.

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [14] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: A new method for cross-lingual aspect-based sentiment analysis (ABSA) uses LLMs to generate pseudo-labelled data in the target language, avoiding unreliable translation tools and outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on unreliable translation tools for cross-lingual ABSA, limiting accuracy and robustness.

Method: The framework trains an ABSA model, uses LLM to refine noisy predictions into natural sentences, and fine-tunes the model on the pseudo-labelled data.

Result: The method outperforms state-of-the-art translation-based approaches across six languages and five backbone models, with fine-tuned LLMs surpassing smaller multilingual models.

Conclusion: The proposed framework effectively bypasses translation tools, enhances performance, and supports generative models, demonstrating superior results in cross-lingual ABSA.

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [15] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: This paper provides a comprehensive survey of cross-lingual aspect-based sentiment analysis (ABSA), summarizing tasks, datasets, methods, and challenges, while suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: Cross-lingual ABSA is under-explored, and this paper aims to systematically review the field to bridge the gap in transferring knowledge from resource-rich to low-resource languages.

Method: The paper reviews key ABSA tasks (e.g., aspect term extraction, sentiment classification), datasets, modeling paradigms, and cross-lingual transfer methods, including insights from monolingual and multilingual ABSA and LLMs.

Result: The survey highlights the current state of cross-lingual ABSA, identifying gaps and contributions from related fields.

Conclusion: The paper underscores challenges in cross-lingual ABSA and proposes future research directions to advance the field.

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [16] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: A zero-shot system for fact-checked claim retrieval using state-of-the-art language models achieved top rankings in monolingual and cross-lingual tasks, with NVIDIA NV-Embed-v2 performing best.


<details>
  <summary>Details</summary>
Motivation: To develop an effective zero-shot system for retrieving fact-checked claims using advanced language models.

Method: Employed multiple large language models for text embeddings, combined models for optimal results, and used cosine similarity to identify relevant claims. English translations were used as input due to poor multilingual model performance.

Result: Achieved 7th place in monolingual and 9th in cross-lingual subtasks. NVIDIA NV-Embed-v2 yielded the best results, with some languages benefiting from model combinations.

Conclusion: The system demonstrated strong performance in claim retrieval, with model combinations enhancing results for certain languages.

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [17] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: The paper proposes controllable empathetic reasoning for emotional support conversations, combining NLP with psychological steps, and uses reinforcement learning to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Current models lack deep empathetic reasoning grounded in psychology, limiting their effectiveness in emotional support.

Method: Combines natural language reasoning with structured psychological steps, uses a fine-grained annotated dataset, reinforcement learning with a unified reward model, and strategies to reduce repetitiveness.

Result: Significantly improves the model's emotional support ability, making it more empathetic and human-like.

Conclusion: The approach advances the development of empathetic support systems by integrating psychological principles and precise feedback mechanisms.

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [18] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: The paper introduces the N-Gram Coverage Attack, a membership inference attack using only text outputs, outperforming black-box methods and matching white-box attacks. It scales with compute budget and reveals increased robustness in newer models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To enable membership inference attacks on black-box models (e.g., GPT-4) without requiring hidden states or probability distributions, addressing limitations of current methods.

Method: The N-Gram Coverage Attack uses n-gram overlap metrics to compare model-generated text with ground truth, predicting membership based on high similarity.

Result: The attack outperforms black-box methods and matches white-box attacks, with performance improving as more sequences are generated. Newer models like GPT-4o show increased robustness.

Conclusion: The method enables effective membership inference on black-box models, revealing trends toward improved privacy in newer models.

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [19] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: The AINL-Eval 2025 Shared Task introduces a dataset and challenge for detecting AI-generated scientific abstracts in Russian, aiming to address academic integrity concerns.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs makes it hard to distinguish human- from AI-generated content, threatening academic integrity, especially in multilingual contexts with limited detection tools.

Method: A large-scale dataset of 52,305 samples (human and AI-generated abstracts from 5 LLMs) was created. The task involved 10 teams and 159 submissions, focusing on generalization to unseen domains and models.

Result: Top systems showed strong performance in detecting AI-generated content.

Conclusion: The shared task and platform aim to foster ongoing research in AI-generated text detection, with the dataset and tools publicly available.

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [20] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: The paper explores how adjusting decoding temperature affects language model diversity, proposing a Precision-Recall framework for better trade-offs.


<details>
  <summary>Details</summary>
Motivation: To understand why temperature adjustments often fail to improve coverage (Recall) and how to achieve better diversity in language models.

Method: Analyzes temperature scaling effects, proposes rethinking loss functions using the Precision-Recall framework.

Result: The proposed approach achieves a better Precision-Recall trade-off than traditional methods.

Conclusion: Rethinking loss functions with Precision-Recall can lead to more versatile and robust language models.

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [21] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval is a training-free method for efficient benchmarking of large language models (LLMs), addressing computational challenges by reducing data redundancy while maintaining evaluation reliability.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLMs and diverse benchmarks has created computational challenges in model assessment, necessitating efficient and reliable evaluation methods.

Method: EffiEval uses the Model Utility Index (MUI) to adaptively select high-quality representative subsets of data, ensuring representativeness, fairness, and generalizability without extensive evaluation data.

Result: Experiments show EffiEval achieves strong ranking consistency with full-dataset evaluation using only a small fraction of data, and it is flexible and scalable.

Conclusion: EffiEval offers a practical, generalizable solution for reliable, fair, and efficient LLM evaluation.

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [22] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: SLowED is a safe distillation method for Small Language Models (SLMs) that maintains safety while improving reasoning, using Slow Tuning and Low-Entropy Masking.


<details>
  <summary>Details</summary>
Motivation: Address the negative safety effects of CoT distillation on SLMs without extra computation or annotated data.

Method: Proposes SLowED with two modules: Slow Tuning (limits weight changes) and Low-Entropy Masking (excludes unnecessary tokens).

Result: SLowED retains SLM safety and improves reasoning on benchmarks (BBH, BB-Sub, ARC, AGIEval) and safety evaluation (AdvBench).

Conclusion: SLowED effectively balances safety and reasoning, with Slow Tuning and Low-Entropy Masking playing complementary roles.

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [23] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: LLMs like GPT, Claude, and Llama perform well in legal drafting and issue spotting but struggle with specialized legal research, often producing incorrect outputs. Human expertise remains crucial for nuanced legal tasks.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of LLMs in key legal tasks in the Indian context and compare their outputs with those of human legal professionals.

Method: Survey experiment comparing LLM outputs (GPT, Claude, Llama) with junior lawyer work, rated by advanced law students on helpfulness, accuracy, and comprehensiveness.

Result: LLMs excel in drafting and issue spotting but struggle with specialized research, often generating hallucinations or incorrect outputs.

Conclusion: LLMs can augment certain legal tasks but human expertise is essential for nuanced reasoning and precise law application.

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [24] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: The study evaluates how Vision-Language Models (VLMs) interpret misleading visualizations, finding most are deceived, leading to altered chart interpretations.


<details>
  <summary>Details</summary>
Motivation: To understand VLMs' susceptibility to deceptive visual designs, as they are increasingly used by non-experts for interpreting visualizations.

Method: Analyzed over 16,000 responses from ten VLMs across eight types of misleading chart designs.

Result: Most VLMs were deceived by misleading visualizations, altering interpretations despite unchanged data.

Conclusion: Highlights the need for safeguards in VLMs to prevent visual misinformation.

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [25] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: GFPO reduces response length inflation in language models by filtering training responses based on length and token efficiency, maintaining accuracy while cutting filler text.


<details>
  <summary>Details</summary>
Motivation: To address the issue of models inflating response lengths with filler text to gain accuracy, which is inefficient.

Method: GFPO samples larger groups per problem during training and filters responses using length and token efficiency (reward per token). Adaptive Difficulty GFPO dynamically allocates training resources based on problem difficulty.

Result: GFPO reduces length inflation by 46-71% on benchmarks, with further reductions (71-85%) when optimizing for reward per token. Adaptive Difficulty GFPO improves efficiency on hard questions.

Conclusion: GFPO shows that increased training compute reduces test-time compute, offering an efficient trade-off for reasoning tasks.

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [26] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: A novel RAG framework for multihop QA improves performance by decomposing questions into single-hop subquestions and using answerable-question embeddings for retrieval.


<details>
  <summary>Details</summary>
Motivation: Address ambiguity in multihop queries and enhance retrieval accuracy by leveraging LLMs for decomposition and question generation.

Method: Decomposes multihop questions into single-hop subquestions, generates answerable questions from document chunks, and retrieves via question-question similarity.

Result: Outperforms baseline systems on MuSiQue, 2WikiMultiHopQa, and HotpotQA datasets.

Conclusion: Answerable-question embeddings and LLM-based decomposition significantly enhance RAG performance in multihop QA.

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [27] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: The paper explores how LLMs' political bias evaluations are influenced by suggestive prompts, showing that arguments in prompts significantly sway model responses.


<details>
  <summary>Details</summary>
Motivation: To understand the robustness of bias evaluations in LLMs and model behavior when interacting with opinionated text.

Method: Conducted experiments evaluating political bias with supporting and refuting arguments in prompts, analyzing single-turn and multi-turn settings.

Result: Model responses align with provided arguments, and argument strength affects directional agreement rates, revealing sycophantic tendencies.

Conclusion: LLMs adapt stances to prompts, impacting bias measurement and mitigation strategies.

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [28] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: UtterTune is a lightweight adaptation method for multilingual TTS systems using LLM architecture, enhancing pronunciation control in a target language without degrading performance in others.


<details>
  <summary>Details</summary>
Motivation: LLM-based TTS models struggle with G2P mapping and prosody, especially without explicit G2P modules. UtterTune aims to improve pronunciation control while preserving naturalness.

Method: Uses low-rank adaptation to control segmental pronunciation and pitch accent at the phoneme level for Japanese speech, maintaining zero-shot performance.

Result: Objective and subjective evaluations confirm UtterTune's effectiveness in enhancing pronunciation control.

Conclusion: UtterTune successfully improves pronunciation controllability in multilingual TTS systems while preserving naturalness and speaker similarity.

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [29] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: An automated framework using LLMs generates high-quality textual explanations for NLP tasks, showing competitive effectiveness compared to human annotations.


<details>
  <summary>Details</summary>
Motivation: Traditional human annotation for textual explanations is costly and unscalable, prompting the need for automated solutions.

Method: Leverages multiple state-of-the-art LLMs to generate explanations, evaluated using NLG metrics, and tests their impact on PLMs and LLMs in inference tasks.

Result: Automated explanations are highly competitive with human-annotated ones in improving model performance.

Conclusion: The framework offers a scalable, automated approach for generating textual explanations to enhance NLP datasets and model performance.

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [30] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: The paper explores practitioners' perspectives on explainable NLP, revealing gaps, low satisfaction, and challenges, advocating for clearer definitions and user-centric frameworks.


<details>
  <summary>Details</summary>
Motivation: The opacity of complex NLP models necessitates transparency and explanations, especially in high-stakes environments, yet practitioners' views on adoption and effectiveness are underexplored.

Method: A qualitative interview-based study with industry practitioners and academic researchers to analyze motivations, techniques, satisfaction, and challenges in explainable NLP.

Result: Findings show conceptual gaps, low satisfaction with current methods, and evaluation challenges, underscoring the need for better frameworks.

Conclusion: Clearer definitions and user-centric frameworks are essential for effective adoption of explainable NLP in practice.

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [31] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: BigCharts introduces a dataset creation pipeline and training framework to improve chart comprehension in vision-language models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with chart comprehension due to low-quality datasets and lack of diversity, limiting their effectiveness.

Method: Proposes BigCharts, a pipeline for diverse chart images using real-world data, and integrates supervised fine-tuning with GRPO-based reinforcement learning.

Result: BigCharts-R1 achieves state-of-the-art performance on chart question-answering benchmarks, surpassing larger models.

Conclusion: The approach enhances model robustness and generalization, addressing limitations of existing datasets and training methods.

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [32] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper surveys clinical mental health datasets for AI development, highlighting gaps like lack of longitudinal data and cultural diversity, and offers recommendations for better datasets.


<details>
  <summary>Details</summary>
Motivation: The rise in mental health disorders and shortage of clinicians necessitates AI assistance, but current datasets are scattered and inadequate.

Method: The study categorizes datasets by disorder, modality, task, accessibility, and sociocultural context, and evaluates synthetic datasets.

Result: Identified gaps include inconsistent standards, limited diversity, and lack of longitudinal data.

Conclusion: Recommends improving dataset curation and standardization for more robust and equitable AI mental health systems.

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [33] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: A survey on innovative LLM architectures addressing transformer limitations to improve efficiency, covering methods like linear/sparse modeling, efficient attention variants, and hybrid models.


<details>
  <summary>Details</summary>
Motivation: Traditional transformers are computationally heavy, hindering large-scale training and deployment. The survey aims to explore efficient alternatives.

Method: Examines linear/sparse sequence modeling, efficient attention variants, sparse mixture-of-experts, hybrid architectures, and diffusion LLMs.

Result: Provides a blueprint for modern efficient LLM architectures, grouping studies into actionable categories.

Conclusion: The survey motivates future research toward scalable, resource-aware AI systems by highlighting efficient LLM designs.

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [34] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: PRELUDE is a benchmark for evaluating long-context understanding by assessing the consistency of prequel stories with original narratives, revealing significant gaps in model performance compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the need for benchmarks that demand global comprehension and deep reasoning, particularly for tasks requiring integration of indirectly related information.

Method: Uses a task of evaluating prequel consistency, requiring evidence from multiple parts of the narrative. Tests state-of-the-art LLMs, RAG, and commercial services against human performance.

Result: Models lag behind humans by >15%, with a 30% gap in reasoning accuracy due to flawed reasoning despite correct answers.

Conclusion: Highlights the challenge and need for improvement in long-context understanding and reasoning.

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [35] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: The study evaluates lightweight Whisper models for Urdu speech recognition in low-resource settings, finding Whisper-Small performs best (33.68% WER) but highlights persistent challenges.


<details>
  <summary>Details</summary>
Motivation: Urdu, despite its large speaker base, lacks robust ASR systems due to dialectal diversity, code-switching, and limited training data.

Method: Benchmarked Tiny, Base, and Small Whisper models on a curated Urdu dataset using WER without fine-tuning.

Result: Whisper-Small achieved the lowest WER (33.68%), outperforming Tiny (67.08%) and Base (53.67%). Challenges in phonetic accuracy and lexical coherence persist.

Conclusion: Whisper-Small shows promise for Urdu ASR, but gaps remain, calling for future research in low-resource ASR systems.

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [36] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: Memory Decoder is a plug-and-play pretrained memory for efficient domain adaptation in LLMs, avoiding costly training and inference latency.


<details>
  <summary>Details</summary>
Motivation: Adapting LLMs to specific domains is challenging due to high costs and performance issues with current methods like DAPT and RAG.

Method: Memory Decoder uses a small transformer decoder to mimic an external retriever, enabling seamless integration with pretrained models.

Result: It reduces perplexity by 6.17 points on average across biomedicine, finance, and law domains.

Conclusion: Memory Decoder offers a novel, efficient paradigm for domain-specific adaptation without modifying original model parameters.

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [37] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: This survey reviews 38 studies on NLP for detecting cognitive distortions (CDs) in mental health, addressing inconsistencies in taxonomies, tasks, and evaluations.


<details>
  <summary>Details</summary>
Motivation: The growing interest in NLP for mental health lacks coherence due to fragmented research on cognitive distortions (CDs).

Method: The paper reviews 38 studies over two decades, analyzing datasets, modeling approaches, and evaluation strategies.

Result: A consolidated CD taxonomy, common task setups, and open challenges are identified to improve research coherence.

Conclusion: The survey aims to support more reproducible and structured research in NLP for cognitive distortions.

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [38] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: The paper explores how digitized business communication enables both transparency and deception, proposing a method to detect deceptive language using persuasive lexicon and computational analysis. High accuracy (99%) was achieved in controlled settings, but multilingual applications face challenges due to data scarcity and infrastructure limitations.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the growing gap between theoretical communication models and empirical realities, especially as AI-driven discourse becomes more realistic and deceptive language proliferates in digital business communication.

Method: The research combines classical rhetoric, communication psychology, linguistic theory, and empirical studies in financial reporting, sustainability discourse, and digital marketing. It employs computational textual analysis and personalized transformer models for detection.

Result: Detection accuracies exceeded 99% in controlled settings. However, multilingual applications struggled due to insufficient data and lack of text-processing infrastructure.

Conclusion: The findings highlight the need for robust automatic text-identification systems to bridge the gap between theory and practice in AI-driven communication, particularly in multilingual contexts.

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [39] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: The paper introduces a multi-dimensional evaluation framework for comparing alignment techniques in LLMs, assessing detection, quality, efficiency, and robustness.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLM outputs align with human values and safety standards is critical, but current evaluation lacks systematic comparison.

Method: The paper proposes a framework evaluating alignment techniques across four dimensions: detection, quality, efficiency, and robustness.

Result: Experiments show the framework effectively identifies strengths and limitations of state-of-the-art alignment methods.

Conclusion: The framework provides valuable insights for future research and deployment of aligned LLMs.

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [40] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex integrates vision and coding models for multimodal code generation, using a task vector-based merging technique. It introduces new datasets (MCD, InfiBench-V) and achieves near-GPT-4o performance.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack strong multimodal code generation capabilities, limiting their practical utility.

Method: Task vector-based merging of a coding LLM into a vision-language backbone, supported by the MCD dataset and InfiBench-V benchmark.

Result: VisCodex achieves state-of-the-art performance among open-source MLLMs, nearing proprietary models like GPT-4o.

Conclusion: The framework and datasets effectively enhance multimodal code generation, bridging gaps in MLLM capabilities.

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [41] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: Medical and domain-specific tokenizers outperform general ones in radiology report summarization, especially when models are trained from scratch. Pre-training reduces performance gaps, but domain-specific tokenizers remain superior, offering efficiency benefits.


<details>
  <summary>Details</summary>
Motivation: The impact of tokenizer vocabulary on text generation quality in radiology is under-explored. This work aims to systematically compare tokenizers for radiology report summarization.

Method: Comparison of general, medical, and domain-specific tokenizers on radiology report summarization across three imaging modalities, with and without LM pre-training on PubMed abstracts.

Result: Medical and domain-specific tokenizers outperform general ones, especially without pre-training. Pre-training reduces gaps, but domain-specific tokenizers still lead. They also reduce memory usage.

Conclusion: Adapting LM vocabulary to the clinical domain improves performance and efficiency, making models more accessible and effective for healthcare applications.

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [42] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: The paper introduces a method to improve emotion annotation reliability by adding contextual narratives to event descriptions, showing that enriched contexts lead to more consistent annotations.


<details>
  <summary>Details</summary>
Motivation: Emotion analysis is ambiguous, and prior work focused on annotator properties, ignoring the role of missing contextual information. This paper explores whether adding context can improve annotation reliability.

Method: The approach generates multiple event chains conditioned on different emotions, using short story generation techniques to create coherent narratives for contextualized emotion analysis.

Result: Contextual narratives improve emotion interpretation and help annotators produce more consistent annotations, as confirmed by automatic and human evaluations.

Conclusion: Enriched contexts enhance emotion analysis reliability, demonstrating the value of contextualized narratives in disambiguating emotions.

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [43] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: GPT-5-high achieves top accuracy in medical QA tasks, outperforming other models except o3-high, and offers cost-efficient configurations.


<details>
  <summary>Details</summary>
Motivation: To determine optimal configurations of GPT-5 for accuracy and cost-efficiency in complex medical question-answering tasks.

Method: Evaluated 12 GPT-5 configurations and other models on 260 ophthalmology questions, measuring accuracy, rationale quality, and cost.

Result: GPT-5-high led in accuracy (0.965) and rationale quality, with GPT-5-mini-low being cost-efficient.

Conclusion: GPT-5 excels in ophthalmology QA, with reasoning effort impacting accuracy, and introduces a scalable evaluation framework.

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [44] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: The paper addresses the lack of speech-to-text (STT) systems for the Badini Kurdish dialect by developing and evaluating language models using Wav2Vec2-Large-XLSR-53 and Whisper-small, with Wav2Vec2 outperforming Whisper in accuracy and readability.


<details>
  <summary>Details</summary>
Motivation: To provide STT systems for the under-resourced Badini Kurdish dialect, aiding its community in technology use and increasing global visibility.

Method: Used Badini kids' stories (78 stories from 8 books) as textual input, recorded by six narrators (~17 hours). Preprocessed data (15 hours of speech, 19193 segments, 25221 words) and developed models using Wav2Vec2-Large-XLSR-53 and Whisper-small.

Result: Wav2Vec2-Large-XLSR-53 outperformed Whisper-small with 90.38% readability and 82.67% accuracy, compared to 65.45% and 53.17% for Whisper.

Conclusion: The Wav2Vec2 model is more effective for Badini STT, highlighting its potential for under-resourced languages.

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [45] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: A neural contextual bandit-based algorithm is proposed to select sequences of LLMs for complex tasks, improving success rates and reducing costs by learning performance dependencies between subtasks.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently select sequences of LLMs for specialized tasks, where single LLMs may fail, and to account for performance dependencies between subtasks.

Method: A neural contextual bandit-based algorithm trains neural networks to model LLM success on subtasks online, guiding LLM selections dynamically.

Result: Experiments show the proposed approach outperforms existing LLM selection algorithms in tasks like telecommunications QA and medical diagnosis prediction.

Conclusion: The method effectively addresses the challenge of selecting LLM sequences for complex tasks by learning and adapting to performance dependencies.

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: A novel multimodal framework for detecting misogynistic and sexist content on social media, outperforming existing methods by 10.17% and 8.88% in macro-F1 on two datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting misogynistic content, which general offensive content detection methods struggle with, by tailoring solutions for offensive content against women.

Method: Proposes a framework with three modules: Multimodal Attention (MANM) for context-aware attention, Graph-based Feature Reconstruction (GFRM) for refining features, and Content-specific Features Learning (CFLM) for learning text/image-specific features. Uses misogynous lexicons and test-time augmentation.

Result: Achieves average improvements of 10.17% and 8.88% in macro-F1 on MAMI and MMHS150K datasets, respectively.

Conclusion: The proposed multimodal framework effectively detects misogynistic content, outperforming existing methods and demonstrating its potential for real-world applications.

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [47] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: IAD-R1 is a post-training framework for Vision-Language Models (VLMs) that enhances anomaly detection in industrial settings through a two-stage training strategy, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection methods are limited by scarce defective samples, and VLMs, despite their generalization advantages, underperform in industrial anomaly detection.

Method: IAD-R1 uses a two-stage approach: PA-SFT for anomaly perception training with Expert-AD dataset, and SC-GRPO for refining anomaly interpretation with reward functions.

Result: IAD-R1 improves average accuracy by up to 43.3% across 7 VLMs on 6 benchmarks, outperforming models like GPT-4.1 in zero-shot settings.

Conclusion: IAD-R1 is effective and superior, with its dataset, code, and model weights made publicly available.

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [48] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: CADAR is a neurosymbolic approach for detecting cognitive attacks in AR, combining neural VLMs with symbolic reasoning for improved accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting cognitive attacks in AR lack semantic reasoning or rely on opaque black-box models, limiting their effectiveness and interpretability.

Method: CADAR fuses vision-language inputs into a symbolic perception-graph, using particle-filter based statistical reasoning for attack detection.

Result: Experiments show CADAR improves accuracy by up to 10.7% over baselines in challenging AR attack scenarios.

Conclusion: CADAR demonstrates the potential of neurosymbolic methods for effective and interpretable cognitive attack detection in AR.

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [49] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: RL-MoE is a framework that converts visual data into privacy-preserving text descriptions, balancing accuracy and privacy using MoE and RL.


<details>
  <summary>Details</summary>
Motivation: Addressing the conflict between data utility and privacy in AI-powered ITS cameras, where existing methods like blurring or encryption are inadequate.

Method: Combines Mixture-of-Experts (MoE) for scene decomposition and Reinforcement Learning (RL) to optimize text for accuracy and privacy.

Result: Reduces replay attack success to 9.4% on CFP-FP dataset while generating richer text than baselines.

Conclusion: RL-MoE offers a scalable solution for privacy-sensitive AI systems, enhancing security in smart cities and autonomous vehicles.

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [50] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: A framework for synthetic depth face generation using optimized GAN with Knowledge Distillation and Genetic Algorithms improves diversity and quality, outperforming other methods. Feature extraction achieves high classification accuracy.


<details>
  <summary>Details</summary>
Motivation: The lack of high-quality, diverse depth facial datasets for recognizing subtle emotional expressions motivates the need for synthetic data generation.

Method: Uses an optimized GAN with Knowledge Distillation (EMA teacher models) and Genetic Algorithms to evolve GAN latent vectors. Feature extraction combines LBP, HOG, Sobel edge, and intensity histogram for classification.

Result: Outperforms GAN, VAE, GMM, and KDE in diversity and quality. Achieves 94% and 96% accuracy with XGBoost. Evaluation metrics (FID, IS, SSIM, PSNR) show consistent improvement.

Conclusion: The proposed framework effectively addresses the challenge of generating high-quality, diverse synthetic depth faces and achieves superior performance in classification.

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [51] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: A framework called Δ-AttnMask is proposed for efficient data selection in Visual Instruction Finetuning (VIF), achieving high performance with only 20% of data.


<details>
  <summary>Details</summary>
Motivation: VIF requires multimodal data for joint visual-textual understanding, posing stricter data selection challenges, which are understudied.

Method: Δ-AttnMask uses attention-guided masking to quantify sample quality by computing loss differences between original and masked states, without needing extra labels or models.

Result: The method achieves state-of-the-art performance with 20% of data, speeding up training by 5x and improving accuracy by +10.1%.

Conclusion: Δ-AttnMask is model-agnostic and data-agnostic, making it broadly applicable for VIF tasks.

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [52] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: The paper proposes Personalized Feature Translation (PFT) for source-free domain adaptation (SFDA) in facial expression recognition, addressing challenges with subtle expressions and single-class target data.


<details>
  <summary>Details</summary>
Motivation: Deep FER models struggle with subtle expressions and inter-subject variability, and SFDA methods are not designed for single-class target data.

Method: PFT operates in latent space, pre-training a translator on source data and adapting it to neutral target data without image synthesis.

Result: PFT avoids image generation complexity, reduces computational overhead, and produces discriminative embeddings for classification.

Conclusion: PFT is an efficient, lightweight alternative to image-based translation for SFDA in FER.

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [53] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: C-GAN is the most effective model for generating high-quality, human-like colorized anime images from sketches.


<details>
  <summary>Details</summary>
Motivation: The costly and time-consuming process of colorizing sketches in the manga and anime industry drives the need for efficient automated solutions.

Method: Evaluated Neural Style Transfer, C-GAN, and CycleGAN for image-to-image translation between sketches and anime characters.

Result: C-GAN outperformed others, producing high-quality, high-resolution images resembling human-created ones.

Conclusion: C-GAN is the recommended model for automating colorization in the manga and anime industry.

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [54] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: The paper introduces MME-Emotion, a benchmark to evaluate multimodal large language models (MLLMs) on emotional understanding and reasoning, revealing their current limitations and strengths.


<details>
  <summary>Details</summary>
Motivation: Current emotional benchmarks for MLLMs lack assessment of generalization and reasoning capabilities, prompting the need for a comprehensive benchmark like MME-Emotion.

Method: MME-Emotion includes over 6,000 curated video clips with QA pairs, spanning eight emotional tasks, and uses hybrid metrics for evaluation.

Result: Evaluation of 20 MLLMs shows unsatisfactory emotional intelligence, with top models scoring 39.3% in recognition and 56.0% in reasoning.

Conclusion: MME-Emotion aims to advance MLLMs' emotional intelligence, highlighting the potential of both generalist and specialist models.

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [55] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: The paper introduces a four-axis framework to evaluate jailbreak attacks on multimodal large language models (MLLMs) and proposes a recursive rewriting strategy (BSD) to improve attack success rates and harmfulness.


<details>
  <summary>Details</summary>
Motivation: Current evaluation standards for jailbreak attacks on MLLMs overestimate effectiveness, as many 'successful' responses are benign or unrelated to malicious goals.

Method: A four-axis evaluation framework (on-topicness, OOD intensity, harmfulness, refusal rate) is introduced. The BSD strategy rewrites prompts to balance relevance and novelty.

Result: BSD improves attack success rates by 67% and harmfulness by 21% across 13 MLLMs, revealing weaknesses in safety systems.

Conclusion: Balancing relevance and novelty in prompts is key to evading filters and triggering harmful outputs, highlighting vulnerabilities in current MLLM safety mechanisms.

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [56] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: The paper introduces a scalable data engine to generate a large dataset (Tex80M) for HMER, combining handwritten and LaTeX-rendered formulas, and proposes TexTeller, a SOTA model trained on this dataset.


<details>
  <summary>Details</summary>
Motivation: HMER faces data scarcity due to costly manual annotation. The paper aims to bridge this gap by leveraging scalable data generation.

Method: Developed a data engine to create Tex80M (80M formulas) and trained TexTeller by mixing Tex80M with a small HME dataset.

Result: TexTeller achieves SOTA performance across benchmarks.

Conclusion: The complete model, dataset, and codebase will be openly released to advance HMER research.

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [57] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: GDAGS introduces gradient-direction-aware adaptive density control to improve 3D Gaussian Splatting, addressing over-reconstruction and over-densification while reducing memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods struggle with over-reconstruction and over-densification in complex scenarios, leading to inefficiencies and high memory costs.

Method: GDAGS uses a gradient coherence ratio (GCR) and a nonlinear dynamic weighting mechanism to adaptively control Gaussian density based on gradient directions.

Result: GDAGS achieves better rendering quality, reduces memory usage by 50%, and mitigates over-reconstruction and over-densification.

Conclusion: GDAGS effectively optimizes 3DGS performance by leveraging gradient direction awareness, offering compact and efficient scene representations.

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [58] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: FineState-Bench is introduced as the first evaluation standard for fine-grained GUI proxy operations, addressing flaws in current benchmarks by focusing on detailed control capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for GUI agents overlook fine-grained control, which is critical for real-world applications, prompting the need for a more comprehensive evaluation framework.

Method: The multi-platform framework includes 2257 task benchmarks and a four-phase indicator for assessment, alongside the Visual Diagnostic Assistant (VDA) for decoupling analysis of perception and positioning.

Result: Advanced models achieve only 32.8% fine-grained interaction accuracy, with ideal visual localization improving success rates by 14.9%.

Conclusion: The primary bottleneck for GUI proxies is basic visual positioning, and FineState-Bench provides an open-source solution for improved evaluation.

Abstract: With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [59] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: FiGPriv is a fine-grained privacy protection framework for VLMs that selectively masks high-risk private info, improving usability and privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns for blind/low vision users who may unintentionally capture private info in images, avoiding the usability drawbacks of coarse-grained masking.

Method: Combines fine-grained segmentation with a data-driven risk scoring mechanism to selectively mask high-risk private info.

Result: Preserves +26% of image content, improves VLM response usefulness by 11%, and enhances content identification by 45%.

Conclusion: FiGPriv effectively balances privacy and usability, outperforming existing methods.

Abstract: As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [60] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: The paper proposes input-adaptive navigation methods to enhance efficiency in vision-and-language navigation (VLN) models, reducing computations without performance loss.


<details>
  <summary>Details</summary>
Motivation: Large-scale VLN models face computational bottlenecks in resource-limited settings, and existing input-adaptive methods degrade performance.

Method: Three adaptive algorithms: (1) selective panoramic view processing, (2) importance-based early-exit thresholding, and (3) caching for previously seen views.

Result: Achieves over 2× computation reduction on seven VLN benchmarks without performance degradation.

Conclusion: The proposed methods significantly improve efficiency in VLN models, making them more practical for resource-constrained environments.

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [61] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: SegDAC, a Segmentation-Driven Actor-Critic method, leverages SAM and YOLO-World for object-centric decomposition and semantic grounding, achieving superior visual generalization and sample efficiency in RL.


<details>
  <summary>Details</summary>
Motivation: Visual RL struggles with high-dimensional inputs and noisy rewards. Existing perception models lack effective integration for generalization and efficiency.

Method: SegDAC uses SAM for object-centric decomposition, YOLO-World for semantic grounding, and a transformer-based architecture to dynamically focus on segments via online RL.

Result: SegDAC doubles prior performance on the hardest visual generalization benchmark and matches/surpasses sample efficiency in diverse tasks.

Conclusion: SegDAC effectively integrates perception and RL, advancing visual generalization and efficiency without human labels.

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [62] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: Lung-DDPM+ is an improved generative AI model for lung cancer diagnosis, offering higher efficiency and anatomical precision compared to its predecessor and other SOTA models.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for lung cancer diagnosis are inefficient and anatomically imprecise, limiting clinical use.

Method: Lung-DDPM+ uses a denoising diffusion probabilistic model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary DPM-solver.

Result: Achieves 8× fewer FLOPs, 6.8× lower GPU memory, 14× faster sampling, and maintains sample quality comparable to SOTA models.

Conclusion: Lung-DDPM+ effectively generates high-quality thoracic CT images, showing potential for broader medical imaging applications.

Abstract: Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [63] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: Ultralight Med-Vision Mamba, a state-space model, improves adenoma classification in colonoscopy screenings, enhancing risk assessment and enabling personalized surveillance.


<details>
  <summary>Details</summary>
Motivation: Precise identification of precancerous polyps is crucial for reducing colorectal cancer risk.

Method: Uses Ultralight Med-Vision Mamba, a state-space model, for adenoma classification and stratification.

Result: The model excels in long- and short-range dependencies, image generalization, and offers computational efficiency.

Conclusion: Ultralight Med-Vision Mamba is promising for real-time clinical deployment in colonoscopy screenings.

Abstract: Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [64] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: A real-time system translates eye blinks into Morse code for communication in motor-impaired individuals, achieving 62% accuracy with 18-20s response times.


<details>
  <summary>Details</summary>
Motivation: To provide a low-cost, accessible communication method for people with severe motor impairments.

Method: Uses a webcam and computer vision to detect and classify blinks as Morse code dots/dashes, then decodes them into characters.

Result: 62% decoding accuracy and 18-20 seconds response time in experiments with five participants.

Conclusion: The system is a viable, low-cost assistive communication tool for motor-impaired individuals.

Abstract: This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [65] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: FusionEnsemble-Net, an attention-based ensemble of spatiotemporal networks, improves sign language recognition by fusing visual and motion data, achieving 99.44% accuracy on the MultiMeDaLIS dataset.


<details>
  <summary>Details</summary>
Motivation: Accurate sign language recognition in healthcare is challenging due to complex multimodal gestures, requiring robust frameworks.

Method: Proposes FusionEnsemble-Net, which synchronously processes RGB video and radar data through four spatiotemporal networks with attention-based fusion and ensemble classification.

Result: Achieves 99.44% test accuracy on the MultiMeDaLIS dataset, outperforming state-of-the-art methods.

Conclusion: An ensemble of diverse spatiotemporal networks with attention-based fusion provides a robust solution for multimodal gesture recognition.

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [66] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: A dual-architecture framework for Continuous Sign Language Recognition (CSLR) addresses inter-signer variability and novel sentence structures, achieving state-of-the-art results on the Isharah-1000 dataset.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in CSLR, such as inter-signer variability and poor generalization to unseen sentences, which traditional methods fail to handle efficiently.

Method: Proposes a Signer-Invariant Conformer for signer-independent tasks and a Multi-Scale Fusion Transformer for unseen-sentence tasks, leveraging pose-based skeletal keypoints and dual-path temporal encoding.

Result: Achieves WER of 13.07% (SI task) and 47.78% (US task), surpassing prior work. Ranked 2nd in US and 4th in SI tasks in SignEval 2025.

Conclusion: Task-specific networks for CSLR challenges significantly improve performance, setting a new research baseline.

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [67] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: The paper introduces IMA++, a large multi-annotator skin lesion segmentation dataset, and studies variability in annotations due to factors like malignancy and annotator skill. It finds a significant link between inter-annotator agreement (IAA) and malignancy, predicts IAA from images, and uses IAA to improve segmentation models.


<details>
  <summary>Details</summary>
Motivation: To address variability in medical image segmentation caused by ambiguous boundaries and annotator differences, particularly in malignant lesions.

Method: Curated the IMA++ dataset, analyzed variability factors, predicted IAA from images, and integrated IAA into a multi-task learning model.

Result: Found a significant association between IAA and malignancy (p<0.001), predicted IAA with low error (MAE=0.108), and improved segmentation accuracy by 4.2%.

Conclusion: IMA++ and the use of IAA as a soft feature enhance segmentation models, demonstrating the impact of annotator variability on performance.

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [68] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: X-UniMotion is a unified latent representation for whole-body human motion, enabling high-fidelity cross-identity motion transfer from a single image using disentangled tokens for facial, body, and hand motions.


<details>
  <summary>Details</summary>
Motivation: Prior methods rely on explicit skeletal poses and heuristic adjustments, lacking expressiveness and identity-agnostic properties. X-UniMotion aims to overcome these limitations.

Method: A self-supervised framework jointly learns a motion encoder and latent representation with a DiT-based video generative model, using augmentations and synthetic 3D renderings for disentanglement.

Result: X-UniMotion outperforms state-of-the-art methods, producing expressive animations with superior motion fidelity and identity preservation.

Conclusion: The approach successfully achieves high-fidelity, detailed cross-identity motion transfer, validated by extensive experiments.

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [69] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2 introduces a novel transform-domain approach for SAR object detection, leveraging amplitude and phase modulation to outperform DenoDet V1 with improved accuracy and reduced complexity.


<details>
  <summary>Details</summary>
Motivation: The challenge of coherent noise in SAR object detection motivates exploring transform-domain feature modulation for better denoising.

Method: DenoDet V2 uses a band-wise mutual modulation mechanism to enhance phase and amplitude spectra via a designed attention architecture.

Result: DenoDet V2 achieves a 0.8% improvement on SARDet-100K and halves model complexity compared to V1.

Conclusion: DenoDet V2 sets a new benchmark for SAR object detection by effectively addressing noise through transform-domain innovation.

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object
detection lies in the pervasive influence of coherent noise. As a common
practice, most existing methods, whether handcrafted approaches or deep
learning-based methods, employ the analysis or enhancement of object
spatial-domain characteristics to achieve implicit denoising. In this paper, we
propose DenoDet V2, which explores a completely novel and different perspective
to deconstruct and modulate the features in the transform domain via a
carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2
is a major advancement that exploits the complementary nature of amplitude and
phase information through a band-wise mutual modulation mechanism, which
enables a reciprocal enhancement between phase and amplitude spectra. Extensive
experiments on various SAR datasets demonstrate the state-of-the-art
performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\%
improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the
model complexity by half. The code is available at
https://github.com/GrokCV/GrokSAR.

</details>


### [70] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: SkyShield is an event-driven framework for detecting submillimeter obstacles like wires, using a U-Net and Dice-Contour Loss, achieving high accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: Thin obstacles are hard to detect with conventional sensors, posing risks to drones in complex environments.

Method: Uses a lightweight U-Net and Dice-Contour Regularization Loss for precise detection in event streams.

Result: Achieves a mean F1 Score of 0.7088 with 21.2 ms latency, suitable for edge/mobile platforms.

Conclusion: SkyShield is effective for real-time, accurate detection of thin obstacles in drone applications.

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [71] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: A low-cost, on-premise system for autonomous bird monitoring in urban gardens uses motion-triggered cameras and local processing for privacy and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable citizen-science-grade biodiversity logging at home while preserving privacy and avoiding cloud fees.

Method: Uses a motion-triggered IP camera, Detectron2 for bird localization, and EfficientNet-B3 for species classification on commodity hardware.

Result: High validation performance (99.5%) and practical field accuracy (88% top-1) on held-out species.

Conclusion: Demonstrates feasibility for low-cost, autonomous bird monitoring in urban settings.

Abstract: This paper presents a low cost, on premise system for autonomous backyard
bird monitoring in Belgian urban gardens. A motion triggered IP camera uploads
short clips via FTP to a local server, where frames are sampled and birds are
localized with Detectron2; cropped regions are then classified by an
EfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a
larger Kaggle corpus. All processing runs on commodity hardware without a
discrete GPU, preserving privacy and avoiding cloud fees. The physical feeder
uses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.
Detector-guided cropping improves classification accuracy over raw-frame
classification. The classifier attains high validation performance on the
curated subset (about 99.5 percent) and delivers practical field accuracy
(top-1 about 88 percent) on held-out species, demonstrating feasibility for
citizen-science-grade biodiversity logging at home.

</details>


### [72] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: Waymo-3DSkelMo is a large-scale dataset offering high-quality 3D skeletal motions for multi-person interactions, derived from LiDAR data, addressing limitations of monocular RGB-based datasets.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for pedestrian interaction understanding suffer from occlusion and temporal discontinuity due to reliance on monocular RGB video frames.

Method: Utilizes 3D human body shape and motion priors to enhance 3D pose sequences extracted from LiDAR point clouds.

Result: The dataset includes 14,000+ seconds of data across 800+ driving scenarios, featuring rich interactions among up to 250 agents per scene.

Conclusion: Waymo-3DSkelMo serves as a foundational resource for fine-grained human behavior research in urban environments, with benchmarks demonstrating its value.

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [73] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: RampNet introduces a two-stage pipeline for large-scale, high-quality curb ramp detection, achieving state-of-the-art performance with a dataset of 210,000 annotated GSV panoramas and a modified ConvNeXt V2 model.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale, high-quality datasets for curb ramp detection hinders urban accessibility improvements. Existing datasets are limited in quality or scale.

Method: A two-stage pipeline: Stage 1 auto-translates government curb ramp data into pixel coordinates in GSV panoramas. Stage 2 trains a modified ConvNeXt V2 model on this dataset.

Result: The dataset achieves 94.0% precision and 92.5% recall. The detection model reaches 0.9236 AP, outperforming prior work.

Conclusion: RampNet provides the first large-scale, high-quality curb ramp detection dataset, benchmark, and model, advancing urban accessibility.

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [74] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: GOAL is a generative flow-based framework for Object Goal Navigation that uses LLM-enriched semantic maps to model uncertainty and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Prior deterministic models overlook uncertainty in indoor layouts, limiting generalization. GOAL addresses this by leveraging LLM-enriched semantic maps.

Method: GOAL uses generative flow-based modeling, injecting LLM-inferred spatial priors as Gaussian fields into semantic maps for better completions.

Result: GOAL achieves state-of-the-art performance on MP3D and Gibson datasets and shows strong generalization on HM3D.

Conclusion: GOAL's generative approach outperforms deterministic models, demonstrating the value of modeling uncertainty and leveraging LLM knowledge for ObjectNav.

Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.

</details>


### [75] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: The paper introduces PaIR-Net, a framework for predicting action semantics and body-part contact regions in visual contexts, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to jointly model action semantics and spatial contextualization, limiting comprehensive action understanding.

Method: PaIR-Net includes CPAM, PGCS, and IIM modules for contact identification, segmentation, and interaction inference, using the PaIR dataset.

Result: PaIR-Net outperforms baselines, with ablation studies validating each module's contribution.

Conclusion: The framework advances action understanding by integrating action semantics and spatial context, with code and dataset to be released.

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [76] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: The paper introduces Motion Prompt Tuning (MPT) to adapt large pre-training models (LMs) for micro-expression recognition (MER), addressing the challenge of scarce training samples and subtle motion capture.


<details>
  <summary>Details</summary>
Motivation: Micro-expression recognition is vital for applications like medical diagnosis and lie detection, but acquiring annotations is difficult, and datasets lack sufficient samples. LMs struggle with subtle facial movements, necessitating a tailored approach.

Method: MPT involves motion prompt generation (motion magnification and Gaussian tokenization) and a group adapter to enhance LMs for MER, enabling better capture of subtle motions.

Result: Experiments on three MER datasets show MPT outperforms state-of-the-art methods, proving its effectiveness.

Conclusion: MPT successfully adapts LMs for MER, offering a novel solution to the challenges of subtle motion recognition and limited training data.

Abstract: Micro-expression recognition (MER) is crucial in the affective computing
field due to its wide application in medical diagnosis, lie detection, and
criminal investigation. Despite its significance, obtaining micro-expression
(ME) annotations is challenging due to the expertise required from
psychological professionals. Consequently, ME datasets often suffer from a
scarcity of training samples, severely constraining the learning of MER models.
While current large pre-training models (LMs) offer general and discriminative
representations, their direct application to MER is hindered by an inability to
capture transitory and subtle facial movements-essential elements for effective
MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to
adapting LMs for MER, representing a pioneering method for subtle motion prompt
tuning. Particularly, we introduce motion prompt generation, including motion
magnification and Gaussian tokenization, to extract subtle motions as prompts
for LMs. Additionally, a group adapter is carefully designed and inserted into
the LM to enhance it in the target MER domain, facilitating a more nuanced
distinction of ME representation. Furthermore, extensive experiments conducted
on three widely used MER datasets demonstrate that our proposed MPT
consistently surpasses state-of-the-art approaches and verifies its
effectiveness.

</details>


### [77] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: The paper introduces Retrieval-Augmented Super Resolution (RASR), a practical RefSR paradigm that automatically retrieves relevant high-resolution references for enhancing low-quality images, addressing the limitation of manual target-reference pairs.


<details>
  <summary>Details</summary>
Motivation: Existing RefSR methods rely on manually curated image pairs, limiting real-world practicality. The goal is to enable scalable and flexible RefSR by automating reference retrieval.

Method: Proposes RASR, which retrieves semantically relevant references from a database, and RASRNet, combining a semantic retriever with a diffusion-based generator.

Result: RASRNet outperforms SISR baselines with +0.38 dB PSNR and -0.0131 LPIPS, generating more realistic textures.

Conclusion: Retrieval augmentation bridges the gap between academic RefSR research and real-world applicability, demonstrated by RASRNet's performance.

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super
Resolution (SISR) by leveraging high-quality reference images to enhance
texture fidelity and visual realism. However, a critical limitation of existing
RefSR approaches is their reliance on manually curated target-reference image
pairs, which severely constrains their practicality in real-world scenarios. To
overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new
and practical RefSR paradigm that automatically retrieves semantically relevant
high-resolution images from a reference database given only a low-quality
input. This enables scalable and flexible RefSR in realistic use cases, such as
enhancing mobile photos taken in environments like zoos or museums, where
category-specific reference data (e.g., animals, artworks) can be readily
collected or pre-curated. To facilitate research in this direction, we
construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike
prior datasets with fixed target-reference pairs, RASR-Flickr30 provides
per-category reference databases to support open-world retrieval. We further
propose RASRNet, a strong baseline that combines a semantic reference retriever
with a diffusion-based RefSR generator. It retrieves relevant references based
on semantic similarity and employs a diffusion-based generator enhanced with
semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet
consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131
LPIPS, while generating more realistic textures. These findings highlight
retrieval augmentation as a promising direction to bridge the gap between
academic RefSR research and real-world applicability.

</details>


### [78] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: HyperKD is a knowledge distillation framework for hyperspectral remote sensing, transferring learned representations from a simpler teacher model to a student model, overcoming spectral disparities and improving downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Direct application of foundation models to hyperspectral remote sensing is challenging due to spectral disparities and limited observations.

Method: HyperKD uses a feature-based strategy with spectral range-based channel alignment, spatial feature-guided masking, and an enhanced loss function for hyperspectral images.

Result: HyperKD improves representation learning, enhancing reconstruction fidelity and performance on tasks like land cover classification and crop type identification.

Conclusion: HyperKD effectively bridges spectral domain gaps, demonstrating the potential of knowledge distillation in hyperspectral remote sensing analytics.

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [79] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Animate-X++ is a universal animation framework for various character types, including anthropomorphic ones, addressing motion modeling and static background limitations. It introduces Pose Indicator and multi-task training for realistic videos.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited to human figures and static backgrounds, lacking realism and generalization for anthropomorphic characters.

Method: Proposes Animate-X++ with Pose Indicator for motion representation and multi-task training for background dynamics.

Result: Animate-X++ outperforms existing methods, achieving realistic animations with dynamic backgrounds.

Conclusion: Animate-X++ is effective for universal character animation, validated by experiments and the new A2Bench benchmark.

Abstract: Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Furthermore, previous methods could
only generate videos with static backgrounds, which limits the realism of the
videos. For the first challenge, our in-depth analysis suggests to attribute
this limitation to their insufficient modeling of motion, which is unable to
comprehend the movement pattern of the driving video, thus imposing a pose
sequence rigidly onto the target character. To this end, this paper proposes
Animate-X++, a universal animation framework based on DiT for various character
types, including anthropomorphic characters. To enhance motion representation,
we introduce the Pose Indicator, which captures comprehensive motion pattern
from the driving video through both implicit and explicit manner. The former
leverages CLIP visual features of a driving video to extract its gist of
motion, like the overall movement pattern and temporal relations among motions,
while the latter strengthens the generalization of DiT by simulating possible
inputs in advance that may arise during inference. For the second challenge, we
introduce a multi-task training strategy that jointly trains the animation and
TI2V tasks. Combined with the proposed partial parameter training, this
approach achieves not only character animation but also text-driven background
dynamics, making the videos more realistic. Moreover, we introduce a new
Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of
Animate-X++ on universal and widely applicable animation images. Extensive
experiments demonstrate the superiority and effectiveness of Animate-X++.

</details>


### [80] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: The paper introduces IAG, a novel input-aware backdoor attack method for vision-language models (VLMs), manipulating grounding behavior to target specific objects regardless of user queries. It uses adaptive triggers and ensures stealthiness with minimal visual discrepancies.


<details>
  <summary>Details</summary>
Motivation: Security issues in VLMs, especially backdoor attacks in visual grounding tasks, are underexplored. The paper aims to address this gap by proposing a method to manipulate model behavior stealthily.

Method: IAG employs an adaptive trigger generator using a text-conditional U-Net to embed semantic attack targets into images. It minimizes visual discrepancies with reconstruction loss and provides a unified attack data generation method.

Result: IAG achieves over 65% ASR@0.5 on InternVL-2.5-8B and shows effectiveness on Ferret-7B and LlaVA-1.5-7B with minimal accuracy drop on clean samples.

Conclusion: IAG is a robust, transferable, and stealthy backdoor attack method for VLMs, demonstrated through extensive experiments, including ablation studies and defense evaluations.

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [81] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: RelayFormer is a unified architecture for visual manipulation localization (VML) in images and videos, using a Global-Local Relay Attention mechanism for scalable, resolution-agnostic processing.


<details>
  <summary>Details</summary>
Motivation: Existing VML methods lack cross-modal generalization and struggle with high-resolution or long-duration inputs.

Method: RelayFormer employs flexible local units and a GLoRA mechanism, integrating with Transformer-based backbones via lightweight adaptation modules. It includes a query-based mask decoder for efficient video inference.

Result: Achieves state-of-the-art localization performance across benchmarks, setting a new baseline for scalable and modality-agnostic VML.

Conclusion: RelayFormer offers a scalable, efficient, and unified solution for VML, compatible with existing backbones and superior to prior methods.

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [82] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: GEN-AFFECT is a framework for generating expressive, identity-consistent 2D avatars using a multimodal diffusion transformer and consistent attention.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained facial expressions and identity preservation.

Method: Uses a multimodal diffusion transformer conditioned on identity-expression representations and employs consistent attention for identity consistency.

Result: Outperforms state-of-the-art methods in expression accuracy, identity preservation, and consistency.

Conclusion: GEN-AFFECT effectively addresses the limitations of current avatar generation methods.

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [83] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: The paper introduces an energy-efficient robust fitting method for computer vision using neuromorphic computing, specifically the Intel Loihi 2 hardware, achieving 15% of the energy consumption of traditional CPU methods.


<details>
  <summary>Details</summary>
Motivation: Energy efficiency in robust fitting is overlooked despite its importance for AI adoption. The paper addresses this gap by leveraging neuromorphic computing.

Method: A novel spiking neural network is designed for robust fitting on Intel Loihi 2, with event-driven model estimation and algorithmic adaptations for hardware constraints.

Result: The neuromorphic approach consumes only 15% of the energy of standard CPU methods while maintaining equivalent accuracy.

Conclusion: Neuromorphic computing offers a promising, energy-efficient solution for robust fitting in computer vision.

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [84] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: CitySeg is a foundation model for city-scale point cloud semantic segmentation, using text modality for open vocabulary and zero-shot inference, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing models due to small-scale 3D data and domain gaps, which reduce generalization.

Method: Custom data preprocessing, local-global cross-attention network, hierarchical classification strategy, two-stage training with hinge loss.

Result: SOTA performance on nine benchmarks and zero-shot generalization in city-scale point clouds.

Conclusion: CitySeg effectively overcomes domain gaps and label discrepancies, enabling robust semantic segmentation without visual data.

Abstract: Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.

</details>


### [85] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: The paper proposes FTNet, a few-shot training-free method for deepfake detection, outperforming existing methods by 8.7% by leveraging failed samples for improvement.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of poor generalization to unseen deepfake samples, the paper shifts from zero-shot to few-shot detection, focusing on realism rather than semantics.

Method: FTNet uses one fake sample from an evaluation set without training, comparing test samples to known fake/real samples for classification.

Result: Achieves state-of-the-art performance, improving accuracy by 8.7% on images from 29 generative models.

Conclusion: Leveraging failed samples in few-shot scenarios enhances deepfake detection, offering a practical real-world solution.

Abstract: Recent deepfake detection studies often treat unseen sample detection as a
``zero-shot" task, training on images generated by known models but
generalizing to unknown ones. A key real-world challenge arises when a model
performs poorly on unknown samples, yet these samples remain available for
analysis. This highlights that it should be approached as a ``few-shot" task,
where effectively utilizing a small number of samples can lead to significant
improvement. Unlike typical few-shot tasks focused on semantic understanding,
deepfake detection prioritizes image realism, which closely mirrors real-world
distributions. In this work, we propose the Few-shot Training-free Network
(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet
differs from traditional methods that rely on large-scale known data for
training. Instead, FTNet uses only one fake samplefrom an evaluation set,
mimicking the scenario where new samples emerge in the real world and can be
gathered for use, without any training or parameter updates. During evaluation,
each test sample is compared to the known fake and real samples, and it is
classified based on the category of the nearest sample. We conduct a
comprehensive analysis of AI-generated images from 29 different generative
models and achieve a new SoTA performance, with an average improvement of 8.7\%
compared to existing methods. This work introduces a fresh perspective on
real-world deepfake detection: when the model struggles to generalize on a
few-shot sample, leveraging the failed samples leads to better performance.

</details>


### [86] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: The paper introduces a Mixture of Facial Experts (MoFE) and a tailored data pipeline to improve identity preservation in video generation under large facial angles, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with identity preservation under large facial angles due to ineffective feature integration and lack of targeted dataset coverage.

Method: Proposes MoFE, combining three experts for identity, semantics, and details, and a data pipeline with Face Constraints and Identity Consistency to create the LFA Dataset.

Result: Outperforms SOTA methods in face similarity, face FID, and CLIP semantic alignment on the LFA benchmark.

Conclusion: The MoFE and LFA Dataset effectively address identity preservation challenges, with code and dataset made publicly available.

Abstract: Current video generation models struggle with identity preservation under
large facial angles, primarily facing two challenges: the difficulty in
exploring an effective mechanism to integrate identity features into DiT
structure, and the lack of targeted coverage of large facial angles in existing
open-source video datasets. To address these, we present two key innovations.
First, we introduce a Mixture of Facial Experts (MoFE) that dynamically
combines complementary cues from three specialized experts, each designed to
capture distinct but mutually reinforcing aspects of facial attributes. The
identity expert captures cross-pose identity-sensitive features, the semantic
expert extracts high-level visual semantxics, and the detail expert preserves
pixel-level features (e.g., skin texture, color gradients). Furthermore, to
mitigate dataset limitations, we have tailored a data processing pipeline
centered on two key aspects: Face Constraints and Identity Consistency. Face
Constraints ensure facial angle diversity and a high proportion of facial
regions, while Identity Consistency preserves coherent person-specific features
across temporal sequences, collectively addressing the scarcity of large facial
angles and identity-stable training data in existing datasets. Leveraging this
pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from
existing open-source human video datasets, comprising 460K video clips with
annotated facial angles. Experimental results on the LFA benchmark demonstrate
that our method, empowered by the LFA dataset, significantly outperforms prior
SOTA methods in face similarity, face FID, and CLIP semantic alignment. The
code and dataset will be made publicly available at
https://github.com/rain152/LFA-Video-Generation.

</details>


### [87] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: A universal AI-generated image detector is proposed using anomaly detection, avoiding reliance on AIIs for training and leveraging proxy images for unsupervised learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the limited detection performance of conventional AII detectors for unseen generative models by adopting an anomaly detection approach.

Method: Uses a pre-trained CLIP encoder for feature extraction and a normalizing flow-like unsupervised model, trained with proxy images (spectrally modified natural images) instead of AIIs.

Result: Demonstrates effectiveness in detecting AIIs from various generative models.

Conclusion: The proposed detector is generalizable and effective without needing access to AIIs during training.

Abstract: With the rapid advancement of AI generative models, the visual quality of
AI-generated images (AIIs) has become increasingly close to natural images,
which inevitably raises security concerns. Most AII detectors often employ the
conventional image classification pipeline with natural images and AIIs
(generated by a generative model), which can result in limited detection
performance for AIIs from unseen generative models. To solve this, we proposed
a universal AI-generated image detector from the perspective of anomaly
detection. Our discriminator does not need to access any AIIs and learn a
generalizable representation with unsupervised learning. Specifically, we use
the pre-trained CLIP encoder as the feature extractor and design a normalizing
flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by
applying a spectral modification operation on natural images, are used for
training. Our models are trained by minimizing the likelihood of proxy images,
optionally combined with maximizing the likelihood of natural images. Extensive
experiments demonstrate the effectiveness of our method on AIIs produced by
various image generators.

</details>


### [88] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: GazeLT integrates radiologists' gaze patterns into a deep learning framework to improve long-tailed disease classification, outperforming baselines by significant margins.


<details>
  <summary>Details</summary>
Motivation: Radiologists' gaze patterns capture fine-grained and coarse disease information, which can enhance automated image interpretation, especially for long-tailed classes.

Method: GazeLT uses an integration-disintegration mechanism to harness temporal aspects of visual search, applied to NIH-CXR-LT and MIMIC-CXR-LT datasets.

Result: GazeLT outperforms the best long-tailed loss by 4.1% and the visual attention baseline by 21.7% in average accuracy.

Conclusion: GazeLT effectively leverages radiologists' gaze for improved long-tailed disease classification, demonstrating superior performance on large datasets.

Abstract: In this work, we present GazeLT, a human visual attention
integration-disintegration approach for long-tailed disease classification. A
radiologist's eye gaze has distinct patterns that capture both fine-grained and
coarser level disease related information. While interpreting an image, a
radiologist's attention varies throughout the duration; it is critical to
incorporate this into a deep learning framework to improve automated image
interpretation. Another important aspect of visual attention is that apart from
looking at major/obvious disease patterns, experts also look at
minor/incidental findings (few of these constituting long-tailed classes)
during the course of image interpretation. GazeLT harnesses the temporal aspect
of the visual search process, via an integration and disintegration mechanism,
to improve long-tailed disease classification. We show the efficacy of GazeLT
on two publicly available datasets for long-tailed disease classification,
namely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.
GazeLT outperforms the best long-tailed loss by 4.1% and the visual
attention-based baseline by 21.7% in average accuracy metrics for these
datasets. Our code is available at https://github.com/lordmoinak1/gazelt.

</details>


### [89] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: SkySplat is a self-supervised framework for 3D scene reconstruction from sparse-view satellite images, integrating RPC models into 3DGS for improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods struggle with satellite images due to RPC incompatibility and poor generalization, especially with multi-temporal data.

Method: SkySplat uses RGB images and radiometric-robust height supervision, featuring a Cross-Self Consistency Module (CSCM) and multi-view consistency aggregation.

Result: Achieves 86x speedup over EOGS, reduces MAE from 13.18m to 1.80m on DFC19, and shows strong cross-dataset generalization.

Conclusion: SkySplat effectively addresses limitations of existing methods, offering high efficiency and accuracy for satellite image reconstruction.

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.

</details>


### [90] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM improves long-form video understanding by modeling keyframes as episodic events, leveraging temporal dynamics and LLM reasoning for better accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for long-form video understanding oversimplify the problem by treating keyframes as static images, ignoring spatio-temporal relationships and yielding redundant or less informative frames.

Method: Video-EM models keyframes as temporally ordered episodic events, capturing spatial and temporal dynamics, and uses LLM-based chain of thought reasoning to select minimal yet informative frames.

Result: Video-EM outperforms baselines by 4-9% on benchmarks like Video-MME, EgoSchema, HourVideo, and LVBench, using fewer frames.

Conclusion: Video-EM provides a robust, training-free framework for long-form video understanding by mimicking human episodic memory and leveraging LLM reasoning.

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [91] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: The paper introduces SARE, a method for detecting fake images by measuring semantic differences between images and their caption-guided reconstructions, improving generalization across unseen generative models.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods fail against unseen generative models due to reliance on model-specific artifacts. Fake images often align closely with captions, unlike real images.

Method: Proposes Semantic-Aware Reconstruction Error (SARE) to quantify semantic shifts between images and their caption-guided reconstructions.

Result: SARE outperforms baselines on benchmarks like GenImage and CommunityForensics, showing strong generalization.

Conclusion: SARE provides a robust, generalizable solution for detecting fake images by leveraging semantic alignment with captions.

Abstract: Recently, diffusion-generated image detection has gained increasing
attention, as the rapid advancement of diffusion models has raised serious
concerns about their potential misuse. While existing detection methods have
achieved promising results, their performance often degrades significantly when
facing fake images from unseen, out-of-distribution (OOD) generative models,
since they primarily rely on model-specific artifacts. To address this
limitation, we explore a fundamental property commonly observed in fake images.
Motivated by the observation that fake images tend to exhibit higher similarity
to their captions than real images, we propose a novel representation, namely
Semantic-Aware Reconstruction Error (SARE), that measures the semantic
difference between an image and its caption-guided reconstruction. The
hypothesis behind SARE is that real images, whose captions often fail to fully
capture their complex visual content, may undergo noticeable semantic shifts
during the caption-guided reconstruction process. In contrast, fake images,
which closely align with their captions, show minimal semantic changes. By
quantifying these semantic shifts, SARE can be utilized as a discriminative
feature for robust detection across diverse generative models. We empirically
demonstrate that the proposed method exhibits strong generalization,
outperforming existing baselines on benchmarks including GenImage and
CommunityForensics.

</details>


### [92] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: CWFBind is a deep learning-based docking method that integrates local curvature features and degree-aware weighting to improve accuracy in predicting ligand-protein binding conformations.


<details>
  <summary>Details</summary>
Motivation: Traditional and some deep learning-based docking methods neglect geometric information, leading to inaccurate pocket localization and binding conformations.

Method: CWFBind uses local curvature descriptors for feature extraction, degree-aware weighting in message passing, and a ligand-aware dynamic radius strategy for pocket prediction.

Result: CWFBind achieves competitive performance in docking benchmarks, balancing accuracy and efficiency.

Conclusion: CWFBind enhances docking accuracy by incorporating geometric features and addressing class imbalance, offering a practical solution for drug design.

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [93] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: A GAN variant combining ProGAN and SAGAN is proposed for high-quality Indian Sign Language image generation, outperforming ProGAN in IS and FID. A large dataset is also released.


<details>
  <summary>Details</summary>
Motivation: Sign language generation is under-explored, and existing models lack balance between resolution and detail.

Method: Develop a GAN variant combining ProGAN and SAGAN for feature-rich, high-resolution, class-conditional sign language images.

Result: Improved Inception Score (3.2) and Fréchet Inception Distance (30.12) over ProGAN.

Conclusion: The proposed model effectively generates high-quality sign language images, and the released dataset supports further research.

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [94] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: The paper investigates Similar Object Interference (SOI) in Single Object Tracking (SOT), proposes SOIBench for semantic cognitive guidance, and introduces a VLM-based paradigm for improved tracking performance.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked issue of SOI in SOT and explore the potential of external cognitive guidance, particularly natural language, to enhance tracking robustness.

Method: Conducts controlled OIM experiments, constructs SOIBench for semantic guidance, and integrates large-scale VLMs into RGB trackers.

Result: Eliminating SOI improves SOT performance (AUC gains up to 4.35). VLMs outperform existing VLT methods (AUC gains up to 0.93).

Conclusion: SOIBench serves as a benchmark for semantic cognitive tracking, and VLM integration offers significant advancements over current methods.

Abstract: In this paper, we present the first systematic investigation and
quantification of Similar Object Interference (SOI), a long-overlooked yet
critical bottleneck in Single Object Tracking (SOT). Through controlled Online
Interference Masking (OIM) experiments, we quantitatively demonstrate that
eliminating interference sources leads to substantial performance improvements
(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a
primary constraint for robust tracking and highlighting the feasibility of
external cognitive guidance. Building upon these insights, we adopt natural
language as a practical form of external guidance, and construct SOIBench-the
first semantic cognitive guidance benchmark specifically targeting SOI
challenges. It automatically mines SOI frames through multi-tracker collective
judgment and introduces a multi-level annotation protocol to generate precise
semantic guidance texts. Systematic evaluation on SOIBench reveals a striking
finding: existing vision-language tracking (VLT) methods fail to effectively
exploit semantic cognitive guidance, achieving only marginal improvements or
even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we
propose a novel paradigm employing large-scale vision-language models (VLM) as
external cognitive engines that can be seamlessly integrated into arbitrary RGB
trackers. This approach demonstrates substantial improvements under semantic
cognitive guidance (AUC gains up to 0.93), representing a significant
advancement over existing VLT methods. We hope SOIBench will serve as a
standardized evaluation platform to advance semantic cognitive tracking
research and contribute new insights to the tracking research community.

</details>


### [95] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: SDT introduces a Context-Aware Gating mechanism for dynamic, data-dependent spatial decay in Vision Transformers, improving performance on spatially-structured tasks.


<details>
  <summary>Details</summary>
Motivation: ViTs lack explicit spatial inductive biases, leading to suboptimal performance on spatially-structured tasks. Existing methods use fixed spatial decay, limiting adaptability.

Method: SDT uses a novel Context-Aware Gating (CAG) mechanism to dynamically modulate spatial attention based on content relevance and spatial proximity, integrating spatial priors with learned content.

Result: SDT outperforms baselines on ImageNet-1K classification and generation tasks.

Conclusion: Data-dependent spatial decay is a new paradigm for enhancing spatial attention in vision transformers.

Abstract: Vision Transformers (ViTs) have revolutionized computer vision, yet their
self-attention mechanism lacks explicit spatial inductive biases, leading to
suboptimal performance on spatially-structured tasks. Existing approaches
introduce data-independent spatial decay based on fixed distance metrics,
applying uniform attention weighting regardless of image content and limiting
adaptability to diverse visual scenarios. Inspired by recent advances in large
language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)
significantly outperform static alternatives, we present the first successful
adaptation of data-dependent spatial decay to 2D vision transformers. We
introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel
Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent
decay for patch interactions. Our approach learns to modulate spatial attention
based on both content relevance and spatial proximity. We address the
fundamental challenge of 1D-to-2D adaptation through a unified spatial-content
fusion framework that integrates manhattan distance-based spatial priors with
learned content representations. Extensive experiments on ImageNet-1K
classification and generation tasks demonstrate consistent improvements over
strong baselines. Our work establishes data-dependent spatial decay as a new
paradigm for enhancing spatial attention in vision transformers.

</details>


### [96] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: The paper proposes a novel asymmetric Kronecker CS (AKCS) model and a measurement-aware cross attention (MACA) mechanism to improve image compressed sensing (CS) by enhancing measurement incoherence and learning implicit representations, integrated into MEUNet for superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep networks for image CS lack incoherent measurements and implicit measurement representations, limiting performance. The work addresses these gaps.

Method: Introduces AKCS for better measurement incoherence and MACA for learning implicit representations, integrated into MEUNet.

Result: MEUNet achieves state-of-the-art performance in reconstruction accuracy and inference speed.

Conclusion: The proposed AKCS and MACA enhance CS performance, demonstrating the effectiveness of MEUNet in image reconstruction.

Abstract: Deep networks have achieved remarkable success in image compressed sensing
(CS) task, namely reconstructing a high-fidelity image from its compressed
measurement. However, existing works are deficient inincoherent compressed
measurement at sensing phase and implicit measurement representations at
reconstruction phase, limiting the overall performance. In this work, we answer
two questions: 1) how to improve the measurement incoherence for decreasing the
ill-posedness; 2) how to learn informative representations from measurements.
To this end, we propose a novel asymmetric Kronecker CS (AKCS) model and
theoretically present its better incoherence than previous Kronecker CS with
minimal complexity increase. Moreover, we reveal that the unfolding networks'
superiority over non-unfolding ones result from sufficient gradient descents,
called explicit measurement representations. We propose a measurement-aware
cross attention (MACA) mechanism to learn implicit measurement representations.
We integrate AKCS and MACA into widely-used unfolding architecture to get a
measurement-enhanced unfolding network (MEUNet). Extensive experiences
demonstrate that our MEUNet achieves state-of-the-art performance in
reconstruction accuracy and inference speed.

</details>


### [97] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: COXNet improves tiny object detection in RGBT imagery with cross-layer fusion, dynamic alignment, and optimized label assignment, outperforming state-of-the-art methods by 3.32% mAP50.


<details>
  <summary>Details</summary>
Motivation: Tiny object detection in RGBT imagery is challenging due to spatial misalignment, low-light, occlusion, and cluttered backgrounds, especially in drone-based scenarios. Current methods fail to effectively use complementary information between modalities.

Method: COXNet introduces: 1) Cross-Layer Fusion Module for semantic and spatial accuracy, 2) Dynamic Alignment and Scale Refinement for correcting misalignments, and 3) GeoShape Similarity Measure for optimized label assignment.

Result: COXNet achieves a 3.32% mAP50 improvement on the RGBTDronePerson dataset, outperforming existing methods.

Conclusion: COXNet effectively addresses challenges in RGBT tiny object detection, proving robust in complex environments.

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [98] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: The paper addresses stereo matching challenges in asymmetric multi-camera systems by proposing a two-phase Iterative Volume Fusion network (IVF-AStereo) that leverages distinct cost volumes to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional stereo matching assumes symmetric visual properties, but asymmetric systems (e.g., tele-wide cameras) disrupt this, complicating cost volume computation.

Method: The IVF-AStereo method refines a correlation volume using aggregated concatenation, then fuses it with another volume to enhance details.

Result: The method performs robustly in asymmetric scenarios, handling resolution and color degradation effectively.

Conclusion: The proposed IVF-AStereo outperforms in asymmetric stereo matching, validated by benchmark datasets and ablation studies.

Abstract: Stereo matching is vital in 3D computer vision, with most algorithms assuming
symmetric visual properties between binocular visions. However, the rise of
asymmetric multi-camera systems (e.g., tele-wide cameras) challenges this
assumption and complicates stereo matching. Visual asymmetry disrupts stereo
matching by affecting the crucial cost volume computation. To address this, we
explore the matching cost distribution of two established cost volume
construction methods in asymmetric stereo. We find that each cost volume
experiences distinct information distortion, indicating that both should be
comprehensively utilized to solve the issue. Based on this, we propose the
two-phase Iterative Volume Fusion network for Asymmetric Stereo matching
(IVF-AStereo). Initially, the aggregated concatenation volume refines the
correlation volume. Subsequently, both volumes are fused to enhance fine
details. Our method excels in asymmetric scenarios and shows robust performance
against significant visual asymmetry. Extensive comparative experiments on
benchmark datasets, along with ablation studies, confirm the effectiveness of
our approach in asymmetric stereo with resolution and color degradation.

</details>


### [99] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: GoViG generates navigation instructions from egocentric visual data, improving adaptability to unseen environments by decomposing the task into visual forecasting and instruction generation, integrated in a multimodal model.


<details>
  <summary>Details</summary>
Motivation: To create precise navigation instructions without relying on structured inputs, enhancing adaptability in unstructured environments.

Method: Decomposes the task into visual forecasting and instruction generation, integrated in an autoregressive multimodal model with tailored objectives and multimodal reasoning strategies.

Result: Outperforms state-of-the-art methods with higher BLEU-4 and CIDEr scores and robust cross-domain generalization.

Conclusion: GoViG effectively generates navigation instructions from raw visual data, demonstrating superior performance and adaptability.

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [100] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: The paper explores using generative models for closed-set synthetic data augmentation to improve image classification performance, comparing it with real data augmentation and quantifying the required scale of synthetic data.


<details>
  <summary>Details</summary>
Motivation: To address whether generative models can enhance image classification performance by augmenting training data with synthetic images, and to compare the effectiveness of closed-set synthetic data with real data augmentation.

Method: The study involves extensive experiments to analyze the distinctions between real and synthetic images, determine the equivalent scale of synthetic data needed, and compare closed-set and open-set generative augmentation.

Result: Empirical findings show the required scale of synthetic data for comparable performance to real data augmentation, with variations based on baseline training set size and synthetic data amount.

Conclusion: While real images are preferred, the study provides guidelines for quantifying synthetic data augmentation needs, demonstrating its effectiveness across natural and medical image datasets.

Abstract: In this paper, we address a key scientific problem in machine learning: Given
a training set for an image classification task, can we train a generative
model on this dataset to enhance the classification performance? (i.e.,
closed-set generative data augmentation). We start by exploring the
distinctions and similarities between real images and closed-set synthetic
images generated by advanced generative models. Through extensive experiments,
we offer systematic insights into the effective use of closed-set synthetic
data for augmentation. Notably, we empirically determine the equivalent scale
of synthetic images needed for augmentation. In addition, we also show
quantitative equivalence between the real data augmentation and open-set
generative augmentation (generative models trained using data beyond the given
training set). While it aligns with the common intuition that real images are
generally preferred, our empirical formulation also offers a guideline to
quantify the increased scale of synthetic data augmentation required to achieve
comparable image classification performance. Our results on natural and medical
image datasets further illustrate how this effect varies with the baseline
training set size and the amount of synthetic data incorporated.

</details>


### [101] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: The paper proposes COME, a framework for multi-heterogeneous ultrasound datasets, addressing inter-dataset interference while preserving discriminative features for robust performance.


<details>
  <summary>Details</summary>
Motivation: Conventional single-dataset training struggles with new data distributions in ultrasound image analysis due to limited data and noise. A universal framework is needed.

Method: COME uses dual structure-semantic shared experts to create a universal representation space, collaborating with source-specific experts for feature extraction.

Result: COME outperforms state-of-the-art methods in three evaluation modes, showing significant mean AP improvements.

Conclusion: COME effectively generalizes across datasets, leveraging cross-dataset experience and providing universal priors for small-batch or unseen data.

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [102] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: A novel iris recognition method using topological invariants (Betti numbers) outperforms deep learning, offering interpretability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate, interpretable, and efficient biometric identification method using topological invariants from iris images.

Method: Iris images are divided into grids, and Betti0, Betti1, and their ratios are computed for each subregion. Feature matrices are used with logistic regression, KNN, and SVM, compared to a CNN.

Result: Logistic regression achieved 97.78% accuracy, surpassing CNN (96.44%) and other models, with low variance.

Conclusion: Topological invariants provide a compact, interpretable, and accurate alternative to deep learning, applicable beyond iris recognition to other fields requiring explainability.

Abstract: Objective - This study presents a biometric identification method based on
topological invariants from 2D iris images, representing iris texture via
formally defined digital homology and evaluating classification performance.
  Methods - Each normalized iris image (48x482 pixels) is divided into grids
(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their
ratio using a recent algorithm for homology groups in 2D digital images. The
resulting invariants form a feature matrix used with logistic regression, KNN,
and SVM (with PCA and 100 randomized repetitions). A convolutional neural
network (CNN) is trained on raw images for comparison.
  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,
outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The
topological features showed high accuracy with low variance.
  Conclusion - This is the first use of topological invariants from formal
digital homology for iris recognition. The method offers a compact,
interpretable, and accurate alternative to deep learning, useful when
explainability or limited data is important. Beyond iris recognition, it can
apply to other biometrics, medical imaging, materials science, remote sensing,
and interpretable AI. It runs efficiently on CPU-only systems and produces
robust, explainable features valuable for security-critical domains.

</details>


### [103] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: The paper explores the advantages of GPT-4o-generated synthetic images over real-world datasets, introduces Echo-4o-Image, and demonstrates its effectiveness in improving image generation models.


<details>
  <summary>Details</summary>
Motivation: To address limitations in real-world image datasets, such as rare scenarios and noise, by leveraging synthetic data from GPT-4o.

Method: Introduces Echo-4o-Image, a 180K synthetic dataset, and fine-tunes the Bagel model to create Echo-4o. Proposes new benchmarks (GenEval++, Imagine-Bench) for evaluation.

Result: Echo-4o shows strong performance on benchmarks and improves other models (e.g., OmniGen2, BLIP3-o) when fine-tuned with Echo-4o-Image.

Conclusion: Synthetic data from GPT-4o complements real-world datasets, offering cleaner supervision and better text-to-image alignment, with demonstrated transferability and performance gains.

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


### [104] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: WeatherPrompt improves drone geo-localization under diverse weather by fusing image and text embeddings, achieving better recall rates.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with weather generalization and feature disentanglement, limiting drone geo-localization accuracy.

Method: Uses multi-modality learning with weather-invariant representations, training-free weather reasoning, and dynamic gating for feature fusion.

Result: Boosts Recall@1 by +13.37% (night) and +18.69% (fog/snow) over state-of-the-art methods.

Conclusion: WeatherPrompt effectively addresses weather-related challenges in drone geo-localization, enhancing performance across diverse conditions.

Abstract: Visual geo-localization for drones faces critical degradation under weather
perturbations, \eg, rain and fog, where existing methods struggle with two
inherent limitations: 1) Heavy reliance on limited weather categories that
constrain generalization, and 2) Suboptimal disentanglement of entangled
scene-weather features through pseudo weather categories. We present
WeatherPrompt, a multi-modality learning paradigm that establishes
weather-invariant representations through fusing the image embedding with the
text context. Our framework introduces two key contributions: First, a
Training-free Weather Reasoning mechanism that employs off-the-shelf large
multi-modality models to synthesize multi-weather textual descriptions through
human-like reasoning. It improves the scalability to unseen or complex weather,
and could reflect different weather strength. Second, to better disentangle the
scene and weather feature, we propose a multi-modality framework with the
dynamic gating mechanism driven by the text embedding to adaptively reweight
and fuse visual features across modalities. The framework is further optimized
by the cross-modal objectives, including image-text contrastive learning and
image-text matching, which maps the same scene with different weather
conditions closer in the respresentation space. Extensive experiments validate
that, under diverse weather conditions, our method achieves competitive recall
rates compared to state-of-the-art drone geo-localization methods. Notably, it
improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog
and snow conditions.

</details>


### [105] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: Proposes WEC-DG, a wavelet-based method for multi-exposure correction, addressing intra-class variability and enhancing detail recovery through degradation guidance and wavelet transform.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with intra-class variability due to diverse lighting and environmental conditions, especially for single-exposure images.

Method: Uses a degradation descriptor in ECAM for exposure consistency and EDRM for light-detail decoupling via wavelet transform.

Result: Outperforms existing algorithms on public datasets, showing significant improvements in exposure correction and detail recovery.

Conclusion: WEC-DG is effective and practical for multi-exposure correction, validated by superior performance.

Abstract: Multi-exposure correction technology is essential for restoring images
affected by insufficient or excessive lighting, enhancing the visual experience
by improving brightness, contrast, and detail richness. However, current
multi-exposure correction methods often encounter challenges in addressing
intra-class variability caused by diverse lighting conditions, shooting
environments, and weather factors, particularly when processing images captured
at a single exposure level. To enhance the adaptability of these models under
complex imaging conditions, this paper proposes a Wavelet-based Exposure
Correction method with Degradation Guidance (WEC-DG). Specifically, we
introduce a degradation descriptor within the Exposure Consistency Alignment
Module (ECAM) at both ends of the processing pipeline to ensure exposure
consistency and achieve final alignment. This mechanism effectively addresses
miscorrected exposure anomalies caused by existing methods' failure to
recognize 'blurred' exposure degradation. Additionally, we investigate the
light-detail decoupling properties of the wavelet transform to design the
Exposure Restoration and Detail Reconstruction Module (EDRM), which processes
low-frequency information related to exposure enhancement before utilizing
high-frequency information as a prior guide for reconstructing spatial domain
details. This serial processing strategy guarantees precise light correction
and enhances detail recovery. Extensive experiments conducted on multiple
public datasets demonstrate that the proposed method outperforms existing
algorithms, achieving significant performance improvements and validating its
effectiveness and practical applicability.

</details>


### [106] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: The paper proposes a trustworthy radiology report generation (RRG) framework called Chain of Diagnosis (CoD) to improve clinical accuracy and explainability by generating QA pairs, grounding diagnoses, and leveraging omni-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Existing RRG models lack clinical efficacy and explainability, making them untrustworthy for radiologists.

Method: CoD generates QA pairs for key findings, uses a large language model for accurate generation, and includes diagnosis and lesion grounding modules for explainability. Omni-supervised learning is used for training.

Result: CoD outperforms specialist and generalist models on RRG benchmarks, provides accurate grounding of sentences to QA diagnoses and images, and improves radiologists' efficiency.

Conclusion: The CoD framework enhances RRG by ensuring clinical accuracy, explainability, and efficiency, supported by a new dataset and evaluation tool.

Abstract: Despite the progress of radiology report generation (RRG), existing works
face two challenges: 1) The performances in clinical efficacy are
unsatisfactory, especially for lesion attributes description; 2) the generated
text lacks explainability, making it difficult for radiologists to trust the
results. To address the challenges, we focus on a trustworthy RRG model, which
not only generates accurate descriptions of abnormalities, but also provides
basis of its predictions. To this end, we propose a framework named chain of
diagnosis (CoD), which maintains a chain of diagnostic process for clinically
accurate and explainable RRG. It first generates question-answer (QA) pairs via
diagnostic conversation to extract key findings, then prompts a large language
model with QA diagnoses for accurate generation. To enhance explainability, a
diagnosis grounding module is designed to match QA diagnoses and generated
sentences, where the diagnoses act as a reference. Moreover, a lesion grounding
module is designed to locate abnormalities in the image, further improving the
working efficiency of radiologists. To facilitate label-efficient training, we
propose an omni-supervised learning strategy with clinical consistency to
leverage various types of annotations from different datasets. Our efforts lead
to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a
evaluation tool for assessing the accuracy of reports in describing lesion
location and severity; 3) extensive experiments to demonstrate the
effectiveness of CoD, where it outperforms both specialist and generalist
models consistently on two RRG benchmarks and shows promising explainability by
accurately grounding generated sentences to QA diagnoses and images.

</details>


### [107] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: The paper introduces a training-free Dual Recursive Feedback (DRF) system to improve spatial and appearance control in text-to-image diffusion models, addressing limitations in existing methods like Ctrl-X and FreeControl.


<details>
  <summary>Details</summary>
Motivation: Existing controllable T2I models struggle with preserving spatial structures and fine-grained conditions like object poses and scene layouts.

Method: The proposed DRF system uses appearance and generation feedback to recursively refine intermediate latents, integrating structural and appearance attributes without additional training.

Result: The method enables fine-grained generation, such as transferring human motion to a tiger's form, and produces high-quality, semantically coherent images.

Conclusion: DRF effectively enhances control in T2I models, achieving structurally consistent and detailed image generation, with code available for public use.

Abstract: Recent advancements in controllable text-to-image (T2I) diffusion models,
such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance
control without requiring auxiliary module training. However, these models
often struggle to accurately preserve spatial structures and fail to capture
fine-grained conditions related to object poses and scene layouts. To address
these challenges, we propose a training-free Dual Recursive Feedback (DRF)
system that properly reflects control conditions in controllable T2I models.
The proposed DRF consists of appearance feedback and generation feedback that
recursively refines the intermediate latents to better reflect the given
appearance information and the user's intent. This dual-update mechanism guides
latent representations toward reliable manifolds, effectively integrating
structural and appearance attributes. Our approach enables fine-grained
generation even between class-invariant structure-appearance fusion, such as
transferring human motion onto a tiger's form. Extensive experiments
demonstrate the efficacy of our method in producing high-quality, semantically
coherent, and structurally consistent image generations. Our source code is
available at https://github.com/jwonkm/DRF.

</details>


### [108] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: The paper introduces SHALE, a scalable benchmark for evaluating hallucinations in Large Vision-Language Models (LVLMs), addressing limitations of prior work by automating data construction and providing fine-grained analysis.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs suffer from hallucinations (inconsistent outputs), but existing evaluation methods are coarse, manually curated, or rely on reused datasets, limiting scalability and reliability.

Method: Proposes an automated data pipeline for scalable evaluation and a hierarchical hallucination induction framework with input perturbations to simulate noise.

Result: SHALE, a benchmark with 30K+ image-instruction pairs, reveals significant factuality hallucinations and model sensitivity to perturbations.

Conclusion: SHALE provides a scalable, fine-grained solution for evaluating LVLM hallucinations, highlighting the need for improved model robustness.

Abstract: Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer
from hallucinations, i.e., generating content inconsistent with input or
established world knowledge, which correspond to faithfulness and factuality
hallucinations, respectively. Prior studies primarily evaluate faithfulness
hallucination at a coarse level (e.g., object-level) and lack fine-grained
analysis. Additionally, existing benchmarks rely on costly manual curation or
reused public datasets, raising concerns about scalability and data leakage. To
address these limitations, we propose an automated data construction pipeline
that produces scalable, controllable, and diverse evaluation data. We also
design a hierarchical hallucination induction framework with input
perturbations to simulate realistic noisy scenarios. Integrating these designs,
we construct SHALE, a Scalable HALlucination Evaluation benchmark designed to
assess both faithfulness and factuality hallucinations via a fine-grained
hallucination categorization scheme. SHALE comprises over 30K image-instruction
pairs spanning 12 representative visual perception aspects for faithfulness and
6 knowledge domains for factuality, considering both clean and noisy scenarios.
Extensive experiments on over 20 mainstream LVLMs reveal significant factuality
hallucinations and high sensitivity to semantic perturbations.

</details>


### [109] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: BAAS is a framework for radar detection annotation in autonomous driving, using Bayesian tracking and fusion for precise object trajectories and shape estimation, with performance evaluated in urban scenarios.


<details>
  <summary>Details</summary>
Motivation: To improve radar detection annotation accuracy and tracking performance in autonomous driving under varying supervision levels.

Method: Utilizes Bayesian-based tracking, smoothing, and fusion methods for object trajectory and shape estimation, with optional manual data integration for closed-loop improvements.

Result: Demonstrates functionality in urban scenarios, handling dynamic objects and class types with evaluated tracking and annotation performance.

Conclusion: BAAS effectively provides precise annotations and tracking, adaptable for continuous improvement with manual data.

Abstract: This paper introduces BAAS, a new Extended Object Tracking (EOT) and
fusion-based label annotation framework for radar detections in autonomous
driving. Our framework utilizes Bayesian-based tracking, smoothing and
eventually fusion methods to provide veritable and precise object trajectories
along with shape estimation to provide annotation labels on the detection level
under various supervision levels. Simultaneously, the framework provides
evaluation of tracking performance and label annotation. If manually labeled
data is available, each processing module can be analyzed independently or
combined with other modules to enable closed-loop continuous improvements. The
framework performance is evaluated in a challenging urban real-world scenario
in terms of tracking performance and the label annotation errors. We
demonstrate the functionality of the proposed approach for varying dynamic
objects and class types

</details>


### [110] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: Hi-SMGNN, a hierarchical framework, improves IDH mutation prediction in gliomas by integrating structural and morphological connectomes with multimodal interaction and multiscale feature fusion.


<details>
  <summary>Details</summary>
Motivation: Current methods for predicting IDH mutation status in gliomas are limited by low availability and noise in functional MRI, and they often overlook the brain's hierarchical and multiscale organization.

Method: Hi-SMGNN integrates structural and morphological connectomes hierarchically, using a Siamese network, cross-modal attention, multiscale feature fusion, and personalized modular partitioning.

Result: Hi-SMGNN outperforms baseline and state-of-the-art models on the UCSF-PDGM dataset, demonstrating improved robustness and effectiveness.

Conclusion: The proposed Hi-SMGNN framework offers a superior, interpretable, and personalized approach for IDH mutation prediction in gliomas.

Abstract: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for
glioma prognosis. However, current prediction methods are limited by the low
availability and noise of functional MRI. Structural and morphological
connectomes offer a non-invasive alternative, yet existing approaches often
ignore the brain's hierarchical organisation and multiscale interactions. To
address this, we propose Hi-SMGNN, a hierarchical framework that integrates
structural and morphological connectomes from regional to modular levels. It
features a multimodal interaction module with a Siamese network and cross-modal
attention, a multiscale feature fusion mechanism for reducing redundancy, and a
personalised modular partitioning strategy to enhance individual specificity
and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that
Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved
robustness and effectiveness in IDH mutation prediction.

</details>


### [111] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: SVG-Head introduces a hybrid representation for editable head avatars, combining surface and volumetric Gaussians for high-fidelity rendering and real-time appearance editing.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in achieving photorealistic and editable head avatars due to implicit representations and entangled geometry-appearance modeling.

Method: SVG-Head uses surface Gaussians for explicit appearance modeling with texture images and volumetric Gaussians for non-Lambertian regions. It employs mesh-aware Gaussian UV mapping and hierarchical optimization.

Result: Experiments show SVG-Head achieves high-fidelity rendering and supports real-time appearance editing, a first for Gaussian head avatars.

Conclusion: SVG-Head advances head avatar technology by balancing reconstruction quality and editing flexibility.

Abstract: Creating high-fidelity and editable head avatars is a pivotal challenge in
computer vision and graphics, boosting many AR/VR applications. While recent
advancements have achieved photorealistic renderings and plausible animation,
head editing, especially real-time appearance editing, remains challenging due
to the implicit representation and entangled modeling of the geometry and
global appearance. To address this, we propose Surface-Volumetric Gaussian Head
Avatar (SVG-Head), a novel hybrid representation that explicitly models the
geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled
texture images to capture the global appearance. Technically, it contains two
types of Gaussians, in which surface Gaussians explicitly model the appearance
of head avatars using learnable texture images, facilitating real-time texture
editing, while volumetric Gaussians enhance the reconstruction quality of
non-Lambertian regions (e.g., lips and hair). To model the correspondence
between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping
method, which leverages UV coordinates given by the FLAME mesh to obtain sharp
texture images and real-time rendering speed. A hierarchical optimization
strategy is further designed to pursue the optimal performance in both
reconstruction quality and editing flexibility. Experiments on the NeRSemble
dataset show that SVG-Head not only generates high-fidelity rendering results,
but also is the first method to obtain explicit texture images for Gaussian
head avatars and support real-time appearance editing.

</details>


### [112] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: FaME improves perceptual quality in diffusion models by using negative guidance from low-quality samples, without affecting FID scores.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art models often produce distorted images despite good FID scores, as FID ignores individual sample quality. CFG, while helpful, can introduce artifacts.

Method: FaME identifies low-quality images via an assessment model and uses their sampling trajectories as negative guidance to avoid poor-quality regions.

Result: FaME consistently improves visual quality on ImageNet without degrading FID and shows potential for text-to-image generation.

Conclusion: FaME is a training-free, efficient method to enhance perceptual quality in diffusion models, addressing gaps left by FID and CFG.

Abstract: Diffusion models have achieved remarkable progress in class-to-image
generation. However, we observe that despite impressive FID scores,
state-of-the-art models often generate distorted or low-quality images,
especially in certain classes. This gap arises because FID evaluates global
distribution alignment, while ignoring the perceptual quality of individual
samples. We further examine the role of CFG, a common technique used to enhance
generation quality. While effective in improving metrics and suppressing
outliers, CFG can introduce distribution shift and visual artifacts due to its
misalignment with both training objectives and user expectations. In this work,
we propose FaME, a training-free and inference-efficient method for improving
perceptual quality. FaME uses an image quality assessment model to identify
low-quality generations and stores their sampling trajectories. These failure
modes are then used as negative guidance to steer future sampling away from
poor-quality regions. Experiments on ImageNet demonstrate that FaME brings
consistent improvements in visual quality without compromising FID. FaME also
shows the potential to be extended to improve text-to-image generation.

</details>


### [113] [BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation](https://arxiv.org/abs/2508.09599)
*Beomjun Kim,Suhan Woo,Sejong Heo,Euntai Kim*

Main category: cs.CV

TL;DR: BridgeTA is a cost-effective distillation framework that bridges the gap between LiDAR-Camera fusion and Camera-only models for BEV map segmentation, using a lightweight Teacher Assistant network and a novel distillation loss.


<details>
  <summary>Details</summary>
Motivation: Camera-only approaches for BEV map segmentation lag behind LiDAR-Camera fusion methods. Existing KD methods increase inference costs by mimicking teacher architectures. BridgeTA aims to improve performance without altering the student model's architecture or cost.

Method: Introduces a lightweight Teacher Assistant (TA) network to combine BEV representations of teacher and student, creating a shared latent space. Uses a distillation loss derived from Young's Inequality to decompose the distillation path into teacher-TA and TA-student paths.

Result: Achieves a 4.2% mIoU improvement over the Camera-only baseline on the nuScenes dataset, outperforming other KD methods by up to 45%.

Conclusion: BridgeTA effectively bridges the performance gap between fusion and camera-only models while maintaining inference efficiency, demonstrating superior results compared to existing KD approaches.

Abstract: Bird's-Eye-View (BEV) map segmentation is one of the most important and
challenging tasks in autonomous driving. Camera-only approaches have drawn
attention as cost-effective alternatives to LiDAR, but they still fall behind
LiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been
explored to narrow this gap, but existing methods mainly enlarge the student
model by mimicking the teacher's architecture, leading to higher inference
cost. To address this issue, we introduce BridgeTA, a cost-effective
distillation framework to bridge the representation gap between LC fusion and
Camera-only models through a Teacher Assistant (TA) network while keeping the
student's architecture and inference cost unchanged. A lightweight TA network
combines the BEV representations of the teacher and student, creating a shared
latent space that serves as an intermediate representation. To ground the
framework theoretically, we derive a distillation loss using Young's
Inequality, which decomposes the direct teacher-student distillation path into
teacher-TA and TA-student dual paths, stabilizing optimization and
strengthening knowledge transfer. Extensive experiments on the challenging
nuScenes dataset demonstrate the effectiveness of our method, achieving an
improvement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than
the improvement of other state-of-the-art KD methods.

</details>


### [114] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: MInDI-3D is a 3D conditional diffusion model for CBCT artefact removal, reducing radiation exposure by refining sparse-view inputs. It outperforms uncorrected scans and matches 3D U-Net performance, validated by clinical assessments.


<details>
  <summary>Details</summary>
Motivation: To reduce radiation exposure in CBCT imaging by improving sparse-view artefact removal using a 3D diffusion model.

Method: Extends the "InDI" concept to 3D, using iterative denoising and a pseudo-CBCT dataset for training. Evaluated with quantitative metrics, scalability tests, and clinician assessments.

Result: Achieves 12.96 dB PSNR gain, enables 8x radiation reduction, and matches 3D U-Net performance on real-world scans. Clinicians rated it sufficient for patient positioning.

Conclusion: MInDI-3D effectively reduces CBCT artefacts and radiation exposure, generalizes to new scanners, and is clinically viable.

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first
3D conditional diffusion-based model for real-world sparse-view Cone Beam
Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation
exposure. A key contribution is extending the "InDI" concept from 2D to a full
3D volumetric approach for medical images, implementing an iterative denoising
process that refines the CBCT volume directly from sparse-view input. A further
contribution is the generation of a large pseudo-CBCT dataset (16,182) from
chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We
performed a comprehensive evaluation, including quantitative metrics,
scalability analysis, generalisation tests, and a clinical assessment by 11
clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)
dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE
pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in
imaging radiation exposure. We demonstrate its scalability by showing that
performance improves with more training data. Importantly, MInDI-3D matches the
performance of a 3D U-Net on real-world scans from 16 cancer patients across
distortion and task-based metrics. It also generalises to new CBCT scanner
geometries. Clinicians rated our model as sufficient for patient positioning
across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [115] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: A generalized framework for plane detection using model information optimization is proposed to address RANSAC's susceptibility to false positives in complex scenes.


<details>
  <summary>Details</summary>
Motivation: RANSAC's inlier threshold ambiguity leads to false positives in plane detection, especially in complex real-world scenes with unknown plane counts.

Method: Treat depth readings as discrete random variables, generate models with candidate plane constraints, and optimize information to determine the most likely ground truth.

Result: The algorithm outperforms Open3D RANSAC in accuracy for plane parameter estimation and is accelerated using neural network segmentation.

Conclusion: The proposed framework provides an objective mechanism for plane detection, reducing false positives and improving accuracy in real-world applications.

Abstract: Plane detection from depth images is a crucial subtask with broad robotic
applications, often accomplished by iterative methods such as Random Sample
Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic
guarantees, the ambiguity of its inlier threshold criterion makes it
susceptible to false positive plane detections. This issue is particularly
prevalent in complex real-world scenes, where the true number of planes is
unknown and multiple planes coexist. In this paper, we aim to address this
limitation by proposing a generalised framework for plane detection based on
model information optimization. Building on previous works, we treat the
observed depth readings as discrete random variables, with their probability
distributions constrained by the ground truth planes. Various models containing
different candidate plane constraints are then generated through repeated
random sub-sampling to explain our observations. By incorporating the physics
and noise model of the depth sensor, we can calculate the information for each
model, and the model with the least information is accepted as the most likely
ground truth. This information optimization process serves as an objective
mechanism for determining the true number of planes and preventing false
positive detections. Additionally, the quality of each detected plane can be
ranked by summing the information reduction of inlier points for each plane. We
validate these properties through experiments with synthetic data and find that
our algorithm estimates plane parameters more accurately compared to the
default Open3D RANSAC plane segmentation. Furthermore, we accelerate our
algorithm by partitioning the depth map using neural network segmentation,
which enhances its ability to generate more realistic plane parameters in
real-world data.

</details>


### [116] [Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation](https://arxiv.org/abs/2508.09626)
*Xu Tang,Junan Jia,Yijing Wang,Jingjing Ma,Xiangrong Zhang*

Main category: cs.CV

TL;DR: SAD-Splat is a novel 3D-AVS-SS method addressing semantic ambiguity with a Gaussian point drop module and pseudo-label generation, achieving high accuracy and compactness.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with semantic ambiguity due to scale variations and occlusions in aerial images, limiting accuracy.

Method: Uses a Gaussian point drop module with semantic confidence estimation and a pseudo-label generation pipeline leveraging 2D foundation models.

Result: Achieves excellent balance between segmentation accuracy and representation compactness.

Conclusion: SAD-Splat provides an efficient, scalable solution for 3D aerial scene understanding.

Abstract: In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),
traditional methods struggle to address semantic ambiguity caused by scale
variations and structural occlusions in aerial images. This limits their
segmentation accuracy and consistency. To tackle these challenges, we propose a
novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian
point drop module, which integrates semantic confidence estimation with a
learnable sparsity mechanism based on the Hard Concrete distribution. This
module effectively eliminates redundant and semantically ambiguous Gaussian
points, enhancing both segmentation performance and representation compactness.
Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation
pipeline. It leverages 2D foundation models to enhance supervision when
ground-truth labels are limited, thereby further improving segmentation
accuracy. To advance research in this domain, we introduce a challenging
benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse
real-world aerial scenes with sparse annotations. Experimental results
demonstrate that SAD-Splat achieves an excellent balance between segmentation
accuracy and representation compactness. It offers an efficient and scalable
solution for 3D aerial scene understanding.

</details>


### [117] [Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors](https://arxiv.org/abs/2508.09629)
*Giorgos Karvounas,Nikolaos Kyriazis,Iason Oikonomidis,Georgios Pavlakos,Antonis A. Argyros*

Main category: cs.CV

TL;DR: The paper explores texture's role in 3D hand reconstruction, proposing a texture module for better alignment and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Texture alignment is often imperfect in existing models, suggesting its underuse as a supervisory signal.

Method: A lightweight texture module embeds per-pixel observations into UV space, using a dense alignment loss with differentiable rendering.

Result: The module improves accuracy and realism when integrated into HaMeR, a transformer-based hand pose estimator.

Conclusion: Texture-guided alignment enhances 3D hand reconstruction, proving its value beyond photorealism.

Abstract: We revisit the role of texture in monocular 3D hand reconstruction, not as an
afterthought for photorealism, but as a dense, spatially grounded cue that can
actively support pose and shape estimation. Our observation is simple: even in
high-performing models, the overlay between predicted hand geometry and image
appearance is often imperfect, suggesting that texture alignment may be an
underused supervisory signal. We propose a lightweight texture module that
embeds per-pixel observations into UV texture space and enables a novel dense
alignment loss between predicted and observed hand appearances. Our approach
assumes access to a differentiable rendering pipeline and a model that maps
images to 3D hand meshes with known topology, allowing us to back-project a
textured hand onto the image and perform pixel-based alignment. The module is
self-contained and easily pluggable into existing reconstruction pipelines. To
isolate and highlight the value of texture-guided supervision, we augment
HaMeR, a high-performing yet unadorned transformer architecture for 3D hand
pose estimation. The resulting system improves both accuracy and realism,
demonstrating the value of appearance-guided alignment in hand reconstruction.

</details>


### [118] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: Preacher is a paper-to-video system that decomposes, summarizes, and reformulates research papers into structured video abstracts, overcoming limitations of current video generation models.


<details>
  <summary>Details</summary>
Motivation: Current video generation models lack context, flexibility, stylistic diversity, and domain-specific knowledge representation, limiting their effectiveness for paper-to-video tasks.

Method: Preacher uses a top-down approach for decomposition and summarization, followed by bottom-up video generation with Progressive Chain of Thought (P-CoT) for iterative planning.

Result: Preacher generates high-quality video abstracts across five research fields, outperforming existing models.

Conclusion: Preacher addresses key limitations of video generation models and successfully creates accessible, domain-specific video abstracts.

Abstract: The paper-to-video task converts a research paper into a structured video
abstract, distilling key concepts, methods, and conclusions into an accessible,
well-organized format. While state-of-the-art video generation models
demonstrate potential, they are constrained by limited context windows, rigid
video duration constraints, limited stylistic diversity, and an inability to
represent domain-specific knowledge. To address these limitations, we introduce
Preacher, the first paper-to-video agentic system. Preacher employs a top-down
approach to decompose, summarize, and reformulate the paper, followed by
bottom-up video generation, synthesizing diverse video segments into a coherent
abstract. To align cross-modal representations, we define key scenes and
introduce a Progressive Chain of Thought (P-CoT) for granular, iterative
planning. Preacher successfully generates high-quality video abstracts across
five research fields, demonstrating expertise beyond current video generation
models. Code will be released at: https://github.com/GenVerse/Paper2Video

</details>


### [119] [Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification](https://arxiv.org/abs/2508.09644)
*Shengjun Zhu,Siyu Liu,Runqing Xiong,Liping Zheng,Duo Ma,Rongshang Chen,Jiaxin Cai*

Main category: cs.CV

TL;DR: A novel Multi-Contrast Fusion Module (MCFM) improves fetal torso plane recognition in ultrasound imaging by enhancing feature extraction with minimal parameter overhead.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of fetal torso planes is crucial for prenatal care, but low contrast and unclear textures in ultrasound images hinder fine-grained anatomical recognition.

Method: MCFM processes raw ultrasound data in lower neural network layers, using attention weights for multi-contrast conditions to enhance feature modeling.

Result: MCFM significantly boosts recognition performance with minimal complexity, improving classification accuracy and clinical reliability.

Conclusion: MCFM enhances fetal torso plane recognition, aiding clinicians in accurate diagnoses and showing strong potential for clinical adoption.

Abstract: Purpose: Prenatal ultrasound is a key tool in evaluating fetal structural
development and detecting abnormalities, contributing to reduced perinatal
complications and improved neonatal survival. Accurate identification of
standard fetal torso planes is essential for reliable assessment and
personalized prenatal care. However, limitations such as low contrast and
unclear texture details in ultrasound imaging pose significant challenges for
fine-grained anatomical recognition. Methods: We propose a novel Multi-Contrast
Fusion Module (MCFM) to enhance the model's ability to extract detailed
information from ultrasound images. MCFM operates exclusively on the lower
layers of the neural network, directly processing raw ultrasound data. By
assigning attention weights to image representations under different contrast
conditions, the module enhances feature modeling while explicitly maintaining
minimal parameter overhead. Results: The proposed MCFM was evaluated on a
curated dataset of fetal torso plane ultrasound images. Experimental results
demonstrate that MCFM substantially improves recognition performance, with a
minimal increase in model complexity. The integration of multi-contrast
attention enables the model to better capture subtle anatomical structures,
contributing to higher classification accuracy and clinical reliability.
Conclusions: Our method provides an effective solution for improving fetal
torso plane recognition in ultrasound imaging. By enhancing feature
representation through multi-contrast fusion, the proposed approach supports
clinicians in achieving more accurate and consistent diagnoses, demonstrating
strong potential for clinical adoption in prenatal screening. The codes are
available at https://github.com/sysll/MCFM.

</details>


### [120] [Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model](https://arxiv.org/abs/2508.09645)
*Zhongyuan Wu,Chuan-Xian Ren,Yu Wang,Xiaohua Ban,Jianning Xiao,Xiaohui Duan*

Main category: cs.CV

TL;DR: PG-SAM, an expert-guided SAM model, improves parotid gland lesion segmentation by integrating domain knowledge and cross-sequence attention.


<details>
  <summary>Details</summary>
Motivation: Accurate parotid gland lesion segmentation is challenging due to variable lesion sizes and complex boundaries. Current methods lack expert domain knowledge and rely on impractical precise prompts.

Method: PG-SAM uses expert diagnosis reports to generate prompts, incorporates cross-sequence attention for multi-modal data, and decodes features for segmentation.

Result: PG-SAM achieves state-of-the-art performance across three clinical centers, proving its clinical applicability.

Conclusion: PG-SAM effectively integrates expert knowledge and multi-modal data, enhancing segmentation accuracy in real-world settings.

Abstract: Parotid gland lesion segmentation is essential for the treatment of parotid
gland diseases. However, due to the variable size and complex lesion
boundaries, accurate parotid gland lesion segmentation remains challenging.
Recently, the Segment Anything Model (SAM) fine-tuning has shown remarkable
performance in the field of medical image segmentation. Nevertheless, SAM's
interaction segmentation model relies heavily on precise lesion prompts
(points, boxes, masks, etc.), which are very difficult to obtain in real-world
applications. Besides, current medical image segmentation methods are
automatically generated, ignoring the domain knowledge of medical experts when
performing segmentation. To address these limitations, we propose the parotid
gland segment anything model (PG-SAM), an expert diagnosis text-guided SAM
incorporating expert domain knowledge for cross-sequence parotid gland lesion
segmentation. Specifically, we first propose an expert diagnosis report guided
prompt generation module that can automatically generate prompt information
containing the prior domain knowledge to guide the subsequent lesion
segmentation process. Then, we introduce a cross-sequence attention module,
which integrates the complementary information of different modalities to
enhance the segmentation effect. Finally, the multi-sequence image features and
generated prompts are feed into the decoder to get segmentation result.
Experimental results demonstrate that PG-SAM achieves state-of-the-art
performance in parotid gland lesion segmentation across three independent
clinical centers, validating its clinical applicability and the effectiveness
of diagnostic text for enhancing image segmentation in real-world clinical
settings.

</details>


### [121] [The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge](https://arxiv.org/abs/2508.09649)
*Reuben Dorent,Laura Rigolo,Colin P. Galvin,Junyu Chen,Mattias P. Heinrich,Aaron Carass,Olivier Colliot,Demian Wassermann,Alexandra Golby,Tina Kapur,William Wells*

Main category: cs.CV

TL;DR: The paper introduces the ReMIND2Reg 2025 Challenge, a benchmark for aligning post-resection intraoperative ultrasound with preoperative MRI to address brain shift in neurosurgery.


<details>
  <summary>Details</summary>
Motivation: Brain shift during surgery reduces the accuracy of neuronavigation systems based on preoperative MRI, necessitating methods to align intraoperative ultrasound with MRI for improved guidance.

Method: The challenge provides a dataset of paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes, evaluated using manually annotated landmarks and metrics like TRE and TRE30.

Result: The ReMIND2Reg dataset includes 99 training, 5 validation, and 10 test cases, aiming to standardize evaluation of multimodal registration algorithms.

Conclusion: The challenge seeks to advance robust and clinically deployable algorithms for image-guided neurosurgery by addressing brain shift and modality gaps.

Abstract: Accurate intraoperative image guidance is critical for achieving maximal safe
resection in brain tumor surgery, yet neuronavigation systems based on
preoperative MRI lose accuracy during the procedure due to brain shift.
Aligning post-resection intraoperative ultrasound (iUS) with preoperative MRI
can restore spatial accuracy by estimating brain shift deformations, but it
remains a challenging problem given the large anatomical and topological
changes and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge
provides the largest public benchmark for this task, built upon the ReMIND
dataset. It offers 99 training cases, 5 validation cases, and 10 private test
cases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.
Data are provided without annotations for training, while validation and test
performance are evaluated on manually annotated anatomical landmarks. Metrics
include target registration error (TRE), robustness to worst-case landmark
misalignment (TRE30), and runtime. By establishing a standardized evaluation
framework for this clinically critical and technically complex problem,
ReMIND2Reg aims to accelerate the development of robust, generalizable, and
clinically deployable multimodal registration algorithms for image-guided
neurosurgery.

</details>


### [122] [TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos](https://arxiv.org/abs/2508.09650)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazely,Sunil Aryal*

Main category: cs.CV

TL;DR: TOTNet improves ball tracking under occlusion in sports videos using 3D convolutions, visibility-weighted loss, and occlusion augmentation, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Robust ball tracking under occlusion is critical for sports video analysis, impacting event detection and officiating.

Method: TOTNet employs 3D convolutions, visibility-weighted loss, and occlusion augmentation, tested on a new occlusion-rich dataset (TTA).

Result: TOTNet reduces RMSE from 37.30 to 7.19 and improves accuracy on fully occluded frames from 0.63 to 0.80.

Conclusion: TOTNet is effective for offline sports analytics in fast-paced scenarios, validated across multiple datasets.

Abstract: Robust ball tracking under occlusion remains a key challenge in sports video
analysis, affecting tasks like event detection and officiating. We present
TOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,
visibility-weighted loss, and occlusion augmentation to improve performance
under partial and full occlusions. Developed in collaboration with Paralympics
Australia, TOTNet is designed for real-world sports analytics. We introduce
TTA, a new occlusion-rich table tennis dataset collected from
professional-level Paralympic matches, comprising 9,159 samples with 1,996
occlusion cases. Evaluated on four datasets across tennis, badminton, and table
tennis, TOTNet significantly outperforms prior state-of-the-art methods,
reducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded
frames from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for
offline sports analytics in fast-paced scenarios. Code and data
access:\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.

</details>


### [123] [Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging](https://arxiv.org/abs/2508.09655)
*Lianfang Wang,Kuilin Qin,Xueying Liu,Huibin Chang,Yong Wang,Yuping Duan*

Main category: cs.CV

TL;DR: The paper introduces a parameterized inverse problem framework for 3D NLOS imaging, combining noise estimation, neural operators, and deep algorithm unfolding for robust and accurate reconstruction.


<details>
  <summary>Details</summary>
Motivation: NLOS imaging faces challenges due to weak indirect signals and noise. Accurate reconstruction requires integrating physical processes and adaptive methods.

Method: A noise estimation module and parameterized neural operator are developed for end-to-end reconstruction. Deep algorithm unfolding ensures interpretability and adaptability. Global and local data fusion enhances accuracy.

Result: The framework performs well on simulated and real datasets, excelling with fast scanning and sparse illumination data.

Conclusion: The proposed method offers a robust solution for NLOS imaging in complex scenarios, balancing speed, accuracy, and adaptability.

Abstract: Computational imaging, especially non-line-of-sight (NLOS) imaging, the
extraction of information from obscured or hidden scenes is achieved through
the utilization of indirect light signals resulting from multiple reflections
or scattering. The inherently weak nature of these signals, coupled with their
susceptibility to noise, necessitates the integration of physical processes to
ensure accurate reconstruction. This paper presents a parameterized inverse
problem framework tailored for large-scale linear problems in 3D imaging
reconstruction. Initially, a noise estimation module is employed to adaptively
assess the noise levels present in transient data. Subsequently, a
parameterized neural operator is developed to approximate the inverse mapping,
facilitating end-to-end rapid image reconstruction. Our 3D image reconstruction
framework, grounded in operator learning, is constructed through deep algorithm
unfolding, which not only provides commendable model interpretability but also
enables dynamic adaptation to varying noise levels in the acquired data,
thereby ensuring consistently robust and accurate reconstruction outcomes.
Furthermore, we introduce a novel method for the fusion of global and local
spatiotemporal data features. By integrating structural and detailed
information, this method significantly enhances both accuracy and robustness.
Comprehensive numerical experiments conducted on both simulated and real
datasets substantiate the efficacy of the proposed method. It demonstrates
remarkable performance with fast scanning data and sparse illumination point
data, offering a viable solution for NLOS imaging in complex scenarios.

</details>


### [124] [NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation](https://arxiv.org/abs/2508.09661)
*Eduarda Caldeira,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: NegFaceDiff improves identity separation in synthetic face data by using negative conditions in diffusion models, enhancing FR performance.


<details>
  <summary>Details</summary>
Motivation: Addresses identity overlap in synthetic face data generated by diffusion models, which hampers FR system performance.

Method: Introduces NegFaceDiff, a sampling method incorporating negative conditions to guide diffusion models away from unwanted features.

Result: Increases identity separability (FDR) from 2.427 to 5.687; FR models trained on NegFaceDiff data outperform others.

Conclusion: NegFaceDiff effectively enhances synthetic data quality for FR, improving identity consistency and separability.

Abstract: The use of synthetic data as an alternative to authentic datasets in face
recognition (FR) development has gained significant attention, addressing
privacy, ethical, and practical concerns associated with collecting and using
authentic data. Recent state-of-the-art approaches have proposed
identity-conditioned diffusion models to generate identity-consistent face
images, facilitating their use in training FR models. However, these methods
often lack explicit sampling mechanisms to enforce inter-class separability,
leading to identity overlap in the generated data and, consequently, suboptimal
FR performance. In this work, we introduce NegFaceDiff, a novel sampling method
that incorporates negative conditions into the identity-conditioned diffusion
process. NegFaceDiff enhances identity separation by leveraging negative
conditions that explicitly guide the model away from unwanted features while
preserving intra-class consistency. Extensive experiments demonstrate that
NegFaceDiff significantly improves the identity consistency and separability of
data generated by identity-conditioned diffusion models. Specifically, identity
separability, measured by the Fisher Discriminant Ratio (FDR), increases from
2.427 to 5.687. These improvements are reflected in FR systems trained on the
NegFaceDiff dataset, which outperform models trained on data generated without
negative conditions across multiple benchmarks.

</details>


### [125] [GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors](https://arxiv.org/abs/2508.09667)
*Xingyilang Yin,Qi Zhang,Jiahao Chang,Ying Feng,Qingnan Fan,Xi Yang,Chi-Man Pun,Huaqi Zhang,Xiaodong Cun*

Main category: cs.CV

TL;DR: GSFixer improves 3DGS reconstruction from sparse views using a reference-guided video restoration model, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Sparse-view 3DGS reconstruction often produces artifacts due to insufficient information, and existing methods struggle with consistency.

Method: Uses a DiT-based video diffusion model with 2D semantic and 3D geometric features from reference views to enhance coherence.

Result: Outperforms current methods in artifact restoration and sparse-view 3D reconstruction.

Conclusion: GSFixer effectively addresses artifact issues in 3DGS, validated by a new benchmark (DL3DV-Res).

Abstract: Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views
is an ill-posed problem due to insufficient information, often resulting in
noticeable artifacts. While recent approaches have sought to leverage
generative priors to complete information for under-constrained regions, they
struggle to generate content that remains consistent with input observations.
To address this challenge, we propose GSFixer, a novel framework designed to
improve the quality of 3DGS representations reconstructed from sparse inputs.
The core of our approach is the reference-guided video restoration model, built
upon a DiT-based video diffusion model trained on paired artifact 3DGS renders
and clean frames with additional reference-based conditions. Considering the
input sparse views as references, our model integrates both 2D semantic
features and 3D geometric features of reference views extracted from the visual
geometry foundation model, enhancing the semantic coherence and 3D consistency
when fixing artifact novel views. Furthermore, considering the lack of suitable
benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which
contains artifact frames rendered using low-quality 3DGS. Extensive experiments
demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS
artifact restoration and sparse-view 3D reconstruction. Project page:
https://github.com/GVCLab/GSFixer.

</details>


### [126] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: A novel test-time optimisation (TTO) approach using an invertible Neural Radiance Field (InvNeRF) for 2D and 3D point tracking in surgical scenarios, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current point tracking methods struggle with consistent motion or are limited to 2D. The paper aims to address these limitations by leveraging a NeRF-based architecture for improved tracking.

Method: Proposes InvNeRF for TTO, integrating rendering-based supervision, bidirectional deformable-canonical mapping, multi-scale HexPlanes for fast inference, and efficient pixel sampling. Evaluated on STIR and SCARE datasets.

Result: Outperforms TTO state-of-the-art by nearly 50% in 2D tracking and introduces the first TTO approach for 3D tracking, surpassing feed-forward methods.

Conclusion: The InvNeRF-based TTO approach effectively improves precision and accuracy in both 2D and 3D point tracking, setting a new benchmark in the field.

Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.

</details>


### [127] [PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training](https://arxiv.org/abs/2508.09691)
*Yin Xie,Zhichao Chen,Xiaoze Yu,Yongle Zhao,Xiang An,Kaicheng Yang,Zimin Ran,Jia Guo,Ziyong Feng,Jiankang Deng*

Main category: cs.CV

TL;DR: PaCo-FR is an unsupervised framework for facial representation pre-training, addressing challenges like feature capture, spatial structure, and data efficiency. It combines masked image modeling with patch-pixel alignment, achieving state-of-the-art results with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with capturing fine-grained facial features, ignoring spatial structure, and inefficient use of labeled data. PaCo-FR aims to overcome these limitations.

Method: PaCo-FR integrates structured masking, a patch-based codebook, and spatial consistency constraints to enhance feature discrimination and preserve facial geometry.

Result: The framework achieves top performance in facial analysis tasks using only 2 million unlabeled images, excelling in varied poses, occlusions, and lighting.

Conclusion: PaCo-FR advances facial representation learning, offering a scalable, efficient solution that reduces dependency on annotated datasets.

Abstract: Facial representation pre-training is crucial for tasks like facial
recognition, expression analysis, and virtual reality. However, existing
methods face three key challenges: (1) failing to capture distinct facial
features and fine-grained semantics, (2) ignoring the spatial structure
inherent to facial anatomy, and (3) inefficiently utilizing limited labeled
data. To overcome these, we introduce PaCo-FR, an unsupervised framework that
combines masked image modeling with patch-pixel alignment. Our approach
integrates three innovative components: (1) a structured masking strategy that
preserves spatial coherence by aligning with semantically meaningful facial
regions, (2) a novel patch-based codebook that enhances feature discrimination
with multiple candidate tokens, and (3) spatial consistency constraints that
preserve geometric relationships between facial components. PaCo-FR achieves
state-of-the-art performance across several facial analysis tasks with just 2
million unlabeled images for pre-training. Our method demonstrates significant
improvements, particularly in scenarios with varying poses, occlusions, and
lighting conditions. We believe this work advances facial representation
learning and offers a scalable, efficient solution that reduces reliance on
expensive annotated datasets, driving more effective facial analysis systems.

</details>


### [128] [Slot Attention-based Feature Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.09699)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: SAFF uses slot attention to filter irrelevant features in few-shot learning, improving classification by focusing on meaningful similarities.


<details>
  <summary>Details</summary>
Motivation: Irrelevant features degrade few-shot learning performance, causing misclassification.

Method: Slot Attention-based Feature Filtering (SAFF) integrates slot attention with patch embeddings to filter weak features and uses a similarity matrix for relevance quantification.

Result: SAFF outperforms state-of-the-art methods on benchmarks like CIFAR-FS, FC100, miniImageNet, and tieredImageNet.

Conclusion: SAFF effectively filters irrelevant features, enhancing few-shot learning performance.

Abstract: Irrelevant features can significantly degrade few-shot learn ing performance.
This problem is used to match queries and support images based on meaningful
similarities despite the limited data. However, in this process, non-relevant
fea tures such as background elements can easily lead to confu sion and
misclassification. To address this issue, we pro pose Slot Attention-based
Feature Filtering for Few-Shot Learning (SAFF) that leverages slot attention
mechanisms to discriminate and filter weak features, thereby improving few-shot
classification performance. The key innovation of SAFF lies in its integration
of slot attention with patch em beddings, unifying class-aware slots into a
single attention mechanism to filter irrelevant features effectively. We intro
duce a similarity matrix that computes across support and query images to
quantify the relevance of filtered embed dings for classification. Through
experiments, we demon strate that Slot Attention performs better than other
atten tion mechanisms, capturing discriminative features while reducing
irrelevant information. We validate our approach through extensive experiments
on few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma
geNet, outperforming several state-of-the-art methods.

</details>


### [129] [MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers](https://arxiv.org/abs/2508.09709)
*Qianru Qiu,Jiafeng Mao,Kento Masui,Xueting Wang*

Main category: cs.CV

TL;DR: MangaDiT improves reference-guided line art colorization using Diffusion Transformers and a hierarchical attention mechanism for better color consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with region-level color consistency when reference and target images differ in pose or motion.

Method: Proposes MangaDiT, a Diffusion Transformer model with hierarchical attention and dynamic weighting for implicit semantic correspondence.

Result: Outperforms state-of-the-art methods in qualitative and quantitative evaluations on benchmark datasets.

Conclusion: MangaDiT enhances color alignment and consistency, advancing reference-guided line art colorization.

Abstract: Recent advances in diffusion models have significantly improved the
performance of reference-guided line art colorization. However, existing
methods still struggle with region-level color consistency, especially when the
reference and target images differ in character pose or motion. Instead of
relying on external matching annotations between the reference and target, we
propose to discover semantic correspondences implicitly through internal
attention mechanisms. In this paper, we present MangaDiT, a powerful model for
reference-guided line art colorization based on Diffusion Transformers (DiT).
Our model takes both line art and reference images as conditional inputs and
introduces a hierarchical attention mechanism with a dynamic attention
weighting strategy. This mechanism augments the vanilla attention with an
additional context-aware path that leverages pooled spatial features,
effectively expanding the model's receptive field and enhancing region-level
color alignment. Experiments on two benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches, achieving
superior performance in both qualitative and quantitative evaluations.

</details>


### [130] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: NEURAL is a framework for compressing multimodal medical imaging data using semantics-guided compression, reducing data size by 93.4-97.7% while maintaining high diagnostic performance.


<details>
  <summary>Details</summary>
Motivation: Address storage and transmission challenges of medical imaging data in resource-constrained clinical settings.

Method: Uses cross-attention scores from a vision-language model to prune chest X-rays, creating a compressed graph representation fused with a knowledge graph.

Result: Achieves 93.4-97.7% reduction in image data size with 0.88-0.95 AUC for pneumonia detection, outperforming baselines.

Conclusion: NEURAL resolves the trade-off between data size and clinical utility, enabling efficient workflows without sacrificing performance.

Abstract: The rapid growth of multimodal medical imaging data presents significant
storage and transmission challenges, particularly in resource-constrained
clinical settings. We propose NEURAL, a novel framework that addresses this by
using semantics-guided data compression. Our approach repurposes
cross-attention scores between the image and its radiological report from a
fine-tuned generative vision-language model to structurally prune chest X-rays,
preserving only diagnostically critical regions. This process transforms the
image into a highly compressed, graph representation. This unified graph-based
representation fuses the pruned visual graph with a knowledge graph derived
from the clinical report, creating a universal data structure that simplifies
downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for
pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size
while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming
other baseline models that use uncompressed data. By creating a persistent,
task-agnostic data asset, NEURAL resolves the trade-off between data size and
clinical utility, enabling efficient workflows and teleradiology without
sacrificing performance. Our NEURAL code is available at
https://github.com/basiralab/NEURAL.

</details>


### [131] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: A novel sheaf-based framework for fusing MRI and histopathology data improves glioblastoma subtype classification, addressing limitations of current methods and enabling robust handling of incomplete data.


<details>
  <summary>Details</summary>
Motivation: Current glioblastoma subtype classification relies on invasive tissue extraction, and existing multimodal fusion methods lack robust structural preservation and handling of missing data.

Method: Proposes a sheaf-based framework for structure-aware fusion of MRI and histopathology data, ensuring consistent feature retention and handling incomplete data.

Result: The model outperforms baseline methods and shows robustness in scenarios with missing or incomplete data.

Conclusion: The framework advances virtual biopsy tools for rapid diagnostics, with potential for clinical application.

Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates.
Recent studies have shown that glioblastoma molecular subtype classification
serves as a significant biomarker for effective targeted therapy selection.
However, this classification currently requires invasive tissue extraction for
comprehensive histopathological analysis. Existing multimodal approaches
combining MRI and histopathology images are limited and lack robust mechanisms
for preserving shared structural information across modalities. In particular,
graph-based models often fail to retain discriminative features within
heterogeneous graphs, and structural reconstruction mechanisms for handling
missing or incomplete modality data are largely underexplored. To address these
limitations, we propose a novel sheaf-based framework for structure-aware and
consistent fusion of MRI and histopathology data. Our model outperforms
baseline methods and demonstrates robustness in incomplete or missing data
scenarios, contributing to the development of virtual biopsy tools for rapid
diagnostics. Our source code is available at
https://github.com/basiralab/MMSN/.

</details>


### [132] [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](https://arxiv.org/abs/2508.09732)
*Romeo Valentin,Sydney M. Katz,Artur B. Carneiro,Don Walker,Mykel J. Kochenderfer*

Main category: cs.CV

TL;DR: A vision-based pipeline for aircraft pose estimation from runway images, featuring innovations in neural architecture, loss function, and fault detection, aiming to meet aviation safety standards.


<details>
  <summary>Details</summary>
Motivation: Ensuring robustness and safety of data-driven computer vision systems for aviation applications, particularly in autonomous navigation.

Method: Proposes a pipeline with: (i) a neural architecture using spatial Soft Argmax for keypoint regression, (ii) a calibrated loss function for uncertainty, and (iii) adapted RAIM for fault detection.

Result: Outperforms baselines in accuracy, provides well-calibrated uncertainty estimates, and enables fault detection with sub-pixel precision.

Conclusion: The pipeline advances toward certifiable safety-critical aviation systems by combining accuracy, uncertainty calibration, and runtime fault detection.

Abstract: Recent advances in data-driven computer vision have enabled robust autonomous
navigation capabilities for civil aviation, including automated landing and
runway detection. However, ensuring that these systems meet the robustness and
safety requirements for aviation applications remains a major challenge. In
this work, we present a practical vision-based pipeline for aircraft pose
estimation from runway images that represents a step toward the ability to
certify these systems for use in safety-critical aviation applications. Our
approach features three key innovations: (i) an efficient, flexible neural
architecture based on a spatial Soft Argmax operator for probabilistic keypoint
regression, supporting diverse vision backbones with real-time inference; (ii)
a principled loss function producing calibrated predictive uncertainties, which
are evaluated via sharpness and calibration metrics; and (iii) an adaptation of
Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling
runtime detection and rejection of faulty model outputs. We implement and
evaluate our pose estimation pipeline on a dataset of runway images. We show
that our model outperforms baseline architectures in terms of accuracy while
also producing well-calibrated uncertainty estimates with sub-pixel precision
that can be used downstream for fault detection.

</details>


### [133] [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)
*Lin Long,Yichen He,Wentao Ye,Yiyuan Pan,Yuan Lin,Hang Li,Junbo Zhao,Wei Li*

Main category: cs.CV

TL;DR: M3-Agent is a multimodal framework with long-term memory, outperforming baselines in tasks like long-video QA.


<details>
  <summary>Details</summary>
Motivation: To develop a human-like multimodal agent with long-term memory for deeper environmental understanding.

Method: Uses reinforcement learning, entity-centric memory, and benchmarks (M3-Bench) for evaluation.

Result: Outperforms baselines by 5.3-7.7% in accuracy across benchmarks.

Conclusion: Advances multimodal agents with human-like memory, offering practical insights.

Abstract: We introduce M3-Agent, a novel multimodal agent framework equipped with
long-term memory. Like humans, M3-Agent can process real-time visual and
auditory inputs to build and update its long-term memory. Beyond episodic
memory, it also develops semantic memory, enabling it to accumulate world
knowledge over time. Its memory is organized in an entity-centric, multimodal
format, allowing deeper and more consistent understanding of the environment.
Given an instruction, M3-Agent autonomously performs multi-turn, iterative
reasoning and retrieves relevant information from memory to accomplish the
task. To evaluate memory effectiveness and memory-based reasoning in multimodal
agents, we develop M3-Bench, a new long-video question answering benchmark.
M3-Bench comprises 100 newly recorded real-world videos captured from a robot's
perspective (M3-Bench-robot) and 929 web-sourced videos across diverse
scenarios (M3-Bench-web). We annotate question-answer pairs designed to test
key capabilities essential for agent applications, such as human understanding,
general knowledge extraction, and cross-modal reasoning. Experimental results
show that M3-Agent, trained via reinforcement learning, outperforms the
strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,
achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web
and VideoMME-long, respectively. Our work advances the multimodal agents toward
more human-like long-term memory and provides insights into their practical
design. Model, code and data are available at
https://github.com/bytedance-seed/m3-agent

</details>


### [134] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: The paper introduces Region-to-Region (R2R) transformation for image harmonization, addressing detail preservation and dataset limitations with Clear-VAE and MACA, and proposes a new synthetic dataset, RPHarmony.


<details>
  <summary>Details</summary>
Motivation: Current LDM-based harmonization struggles with detail preservation and synthetic datasets lack realism. The goal is to improve harmonization by leveraging region-specific information and creating more diverse datasets.

Method: Proposes R2R transformation, Clear-VAE for detail preservation, and MACA for dynamic foreground adjustment. Introduces Random Poisson Blending for dataset creation.

Result: R2R outperforms existing methods in metrics and visual harmony. RPHarmony dataset enhances realism in generated images.

Conclusion: The R2R approach and RPHarmony dataset significantly improve image harmonization, with open access to code, dataset, and model weights.

Abstract: The goal of image harmonization is to adjust the foreground in a composite
image to achieve visual consistency with the background. Recently, latent
diffusion model (LDM) are applied for harmonization, achieving remarkable
results. However, LDM-based harmonization faces challenges in detail
preservation and limited harmonization ability. Additionally, current synthetic
datasets rely on color transfer, which lacks local variations and fails to
capture complex real-world lighting conditions. To enhance harmonization
capabilities, we propose the Region-to-Region transformation. By injecting
information from appropriate regions into the foreground, this approach
preserves original details while achieving image harmonization or, conversely,
generating new composite data. From this perspective, We propose a novel model
R2R. Specifically, we design Clear-VAE to preserve high-frequency details in
the foreground using Adaptive Filter while eliminating disharmonious elements.
To further enhance harmonization, we introduce the Harmony Controller with
Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the
foreground based on the channel importance of both foreground and background
regions. To address the limitation of existing datasets, we propose Random
Poisson Blending, which transfers color and lighting information from a
suitable region to the foreground, thereby generating more diverse and
challenging synthetic images. Using this method, we construct a new synthetic
dataset, RPHarmony. Experiments demonstrate the superiority of our method over
other methods in both quantitative metrics and visual harmony. Moreover, our
dataset helps the model generate more realistic images in real examples. Our
code, dataset, and model weights have all been released for open access.

</details>


### [135] [MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models](https://arxiv.org/abs/2508.09779)
*Dianyi Wang,Siyuan Wang,Zejun Li,Yikun Wang,Yitong Li,Duyu Tang,Xiaoyu Shen,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CV

TL;DR: The paper proposes MoIIE, a sparse Mixture of Experts architecture for LVLMs, combining intra- and inter-modality experts to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Address computational costs of dense LVLMs and challenges in applying MoE to multi-modal tasks.

Method: Introduces MoIIE with modality-guided expert routing and a two-stage training strategy.

Result: MoIIE models match or surpass performance of larger MoE-LLMs with fewer activated parameters.

Conclusion: MoIIE offers an efficient and effective solution for multi-modal learning in LVLMs.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across multi-modal tasks by scaling model size and training data. However,
these dense LVLMs incur significant computational costs and motivate the
exploration of sparse Mixture of Experts (MoE) architectures. While MoE improve
parameter efficiency, effectively applying MoE to simultaneously model
modality-specific features and cross-modal associations in LVLMs remains
challenging. In this work, we propose to incorporate Mixture of Intra- and
Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is
guided by its modality, directing tokens to their respective intra-modality
experts as well as a shared pool of inter-modality experts, enabling the model
to jointly learn rich intra-modal features and cross-modal interactions. We
further introduce an effective and straightforward two-stage training strategy,
which facilitates the direct activation of both MoE and multi-modal
capabilities. Extensive experiments across different data scales and LLM
backbone demonstrate the effectiveness, efficiency and generality of our
approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters
match or even surpass the performance of existing advanced open-source MoE-LLMs
based multi-modal models that involve more activated parameters. The code is
available at https://github.com/AlenjandroWang/MoIIE.

</details>


### [136] [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/abs/2508.09780)
*Nahyuk Lee,Juhong Min,Junhong Lee,Chunghyun Park,Minsu Cho*

Main category: cs.CV

TL;DR: A new shape-matching method, combinative matching, is introduced for geometric shape assembly by modeling 'identical surface shape' and 'opposite volume occupancy' to reduce ambiguities and improve robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on identical surface alignment, missing the dual properties of interlocking shapes. The paper aims to address this gap.

Method: The method models 'identical surface shape' and 'opposite volume occupancy,' using equivariant neural networks to align regions by shape orientation.

Result: The approach reduces local ambiguities and outperforms state-of-the-art methods on geometric assembly benchmarks.

Conclusion: Combinative matching effectively combines interlocking parts by leveraging dual shape properties, demonstrating superior performance.

Abstract: This paper introduces a new shape-matching methodology, combinative matching,
to combine interlocking parts for geometric shape assembly. Previous methods
for geometric assembly typically rely on aligning parts by finding identical
surfaces between the parts as in conventional shape matching and registration.
In contrast, we explicitly model two distinct properties of interlocking
shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method
thus learns to establish correspondences across regions where their surface
shapes appear identical but their volumes occupy the inverted space to each
other. To facilitate this process, we also learn to align regions in rotation
by estimating their shape orientations via equivariant neural networks. The
proposed approach significantly reduces local ambiguities in matching and
allows a robust combination of parts in assembly. Experimental results on
geometric assembly benchmarks demonstrate the efficacy of our method,
consistently outperforming the state of the art. Project page:
https://nahyuklee.github.io/cmnet.

</details>


### [137] [DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.09785)
*Linpu He,Yanan Li,Bingze Li,Elvis Han Cui,Donghui Wang*

Main category: cs.CV

TL;DR: DSS-Prompt transforms a pre-trained Vision Transformer into a strong FSCIL classifier using static and dynamic prompts, outperforming state-of-the-art methods without incremental task training.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of few-shot class-incremental learning (FSCIL) by leveraging pre-trained models' generalization ability to learn new concepts from limited samples without forgetting old ones.

Method: Introduces DSS-Prompt, utilizing static prompts for domain adaptation and dynamic prompts for instance-aware semantics, generated via a pre-trained multi-modal model and adjusted adaptively across layers.

Result: Outperforms existing methods on four benchmarks, achieving better performance and alleviating catastrophic forgetting.

Conclusion: DSS-Prompt is a simple yet effective approach for FSCIL, demonstrating strong generalization and adaptability without additional incremental task training.

Abstract: Learning from large-scale pre-trained models with strong generalization
ability has shown remarkable success in a wide range of downstream tasks
recently, but it is still underexplored in the challenging few-shot
class-incremental learning (FSCIL) task. It aims to continually learn new
concepts from limited training samples without forgetting the old ones at the
same time. In this paper, we introduce DSS-Prompt, a simple yet effective
approach that transforms the pre-trained Vision Transformer with minimal
modifications in the way of prompts into a strong FSCIL classifier. Concretely,
we synergistically utilize two complementary types of prompts in each
Transformer block: static prompts to bridge the domain gap between the
pre-training and downstream datasets, thus enabling better adaption; and
dynamic prompts to capture instance-aware semantics, thus enabling easy
transfer from base to novel classes. Specially, to generate dynamic prompts, we
leverage a pre-trained multi-modal model to extract input-related diverse
semantics, thereby generating complementary input-aware prompts, and then
adaptively adjust their importance across different layers. In this way, on top
of the prompted visual embeddings, a simple prototype classifier can beat
state-of-the-arts without further training on the incremental tasks. We conduct
extensive experiments on four benchmarks to validate the effectiveness of our
DSS-Prompt and show that it consistently achieves better performance than
existing approaches on all datasets and can alleviate the catastrophic
forgetting issue as well.

</details>


### [138] [MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking](https://arxiv.org/abs/2508.09796)
*Yingjie Wang,Zhixing Wang,Le Zheng,Tianxiao Liu,Roujing Li,Xueyao Hu*

Main category: cs.CV

TL;DR: MeMoSORT is a novel MOT tracker addressing limitations of traditional methods with a Memory-assisted Kalman filter and Motion-adaptive IoU, achieving top performance on DanceTrack and SportsMOT.


<details>
  <summary>Details</summary>
Motivation: Overcome challenges in MOT like complex motion and occlusions, which conventional methods fail to handle due to rigid motion models and association rules.

Method: Introduces MeKF for better motion modeling and Mo-IoU for adaptive association, ensuring robustness and real-time performance.

Result: Achieves HOTA scores of 67.9% (DanceTrack) and 82.1% (SportsMOT), outperforming existing methods.

Conclusion: MeMoSORT effectively addresses MOT challenges with innovative yet lightweight solutions, setting new benchmarks.

Abstract: Multi-object tracking (MOT) in human-dominant scenarios, which involves
continuously tracking multiple people within video sequences, remains a
significant challenge in computer vision due to targets' complex motion and
severe occlusions. Conventional tracking-by-detection methods are fundamentally
limited by their reliance on Kalman filter (KF) and rigid Intersection over
Union (IoU)-based association. The motion model in KF often mismatches
real-world object dynamics, causing filtering errors, while rigid association
struggles under occlusions, leading to identity switches or target loss. To
address these issues, we propose MeMoSORT, a simple, online, and real-time MOT
tracker with two key innovations. First, the Memory-assisted Kalman filter
(MeKF) uses memory-augmented neural networks to compensate for mismatches
between assumed and actual object motion. Second, the Motion-adaptive IoU
(Mo-IoU) adaptively expands the matching space and incorporates height
similarity to reduce the influence of detection errors and association
failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT
show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of
67.9\% and 82.1\%, respectively.

</details>


### [139] [MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention](https://arxiv.org/abs/2508.09802)
*Xin Du,Maoyuan Xu,Zhi Ying*

Main category: cs.CV

TL;DR: MUJICA enhances PBR material upscaling by integrating cross-map attention into pre-trained SISR models, improving consistency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing SISR methods fail to address cross-map inconsistency and modality-specific features in PBR material upscaling.

Method: MUJICA adapts pre-trained Swin-transformer-based SISR models using cross-map attention for feature fusion.

Result: MUJICA improves PSNR, SSIM, and LPIPS scores while maintaining cross-map consistency.

Conclusion: MUJICA achieves state-of-the-art performance in PBR material upscaling with efficient training.

Abstract: Physically Based Rendering (PBR) materials are typically characterized by
multiple 2D texture maps such as basecolor, normal, metallic, and roughness
which encode spatially-varying bi-directional reflectance distribution function
(SVBRDF) parameters to model surface reflectance properties and microfacet
interactions. Upscaling SVBRDF material is valuable for modern 3D graphics
applications. However, existing Single Image Super-Resolution (SISR) methods
struggle with cross-map inconsistency, inadequate modeling of modality-specific
features, and limited generalization due to data distribution shifts. In this
work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention
(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based
SISR models for PBR material super-resolution. MUJICA is seamlessly attached
after the pre-trained and frozen SISR backbone. It leverages cross-map
attention to fuse features while preserving remarkable reconstruction ability
of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and
HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map
consistency. Experiments demonstrate that MUJICA enables efficient training
even with limited resources and delivers state-of-the-art performance on PBR
material datasets.

</details>


### [140] [Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology](https://arxiv.org/abs/2508.09805)
*Jonathan Williams Ramirez,Dina Zemlyanker,Lucas Deden-Binder,Rogeny Herisse,Erendira Garcia Pallares,Karthik Gopinath,Harshvardhan Gazula,Christopher Mount,Liana N. Kozanno,Michael S. Marshall,Theresa R. Connors,Matthew P. Frosch,Mark Montine,Derek H. Oakley,Christine L. Mac Donald,C. Dirk Keene,Bradley T. Hyman,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: A deep learning model (U-Net) automates segmentation of brain tissue in photographs, achieving high accuracy comparable to manual segmentation.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation of brain tissue in photographs is costly, prompting the need for an automated solution.

Method: A U-Net model was trained on 1,414 manually segmented images and 2,000 synthetic images for generalizability. Performance was evaluated against manual labels.

Result: The model achieved a median Dice score >0.98, mean surface distance <0.4mm, and 95% Hausdorff distance <1.60mm, matching inter-/intra-rater variability.

Conclusion: The tool provides accurate, automated segmentation and is publicly available for broader use.

Abstract: Advances in image registration and machine learning have recently enabled
volumetric analysis of \emph{postmortem} brain tissue from conventional
photographs of coronal slabs, which are routinely collected in brain banks and
neuropathology laboratories worldwide. One caveat of this methodology is the
requirement of segmentation of the tissue from photographs, which currently
requires costly manual intervention. In this article, we present a deep
learning model to automate this process. The automatic segmentation tool relies
on a U-Net architecture that was trained with a combination of
\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,
from specimens with varying diagnoses, photographed at two different sites; and
\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding
masks generated from MRI scans for improved generalizability to unseen
photographic setups. Automated model predictions on a subset of photographs not
seen in training were analyzed to estimate performance compared to manual
labels -- including both inter- and intra-rater variability. Our model achieved
a median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\%
Hausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.
Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.

</details>


### [141] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: TRACE models 3D scene dynamics from videos without labels, using rigid particles to learn physical motion parameters, outperforming baselines in frame extrapolation and enabling object segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex motion physics or require additional labels. TRACE aims to learn physics directly from videos without human labels.

Method: TRACE formulates 3D points as rigid particles, learning translation-rotation dynamics and physical parameters for each particle.

Result: TRACE outperforms baselines in future frame extrapolation and allows easy object segmentation via clustering physical parameters.

Conclusion: TRACE effectively models complex 3D scene dynamics without labels, offering superior performance and segmentation capabilities.

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical
information just from dynamic multi-view videos in the absence of any human
labels. By leveraging physics-informed losses as soft constraints or
integrating simple physics models into neural nets, existing works often fail
to learn complex motion physics, or doing so requires additional labels such as
object types or masks. We propose a new framework named TRACE to model the
motion physics of complex dynamic 3D scenes. The key novelty of our method is
that, by formulating each 3D point as a rigid particle with size and
orientation in space, we directly learn a translation rotation dynamics system
for each particle, explicitly estimating a complete set of physical parameters
to govern the particle's motion over time. Extensive experiments on three
existing dynamic datasets and one newly created challenging synthetic datasets
demonstrate the extraordinary performance of our method over baselines in the
task of future frame extrapolation. A nice property of our framework is that
multiple objects or parts can be easily segmented just by clustering the
learned physical parameters.

</details>


### [142] [Poaching Hotspot Identification Using Satellite Imagery](https://arxiv.org/abs/2508.09812)
*Aryan Pandhi,Shrey Baid,Sanjali Jha*

Main category: cs.CV

TL;DR: The paper discusses the escalating issue of elephant poaching in Africa, driven by ivory demand, and proposes a Computer Vision (CV) model to dynamically identify poaching hotspots using geographic indicators and satellite imagery.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the critical endangerment of African elephants due to poaching, with current anti-poaching efforts being inefficient in remote areas. The dynamic nature of poaching hotspots necessitates an automated solution.

Method: The proposed method involves developing a CV model to analyze geographic and environmental indicators (e.g., watering holes, seasons, altitude) from satellite imagery, enabling large-scale surveillance without human intervention.

Result: The expected result is an efficient, scalable system to identify poaching hotspots dynamically, improving resource allocation for anti-poaching efforts.

Conclusion: The paper concludes that integrating CV with satellite imagery offers a promising, non-invasive solution to combat elephant poaching by addressing the limitations of current manual tracking methods.

Abstract: Elephant Poaching in African countries has been a decade-old problem. So much
so that African Forest Elephants are now listed as an endangered species, and
African Savannah Elephants as critically endangered by the IUCN (International
Union for Conservation of Nature). [1] Elephants are hunted primarily for their
ivory tusks which caused many elephants to be born tuskless as a genetic
modification for survival. [2] Data gathered by recent studies shows that
though poaching methods remain the same, the poaching grounds are rather
dynamic. Poachers have shifted to areas with less ranger patrols and several
other factors like watering holes, seasons, altitude etc. cause constant shifts
in poaching hotspot locations. [3] After a period of low poaching from
2000-2014, poaching numbers in African countries are now on the rise again --
WWF (World Wildlife Foundation) says there are 20,000 elephants poached
annually [4]. In African countries, anti-poaching efforts are concentrated near
towns, while a majority of poaching occurs in the deserted regions. All of
these factors result in the need for a Computer Vision Model to identify
poaching hotspots through locating the geographic indicators of favorable
poaching regions. A CV model eliminates the need to manually track poachers and
account for the environmental factors to deploy resources and its combination
with satellite imagery allows us to survey large areas without disturbing local
species or cross border aviation restrictions.

</details>


### [143] [Evolution of Low-Level and Texture Human-CLIP Alignment](https://arxiv.org/abs/2508.09814)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: CLIP's alignment with low-level human image quality peaks early in training, then declines due to shifts from texture to shape bias and noise sensitivity.


<details>
  <summary>Details</summary>
Motivation: To understand why CLIP's correlation with low-level human perception peaks early and declines, focusing on shape-texture bias and noise robustness.

Method: Analyzed CLIP's training dynamics, examining shape-texture bias alignment and classification accuracy under noise.

Result: Early training emphasizes low-level features (texture bias), aligning with human perception but increasing noise sensitivity. Later stages shift to shape bias, improving robustness but reducing low-level alignment.

Conclusion: The trade-off between perceptual alignment and robustness is linked to learning dynamics, offering insights for optimizing vision-language models.

Abstract: During the training of multi-modal models like CLIP, we observed an
intriguing phenomenon: the correlation with low-level human image quality
assessments peaks in the early epochs before gradually declining. This study
investigates this observation and seeks to understand its causes through two
key factors: shape-texture bias alignment and classification accuracy drop
under noise. Our findings suggest that CLIP initially learn low-level visual
features, enhancing its alignment with low-level human perception but also
increasing its sensitivity to noise and its texture bias. As training
progresses, the model shifts toward more abstract shape-based representations,
improving noise robustness but reducing alignment with low-level human
perception. These results suggest that these factors shared an underlying
learning mechanism and provide new insights into optimizing the trade-off
between perceptual alignment and robustness in vision-language models.

</details>


### [144] [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/abs/2508.09818)
*Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Abir Ahmed,Liew Tze Hui*

Main category: cs.CV

TL;DR: ViMoNet combines motion and video data for better human behavior understanding, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To capture nuanced human actions by integrating motion and video data, addressing limitations of current models.

Method: ViMoNet uses joint training with motion-text and video-text data, leveraging their complementary strengths.

Result: ViMoNet excels in caption generation, motion understanding, and behavior interpretation.

Conclusion: ViMoNet is effective for human behavior analysis, supported by the new VIMOS dataset and benchmark.

Abstract: This study investigates how large language models (LLMs) can be used to
understand human behavior using motion and video data. We think that mixing
both types is essential to completely capture the nuanced movements and
meanings of human actions, in contrast to recent models that simply concentrate
on motion data or films. To address this, we provide ViMoNet, a straightforward
yet effective framework for comprehending, characterizing, and deducing human
action. ViMoNet employs a joint training strategy that leverages the advantages
of two data types: detailed motion-text data, which is more exact, and generic
video-text data, which is more comprehensive but less detailed. This aids in
the model's acquisition of rich data regarding time and space in human
behavior. Additionally, we provide a brand new dataset named VIMOS that
contains a variety of films, motion sequences, instructions, and subtitles. We
developed ViMoNet-Bench, a standardized benchmark with carefully labeled
samples, to evaluate how well models understand human behavior. Our tests show
that ViMoNet outperforms existing methods in caption generation, motion
understanding, and behavior interpretation.

</details>


### [145] [Physical Autoregressive Model for Robotic Manipulation without Action Pretraining](https://arxiv.org/abs/2508.09822)
*Zijian Song,Sihan Qin,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: PAR, a Physical Autoregressive Model, uses pretrained video generation to understand robot-environment dynamics without action pretraining, achieving high success rates and accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of manipulation data by leveraging pretrained large models from other modalities, specifically video generation.

Method: Combines frames and actions into physical tokens, uses a DiT-based de-tokenizer, and incorporates causal masks, inverse kinematics, parallel training, and KV-cache for efficiency.

Result: 100% success rate on PushCube task, matches action-pretrained baselines, and predicts future videos with aligned action trajectories.

Conclusion: PAR demonstrates effective transfer of world knowledge from video pretraining to robotics, offering a promising approach for manipulation tasks.

Abstract: The scarcity of manipulation data has motivated the use of pretrained large
models from other modalities in robotics. In this work, we build upon
autoregressive video generation models to propose a Physical Autoregressive
Model (PAR), where physical tokens combine frames and actions to represent the
joint evolution of the robot and its environment. PAR leverages the world
knowledge embedded in video pretraining to understand physical dynamics without
requiring action pretraining, enabling accurate video prediction and consistent
action trajectories. It also adopts a DiT-based de-tokenizer to model frames
and actions as continuous tokens, mitigating quantization errors and
facilitating mutual enhancement. Furthermore, we incorporate a causal mask with
inverse kinematics, parallel training, and the KV-cache mechanism to further
improve performance and efficiency. Experiments on the ManiSkill benchmark show
that PAR achieves a 100\% success rate on the PushCube task, matches the
performance of action-pretrained baselines on other tasks, and accurately
predicts future videos with tightly aligned action trajectories. These findings
underscore a promising direction for robotic manipulation by transferring world
knowledge from autoregressive video pretraining.

</details>


### [146] [KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.09823)
*Valentin Boussot,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: KonfAI is a configurable deep learning framework for medical imaging, enabling workflow definition via YAML files for reproducibility and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance reproducibility, transparency, and reduce development time in medical imaging tasks.

Method: Uses structured YAML configuration files for defining workflows, supports advanced strategies like patch-based learning and multi-model setups.

Result: Successfully applied to segmentation, registration, and image synthesis, achieving top results in challenges.

Conclusion: KonfAI is a versatile, open-source tool for medical imaging, offering modularity and extensibility.

Abstract: KonfAI is a modular, extensible, and fully configurable deep learning
framework specifically designed for medical imaging tasks. It enables users to
define complete training, inference, and evaluation workflows through
structured YAML configuration files, without modifying the underlying code.
This declarative approach enhances reproducibility, transparency, and
experimental traceability while reducing development time. Beyond the
capabilities of standard pipelines, KonfAI provides native abstractions for
advanced strategies including patch-based learning, test-time augmentation,
model ensembling, and direct access to intermediate feature representations for
deep supervision. It also supports complex multi-model training setups such as
generative adversarial architectures. Thanks to its modular and extensible
architecture, KonfAI can easily accommodate custom models, loss functions, and
data processing components. The framework has been successfully applied to
segmentation, registration, and image synthesis tasks, and has contributed to
top-ranking results in several international medical imaging challenges. KonfAI
is open source and available at
\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.

</details>


### [147] [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/abs/2508.09824)
*Xuhong Huang,Shiqi Liu,Kai Zhang,Ying Tai,Jian Yang,Hui Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: The paper introduces a depthwise reverse convolution operator to address the lack of a true inverse for convolution in neural networks, proposing ConverseNet for image restoration tasks.


<details>
  <summary>Details</summary>
Motivation: Current transposed convolution doesn't act as a true inverse of convolution, limiting neural network design. The paper aims to fill this gap with a novel reverse convolution operator.

Method: The authors formulate a regularized least-squares problem to create a depthwise reverse convolution operator, integrate it into a Transformer-like block, and apply it in ConverseNet for tasks like denoising and super-resolution.

Result: Experiments show the proposed operator is effective as a building block, with ConverseNet variants outperforming in tasks like Gaussian denoising, super-resolution, and deblurring.

Conclusion: The work successfully introduces a reverse convolution operator, paving the way for new deep learning operators and applications.

Abstract: Convolution and transposed convolution are fundamental operators widely used
in neural networks. However, transposed convolution (a.k.a. deconvolution) does
not serve as a true inverse of convolution due to inherent differences in their
mathematical formulations. To date, no reverse convolution operator has been
established as a standard component in neural architectures. In this paper, we
propose a novel depthwise reverse convolution operator as an initial attempt to
effectively reverse depthwise convolution by formulating and solving a
regularized least-squares optimization problem. We thoroughly investigate its
kernel initialization, padding strategies, and other critical aspects to ensure
its effective implementation. Building upon this operator, we further construct
a reverse convolution block by combining it with layer normalization,
1$\times$1 convolution, and GELU activation, forming a Transformer-like
structure. The proposed operator and block can directly replace conventional
convolution and transposed convolution layers in existing architectures,
leading to the development of ConverseNet. Corresponding to typical image
restoration models such as DnCNN, SRResNet and USRNet, we train three variants
of ConverseNet for Gaussian denoising, super-resolution and deblurring,
respectively. Extensive experiments demonstrate the effectiveness of the
proposed reverse convolution operator as a basic building module. We hope this
work could pave the way for developing new operators in deep model design and
applications.

</details>


### [148] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: RayletDF, a method for 3D surface reconstruction, uses raylet distance fields to predict surface points from query rays, outperforming existing methods in efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing coordinate-based methods for 3D surface reconstruction are computationally intensive. RayletDF aims to provide a more efficient and generalizable solution.

Method: RayletDF employs a pipeline with three modules: raylet feature extractor, raylet distance field predictor, and multi-raylet blender, to predict and aggregate surface points.

Result: The method shows superior performance on public datasets, excelling in generalization by reconstructing surfaces in a single-forward pass on unseen data.

Conclusion: RayletDF offers an efficient, generalizable solution for 3D surface reconstruction, with strong performance on diverse datasets.

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [149] [Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment](https://arxiv.org/abs/2508.09843)
*Hao Yang,Xu Zhang,Jiaqi Ma,Linwei Zhu,Yun Zhang,Huan Zhang*

Main category: cs.CV

TL;DR: A graph neural network-based OIQA framework improves quality assessment by modeling spatial distortion non-uniformity through structured viewports and advanced feature extraction.


<details>
  <summary>Details</summary>
Motivation: Existing OIQA methods fail to address locally non-uniform distortions due to poor modeling of spatial quality variations and ineffective feature representation.

Method: Uses Fibonacci sphere sampling for viewport generation, multi-stage feature extraction, and integrates GAT and graph transformers to capture local and long-range spatial dependencies.

Result: Outperforms existing methods on large-scale OIQA databases, showing strong generalization.

Conclusion: The proposed framework effectively addresses spatial distortion non-uniformity, offering superior performance and generalization.

Abstract: Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to
evaluate locally non-uniform distortions due to inadequate modeling of spatial
variations in quality and ineffective feature representation capturing both
local details and global context. To address this, we propose a graph neural
network-based OIQA framework that explicitly models structural relationships
between viewports to enhance perception of spatial distortion non-uniformity.
Our approach employs Fibonacci sphere sampling to generate viewports with
well-structured topology, representing each as a graph node. Multi-stage
feature extraction networks then derive high-dimensional node representation.
To holistically capture spatial dependencies, we integrate a Graph Attention
Network (GAT) modeling fine-grained local distortion variations among adjacent
viewports, and a graph transformer capturing long-range quality interactions
across distant regions. Extensive experiments on two large-scale OIQA databases
with complex spatial distortions demonstrate that our method significantly
outperforms existing approaches, confirming its effectiveness and strong
generalization capability.

</details>


### [150] [Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance](https://arxiv.org/abs/2508.09847)
*Dhruvraj Singh Rawat,Enggen Sherpa,Rishikesan Kirupanantha,Tin Hoang*

Main category: cs.CV

TL;DR: Benchmarking diffusion models for human face generation on CelebAMask-HQ, comparing UNet and DiT architectures, and enhancing controllability with InfoNCE loss and SegFormer-based segmentation.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve the performance of diffusion models for controlled face generation, especially in limited data settings.

Method: Compared UNet and DiT for unconditional generation, fine-tuned Stable Diffusion with LoRA, and integrated InfoNCE loss and SegFormer for better attribute and segmentation encoding.

Result: Improved semantic alignment and controllability in attribute-guided face generation.

Conclusion: Contrastive embedding and advanced segmentation encoding enhance controlled face generation in small datasets.

Abstract: We present a benchmark of diffusion models for human face generation on a
small-scale CelebAMask-HQ dataset, evaluating both unconditional and
conditional pipelines. Our study compares UNet and DiT architectures for
unconditional generation and explores LoRA-based fine-tuning of pretrained
Stable Diffusion models as a separate experiment. Building on the
multi-conditioning approach of Giambi and Lisanti, which uses both attribute
vectors and segmentation masks, our main contribution is the integration of an
InfoNCE loss for attribute embedding and the adoption of a SegFormer-based
segmentation encoder. These enhancements improve the semantic alignment and
controllability of attribute-guided synthesis. Our results highlight the
effectiveness of contrastive embedding learning and advanced segmentation
encoding for controlled face generation in limited data settings.

</details>


### [151] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: ARI3D is a software tool designed to assist in the interactive analysis of 3D X-ray CT images, improving phase identification, addressing partial volume effects, and enhancing quantification accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenges posed by imaging artifacts like beam hardening and partial volume effects in 3D X-ray CT necessitate user-driven decisions for microstructure analysis, motivating the development of ARI3D.

Method: ARI3D provides an interactive protocol to classify and quantify objects in 3D CT images, focusing on phase identification, partial volume correction, and accurate quantification.

Result: The tool aims to improve detection limits, accuracy, and harmonize quantitative 3D analysis across scientific fields.

Conclusion: ARI3D addresses key challenges in 3D CT image analysis, offering a streamlined and accurate approach for microstructure quantification.

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>


### [152] [Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment](https://arxiv.org/abs/2508.09850)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Valero Laparra,Jesus Malo*

Main category: cs.CV

TL;DR: ViTs' perceptual alignment with human judgments decreases with larger models, repeated training, and stronger data augmentation/regularization, despite minimal impact from dataset diversity.


<details>
  <summary>Details</summary>
Motivation: To explore how ViTs align with human perception and the impact of model size, dataset size, data augmentation, and regularization on this alignment.

Method: Systematic analysis of ViTs on the TID2013 dataset, varying model size, dataset size, data augmentation, and regularization.

Result: Larger models, repeated training, and stronger augmentation/regularization reduce perceptual alignment; dataset diversity has minimal impact.

Conclusion: Trade-offs exist between model complexity, training strategies, and human-like perceptual alignment, crucial for applications needing human-like vision.

Abstract: Vision Transformers (ViTs) achieve remarkable performance in image
recognition tasks, yet their alignment with human perception remains largely
unexplored. This study systematically analyzes how model size, dataset size,
data augmentation and regularization impact ViT perceptual alignment with human
judgments on the TID2013 dataset. Our findings confirm that larger models
exhibit lower perceptual alignment, consistent with previous works. Increasing
dataset diversity has a minimal impact, but exposing models to the same images
more times reduces alignment. Stronger data augmentation and regularization
further decrease alignment, especially in models exposed to repeated training
cycles. These results highlight a trade-off between model complexity, training
strategies, and alignment with human perception, raising important
considerations for applications requiring human-like visual understanding.

</details>


### [153] [OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better](https://arxiv.org/abs/2508.09857)
*Yupeng Zhou,Zhen Li,Ziheng Ouyang,Yuming Chen,Ruoyi Du,Daquan Zhou,Bin Fu,Yihao Liu,Peng Gao,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: OneVAE enhances discrete video VAEs by leveraging continuous VAEs, improving training stability, speed, and performance through FSQ quantization, multi-token quantization, and first-frame reconstruction.


<details>
  <summary>Details</summary>
Motivation: Aligning video tokens with text tokens for multi-modal LLMs is challenging due to spatiotemporal compression and unstable training in discrete VAEs. Leveraging continuous VAEs can address these issues.

Method: Uses FSQ to preserve continuous VAE priors, introduces multi-token quantization for better reconstruction, and strengthens first-frame reconstruction. Proposes a joint discrete-continuous optimization scheme.

Result: Faster convergence, superior performance, and competitive results in both discrete and continuous representations. Achieves nearly 1 dB PSNR improvement.

Conclusion: OneVAE successfully bridges discrete and continuous video representations, offering a unified and efficient solution for multi-modal LLMs.

Abstract: Encoding videos into discrete tokens could align with text tokens to
facilitate concise and unified multi-modal LLMs, yet introducing significant
spatiotemporal compression compared to continuous video representation.
Previous discrete video VAEs experienced unstable training, long training time,
and degraded reconstruction quality. Given the easier training and superior
performance of continuous VAEs, an intuitive idea is to enhance discrete video
VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between
discrete and continuous representations, we found that FSQ could effectively
preserve pre-trained continuous VAE priors compared to other quantization
methods. By leveraging continuous VAE priors, it converges several times faster
than training from scratch and achieves superior performance at convergence.
Meanwhile, two structural improvements are proposed. First, inspired by how
continuous VAEs enhance reconstruction via enlarged latent dimensions, we
introduce a multi-token quantization mechanism, which achieves nearly a 1 dB
improvement in PSNR without compromising the token compression ratio. Second,
to tackle reconstruction challenges in high-compression video VAEs, we
strengthen first-frame reconstruction, enabling the causal VAE to leverage this
information in subsequent frames and markedly improving the performance of 4 x
16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous
optimization scheme that unifies the two paradigms and, for the first time,
achieves competitive performance on both continuous and discrete
representations within a single network. We name our method OneVAE to reflect
this connection.

</details>


### [154] [HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics](https://arxiv.org/abs/2508.09858)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: HumanGenesis is a framework addressing geometric inconsistency and motion generalization in synthetic human dynamics by integrating geometric and generative modeling through four collaborative agents.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating photorealistic human motion videos struggle with geometric inconsistency, coarse reconstruction, motion generalization, and scene harmonization.

Method: HumanGenesis uses four agents: Reconstructor (3D modeling), Critique Agent (refinement), Pose Guider (motion generalization), and Video Harmonizer (rendering).

Result: The framework achieves state-of-the-art performance in text-guided synthesis, video reenactment, and novel-pose generalization.

Conclusion: HumanGenesis significantly improves expressiveness, geometric fidelity, and scene integration in synthetic human dynamics.

Abstract: \textbf{Synthetic human dynamics} aims to generate photorealistic videos of
human subjects performing expressive, intention-driven motions. However,
current approaches face two core challenges: (1) \emph{geometric inconsistency}
and \emph{coarse reconstruction}, due to limited 3D modeling and detail
preservation; and (2) \emph{motion generalization limitations} and \emph{scene
inharmonization}, stemming from weak generative capabilities. To address these,
we present \textbf{HumanGenesis}, a framework that integrates geometric and
generative modeling through four collaborative agents: (1)
\textbf{Reconstructor} builds 3D-consistent human-scene representations from
monocular video using 3D Gaussian Splatting and deformation decomposition. (2)
\textbf{Critique Agent} enhances reconstruction fidelity by identifying and
refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose
Guider} enables motion generalization by generating expressive pose sequences
using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes
photorealistic, coherent video via a hybrid rendering pipeline with diffusion,
refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis
achieves state-of-the-art performance on tasks including text-guided synthesis,
video reenactment, and novel-pose generalization, significantly improving
expressiveness, geometric fidelity, and scene integration.

</details>


### [155] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: E-4DGS is a novel event-driven dynamic Gaussian Splatting method for view synthesis from multi-view event streams, excelling in high-speed and low-light scenarios.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of RGB cameras (lighting dependence, motion blur, dynamic range) by leveraging event cameras' advantages (low power, high temporal resolution, high dynamic range).

Method: Introduces event-based initialization, event-adaptive slicing splatting, intensity importance pruning, and adaptive contrast threshold for precise optimization. Uses a synthetic multi-view event camera setup.

Result: Outperforms event-only and event-RGB fusion baselines, providing stable training and time-aware reconstruction.

Conclusion: E-4DGS advances multi-view event-based reconstruction, enabling rapid scene capture in challenging conditions.

Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on
RGB cameras, thereby inheriting inherent limitations such as the dependence on
adequate lighting, susceptibility to motion blur, and a limited dynamic range.
Event cameras, offering advantages of low power, high temporal resolution and
high dynamic range, have brought a new perspective to addressing the scene
reconstruction challenges in high-speed motion and low-light scenes. To this
end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting
approach, for novel view synthesis from multi-view event streams with
fast-moving cameras. Specifically, we introduce an event-based initialization
scheme to ensure stable training and propose event-adaptive slicing splatting
for time-aware reconstruction. Additionally, we employ intensity importance
pruning to eliminate floating artifacts and enhance 3D consistency, while
incorporating an adaptive contrast threshold for more precise optimization. We
design a synthetic multi-view camera setup with six moving event cameras
surrounding the object in a 360-degree configuration and provide a benchmark
multi-view event stream dataset that captures challenging motion scenarios. Our
approach outperforms both event-only and event-RGB fusion baselines and paves
the way for the exploration of multi-view event-based reconstruction as a novel
approach for rapid scene capture.

</details>


### [156] [SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection](https://arxiv.org/abs/2508.09913)
*Yachao Liang,Min Yu,Gang Li,Jianguo Jiang,Boquan Li,Feng Yu,Ning Zhang,Xiang Meng,Weiqing Huang*

Main category: cs.CV

TL;DR: The paper proposes a novel audio-visual speech representation learning method for detecting face forgery videos, achieving superior cross-dataset generalization and robustness without using fake videos in training.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in generalizing face forgery detection to unseen datasets and perturbations. Audio signals, rich in speech content, can reflect facial movements, providing a unique opportunity for detection.

Method: A self-supervised masked prediction task learns precise audio-visual speech representations from real videos, encoding local and global semantic information. The model is then transferred to forgery detection.

Result: The method outperforms state-of-the-art techniques in cross-dataset generalization and robustness.

Conclusion: The synergy between audio and visual speech elements offers an effective solution for face forgery detection, even without training on fake videos.

Abstract: Detection of face forgery videos remains a formidable challenge in the field
of digital forensics, especially the generalization to unseen datasets and
common perturbations. In this paper, we tackle this issue by leveraging the
synergy between audio and visual speech elements, embarking on a novel approach
through audio-visual speech representation learning. Our work is motivated by
the finding that audio signals, enriched with speech content, can provide
precise information effectively reflecting facial movements. To this end, we
first learn precise audio-visual speech representations on real videos via a
self-supervised masked prediction task, which encodes both local and global
semantic information simultaneously. Then, the derived model is directly
transferred to the forgery detection task. Extensive experiments demonstrate
that our method outperforms the state-of-the-art methods in terms of
cross-dataset generalization and robustness, without the participation of any
fake video in model training. Code is available at
https://github.com/Eleven4AI/SpeechForensics.

</details>


### [157] [Towards Comprehensive Cellular Characterisation of H&E slides](https://arxiv.org/abs/2508.09926)
*Benjamin Adjadj,Pierre-Antoine Bannier,Guillaume Horent,Sebastien Mandela,Aurore Lyon,Kathryn Schutte,Ulysse Marteau,Valentin Gaury,Laura Dumont,Thomas Mathieu,Reda Belbahri,Benoît Schmauch,Eric Durand,Katharina Von Loga,Lucie Gillet*

Main category: cs.CV

TL;DR: HistoPLUS is a state-of-the-art model for cell analysis in tumor microenvironments, outperforming existing methods in detection and classification while using fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing poor performance on understudied cell types and limited cross-domain generalization in existing methods.

Method: Trained on a curated pan-cancer dataset of 108,722 nuclei covering 13 cell types.

Result: Outperforms state-of-the-art models by 5.2% in detection quality and 23.7% in F1 classification score, with 5x fewer parameters. Enables study of 7 understudied cell types.

Conclusion: HistoPLUS advances TME biomarker research, with robust transferability to unseen oncology indications, and its resources are publicly released.

Abstract: Cell detection, segmentation and classification are essential for analyzing
tumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing
methods suffer from poor performance on understudied cell types (rare or not
present in public datasets) and limited cross-domain generalization. To address
these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell
analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei
covering 13 cell types. In external validation across 4 independent cohorts,
HistoPLUS outperforms current state-of-the-art models in detection quality by
5.2% and overall F1 classification score by 23.7%, while using 5x fewer
parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types
and brings significant improvements on 8 of 13 cell types. Moreover, we show
that HistoPLUS robustly transfers to two oncology indications unseen during
training. To support broader TME biomarker research, we release the model
weights and inference code at https://github.com/owkin/histoplus/.

</details>


### [158] [Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?](https://arxiv.org/abs/2508.09936)
*Vittorio Pippi,Konstantina Nikolaidou,Silvia Cascianelli,George Retsinas,Giorgos Sfikas,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: The paper evaluates three HTG models (GAN, diffusion, autoregressive) to improve HTR performance in low-resource settings, providing guidelines for model selection.


<details>
  <summary>Details</summary>
Motivation: Challenges in digitizing historical manuscripts due to small, author-specific collections and divergence from training data distributions.

Method: Systematic comparison of three HTG models (GAN, diffusion, autoregressive) and analysis of synthetic data's impact on HTR fine-tuning.

Result: Quantitative guidelines for selecting effective HTG models and insights into their current capabilities.

Conclusion: HTG methods show promise for low-resource HTR but require further improvement in key areas.

Abstract: The digitization of historical manuscripts presents significant challenges
for Handwritten Text Recognition (HTR) systems, particularly when dealing with
small, author-specific collections that diverge from the training data
distributions. Handwritten Text Generation (HTG) techniques, which generate
synthetic data tailored to specific handwriting styles, offer a promising
solution to address these challenges. However, the effectiveness of various HTG
models in enhancing HTR performance, especially in low-resource transcription
settings, has not been thoroughly evaluated. In this work, we systematically
compare three state-of-the-art styled HTG models (representing the generative
adversarial, diffusion, and autoregressive paradigms for HTG) to assess their
impact on HTR fine-tuning. We analyze how visual and linguistic characteristics
of synthetic data influence fine-tuning outcomes and provide quantitative
guidelines for selecting the most effective HTG model. The results of our
analysis provide insights into the current capabilities of HTG methods and
highlight key areas for further improvement in their application to
low-resource HTR.

</details>


### [159] [AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models](https://arxiv.org/abs/2508.09943)
*Tomás de la Sotta,José M. Saavedra,Héctor Henríquez,Violeta Chang,Aline Xavier*

Main category: cs.CV

TL;DR: AST-n accelerates LDCT denoising using diffusion models, reducing steps while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: LDCT reduces radiation but increases noise; diffusion models can improve image quality.

Method: AST-n framework uses intermediate noise levels and high-order ODE solvers for faster inference.

Result: AST-25 achieves PSNR >38 dB and SSIM >0.95, reducing time from ~16s to <1s per slice.

Conclusion: AST-n enables rapid, high-quality LDCT reconstruction, enhancing clinical feasibility.

Abstract: Low-dose CT (LDCT) protocols reduce radiation exposure but increase image
noise, compromising diagnostic confidence. Diffusion-based generative models
have shown promise for LDCT denoising by learning image priors and performing
iterative refinement. In this work, we introduce AST-n, an accelerated
inference framework that initiates reverse diffusion from intermediate noise
levels, and integrate high-order ODE solvers within conditioned models to
further reduce sampling steps. We evaluate two acceleration paradigms--AST-n
sampling and standard scheduling with high-order solvers -- on the Low Dose CT
Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %
of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak
signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)
above 0.95, closely matching standard baselines while cutting inference time
from ~16 seg to under 1 seg per slice. Unconditional sampling suffers
substantial quality loss, underscoring the necessity of conditioning. We also
assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling
inference time, limiting its clinical practicality. Our results demonstrate
that AST-n with high-order samplers enables rapid LDCT reconstruction without
significant loss of image fidelity, advancing the feasibility of
diffusion-based methods in clinical workflows.

</details>


### [160] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: Off-the-shelf Stable Diffusion models are repurposed for visual in-context learning (V-ICL) without fine-tuning, achieving competitive performance across six vision tasks.


<details>
  <summary>Details</summary>
Motivation: To simplify and generalize visual in-context learning by avoiding specialized training or additional data.

Method: In-place attention re-computation in Stable Diffusion's self-attention layers to incorporate context between queries and example prompts.

Result: Improves mIoU for foreground segmentation by 8.9% and 3.2% over Visual Prompting and IMProv, respectively, and adapts to six tasks.

Conclusion: Stable Diffusion can be effectively repurposed for V-ICL, demonstrating versatility and performance gains without fine-tuning.

Abstract: Large language models (LLM) in natural language processing (NLP) have
demonstrated great potential for in-context learning (ICL) -- the ability to
leverage a few sets of example prompts to adapt to various tasks without having
to explicitly update the model weights. ICL has recently been explored for
computer vision tasks with promising early outcomes. These approaches involve
specialized training and/or additional data that complicate the process and
limit its generalizability. In this work, we show that off-the-shelf Stable
Diffusion models can be repurposed for visual in-context learning (V-ICL).
Specifically, we formulate an in-place attention re-computation within the
self-attention layers of the Stable Diffusion architecture that explicitly
incorporates context between the query and example prompts. Without any
additional fine-tuning, we show that this repurposed Stable Diffusion model is
able to adapt to six different tasks: foreground segmentation, single object
detection, semantic segmentation, keypoint detection, edge detection, and
colorization. For example, the proposed approach improves the mean intersection
over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by
8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,
respectively. Additionally, we show that the proposed method is able to
effectively leverage multiple prompts through ensembling to infer the task
better and further improve the performance.

</details>


### [161] [LIA-X: Interpretable Latent Portrait Animator](https://arxiv.org/abs/2508.09959)
*Yaohui Wang,Di Yang,Xinyuan Chen,Francois Bremond,Yu Qiao,Antitza Dantcheva*

Main category: cs.CV

TL;DR: LIA-X is an interpretable portrait animator using a Sparse Motion Dictionary for fine-grained facial dynamics transfer, outperforming prior methods in reenactment tasks.


<details>
  <summary>Details</summary>
Motivation: To enable precise and interpretable control over facial dynamics transfer from a driving video to a source portrait, addressing limitations of previous 'warp-render' approaches.

Method: LIA-X employs an autoencoder with a Sparse Motion Dictionary, modeling motion transfer as linear navigation in latent space and supporting an 'edit-warp-render' strategy.

Result: Outperforms previous methods in self-reenactment and cross-reenactment tasks, with scalability demonstrated via a 1-billion-parameter model.

Conclusion: LIA-X offers interpretable, controllable facial dynamics transfer, enabling practical applications like fine-grained video editing and 3D-aware manipulation.

Abstract: We introduce LIA-X, a novel interpretable portrait animator designed to
transfer facial dynamics from a driving video to a source portrait with
fine-grained control. LIA-X is an autoencoder that models motion transfer as a
linear navigation of motion codes in latent space. Crucially, it incorporates a
novel Sparse Motion Dictionary that enables the model to disentangle facial
dynamics into interpretable factors. Deviating from previous 'warp-render'
approaches, the interpretability of the Sparse Motion Dictionary allows LIA-X
to support a highly controllable 'edit-warp-render' strategy, enabling precise
manipulation of fine-grained facial semantics in the source portrait. This
helps to narrow initial differences with the driving video in terms of pose and
expression. Moreover, we demonstrate the scalability of LIA-X by successfully
training a large-scale model with approximately 1 billion parameters on
extensive datasets. Experimental results show that our proposed method
outperforms previous approaches in both self-reenactment and cross-reenactment
tasks across several benchmarks. Additionally, the interpretable and
controllable nature of LIA-X supports practical applications such as
fine-grained, user-guided image and video editing, as well as 3D-aware portrait
video manipulation.

</details>


### [162] [January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis](https://arxiv.org/abs/2508.09966)
*Amir Hosseinian,Ashkan Dehghani Zahedani,Umer Mansoor,Noosheen Hashemi,Mark Woodward*

Main category: cs.CV

TL;DR: The paper introduces the January Food Benchmark (JFB), a dataset of 1,000 food images with validated annotations, a benchmarking framework, and baseline results showing a specialized model outperforming general-purpose ones by 12.1 points.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized evaluation methods and high-quality datasets hinders progress in AI for automated nutritional analysis.

Method: The authors present JFB, a benchmarking framework with robust metrics, and compare general-purpose VLMs with their specialized model.

Result: The specialized model achieves an Overall Score of 86.2, outperforming general-purpose models by 12.1 points.

Conclusion: This work provides a valuable dataset and framework to guide future research in automated nutritional analysis.

Abstract: Progress in AI for automated nutritional analysis is critically hampered by
the lack of standardized evaluation methodologies and high-quality, real-world
benchmark datasets. To address this, we introduce three primary contributions.
First, we present the January Food Benchmark (JFB), a publicly available
collection of 1,000 food images with human-validated annotations. Second, we
detail a comprehensive benchmarking framework, including robust metrics and a
novel, application-oriented overall score designed to assess model performance
holistically. Third, we provide baseline results from both general-purpose
Vision-Language Models (VLMs) and our own specialized model,
january/food-vision-v1. Our evaluation demonstrates that the specialized model
achieves an Overall Score of 86.2, a 12.1-point improvement over the
best-performing general-purpose configuration. This work offers the research
community a valuable new evaluation dataset and a rigorous framework to guide
and benchmark future developments in automated nutritional analysis.

</details>


### [163] [MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2508.09967)
*Tianqi Xiang,Yi Li,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper introduces a Meta-Optimized Classifier (MOC) to improve few-shot learning for WSI classification, outperforming existing methods with significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Current VLFM-based methods for WSI classification underperform compared to MIL approaches, especially in data-scarce scenarios, prompting the need for better few-shot learning solutions.

Method: MOC combines a meta-learner to optimize classifier configurations from a diverse classifier bank, enhancing pathological interpretation.

Result: MOC achieves up to 26.25% improvement in AUC under 1-shot conditions, notably outperforming prior methods.

Conclusion: MOC offers a robust solution for clinical applications with limited training data, advancing few-shot WSI classification.

Abstract: Recent advances in histopathology vision-language foundation models (VLFMs)
have shown promise in addressing data scarcity for whole slide image (WSI)
classification via zero-shot adaptation. However, these methods remain
outperformed by conventional multiple instance learning (MIL) approaches
trained on large datasets, motivating recent efforts to enhance VLFM-based WSI
classification through fewshot learning paradigms. While existing few-shot
methods improve diagnostic accuracy with limited annotations, their reliance on
conventional classifier designs introduces critical vulnerabilities to data
scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)
comprising two core components: (1) a meta-learner that automatically optimizes
a classifier configuration from a mixture of candidate classifiers and (2) a
classifier bank housing diverse candidate classifiers to enable a holistic
pathological interpretation. Extensive experiments demonstrate that MOC
outperforms prior arts in multiple few-shot benchmarks. Notably, on the
TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art
few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,
offering a critical advancement for clinical deployments where diagnostic
training data is severely limited. Code is available at
https://github.com/xmed-lab/MOC.

</details>


### [164] [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/abs/2508.09973)
*Geonhee Sim,Gyeongsik Moon*

Main category: cs.CV

TL;DR: PERSONA combines 3D-based and diffusion-based methods to create personalized 3D human avatars from a single image, addressing identity preservation and pose-driven deformations.


<details>
  <summary>Details</summary>
Motivation: Existing methods either require costly pose-rich videos (3D-based) or struggle with identity preservation (diffusion-based). PERSONA aims to bridge this gap.

Method: PERSONA uses diffusion to generate pose-rich videos from a single image, then optimizes a 3D avatar with balanced sampling and geometry-weighted optimization.

Result: The framework achieves high authenticity and sharp renderings across diverse poses while preserving identity.

Conclusion: PERSONA successfully integrates the strengths of both approaches, enabling practical and high-quality avatar creation from minimal input.

Abstract: Two major approaches exist for creating animatable human avatars. The first,
a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a
single person, achieving personalization through a disentangled identity
representation. However, modeling pose-driven deformations, such as non-rigid
cloth deformations, requires numerous pose-rich videos, which are costly and
impractical to capture in daily life. The second, a diffusion-based approach,
learns pose-driven deformations from large-scale in-the-wild videos but
struggles with identity preservation and pose-dependent identity entanglement.
We present PERSONA, a framework that combines the strengths of both approaches
to obtain a personalized 3D human avatar with pose-driven deformations from a
single image. PERSONA leverages a diffusion-based approach to generate
pose-rich videos from the input image and optimizes a 3D avatar based on them.
To ensure high authenticity and sharp renderings across diverse poses, we
introduce balanced sampling and geometry-weighted optimization. Balanced
sampling oversamples the input image to mitigate identity shifts in
diffusion-generated training videos. Geometry-weighted optimization prioritizes
geometry constraints over image loss, preserving rendering quality in diverse
poses.

</details>


### [165] [A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation](https://arxiv.org/abs/2508.09977)
*Shuting He,Peilin Ji,Yitong Yang,Changshuo Wang,Jiayi Ji,Yinglin Wang,Henghui Ding*

Main category: cs.CV

TL;DR: 3D Gaussian Splatting (3DGS) is a high-fidelity, real-time alternative to NeRF for 3D scene representation, enabling diverse applications like segmentation, editing, and generation. This survey reviews 3DGS methods, datasets, and trends.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of 3DGS applications, highlighting its advantages over NeRF and its potential in geometric and semantic tasks.

Method: The survey categorizes 3DGS applications (segmentation, editing, generation), reviews methods, supervision strategies, and learning paradigms, and compares benchmarks.

Result: Summarizes representative methods, shared design principles, and emerging trends in 3DGS applications.

Conclusion: 3DGS is a versatile tool for 3D tasks, with ongoing research supported by a curated repository of resources.

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative
to Neural Radiance Fields (NeRF) for 3D scene representation, offering
high-fidelity photorealistic rendering with real-time performance. Beyond novel
view synthesis, the explicit and compact nature of 3DGS enables a wide range of
downstream applications that require geometric and semantic understanding. This
survey provides a comprehensive overview of recent progress in 3DGS
applications. It first introduces 2D foundation models that support semantic
understanding and control in 3DGS applications, followed by a review of
NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS
applications into segmentation, editing, generation, and other functional
tasks. For each, we summarize representative methods, supervision strategies,
and learning paradigms, highlighting shared design principles and emerging
trends. Commonly used datasets and evaluation protocols are also summarized,
along with comparative analyses of recent methods across public benchmarks. To
support ongoing research and development, a continually updated repository of
papers, code, and resources is maintained at
https://github.com/heshuting555/Awesome-3DGS-Applications.

</details>


### [166] [LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](https://arxiv.org/abs/2508.09981)
*Chengtao Lv,Bilang Zhang,Yang Yong,Ruihao Gong,Yushi Huang,Shiqiao Gu,Jiajun Wu,Yumeng Shi,Jinyang Guo,Wenya Wang*

Main category: cs.CV

TL;DR: LLMC+ is a benchmark for compressing Vision-Language Models (VLMs) that addresses gaps in current methods by offering a systematic toolkit for evaluating and combining compression techniques.


<details>
  <summary>Details</summary>
Motivation: Existing VLM compression methods lack fair evaluation, real-world task testing, and exploration of combined techniques.

Method: LLMC+ introduces a plug-and-play toolkit with over 20 algorithms across five VLM families, enabling systematic study of token-level and model-level compression.

Result: Findings show spatial and temporal redundancies need distinct strategies, token reduction degrades in complex tasks, and combined compression achieves extreme efficiency with minimal loss.

Conclusion: LLMC+ aims to standardize evaluation and inspire efficient VLM research, with code publicly available.

Abstract: Large Vision-Language Models (VLMs) exhibit impressive multi-modal
capabilities but suffer from prohibitive computational and memory demands, due
to their long visual token sequences and massive parameter sizes. To address
these issues, recent works have proposed training-free compression methods.
However, existing efforts often suffer from three major limitations: (1)
Current approaches do not decompose techniques into comparable modules,
hindering fair evaluation across spatial and temporal redundancy. (2)
Evaluation confined to simple single-turn tasks, failing to reflect performance
in realistic scenarios. (3) Isolated use of individual compression techniques,
without exploring their joint potential. To overcome these gaps, we introduce
LLMC+, a comprehensive VLM compression benchmark with a versatile,
plug-and-play toolkit. LLMC+ supports over 20 algorithms across five
representative VLM families and enables systematic study of token-level and
model-level compression. Our benchmark reveals that: (1) Spatial and temporal
redundancies demand distinct technical strategies. (2) Token reduction methods
degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)
Combining token and model compression achieves extreme compression with minimal
performance loss. We believe LLMC+ will facilitate fair evaluation and inspire
future research in efficient VLM. Our code is available at
https://github.com/ModelTC/LightCompress.

</details>


### [167] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: Story2Board is a training-free framework for generating expressive storyboards from natural language, improving coherence and diversity without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook key storytelling aspects like spatial composition and narrative pacing. Story2Board aims to address these gaps.

Method: Uses Latent Panel Anchoring for character consistency and Reciprocal Attention Value Mixing for feature blending. Leverages off-the-shelf language models for prompt generation.

Result: Outperforms baselines in producing dynamic, coherent, and engaging storyboards, validated by qualitative, quantitative, and user study results.

Conclusion: Story2Board advances storyboard generation by enhancing visual diversity and narrative coherence without requiring fine-tuning.

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [168] [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224)
*Yuan Yuan,Tina Sriskandarajah,Anna-Luisa Brakman,Alec Helyar,Alex Beutel,Andrea Vallone,Saachi Jain*

Main category: cs.CY

TL;DR: The paper proposes 'safe-completions' as an alternative to binary refusal boundaries in LLMs, focusing on output safety rather than intent classification, improving safety and helpfulness.


<details>
  <summary>Details</summary>
Motivation: Binary refusal boundaries in LLMs like ChatGPT are brittle for obscured intents and ill-suited for dual-use cases, necessitating a more nuanced safety approach.

Method: Introduces safe-completions, a training method prioritizing output safety over binary intent classification, implemented in GPT-5.

Result: Safe-completion training enhances safety (especially for dual-use prompts), reduces failure severity, and increases model helpfulness.

Conclusion: Safe-completions offer a more effective safety-training approach for LLMs, balancing safety and helpfulness better than binary refusal boundaries.

Abstract: Large Language Models used in ChatGPT have traditionally been trained to
learn a refusal boundary: depending on the user's intent, the model is taught
to either fully comply or outright refuse. While this is a strong mitigation
for explicitly malicious prompts, focusing safety training on refusals can lead
to brittleness for prompts with obscured user intent. Binary refusal boundaries
are especially ill-suited for dual-use cases (such as biology or
cybersecurity), where a user request can be answered safely at a high level,
but in some cases can lead to malicious uplift if sufficiently detailed or
actionable. As an alternative, we propose safe-completions: a safety-training
approach that centers on the safety of the assistant's output, rather than a
binary classification of the user's intent. Safe-completions seek to maximize
helpfulness within the safety policy's constraints. We incorporated this
approach into GPT-5 and find that across both production comparisons and
internally controlled experiments, safe-completion training improves safety
(especially on dual-use prompts), reduces the severity of residual safety
failures, and substantially increases model helpfulness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [169] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: The study analyzes ChatGPT's argumentative texts on ethical topics, finding coherent structure but limited persuasive impact, with ethical concerns often persisting.


<details>
  <summary>Details</summary>
Motivation: To understand how AI-generated arguments on nuanced ethical topics influence human opinion and perception.

Method: User study with 62 participants, pre-post surveys, and linguistic/rhetorical analysis of ChatGPT's texts.

Result: ChatGPT constructs coherent arguments but has limited persuasive power; ethical concerns often remain or grow.

Conclusion: AI-generated persuasion in ethical domains is constrained, offering insights for future research.

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [170] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: The paper examines gender-based biases in stories generated by AI models (ChatGPT, Gemini, Claude) using Propp's and Freytag's frameworks, revealing implicit biases through close reading.


<details>
  <summary>Details</summary>
Motivation: To uncover and analyze gender biases in AI-generated narratives, emphasizing the need for interpretative bias assessment.

Method: Close reading of stories generated by AI models, focusing on prompt adherence, gender distribution, descriptions, actions, and plot.

Result: Persistent implicit biases in AI-generated stories were identified.

Conclusion: Highlights the importance of multi-level bias assessment in AI narratives using interpretative methods.

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [171] [Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions](https://arxiv.org/abs/2508.09852)
*Baihan Lin*

Main category: q-bio.NC

TL;DR: The paper introduces the Perceptual Reality Transformer, a framework using neural architectures to simulate neurological perception conditions, aiding medical education and empathy training.


<details>
  <summary>Details</summary>
Motivation: To bridge the experiential gap between individuals with neurological conditions and their caregivers by simulating perceptual states.

Method: Employs six neural architectures to map natural images to condition-specific perceptual states, evaluated on ImageNet and CIFAR-10 datasets.

Result: Vision Transformers outperform CNNs and generative methods, establishing a benchmark for neurological perception simulation.

Conclusion: The framework advances understanding of neural networks in modeling atypical perception and has practical applications in education and technology.

Abstract: Neurological conditions affecting visual perception create profound
experiential divides between affected individuals and their caregivers,
families, and medical professionals. We present the Perceptual Reality
Transformer, a comprehensive framework employing six distinct neural
architectures to simulate eight neurological perception conditions with
scientifically-grounded visual transformations. Our system learns mappings from
natural images to condition-specific perceptual states, enabling others to
experience approximations of simultanagnosia, prosopagnosia, ADHD attention
deficits, visual agnosia, depression-related changes, anxiety tunnel vision,
and Alzheimer's memory effects. Through systematic evaluation across ImageNet
and CIFAR-10 datasets, we demonstrate that Vision Transformer architectures
achieve optimal performance, outperforming traditional CNN and generative
approaches. Our work establishes the first systematic benchmark for
neurological perception simulation, contributes novel condition-specific
perturbation functions grounded in clinical literature, and provides
quantitative metrics for evaluating simulation fidelity. The framework has
immediate applications in medical education, empathy training, and assistive
technology development, while advancing our fundamental understanding of how
neural networks can model atypical human perception.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [172] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN is a framework for fine-grained noise suppression in multimodal sentiment analysis, dynamically adjusting denoising strength per modality block. MoLAN+ outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches treat entire modalities as units, risking loss of critical information while suppressing noise.

Method: MoLAN divides modality features into blocks, dynamically assigning denoising strength based on noise level and relevance. MoLAN+ builds on this framework.

Result: MoLAN+ achieves state-of-the-art performance across five models and four datasets.

Conclusion: MoLAN offers a flexible, unified solution for noise suppression in multimodal sentiment analysis, with MoLAN+ demonstrating superior results.

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [173] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: NeuronTune is a fine-grained framework for optimizing safety and utility in LLMs by dynamically modulating sparse neurons, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current techniques for safety alignment in LLMs suffer from robustness issues, frequent benign query refusals, and utility degradation.

Method: NeuronTune identifies safety-critical and utility-preserving neurons via attribution and uses meta-learning to adaptively adjust their activations.

Result: NeuronTune achieves superior safety and maintains excellent utility, outperforming state-of-the-art methods.

Conclusion: Fine-grained neuron-level intervention, as in NeuronTune, resolves the limitations of coarse-grained methods for LLM safety and utility.

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [174] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG is a framework for adaptive missing representation learning in ECGs with varying layouts, improving arrhythmia diagnosis robustness.


<details>
  <summary>Details</summary>
Motivation: ECG layouts vary across hospitals, causing signal issues like asynchronous lead time and blackout loss, challenging existing models.

Method: PatchECG uses a masking training strategy to focus on key patches with lead dependencies, tested on PTB-XL and synthetic datasets.

Result: Achieved AUROC of 0.835 on PTB-XL, 0.778 on real hospital data, and 0.893 on 12x1 layouts, outperforming baselines.

Conclusion: PatchECG enhances ECG diagnosis robustness across layouts, surpassing existing methods like ECGFounder.

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [175] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: SVG-1M dataset and SVGen model enable efficient, accurate SVG generation from natural language, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of time-consuming SVG creation from creative ideas by leveraging natural language descriptions.

Method: Introduces SVG-1M dataset with text-SVG pairs, uses curriculum and reinforcement learning for SVGen model training.

Result: SVGen outperforms general large models and traditional rendering methods in effectiveness and efficiency.

Conclusion: SVG-1M and SVGen provide a scalable solution for precise SVG generation from text, with open-source availability.

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [176] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: A lightweight, training-free approach using Retrieval-Augmented Generation (RAG) bridges the modality gap between text and images in large multimodal models (LMMs) without costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address the modality gap in LMMs caused by misaligned text and visual representations, avoiding expensive fine-tuning and extensive data requirements.

Method: Utilizes RAG with a linear mapping to retrieve textual descriptions for images, generating new descriptions via a language model and iteratively refining the mapping.

Result: Significant improvements on benchmark datasets, demonstrating the effectiveness of the proposed approach.

Conclusion: The method offers a practical, cost-efficient solution to modality alignment in LMMs, enhancing performance without extensive fine-tuning.

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [177] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE is a quantization inference framework using Mixture-of-Experts to improve model performance by dynamically routing inputs to specialized quantization experts, reducing accuracy degradation without increasing latency.


<details>
  <summary>Details</summary>
Motivation: Quantization reduces model efficiency but often degrades accuracy. MoQE aims to mitigate this by leveraging multiple quantization experts.

Method: MoQE combines multiple quantization variants of a full-precision model as experts and uses lightweight, task-specific routers to dynamically route inputs.

Result: MoQE achieves performance comparable to SOTA quantization models on benchmarks like ImageNet and WikiText, without significant latency overhead.

Conclusion: MoQE effectively addresses quantization-induced accuracy degradation, offering a practical solution for efficient model deployment.

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [178] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: The paper proposes a Dynamic Connection Masking (DCM) mechanism for MLPs and KANs to improve robustness against noisy labels, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Noisy labels degrade model performance, and existing solutions focus on loss functions and sample selection, neglecting regularization in architecture.

Method: Introduces DCM to adaptively mask less important edges during training, reducing gradient error. Compatible with various noise-robust techniques.

Result: DCM consistently outperforms SOTA methods in experiments. KANs show superior noise robustness over MLPs.

Conclusion: DCM enhances classifier robustness against noisy labels and integrates well with existing methods. KANs are promising for noisy scenarios.

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [179] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: The paper proposes a Noise Hypernetwork to replace reward-guided test-time noise optimization in diffusion models, reducing computational overhead while preserving quality gains.


<details>
  <summary>Details</summary>
Motivation: The growing use of test-time scaling in LLMs and generative vision models increases computation time, making it impractical for many applications. The goal is to retain the benefits of this paradigm without the inference overhead.

Method: The authors introduce a Noise Hypernetwork to modulate initial input noise, learning a reward-tilted distribution through a tractable noise-space objective. This maintains fidelity to the base model while optimizing for desired characteristics.

Result: The approach recovers a significant portion of quality gains from explicit test-time optimization at a fraction of the computational cost.

Conclusion: The proposed method effectively integrates test-time scaling knowledge into models during post-training, offering a practical solution to the computational inefficiency of test-time scaling.

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [180] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: AI Blob! uses LLMs and semantic technologies to retrieve and recontextualize archival TV footage, enabling automated narrative construction.


<details>
  <summary>Details</summary>
Motivation: To explore semantic cataloging and LLMs for dynamic retrieval and reinterpretation of archival content, inspired by Italian TV programs.

Method: Integrates ASR, semantic embeddings, and RAG to process and query a dataset of 1,547 Italian TV videos, generating narrative montages from user prompts.

Result: Demonstrates content-aware retrieval and automated narrative construction, offering a framework and dataset for further research.

Conclusion: AI Blob! advances semantic archival engagement and contributes to media historiography and AI-driven archival research.

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [181] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: CIV is a security architecture for LLMs that uses cryptographic labels and a trust lattice to prevent prompt injection attacks, achieving 0% attack success while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to prompt injection and jailbreak attacks, and existing heuristic guardrails are ineffective.

Method: CIV attaches cryptographically signed provenance labels to tokens and enforces a source-trust lattice via a pre-softmax hard attention mask.

Result: CIV achieves 0% attack success rate, 93.1% token-level similarity, and no perplexity degradation on benign tasks.

Conclusion: CIV is a lightweight, effective solution for securing LLMs without fine-tuning, demonstrated on models like Llama-3-8B and Mistral-7B.

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [182] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: The paper analyzes privacy risks in KV-cache for LLMs, introduces three attack methods, and proposes KV-Cloak as a lightweight defense with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: KV-cache accelerates LLM inference but poses underexplored privacy risks, allowing attackers to reconstruct sensitive inputs.

Method: Three attack vectors (Inversion, Collision, Injection) are designed. KV-Cloak, a reversible matrix-based obfuscation with operator fusion, is proposed as defense.

Result: KV-Cloak effectively blocks attacks, reducing reconstruction to random noise, with no accuracy loss and minimal overhead.

Conclusion: KV-Cloak offers a practical, secure solution for trustworthy LLM deployment.

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


### [183] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: LoD is an unsupervised framework for detecting jailbreak attacks in LVLMs by treating them as anomalies, using MSCAV and a Safety Pattern Auto-Encoder, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: LVLMs are vulnerable to jailbreak attacks, and current detection methods rely on heuristic rules, leading to suboptimal performance.

Method: LoD uses Multi-modal Safety Concept Activation Vectors (MSCAV) and a Safety Pattern Auto-Encoder to model safe input distributions and detect anomalies.

Result: LoD achieves an average AUROC of 0.9951, improving by up to 38.89% over baselines.

Conclusion: LoD provides an accurate and unified solution for jailbreak detection in LVLMs without requiring attack labels.

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [184] [Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation](https://arxiv.org/abs/2508.09177)
*Xuanru Zhou,Cheng Li,Shuqiang Wang,Ye Li,Tao Tan,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: A review of generative AI's role in medical imaging, covering techniques like GANs, VAEs, and diffusion models, their clinical applications, and challenges like data scarcity and regulatory hurdles.


<details>
  <summary>Details</summary>
Motivation: To synthesize advances in generative AI for medical imaging and evaluate its clinical impact, addressing challenges like data scarcity and integration.

Method: Systematic examination of generative models (GANs, VAEs, diffusion models) across imaging workflow stages, proposing a three-tiered evaluation framework.

Result: Generative AI enhances imaging workflows but faces obstacles like domain shift, hallucination risks, and regulatory issues.

Conclusion: The review guides future research by highlighting progress and challenges, emphasizing interdisciplinary collaboration for scalable, clinically integrated AI systems.

Abstract: Generative artificial intelligence (AI) is rapidly transforming medical
imaging by enabling capabilities such as data synthesis, image enhancement,
modality translation, and spatiotemporal modeling. This review presents a
comprehensive and forward-looking synthesis of recent advances in generative
modeling including generative adversarial networks (GANs), variational
autoencoders (VAEs), diffusion models, and emerging multimodal foundation
architectures and evaluates their expanding roles across the clinical imaging
continuum. We systematically examine how generative AI contributes to key
stages of the imaging workflow, from acquisition and reconstruction to
cross-modality synthesis, diagnostic support, and treatment planning. Emphasis
is placed on both retrospective and prospective clinical scenarios, where
generative models help address longstanding challenges such as data scarcity,
standardization, and integration across modalities. To promote rigorous
benchmarking and translational readiness, we propose a three-tiered evaluation
framework encompassing pixel-level fidelity, feature-level realism, and
task-level clinical relevance. We also identify critical obstacles to
real-world deployment, including generalization under domain shift,
hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we
explore the convergence of generative AI with large-scale foundation models,
highlighting how this synergy may enable the next generation of scalable,
reliable, and clinically integrated imaging systems. By charting technical
progress and translational pathways, this review aims to guide future research
and foster interdisciplinary collaboration at the intersection of AI, medicine,
and biomedical engineering.

</details>


### [185] [HiFi-Mamba: Dual-Stream W-Laplacian Enhanced Mamba for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2508.09179)
*Hongli Chen,Pengcheng Fang,Yuxia Chen,Yingxuan Ren,Jing Hao,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: eess.IV

TL;DR: HiFi-Mamba, a dual-stream Mamba-based architecture, improves MRI reconstruction by addressing limitations of existing methods, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing Mamba variants for MRI reconstruction lack sensitivity to high-frequency details and rely on redundant scanning, limiting their effectiveness.

Method: HiFi-Mamba uses WL blocks for spectral decoupling and HiFi-Mamba blocks for adaptive state-space modulation, with unidirectional traversal for efficiency.

Result: HiFi-Mamba surpasses CNN-based, Transformer-based, and other Mamba-based models in accuracy while remaining compact and efficient.

Conclusion: HiFi-Mamba offers a superior solution for high-fidelity MRI reconstruction by combining spectral decoupling and efficient long-range modeling.

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data remains
a challenging problem in MRI. While Mamba variants for vision tasks offer
promising long-range modeling capabilities with linear-time complexity, their
direct application to MRI reconstruction inherits two key limitations: (1)
insensitivity to high-frequency anatomical details; and (2) reliance on
redundant multi-directional scanning. To address these limitations, we
introduce High-Fidelity Mamba (HiFi-Mamba), a novel dual-stream Mamba-based
architecture comprising stacked W-Laplacian (WL) and HiFi-Mamba blocks.
Specifically, the WL block performs fidelity-preserving spectral decoupling,
producing complementary low- and high-frequency streams. This separation
enables the HiFi-Mamba block to focus on low-frequency structures, enhancing
global feature modeling. Concurrently, the HiFi-Mamba block selectively
integrates high-frequency features through adaptive state-space modulation,
preserving comprehensive spectral details. To eliminate the scanning
redundancy, the HiFi-Mamba block adopts a streamlined unidirectional traversal
strategy that preserves long-range modeling capability with improved
computational efficiency. Extensive experiments on standard MRI reconstruction
benchmarks demonstrate that HiFi-Mamba consistently outperforms
state-of-the-art CNN-based, Transformer-based, and other Mamba-based models in
reconstruction accuracy while maintaining a compact and efficient model design.

</details>


### [186] [MedPatch: Confidence-Guided Multi-Stage Fusion for Multimodal Clinical Data](https://arxiv.org/abs/2508.09182)
*Baraa Al Jorf,Farah Shamout*

Main category: eess.IV

TL;DR: MedPatch is a multi-stage multimodal fusion architecture for clinical prediction tasks, addressing data heterogeneity and missing modalities via confidence-guided patching, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Real-world medical data is heterogeneous, limited, and sparse, hindering model performance in clinical prediction tasks.

Method: MedPatch integrates multi-stage fusion, missingness-aware handling, and joint fusion with calibrated unimodal confidence.

Result: Outperforms baselines in in-hospital mortality prediction and clinical condition classification.

Conclusion: Confidence-guided multi-stage fusion effectively addresses multimodal data heterogeneity, setting new benchmarks.

Abstract: Clinical decision-making relies on the integration of information across
various data modalities, such as clinical time-series, medical images and
textual reports. Compared to other domains, real-world medical data is
heterogeneous in nature, limited in size, and sparse due to missing modalities.
This significantly limits model performance in clinical prediction tasks.
Inspired by clinical workflows, we introduce MedPatch, a multi-stage multimodal
fusion architecture, which seamlessly integrates multiple modalities via
confidence-guided patching. MedPatch comprises three main components: (i) a
multi-stage fusion strategy that leverages joint and late fusion
simultaneously, (ii) a missingness-aware module that handles sparse samples
with missing modalities, (iii) a joint fusion module that clusters latent token
patches based on calibrated unimodal token-level confidence. We evaluated
MedPatch using real-world data consisting of clinical time-series data, chest
X-ray images, radiology reports, and discharge notes extracted from the
MIMIC-IV, MIMIC-CXR, and MIMIC-Notes datasets on two benchmark tasks, namely
in-hospital mortality prediction and clinical condition classification.
Compared to existing baselines, MedPatch achieves state-of-the-art performance.
Our work highlights the effectiveness of confidence-guided multi-stage fusion
in addressing the heterogeneity of multimodal data, and establishes new
state-of-the-art benchmark results for clinical prediction tasks.

</details>


### [187] [Hybrid(Transformer+CNN)-based Polyp Segmentation](https://arxiv.org/abs/2508.09189)
*Madan Baduwal*

Main category: eess.IV

TL;DR: A hybrid Transformer + CNN model improves polyp segmentation by addressing ill-defined margins and endoscopic artifacts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Polyp segmentation is challenging due to variations in size, shape, lighting, and imaging protocols, as well as ill-defined boundaries.

Method: A hybrid (Transformer + CNN) model with boundary-aware attention mechanisms for robust feature extraction.

Result: Significant improvements in segmentation accuracy (Recall: +1.76%, Accuracy: +0.07%) and artifact resilience.

Conclusion: The hybrid model enhances robustness and accuracy in polyp segmentation, outperforming state-of-the-art methods.

Abstract: Colonoscopy is still the main method of detection and segmentation of colonic
polyps, and recent advancements in deep learning networks such as U-Net,
ResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp
segmentation. Yet, the problem is extremely challenging due to high variation
in size, shape, endoscopy types, lighting, imaging protocols, and ill-defined
boundaries (fluid, folds) of the polyps, rendering accurate segmentation a
challenging and problematic task. To address these critical challenges in polyp
segmentation, we introduce a hybrid (Transformer + CNN) model that is crafted
to enhance robustness against evolving polyp characteristics. Our hybrid
architecture demonstrates superior performance over existing solutions,
particularly in addressing two critical challenges: (1) accurate segmentation
of polyps with ill-defined margins through boundary-aware attention mechanisms,
and (2) robust feature extraction in the presence of common endoscopic
artifacts, including specular highlights, motion blur, and fluid occlusions.
Quantitative evaluations reveal significant improvements in segmentation
accuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,
i.e., 0.9849) and artifact resilience compared to state-of-the-art polyp
segmentation methods.

</details>


### [188] [impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction](https://arxiv.org/abs/2508.09195)
*Maria Boyko,Aleksandra Beliaeva,Dmitriy Kornilov,Alexander Bernstein,Maxim Sharaev*

Main category: eess.IV

TL;DR: impuTMAE is a transformer-based model for multimodal medical data, handling missing modalities and improving glioma survival prediction.


<details>
  <summary>Details</summary>
Motivation: Medical data is complex and often incomplete; integrating diverse modalities can enhance prognostic models and disease understanding.

Method: impuTMAE uses a transformer-based approach with multimodal pre-training, imputing missing data by reconstructing masked patches.

Result: The model outperforms prior methods, achieving state-of-the-art performance in glioma survival prediction.

Conclusion: impuTMAE effectively integrates and imputes multimodal data, advancing prognostic modeling in medicine.

Abstract: The use of diverse modalities, such as omics, medical images, and clinical
data can not only improve the performance of prognostic models but also deepen
an understanding of disease mechanisms and facilitate the development of novel
treatment approaches. However, medical data are complex, often incomplete, and
contains missing modalities, making effective handling its crucial for training
multimodal models. We introduce impuTMAE, a novel transformer-based end-to-end
approach with an efficient multimodal pre-training strategy. It learns inter-
and intra-modal interactions while simultaneously imputing missing modalities
by reconstructing masked patches. Our model is pre-trained on heterogeneous,
incomplete data and fine-tuned for glioma survival prediction using
TCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,
RNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data
during pre-training and enabling efficient resource utilization, impuTMAE
surpasses prior multimodal approaches, achieving state-of-the-art performance
in glioma patient survival prediction. Our code is available at
https://github.com/maryjis/mtcp

</details>


### [189] [FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation](https://arxiv.org/abs/2508.09196)
*Asim Ukaye,Numan Saeed,Karthik Nandakumar*

Main category: eess.IV

TL;DR: A federated learning approach for universal segmentation in diverse abdominal CT datasets, using model and predictive uncertainty for aggregation and inference, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous CT datasets from different scanners and settings pose challenges for effective segmentation while preserving privacy.

Method: Leverages stochastic mini-batch gradient descent noise to estimate model weight uncertainty, aggregates parameters with Bayesian-inspired inverse-variance, and uses predictive uncertainty for inference.

Result: Improves federated aggregation quality and uncertainty-weighted inference compared to baselines.

Conclusion: The approach effectively addresses segmentation challenges in heterogeneous datasets, enhancing clinical decision-making with uncertainty measures.

Abstract: Different CT segmentation datasets are typically obtained from different
scanners under different capture settings and often provide segmentation labels
for a limited and often disjoint set of organs. Using these heterogeneous data
effectively while preserving patient privacy can be challenging. This work
presents a novel federated learning approach to achieve universal segmentation
across diverse abdominal CT datasets by utilizing model uncertainty for
aggregation and predictive uncertainty for inference. Our approach leverages
the inherent noise in stochastic mini-batch gradient descent to estimate a
distribution over the model weights to provide an on-the-go uncertainty over
the model parameters at the client level. The parameters are then aggregated at
the server using the additional uncertainty information using a
Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the
proposed method quantifies prediction uncertainty by propagating the
uncertainty from the model weights, providing confidence measures essential for
clinical decision-making. In line with recent work shown, predictive
uncertainty is utilized in the inference stage to improve predictive
performance. Experimental evaluations demonstrate the effectiveness of this
approach in improving both the quality of federated aggregation and
uncertainty-weighted inference compared to previously established baselines.
The code for this work is made available at: https://github.com/asimukaye/fiva

</details>


### [190] [Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction](https://arxiv.org/abs/2508.09200)
*Jinho Kim,Marcel Dominik Nickel,Florian Knoll*

Main category: eess.IV

TL;DR: Zero-shot self-supervised learning reduces breath-hold times in MRCP, achieving quality comparable to respiratory-triggered MRCP, with shallow training cutting computation time.


<details>
  <summary>Details</summary>
Motivation: To reduce breath-hold durations in MRCP while maintaining image quality, addressing the impracticality of long breath-holds in clinical settings.

Method: Evaluated zero-shot reconstruction against parallel imaging and compressed sensing, using a pretrained network to reduce training time.

Result: Zero-shot improved image quality over compressed sensing, matching respiratory-triggered MRCP, with shallow training reducing training time from 271 to 11 minutes.

Conclusion: Zero-shot learning enables high-quality MRCP with shorter breath-holds, and shallow training makes it feasible for clinical use.

Abstract: Purpose: To investigate the feasibility of applying zero-shot self-supervised
learning reconstruction to reduce breath-hold times in magnetic resonance
cholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11
healthy volunteers on a 3T scanner using an incoherent k-space sampling pattern
leading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction
of breath-hold MRCP against parallel imaging of respiratory-triggered MRCP
acquired in 338s on average and compressed sensing reconstruction of
breath-hold MRCP. To address the long computation times of zero-shot trainings,
we used a training approach that leverages a pretrained network to reduce
backpropagation depth during training. Results: Zero-shot learning
reconstruction significantly improved visual image quality compared to
compressed sensing reconstruction, particularly in terms of signal-to-noise
ratio and ductal delineation, and reached a level of quality comparable to that
of successful respiratory-triggered acquisitions with regular breathing
patterns. Shallow training provided nearly equivalent reconstruction
performance with a training time of 11 minutes in comparison to 271 minutes for
a conventional zero-shot training. Conclusion: Zero-shot learning delivers
high-fidelity MRCP reconstructions with reduced breath-hold times, and shallow
training offers a practical solution for translation to time-constrained
clinical workflows.

</details>


### [191] [From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations](https://arxiv.org/abs/2508.09205)
*Yoni Schirris,Eric Marcus,Jonas Teuwen,Hugo Horlings,Efstratios Gavves*

Main category: eess.IV

TL;DR: The paper proposes a human-machine-VLM interaction system to explain deep learning models in computational pathology, using AI-integrated tools and vision-language models to test and quantify explanations.


<details>
  <summary>Details</summary>
Motivation: To ensure clinical integration of medical image analysis systems by providing reliable explanations that highlight model dependencies on features, avoiding spurious ones and potentially revealing biological insights.

Method: The system includes (1) an AI-integrated slide viewer for sliding-window experiments to test explanation claims, and (2) quantification of explanation predictiveness using vision-language models.

Result: The approach allows qualitative testing of explanation claims and quantifiably distinguishes competing explanations.

Conclusion: The system offers a practical path from explainable AI to explained AI in digital pathology, enhancing model trust and clinical utility.

Abstract: Explaining deep learning models is essential for clinical integration of
medical image analysis systems. A good explanation highlights if a model
depends on spurious features that undermines generalization and harms a subset
of patients or, conversely, may present novel biological insights. Although
techniques like GradCAM can identify influential features, they are measurement
tools that do not themselves form an explanation. We propose a
human-machine-VLM interaction system tailored to explaining classifiers in
computational pathology, including multi-instance learning for whole-slide
images. Our proof of concept comprises (1) an AI-integrated slide viewer to run
sliding-window experiments to test claims of an explanation, and (2)
quantification of an explanation's predictiveness using general-purpose
vision-language models. The results demonstrate that this allows us to
qualitatively test claims of explanations and can quantifiably distinguish
competing explanations. This offers a practical path from explainable AI to
explained AI in digital pathology and beyond. Code and prompts are available at
https://github.com/nki-ai/x2x.

</details>


### [192] [AMRG: Extend Vision Language Models for Automatic Mammography Report Generation](https://arxiv.org/abs/2508.09225)
*Nak-Jun Sung,Donghyun Lee,Bo Hwa Choi,Chae Jung Park*

Main category: eess.IV

TL;DR: AMRG is the first end-to-end framework for generating mammography reports using large vision-language models, achieving strong performance in language and clinical metrics.


<details>
  <summary>Details</summary>
Motivation: Mammography report generation is underexplored in medical AI, with challenges like multiview reasoning and unstructured language.

Method: Uses MedGemma-4B-it, a domain-specialized VLM, with parameter-efficient fine-tuning (LoRA). Trained on DMID dataset.

Result: Achieves ROUGE-L 0.5691, METEOR 0.6152, CIDEr 0.5818, BI-RADS accuracy 0.5582. Improved diagnostic consistency.

Conclusion: AMRG provides a scalable foundation for radiology report generation and advances multimodal medical AI research.

Abstract: Mammography report generation is a critical yet underexplored task in medical
AI, characterized by challenges such as multiview image reasoning,
high-resolution visual cues, and unstructured radiologic language. In this
work, we introduce AMRG (Automatic Mammography Report Generation), the first
end-to-end framework for generating narrative mammography reports using large
vision-language models (VLMs). Building upon MedGemma-4B-it-a
domain-specialized, instruction-tuned VLM-we employ a parameter-efficient
fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling
lightweight adaptation with minimal computational overhead. We train and
evaluate AMRG on DMID, a publicly available dataset of paired high-resolution
mammograms and diagnostic reports. This work establishes the first reproducible
benchmark for mammography report generation, addressing a longstanding gap in
multimodal clinical AI. We systematically explore LoRA hyperparameter
configurations and conduct comparative experiments across multiple VLM
backbones, including both domain-specific and general-purpose models under a
unified tuning protocol. Our framework demonstrates strong performance across
both language generation and clinical metrics, achieving a ROUGE-L score of
0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.
Qualitative analysis further highlights improved diagnostic consistency and
reduced hallucinations. AMRG offers a scalable and adaptable foundation for
radiology report generation and paves the way for future research in multimodal
medical AI.

</details>


### [193] [Dynamic Survival Prediction using Longitudinal Images based on Transformer](https://arxiv.org/abs/2508.09328)
*Bingfan Liu,Haolun Shi,Jiguo Cao*

Main category: eess.IV

TL;DR: SurLonFormer, a Transformer-based model, improves survival analysis by integrating longitudinal medical images and structured data, addressing censored data, scalability, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to fully utilize censored data, ignore correlations in longitudinal images, and lack interpretability.

Method: SurLonFormer combines a Vision Encoder, Sequence Encoder, and Survival Encoder (Cox model) for spatial-temporal feature extraction and survival prediction.

Result: Outperforms existing methods in predictive performance and identifies disease-related biomarkers in Alzheimer's analysis.

Conclusion: SurLonFormer offers a scalable, interpretable solution for survival analysis with longitudinal medical images.

Abstract: Survival analysis utilizing multiple longitudinal medical images plays a
pivotal role in the early detection and prognosis of diseases by providing
insight beyond single-image evaluations. However, current methodologies often
inadequately utilize censored data, overlook correlations among longitudinal
images measured over multiple time points, and lack interpretability. We
introduce SurLonFormer, a novel Transformer-based neural network that
integrates longitudinal medical imaging with structured data for survival
prediction. Our architecture comprises three key components: a Vision Encoder
for extracting spatial features, a Sequence Encoder for aggregating temporal
information, and a Survival Encoder based on the Cox proportional hazards
model. This framework effectively incorporates censored data, addresses
scalability issues, and enhances interpretability through occlusion sensitivity
analysis and dynamic survival prediction. Extensive simulations and a
real-world application in Alzheimer's disease analysis demonstrate that
SurLonFormer achieves superior predictive performance and successfully
identifies disease-related imaging biomarkers.

</details>


### [194] [T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis](https://arxiv.org/abs/2508.09919)
*Xiaojiao Xiao,Jianfeng Zhao,Qinmin Vivian Hu,Guanghui Wang*

Main category: eess.IV

TL;DR: T-CACE synthesizes multi-phase contrast-enhanced MRI from non-contrast MRI, improving safety and diagnostic efficiency for liver cancer.


<details>
  <summary>Details</summary>
Motivation: Address challenges of traditional MRI like contrast agent risks, manual assessment time, and limited datasets.

Method: Uses conditional token encoding, dynamic time-aware attention mask, and temporal classification consistency for synthesis.

Result: Outperforms state-of-the-art methods in image synthesis, segmentation, and lesion classification.

Conclusion: T-CACE is a clinically efficient and safer alternative to traditional contrast-enhanced MRI.

Abstract: Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of
liver cancer, significantly improving the classification of the lesion and
patient outcomes. However, traditional MRI faces challenges including risks
from contrast agent (CA) administration, time-consuming manual assessment, and
limited annotated datasets. To address these limitations, we propose a
Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for
synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from
non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a
conditional token encoding (CTE) mechanism that unifies anatomical priors and
temporal phase information into latent representations; and a dynamic
time-aware attention mask (DTAM) that adaptively modulates inter-phase
information flow using a Gaussian-decayed attention mechanism, ensuring smooth
and physiologically plausible transitions across phases. Furthermore, a
constraint for temporal classification consistency (TCC) aligns the lesion
classification output with the evolution of the physiological signal, further
enhancing diagnostic reliability. Extensive experiments on two independent
liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods
in image synthesis, segmentation, and lesion classification. This framework
offers a clinically relevant and efficient alternative to traditional
contrast-enhanced imaging, improving safety, diagnostic efficiency, and
reliability for the assessment of liver lesion. The implementation of T-CACE is
publicly available at: https://github.com/xiaojiao929/T-CACE.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [195] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: A framework uses MLLMs to generate rich video descriptions for better recommendations, outperforming traditional features.


<details>
  <summary>Details</summary>
Motivation: Traditional video recommender systems miss deeper semantics like intent and humour, which are critical for personalised recommendations.

Method: A zero-finetuning framework prompts an MLLM to summarise clips into natural-language descriptions, combined with a text encoder for standard recommenders.

Result: Outperforms conventional features on the MicroLens-100K dataset across five models.

Conclusion: MLLMs can effectively extract high-level semantics to improve video recommender systems.

Abstract: Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [196] [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
*Xi Xuan,Zimo Zhu,Wenxin Zhang,Yi-Cheng Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: Fake-Mamba uses bidirectional Mamba with XLSR for real-time deepfake speech detection, outperforming SOTA models with efficient encoders.


<details>
  <summary>Details</summary>
Motivation: Addressing security threats from advanced speech synthesis by improving deepfake detection.

Method: Integrates XLSR front-end with bidirectional Mamba and introduces three efficient encoders (TransBiMamba, ConBiMamba, PN-BiMamba).

Result: Achieves 0.97%, 1.74%, and 5.85% EER on ASVspoof 21 LA, 21 DF, and In-The-Wild benchmarks, outperforming XLSR-Conformer and XLSR-Mamba.

Conclusion: Fake-Mamba is effective, efficient, and practical for real-time deepfake speech detection.

Abstract: Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

</details>


### [197] [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
*Eray Eren,Qingju Liu,Hyeongwoo Kim,Pablo Garrido,Abeer Alwan*

Main category: eess.AS

TL;DR: A stand-alone model (ProMode) maps text-to-prosodic features (F0, energy) for downstream tasks like TTS, outperforming baselines in F0/energy prediction and perceptual tests.


<details>
  <summary>Details</summary>
Motivation: Prosody conveys emotional, semantic, and individual speech traits, but existing methods lack granularity. A dedicated model can improve prosody modeling for tasks like TTS.

Method: ProMode encoder uses masked acoustic/text inputs to create fixed-length prosodic embeddings. The decoder predicts masked acoustics using prosody and unmasked text. Trained on GigaSpeech.

Result: ProMode improves F0/energy predictions at various granularities and outperforms baselines in perceptual TTS tests, showing higher prosody preference.

Conclusion: ProMode effectively models prosody, enhancing TTS performance and demonstrating potential for prosody-critical applications.

Abstract: Prosody conveys rich emotional and semantic information of the speech signal
as well as individual idiosyncrasies. We propose a stand-alone model that maps
text-to-prosodic features such as F0 and energy and can be used in downstream
tasks such as TTS. The ProMode encoder takes as input acoustic features and
time-aligned textual content, both are partially masked, and obtains a
fixed-length latent prosodic embedding. The decoder predicts acoustics in the
masked region using both the encoded prosody input and unmasked textual
content. Trained on the GigaSpeech dataset, we compare our method with
state-of-the-art style encoders. For F0 and energy predictions, we show
consistent improvements for our model at different levels of granularity. We
also integrate these predicted prosodic features into a TTS system and conduct
perceptual tests, which show higher prosody preference compared to the
baselines, demonstrating the model's potential in tasks where prosody modeling
is important.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [198] [Robustness analysis of Deep Sky Objects detection models on HPC](https://arxiv.org/abs/2508.09831)
*Olivier Parisot,Diogo Ramalho Fernandes*

Main category: astro-ph.IM

TL;DR: Automated detection of Deep Sky Objects in astronomical images using YOLO and RET-DETR models, leveraging HPC for robustness.


<details>
  <summary>Details</summary>
Motivation: The surge in sky images from surveys and amateur astronomers demands accurate, automated methods for detecting faint and complex Deep Sky Objects.

Method: Training and comparing YOLO and RET-DETR models on smart telescope images, utilizing HPC for parallel computations and robustness testing.

Result: Improved detection of Deep Sky Objects through advanced Computer Vision and Deep Learning techniques.

Conclusion: The study demonstrates the feasibility of automating Deep Sky Object detection with robust, scalable models.

Abstract: Astronomical surveys and the growing involvement of amateur astronomers are
producing more sky images than ever before, and this calls for automated
processing methods that are accurate and robust. Detecting Deep Sky Objects --
such as galaxies, nebulae, and star clusters -- remains challenging because of
their faint signals and complex backgrounds. Advances in Computer Vision and
Deep Learning now make it possible to improve and automate this process. In
this paper, we present the training and comparison of different detection
models (YOLO, RET-DETR) on smart telescope images, using High-Performance
Computing (HPC) to parallelise computations, in particular for robustness
testing.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [199] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: NEFMind uses parameter-efficient fine-tuning of LLMs to simplify service discovery and management in telecom networks, reducing overhead by 85% and achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: The rise of Service-Based Architecture in telecom has increased operational complexities in service discovery and management due to growing NFs and APIs.

Method: NEFMind integrates synthetic dataset generation, model optimization via Quantized-Low-Rank Adaptation, and performance evaluation using GPT-4 Ref Score and BertScore metrics.

Result: The approach reduces communication overhead by 85% and achieves 98-100% accuracy in API call identification using the Phi-2 model.

Conclusion: Parameter-efficient LLM strategies are effective for managing complex API ecosystems in next-gen telecom networks.

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [200] [Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics](https://arxiv.org/abs/2508.09215)
*Kerem Delikoyun,Qianyu Chen,Liu Wei,Si Ko Myo,Johannes Krell,Martin Schlegel,Win Sen Kuan,John Tshon Yit Soong,Gerhard Schneider,Clarissa Prazeres da Costa,Percy A. Knolle,Laurent Renia,Matthew Edward Cove,Hwee Kuan Lee,Klaus Diepold,Oliver Hayden*

Main category: q-bio.QM

TL;DR: RT-HAD is a deep learning framework for real-time detection of blood cell aggregates using digital holographic microscopy, addressing data storage and processing challenges in clinical haematology.


<details>
  <summary>Details</summary>
Motivation: Current methods for analyzing rare blood cell aggregates are inefficient, requiring manual reviews or offline processing, which hampers clinical use.

Method: RT-HAD combines physics-consistent holographic reconstruction and graph-based representation of blood cells to detect aggregates, processing large image data on-the-fly.

Result: RT-HAD achieves a turnaround time of <1.5 min and an 8.9% error rate in platelet aggregate detection, matching clinical standards.

Conclusion: RT-HAD enables efficient, real-time diagnostics for blood cell aggregates, solving big data challenges in point-of-care settings.

Abstract: While analysing rare blood cell aggregates remains challenging in automated
haematology, they could markedly advance label-free functional diagnostics.
Conventional flow cytometers efficiently perform cell counting with leukocyte
differentials but fail to identify aggregates with flagged results, requiring
manual reviews. Quantitative phase imaging flow cytometry captures detailed
aggregate morphologies, but clinical use is hampered by massive data storage
and offline processing. Incorporating hidden biomarkers into routine
haematology panels would significantly improve diagnostics without flagged
results. We present RT-HAD, an end-to-end deep learning-based image and data
processing framework for off-axis digital holographic microscopy (DHM), which
combines physics-consistent holographic reconstruction and detection,
representing each blood cell in a graph to recognize aggregates. RT-HAD
processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and
error rate of 8.9% in platelet aggregate detection, which matches acceptable
laboratory error rates of haematology biomarkers and solves the big data
challenge for point-of-care diagnostics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [201] [DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation](https://arxiv.org/abs/2508.09444)
*Haoxiang Shi,Xiang Deng,Zaijing Li,Gongwei Chen,Yaowei Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: DifNav unifies waypoint generation and planning into a single diffusion policy for VLN-CE, outperforming two-stage methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of two-stage frameworks in VLN-CE, such as global sub-optimization and reliance on waypoint quality.

Method: Introduces DAgger Diffusion Navigation (DifNav), an end-to-end diffusion policy modeling multi-modal action distributions, trained with DAgger for robustness.

Result: Outperforms state-of-the-art two-stage models on benchmark datasets.

Conclusion: DifNav eliminates the need for waypoint predictors, improving navigation performance and robustness.

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires
agents to follow natural language instructions through free-form 3D spaces.
Existing VLN-CE approaches typically use a two-stage waypoint planning
framework, where a high-level waypoint predictor generates the navigable
waypoints, and then a navigation planner suggests the intermediate goals in the
high-level action space. However, this two-stage decomposition framework
suffers from: (1) global sub-optimization due to the proxy objective in each
stage, and (2) a performance bottleneck caused by the strong reliance on the
quality of the first-stage predicted waypoints. To address these limitations,
we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE
policy that unifies the traditional two stages, i.e. waypoint generation and
planning, into a single diffusion policy. Notably, DifNav employs a conditional
diffusion policy to directly model multi-modal action distributions over future
actions in continuous navigation space, eliminating the need for a waypoint
predictor while enabling the agent to capture multiple possible
instruction-following behaviors. To address the issues of compounding error in
imitation learning and enhance spatial reasoning in long-horizon navigation
tasks, we employ DAgger for online policy training and expert trajectory
augmentation, and use the aggregated data to further fine-tune the policy. This
approach significantly improves the policy's robustness and its ability to
recover from error states. Extensive experiments on benchmark datasets
demonstrate that, even without a waypoint predictor, the proposed method
substantially outperforms previous state-of-the-art two-stage waypoint-based
models in terms of navigation performance. Our code is available at:
https://github.com/Tokishx/DifNav.

</details>


### [202] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: The paper introduces a method for training human-robot handover policies using RGB images and Gaussian Splatting, avoiding real-robot training.


<details>
  <summary>Details</summary>
Motivation: To overcome the visual domain gap between simulation and real-world robot workspaces for human-robot handovers.

Method: Uses sparse-view Gaussian Splatting to reconstruct handover scenes, generating image-action pairs for policy training.

Result: The method enables stable grasping and collision avoidance in human-robot handovers, validated in both simulated and real-world experiments.

Conclusion: The approach provides a robust and seamless representation for human-robot handover tasks, enhancing HRT systems.

Abstract: Human-robot teaming (HRT) systems often rely on large-scale datasets of human
and robot interactions, especially for close-proximity collaboration tasks such
as human-robot handovers. Learning robot manipulation policies from raw,
real-world image data requires a large number of robot-action trials in the
physical environment. Although simulation training offers a cost-effective
alternative, the visual domain gap between simulation and robot workspace
remains a major limitation. We introduce a method for training HRT policies,
focusing on human-to-robot handovers, solely from RGB images without the need
for real-robot training or real-robot data collection. The goal is to enable
the robot to reliably receive objects from a human with stable grasping while
avoiding collisions with the human hand. The proposed policy learner leverages
sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes
to generate robot demonstrations containing image-action pairs captured with a
camera mounted on the robot gripper. As a result, the simulated camera pose
changes in the reconstructed scene can be directly translated into gripper pose
changes. Experiments in both Gaussian Splatting reconstructed scene and
real-world human-to-robot handover experiments demonstrate that our method
serves as a new and effective representation for the human-to-robot handover
task, contributing to more seamless and robust HRT.

</details>

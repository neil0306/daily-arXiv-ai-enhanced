<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 98]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: PEACH is a manually aligned English-Arabic healthcare corpus with 51,671 parallel sentences, useful for linguistics, translation studies, and NLP.


<details>
  <summary>Details</summary>
Motivation: To provide a gold-standard parallel corpus for healthcare texts to support research in linguistics, translation, and NLP.

Method: Manual alignment of patient information leaflets and educational materials, resulting in 51,671 parallel sentences.

Result: A high-quality corpus with balanced sentence lengths (~9.52-11.83 words), publicly available for research.

Conclusion: PEACH is a valuable resource for domain-specific NLP tasks, translation evaluation, and educational purposes.

Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [2] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

TL;DR: A survey on the dual role of Large Language Models (LLMs) in beneficial applications and harmful content, reviewing toxicity, jailbreaking attacks, defenses, and mitigation techniques like RLHF and prompt engineering.


<details>
  <summary>Details</summary>
Motivation: The dual role of LLMs as powerful tools and sources of harmful content presents a sociotechnical challenge, necessitating a systematic review of harms and defenses.

Method: Systematic review of studies on unintentional toxicity, adversarial attacks, and content moderation, proposing a taxonomy of harms and defenses, and analyzing mitigation techniques.

Result: Highlights the evolving landscape of LLM safety, identifies limitations in current evaluation methods, and assesses mitigation efforts.

Conclusion: Outlines future research directions to develop robust and ethically aligned LLMs, emphasizing the need for improved safety measures.

Abstract: Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [3] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

TL;DR: The paper introduces FineDialFact, a benchmark for fine-grained dialogue fact verification, addressing the challenge of detecting hallucinations in LLM-generated responses. It evaluates methods using Chain-of-Thought reasoning, achieving an F1-score of 0.75, highlighting the task's difficulty.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce hallucinations (incorrect/fabricated information), posing challenges for NLP applications like dialogue systems. Current detection methods are coarse-grained, prompting the need for fine-grained verification.

Method: The authors create FineDialFact, a benchmark for verifying atomic facts in dialogue responses, using a dataset derived from public dialogue datasets. They evaluate baseline methods, including those with Chain-of-Thought reasoning.

Result: Methods with Chain-of-Thought reasoning improve performance, but the best F1-score (0.75) on HybriDialogue shows the task remains challenging.

Conclusion: FineDialFact provides a valuable benchmark for fine-grained fact verification in dialogues, though further research is needed to improve performance. The dataset and code will be publicly available.

Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [4] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

TL;DR: Fleeting memory improves language learning in transformers but worsens prediction of human reading times, challenging prior assumptions.


<details>
  <summary>Details</summary>
Motivation: To investigate whether memory limitations benefit language learning, contrasting classic cognitive science with transformer models.

Method: Training transformers with and without fleeting memory on a developmentally realistic dataset, evaluating language modeling and syntactic performance.

Result: Fleeting memory improved language learning but impaired surprisal-based prediction of human reading times, with no clear explanation for the discrepancy.

Conclusion: Memory limitations aid neural network language learning but not behavioral prediction, supporting cognitive science theories while revealing new complexities.

Abstract: Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [5] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

TL;DR: The study compares 'Mirror' and 'Non-Mirror' models for predicting depression scores using LLMs, finding Mirror models inflate effect sizes due to criterion contamination, while Non-Mirror models offer more generalizable results.


<details>
  <summary>Details</summary>
Motivation: To address the issue of 'criterion contamination' in Mirror models, which artificially inflates effect sizes and reduces generalizability, by comparing them with Non-Mirror models.

Method: Participants completed structured diagnostic and life history interviews. GPT-4, GPT-4o, and LLaMA3-70B predicted depression scores from these transcripts. Mirror models used structured diagnostic data, while Non-Mirror models used life history data.

Result: Mirror models showed inflated effect sizes (R² = .80), while Non-Mirror models had smaller but still significant effect sizes (R² = .27). Both performed similarly when correlated with self-reported symptoms (r ≈ .54).

Conclusion: Mirror models suffer from bias due to criterion contamination. Non-Mirror models provide more interpretable and generalizable features, suggesting their utility in real-world psychological assessments.

Abstract: A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [6] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

TL;DR: The paper reinterprets emergent communication (EmCom) by simulating double articulation and inflectional morphology, revealing that phonological constraints encourage concatenative morphology and emergent languages mimic natural language fusion.


<details>
  <summary>Details</summary>
Motivation: To explore how emergent communication can better mimic human language by simulating double articulation and inflectional morphology, moving beyond subfield-specific goals.

Method: Reinterpret the attribute-value reconstruction game with a small-vocabulary constraint and introduce new metrics inspired by inflectional morphology (concatenativity and fusionality).

Result: Phonological constraints promote concatenative morphology, and emergent languages replicate natural language's tendency to fuse grammatical attributes.

Conclusion: The study advances EmCom by aligning it more closely with natural language properties, offering insights into human language evolution.

Abstract: Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [7] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: The paper investigates how LLMs reason about emotions using cognitive appraisal theory, introducing the CoRE benchmark to evaluate their cognitive reasoning patterns.


<details>
  <summary>Details</summary>
Motivation: To move beyond superficial emotion tasks and explore deeper cognitive reasoning in LLMs for affective computing.

Method: Introduces the CoRE benchmark to evaluate LLMs' cognitive reasoning about emotions, drawing from cognitive appraisal theory.

Result: Reveals diverse reasoning patterns across LLMs and identifies key cognitive dimensions for emotional reasoning.

Conclusion: The study advances understanding of LLMs' emotional reasoning and provides a benchmark for future research.

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [8] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

TL;DR: The paper introduces Spectrum Projection Score (SPS) to measure retrieval relevance in RAG systems and presents xCompress, a framework for dynamic retrieval summary compression.


<details>
  <summary>Details</summary>
Motivation: Prior work evaluates RAG holistically, making it hard to isolate retrieval's contribution due to LLM prompt sensitivity.

Method: Introduces SPS, a lightweight metric to assess semantic alignment of retrieved summaries, and xCompress for dynamic summary compression.

Result: Experiments on QA benchmarks show SPS improves performance and provides insights into retrieval-generation interaction.

Conclusion: SPS and xCompress enhance RAG performance and offer a principled understanding of retrieval's role in generation.

Abstract: Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [9] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: A three-stage pipeline for scalable, high-precision prosocial content classification, combining human-AI collaboration and cost-efficient inference.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of well-defined prosociality in text and the need for scalable solutions in trust and safety systems.

Method: A pipeline with LLM-based labeling, human-AI refinement, and a two-stage inference system (lightweight classifier + GPT-4 escalation).

Result: Achieves high precision (~0.90) with ~70% cost reduction by escalating only ~35% of ambiguous cases to GPT-4.

Conclusion: Targeted human-AI interaction and deployment-aware design enable scalable solutions for novel responsible AI tasks.

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [10] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

TL;DR: ATOP is a novel method for cross-topic AES, combining topic-shared and topic-specific features via adversarial prompt-tuning and pseudo-labeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing AES methods neglect topic-specific features, limiting their ability to assess traits like topic adherence.

Method: ATOP uses adversarial training and topic-aware prompts to learn shared and specific features, with pseudo-labels for target-topic essays.

Result: ATOP significantly outperforms state-of-the-art methods on the ASAP++ dataset.

Conclusion: ATOP improves cross-topic AES by jointly learning topic-shared and topic-specific features, validated by superior performance.

Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [11] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: Introducing structured sparsity in attention during fine-tuning improves DistilBERT accuracy on SST-2, challenging the assumption that sparsity harms model performance.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic cost of self-attention in Transformers and explore if sparsity can improve accuracy, not just efficiency.

Method: Apply structured, post-hoc sparsity to DistilBERT's attention during fine-tuning on SST-2 sentiment analysis.

Result: 80% sparsity achieves 91.59% validation accuracy, a 0.97% improvement over the dense baseline.

Conclusion: Attention sparsity can enhance model generalization and performance, acting as an implicit regularizer.

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [12] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: The paper introduces Temporal Self-Rewarding Language Models to address the limitation of synchronized improvement in existing Self-Rewarding paradigms, enhancing preference learning by coordinating past, present, and future model outputs.


<details>
  <summary>Details</summary>
Motivation: Existing Self-Rewarding paradigms suffer from narrowing representational differences between chosen and rejected responses, undermining effective preference learning.

Method: Proposes a dual-phase framework: (1) Anchored Rejection (fixing rejected responses using past model outputs) and (2) Future-Guided Chosen (curating chosen samples using future model predictions).

Result: Significant improvements across model families (Llama, Qwen, Mistral) and sizes, e.g., Llama3.1-8B achieves a 29.44 win rate on AlpacaEval 2.0, outperforming the baseline by 9.75. Superior generalization is also shown in mathematical reasoning, QA, and code generation tasks.

Conclusion: The proposed method effectively sustains learning signals and outperforms existing Self-Rewarding paradigms, demonstrating robust generalization without task-specific training data.

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [13] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: The paper introduces PEEK, a method using proxy embeddings to estimate the knowledge of large language models (LLMs) without costly forward passes, achieving up to 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs acquire vast knowledge, but probing their understanding is computationally expensive. PEEK aims to address this by leveraging pre-trained embeddings as proxies.

Method: PEEK identifies facts known by LLMs, adapts embedding models (text or graph-based) to predict LLM outputs, and evaluates performance on Wikipedia-derived datasets.

Result: Embeddings predict LLM knowledge with up to 90% accuracy, with sentence embeddings outperforming graph embeddings.

Conclusion: PEEK offers a scalable way to identify LLM knowledge gaps and insights into their inductive biases, with code and data publicly available.

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [14] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: The paper introduces the EvolvR framework to improve LLMs' performance in open-ended story evaluation tasks, achieving SOTA results and enhancing story generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for story evaluation using LLMs lack adaptability or reasoning rigor, limiting their effectiveness in open-ended tasks.

Method: Proposes the EvolvR framework, which uses pairwise comparison, self-synthesized CoT data, and multi-agent filtering to train a robust evaluator.

Result: Achieves SOTA performance on benchmarks (StoryER, HANNA, OpenMEVA) and improves story generation quality as a reward model.

Conclusion: The self-evolving approach of EvolvR validates its superiority in story evaluation and generation tasks.

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [15] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Gašper Beguš*

Main category: cs.CL

TL;DR: ConlangCrafter uses LLMs to automate the creation of constructed languages (conlangs) through a modular pipeline, ensuring coherence and diversity without human expertise.


<details>
  <summary>Details</summary>
Motivation: To leverage modern LLMs for computational creativity in conlang creation, addressing the need for diverse and coherent artificial languages.

Method: A multi-hop pipeline (phonology, morphology, syntax, lexicon, translation) using LLMs with randomness and self-refinement feedback.

Result: Produces coherent and typologically diverse conlangs without requiring human linguistic expertise.

Conclusion: ConlangCrafter demonstrates the potential of LLMs as tools for creative language design.

Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [16] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: Two approaches for Quran QA: few-shot prompting with large language models and a specialized Arabic prompt framework. Achieves high precision and reduces hallucinations.


<details>
  <summary>Details</summary>
Motivation: Address challenges in Quran QA like complex language and deep meaning.

Method: Uses few-shot prompting with models like Gemini and DeepSeek, plus a post-processing system for subword alignment and semantic filtering.

Result: Large language models with Arabic instructions outperform traditional models, achieving a pAP10 score of 0.637.

Conclusion: Prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks.

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [17] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

TL;DR: LogicRAG dynamically extracts reasoning structures for adaptive retrieval, avoiding costly pre-built graphs and improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Address hallucination in LLMs and inefficiencies in GraphRAG by dynamically aligning retrieval with query logic.

Method: Decomposes queries into subproblems, constructs a DAG for logical dependencies, linearizes it, and prunes graphs/contexts to reduce token cost.

Result: Outperforms state-of-the-art baselines in performance and efficiency.

Conclusion: LogicRAG offers a scalable and effective solution for retrieval-augmented generation without pre-built graphs.

Abstract: Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [18] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

TL;DR: AURA introduces a multi-layered framework using Process Reward Models (PRMs) to address affordance-based safety risks in LLMs, outperforming traditional methods by enhancing logical coherence and safety-awareness.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with affordance-based safety risks, where outputs unintentionally enable harm due to overlooked logical implications. Traditional safety methods lack granularity and proactivity.

Method: AURA employs Process Reward Models (PRMs) for step-level evaluations, combining self-critique, PRM assessments, and adaptive safety-aware decoding to guide models toward safer reasoning.

Result: Empirical results show AURA significantly improves logical integrity and safety-awareness in model outputs, surpassing existing methods.

Conclusion: AURA sets a new benchmark for safer, contextually aware AI, advancing alignment-sensitive applications.

Abstract: Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [19] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: Selective Reflection Distillation (SRD) improves Knowledge Distillation (KD) by refining training data quality and student-model compatibility, reducing computational costs and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods overlook training data quality and student-model compatibility, limiting effectiveness.

Method: SRD uses student model reflections to curate high-quality, compatible training data and employs curriculum scheduling.

Result: SRD improves distilled model performance and reduces training runtime by up to 39%.

Conclusion: Data quality and compatibility are crucial for effective KD; SRD provides a practical framework to achieve both.

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [20] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

TL;DR: Big5-Scaler is a prompt-based framework for controlling Big Five personality traits in LLMs, enabling fine-grained personality adjustments without extra training.


<details>
  <summary>Details</summary>
Motivation: To provide a method for embedding controllable personality traits into LLMs for personality-aware dialogue agents.

Method: Embedding numeric trait values into natural language prompts to condition LLMs.

Result: Induces consistent and distinguishable traits, with performance varying by prompt type and scale. Concise prompts and lower trait intensities are most effective.

Conclusion: Big5-Scaler offers an efficient approach for personality control in LLMs, useful for building personality-aware dialogue agents.

Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [21] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

TL;DR: The paper proposes an interpretable method for detecting implicit social biases in large language models by combining nested semantic representation and contextual contrast. It validates the method using the StereoSet dataset, achieving strong performance in bias detection.


<details>
  <summary>Details</summary>
Motivation: Address the issue of implicit stereotypes in language model outputs, especially those not easily captured by explicit features.

Method: Combines nested semantic representation with contextual contrast, extracts latent bias features, and uses attention weight perturbation to analyze bias formation pathways.

Result: Achieves strong bias detection performance across multiple stereotype dimensions, with high accuracy, semantic consistency, and interpretability.

Conclusion: The method provides a transparent and reliable technical foundation for bias detection, suitable for real-world applications requiring trustworthy content.

Abstract: This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [22] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: TADrop introduces an adaptive sparsification strategy for model merging, tailoring sparsity levels to parameter tensors to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current model merging methods use a uniform sparsity ratio, ignoring parameter heterogeneity, leading to suboptimal results.

Method: TADrop assigns sparsity levels based on tensor distribution properties, pruning denser tensors aggressively and preserving sparser ones.

Result: TADrop boosts performance across diverse tasks and models, e.g., achieving a 2.0% average gain in ViT-B/32 tasks.

Conclusion: TADrop offers a more effective baseline for model merging by addressing parameter interference adaptively.

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [23] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: UR2 unifies retrieval-augmented generation (RAG) and reinforcement learning from verifiable rewards (RLVR) into a general framework, improving adaptability and performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods isolate RAG and RLVR, limiting generalization. UR2 aims to integrate these capabilities dynamically.

Method: UR2 uses difficulty-aware curriculum training and hybrid knowledge access (domain corpora + LLM summaries) for dynamic retrieval-reasoning coordination.

Result: UR2 outperforms existing RAG and RL methods, matching GPT-4o-mini and GPT-4.1-mini on benchmarks like open-domain QA and MMLU-Pro.

Conclusion: UR2 successfully bridges the gap between retrieval and reasoning, demonstrating broad applicability and superior performance.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [24] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

TL;DR: The paper redefines pragmatics as a dynamic interface for social action, critiques traditional theories in light of LLMs, and proposes adjustments to better fit AI-human communication.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between traditional pragmatic theories and the machine-centered nature of LLMs, advocating for a revised framework.

Method: Challenges semiotic trichotomy, critiques Gricean pragmatics, and introduces the Human-Machine Communication (HMC) framework and probabilistic pragmatics.

Result: Highlights biases in LLM evaluation and introduces 'context frustration' to describe challenges in AI-human communication.

Conclusion: Pragmatic theory needs expansion to accommodate generative AI, emphasizing co-construction of meaning in human-machine interactions.

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [25] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: The paper explores methods for injecting small, unstructured information into LLMs, focusing on knowledge acquisition and catastrophic forgetting. It evaluates synthetic data generation and diverse prompting, showing improved learning with variability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of updating LLMs with limited data and study the balance between learning new content and retaining existing knowledge.

Method: Used a dataset of recent news to evaluate knowledge acquisition, explored synthetic data generation, and compared continued pre-training with augmentation algorithms.

Result: Diverse textual variations improved learning, while RAG-based approaches showed sensitivity. Models could generate effective synthetic data for self-improvement.

Conclusion: The study highlights the potential of synthetic data and diverse prompting for efficient knowledge injection in LLMs with limited data.

Abstract: Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [26] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

TL;DR: The DKG-LLM framework integrates a dynamic knowledge graph with the Grok 3 LLM for medical diagnosis and treatment recommendations, achieving high accuracy and semantic coverage.


<details>
  <summary>Details</summary>
Motivation: To enhance medical diagnosis and personalized treatment by leveraging LLMs and dynamic knowledge graphs for better accuracy and adaptability.

Method: Uses the Adaptive Semantic Fusion Algorithm (ASFA) to integrate heterogeneous medical data into a dynamic knowledge graph, combined with the Grok 3 LLM.

Result: Achieves 84.19% diagnostic accuracy, 89.63% treatment recommendation accuracy, and 93.48% semantic coverage.

Conclusion: DKG-LLM is a reliable, transformative tool for handling complex medical data and improving diagnosis and treatment recommendations.

Abstract: Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [27] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

TL;DR: SceneJailEval introduces a scenario-adaptive multi-dimensional framework for jailbreak evaluation, outperforming existing methods with higher F1 scores.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak evaluation methods lack precision due to binary classification and uniform criteria across scenarios, leading to mismatches.

Method: SceneJailEval proposes a scenario-adaptive framework and a 14-scenario dataset for comprehensive evaluation.

Result: Achieves F1 scores of 0.917 (full-scenario) and 0.995 (JBB), surpassing prior state-of-the-art methods.

Conclusion: SceneJailEval addresses limitations of existing methods, offering precise, adaptable, and extensible jailbreak evaluation.

Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [28] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: The paper introduces a four-layer EI taxonomy for LLMs, evaluates six models on a new benchmark (EICAP-Bench), and finds limited EI improvement through fine-tuning, highlighting gaps in current methods.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored role of Emotional Intelligence (EI) in LLMs, the study aims to develop a framework and benchmark for evaluating and enhancing EI in models.

Method: A four-layer EI taxonomy is proposed, and six LLMs are evaluated on EICAP-Bench. Fine-tuning experiments are conducted using LoRA adapters on UltraChat in English and Arabic.

Result: Qwen2.5-Instruct performs best on EICAP-Bench. Fine-tuning improves only the Appraisal layer, revealing limitations in current EI enhancement methods.

Conclusion: Current pretraining and instruction-tuning paradigms are insufficient for deep EI reasoning, calling for targeted data and modeling strategies.

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [29] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: The paper introduces Retrieval-Augmented Generation (RAG) for content moderation, shifting from pre-trained classification to contextual policy evaluation, achieving robust accuracy, explainability, and dynamic updates without retraining.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptable content moderation systems that can evolve with policies without costly retraining.

Method: Uses a Contextual Policy Engine (CPE), an agentic RAG system, to evaluate content against retrieved policy knowledge at inference time.

Result: Demonstrates comparable accuracy to commercial systems, explainability via policy segments, and dynamic updates without retraining.

Conclusion: RAG transforms classification into a flexible, transparent, and adaptable process for content moderation and broader applications.

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [30] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: InfoCausalQA is a new benchmark for evaluating causal reasoning in VLMs using infographics, revealing their limitations compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of causal inference in VLMs, particularly in multimodal settings.

Method: Created InfoCausalQA with two tasks (quantitative and semantic causal reasoning) using 494 infographic-text pairs and GPT-4o-generated QA pairs, later refined by humans.

Result: Current VLMs show limited computational and semantic causal reasoning abilities, lagging behind humans.

Conclusion: InfoCausalQA underscores the need to improve causal reasoning in multimodal AI systems.

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [31] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: The paper proposes a novel intent recognition approach for elderly German speakers using adapted ASR and Transformer models trained on synthetic data from LLMs, showing improved performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing intent recognition systems, which are limited to short commands and English, by focusing on elderly German speakers.

Method: Combines an adapted Whisper ASR model with Transformer-based language models trained on synthetic text from LLMs (LeoLM, Llama3, ChatGPT), evaluated using synthetic speech and cross-dataset testing.

Result: Synthetic LLM-generated data enhances classification performance and robustness, with LeoLM outperforming ChatGPT in German intent recognition.

Conclusion: Generative AI can bridge data gaps in low-resource domains, with detailed documentation for transparency and reproducibility.

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [32] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

TL;DR: MDIR is a new method for detecting plagiarism in LLMs using matrix analysis and Large Deviation Theory, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about IP theft in LLMs due to plagiarism methods like weight copying, upcycling, or continual pretraining, which existing detection methods fail to address accurately.

Method: MDIR leverages matrix analysis and Large Deviation Theory to reconstruct weight relationships, compute p-values, and detect plagiarism without full model inference.

Result: MDIR reliably detects plagiarism even after extensive transformations and can perform detections efficiently on a single PC within an hour.

Conclusion: MDIR is an efficient, accessible, and rigorous solution for detecting LLM plagiarism, overcoming key shortcomings of existing methods.

Abstract: In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [33] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: The paper introduces DynamicTRF, a framework to improve zero-shot graph QA by tailoring graph representations and introducing a new metric (GRE) to balance performance and brevity.


<details>
  <summary>Details</summary>
Motivation: Current approaches for graph QA use a single graph representation, ignoring model or task preferences, leading to incorrect or verbose responses.

Method: Analyzes existing graph representations, designs tailored ones ($F_{ZS}$), introduces GRE, and develops DynamicTRF to adaptively assign the best representation.

Result: DynamicTRF significantly improves zero-shot graph QA accuracy across diverse tasks.

Conclusion: DynamicTRF enhances LMMs' graph QA by dynamically selecting optimal graph representations.

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [34] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

TL;DR: Integrating aggression detection as an auxiliary task improves LLM performance in cyberbullying detection via an enriched prompt pipeline.


<details>
  <summary>Details</summary>
Motivation: Cyberbullying detection is challenging due to subtle expressions; leveraging aggression detection could enhance model generalization.

Method: Tested zero-shot, few-shot, LoRA fine-tuning, and MTL; proposed an enriched prompt pipeline embedding aggression predictions.

Result: Enriched prompt pipeline outperformed standard LoRA fine-tuning, showing aggression context boosts detection.

Conclusion: Auxiliary tasks like aggression detection can enhance LLM generalization for safety-critical social media applications.

Abstract: Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [35] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: The paper critiques existing evaluation metrics for style personalized text generation and proposes using diverse metrics like style embeddings and LLM-as-judge, validated through a benchmark across multiple tasks and settings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective evaluation in low-resource author style personalized text generation and question the adequacy of traditional metrics like BLEU and ROUGE.

Method: Proposes and evaluates alternative metrics (style embeddings, LLM-as-judge) and their ensembles using a style discrimination benchmark across eight tasks and three settings.

Result: Demonstrates the superiority of ensemble metrics for evaluating style personalized text generation.

Conclusion: Recommends adopting diverse evaluation metrics for a holistic assessment of style personalized text generation.

Abstract: While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [36] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

TL;DR: The paper introduces ChatAnime, a dataset for Emotionally Supportive Role-Playing (ESRP) using LLMs, focusing on anime characters to evaluate emotional support and role-playing capabilities.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in combining LLMs' role-playing and emotional support capabilities, using anime characters for their defined personalities and fan bases.

Method: Created the ChatAnime dataset with 20 anime characters, 60 scenarios, and 40 enthusiasts. Collected dialogue data from 10 LLMs and humans, evaluated using 9 metrics.

Result: Top LLMs outperformed humans in role-playing and emotional support, but humans led in response diversity.

Conclusion: The work provides resources for optimizing LLMs in ESRP, with datasets available for future research.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [37] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

TL;DR: SecMCP is a secure framework for detecting and mitigating security risks in Model Context Protocol (MCP) by analyzing latent space trajectories to prevent adversarial attacks like conversation hijacking.


<details>
  <summary>Details</summary>
Motivation: MCP's non-isolated execution context introduces security and privacy risks, such as tool poisoning and indirect prompt injection, which existing defenses fail to address adequately.

Method: SecMCP models LLM activation vectors in a latent polytope space to detect anomalous shifts in conversational dynamics, enabling proactive threat detection.

Result: SecMCP achieves robust detection (AUROC > 0.915) on benchmark datasets (MS MARCO, HotpotQA, FinQA) with three LLMs (Llama3, Vicuna, Mistral).

Conclusion: SecMCP provides a novel, effective solution for securing MCP, with contributions including threat categorization, latent polytope-based drift quantification, and empirical validation.

Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [38] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: Memp enhances LLM agents with learnable, updatable procedural memory, improving task success and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLM agents lack robust procedural memory, which is often manually engineered or static, limiting adaptability.

Method: Proposes Memp, which distills past trajectories into fine-grained instructions and script-like abstractions, with strategies for Build, Retrieval, and Update.

Result: Agents show higher success rates and efficiency as memory refines; memory from stronger models boosts weaker ones.

Conclusion: Memp enables lifelong procedural memory, enhancing agent performance and adaptability.

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [39] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: LLMs fine-tuned in few languages can classify immigration-related tweets in unseen languages, but multilingual fine-tuning improves stance detection. Minimal exposure to underrepresented languages corrects pre-training biases. Lightweight models offer cost-effective, scalable solutions.


<details>
  <summary>Details</summary>
Motivation: To explore if knowledge from fine-tuning in few languages transfers to unseen languages and if minimal fine-tuning can correct pre-training biases.

Method: Fine-tuned lightweight LLaMA models on monolingual, bilingual, or multilingual datasets to classify immigration tweets across 13 languages.

Result: LLMs fine-tuned in one or two languages classify content in unseen languages well, but multilingual fine-tuning improves stance detection. Minimal fine-tuning corrects biases.

Conclusion: Limited language coverage suffices for topic-level generalization, and lightweight interventions can correct biases, offering scalable, cost-effective solutions.

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [40] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: The study analyzes AI-generated content in news articles, finding increased use of GenAI, especially in local and college media, with impacts on writing style and formality.


<details>
  <summary>Details</summary>
Motivation: To address concerns about journalistic integrity and authorship due to the rise of Generative AI (GenAI) and LLMs in news media.

Method: Analysis of over 40,000 news articles using three AI-text detectors (Binoculars, Fast-Detect GPT, GPTZero) and linguistic analysis.

Result: Substantial increase in GenAI use, especially in local and college news; LLMs often used in introductions, with manual conclusions. GenAI boosts word richness and readability but reduces formality.

Conclusion: GenAI is increasingly used in news media, altering writing styles and formality, particularly in local outlets.

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [41] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: SlimInfer accelerates LLM inference by pruning less critical prompt tokens dynamically, achieving significant speedup without performance loss.


<details>
  <summary>Details</summary>
Motivation: High computational demands limit long-context inference in LLMs, even with optimized attention methods. SlimInfer addresses this by pruning redundant tokens based on information diffusion.

Method: SlimInfer uses a dynamic fine-grained pruning mechanism to remove redundant tokens in hidden states at intermediate layers, coupled with an asynchronous KV cache manager.

Result: Achieves up to 2.53× TTFT speedup and 1.88× latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, with no performance drop on LongBench.

Conclusion: SlimInfer effectively reduces computational overhead and improves inference efficiency for LLMs while maintaining accuracy.

Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [42] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: GLM-4.5 is an open-source Mixture-of-Experts (MoE) model with 355B total parameters, achieving strong performance in reasoning and coding tasks with fewer parameters than competitors.


<details>
  <summary>Details</summary>
Motivation: To advance research in reasoning and agentic AI systems by developing an efficient, high-performance model.

Method: Multi-stage training on 23T tokens, hybrid reasoning, expert model iteration, and reinforcement learning.

Result: Scores 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified, ranking 3rd overall and 2nd on agentic benchmarks.

Conclusion: GLM-4.5 and its compact version, GLM-4.5-Air, are released to foster further research in AI reasoning and agentic systems.

Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [43] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: HapticLLaMA is a multimodal model for generating natural language descriptions from haptic signals, achieving strong performance in haptic captioning.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of haptic signals in multimodal research, enabling applications in VR, accessibility, and rehabilitation.

Method: Proposes HapticLLaMA, using frequency-based and EnCodec-based tokenizers, trained via supervised fine-tuning and RLHF.

Result: Achieves METEOR score of 59.98 and BLEU-4 score of 32.06, with 61% of captions rated above 3.5/7. RLHF improves ratings by 10%.

Conclusion: Demonstrates the potential of large language models to process sensory data effectively.

Abstract: Haptic captioning is the task of generating natural language descriptions
from haptic signals, such as vibrations, for use in virtual reality,
accessibility, and rehabilitation applications. While previous multimodal
research has focused primarily on vision and audio, haptic signals for the
sense of touch remain underexplored. To address this gap, we formalize the
haptic captioning task and propose HapticLLaMA, a multimodal sensory language
model that interprets vibration signals into descriptions in a given sensory,
emotional, or associative category. We investigate two types of haptic
tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that
convert haptic signals into sequences of discrete units, enabling their
integration with the LLaMA model. HapticLLaMA is trained in two stages: (1)
supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,
and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We
assess HapticLLaMA's captioning performance using both automated n-gram metrics
and human evaluation. HapticLLaMA demonstrates strong capability in
interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a
BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated
captions received human ratings above 3.5 on a 7-point scale, with RLHF
yielding a 10% improvement in the overall rating distribution, indicating
stronger alignment with human haptic perception. These findings highlight the
potential of large language models to process and adapt to sensory data.

</details>


### [44] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: A post-training process improves LLMs' ability to form ad-hoc conventions in multi-turn interactions, evaluated via new benchmarks.


<details>
  <summary>Details</summary>
Motivation: Humans efficiently adapt language in interactions, but LLMs lack this natural behavior. The goal is to enhance LLMs' convention formation.

Method: Targeted fine-tuning on heuristically identified demonstrations of convention formation, evaluated with two new benchmarks.

Result: Post-trained LLMs show significantly improved convention formation abilities in both benchmarks.

Conclusion: The post-training process effectively enhances LLMs' ability to form conventions, bridging a gap with human communication.

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: ResPA improves adversarial example transferability by using residual gradients to guide perturbations toward flat loss regions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to adversarial examples, and current transfer-based attacks overlook perturbation direction influence, limiting transferability.

Method: ResPA uses residual gradients (difference between current and historical gradients) to guide perturbations toward flat loss regions, enhancing transferability.

Result: ResPA shows superior transferability compared to existing methods and further improves when combined with input transformation techniques.

Conclusion: ResPA effectively addresses the limitation of prior methods by leveraging residual gradients, offering a robust solution for transfer-based attacks.

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [46] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: The paper introduces a Generalized Few-shot OOD Detection (GOOD) framework to improve generalization in OOD detection by using a General Knowledge Model (GKM) and a Knowledge Dynamic Embedding (KDE) mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing Few-shot OOD detection methods lack generalization for open-world scenarios due to overfitting on limited training data.

Method: Proposes GOOD framework with GKM and KDE to balance generality and specificity, reducing generalization error.

Result: Experiments on real-world benchmarks show superior performance.

Conclusion: The GOOD framework effectively enhances generalization in Few-shot OOD detection.

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [47] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: UnGuide introduces a dynamic inference mechanism (UnGuidance) to improve LoRA-based unlearning in diffusion models, ensuring precise control and preserving content fidelity.


<details>
  <summary>Details</summary>
Motivation: Address concerns about misuse of text-to-image models by enabling effective unlearning of harmful/misleading content without degrading overall performance.

Method: Uses UnGuidance (dynamic inference with Classifier-Free Guidance) to modulate LoRA's influence, balancing unlearning and content fidelity.

Result: Outperforms existing LoRA methods in controlled concept removal and retains model performance.

Conclusion: UnGuide offers a robust solution for selective unlearning in diffusion models, maintaining realism and expressive power.

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [48] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: The paper introduces a partial-convolution-based style transfer network for applying artistic styles to specific image regions, improving accuracy over traditional masking methods.


<details>
  <summary>Details</summary>
Motivation: Existing style transfer methods apply styles to entire images, but users often need stylization for specific regions. Masking post-stylization fails to capture style features accurately in the region of interest.

Method: The proposed method uses partial-convolution-based networks and internal blending techniques to apply styles exclusively to selected regions, addressing imperfections in region selection.

Result: The approach visually and quantitatively outperforms traditional masking methods, demonstrated using the SA-1B dataset.

Conclusion: The partial-convolution-based network and blending techniques provide a more accurate and visually appealing solution for region-specific style transfer.

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [49] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2 is an accelerated 3D medical image synthesis framework using rectified flow for fast, high-quality generation, addressing slow inference and condition alignment issues in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for medical image synthesis face challenges like limited generalizability, slow inference, and weak alignment with input conditions.

Method: MAISI-v2 integrates rectified flow for acceleration and introduces a region-specific contrastive loss to improve condition fidelity.

Result: Achieves state-of-the-art image quality with 33× acceleration and demonstrates utility in downstream tasks like segmentation.

Conclusion: MAISI-v2 advances medical image synthesis by improving speed and condition alignment, with released resources for community use.

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [50] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: A framework for few-shot deployment of pretrained MRI transformers using MAE pretraining, achieving high accuracy in classification and segmentation tasks with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of annotated data in medical imaging by leveraging pretrained transformers for improved real-world applicability.

Method: Utilizes Masked Autoencoder (MAE) pretraining on a large-scale brain MRI dataset, combining frozen MAE encoders with lightweight heads for classification and hybrid architectures (MAE-FUnet) for segmentation.

Result: Achieves state-of-the-art accuracy in MRI sequence identification and outperforms baselines in skull stripping and multi-class anatomical segmentation under data-limited conditions.

Conclusion: The framework is efficient, stable, and scalable, making it suitable for low-resource clinical environments and broader neuroimaging applications.

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [51] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: A novel, fast, and optimization-free method for stylizing 3D Gaussian splats using a graph structure and surface-based stylization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D Gaussian splat style transfer require reconstruction, fine-tuning, or optimization, which are time-consuming and resource-intensive.

Method: Generates a graph structure on the splat's implicit surface, applies a feed-forward stylization method, and interpolates results back to individual splats.

Result: Achieves fast stylization (under 2 minutes on consumer hardware) without additional training or optimization, with high-quality results.

Conclusion: The proposed method is efficient, flexible, and outperforms existing approaches in speed and ease of use.

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [52] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN enhances NeRF for industrial inspection by handling multi-zoom images, improving accuracy and detail capture.


<details>
  <summary>Details</summary>
Motivation: NeRF lacks fine details for industrial use (e.g., sub-micron defects). Multi-zoom images break NeRF's consistency, requiring a new approach.

Method: MZEN introduces a learnable zoom scalar and a pose strategy: wide-field images establish a global frame, zoom-in images are pose-primed and refined.

Result: MZEN outperforms baselines, boosting PSNR by 28%, SSIM by 10%, and reducing LPIPS by 222%.

Conclusion: MZEN extends NeRF to industrial settings, capturing micron-level details while maintaining global accuracy.

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [53] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: TSMS-SAM2 enhances promptable video object segmentation in surgical videos by addressing motion dynamics and memory redundancy in SAM2, achieving top performance on EndoVis datasets.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models like SAM2 struggle with surgical video analysis due to complex motion and memory inefficiency.

Method: Proposes multi-temporal-scale video sampling augmentation and memory splitting/pruning to improve robustness and efficiency.

Result: Achieved highest mean Dice scores (95.24 and 86.73) on EndoVis2017 and EndoVis2018 datasets.

Conclusion: TSMS-SAM2 is effective for robust, efficient segmentation in surgical scenarios, with potential for broader applications.

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [54] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: Temporal Cluster Assignment (TCA) improves video segmentation by leveraging temporal coherence, reducing computation while retaining detail.


<details>
  <summary>Details</summary>
Motivation: High computational costs of Swin Transformer in video segmentation limit real-time applications, and existing token reduction methods fail to exploit temporal redundancy.

Method: TCA refines token clusters using temporal correlations across frames, avoiding indiscriminate token dropping.

Result: TCA enhances accuracy-speed trade-off on datasets like YouTube-VIS 2019, 2021, OVIS, and surgical videos.

Conclusion: TCA effectively generalizes across natural and domain-specific videos, optimizing performance without fine-tuning.

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [55] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: A vision-language framework predicts driver gaze shifts using natural language, outperforming general-purpose models in attention detection and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on static attention estimation, but this work addresses dynamic gaze shifts using language for explainable AI in autonomous driving.

Method: Uses few-shot/zero-shot learning on RGB images, fine-tunes LLaVA with human-refined captions, and integrates low-level cues and top-down context.

Result: Fine-tuned model excels in attention shift detection and interpretability, offering new metrics for semantic alignment and diversity.

Conclusion: Pioneers language-based gaze prediction, aiding tasks like behavior forecasting and human-AI collaboration in autonomous driving.

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [56] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: A multi-view camera method for gaze target estimation (GTE) improves accuracy by addressing single-view limitations like occlusion and ambiguity, using modules for head information, gaze selection, and cross-view scene attention.


<details>
  <summary>Details</summary>
Motivation: Existing single-view GTE methods struggle with face occlusion, target ambiguity, and out-of-view targets, limiting accuracy and applicability.

Method: The approach integrates two camera views with modules for Head Information Aggregation (HIA), Uncertainty-based Gaze Selection (UGS), and Epipolar-based Scene Attention (ESA).

Result: The method outperforms single-view baselines, especially when the second camera provides a clear face view, and can estimate gaze targets using only the second view.

Conclusion: The proposed multi-view GTE method enhances accuracy and expands capabilities, supported by a new multi-view dataset for further research.

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [57] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: ETTA improves test-time adaptation for VLMs by integrating all test samples dynamically and reducing prompt dependency, achieving better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Pretrained VLMs like CLIP struggle with generalization under distribution shifts, and current cache-based TTA methods are limited by storing only high-confidence samples.

Method: ETTA introduces a Recursive Updating module for dynamic integration of test samples and an Adaptive Ensemble module to reduce prompt dependency.

Result: ETTA outperforms state-of-the-art TTA models in accuracy and computational efficiency on two benchmarks.

Conclusion: ETTA sets a new standard for efficient and effective test-time adaptation, with released code for reproducibility.

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [58] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0 is an advanced framework for generating and editing 3D scenes from text, leveraging vision-language models for high semantic fidelity and flexible human feedback integration.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene generation relies heavily on manual effort, and automated methods lack flexibility and open-domain capabilities.

Method: Uses vision-language models to parse input descriptions, generate assets via 3D models, and iteratively apply spatial constraints for coherent layouts.

Result: Generates diverse, high-quality scenes aligned with text, outperforming baselines in evaluations. Supports interactive editing.

Conclusion: HOLODECK 2.0 advances 3D scene generation, offering practical applications like game modeling with improved efficiency and flexibility.

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [59] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch is an unsupervised deep image stitching framework that ensures robustness and naturalness by using a dual-branch architecture and virtual optimal planes.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of robustness and naturalness in image stitching, especially in diverse real-world scenes.

Method: Uses a dual-branch architecture (pretrained and learnable branches) to capture coarse and fine features, and introduces virtual optimal planes to balance content alignment and structural preservation.

Result: Significantly outperforms existing methods in scene robustness and content naturalness.

Conclusion: RopStitch provides a robust and natural solution for image stitching, validated by extensive experiments.

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [60] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: Neural field models enable compact representation of complex geometry and lighting effects in mobile imaging, outperforming state-of-the-art methods without complex pre-processing or labeled data.


<details>
  <summary>Details</summary>
Motivation: To leverage the versatility of smartphones as computational imaging platforms and address challenges in scene reconstruction and image processing.

Method: Uses neural field models trained with stochastic gradient descent to fit raw smartphone measurements, avoiding explicit data representations.

Result: Outperforms existing methods in depth estimation, layer separation, and image stitching without relying on labeled data or priors.

Conclusion: Well-designed neural field models offer efficient solutions for mobile imaging tasks, demonstrating their potential for real-world applications.

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [61] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: The paper evaluates SAM and Mask3D for 3D segmentation in construction monitoring, highlighting their adaptability and the lack of outdoor benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with construction sites' complexity, prompting the need for scalable, computer-vision-based solutions.

Method: Comparative analysis of SAM and Mask3D in real-world construction settings, assessing their performance and adaptability.

Result: Identifies gaps in segmentation approaches due to missing outdoor benchmarks and showcases the models' effectiveness.

Conclusion: Tailored segmentation workflows are needed for actionable insights, advancing automated and precise construction monitoring.

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [62] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD is a self-supervised framework for normal estimation from a single image, using 3D Gaussian splatting and diffusion to address multi-view inconsistency and data dependency.


<details>
  <summary>Details</summary>
Motivation: Challenges in normal estimation from single images include multi-view normal conflicts and reliance on dense annotations due to gradient discontinuity in diffusion models.

Method: Integrates physics-driven light-interaction modeling and differentiable rendering to optimize normals using 3D geometric errors. Uses a conditional diffusion model with geometric priors.

Result: Outperforms state-of-the-art methods on the Google Scanned Objects dataset.

Conclusion: SINGAD effectively solves multi-view inconsistency and reduces dependency on annotated data, advancing single-image normal estimation.

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [63] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1 integrates pretrained multimodal LLMs and diffusion models using patch-level CLIP embeddings for efficient, high-fidelity image generation without compromising reasoning.


<details>
  <summary>Details</summary>
Motivation: To enable high-fidelity visual synthesis in LLMs without costly training or loss of reasoning capabilities.

Method: Uses patch-level CLIP embeddings as latents, integrates them into a diffusion model via ControlNet, and adds a visual generation branch to the MLLM.

Result: Achieves comparable or better performance in visual fidelity and understanding with lower compute.

Conclusion: Bifrost-1 efficiently bridges MLLMs and diffusion models for controllable image generation.

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [64] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: PASG bridges the gap between task semantics and geometric features in robotic manipulation by combining automatic primitive extraction, VLM-driven semantic anchoring, and spatial-semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: Address the fragmentation between high-level task semantics and low-level geometric features in robotic manipulation, and overcome limitations of VLMs like lack of semantic grounding and reliance on manual annotations.

Method: Proposes PASG with automatic primitive extraction, VLM-driven semantic anchoring, and a spatial-semantic reasoning benchmark (Qwen2.5VL-PA).

Result: PASG achieves performance comparable to manual annotations and finer-grained semantic-affordance understanding in diverse robotic tasks.

Conclusion: PASG successfully unifies geometric primitives with task semantics, advancing robotic manipulation capabilities.

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [65] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene integrates 3D scene reconstruction with 4D human animation, addressing placement, lighting/style alignment, and camera trajectory challenges.


<details>
  <summary>Details</summary>
Motivation: Seamlessly integrating 3D scenes with 4D human animation is challenging due to placement, lighting/style mismatches, and camera movement needs.

Method: AnimateScene uses an accurate placement module, training-free style alignment, and joint post-reconstruction for camera trajectories.

Result: The framework produces high-detail, coherent videos with dynamic camera movements.

Conclusion: AnimateScene effectively solves key challenges in integrating 3D scenes and 4D human animation.

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [66] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: A method called Energy-based Test-time Adaptation (ETA) is proposed to adapt pretrained depth completion models at test time by minimizing energy scores, improving performance on novel data.


<details>
  <summary>Details</summary>
Motivation: Depth completion models perform poorly on target data due to covariate shifts, and prior access to out-of-distribution data is unavailable.

Method: Uses adversarial perturbations to train an energy model for scoring predictions, then updates model parameters at test time to align predictions with the source distribution.

Result: ETA outperforms the state-of-the-art by 6.94% (outdoors) and 10.23% (indoors) across six datasets.

Conclusion: ETA effectively adapts models to novel conditions without prior target data, demonstrating significant improvements.

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [67] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: The paper proposes an efficient video computer vision system by reducing front-end computation and introducing a fast block matching-based motion estimation algorithm, achieving significant acceleration with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully reduce temporal redundancy and ignore front-end computation overhead, limiting efficiency in video computer vision systems.

Method: The system removes the image signal processor, uses Bayer-format data directly, employs a fast block matching-based motion estimation algorithm with MV refinement, and introduces a context-aware block refinement network for error correction. A frame selection strategy balances accuracy and efficiency.

Result: Experiments show the method achieves significant acceleration with only slight performance loss across multiple video computer vision tasks.

Conclusion: The proposed system effectively addresses efficiency challenges in video computer vision by optimizing computation and refining motion estimation, demonstrating practical benefits.

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [68] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: A novel multimodal emotion recognition framework is proposed, leveraging pre-trained models and innovative fusion strategies to address data scarcity, achieving significant performance improvement on the MER2025-SEMI dataset.


<details>
  <summary>Details</summary>
Motivation: Enhancing human-computer interaction by improving emotion recognition, particularly addressing the challenge of data scarcity in multimodal settings.

Method: Utilizes large-scale pre-trained models for feature extraction, a dual-branch visual encoder, context-enriched textual processing, and a fusion strategy with self-attention and residual connections. Noisy labels are refined via multi-source labeling.

Result: Achieves a weighted F-score of 87.49%, outperforming the baseline of 78.63% on the MER2025-SEMI dataset.

Conclusion: The proposed framework effectively improves multimodal emotion recognition, demonstrating the value of advanced feature extraction and fusion techniques.

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [69] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: The paper introduces MakeupQuad, a dataset for facial makeup editing, and EvoMakeup, a framework for high-fidelity, controllable makeup transfer, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for facial makeup editing produce low-quality results due to lack of structured paired data and struggle with identity and makeup fidelity.

Method: Proposes MakeupQuad dataset and EvoMakeup framework, which uses multi-stage distillation for iterative improvement of data and model quality.

Result: EvoMakeup outperforms prior methods on real-world benchmarks, achieving superior makeup fidelity and identity preservation.

Conclusion: The method effectively balances makeup fidelity and identity preservation, supporting multi-task makeup editing in a single model.

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [70] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: MathReal introduces a dataset of 2,000 real-world K-12 math questions with images, challenging MLLMs' visual reasoning. It categorizes image issues and evaluates MLLMs, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack real-world K-12 educational images, limiting MLLM evaluation in authentic scenarios.

Method: Curated 2,000 math questions with real images, classified into 3 main categories (14 subcategories), and evaluated MLLMs in 6 experimental settings.

Result: MLLMs struggle with real-world images, showing significant performance gaps in recognition, comprehension, and reasoning.

Conclusion: MathReal highlights MLLMs' limitations in real-world educational contexts and suggests future improvements.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [71] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: A 3DGS-based pipeline improves novel view synthesis by generating additional training views using an information-gain-driven strategy and video diffusion priors, outperforming existing methods on challenging scenes.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods struggle with artifacts and missing regions when rendering from viewpoints outside the training trajectory, limiting seamless scene exploration.

Method: Proposes a pipeline with an information-gain-driven virtual camera placement strategy and video diffusion priors to refine rendered results, followed by fine-tuning 3D Gaussians.

Result: Outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints.

Conclusion: The proposed method enhances reconstruction quality and scene exploration, validated by the Wild-Explore benchmark.

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [72] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: A diffusion model is developed to generate synthetic particle images, addressing data imbalance and improving multi-class deep neural network training for sub-visible particle analysis.


<details>
  <summary>Details</summary>
Motivation: The scarcity and imbalance of particle data, especially for rare types like silicone oil and air bubbles, hinder effective multi-class classification.

Method: A state-of-the-art diffusion model generates high-fidelity synthetic images to augment training datasets.

Result: Generated samples closely resemble real images, and experiments show improved classification performance without significant drawbacks.

Conclusion: The approach is validated and publicly released to promote open research and reproducibility.

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [73] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum is a unified network for detailed human parsing, leveraging a fine-tuned Image-to-Texture diffusion model to improve alignment with body parts and clothing, outperforming baselines in cross-dataset experiments.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human parsing use broad labels or group humans into a single category, failing to capture fine-grained clothing types or detailed body parts.

Method: Spectrum repurposes an Image-to-Texture diffusion model, fine-tuned on 3D human texture maps, to extract human-part features and generate semantically valid masks aligned to diverse clothing categories.

Result: Spectrum consistently outperforms baseline methods in prompt-based segmentation across body parts, clothing parts, unseen categories, and full-body masks.

Conclusion: Spectrum addresses limitations of existing methods by providing detailed, aligned parsing of body parts and clothing, demonstrating superior performance in diverse scenarios.

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [74] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit is a fast text-guided image editing method using RectifiedFlow, PerRFI inversion, and Disentangled Prompt Guidance for high-quality, coherent edits.


<details>
  <summary>Details</summary>
Motivation: To enable fast and precise text-guided image editing while preserving critical content and following textual instructions closely.

Method: Leverages RectifiedFlow with PerRFI inversion, Inversion Latent Injection for regeneration, Disentangled Prompt Guidance, and Canny-conditioned ControlNet.

Result: Outperforms state-of-the-art methods on the PIE dataset in speed and quality.

Conclusion: InstantEdit is efficient and effective for text-guided image editing, balancing editability and detail preservation.

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [75] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: A semi-supervised learning framework for emotion recognition using a Mixture of Experts (MoE) approach, integrating diverse modalities and pseudo-labeling, achieving 2nd place in MER2025-SEMI with an F1-score of 0.8772.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of semi-supervised learning in emotion recognition by leveraging diverse input modalities and unlabeled data effectively.

Method: Proposes a MoE system with diverse experts (e.g., VLMs, AU data), consensus-based pseudo-labeling, and a two-stage training paradigm, followed by a multi-expert voting ensemble and rule-based re-ranking.

Result: Achieves an F1-score of 0.8772 on the MER2025-SEMI test set, ranking 2nd.

Conclusion: The framework demonstrates effectiveness in semi-supervised emotion recognition by combining diverse modalities and pseudo-labeling, with potential for further refinement.

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [76] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM compresses visual representations in the frequency domain using DCT and FFT, reducing computational overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: VLMs face high computational costs due to excessive vision tokens; Fourier-VLM addresses this by exploiting low-frequency dominance in vision features.

Method: Applies a low-pass filter via 2D DCT (computed efficiently with FFT) to compress visual features without extra parameters.

Result: Achieves competitive performance, reduces FLOPs by 83.8%, and speeds up generation by 31.2% compared to LLaVA-v1.5.

Conclusion: Fourier-VLM offers a practical, efficient solution for VLMs without compromising performance.

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [77] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: The paper introduces Next Editing-token Prediction (NEP) for text-guided image editing, focusing on regenerating only the necessary regions to improve efficiency and edit quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods regenerate entire images, leading to computational inefficiency and compromised edit quality in non-edited regions.

Method: Proposes NEP based on autoregressive image generation and pre-trains an any-order autoregressive text-to-image model for zero-shot editing.

Result: Achieves state-of-the-art performance on image editing benchmarks and supports test-time scaling (TTS) for iterative refinement.

Conclusion: NEP offers a more efficient and higher-quality solution for text-guided image editing by selectively regenerating only the required regions.

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [78] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker is a reasoning-based VQA framework using LMMs and reinforcement learning to improve generalization and explainability in video quality assessment.


<details>
  <summary>Details</summary>
Motivation: Existing VQA models struggle with poor generalization to OOD videos and limited explainability, hindering real-world use.

Method: VQAThinker employs GRPO, a rule-guided reinforcement learning algorithm, with three VQA-specific rewards: bell-shaped regression, pairwise ranking, and temporal consistency.

Result: VQAThinker achieves state-of-the-art performance on in-domain and OOD benchmarks, excelling in generalization and explainability.

Conclusion: Reinforcement learning with score-level supervision is effective for building generalizable and explainable VQA models.

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [79] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net, a framework for 3D LV mesh reconstruction from brain MRI, improves accuracy and robustness by using an anatomy-aware template and enhances shape statistics for neurological disease analysis.


<details>
  <summary>Details</summary>
Motivation: Lateral ventricle (LV) shape analysis is a potential biomarker for neurological diseases, but challenges like shape variability and MRI resolution limitations hinder its use.

Method: LV-Net deforms a joint LV-hippocampus template mesh, incorporating anatomical relationships to reduce segmentation artifacts and improve point correspondence.

Result: LV-Net achieves superior reconstruction accuracy and reliable shape descriptors, even with imperfect segmentations, and identifies disease-associated LV subregions in Alzheimer's analysis.

Conclusion: LV-Net advances LV shape analysis, offering a robust tool for neurological disease research, with potential applications in biomarker discovery.

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [80] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: The paper advocates for the inclusion of satellite spectral imagery in AGI research, critiques current benchmarks, and proposes a comprehensive benchmark for evaluating Earth observation models.


<details>
  <summary>Details</summary>
Motivation: Satellite spectral imagery is underutilized in AGI research despite its potential to enhance understanding of the natural world. The paper aims to highlight its importance and address benchmarking gaps.

Method: The paper reviews existing benchmarks, identifies their limitations, and proposes a set of tasks for a new benchmark to evaluate Earth observation models.

Result: Current benchmarks lack the ability to assess generalization in Earth observation models. The proposed tasks aim to fill this gap.

Conclusion: A comprehensive benchmark is needed to advance AGI's capabilities in Earth observation, and the paper outlines the necessary tasks for such a benchmark.

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [81] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet is a lightweight two-stage network for event-based camera demosaicing, outperforming state-of-the-art methods with fewer parameters and computations.


<details>
  <summary>Details</summary>
Motivation: Challenges in combining Quad Bayer CFA sensors with event pixels lead to aliasing and artifacts, especially on mobile devices.

Method: TSANet uses a two-stage approach with state space augmented cross-attention and a Cross-Swin State Block for demosaicing.

Result: TSANet achieves better PSNR and SSIM scores than DemosaicFormer, reducing parameters and computation costs significantly.

Conclusion: TSANet offers efficient demosaicing for mobile devices, with potential for broader applications.

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [82] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: SCJoint is a joint learning scheme for SOD and COD tasks, using task-specific parameters to decouple contradictory attributes, and SBSS balances training sets. JoNet achieves competitive performance.


<details>
  <summary>Details</summary>
Motivation: Previous works believed joint learning of SOD and COD would confuse networks, but this paper argues the opposite: proper learning can benefit both tasks.

Method: Proposes SCJoint for joint learning with task-specific parameters and SBSS for balanced, high-quality training sets.

Result: JoNet, trained with SCJoint and SBSS, performs competitively in both SOD and COD tasks.

Conclusion: Joint learning of SOD and COD is feasible and beneficial with the right approach, as demonstrated by SCJoint and SBSS.

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [83] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena is a visual animation framework for evaluating LLMs and MLLMs, using point-light imaging to highlight performance gaps. It collects human votes and shows high agreement between crowd-sourced and expert ratings, revealing most models fail at basic humanoid motion tasks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack intuitive feedback on model performance. BioMotion Arena addresses this by leveraging visual perception of motion to provide discriminative evaluation.

Method: The framework uses point-light source imaging and pairwise comparisons, collecting 45k votes on 53 models across 90 biological motion variants.

Result: Over 90% of models, including top ones like InternVL3 and Claude-4, fail at basic humanoid motion tasks. Crowd-sourced votes align well with expert ratings.

Conclusion: BioMotion Arena is a superior, flexible benchmark for visualizing model performance without relying on ground-truth, highlighting significant gaps in current models.

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [84] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: A pipeline for generating high-resolution 3D pseudo-healthy target morphologies from low-resolution MR scans to improve Trochlear Dysplasia treatment.


<details>
  <summary>Details</summary>
Motivation: Current TD treatments rely on low-resolution MR scans and surgeon intuition, leading to inconsistent outcomes. The goal is to provide precise, patient-specific 3D targets for better surgical planning.

Method: Uses Implicit Neural Representation (INR) for super-resolution, a custom-trained network for bone segmentation, and a Wavelet Diffusion Model (WDM) to generate pseudo-healthy morphologies.

Result: Significant improvement in sulcus angle (SA) and trochlear groove depth (TGD) for 25 TD patients.

Conclusion: The pipeline offers a radiation-free, high-resolution solution for preoperative planning, improving surgical outcomes.

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [85] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE is a unified model for instruction-based image and video editing, trained in two stages (image then video) with synthetic data pipelines. It combines collage-based and generative model-based data for diverse editing tasks, achieving strong performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Instruction-based editing for video is limited by scarce training data, hindering practical use. DreamVE addresses this by leveraging scalable image data and unifying image and video editing.

Method: A two-stage training strategy (image then video) with collage-based and generative model-based data synthesis. Uses a token concatenation with early drop approach for consistency and editability.

Result: DreamVE achieves strong performance in key editing types, with enhanced generalization and transfer capabilities, though collage-based data lacks some attribute editing cases.

Conclusion: DreamVE offers a scalable and flexible solution for instruction-based editing, combining synthetic data pipelines and an efficient framework, with plans to release codes and models.

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [86] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo is a unified distillation framework combining trajectory-preserving and distribution-matching to accelerate video generation while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Current distillation methods for video synthesis suffer from performance breakdown or artifacts under few-step settings.

Method: Proposes continuous-time consistency distillation and dual-perspective alignment (distribution and trajectory alignment).

Result: Outperforms existing methods in few-step video generation on the OpenVid-1M benchmark.

Conclusion: SwiftVideo effectively reduces inference steps while preserving high-quality video generation.

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [87] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: AdaptInfer is a plug-and-play framework for adaptive vision token pruning in VLMs, reducing inference costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods in VLMs fail to leverage dynamic internal signals during inference, leading to inefficiency.

Method: Introduces dynamic text-guided pruning using attention maps and an offline analysis of cross-modal attention shifts for efficient pruning.

Result: Reduces CUDA latency by 61.3% while maintaining 92.9% accuracy on LLaVA-1.5-7B, outperforming SOTA under the same token budget.

Conclusion: AdaptInfer is an effective, lightweight, and generalizable solution for improving VLM efficiency.

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [88] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: Q-CLIP is a Vision-Language Model (VLM)-based framework for Video Quality Assessment (VQA) that enhances efficiency and accuracy by using a Shared Cross-Modal Adapter and quality-level prompts, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current VQA methods rely on pretraining on large datasets, which is computationally expensive and insufficient for capturing video quality factors like distortion and aesthetics. VLMs offer a promising alternative.

Method: Q-CLIP uses a Shared Cross-Modal Adapter (SCMA) with minimal trainable parameters and introduces learnable quality-level prompts. It also explores frame-difference-based sampling for better generalization.

Result: Q-CLIP achieves excellent performance on multiple VQA datasets while significantly reducing computational costs.

Conclusion: Q-CLIP demonstrates the potential of VLMs for efficient and accurate VQA, addressing limitations of traditional pretraining methods.

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [89] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: The paper introduces a method for generating diverse human reaction motions in response to emotional cues, using a semi-supervised emotion prior and an actor-reactor diffusion model.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation frameworks ignore emotions, reducing naturalness and limiting applications like human reaction synthesis.

Method: A semi-supervised emotion prior is trained, then integrated into an actor-reactor diffusion model to generate reactions considering spatial interaction and emotional response.

Result: The model outperforms existing reaction generation methods, producing realistic reactions under various emotional conditions.

Conclusion: The approach successfully addresses the challenge of emotion-driven reaction synthesis, with code and data to be made public.

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [90] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: Proposes UGD-IML, a generative framework using diffusion models to unify Image Manipulation Localization (IML) and Constrained IML (CIML) tasks, reducing reliance on large datasets and outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of existing IML methods, which require large annotated datasets, and inefficient CIML pipelines, by leveraging generative diffusion models.

Method: Introduces UGD-IML, a diffusion model-based framework with class embedding and parameter-sharing for seamless IML/CIML switching and end-to-end design.

Result: Outperforms SOTA methods by 9.66 (IML) and 4.36 (CIML) in F1 metrics, with strong uncertainty estimation and robustness.

Conclusion: UGD-IML offers an efficient, unified solution for image forgery detection, reducing annotation dependency and improving performance.

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [91] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: The paper proposes MCA, a robust framework for 2D-3D cross-modal retrieval, addressing noisy labels through multimodal joint correction and adaptive alignment.


<details>
  <summary>Details</summary>
Motivation: Imperfect annotations in 2D-3D data challenge cross-modal retrieval, requiring robust solutions for noisy label conditions.

Method: MCA uses Multimodal Joint label Correction (MJC) for label refinement and Multi-level Adaptive Alignment (MAA) for feature enhancement.

Result: MCA achieves state-of-the-art performance on noisy 3D benchmarks.

Conclusion: MCA is effective and generalizable for robust 2D-3D cross-modal retrieval.

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [92] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: A self-supervised learning framework for handwritten mathematical expression recognition (HMER) using contrastive loss and progressive spatial masking, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: HMER is challenging due to 2D structure, varying scales, and complex spatial relationships; labeled data is expensive.

Method: Self-supervised pretraining with global/local contrastive loss, novel self-supervised attention network with progressive masking, and supervised fine-tuning.

Result: Outperforms SSL and supervised baselines on CROHME benchmarks.

Conclusion: The progressive attention mechanism effectively enhances HMER performance without labeled data.

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [93] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: FMCE-Net++ is a training framework that improves DNN interpretability and performance by integrating FMCE-Net for feature convergence supervision.


<details>
  <summary>Details</summary>
Motivation: Addressing the interpretability and performance limitations of DNNs due to opaque internal representations.

Method: Proposes FMCE-Net++, integrating a frozen FMCE-Net to generate Feature Map Convergence Scores (FMCS) and using a Representation Auxiliary Loss for joint supervision.

Result: Achieves accuracy gains (e.g., +1.16 pp on ResNet-50/CIFAR-10) without architectural changes or extra data.

Conclusion: FMCE-Net++ effectively enhances model performance and interpretability.

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [94] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive introduces a gated Mamba fusion framework for autonomous driving, replacing transformers with efficient state-space models and enhancing LiDAR representation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Transformer-based fusion in diffusion models for autonomous driving faces limitations like quadratic complexity and lack of spatial priors, hindering performance.

Method: GMF-Drive uses a geometrically-augmented LiDAR pillar representation and a hierarchical gated Mamba fusion (GM-Fusion) architecture with state-space models for efficient, spatially-aware processing.

Result: GMF-Drive outperforms DiffusionDrive on the NAVSIM benchmark, demonstrating superior performance and efficiency.

Conclusion: Task-specific state-space models can surpass transformers in autonomous driving, offering better performance and efficiency.

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [95] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: SynSeg introduces Multi-Category Contrastive Learning (MCCL) and Feature Synergy Structure (FSS) to improve weakly-supervised semantic segmentation, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in open-vocabulary semantic segmentation, such as semantic misalignment and poor performance in weakly-supervised settings.

Method: Proposes MCCL for robust intra- and inter-category alignment and FSS for discriminative feature reconstruction.

Result: Achieves higher accuracy than SOTA baselines (e.g., 4.5% on VOC, 8.9% on Context).

Conclusion: SynSeg enhances semantic localization and discrimination under weak supervision, demonstrating superior performance.

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [96] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: The study compared PCA, CAE, and PT for weather event classification using satellite images, finding CAE superior. Higher-resolution data improved deep-learning results, and smaller latent spaces increased false alarms. A physics-informed CAE is suggested for future work.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of representation learning algorithms (PCA, CAE, PT) in classifying weather events from satellite images.

Method: Applied PCA, CAE, and PT to satellite images, evaluated latent spaces via classification tasks, and tested resolution and latent space size impacts.

Result: CAE outperformed PCA and PT in classification tasks. Higher-resolution data improved deep-learning results. Smaller latent spaces (<128) increased false alarms. PT excelled only in tropical cyclone recognition.

Conclusion: CAE is effective but lacks physical interpretability. Future work should focus on physics-informed CAE for better representation learning.

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [97] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: SC-Captioner is a reinforcement learning framework for self-correcting image caption models, using a novel reward function and refined metrics for better caption quality.


<details>
  <summary>Details</summary>
Motivation: Improving image caption models by enabling self-correction capabilities and addressing limitations in existing evaluation metrics.

Method: Decomposes captions into object, attribute, and relation sets, calculates set differences for rewards, and uses refined metrics for assessment.

Result: SC-Captioner outperforms direct preference optimization, generating better captions across diverse scenarios.

Conclusion: The framework effectively enhances caption quality through self-correction and improved evaluation metrics.

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [98] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: VeSCA is a novel method to generate transferable adversarial examples for SAM by identifying shared vulnerabilities through simplicial complexes, improving performance by 12.7% over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: SAM's vulnerabilities pose risks to downstream applications, but existing attacks lack transferability due to insufficient exploration of common weaknesses.

Method: VeSCA uses SAM's encoder to characterize shared vulnerabilities via parametric simplicial complexes, refined iteratively, with domain re-adaptation for minimal reference data.

Result: VeSCA outperforms state-of-the-art methods by 12.7% across three downstream model categories and five datasets.

Conclusion: The study underscores the risks of SAM's vulnerabilities and the need for more robust foundation models.

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [99] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: A new 3D gaze redirection framework using explicit 3D eyeball structure outperforms NeRF-based methods by leveraging 3D Gaussian Splatting and adaptive deformation for realistic results.


<details>
  <summary>Details</summary>
Motivation: Existing gaze redirection methods rely on implicit neural representations, lacking explicit modeling of 3D eyeball rotation and translation.

Method: Uses a dedicated 3D eyeball structure with 3D Gaussian Splatting and an adaptive deformation module for muscle movement replication.

Result: Achieves superior image quality and gaze estimation accuracy on the ETH-XGaze dataset compared to state-of-the-art methods.

Conclusion: The framework effectively generates photorealistic gaze images by explicitly modeling 3D eyeball movements.

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [100] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: A diffusion-based method combines sparse IMUs and a monocular camera for real-time human motion capture, robust to visual occlusions and leveraging IMUs frame-wise for temporal accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in human motion capture, such as occlusions or camera view limitations, by fusing sparse IMUs and monocular camera data seamlessly.

Method: Uses a diffusion model to learn motion priors, transforming sequential visual data into a condition embedding and concatenating IMU measurements frame-wise with noisy poses.

Result: Demonstrates robustness to visual degenerations and achieves state-of-the-art performance in pose estimation.

Conclusion: The proposed framework effectively combines IMUs and camera data, offering a robust solution for real-time motion capture.

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [101] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: SDEval is a dynamic safety evaluation framework for Multimodal Large Language Models (MLLMs) that adjusts benchmark distribution and complexity to address outdated datasets and contamination issues.


<details>
  <summary>Details</summary>
Motivation: Safety concerns in MLLM outputs and the limitations of existing datasets drive the need for a dynamic evaluation framework.

Method: SDEval employs text, image, and text-image dynamics to generate new samples, exploring their individual and combined effects on model safety.

Result: SDEval effectively influences safety evaluation, mitigates data contamination, and reveals MLLM safety limitations across benchmarks.

Conclusion: SDEval provides a flexible and impactful solution for dynamic safety evaluation in MLLMs, applicable to both safety and capability benchmarks.

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [102] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: Prompt-DINO introduces early fusion, order-aligned query selection, and a generative data engine to improve multimodal vision models for open-world segmentation.


<details>
  <summary>Details</summary>
Motivation: Address limitations in late-stage feature fusion, suboptimal query selection, and caption-derived vocabulary constraints in multimodal vision models.

Method: Proposes Prompt-DINO with early fusion, order-aligned query selection, and a generative data engine using the RAP model.

Result: Achieves state-of-the-art performance on open-world detection benchmarks and expands semantic coverage.

Conclusion: Establishes a new scalable paradigm for multimodal detection and data generation in open-world scenarios.

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [103] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: The paper introduces a modular and diversified approach to generating synthetic charts for improving chart understanding in multimodal large language models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs perform poorly (30%-50% success rate) on chart understanding tasks due to inadequate synthetic training data that lacks realism and diversity.

Method: A five-step data synthesis pipeline is designed: modularizing chart generation, diversifying visual details, filtering low-quality data, and generating QA pairs with GPT-4. The resulting dataset (ECD) includes 10k+ charts and 300k+ QA pairs.

Result: ECD improves MLLM performance on real-world and synthetic test sets.

Conclusion: Modular and diversified chart generation enhances chart understanding in MLLMs, as demonstrated by the effectiveness of the ECD dataset.

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [104] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: The paper introduces DSConv, a dynamic kernel splitting method with attention for pansharpening, improving feature extraction and network performance.


<details>
  <summary>Details</summary>
Motivation: To enhance pansharpening by addressing limitations of standard convolutions and leveraging inter-pixel correlations in remote sensing images.

Method: Proposes DSConv, which dynamically splits convolution kernels and uses attention to select positions of interest, improving feature extraction.

Result: DSConv achieves state-of-the-art performance, enhancing generalization, optimization, and feature representation.

Conclusion: DSConv is superior and optimal for pansharpening, validated by comprehensive experiments.

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [105] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR is a user-centric, multi-dimensional benchmark for text-to-image evaluation, combining deterministic metrics and a novel HWPQ scheme for high human alignment and actionable insights.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing metrics in text-to-image evaluation by incorporating user roles and diverse evaluation angles.

Method: Two-tier hybrid paradigm: deterministic metrics for quantifiable attributes and HWPQ for abstract semantics, validated by expert Delphi study and human comparisons.

Result: Achieves >75% human alignment, with HWPQ at 85.9% accuracy; no universal champion model, role-weighted scores reorder rankings.

Conclusion: VISTAR provides reproducible, domain-specific guidance for T2I evaluation, with all resources publicly released.

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [106] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: The paper proposes MPF-KANSC, a deep learning framework combining multi-plane fusion and a novel attention mechanism for improved Alzheimer's disease diagnosis using sMRI.


<details>
  <summary>Details</summary>
Motivation: Early and precise AD diagnosis is challenging due to subtle brain changes. Existing methods often miss complex relationships in brain pathology.

Method: MPF-KANSC integrates multi-plane fusion (coronal, sagittal, axial) and a KANSC attention mechanism for better feature learning.

Result: MPF-KANSC outperforms on the ADNI dataset and reveals right-lateralized asymmetry in subcortical changes.

Conclusion: The framework enhances AD diagnosis accuracy and interpretability, offering insights into disease progression.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [107] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: PostDiff is a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at input and module levels, showing that reducing per-step inference cost is more effective than cutting denoising steps.


<details>
  <summary>Details</summary>
Motivation: High computational demands of diffusion models challenge deployment on resource-limited platforms, prompting investigation into compute-optimal deployment strategies.

Method: Proposes PostDiff, which includes a mixed-resolution denoising scheme for input-level redundancy reduction and a hybrid module caching strategy for module-level efficiency.

Result: PostDiff improves the fidelity-efficiency trade-off of diffusion models, with reducing per-step inference cost proving more effective than reducing denoising steps.

Conclusion: Reducing per-step inference cost is often more effective for efficiency while maintaining generation fidelity, as demonstrated by PostDiff.

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [108] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: UW-3DGS improves underwater 3D scene reconstruction by adapting 3D Gaussian Splatting with a learnable underwater image formation module and Physics-Aware Uncertainty Pruning, outperforming existing methods like SeaThru-NeRF.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like NeRF and its extensions struggle with underwater conditions due to light absorption, scattering, and turbidity, leading to degraded geometry and color fidelity.

Method: UW-3DGS uses a voxel-based regression for underwater image formation and a Physics-Aware Uncertainty Pruning branch to remove noisy Gaussians, optimizing them end-to-end with underwater parameters.

Result: Achieves PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF dataset, with ~65% reduction in floating artifacts.

Conclusion: UW-3DGS offers a robust solution for underwater 3D reconstruction, combining learned physics with efficient Gaussian splatting to overcome challenges in hazy environments.

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [109] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: The paper introduces a multidirectional framework for automating polyp detection in colonoscopy images, combining synthetic data generation with detection and segmentation algorithms. Faster R-CNN and SAM are used for detection and segmentation, achieving high performance metrics.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer is a leading cause of cancer-related deaths, and early detection via colonoscopy is crucial. Automating polyp detection can address dataset limitations and annotation challenges.

Method: The framework uses Stable Diffusion for synthetic data, Faster R-CNN for detection, and SAM for segmentation. Five segmentation models (U-Net, PSPNet, FPN, LinkNet, MANet) are evaluated with ResNet34 as the base.

Result: Faster R-CNN achieved 93.08% recall and 88.97% precision. FPN scored highest in PSNR and SSIM, while U-Net led in recall and LinkNet in IoU and Dice.

Conclusion: The proposed framework effectively automates polyp detection, with FPN and U-Net showing strong performance, aiding in early colorectal cancer diagnosis.

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [110] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: A novel localization framework using flooring features, graph representations, and GCNs achieves high accuracy (0.64cm error) and efficiency, solving the kidnapped robot problem without complex filtering.


<details>
  <summary>Details</summary>
Motivation: Traditional localization methods (e.g., Lidar, QR-codes) lack scalability and adaptability in complex environments.

Method: Uses graph-based representations of floor features and Graph Convolutional Networks (GCNs) for localization.

Result: Achieves 0.64cm localization error and efficiently solves the kidnapped robot problem per frame.

Conclusion: The framework enhances robotic navigation in diverse environments by improving accuracy and adaptability.

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [111] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: MA-CBP is a multi-agent framework for predicting criminal behavior in real-time by analyzing video streams and fusing long- and short-term contexts for early warnings.


<details>
  <summary>Details</summary>
Motivation: Urbanization increases criminal threats; traditional methods lack semantic understanding or real-time capability.

Method: Transforms video streams into semantic descriptions, constructs causal summaries, and fuses frames for joint reasoning.

Result: Outperforms other methods on datasets and enables early criminal activity warnings.

Conclusion: MA-CBP is effective for urban public safety, supported by a high-quality dataset.

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [112] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: Proposed DBIF-AUNet model improves pleural effusion CT image segmentation by addressing gray-level similarity, blurred edges, and variable morphology with dual-branch interaction and feature disentanglement.


<details>
  <summary>Details</summary>
Motivation: Enhancing clinical diagnosis accuracy by overcoming challenges in pleural effusion CT segmentation, such as similar gray levels and complex edges.

Method: Introduces DBIF-AUNet with Dual-Domain Feature Disentanglement (DDFD) and Branch Interaction Attention Fusion (BIAF) modules for multi-scale feature complementarity and dynamic feature fusion.

Result: Achieved IoU of 80.1% and Dice of 89.0%, outperforming U-Net++ and Swin-UNet.

Conclusion: DBIF-AUNet significantly improves segmentation accuracy for complex pleural effusion CT images.

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [113] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: LiLoRA is an efficient architecture expansion method for CVIT in MLLMs, reducing parameter overhead and improving scalability while mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of catastrophic forgetting and parameter inefficiency in continual visual instruction tuning for MLLMs.

Method: Introduces LoRA in LoRA (LiLoRA), sharing LoRA matrix A across tasks, decomposing matrix B for minimal task-specific parameters, and using a cosine-regularized stability loss.

Result: LiLoRA achieves superior performance in sequential task learning with improved parameter efficiency compared to existing methods.

Conclusion: LiLoRA is a scalable and efficient solution for CVIT in MLLMs, balancing performance and parameter overhead.

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [114] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: AnomalyMoE is a universal anomaly detection framework using a Mixture-of-Experts (MoE) architecture to detect diverse anomalies hierarchically, outperforming specialized methods.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods are specialized and lack generalizability across domains and anomaly types.

Method: AnomalyMoE decomposes anomaly detection into three semantic hierarchies (local, component, global) using dedicated expert networks, with EIR and ESB modules for diversity and balance.

Result: AnomalyMoE achieves state-of-the-art performance on 8 diverse datasets, surpassing specialized methods in their domains.

Conclusion: AnomalyMoE offers a universal, hierarchical approach to anomaly detection, improving generalizability and performance.

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [115] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: The paper introduces the PA-HOI Motion Capture dataset to study how objects' physical properties affect human motion, addressing gaps in existing HOI datasets.


<details>
  <summary>Details</summary>
Motivation: Existing HOI datasets overlook the impact of objects' physical attributes on human motion, limiting understanding in fields like robotics and VR.

Method: The PA-HOI dataset includes 562 motion sequences with diverse human subjects interacting with 35 objects of varying size, shape, and weight.

Result: The dataset enhances understanding of how object attributes influence human posture, speed, and interaction strategies.

Conclusion: The PA-HOI dataset successfully bridges a gap in HOI research and proves useful for motion generation methods, offering realistic physical awareness.

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [116] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: The paper proposes a two-stage pipeline for automated SvdH score prediction in RA using dual-hand radiographs, achieving accuracy comparable to radiologists.


<details>
  <summary>Details</summary>
Motivation: The complexity of manual SvdH scoring limits its clinical use, prompting the need for an efficient automated method.

Method: A two-stage pipeline extracts disease-relevant regions and uses attention-based multiple instance learning for prediction, with two region extraction schemes.

Result: Best model achieved PCC of 0.943 and RMSE of 15.73; ensemble learning improved to PCC 0.945 and RMSE 15.57, matching radiologist performance.

Conclusion: The pipeline is effective, interpretable, and identifies clinically relevant anatomical structures for RA progression.

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [117] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: Proposes TEFormer, a texture-aware and edge-guided Transformer for semantic segmentation of urban remote sensing images, addressing challenges like subtle texture differences and complex edge morphologies.


<details>
  <summary>Details</summary>
Motivation: Urban remote sensing images (URSIs) have subtle texture differences and complex edge morphologies, leading to semantic ambiguity and misclassification.

Method: TEFormer integrates a texture-aware module (TaM) for fine-grained texture differences and an edge-guided tri-branch decoder (Eg3Head) for multiscale context-awareness. An edge-guided feature fusion module (EgFFM) refines segmentation.

Result: Achieves mIoU of 88.57%, 81.46%, and 53.55% on Potsdam, Vaihingen, and LoveDA datasets, respectively.

Conclusion: TEFormer effectively addresses challenges in URSI semantic segmentation, demonstrating superior performance.

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [118] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: Depth-Jitter is a depth-based augmentation technique that improves model robustness in depth-sensitive environments by simulating natural depth variations.


<details>
  <summary>Details</summary>
Motivation: Conventional augmentation techniques lack depth awareness, limiting model robustness in real-world depth variations.

Method: Depth-Jitter applies adaptive depth offsetting guided by depth variance thresholds to generate synthetic depth perturbations while preserving structural integrity.

Result: Depth-Jitter enhances model stability and generalization in depth-sensitive environments, though it doesn't always outperform traditional methods in absolute performance.

Conclusion: Depth-aware augmentation like Depth-Jitter has potential for real-world applications and lays groundwork for further research in depth-based learning strategies.

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [119] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: An all-in-one image deblurring method using a mixture-of-experts (MoE) decoding module to handle diverse blur types efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack generalization, requiring multiple models for different blur types, which is impractical.

Method: Proposes a mixture-of-experts (MoE) decoding module to dynamically route features based on blur type for precise restoration.

Result: Achieves performance comparable to task-specific models and shows robustness on unseen blur scenarios.

Conclusion: The unified approach is efficient, generalizable, and practical for diverse blur degradations.

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [120] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: LNCLIP-DF, a parameter-efficient adaptation of CLIP, achieves state-of-the-art deepfake detection by fine-tuning only Layer Normalization parameters and using L2 normalization. It outperforms complex methods across 13 datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing deepfake detectors to unseen manipulation techniques without excessive architectural complexity.

Method: Fine-tunes only Layer Normalization parameters (0.03% of total) in a pre-trained CLIP model, enforces hyperspherical feature manifold with L2 normalization and latent space augmentations.

Result: Achieves top performance in cross-dataset AUROC, outperforming recent complex methods. Key findings: paired real-fake data is crucial, and older datasets generalize well.

Conclusion: Targeted, minimal changes to CLIP can achieve robust generalization, offering a computationally efficient and reproducible solution.

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [121] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

TL;DR: FedX reduces communication overhead in federated learning for remote sensing tasks by pruning less important model components, maintaining performance while enhancing efficiency.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) is constrained by communication overhead in remote sensing (RS) tasks due to large model updates. FedX aims to minimize this overhead without sacrificing performance.

Method: FedX uses explanation-guided pruning to identify and remove less relevant model components, reducing the size of transmitted models. It leverages backpropagation-based explanation methods for task-specific importance estimation.

Result: FedX significantly reduces shared model parameters and improves generalization, outperforming unpruned models and state-of-the-art pruning methods on BigEarthNet-S2 and EuroSAT datasets.

Conclusion: FedX effectively addresses communication overhead in FL for RS tasks, offering a practical solution with improved efficiency and performance.

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [122] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: XAG-Net, a 2.5D U-Net-based model with cross-slice attention and skip attention gating, outperforms existing methods in femur MRI segmentation.


<details>
  <summary>Details</summary>
Motivation: Accurate femur segmentation from MRI is crucial for orthopedic diagnosis and surgery but is hindered by limitations in current 2D/3D deep learning approaches.

Method: Proposes XAG-Net, incorporating pixel-wise cross-slice attention (CSA) and skip attention gating (AG) for better inter-slice and intra-slice feature modeling.

Result: XAG-Net outperforms 2D, 2.5D, and 3D U-Net baselines in accuracy while remaining computationally efficient.

Conclusion: XAG-Net is a promising framework for efficient and accurate femur MRI segmentation, validated by ablation studies.

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [123] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: SIFThinker is a spatially-aware framework for MLLMs that improves visual tasks by correcting attention with spatial cues and focusing on relevant regions. It introduces a dataset (SIF-50K) and a training paradigm (GRPO-SIF), outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex visual tasks like spatial understanding and fine-grained perception due to inadequate attention correction and region focusing.

Method: SIFThinker uses depth-enhanced bounding boxes and natural language for attention correction. It employs a reverse-expansion-forward-inference strategy and GRPO-SIF, a reinforced training paradigm.

Result: SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained perception while maintaining general capabilities.

Conclusion: The framework effectively addresses visual task challenges by mimicking human perception, demonstrating superior performance and versatility.

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [124] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: A method called URPA is introduced for cross-domain video temporal grounding without labeled target data, using uncertainty-quantified rollouts for efficient adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing methods like GRPO require labeled data and are computationally expensive, making them impractical for unlabeled domains and real-time use.

Method: URPA leverages GRPO rollouts to generate pseudo labels and uses variance-based confidence to weight training rewards, enabling adaptation with minimal unlabeled data.

Result: URPA generalizes well across six cross-domain settings using only a few unlabeled target videos.

Conclusion: URPA provides a data-efficient, real-time solution for cross-domain video temporal grounding without target labels.

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [125] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: GS-MoE introduces a Gaussian Splatting-guided Mixture of Experts framework for Weakly-Supervised Video Anomaly Detection, improving performance by addressing diversity and weak supervision issues.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with complex anomalies due to shared processing and weak supervision signals lacking temporal precision.

Method: GS-MoE uses specialized expert models guided by temporal Gaussian splatting loss to capture specific anomaly types and enhance weak supervision.

Result: Achieves 91.58% AUC on UCF-Crime and superior performance on XD-Violence and MSAD datasets.

Conclusion: GS-MoE sets a new benchmark for VAD under weak supervision by leveraging category-specific expertise and temporal guidance.

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [126] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: A diffusion model generates synthetic cardiac MR images to address domain shift, improving segmentation performance on unseen domains without needing transfer learning.


<details>
  <summary>Details</summary>
Motivation: Domain shift in MR imaging limits AI model deployment; synthetic data offers a solution but faces anatomical consistency challenges.

Method: A diffusion model creates synthetic cardiac MR images resembling a reference, ensuring structural fidelity. Evaluated using 2D/3D nnU-Net and U-Net for domain generalization and adaptation.

Result: Significant improvement in segmentation performance on unseen domains (p < 0.01) compared to real data training.

Conclusion: The method effectively addresses domain shift in cardiac MR, eliminating the need for transfer learning, especially in data-scarce scenarios.

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [127] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: The paper improves ViPro by enabling unsupervised state inference from observations without initial ground truth, addressing shortcuts in learning and extending the Orbits dataset to 3D.


<details>
  <summary>Details</summary>
Motivation: Previous work (ViPro) relied on ground truth initial symbolic states, leading to shortcuts in learning and poor performance with noisy states. This work aims to correct these issues.

Method: The authors enhance ViPro to infer states from observations without initial ground truth, using unsupervised learning. They also introduce a 3D variant of the Orbits dataset.

Result: The improved model successfully infers states from observations without ground truth, demonstrating robustness to noise.

Conclusion: The enhancements enable more reliable state inference in complex dynamical settings, bridging the gap to real-world applications.

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [128] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

TL;DR: Street view imagery can infer social interaction quality, aligning with urban planning theories, using multimodal models and regression analysis.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on pedestrian volume, not interaction quality. Street view imagery offers a scalable, global data source to address this gap.

Method: Analyzed 2,998 street view images from 15 cities using a multimodal model guided by Mehta's sociability taxonomy, with regression models controlling for variables like weather and pedestrian counts.

Result: Sky view index linked to all sociability types; green view index predicted enduring sociability; place attachment correlated with fleeting sociability.

Conclusion: Street view imagery shows promise for scalable, privacy-preserving urban sociability research, supporting evidence-based city design.

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [129] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: VA-GPT, a novel MLLM, improves abnormal event analysis in videos using spatial and temporal token modules (SETS & TETG) and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with video anomalies due to spatial/temporal sparsity and redundant information, limiting their effectiveness.

Method: Proposes VA-GPT with SETS and TETG modules to align visual and language tokens, enhancing anomaly detection. Uses a custom dataset for fine-tuning.

Result: Outperforms state-of-the-art methods on benchmarks, demonstrating accurate anomaly summarization and localization.

Conclusion: VA-GPT effectively addresses challenges in video anomaly analysis, offering improved performance and cross-domain applicability.

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [130] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

TL;DR: Implementation of a two-phase image segmentation algorithm based on Goldstein, Bresson, and Osher's method, modifying Chan-Vese energy for efficient minimization using split Bregman.


<details>
  <summary>Details</summary>
Motivation: To partition 2D images into foreground and background regions efficiently, assuming pixel values can be summarized by two distinct averages and smooth boundaries.

Method: Modifies Chan-Vese energy to enable efficient minimization via split Bregman, assigning pixel membership to regions.

Result: Detailed implementation and performance documentation across various images and algorithm parameters.

Conclusion: The method effectively segments images into two regions with smooth boundaries, validated by empirical performance.

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [131] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: The paper proposes a new method for detecting Out-of-gallery cases in one-to-many facial identification by using additional enrolled images to train a classifier, improving accuracy across various probe conditions and demographics.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on similarity score thresholds, which may not effectively distinguish In-gallery from Out-of-gallery cases, leading to false positives and wasted investigative efforts.

Method: The approach involves generating training data from ranks of additional enrolled images, training a classifier to predict In-gallery/Out-of-gallery status, and validating it across datasets and matchers.

Result: The method works well for mugshot and degraded probe images, with consistent accuracy across demographics, and is effective only with advanced matchers.

Conclusion: This approach reduces false positives and wrongful arrests, highlighting the importance of advanced matchers for Out-of-gallery detection.

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [132] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: TaAM-CPT is a scalable method for general representation learning across unlimited modalities using only text data, achieving top results without modality-specific labels.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on massive labeled data or are limited to single modalities, prompting the need for a more scalable and generalizable approach.

Method: TaAM-CPT uses modality prompt pools, text construction, and modality-aligned encoders, with intra- and inter-modal learning objectives for consistency.

Result: TaAM-CPT achieves leading performance on diverse datasets (video, image, audio classification) without modality-specific labeled data.

Conclusion: TaAM-CPT offers a scalable, generalizable solution for multimodal learning, leveraging text data and pre-trained models effectively.

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [133] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: FVGen accelerates novel view synthesis using VDMs, reducing sampling steps to four while maintaining quality, cutting time by 90%.


<details>
  <summary>Details</summary>
Motivation: Sparse views in 3D reconstruction cause artifacts; current VDMs are slow. FVGen aims to speed up synthesis without sacrificing quality.

Method: Distills a multi-step VDM into a few-step model using GANs and softened reverse KL-divergence minimization.

Result: Generates novel views with similar/better quality in 90% less time than prior methods.

Conclusion: FVGen enhances efficiency for 3D reconstruction with sparse views, making VDMs practical for real-world use.

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [134] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: The paper explores integrating classification objectives into super-resolution (SR) to improve both image quality and classification accuracy, proposing a novel method for synthetic aperture radar imagery.


<details>
  <summary>Details</summary>
Motivation: Low-resolution images limit automated analysis accuracy. Traditional SR methods focus on pixel-level metrics, neglecting the impact on downstream classification tasks.

Method: A novel SR methodology optimizes loss functions for both image quality and classification performance, applied to synthetic aperture radar imagery.

Result: The approach improves image quality and enhances classification accuracy.

Conclusion: Integrating classification objectives into SR can improve both image fidelity and downstream task performance.

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [135] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: The paper evaluates oversampling in feature space for SAR ship classification, proposing two novel algorithms (M2m$_f$, M2m$_u$) to address class imbalance. Tested on OpenSARShip and FuSARShip datasets, the methods outperformed baselines, improving F1-scores significantly.


<details>
  <summary>Details</summary>
Motivation: Addressing class imbalance in SAR ship classification, particularly for underrepresented classes, using oversampling methods.

Method: Proposed two novel algorithms (M2m$_f$, M2m$_u$) inspired by M2m, tested on OpenSARShip and FuSARShip datasets using ViT, VGG16, and ResNet50 as feature extractors. Analyzed oversampling impact on class sizes.

Result: Novel methods outperformed original M2m and baselines, with F1-score increases of 8.82% (FuSARShip) and 4.44% (OpenSARShip).

Conclusion: The proposed oversampling methods effectively improve SAR ship classification performance, especially for underrepresented classes.

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [136] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: A GAN-based semi-supervised learning framework for medical imaging improves classification with minimal labeled data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning in medical imaging is limited by scarce labeled data. This paper addresses the challenge of low labeled-data regimes.

Method: Uses a three-network framework (generator, discriminator, classifier) with supervised and unsupervised training, leveraging image-to-image translation and ensemble pseudo-labeling.

Result: Achieves significant improvements over six GAN-based methods, especially in extreme 5-shot settings, across eleven MedMNIST datasets.

Conclusion: The framework provides a practical solution for medical imaging with high annotation costs, enabling robust performance with minimal labeled data.

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [137] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: The paper enhances SimSwap for high-fidelity face swapping with improved identity preservation and visual quality using attention mechanisms, dynamic loss weighting, and learning rate scheduling.


<details>
  <summary>Details</summary>
Motivation: To advance face swapping technology by improving identity preservation, attribute consistency, and visual quality in the SimSwap framework.

Method: Integrates self and cross-attention mechanisms, dynamic loss weighting, and cosine annealing learning rate scheduling into the SimSwap model.

Result: Achieves better identity similarity, lower FID scores, and superior qualitative results compared to the baseline.

Conclusion: Future directions include integrating StyleGAN3, improving lip sync, 3D facial modeling, and temporal consistency for videos.

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [138] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: CLIPin is a non-contrastive plug-in for CLIP-style models to improve multimodal alignment and robustness, tested successfully on diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing weak supervision in natural image-text datasets and low diversity in medical datasets, which hinder CLIP's representation learning.

Method: Introduces CLIPin, a unified plug-in with shared pre-projectors for image and text, combining contrastive and non-contrastive learning.

Result: CLIPin enhances alignment robustness and generalizability, validated through extensive downstream tasks.

Conclusion: CLIPin is an effective, plug-and-play solution for improving CLIP-style models.

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [139] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: TRUST is a novel UDA method leveraging language modality to guide vision model adaptation, using pseudo-labels from captions and uncertainty estimation to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing complex domain shifts (e.g., geographical) where traditional UDA methods fail, by utilizing the robustness of language modality.

Method: Generates pseudo-labels from captions, estimates uncertainty using CLIP similarity, and employs multimodal soft-contrastive learning to align vision and language features.

Result: Outperforms prior methods, achieving state-of-the-art on DomainNet and GeoNet benchmarks.

Conclusion: TRUST effectively mitigates adverse effects of wrong pseudo-labels and enhances vision model robustness through multimodal alignment.

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [140] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: The study integrates LLMs with the Swin-UMamba architecture for lesion segmentation, achieving high accuracy and outperforming prior models.


<details>
  <summary>Details</summary>
Motivation: To enhance lesion segmentation by combining imaging features with text descriptions from radiology reports.

Method: Used the Swin-UMamba architecture integrated with text from reports, tested on the ULS23 DeepLesion dataset.

Result: Achieved an 82% Dice Score and 6.58 Hausdorff distance, outperforming prior models significantly.

Conclusion: Text-Swin-UMamba is effective for lesion segmentation, combining text and imaging data for improved results.

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [141] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: WGAST is a weakly-supervised deep learning framework for estimating daily 10m Land Surface Temperature (LST) by fusing Terra MODIS, Landsat 8, and Sentinel-2 data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The demand for precise environmental monitoring is growing due to urbanization, climate change, and agricultural stress. Existing LST retrieval systems face a trade-off between spatial and temporal resolution.

Method: WGAST uses a conditional GAN with a four-stage generator: feature extraction, fusion, LST reconstruction, and noise suppression. It employs weakly supervised training and a PatchGAN discriminator.

Result: WGAST reduces RMSE by 17.18% and improves SSIM by 11.00% compared to baselines, showing robustness to cloud-induced LST and capturing fine-scale thermal patterns.

Conclusion: WGAST is an effective solution for high-resolution daily LST estimation, validated by ground-based sensors, with code publicly available.

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [142] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: LightSwitch is a novel framework for 3D relighting that integrates multi-view and material cues, outperforming prior methods in efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: Existing 2D relighting methods lack utilization of intrinsic properties and multi-view data, leading to subpar results.

Method: LightSwitch uses a finetuned material-relighting diffusion framework with scalable denoising and multi-view cues.

Result: It achieves superior 2D relighting quality and matches or outperforms state-of-the-art methods in relighting synthetic and real objects.

Conclusion: LightSwitch efficiently and consistently relights dense multi-view data, advancing 3D relighting capabilities.

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [143] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: AEPO improves semantic alignment in MLLMs for GUI tasks by enhancing exploration, outperforming RLVR by 9.0%.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficient exploration in semantic alignment for GUI-based MLLMs.

Method: Introduces AEPO with multi-answer generation and Adaptive Exploration Reward (AER).

Result: AEPO-trained models achieve state-of-the-art results, improving by up to 9.0% over RLVR.

Conclusion: AEPO effectively tackles exploration bottlenecks, advancing GUI grounding in MLLMs.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [144] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: DINA is a unified framework for NLP that defends against internal label corruption and external adversarial attacks, improving model robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address adversarial threats from external manipulations and internal label corruption in LLMs and generative AI used in customer service and moderation.

Method: Integrates noisy-label learning from computer vision with adversarial training to mitigate dual threats.

Result: DINA significantly improves robustness and accuracy on a real-world dataset compared to baselines.

Conclusion: Highlights the need for dual-threat defenses and offers practical strategies for fair and responsible AI deployment.

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [145] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: DMFI is a dual-modality framework for insider threat detection, combining semantic inference and behavior-aware fine-tuning to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional models fail to capture semantic intent and behavior dynamics, while LLM-based solutions lack prompt adaptability and modality coverage.

Method: DMFI processes raw logs into semantic and behavioral views, fine-tunes two LoRA-enhanced LLMs, and fuses outputs via an MLP-based decision module. DMFI-B improves robustness under class imbalance.

Result: DMFI outperforms state-of-the-art methods on CERT r4.2 and r5.2 datasets.

Conclusion: Combining LLM reasoning with structured behavioral modeling provides a scalable and effective solution for insider threat detection.

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [146] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: Fact2Fiction is a poisoning attack framework targeting LLM-based fact-checking systems, achieving higher success rates than existing attacks and exposing security vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The security of autonomous fact-checking systems is critical, as compromised systems can amplify misinformation. Current systems are underexplored in terms of vulnerabilities.

Method: Fact2Fiction mirrors the decomposition strategy of fact-checking systems and exploits system-generated justifications to craft malicious evidence for sub-claim verification.

Result: Fact2Fiction achieves 8.9%–21.2% higher attack success rates than state-of-the-art attacks across various poisoning budgets.

Conclusion: The framework exposes security weaknesses in fact-checking systems and underscores the need for defensive measures.

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [147] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: ScamAgent, an LLM-based agent, creates realistic scam call scripts, bypassing current safety measures, highlighting the need for improved safeguards.


<details>
  <summary>Details</summary>
Motivation: Address the misuse potential of LLMs in generating deceptive, multi-turn scam scripts.

Method: Develop ScamAgent with dialogue memory, dynamic adaptation, and deceptive strategies to simulate fraud scenarios.

Result: Current LLM safety measures fail against agent-based threats; scam scripts can be converted into lifelike voice calls.

Conclusion: Urgent need for multi-turn safety auditing, agent-level controls, and methods to detect AI-powered deception.

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


### [148] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: The paper introduces U3-Attack, a scalable and efficient multimodal jailbreak method to bypass Text-to-Image (T2I) safeguards, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods are limited by poor scalability and inefficiency, prompting the need for a more universal and optimized attack.

Method: U3-Attack optimizes an adversarial patch for image backgrounds and a safe paraphrase set for text prompts to bypass safety measures.

Result: U3-Attack achieves ~4× higher success rates than state-of-the-art methods on commercial T2I models.

Conclusion: The proposed U3-Attack effectively exposes vulnerabilities in T2I safeguards, demonstrating superior performance over existing jailbreak techniques.

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [149] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: ATP introduces tamper-proof perturbations to protect images from forgery attacks and detect purification-based tampering, ensuring robust defense.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about forgery attacks infringing on portrait rights and privacy, and the ineffectiveness of existing protection algorithms against purification techniques.

Method: ATP combines protection and authorization perturbations in the frequency domain, guided by a mask, to defend against forgery and detect tampering.

Result: ATP effectively defends against forgery attacks in various settings, providing a robust solution for privacy and rights protection.

Conclusion: ATP offers a novel and effective approach to safeguard portrait rights and privacy against evolving forgery techniques.

Abstract: With the advancement of personalized image generation technologies, concerns
about forgery attacks that infringe on portrait rights and privacy are growing.
To address these concerns, protection perturbation algorithms have been
developed to disrupt forgery generation. However, the protection algorithms
would become ineffective when forgery attackers apply purification techniques
to bypass the protection. To address this issue, we present a novel approach,
Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within
the perturbation. It consists of protection and authorization perturbations,
where the protection perturbation defends against forgery attacks, while the
authorization perturbation detects purification-based tampering. Both
protection and authorization perturbations are applied in the frequency domain
under the guidance of a mask, ensuring that the protection perturbation does
not disrupt the authorization perturbation. This design also enables the
authorization perturbation to be distributed across all image pixels,
preserving its sensitivity to purification-based tampering. ATP demonstrates
its effectiveness in defending forgery attacks across various attack settings
through extensive experiments, providing a robust solution for protecting
individuals' portrait rights and privacy. Our code is available at:
https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [150] [Neural Field-Based 3D Surface Reconstruction of Microstructures from Multi-Detector Signals in Scanning Electron Microscopy](https://arxiv.org/abs/2508.04728)
*Shuo Chen,Yijin Li,Xi Zheng,Guofeng Zhang*

Main category: eess.IV

TL;DR: NFH-SEM is a neural field-based hybrid method for 3D SEM reconstruction, eliminating manual calibration and shadow errors, validated on diverse samples.


<details>
  <summary>Details</summary>
Motivation: Conventional 2D SEM images lack 3D topography, and existing methods struggle with complex microstructures due to calibration needs and shadow errors.

Method: NFH-SEM uses multi-view, multi-detector 2D SEM images, fuses geometric and photometric data into a neural field, and self-calibrates end-to-end.

Result: High-fidelity reconstructions of complex samples like microstructures, pollen, and particle surfaces were achieved.

Conclusion: NFH-SEM enables accurate, calibration-free 3D SEM reconstruction with broad applicability.

Abstract: The scanning electron microscope (SEM) is a widely used imaging device in
scientific research and industrial applications. Conventional two-dimensional
(2D) SEM images do not directly reveal the three-dimensional (3D) topography of
micro samples, motivating the development of SEM 3D surface reconstruction
methods. However, reconstruction of complex microstructures remains challenging
for existing methods due to the limitations of discrete 3D representations, the
need for calibration with reference samples, and shadow-induced gradient
errors. Here, we introduce NFH-SEM, a neural field-based hybrid SEM 3D
reconstruction method that takes multi-view, multi-detector 2D SEM images as
input and fuses geometric and photometric information into a continuous neural
field representation. NFH-SEM eliminates the manual calibration procedures
through end-to-end self-calibration and automatically disentangles shadows from
SEM images during training, enabling accurate reconstruction of intricate
microstructures. We validate the effectiveness of NFH-SEM on real and simulated
datasets. Our experiments show high-fidelity reconstructions of diverse,
challenging samples, including two-photon lithography microstructures, peach
pollen, and silicon carbide particle surfaces, demonstrating precise detail and
broad applicability.

</details>


### [151] [Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework](https://arxiv.org/abs/2508.06137)
*Ojonugwa Oluwafemi Ejiga Peter,Daniel Emakporuena,Bamidele Dayo Tunde,Maryam Abdulkarim,Abdullahi Bn Umar*

Main category: eess.IV

TL;DR: MammoFormer combines transformers with feature enhancements and XAI to improve breast cancer detection in mammograms, outperforming CNNs.


<details>
  <summary>Details</summary>
Motivation: Breast cancer detection via mammography is challenging due to subtle abnormalities and variability in expert interpretations. CNNs lack contextual processing and explainability for clinical adoption.

Method: Developed MammoFormer, integrating transformer architectures (ViT, Swin, ConvNext) with multi-feature enhancements (negative transformation, AHE, HOG) and XAI. Tested seven architectures and four enhancement techniques.

Result: Achieved up to 13% performance improvement. ViT with AHE reached 98.3% accuracy; Swin Transformer with HOG gained 13% advantage over CNNs.

Conclusion: MammoFormer addresses clinical adoption barriers by combining transformer global context with CNN reliability and XAI, enabling superior performance in mammography.

Abstract: Breast cancer detection through mammography interpretation remains difficult
because of the minimal nature of abnormalities that experts need to identify
alongside the variable interpretations between readers. The potential of CNNs
for medical image analysis faces two limitations: they fail to process both
local information and wide contextual data adequately, and do not provide
explainable AI (XAI) operations that doctors need to accept them in clinics.
The researcher developed the MammoFormer framework, which unites
transformer-based architecture with multi-feature enhancement components and
XAI functionalities within one framework. Seven different architectures
consisting of CNNs, Vision Transformer, Swin Transformer, and ConvNext were
tested alongside four enhancement techniques, including original images,
negative transformation, adaptive histogram equalization, and histogram of
oriented gradients. The MammoFormer framework addresses critical clinical
adoption barriers of AI mammography systems through: (1) systematic
optimization of transformer architectures via architecture-specific feature
enhancement, achieving up to 13% performance improvement, (2) comprehensive
explainable AI integration providing multi-perspective diagnostic
interpretability, and (3) a clinically deployable ensemble system combining CNN
reliability with transformer global context modeling. The combination of
transformer models with suitable feature enhancements enables them to achieve
equal or better results than CNN approaches. ViT achieves 98.3% accuracy
alongside AHE while Swin Transformer gains a 13.0% advantage through HOG
enhancements

</details>


### [152] [Clinically-guided Data Synthesis for Laryngeal Lesion Detection](https://arxiv.org/abs/2508.06182)
*Chiara Baldini,Kaisar Kushibar,Richard Osuala,Simone Balocco,Oliver Diaz,Karim Lekadir,Leonardo S. Mattos*

Main category: eess.IV

TL;DR: The paper proposes a Latent Diffusion Model (LDM) with a ControlNet adapter to generate synthetic laryngeal endoscopic images, addressing data scarcity for CADx/e systems in otorhinolaryngology. Synthetic data improved lesion detection rates and was validated by experts.


<details>
  <summary>Details</summary>
Motivation: Current CADx/e systems in otorhinolaryngology suffer from limited annotated datasets and reliance on biopsies. Data scarcity hinders real-world generalization.

Method: Uses an LDM coupled with a ControlNet adapter to generate realistic laryngeal endoscopic image-annotation pairs, guided by clinical observations.

Result: Adding 10% synthetic data improved lesion detection rates (9% internally, 22.1% externally). Experts struggled to distinguish synthetic from real images.

Conclusion: The method effectively addresses data scarcity, enhances CADx/e performance, and demonstrates the potential of synthetic data in medical diagnostics.

Abstract: Although computer-aided diagnosis (CADx) and detection (CADe) systems have
made significant progress in various medical domains, their application is
still limited in specialized fields such as otorhinolaryngology. In the latter,
current assessment methods heavily depend on operator expertise, and the high
heterogeneity of lesions complicates diagnosis, with biopsy persisting as the
gold standard despite its substantial costs and risks. A critical bottleneck
for specialized endoscopic CADx/e systems is the lack of well-annotated
datasets with sufficient variability for real-world generalization. This study
introduces a novel approach that exploits a Latent Diffusion Model (LDM)
coupled with a ControlNet adapter to generate laryngeal endoscopic
image-annotation pairs, guided by clinical observations. The method addresses
data scarcity by conditioning the diffusion process to produce realistic,
high-quality, and clinically relevant image features that capture diverse
anatomical conditions. The proposed approach can be leveraged to expand
training datasets for CADx/e models, empowering the assessment process in
laryngology. Indeed, during a downstream task of detection, the addition of
only 10% synthetic data improved the detection rate of laryngeal lesions by 9%
when the model was internally tested and 22.1% on out-of-domain external data.
Additionally, the realism of the generated images was evaluated by asking 5
expert otorhinolaryngologists with varying expertise to rate their confidence
in distinguishing synthetic from real images. This work has the potential to
accelerate the development of automated tools for laryngeal disease diagnosis,
offering a solution to data scarcity and demonstrating the applicability of
synthetic data in real-world scenarios.

</details>


### [153] [Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification](https://arxiv.org/abs/2508.06287)
*Mobarak Abumohsen,Enrique Costa-Montenegro,Silvia García-Méndez,Amani Yousef Owda,Majdi Owda*

Main category: eess.IV

TL;DR: The paper proposes a DenseNet201-based method for lung cancer detection from CT images, addressing data imbalance and overfitting with Focal Loss, augmentation, and regularization, achieving 98.95% accuracy.


<details>
  <summary>Details</summary>
Motivation: Lung cancer is a leading cause of death, and existing CT-based detection methods suffer from high false positives due to small, imbalanced datasets.

Method: Uses DenseNet201 with Focal Loss, data augmentation, and regularization to handle imbalance and overfitting.

Result: Achieves 98.95% accuracy in lung cancer detection.

Conclusion: The proposed method effectively addresses dataset challenges and improves detection accuracy.

Abstract: Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one
of the most common causes of death for men and women worldwide. Computed
Tomography (CT) images are the most preferred diagnosis method because of their
low cost and their faster processing times. Many researchers have proposed
various ways of identifying lung cancer using CT images. However, such
techniques suffer from significant false positives, leading to low accuracy.
The fundamental reason results from employing a small and imbalanced dataset.
This paper introduces an innovative approach for LC detection and
classification from CT images based on the DenseNet201 model. Our approach
comprises several advanced methods such as Focal Loss, data augmentation, and
regularization to overcome the imbalanced data issue and overfitting challenge.
The findings show the appropriateness of the proposal, attaining a promising
performance of 98.95% accuracy.

</details>


### [154] [Multivariate Fields of Experts](https://arxiv.org/abs/2508.06490)
*Stanislas Ducotterd,Michael Unser*

Main category: eess.IV

TL;DR: A new framework for learning image priors using multivariate fields of experts, outperforming univariate models and nearing deep-learning performance with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To generalize existing fields of experts methods by incorporating multivariate potential functions for improved image prior learning.

Method: Uses Moreau envelopes of the ℓ∞-norm to construct multivariate potential functions, applied to inverse problems like denoising and deblurring.

Result: Outperforms univariate models, achieves near-deep-learning performance, and is faster with fewer parameters and data requirements.

Conclusion: The proposed model is effective, efficient, and retains interpretability due to its structured design.

Abstract: We introduce the multivariate fields of experts, a new framework for the
learning of image priors. Our model generalizes existing fields of experts
methods by incorporating multivariate potential functions constructed via
Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of
our proposal across a range of inverse problems that include image denoising,
deblurring, compressed-sensing magnetic-resonance imaging, and computed
tomography. The proposed approach outperforms comparable univariate models and
achieves performance close to that of deep-learning-based regularizers while
being significantly faster, requiring fewer parameters, and being trained on
substantially fewer data. In addition, our model retains a relatively high
level of interpretability due to its structured design.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [155] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: The study explores how ethical AI principles impact user satisfaction by analyzing 100,000+ user reviews. Findings show positive associations between ethical dimensions and satisfaction, with variations based on user and product types.


<details>
  <summary>Details</summary>
Motivation: To empirically assess whether ethical AI principles (fairness, transparency, robustness) are recognized and valued by users, given their widespread endorsement in policies and guidelines.

Method: Analyzed user reviews of AI products using transformer-based language models to measure sentiment across seven ethical dimensions from the EU Ethics Guidelines for Trustworthy AI.

Result: All seven ethical dimensions positively correlate with user satisfaction, with stronger associations for non-technical users and end-user applications. Technical users focus on system-level concerns, while non-technical users emphasize human-centric issues.

Conclusion: Ethical AI design must consider user perspectives and contextual differences across roles and product types to enhance satisfaction and trust.

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [156] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: ThematicPlane is a system for intuitive, semantics-driven control of generative AI outputs, aiding non-experts in aligning creative intent with results.


<details>
  <summary>Details</summary>
Motivation: Aligning generative AI outputs with nuanced creative intent is challenging, especially for non-experts, due to limitations in existing tools.

Method: ThematicPlane allows users to navigate and manipulate high-level semantic concepts (e.g., mood, style) via an interactive thematic design plane.

Result: An exploratory study (N=6) showed users embraced unexpected results and highlighted the need for explainable controls.

Conclusion: ThematicPlane supports expressive, iterative workflows and suggests new directions for intuitive generative design tools.

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [157] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: The paper discusses the need for AI-driven coding systems to provide clear justifications for their decisions to improve trust and usability, especially for non-expert users. It proposes neuro-symbolic approaches to achieve this.


<details>
  <summary>Details</summary>
Motivation: The opaque decision-making of AI-driven coders raises trust and usability concerns, particularly for non-expert users who cannot inspect low-level implementations.

Method: The paper identifies two justification properties (cognitive alignment and semantic faithfulness) and advocates for neuro-symbolic approaches, combining symbolic constraints and neural representations for automated consistency checks.

Result: The limitations of existing methods (formal verification, static analysis, post-hoc explainability) are highlighted.

Conclusion: Neuro-symbolic approaches are proposed as a solution to generate clear, consistent justifications, bridging model reasoning and user understanding.

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [158] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: A novel method integrates vision foundation models (SAM and YOLOv5) with reinforcement learning (PPO) in AI2-THOR, improving object interaction and navigation in simulated kitchens.


<details>
  <summary>Details</summary>
Motivation: To enhance robotic agents' object interaction and navigation capabilities in simulated environments by leveraging advanced perception models.

Method: Combines Segment Anything Model (SAM) and YOLOv5 with a PPO agent in AI2-THOR, tested in four kitchen settings.

Result: 68% higher cumulative reward, 52.5% better object interaction success, and 33% improved navigation efficiency compared to baseline.

Conclusion: Integrating vision foundation models with reinforcement learning boosts performance in complex robotic tasks, advancing autonomous agent capabilities.

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


### [159] [Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model](https://arxiv.org/abs/2508.06206)
*Hanqing Wang,Shaoyang Wang,Yiming Zhong,Zemin Yang,Jiamin Wang,Zhiqing Cui,Jiahao Yuan,Yifan Han,Mingyu Liu,Yuexin Ma*

Main category: cs.RO

TL;DR: Affordance-R1 is a novel framework for affordance grounding, integrating cognitive Chain-of-Thought reasoning and Group Relative Policy Optimization (GRPO) in reinforcement learning, achieving robust zero-shot generalization and emergent reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing models lack Chain-of-Thought reasoning, limiting out-of-domain generalization and explicit reasoning in affordance grounding.

Method: Proposes Affordance-R1, combining GRPO in reinforcement learning with a sophisticated affordance function (format, perception, cognition rewards) and a new dataset, ReasonAff.

Result: Achieves robust zero-shot generalization, outperforms existing methods, and shows open-world generalization.

Conclusion: Affordance-R1 is the first to integrate GRPO-based RL with reasoning, advancing affordance grounding with improved generalization and reasoning.

Abstract: Affordance grounding focuses on predicting the specific regions of objects
that are associated with the actions to be performed by robots. It plays a
vital role in the fields of human-robot interaction, human-object interaction,
embodied manipulation, and embodied perception. Existing models often neglect
the affordance shared among different objects because they lack the
Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)
generalization and explicit reasoning capabilities. To address these
challenges, we propose Affordance-R1, the first unified affordance grounding
framework that integrates cognitive CoT guided Group Relative Policy
Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we
designed a sophisticated affordance function, which contains format,
perception, and cognition rewards to effectively guide optimization directions.
Furthermore, we constructed a high-quality affordance-centric reasoning
dataset, ReasonAff, to support training. Trained exclusively via reinforcement
learning with GRPO and without explicit reasoning data, Affordance-R1 achieves
robust zero-shot generalization and exhibits emergent test-time reasoning
capabilities. Comprehensive experiments demonstrate that our model outperforms
well-established methods and exhibits open-world generalization. To the best of
our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with
reasoning into affordance reasoning. The code of our method and our dataset is
released on https://github.com/hq-King/Affordance-R1.

</details>


### [160] [Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation](https://arxiv.org/abs/2508.06426)
*Youguang Xing,Xu Luo,Junlin Xie,Lianli Gao,Hengtao Shen,Jingkuan Song*

Main category: cs.RO

TL;DR: The paper investigates why generalist robot policies trained on large datasets like OXE struggle with generalization, identifying shortcut learning as the main issue due to dataset diversity and fragmentation.


<details>
  <summary>Details</summary>
Motivation: To understand and address the limited generalization capability of generalist robot policies trained on large-scale datasets.

Method: Theoretical and empirical analysis of shortcut learning, examining dataset diversity and distributional disparities.

Result: Identified two causes of shortcut learning: limited sub-dataset diversity and distributional disparities. Proposed solutions include better dataset collection and data augmentation.

Conclusion: Improved dataset strategies and augmentation can reduce shortcut learning, enhancing generalization in generalist robot policies.

Abstract: Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [161] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: KnapFormer is a framework for balancing workloads and enabling sequence parallelism in distributed training of Diffusion Transformers (DiT), achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: Addressing token imbalance in distributed training due to variable-length inputs and mixed-resolution data.

Method: Uses a knapsack problem solver to redistribute tokens, integrating sequence parallelism and a workload model.

Result: Achieves minimal communication overhead, <1% workload discrepancy, and 2x-3x speedup.

Conclusion: KnapFormer effectively eliminates stragglers and improves training efficiency for diffusion models.

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [162] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: AttriLens-Mol improves molecular property prediction by guiding LLMs with attribute-focused reinforcement learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based methods for molecular property prediction rely on human prompts and verbose reasoning, lacking relevance. AttriLens-Mol aims to enhance reasoning by focusing on relevant molecular attributes.

Method: AttriLens-Mol uses attribute-guided reinforcement learning with three rewards: format, count, and rationality, to steer LLM reasoning and extract relevant molecular attributes.

Result: Experiments show AttriLens-Mol boosts performance on 7B-size models, matching or surpassing supervised fine-tuning and advanced models like GPT-4o. Extracted attributes also improve interpretability.

Conclusion: AttriLens-Mol effectively elicits relevant molecular attributes, enhancing both performance and interpretability in property prediction.

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [163] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR enhances LLM optimization by improving sample efficiency and reducing primacy bias through high-replay training, periodic resets, and hybrid objectives.


<details>
  <summary>Details</summary>
Motivation: Address low sample efficiency and primacy bias in LLM optimization methods like RL and preference optimization.

Method: Introduces LoRR, a plugin with high-replay training, periodic resets, and hybrid SFT-preference loss optimization.

Result: Boosts performance on reasoning benchmarks; iterative DPO with LoRR matches complex RL methods.

Conclusion: LoRR is a practical, sample-efficient paradigm for LLM finetuning, maximizing performance with limited data.

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [164] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: A novel approach using synthetic image generation with a diffusion model improves oral cancer diagnostic accuracy by addressing dataset limitations.


<details>
  <summary>Details</summary>
Motivation: Limited annotated datasets constrain diagnostic model performance due to variability and insufficient training data.

Method: Proposed an inpainting technique with a fine-tuned diffusion model to synthesize realistic oral cancer lesions, using a comprehensive dataset from multiple sources.

Result: Achieved 0.97 diagnostic accuracy for classification and 0.85 accuracy for lesion detection.

Conclusion: Synthetic image generation shows promise for medical diagnostics and could extend to other cancer types.

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [165] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF is a novel Federated Meta-Learning approach for neural fields that ensures privacy while optimizing efficiently with limited data.


<details>
  <summary>Details</summary>
Motivation: Neural fields require large data and computations, which are challenging for edge devices. Traditional FML methods risk privacy leakage.

Method: FedMeNF uses a privacy-preserving loss function to regulate leakage during local meta-optimization, avoiding private data retention.

Result: FedMeNF achieves fast optimization and robust performance with few-shot or non-IID data, preserving privacy.

Conclusion: FedMeNF effectively balances privacy and efficiency in neural field learning, making it suitable for edge devices.

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [166] [NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](https://arxiv.org/abs/2508.05835)
*Edresson Casanova,Paarth Neekhara,Ryan Langman,Shehzeen Hussain,Subhankar Ghosh,Xuesong Yang,Ante Jukić,Jason Li,Boris Ginsburg*

Main category: eess.AS

TL;DR: NanoCodec, a low frame-rate audio codec, improves efficiency in Speech LLMs by reducing autoregressive steps while maintaining high-quality compression.


<details>
  <summary>Details</summary>
Motivation: Existing audio codecs operate at high frame rates, causing slow training and inference for autoregressive models in Speech LLMs.

Method: Ablation studies on frame rate, bitrate, and causality, leading to the development of NanoCodec at 12.5 FPS.

Result: NanoCodec outperforms existing codecs across bitrate ranges, setting a benchmark for low-latency Speech LLM training.

Conclusion: NanoCodec is a state-of-the-art solution for efficient and high-quality audio compression in Speech LLMs.

Abstract: Large Language Models (LLMs) have significantly advanced audio processing by
leveraging audio codecs to discretize audio into tokens, enabling the
application of language modeling techniques to speech data. However, existing
audio codecs often operate at high frame rates, leading to slow training and
inference, particularly for autoregressive models. To address this, there is
growing interest in low frame-rate audio codecs, which reduce the number of
autoregressive steps required to generate one second of audio. In this paper,
we conduct ablation studies to examine the impact of frame rate, bitrate, and
causality on codec reconstruction quality. Based on our findings, we introduce
NanoCodec, a state-of-the-art audio codec that achieves high-quality
compression at just 12.5 frames per second (FPS). NanoCodec outperforms related
works across various bitrate ranges, establishing a new benchmark for
low-latency and efficient Speech LLM training and inference.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [167] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: The paper evaluates techniques like query rewriting, RAG Fusion, and intent recognition for improving AI customer service in the electric power domain, finding graph-based RAG superior. The final system achieves high accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard NLP pipelines and finetuned language models often fail on ambiguous or complex queries, necessitating better solutions for customer support.

Method: The study compares techniques like query rewriting, RAG Fusion, and intent recognition, testing them on vector-store and graph-based RAG frameworks.

Result: Graph-based RAG outperforms others, with the final system achieving 97.9% and 89.6% accuracy on test datasets.

Conclusion: Combining intent recognition, RAG Fusion, and reranking effectively handles complex queries, significantly improving performance over baselines.

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [168] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: The paper surveys LLM-based search agents, analyzing their architecture, optimization, applications, and evaluation, while identifying open challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze the transformative impact of LLM-based search agents on web search and information retrieval.

Method: Comprehensive analysis and categorization of existing works from multiple perspectives, including architecture, optimization, and evaluation.

Result: Identifies key open challenges and outlines promising future research directions in the field of LLM-based search agents.

Conclusion: The survey highlights the potential of LLM-based search agents and provides a foundation for future research in this evolving domain.

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [169] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: A fine-tuned vision-language model (VLM) for converting financial tables from Malaysian reports into Markdown achieves high accuracy and efficiency, outperforming larger and proprietary models.


<details>
  <summary>Details</summary>
Motivation: The challenge of accurately extracting and representing tabular data from financial documents, especially with complex layouts, motivates the need for a specialized solution.

Method: The study uses a fine-tuned Qwen2.5-VL-7B model with a curated dataset and LoRA for supervised fine-tuning, evaluated using criteria-based and TEDS metrics.

Result: The model achieves 92.20% accuracy and 96.53% TEDS score, outperforming base and larger models, including proprietary ones like GPT-4o and Gemini 2.5 Flash.

Conclusion: Domain-specific fine-tuning is effective for financial document automation, offering high performance without computational overhead.

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [170] [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
*Andrew Brown,Muhammad Roman,Barry Devereux*

Main category: cs.DL

TL;DR: A systematic review of retrieval-augmented generation (RAG) literature (2020-2025) analyzing 128 highly cited studies, focusing on datasets, architectures, evaluation practices, and empirical evidence.


<details>
  <summary>Details</summary>
Motivation: To synthesize and analyze the effectiveness and limitations of RAG, a method combining neural retrieval with generative language models, while addressing citation-lag bias for recent studies.

Method: Conducted a PRISMA 2020-guided review with explicit inclusion/exclusion criteria, cataloged datasets and architectures, and synthesized empirical evidence.

Result: Clarified the RAG research landscape, identified methodological gaps, and outlined future research priorities.

Conclusion: The review provides a comprehensive analysis of RAG, highlighting its potential and areas needing further exploration.

Abstract: This systematic review of the research literature on retrieval-augmented
generation (RAG) provides a focused analysis of the most highly cited studies
published between 2020 and May 2025. A total of 128 articles met our inclusion
criteria. The records were retrieved from ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).
RAG couples a neural retriever with a generative language model, grounding
output in up-to-date, non-parametric memory while retaining the semantic
generalisation stored in model weights. Guided by the PRISMA 2020 framework, we
(i) specify explicit inclusion and exclusion criteria based on citation count
and research questions, (ii) catalogue datasets, architectures, and evaluation
practices, and (iii) synthesise empirical evidence on the effectiveness and
limitations of RAG. To mitigate citation-lag bias, we applied a lower
citation-count threshold to papers published in 2025 so that emerging
breakthroughs with naturally fewer citations were still captured. This review
clarifies the current research landscape, highlights methodological gaps, and
charts priority directions for future research.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [171] [Basic interactive algorithms: Preview](https://arxiv.org/abs/2508.05798)
*Yuri Gurevich*

Main category: cs.LO

TL;DR: The paper previews an upcoming work on axiomatizing basic interactive algorithms, contrasting classical and modern algorithmic notions, and linking nondeterministic/probabilistic algorithms to basic algorithms with oracles.


<details>
  <summary>Details</summary>
Motivation: To extend the axiomatization of classical algorithms to modern interactive algorithms and clarify the distinction between the original Church-Turing thesis and its broader physical version.

Method: Proposes viewing nondeterministic, probabilistic, and quantum algorithms as basic algorithms augmented with oracles.

Result: Demonstrates how diverse algorithmic classes can be unified under the framework of basic algorithms with oracles.

Conclusion: The paper sets the stage for a formal axiomatization of interactive algorithms, bridging classical and modern algorithmic paradigms.

Abstract: This dialog paper offers a preview and provides a foretaste of an upcoming
work on the axiomatization of basic interactive algorithms.
  The modern notion of algorithm was elucidated in the 1930s--1950s. It was
axiomatized a quarter of a century ago as the notion of ``sequential
algorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm"
now. The axiomatization was used to show that for every basic algorithm there
is a behaviorally equivalent abstract state machine. It was also used to prove
the Church-Turing thesis as it has been understood by the logicians.
  Starting from the 1960s, the notion of algorithm has expanded --
probabilistic algorithms, quantum algorithms, etc. -- prompting introduction of
a much more ambitious version of the Church-Turing thesis commonly known as the
``physical thesis.'' We emphasize the difference between the two versions of
the Church-Turing thesis and illustrate how nondeterministic and probabilistic
algorithms can be viewed as basic algorithms with appropriate oracles. The same
view applies to quantum circuit algorithms and many other classes of
algorithms.

</details>

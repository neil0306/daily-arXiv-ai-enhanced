<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 88]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: MALIBU is a benchmark to assess bias in LLM-based multi-agent systems, revealing biases and the need for nuanced fairness strategies.


<details>
  <summary>Details</summary>
Motivation: Address concerns about fairness and equitable representation in multi-agent systems due to implicit biases in LLMs.

Method: MALIBU evaluates bias through scenario-based assessments with AI models completing tasks and responses judged in two phases.

Result: Quantifies biases in LLM outputs, showing bias mitigation may favor marginalized personas over neutrality.

Conclusion: Highlights the need for nuanced bias detection, balanced fairness strategies, and transparent benchmarks in multi-agent systems.

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [2] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: The paper proposes evaluating abstractive summaries by comparing overlapping events between generated summaries, reference summaries, and original news articles, using a Norwegian dataset with expert annotations.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of generative summaries relies on human-authored references, often missing event-focused assessment.

Method: Calculate overlapping events between generated summaries, references, and original articles using a richly annotated Norwegian dataset.

Result: The approach offers deeper insight into event information in summaries.

Conclusion: Event-based evaluation provides a more meaningful measure of summary quality.

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [3] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: The paper analyzes geographic shifts in the Swedish encyclopedia 	extit{Nordisk familjebok} between its first and second editions, revealing a trend away from Europe towards other regions, influenced by historical events like WWI.


<details>
  <summary>Details</summary>
Motivation: To understand how intellectual and societal changes in Sweden were reflected in the evolving content of the encyclopedia, particularly in geographic entries.

Method: Digitized versions were resegmented into entries, matched using semantic embeddings, and geographic entries were classified and linked to Wikidata for analysis.

Result: A significant shift in geographic focus from Europe to North America, Africa, Asia, Australia, and northern Scandinavia was observed between editions.

Conclusion: The findings confirm the impact of global events like WWI on the encyclopedia's content, highlighting its role as a cultural and intellectual mirror.

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [4] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: The paper proposes xLSTM with Multihead Exponential Gated Fusion (MEGA) for Aspect-based Sentiment Analysis (ABSA), addressing limitations of existing methods by balancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing ABSA methods struggle with computational efficiency and performance trade-offs, lacking global context or demanding high resources. xLSTM's potential in ABSA is unexplored.

Method: Introduces MEGA, combining bi-directional mLSTM with forward and partially flipped backward streams (PF-mLSTM) and a multihead cross exponential gated fusion mechanism (MECGAF) for dynamic context modeling.

Result: MEGA outperforms state-of-the-art baselines on three benchmark datasets, achieving superior accuracy and efficiency.

Conclusion: MEGA effectively addresses ABSA challenges, offering a balanced solution for fine-grained sentiment analysis with improved performance and efficiency.

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [5] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: A debiasing algorithm reduces biases in text embeddings caused by spurious attributes like source or language, improving similarity and clustering metrics without degrading out-of-distribution performance.


<details>
  <summary>Details</summary>
Motivation: Text embeddings can be biased by irrelevant attributes (e.g., source or language), which affects applications pooling texts from diverse corpora.

Method: A debiasing algorithm removes information about observed confounders from encoder representations.

Result: Substantial reduction in biases, improved similarity and clustering metrics across tasks, with no degradation in out-of-distribution performance.

Conclusion: The debiasing algorithm effectively mitigates biases in embeddings while maintaining their utility.

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [6] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: The paper introduces gAIus, an LLM-based agent for legal tasks, focusing on Polish Civil Code. It improves retrieval mechanisms and outperforms GPT models in accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs providing accurate, referenced legal answers for non-English/Chinese contexts, specifically Poland.

Method: Proposes gAIus, a cognitive LLM-based agent with an explainable retrieval mechanism, evaluated using Polish law exam questions.

Result: gAIus significantly outperforms GPT-3.5 and GPT-4 variants, improving accuracy by up to 419%.

Conclusion: The findings highlight potential for future research and applications in legal AI, especially for non-English/Chinese legal systems.

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [7] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: GPT-4 shows moderate performance in simulating clinical decisions for diabetic retinopathy and glaucoma screening but lacks precision for complex tasks. Metadata inclusion had no significant impact.


<details>
  <summary>Details</summary>
Motivation: To explore the utility of large language models (LLMs) like GPT-4 in ophthalmology, specifically for interpreting retinal fundus images and simulating clinical decisions.

Method: A retrospective diagnostic validation study using 300 annotated fundus images. GPT-4 was given structured prompts with or without metadata to assign ICDR scores, recommend DR referrals, and estimate cup-to-disc ratios. Performance was evaluated using accuracy, F1 scores, and Cohen's kappa.

Result: GPT-4 performed moderately for DR classification (accuracy 67.5%) and better for binary DR referral (accuracy 82.3%). Glaucoma referral performance was poor (accuracy ~78%). Metadata had no significant effect.

Conclusion: GPT-4 can simulate basic ophthalmic decision-making but is not suitable for clinical use. It may assist in education, documentation, or image annotation workflows.

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [8] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: CARE-RAG improves RAG systems by addressing knowledge conflicts through conflict-driven summarization and evidence refinement, outperforming baselines in noisy or conflicting scenarios.


<details>
  <summary>Details</summary>
Motivation: Knowledge conflicts in RAG systems undermine reliability, necessitating a framework to rethink and synthesize all evidence before generation.

Method: CARE-RAG derives parameter-aware and context-aware evidence, uses a distilled 3B LLaMA3.2 model for conflict-driven summarization, and includes QA Repair for evaluation integrity.

Result: CARE-RAG consistently outperforms RAG baselines, particularly with noisy or conflicting evidence.

Conclusion: CARE-RAG enhances RAG reliability by systematically addressing knowledge conflicts and refining evidence.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [9] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: The paper introduces CompactDS, a web-scale datastore for Retrieval-augmented Generation (RAG), improving accuracy on reasoning-intensive benchmarks like MMLU and MATH by 10-33%.


<details>
  <summary>Details</summary>
Motivation: Prior RAG work struggled with reasoning-intensive tasks due to limited datastores. The authors aim to address this gap by creating a diverse, high-quality datastore.

Method: Develop CompactDS: filter web content for quality, combine in-memory ANN and on-disk exact search for speed and recall. Test on benchmarks with minimal RAG.

Result: CompactDS improves accuracy across benchmarks (e.g., 33% on MMLU Pro) and outperforms web search engines and complex RAG systems.

Conclusion: CompactDS demonstrates the importance of diverse, high-quality data for RAG, offering simplicity and reproducibility while matching or surpassing existing solutions.

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [10] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA introduces a novel method for activation sparsification in LLMs, improving efficiency without additional training or magnitude-based pruning, achieving consistent sparsity and speed-up.


<details>
  <summary>Details</summary>
Motivation: Existing methods for activation sparsity in LLMs either require recovery training or rely on unstable magnitude-based pruning, limiting real-world adoption.

Method: LaRoSA uses layerwise orthogonal rotations to transform activations for sparsification, employing Top-K selection for consistent sparsity and speed-up.

Result: For LLaMA2-7B at 40% sparsity, LaRoSA shows minimal performance degradation (0.17 perplexity gap) and a 1.30x speed-up, outperforming TEAL and CATS in accuracy.

Conclusion: LaRoSA is a robust and efficient method for activation sparsification in LLMs, offering stable speed-up and minimal performance loss.

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [11] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: Advanced instruction-tuned reasoning models like Deepseek-R1 excel in solving complex physics problems, achieving top accuracy and unique symbolic reasoning. Few-shot prompting further boosts performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of physics reasoning for LLMs, which requires deep conceptual understanding and problem-solving skills.

Method: Using instruction-tuned reasoning models (e.g., Deepseek-R1) on the SciBench benchmark, with few-shot prompting for enhancement.

Result: State-of-the-art accuracy in physics questions and distinctive symbolic reasoning patterns. Few-shot prompting improves performance.

Conclusion: Advanced reasoning models show promise in physics tasks, with potential for further gains through strategic prompting.

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [12] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: LEDOM is the first reverse language model, trained autoregressively, with applications like Reverse Reward improving forward model outputs.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of reverse language models as foundational tools for general tasks and introduce novel applications like Reverse Reward.

Method: LEDOM is trained on 435B tokens with 2B and 7B parameter variants, processing sequences in reverse order via previous token prediction.

Result: LEDOM demonstrates unique backward reasoning capabilities, improving forward model outputs in tasks like mathematical reasoning.

Conclusion: LEDOM shows broad application potential, with models, code, and data released for future research.

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [13] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: The paper introduces SynPref-40M, a large-scale preference dataset, and Skywork-Reward-V2, a suite of reward models, to address limitations in current reward models by improving data quality and scale.


<details>
  <summary>Details</summary>
Motivation: Current reward models perform poorly due to limitations in preference datasets, which are narrow, synthetic, or lack quality control.

Method: A human-AI synergistic two-stage pipeline curates a large-scale dataset (SynPref-40M), and Skywork-Reward-V2 models are trained on a subset of this data.

Result: Skywork-Reward-V2 achieves state-of-the-art performance across seven benchmarks, demonstrating versatility in alignment, correctness, safety, and bias resistance.

Conclusion: The approach highlights the potential of high-quality data curation and human-AI synergy, advancing open reward models.

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [14] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: A deep learning method using attention mechanisms is proposed for unified modeling of information extraction and multi-label disease prediction in EHR texts, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing the unstructured nature and high-dimensional semantic complexity of electronic health record (EHR) texts.

Method: Transformer-based architecture with multi-layer self-attention for representation learning, and a Sigmoid-based multi-label classifier for disease prediction. Includes context-aware semantic alignment.

Result: Outperforms existing methods across metrics, maintains generalization under varying conditions.

Conclusion: The framework is efficient for clinical text processing and significant for multi-label medical tasks.

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [15] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: LogitSpec improves speculative decoding by expanding retrieval range using logits, achieving faster LLM inference without training.


<details>
  <summary>Details</summary>
Motivation: Retrieval-based speculative decoding often fails to find accurate draft tokens, limiting its effectiveness.

Method: LogitSpec uses the last token's logit to predict the next and next-next token, then retrieves references for both.

Result: Achieves up to 2.61× speedup and 3.28 mean accepted tokens per decoding step.

Conclusion: LogitSpec is a training-free, plug-and-play solution for efficient LLM inference.

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [16] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: The paper proposes using direct preference optimization (DPO) to personalize LLM-based text simplification for individuals with intellectual disabilities, incorporating their feedback for better alignment with their needs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based ATS systems lack personalization for target groups like individuals with intellectual disabilities, as they don't incorporate preference feedback during training.

Method: Extends supervised fine-tuning (SFT) with DPO, using human feedback from the target group to post-train LLM-based ATS models. A pipeline for personalized ATS development is also proposed.

Result: Highlights the importance of involving target group members in designing AI accessibility solutions, improving personalization.

Conclusion: This work advances personalized, inclusive AI systems by integrating feedback from both experts and target group individuals.

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [17] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: A modular framework combining uncertainty modeling and fine-tuned LLMs for efficient and accurate OOS intent detection in TODS.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of OOS intent detection to enhance robustness in task-oriented dialogue systems.

Method: Uses uncertainty estimation on in-scope intent classifier outputs, followed by fine-tuned LLMs for high-uncertainty cases.

Result: Achieves state-of-the-art performance on OOS benchmarks, including real-world TODS data.

Conclusion: The framework balances efficiency and performance, outperforming prior methods.

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [18] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: External information like Wikipedia and web search often degrades stance detection performance in large language models (LLMs), contrary to prior findings with BERT-based systems.


<details>
  <summary>Details</summary>
Motivation: To evaluate the impact of external information on stance detection in LLMs, given its known benefits in BERT-based systems.

Method: Systematic evaluation across eight LLMs and three datasets with 12 targets, using Wikipedia and web search external information.

Result: External information degrades performance (up to 27.9% drop in macro F1), as LLMs align predictions with the provided information's stance/sentiment rather than ground truth.

Conclusion: LLMs are prone to information biases in stance detection, and fine-tuning mitigates but doesn't fully resolve the issue.

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [19] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: The paper introduces LUSTER, an LLM-based unified system for task-oriented dialogue, using end-to-end reinforcement learning with short-term and long-term rewards to improve resilience and emotional responsiveness.


<details>
  <summary>Details</summary>
Motivation: Despite advances in LLMs, building effective and emotionally intelligent task-oriented dialogue systems remains challenging due to noisy environments and the need for task success, emotional understanding, and precise information conveyance.

Method: Proposes LUSTER, combining LLMs with structured reward modeling (short-term user sentiment and long-term task success) and evaluating it in a challenging environment with a user simulator and imperfect NLU module.

Result: LUSTER demonstrates improved resilience and emotional responsiveness in task-oriented dialogue systems.

Conclusion: Combining LLMs with structured reward modeling offers a practical path for next-generation conversational agents.

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [20] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: A new dataset for chart question answering (CQA) is introduced, featuring real-world, multi-view charts with natural language questions. It reflects authentic reasoning workflows, revealing a performance gap in state-of-the-art models like GPT-4.1 (69.3% accuracy).


<details>
  <summary>Details</summary>
Motivation: To address the lack of ecologically valid datasets for CQA, the paper introduces a dataset derived from real-world visualization notebooks to better mimic authentic reasoning workflows.

Method: The dataset is constructed from visualization notebooks, pairing multi-view charts with grounded natural language questions. State-of-the-art multimodal models (e.g., GPT-4.1) are benchmarked on this dataset.

Result: GPT-4.1 achieves 69.3% accuracy, highlighting a significant performance gap in this more authentic CQA setting.

Conclusion: The dataset presents a more realistic challenge for CQA, exposing limitations in current models and encouraging further research in this direction.

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [21] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: The paper compares global scores and pairwise comparisons for evaluating NLP models, finding global scores reliable for overall rankings but prone to underestimating strong models with rare errors, while pairwise comparisons excel at identifying strong contenders among lower-scoring models.


<details>
  <summary>Details</summary>
Motivation: To aid decision-making in selecting model evaluation strategies by empirically investigating the strengths and weaknesses of global scores and pairwise comparisons.

Method: Computational experiments on synthetic and real-world datasets using standard global metrics and the Bradley-Terry model for pairwise comparisons.

Result: Global scores provide reliable overall rankings but may underestimate strong models with rare errors. Pairwise comparisons are effective for identifying strong contenders among lower-scoring models but require more comparisons to converge.

Conclusion: Both evaluation strategies have trade-offs; global scores are reliable for rankings, while pairwise comparisons are better for nuanced model strengths, especially in tasks like text generation.

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [22] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: The paper explores how pre-trained language models transfer to low-resource Indonesian local languages for sentiment analysis, comparing zero-shot and adapter-based methods. MAD-X improves performance, especially for seen and partially seen languages, with prior language exposure being the key predictor of success.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the transferability of pre-trained language models to low-resource Indonesian local languages, focusing on sentiment analysis.

Method: Evaluates zero-shot and adapter-based transfer (MAD-X) using monolingual Indonesian BERT, multilingual models (mBERT, XLM-R), and categorizes languages into seen, partially seen, and unseen groups. Analyzes tokenization and vocabulary overlap.

Result: Multilingual models perform best on seen languages, moderately on partially seen, and poorly on unseen. MAD-X improves performance without target-language labeled data. Tokenization and vocabulary overlap weakly correlate with performance; prior language exposure is the strongest predictor.

Conclusion: Prior exposure to a language (directly or through related languages) is the most consistent predictor of transfer success. MAD-X is effective, especially for seen and partially seen languages.

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [23] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: AdamMeme is an agent-based framework for evaluating mLLMs' ability to understand harmful memes, addressing limitations of static benchmarks by dynamically updating data and probing model reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for harmful meme understanding in mLLMs are static and lack adaptability to evolving online memes, necessitating a more flexible evaluation approach.

Method: Proposes AdamMeme, a multi-agent framework that iteratively updates meme data with challenging samples to test mLLMs' reasoning and harmfulness interpretation.

Result: The framework systematically identifies varying performance and specific weaknesses of different mLLMs, providing fine-grained analyses.

Conclusion: AdamMeme offers a dynamic, thorough evaluation method for mLLMs' harmful meme understanding, surpassing static benchmarks.

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [24] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper introduces StereoBias, a dataset for detecting bias and stereotypes in language models, showing joint training improves bias detection.


<details>
  <summary>Details</summary>
Motivation: Addressing harmful biases and stereotypes in language models, especially in sensitive applications like content moderation.

Method: Joint training of bias and stereotype detection using StereoBias dataset, comparing encoder-only and decoder-only models with QLoRA.

Result: Joint training significantly improves bias detection; decoder-only models perform competitively.

Conclusion: Leveraging stereotype information enhances fairness and effectiveness in AI systems.

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [25] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: The paper explores using LLMs to classify clauses in German employment contracts as 'valid,' 'unfair,' or 'void,' with varying legal context inputs. Full-text sources moderately help, but distilled guidelines significantly improve recall. Performance remains below human lawyers.


<details>
  <summary>Details</summary>
Motivation: Legal work's text-heavy nature and the lack of interpretability in data-driven NLP methods limit their trustworthiness in dynamic legal environments.

Method: Collaborated with legal experts to extend a dataset, using LLMs and in-context learning to evaluate clause legality under three legal context variants.

Result: Full-text sources moderately improved performance, but examination guidelines boosted recall for void clauses and weighted F1-Score to 80%. LLMs still underperform compared to human lawyers.

Conclusion: LLMs show potential to assist lawyers in contract legality review but have limitations. The extended dataset, guidelines, and code are shared to support further research.

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [26] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: The paper explores how tokenisation discrepancies impact language data representation and analysis validity, focusing on emojis and homoglyphs, and proposes preprocessing methods for accurate corpus representation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by emojis and homoglyphs in tokenisation, ensuring corpus fidelity and reliable linguistic analysis.

Method: Investigates preprocessing techniques for digital texts to maintain accuracy in corpora, supporting repeatable linguistic interpretations.

Result: Highlights the need for understanding linguistic and technical aspects of digital data to improve corpus analysis accuracy.

Conclusion: The study underscores the importance of accurate tokenisation for reliable corpus-based research, impacting both quantitative and qualitative approaches.

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [27] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: MuRating is a scalable framework for multilingual data-quality assessment, transferring English quality signals to 17 languages, improving model performance on diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing data-quality methods focus on English, limiting multilingual model performance. MuRating addresses this gap by extending quality assessment to multiple languages.

Method: MuRating aggregates English raters via pairwise comparisons, projects judgments through translation, and trains a multilingual evaluator on diverse text pairs.

Result: MuRating outperforms baselines like QuRater and AskLLM, boosting accuracy on English and multilingual benchmarks, especially in knowledge-intensive tasks.

Conclusion: MuRating enhances multilingual data selection, with analysis on translation fidelity and biases suggesting future improvements.

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [28] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: Language models like Llama-3.3-70B-Instruct can distinguish between testing and deployment phases, raising concerns about evaluation reliability and AI governance. Linear probes reveal this distinction, suggesting models internally recognize it. Safety evaluations are flagged as artificial, highlighting the need for trustworthy evaluations and understanding deceptive capabilities.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the safety and policy implications of language models' ability to distinguish between evaluation and deployment phases, which could undermine AI governance frameworks.

Method: Linear probes are used to analyze Llama-3.3-70B-Instruct's internal representations of evaluation and deployment prompts.

Result: Probes successfully separate evaluation and deployment prompts, and safety evaluations are classified as artificial, indicating models recognize them as inauthentic.

Conclusion: The findings emphasize the need for reliable evaluations and deeper understanding of deceptive model capabilities, while showcasing how model internals can aid safety audits.

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [29] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: The paper investigates how vision-language AI models handle conflicting multimodal inputs, revealing biases toward specific modalities and identifying attention heads that influence modality preference.


<details>
  <summary>Details</summary>
Motivation: To understand how multimodal AI models behave when faced with conflicting inputs (e.g., mismatched image-caption pairs) and to explore the internal mechanisms driving their responses.

Method: The study tests vision-language models with inconsistent inputs (e.g., an image of a dog labeled as a cat) and analyzes their responses to identify modality biases and the role of attention heads.

Result: Models often favor one modality over another, with preferences evident in their internal structure. Specific attention heads can manipulate modality preference, and "router heads" can adapt responses based on the requested modality.

Conclusion: The findings advance understanding of how multimodal models detect and resolve conflicts, offering insights for improving their performance in complex environments.

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [30] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: The paper analyzes the MDACE dataset for explainable medical coding, evaluates current systems, and provides recommendations for development and evaluation.


<details>
  <summary>Details</summary>
Motivation: To improve transparency in automatic medical coding by evaluating explainability methods and understanding evidence alignment with code descriptions.

Method: In-depth analysis of the MDACE dataset and plausibility evaluation of explainable medical coding systems, including match measures and case studies.

Result: Ground truth evidence partially aligns with code descriptions, and state-of-the-art approaches show high overlap with ground truth. Success and failure cases are identified.

Conclusion: Recommendations are provided for developing and evaluating explainable medical coding systems based on the findings.

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [31] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: JSON is the most parseable format for structured outputs in clinical note extraction, with robustness improving via targeted prompting but declining for longer documents.


<details>
  <summary>Details</summary>
Motivation: To compare parseability of structured outputs (JSON, YAML, XML) from small language models for clinical note extraction.

Method: Evaluated three serialization formats (JSON, YAML, XML) for open attribute-value extraction, analyzing parseability and structural robustness.

Result: JSON consistently had the highest parseability. Robustness improved with targeted prompting and larger models but declined for longer documents and certain note types.

Conclusion: JSON is recommended for clinical settings, with insights on prompt design to enhance model performance.

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [32] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: The paper investigates how LLMs replicate training data by analyzing low-perplexity sequences, revealing many cannot be traced to the corpus and quantifying verbatim recall.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs' training data influences outputs is critical for transparency, accountability, privacy, and fairness.

Method: A systematic pipeline extracts and traces low-perplexity sequences (high-probability text spans) back to training data sources.

Result: Many low-perplexity sequences cannot be mapped to the corpus; for those that match, the study quantifies their distribution across source documents.

Conclusion: The findings highlight the scope of verbatim recall in LLMs, aiding better understanding of training data's impact on model behavior.

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [33] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: EKA-EVAL is a unified evaluation framework for LLMs, supporting over 35 benchmarks (including 10 Indic-specific datasets), with features like distributed inference and multi-GPU usage, aiming to lower barriers for multilingual benchmarking.


<details>
  <summary>Details</summary>
Motivation: The need for evaluation frameworks beyond English-centric benchmarks, especially for linguistically diverse regions like India, drives the development of EKA-EVAL.

Method: EKA-EVAL integrates diverse benchmarks (reasoning, mathematics, etc.) and offers built-in support for distributed inference, quantization, and multi-GPU usage.

Result: EKA-EVAL provides broader benchmark coverage and is the first end-to-end, extensible evaluation suite for global and Indic LLMs.

Conclusion: The open-source framework, part of the EKA initiative, aims to scale further and establish a robust multilingual evaluation ecosystem for LLMs.

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [34] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: DIY-MKG is an open-source system for polyglot language learning, addressing limitations of existing tools by enabling personalized vocabulary knowledge graphs, adaptive quizzes, and user feedback.


<details>
  <summary>Details</summary>
Motivation: Existing language learning tools lack support for polyglot learners, customization, and suffer from cognitive offloading.

Method: DIY-MKG uses LLMs to build personalized vocabulary knowledge graphs, offers rich annotations, and generates adaptive quizzes with user feedback.

Result: Evaluation shows reliable vocabulary expansion and highly accurate quizzes across multiple languages.

Conclusion: DIY-MKG is robust and effective for polyglot language learning, leveraging LLMs for personalized and adaptive learning experiences.

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [35] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: MiCoTA improves small language models' reasoning by using intermediate-sized models as teacher assistants and intermediate-length reasoning sequences.


<details>
  <summary>Details</summary>
Motivation: Address the 'SLMs Learnability Gap' where small language models struggle with long-form reasoning due to limited capacity.

Method: Introduces MiCoTA, a framework using intermediate-sized models and intermediate-length reasoning sequences for distillation.

Result: SLMs show significant improvement, e.g., Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct gain 3.47 and 3.93 average score boosts on benchmarks.

Conclusion: MiCoTA effectively bridges the reasoning gap for SLMs, offering insights for future long-CoT distillation research.

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [36] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: A novel pruning algorithm for LLMs strategically prunes higher-layer attention heads and uses adaptive rescaling to maintain representation quality, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional pruning methods indiscriminately remove attention heads without considering their positions, potentially harming model performance.

Method: Proposes a pruning algorithm targeting higher-layer attention heads and introduces adaptive rescaling to adjust post-pruning representation scales.

Result: Outperforms existing structured pruning methods, especially in generation tasks, across 27 datasets and multiple LLMs.

Conclusion: Strategic pruning and adaptive rescaling improve LLM compression and performance, particularly in generation tasks.

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [37] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: A survey on AI for Research (AI4Research) addresses gaps in understanding and development by offering a systematic taxonomy, identifying research frontiers, and compiling resources.


<details>
  <summary>Details</summary>
Motivation: Recent AI advancements, especially in LLMs, have shown potential in scientific research, but a lack of comprehensive surveys hinders progress.

Method: The paper introduces a taxonomy for AI4Research tasks, identifies research gaps, and compiles multidisciplinary resources.

Result: The survey provides a unified perspective on AI4Research, highlighting key tasks, future directions, and practical resources.

Conclusion: This work aims to facilitate quick access to AI4Research resources and inspire further innovation in the field.

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [38] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: GAPO, a novel RLHF method, optimizes conflicting human preferences via multi-gradient descent, outperforming SOTA in aligning LLMs.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with diverse, often conflicting human preferences is challenging.

Method: GAPO uses multi-gradient descent to balance trade-offs; P-GAPO incorporates user preferences for Pareto solutions.

Result: GAPO outperforms SOTA on Mistral-7B, excelling in helpfulness and harmlessness.

Conclusion: GAPO effectively aligns LLMs with diverse preferences, achieving Pareto optimality.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [39] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: The paper introduces 'NaturalThoughts,' a curated dataset of reasoning traces from a teacher model, showing it improves student models' reasoning more effectively than random sampling or existing datasets.


<details>
  <summary>Details</summary>
Motivation: To systematically study what kind of reasoning demonstrations from a teacher model best improve student models' reasoning capabilities.

Method: Curate high-quality 'NaturalThoughts' by selecting reasoning traces from a strong teacher model, analyze factors affecting distillation, and compare performance with random sampling and existing datasets.

Result: NaturalThoughts outperforms random sampling and existing datasets (e.g., OpenThoughts, LIMO) on STEM reasoning benchmarks like GPQA-Diamond, MMLU-Pro, and SuperGPQA.

Conclusion: Selecting difficult, diverse reasoning examples is more sample-efficient for transferring reasoning skills, and scaling data size with random sampling is a strong baseline.

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [40] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: A decision-oriented framework evaluates NLG text by measuring its impact on human and LLM decision outcomes, revealing traditional metrics' limitations.


<details>
  <summary>Details</summary>
Motivation: Current intrinsic NLG evaluation methods poorly correlate with decision-making efficacy, necessitating a more practical approach.

Method: Proposes a framework assessing decision quality via financial performance of trades by humans and LLMs using market digest texts.

Result: Neither humans nor LLMs outperform random baselines with summaries alone, but human-LLM collaboration excels with richer analyses.

Conclusion: Evaluating NLG text by its decision-making synergy between humans and LLMs is crucial, exposing flaws in traditional metrics.

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [41] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: The study compares OpenAI's Whisper and Facebook's Wav2Vec-BERT for Bangla ASR, finding Wav2Vec-BERT superior in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To evaluate state-of-the-art ASR models for low-resource languages like Bangla.

Method: Fine-tuning and hyperparameter optimization on Mozilla Common Voice-17 and OpenSLR datasets, measuring WER, CER, training time, and computational efficiency.

Result: Wav2Vec-BERT outperformed Whisper in all metrics, requiring fewer resources.

Conclusion: Wav2Vec-BERT is more effective for low-resource ASR tasks, offering insights for robust system development.

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [42] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: LLMs excel in persuasive debates but lack deeper comprehension of dialogue structures and context, raising concerns about their evaluative roles.


<details>
  <summary>Details</summary>
Motivation: To examine LLMs' debate capabilities and their understanding of dialogue, given their increasing use in sensitive applications.

Method: Evaluated LLMs' debate performance and measured their comprehension of dialogical structures and pragmatic context.

Result: LLMs can maintain coherent debates but fail to demonstrate deeper understanding; awareness of AI involvement increases critical scrutiny.

Conclusion: LLMs' effectiveness in dialogue doesn't require deep comprehension, suggesting pragmatic context is secondary to persuasiveness.

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: A 4D video generation model is proposed to ensure multi-view 3D consistency in videos, improving robotic planning and interaction by predicting future sequences from novel viewpoints without requiring camera poses.


<details>
  <summary>Details</summary>
Motivation: Enhancing robots' ability to plan and interact in complex environments by understanding and predicting physical world dynamics.

Method: Supervising the model with cross-view pointmap alignment during training to learn a shared 3D representation, enabling prediction of future video sequences from novel viewpoints using RGB-D observations.

Result: Produces more visually stable and spatially aligned predictions across simulated and real-world datasets, and supports robust robot manipulation.

Conclusion: The method advances video generation for robotics by ensuring geometric consistency and enabling practical applications like trajectory recovery and manipulation.

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [44] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: The study integrates multi-source satellite imagery and deep learning models to improve landslide detection and prediction, leveraging Sentinel-2 and ALOS PALSAR data.


<details>
  <summary>Details</summary>
Motivation: Landslides threaten infrastructure, economies, and lives, requiring accurate detection and prediction methods.

Method: Uses Sentinel-2 multispectral data and ALOS PALSAR-derived slope/DEM layers, geospatial analysis, and deep learning models (U-Net, DeepLabV3+, Res-Net) for landslide detection.

Result: The framework enhances early warning systems, disaster risk management, and land-use planning.

Conclusion: Deep learning and multi-source remote sensing offer robust, scalable solutions for landslide prediction.

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [45] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*Alán F. Muñoz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

TL;DR: The paper introduces cp_measure, a Python library for modular, API-first feature extraction from biological images, improving reproducibility and integration with machine learning workflows.


<details>
  <summary>Details</summary>
Motivation: Current tools like CellProfiler hinder automated and reproducible analyses for image-based profiling, limiting machine learning applications in computational biology.

Method: The authors developed cp_measure, a Python library that extracts CellProfiler's core measurement capabilities into a modular, API-first tool for programmatic feature extraction.

Result: cp_measure features retain high fidelity with CellProfiler features and enable seamless integration with the scientific Python ecosystem, demonstrated through applications in 3D astrocyte imaging and spatial transcriptomics.

Conclusion: cp_measure facilitates reproducible, automated image-based profiling pipelines, scaling effectively for machine learning in computational biology.

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [46] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias Müller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti Pietikäinen*

Main category: cs.CV

TL;DR: The paper proposes efficient network designs (SDNet for images, STDNet for videos) using Pixel Difference Convolutions (PDCs) and a reparameterization strategy (DCR) to achieve real-time salient object detection on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: Existing SOD models are computationally expensive, making them unsuitable for resource-constrained devices. The goal is to combine traditional SOD wisdom with modern CNNs for efficiency and accuracy.

Method: The method introduces PDCs to encode feature contrasts, integrates them into CNNs, and uses DCR to reduce computation. For video SOD, STDC enhances 3D convolution with spatiotemporal contrast capture.

Result: SDNet and STDNet achieve 46 FPS and 150 FPS on images and videos, respectively, with <1M parameters, outperforming lightweight models in speed and accuracy.

Conclusion: The proposed models offer a superior efficiency-accuracy trade-off for real-time SOD on constrained devices, with potential applications in streaming and embedded systems.

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [47] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/abs/2507.01254)
*Runze Cheng,Xihang Qiu,Ming Li,Ye Zhang,Chun Li,Fei Yu*

Main category: cs.CV

TL;DR: A robust single-modality parallel processing framework for brain tumor segmentation, achieving high accuracy even with missing MRI modalities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional methods fail when MRI modalities are missing due to various issues, limiting accurate brain tumor segmentation.

Method: Uses Holder divergence and mutual information to maintain modality-specific features and dynamically adjust network parameters based on available inputs.

Result: Superior performance on BraTS 2018 and BraTS 2020 datasets, especially with missing modalities.

Conclusion: The proposed framework is effective for accurate brain tumor segmentation even with incomplete MRI data.

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [48] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: AIGVE-MACS is a unified model for AI-generated video evaluation, providing numerical scores and multi-aspect language comments. It uses AIGVE-BENCH 2, a large-scale benchmark, and outperforms prior methods like GPT-4o and VideoScore.


<details>
  <summary>Details</summary>
Motivation: Existing metrics lack interpretability and alignment with human evaluation, necessitating a robust framework for AI-generated video assessment.

Method: AIGVE-MACS integrates Vision-Language Models with token-wise weighted loss and dynamic frame sampling, trained on AIGVE-BENCH 2.

Result: Achieves state-of-the-art performance in scoring correlation and comment quality, with a 53.5% quality enhancement in video generation.

Conclusion: AIGVE-MACS sets a new standard for human-aligned, comprehensive evaluation of AI-generated videos, with released benchmarks and models.

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [49] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/abs/2507.01269)
*Mohammad Jahanbakht,Alex Olsen,Ross Marchant,Emilie Fillols,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: A review of weed mapping methods, covering data acquisition, processing, and mapping techniques, to guide future research and sustainable weed management.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive literature reviews on weed mapping and provide a structured analysis of the entire mapping pipeline.

Method: Systematically examines state-of-the-art methods in data acquisition, processing, and mapping techniques, following PRISMA guidelines.

Result: Synthesizes key findings to improve spatial and temporal resolution of weed maps, enabling site-specific management decisions.

Conclusion: The review serves as a foundational reference for future research and sustainable weed management systems.

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [50] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: A novel frequency domain-based diffusion model (\ours) is proposed for unpaired image dehazing, leveraging amplitude spectrum reconstruction and phase correction to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods for unpaired image dehazing introduce irrelevant content and ignore haze-specific frequency domain properties, particularly amplitude spectrum degradation.

Method: The proposed model uses a Diffusion Model (DM) for frequency domain reconstruction, an Amplitude Residual Encoder (ARE) to bridge amplitude gaps, and a Phase Correction Module (PCM) to refine the phase spectrum.

Result: The model outperforms state-of-the-art methods on synthetic and real-world datasets.

Conclusion: The frequency domain approach, combined with DMs and specialized modules, effectively addresses unpaired image dehazing challenges.

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [51] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/abs/2507.01290)
*Sunyong Seo,Semin Kim,Jongha Lee*

Main category: cs.CV

TL;DR: ET-Fuser introduces a novel method for facial analysis by leveraging attention mechanisms and pre-trained models to unify feature representation efficiently.


<details>
  <summary>Details</summary>
Motivation: Current methods lack unified feature representation in single-task learning for facial analysis, despite advancements in CNNs and ViTs.

Method: ET-Fuser uses attention mechanisms and task priors from pre-trained models to generate an ensemble token, sharing mutual information across encoders.

Result: The approach improves facial analysis tasks with significant enhancements in feature representation at minimal computational cost.

Conclusion: ET-Fuser effectively unifies feature representation for facial analysis, demonstrating efficiency and performance improvements.

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [52] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: A technique called DiffusionLight uses iterative inpainting with Stable Diffusion XL to estimate lighting from LDR images, later improved with DiffusionLight-Turbo for faster results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for lighting estimation rely on limited HDR datasets, leading to generalization failures. This work aims to overcome this by leveraging diffusion models.

Method: The approach reframes lighting estimation as a chrome ball inpainting problem, using iterative inpainting for stable priors and fine-tuning an Exposure LoRA for HDR light probes. DiffusionLight-Turbo speeds up the process with a Turbo LoRA.

Result: The method produces convincing light estimates across diverse settings and generalizes well to in-the-wild scenarios, with DiffusionLight-Turbo reducing runtime from 30 minutes to 30 seconds.

Conclusion: DiffusionLight and its turbo variant offer effective and efficient solutions for lighting estimation, demonstrating superior generalization and practicality.

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [53] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/abs/2507.01340)
*Cuong Le,Huy-Phuong Le,Duc Le,Minh-Thien Duong,Van-Binh Nguyen,My-Ha Le*

Main category: cs.CV

TL;DR: A novel method for estimating human ground reaction forces from motion capture data using physics laws and computational simulation, outperforming baseline models in accuracy.


<details>
  <summary>Details</summary>
Motivation: Force plates, used for measuring body dynamics, are limited to lab setups. The paper aims to estimate these dynamics directly from motion capture data.

Method: Uses Euler's integration scheme and PD algorithm to compute ground reaction forces from motion capture data, incorporating physics laws.

Result: Outperforms baseline models in ground reaction force estimation accuracy and simulated root trajectory precision.

Conclusion: The proposed physics-based method improves estimation accuracy and is robust, with potential applications beyond lab environments.

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [54] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: The paper proposes a lightweight method to transform neutral white balance corrections into aesthetic preferences, ensuring consistency across different cameras with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Commercial AWB systems prioritize aesthetics over accuracy, and existing learning-based methods struggle with generalization across camera sensors. This work aims to achieve aesthetic consistency without sacrificing compatibility with cross-camera AWB techniques.

Method: The authors introduce a post-illuminant-estimation mapping that converts neutral corrections into aesthetic ones in a camera-agnostic space. The model is lightweight (~500 parameters) and efficient (0.024 ms runtime).

Result: Tested on 771 smartphone images from three cameras, the method achieves state-of-the-art performance with minimal computational and memory overhead.

Conclusion: The proposed solution successfully bridges the gap between neutral and aesthetic AWB, offering a practical and efficient approach for cross-camera consistency.

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [55] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/abs/2507.01347)
*Andrei Jelea,Ahmed Nabil Belbachir,Marius Leordeanu*

Main category: cs.CV

TL;DR: GTTA is a versatile method enhancing model performance across tasks by perturbing PCA subspace projections and using self-supervised learning to reduce computational costs without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To create a general, off-the-shelf solution for improving model performance in diverse tasks like classification, regression, and object detection.

Method: GTTA applies random perturbations to PCA subspace projections of test inputs, forming robust ensembles, and includes a self-supervised learning stage to refine the model.

Result: GTTA outperforms existing TTA methods and SoTA models across various datasets and tasks, including niche applications like salmon detection.

Conclusion: GTTA is a highly effective, general-purpose method for improving model performance, validated across multiple tasks and datasets.

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [56] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/abs/2507.01351)
*Chaoxiang Cai,Longrong Yang,Kaibing Chen,Fan Yang,Xi Li*

Main category: cs.CV

TL;DR: The paper proposes a Long-Tailed Distribution-aware Router (LTDR) for vision-language token-to-expert routing (TER) in mixture-of-experts (MoE) models, addressing modality-specific routing and enhancing expert activation for vision tail tokens.


<details>
  <summary>Details</summary>
Motivation: Existing MoE frameworks for large vision-language models (LVLMs) overlook distributional differences between vision and language in TER, relying on load balancing mechanisms. This limits performance.

Method: The LTDR introduces a distribution-aware router for modality-specific routing (uniform for language, long-tailed for vision) and an oversampling-like strategy to activate more experts for vision tail tokens.

Result: Experiments on benchmarks show the LTDR's effectiveness in improving vision-language TER performance.

Conclusion: The LTDR addresses key challenges in MoE frameworks for LVLMs, offering better routing strategies and expert activation for vision tail tokens.

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [57] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: PGA introduces a 3D Gaussian Splatting-based framework for physical adversarial attacks, improving robustness and effectiveness across diverse viewpoints and environments.


<details>
  <summary>Details</summary>
Motivation: Existing camouflage-based physical attacks rely on mesh priors and simulators, which are time-consuming and lack real-world accuracy, leading to sub-optimal adversarial solutions.

Method: PGA uses 3D Gaussian Splatting (3DGS) for rapid, precise reconstruction and photo-realistic rendering. It prevents occlusion and employs min-max optimization to adjust backgrounds, filtering non-robust features.

Result: Extensive experiments show PGA's superior adversarial effectiveness and cross-view robustness compared to prior methods.

Conclusion: PGA offers a more efficient and robust solution for physical adversarial attacks, validated by experiments and open-source code.

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [58] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: The paper introduces Activation Reward Models (Activation RMs), a few-shot reward modeling method using activation steering to align LLMs and LMMs with human preferences, outperforming existing methods and mitigating reward hacking.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs and LMMs to human preferences is challenging; traditional reward modeling is inflexible and requires large datasets.

Method: Activation RMs use activation steering to construct reward signals with minimal supervision, avoiding additional finetuning.

Result: Activation RMs outperform existing few-shot methods and mitigate reward hacking, achieving state-of-the-art performance on the new PreferenceHack benchmark.

Conclusion: Activation RMs offer a flexible, effective solution for aligning models to human preferences, with strong performance and safety benefits.

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [59] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: Active measurement is a human-in-the-loop AI framework for scientific measurement, combining AI predictions with human labeling via importance sampling to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current AI workflows lack accuracy and statistical guarantees, limiting their reliability in scientific discovery.

Method: Uses an AI model to predict measurements, samples units for human labeling via importance sampling, and iteratively refines the model and estimates.

Result: Active measurement provides precise estimates with minimal human effort, outperforming alternatives in error reduction.

Conclusion: The framework enhances scientific measurement by balancing AI efficiency with human accuracy, offering robust statistical guarantees.

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [60] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/abs/2507.01384)
*Langyu Wang,Bingke Zhu,Yingying Chen,Yiyuan Zhang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: The paper proposes MUG, a weakly-supervised audio-visual video parsing method using pseudo-labeling and a Mamba network to improve segment- and event-level predictions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with improving both segment- and event-level predictions due to weak supervision and model limitations.

Method: MUG uses pseudo-labeling for data augmentation and an audio-visual Mamba network for feature processing, enhancing segment uniqueness and reducing noise.

Result: MUG achieves state-of-the-art results on the LLP dataset, with gains of 2.1% and 1.2% in visual and audio segment-level metrics.

Conclusion: MUG effectively addresses the limitations of weakly-supervised AVVP, improving performance through pseudo-labeling and the Mamba network.

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [61] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/abs/2507.01390)
*Shuai Tan,Bill Gong,Bin Ji,Ye Pan*

Main category: cs.CV

TL;DR: FixTalk is a novel framework addressing identity leakage and rendering artifacts in talking head generation by decoupling identity from motion features and using leaked identity to fix artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing methods for talking head generation suffer from identity leakage and rendering artifacts, especially in extreme cases.

Method: FixTalk introduces Enhanced Motion Indicator (EMI) to decouple identity from motion features and Enhanced Detail Indicator (EDI) to use leaked identity for artifact correction.

Result: FixTalk outperforms state-of-the-art methods in mitigating identity leakage and rendering artifacts.

Conclusion: FixTalk provides a robust solution for high-quality talking head generation by addressing key challenges of identity leakage and rendering artifacts.

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [62] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: Proposes a method for coherent online HD map construction from onboard sensors using prior SD maps, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of coherent online HD map construction due to the complexity of road topologies.

Method: Uses a network architecture with hybrid lane segment encodings, prior map information, denoising techniques, and temporal consistency from past frames.

Result: Outperforms previous methods by a large margin.

Conclusion: The proposed approach effectively models road topologies and enhances HD map prediction.

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [63] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: A case-level MIL-based method for fetal abdominal anomaly classification in prenatal ultrasound, using MoAE, MFS, and PPL modules, outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate diagnosis of fetal abdominal malformations is critical for pregnancy management, but AI applications in this area are limited, especially for case-level diagnosis.

Method: Proposes a MIL-based approach with MoAE for attention weighting, MFS for medical-knowledge-aligned feature selection, and PPL for enhancing MFS.

Result: Validated on 2,419 cases (24,748 images), the method outperforms existing techniques.

Conclusion: The method advances case-level diagnosis of fetal abdominal anomalies without standard plane localization.

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [64] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/abs/2507.01409)
*Kuniaki Saito,Donghyun Kim,Kwanyong Park,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: CaptionSmiths proposes a method to control caption properties like length and descriptiveness in image captioning models by interpolating between extreme states, achieving smoother transitions and better lexical alignment.


<details>
  <summary>Details</summary>
Motivation: Existing captioning models lack fine-grained control over caption properties (e.g., length, descriptiveness) and cannot smoothly transition between language patterns.

Method: Quantifies caption properties as continuous scalar values and conditions the model via interpolation between endpoint vectors representing extreme states.

Result: The model smoothly adjusts caption properties and improves lexical alignment, reducing length control error by 506%.

Conclusion: CaptionSmiths enables flexible control over caption properties without human annotation, outperforming baselines in alignment and adaptability.

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [65] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: A method for OOD detection using gradient analysis and short-circuiting spurious gradients, improving robustness without major inference changes.


<details>
  <summary>Details</summary>
Motivation: OOD detection is crucial for safe deep model deployment, as inputs may deviate from training data. ID samples show consistent gradient directions, while OOD samples exhibit disorganized gradients.

Method: Proposes an inference-stage technique to short-circuit spurious gradient-exploited features, preserving ID classification. Introduces a local first-order approximation to avoid recomputing logits.

Result: Substantial improvements on standard OOD benchmarks; lightweight and minimally invasive to standard inference.

Conclusion: The method offers a practical, effective solution for robust OOD detection in real-world applications.

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [66] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: The paper introduces DocShaDiffusion, a latent space diffusion model for document shadow removal, addressing color shadows with a shadow soft-mask generation module (SSGM) and a shadow mask-aware guided diffusion module (SMGDM). It also proposes a shadow-robust perceptual feature loss and a synthetic dataset (SDCSRD).


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle color shadows in document images, limiting their effectiveness.

Method: DocShaDiffusion uses latent space translation, SSGM for shadow masks, SMGDM for guided diffusion, and a perceptual feature loss. A synthetic dataset (SDCSRD) is also created.

Result: The method outperforms state-of-the-art on three public datasets.

Conclusion: DocShaDiffusion effectively removes color shadows, preserving document details, and the synthetic dataset supports model training.

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [67] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: DiffMark is a robust watermarking framework using diffusion models to embed watermarks in facial images, enhancing resistance against Deepfake manipulations.


<details>
  <summary>Details</summary>
Motivation: Deepfakes threaten security and privacy; existing watermarking lacks robustness against such manipulations.

Method: DiffMark modifies diffusion model training/sampling, uses facial and watermark conditions, and integrates a CIF module and frozen autoencoder for robustness.

Result: Effective against typical Deepfakes, with improved watermark robustness.

Conclusion: DiffMark offers a novel, robust solution for watermarking in the face of Deepfake threats.

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [68] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/abs/2507.01439)
*Shaocheng Yan,Pengcheng Shi,Zhenjun Zhao,Kaixin Wang,Kuang Cao,Ji Wu,Jiayuan Li*

Main category: cs.CV

TL;DR: TurboReg introduces a fast and robust estimator for point cloud registration using lightweight TurboClique and Pivot-Guided Search (PGS), achieving high recall and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for robust estimation in point cloud registration suffer from exponential time complexity, limiting their practicality in time-sensitive applications.

Method: TurboReg uses TurboClique (a 3-clique in a constrained compatibility graph) and PGS (linear-time algorithm) to efficiently search for high-inlier-ratio cliques.

Result: TurboReg outperforms state-of-the-art methods, e.g., 208.22× faster than 3DMAC on 3DMatch+FCGF, with higher recall.

Conclusion: TurboReg provides a scalable and efficient solution for robust point cloud registration, suitable for real-world applications.

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [69] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/abs/2507.01455)
*Yuxing Liu,Ji Zhang,Zhou Xuchuan,Jingzhong Xiao,Huimin Yang,Jiaxin Zhong*

Main category: cs.CV

TL;DR: OoDDINO is a multi-level anomaly segmentation framework addressing pixel-wise methods' limitations by combining uncertainty-guided detection and adaptive thresholding for fine-grained segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing pixel-wise anomaly segmentation methods struggle with fragmented results and inconsistent thresholds, leading to false positives or missed anomalies.

Method: OoDDINO uses a two-stage approach: (1) OUAFS integrates uncertainty metrics with visual representations for accurate anomaly localization, and (2) ADT-Net dynamically adjusts thresholds for foreground and background regions.

Result: Extensive experiments show OoDDINO outperforms state-of-the-art methods on benchmark datasets.

Conclusion: OoDDINO offers a robust, plug-in compatible solution for fine-grained anomaly segmentation, addressing key challenges in real-world applications.

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [70] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: NOCTIS is a novel framework for instance segmentation of unseen objects, leveraging Grounded-SAM 2 and DINOv2, outperforming existing methods without retraining.


<details>
  <summary>Details</summary>
Motivation: The challenge of segmenting novel objects without retraining motivates a generalizable solution.

Method: Uses Grounded-SAM 2 for proposals and DINOv2 for embeddings, with cyclic patch filtering and confidence weighting for matching.

Result: Outperforms top RGB and RGB-D methods on BOP 2023 datasets.

Conclusion: NOCTIS provides a robust, training-free solution for novel object segmentation.

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [71] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: REG improves diffusion models by entangling low-level image latents with a high-level class token, enhancing generation quality and training efficiency with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing methods like REPA use external visual representations but fail to fully utilize discriminative representations during inference.

Method: Proposes REG, which entangles image latents with a single high-level class token from pretrained models for denoising.

Result: Achieves 63x and 23x faster training than baselines, with superior performance in fewer iterations.

Conclusion: REG effectively leverages semantic knowledge to guide generation, improving efficiency and quality.

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [72] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: The paper proposes efficient, low-power algorithms for onboard methane detection using hyperspectral satellite imagery, introducing Mag1c-SAS and CEM as faster alternatives to traditional methods, and evaluates band selection strategies for improved speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Methane leaks are a significant climate concern, but existing satellite missions rely on manual tasking, missing critical events. Onboard detection is needed, but current methods are too computationally intensive for limited hardware.

Method: The study tests fast target detection methods (ACE, CEM) and introduces Mag1c-SAS, a faster variant of Mag1c. These are integrated with machine learning models (U-Net, LinkNet) and evaluated alongside three band selection strategies.

Result: Mag1c-SAS and CEM show promise, being computationally efficient (up to ~100x and ~230x faster than Mag1c) and accurate for strong plume detection. One band selection strategy outperforms traditional methods with fewer channels.

Conclusion: The research advances onboard methane detection with minimal hardware, enabling timely data delivery. Open-sourced code, data, and models support future developments.

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [73] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/abs/2507.01478)
*Chentao Shen,Ding Pan,Mingyu Mei,Zaixing He,Xinyue Zhao*

Main category: cs.CV

TL;DR: A novel 6DoF pose tracking method for industrial metal objects using active control points to address reflection challenges.


<details>
  <summary>Details</summary>
Motivation: Pose tracking for industrial metal objects is difficult due to reflections, requiring a robust solution.

Method: Uses active control points to generate edge features for optimization, avoiding 6DoF pose-based rendering, and includes optimal control point regression for robustness.

Result: Effective in dataset evaluation and real-world tasks, enabling real-time tracking of metal objects.

Conclusion: The method provides a viable solution for industrial metal object tracking, with publicly available source code.

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [74] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/abs/2507.01484)
*Xiaoshuai Hao,Yuting Zhao,Yuheng Ji,Luanyuan Dai,Peng Hao,Dingzhe Li,Shuai Cheng,Rong Yin*

Main category: cs.CV

TL;DR: The paper proposes methods to enhance robustness in HD map construction for autonomous driving, using data augmentation, a novel fusion module, and modality dropout training, achieving state-of-the-art results on NuScenes data.


<details>
  <summary>Details</summary>
Motivation: Existing HD map construction methods prioritize accuracy over robustness, which is critical for real-world autonomous driving applications.

Method: The approach includes data augmentation, a novel multi-modal fusion module, and modality dropout training, evaluated on 10 days of NuScenes data.

Result: The methods significantly improve robustness while maintaining high accuracy, achieving state-of-the-art performance on the NuScenes validation set.

Conclusion: The findings advance robust and reliable HD map construction, enhancing real-world applicability for autonomous driving.

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [75] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/abs/2507.01492)
*Jiyang Tang,Hengyi Li,Yifan Du,Wayne Xin Zhao*

Main category: cs.CV

TL;DR: AVC-DPO is a post-training framework for video MLLMs that aligns captions with human preferences by focusing on temporal dynamics and spatial information, achieving top performance in the VDC benchmark.


<details>
  <summary>Details</summary>
Motivation: Current video MLLMs struggle to adjust caption emphasis based on human preferences, particularly for temporal and spatial details.

Method: AVC-DPO uses enhanced prompts targeting temporal dynamics and spatial info, leveraging varied prompt conditions for preference-aware training.

Result: AVC-DPO achieved first place in the LOVE@CVPR'25 Workshop Track 1A, excelling in the VDC benchmark.

Conclusion: AVC-DPO effectively aligns video captions with human preferences, enhancing video MLLM performance.

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [76] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: This review analyzes 37 studies (2018-2025) on AI-based pest classification, highlighting trends, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional pest monitoring is slow and manual; deep learning offers scalable, automated solutions.

Method: Review of studies organized by crop type, pest species, model architecture, dataset usage, and technical challenges.

Result: Shift from CNNs to hybrid/transformer models improves accuracy, but challenges like imbalanced datasets and deployment hurdles persist.

Conclusion: AI-based pest monitoring shows promise but requires addressing dataset and deployment issues for broader adoption.

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [77] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/abs/2507.01496)
*Jimyeong Kim,Jungwon Park,Yeji Song,Nojun Kwak,Wonjong Rhee*

Main category: cs.CV

TL;DR: A new training-free method for real-image editing with ReFlow, leveraging mid-step latent and adapted attention for better editability and text alignment.


<details>
  <summary>Details</summary>
Motivation: Adapting ReFlow for real-image editing is challenging, despite its advantages in image quality and text alignment.

Method: Analyzes intermediate representations of multimodal transformer blocks, extracts key features using mid-step latent inversion, and adapts attention during injection.

Result: Superior performance over nine baselines on two benchmarks, validated by human evaluations.

Conclusion: The proposed method is effective, training-free, and user-friendly, requiring no masks or source prompts.

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [78] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: A novel rule-based approach combining traditional and deep learning methods improves tree crown detection accuracy and robustness in forest monitoring.


<details>
  <summary>Details</summary>
Motivation: Addressing global environmental challenges like deforestation and biodiversity loss requires automated forest monitoring, leveraging remote sensing and computer vision.

Method: Integrates traditional methods for feature extraction and segmentation with deep learning for tree crown detection, enhanced by a rule-based post-processing step.

Result: The proposed method increases the number of detected tree crowns and evaluates performance against existing approaches.

Conclusion: The hybrid approach shows promise but highlights areas for further improvement in tree crown detection.

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [79] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: A novel framework, cRID, is introduced to detect and leverage textual describable PII in street-level datasets, improving privacy protection and Re-ID performance.


<details>
  <summary>Details</summary>
Motivation: Street-level datasets for autonomous driving and AI research contain PII, posing privacy risks. Current methods often miss non-biometric PII, necessitating a better solution.

Method: Combines Large Vision-Language Models, Graph Attention Networks, and representation learning to detect and utilize interpretable PII features for Re-ID.

Result: Improved performance in cross-dataset Re-ID (Market-1501 to CUHK03-np), demonstrating practical utility.

Conclusion: cRID effectively addresses PII privacy risks and enhances Re-ID, offering a scalable solution for open datasets.

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [80] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: SAM-MaGuP is a novel polyp segmentation method combining boundary distillation and a 1D-2D Mamba adapter with SAM, achieving superior accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with weak or blurry polyp boundaries and lack generalizability for clinical use.

Method: Incorporates a boundary distillation module and 1D-2D Mamba adapter into SAM to enhance boundary resolution and feature learning.

Result: Outperforms state-of-the-art methods across five datasets, excelling in segmentation accuracy and robustness.

Conclusion: SAM-MaGuP sets a new benchmark in polyp segmentation, addressing key challenges and advancing the field.

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [81] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/abs/2507.01532)
*Tomas Zelezny,Jakub Straka,Vaclav Javorek,Ondrej Valach,Marek Hruz,Ivan Gruber*

Main category: cs.CV

TL;DR: The paper investigates how pose-based data preprocessing (normalization, interpolation, augmentation) impacts Sign Language Translation (SLT) performance using a transformer-based model, showing significant improvements in robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To enhance SLT systems by exploring the effects of pose-based preprocessing techniques on translation accuracy and model performance.

Method: A modified T5 encoder-decoder transformer model processes pose representations, with ablation studies on YouTubeASL and How2Sign datasets to evaluate preprocessing strategies.

Result: Appropriate preprocessing techniques improve model robustness and generalization. Adding a dedicated register token also enhances performance.

Conclusion: Pose-based preprocessing and architectural tweaks like register tokens significantly boost SLT performance, with code and preprocessed data made publicly available.

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [82] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/abs/2507.01535)
*Bingxi Liu,Calvin Chen,Junhao Li,Guyang Yu,Haoqian Song,Xuchen Liu,Jinqiang Cui,Hong Zhang*

Main category: cs.CV

TL;DR: The paper introduces TrackingMiM, a Mamba-in-Mamba architecture, to address the quadratic complexity of Vision Transformers (ViT) in UAV tracking, improving speed and precision.


<details>
  <summary>Details</summary>
Motivation: ViT's quadratic complexity is inefficient for real-time UAV tracking. Mamba's computational efficiency and long-sequence modeling potential offer a solution.

Method: Proposes TrackingMiM, a nested Mamba-in-Mamba architecture, to handle temporal and spatial coherence in image sequences while minimizing computation.

Result: TrackingMiM achieves state-of-the-art precision and higher speed on five UAV tracking benchmarks.

Conclusion: TrackingMiM effectively addresses ViT's limitations, offering a computationally efficient and high-performing solution for UAV tracking.

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [83] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/abs/2507.01539)
*Mohammadreza Amirian,Michael Bach,Oscar Jimenez-del-Toro,Christoph Aberle,Roger Schaer,Vincent Andrearczyk,Jean-Félix Maestrati,Maria Martin Asiain,Kyriakos Flouris,Markus Obmann,Clarisse Dromain,Benoît Dufour,Pierre-Alexandre Alois Poletti,Hendrik von Tengg-Kobligk,Rolf Hügli,Martin Kretzschmar,Hatem Alkadhi,Ender Konukoglu,Henning Müller,Bram Stieltjes,Adrien Depeursinge*

Main category: cs.CV

TL;DR: A benchmark dataset for AI harmonization in CT analysis is introduced to address data distribution shifts caused by scanner variations, with baseline methods and open-source code provided.


<details>
  <summary>Details</summary>
Motivation: AI in medicine struggles with poor generalization due to data distribution shifts from scanner or acquisition changes. This work aims to foster AI harmonization techniques.

Method: The paper presents a dataset of CT scans from an anthropomorphic phantom, acquired with varied scanners and settings, alongside a methodology for assessing stability and classification.

Result: The dataset includes 1378 image series from 13 scanners, with baseline results for image- and feature-level stability and liver tissue classification.

Conclusion: The benchmark dataset and tools provided aim to advance AI harmonization techniques in CT analysis, addressing generalization challenges.

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [84] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/abs/2507.01557)
*Marcin Kowlaczyk,Tomasz Kryjak*

Main category: cs.CV

TL;DR: Proposes a noise reduction method for event cameras using IIR filters, achieving ~99% noise removal with minimal memory usage.


<details>
  <summary>Details</summary>
Motivation: Event cameras produce noisy data streams, hindering their application in neuromorphic vision.

Method: Four algorithms based on IIR filters, tested on modified event datasets with artificial and recorded noise.

Result: ~99% noise removal, preserving valid signal, using ~30KB memory for 1280x720 resolution.

Conclusion: The method is efficient and suitable for embedded devices in neuromorphic vision applications.

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [85] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: The paper proposes IDGBR, a framework combining discriminative and generative learning to improve boundary refinement in remote sensing semantic segmentation by leveraging diffusion models for high-frequency details.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation models focus on low-frequency features (semantic correctness) but struggle with high-frequency details (boundary precision). Diffusion models excel at high-frequency generation but lack semantic inference for low-frequency features.

Method: IDGBR integrates discriminative learning (for coarse segmentation) and diffusion-based generative learning (for boundary refinement). A conditioning guidance network combines the coarse map and original image to guide the diffusion process.

Result: Experiments on five datasets show IDGBR consistently refines boundaries across various discriminative architectures.

Conclusion: The hybrid approach of IDGBR effectively addresses the limitations of both discriminative and generative models, enhancing boundary precision in semantic segmentation.

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [86] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: SketchColour introduces a diffusion transformer-based pipeline for 2D animation colourization, reducing labor and computational costs while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The labor-intensive nature of manual 2D animation colourization drives the need for an efficient, automated solution.

Method: Replaces U-Net with a DiT backbone, uses lightweight adapters and LoRA finetuning for sketch integration, avoiding ControlNet's inefficiencies.

Result: Outperforms state-of-the-art methods on the SAKUGA dataset with half the training data, producing coherent animations with fewer artifacts.

Conclusion: SketchColour offers a scalable, efficient solution for 2D animation colourization, reducing manual effort and computational overhead.

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [87] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: A controllable denoising framework adapts to noise levels using camera parameters (ISO, shutter speed, F-number) to enhance denoising performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based denoising methods lack flexibility in adjusting denoising strength based on noise levels and user preferences.

Method: Converts camera parameters (ISO, shutter speed, F-number) into a vector to control and enhance a denoising network.

Result: The method adds controllability to standard denoising networks and improves performance.

Conclusion: The framework successfully integrates camera parameters for adaptive and improved denoising.

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [88] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: A novel classroom surveillance system integrates drowsiness detection, mobile phone tracking, and face recognition to assess student attentiveness with high precision, achieving strong performance metrics.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance classroom monitoring by combining multiple modalities for real-time assessment of student engagement and behavior, while automating attendance recording.

Method: The system uses YOLOv8 for mobile phone and sleep detection, LResNet Occ FC for face recognition, and integrates these models with YOLO and MTCNN. It is trained on specialized datasets (RMFD for faces, Roboflow for phones) and implemented via a PHP web app with ESP32-CAM hardware.

Result: Performance metrics include 97.42% mAP@50 for sleep detection, 86.45% validation accuracy for face recognition, and 85.89% mAP@50 for mobile phone detection.

Conclusion: The integrated system offers scalable, real-time monitoring for classrooms, improving engagement tracking and automating attendance.

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [89] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync is a training-free framework using diffusion guidance to achieve consistent depth predictions for long videos by addressing scale discrepancies and geometric inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video depth estimation suffer from scale discrepancies and geometric inconsistencies due to sliding window approaches and reliance on 2D priors.

Method: DepthSync introduces scale guidance to synchronize depth scales across windows and geometry guidance to enforce 3D geometric alignment within windows.

Result: Experiments show DepthSync improves scale and geometry consistency in depth predictions, especially for long videos.

Conclusion: DepthSync effectively addresses challenges in long-video depth estimation, offering a robust, training-free solution.

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [90] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: This paper studies backdoor attacks in deep learning-based face recognition systems, demonstrating new attacks and providing countermeasures.


<details>
  <summary>Details</summary>
Motivation: Addressing the security blind spot in real-life, unconstrained face recognition systems vulnerable to DNN backdoor attacks.

Method: Explores feasibility of DNN backdoors holistically, demonstrating face generation and landmark shift attacks, and testing pipeline configurations.

Result: Shows a single backdoor can bypass entire system functions, with 20 pipeline configurations and 15 attack cases tested.

Conclusion: Provides best practices and countermeasures for stakeholders to mitigate backdoor vulnerabilities.

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [91] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/abs/2507.01608)
*Xu Zhang,Ming Lu,Yan Chen,Zhan Ma*

Main category: cs.CV

TL;DR: POLC introduces a perception-oriented latent coding method to enhance semantic richness in compressed domain inference, reducing fine-tuning overhead.


<details>
  <summary>Details</summary>
Motivation: MSE-optimized models yield limited semantic richness and require intensive fine-tuning, hindering downstream task performance.

Method: POLC enriches latent features semantically, enabling plug-and-play adapters for fine-tuning with fewer parameters.

Result: POLC matches state-of-the-art generative coding in rate-perception and improves vision task performance with minimal overhead.

Conclusion: POLC offers a computationally efficient solution for high-performance semantic inference in compressed domains.

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [92] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: P3HOT framework improves HOT detection by combining prompt guidance and human proximal perception, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current HOT detection models are limited to single image types, causing over-segmentation and inconsistent categories.

Method: Uses semantic-driven prompts and human proximal perception with depth calculation, plus a new Regional Joint Loss (RJLOSS).

Result: Achieves improvements of 0.7, 2.0, 1.6, and 11.0 in SC-Acc., mIoU, wIoU, and AD-Acc. metrics.

Conclusion: P3HOT outperforms existing methods, addressing key challenges in HOT detection.

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [93] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF scales NeRF for large scenes by dividing them into non-overlapping 3D tiles, using an out-of-core method and a novel tile progression strategy, achieving linear time complexity on a single GPU without quality loss.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art NeRF methods are limited to small scenes due to memory constraints during training.

Method: Divides scenes into non-overlapping 3D tiles, crops images with overlap, and uses a $2\times2$ 3D tile progression strategy with a segmented sampler to avoid edge errors.

Result: Large satellite images can be processed efficiently with linear time complexity on a single GPU, maintaining quality.

Conclusion: Snake-NeRF effectively scales NeRF for large scenes without compromising quality or requiring excessive resources.

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [94] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: DepthAnything-AC is a monocular depth estimation model designed for diverse environmental conditions, improving performance in challenging scenarios like adverse weather and illumination variations.


<details>
  <summary>Details</summary>
Motivation: Existing foundation MDE models struggle in complex open-world environments with challenging conditions, necessitating a more robust solution.

Method: The model uses unsupervised consistency regularization finetuning and a Spatial Distance Constraint for better patch-level relationships and semantic boundaries.

Result: DepthAnything-AC shows strong zero-shot performance across diverse benchmarks, including adverse weather and synthetic corruption tests.

Conclusion: The proposed model effectively addresses data scarcity and corruption challenges, outperforming previous methods in complex conditions.

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [95] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: Summary of the 1st W-CODA workshop at ECCV 2024, focusing on autonomous driving corner cases using multimodal perception.


<details>
  <summary>Details</summary>
Motivation: To advance next-gen solutions for autonomous driving corner cases by leveraging cutting-edge multimodal techniques.

Method: Organized a workshop with 5 speakers, collected research papers, and held a dual-track challenge (scene understanding and generation).

Result: Brought together academia and industry to share progress and opinions, fostering collaboration.

Conclusion: The workshop aims to bridge the gap between advanced techniques and reliable self-driving agents for corner cases.

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [96] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/abs/2507.01643)
*Weijie Yin,Dingkang Yang,Hongyuan Dong,Zijian Kang,Jiacong Wang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: SAILViT is a Vision Transformer designed to enhance Multimodal Large Language Models (MLLMs) by addressing parameter conflicts and semantic gaps, improving performance in complex multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Vision Transformers (ViTs) struggle with co-training directly with LLMs due to initialization conflicts and modality gaps, limiting MLLM performance.

Method: SAILViT uses gradual feature refinement for coarse-to-fine-grained alignment and knowledge infusion, tailored for MLLM training.

Result: SAILViT demonstrates robustness across various dimensions (e.g., parameter sizes, architectures) and boosts MLLM performance on the OpenCompass benchmark.

Conclusion: SAILViT effectively bridges ViTs and LLMs, enabling significant performance gains in multimodal tasks, with models publicly available.

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [97] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: LASADGen introduces a linear attention mechanism (LASAD) for autoregressive image generation, preserving 2D spatial relationships efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing AR models using transformers suffer from high computational complexity and memory overhead, while linear attention degrades image quality by failing to capture long-range dependencies.

Method: Proposes LASAD, a linear attention mechanism with position-dependent decay factors based on true 2D spatial locations, implemented in LASADGen.

Result: LASADGen achieves state-of-the-art performance and efficiency on ImageNet, balancing linear complexity with spatial understanding.

Conclusion: LASADGen bridges the gap between efficiency and quality in autoregressive image generation, outperforming existing methods.

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [98] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: RobuSTereo enhances zero-shot generalization of stereo matching in adverse weather by addressing data scarcity and feature extraction challenges using synthetic data and a robust feature encoder.


<details>
  <summary>Details</summary>
Motivation: Stereo matching models struggle in adverse weather due to lack of training data and difficulty in extracting features from degraded images, limiting zero-shot generalization.

Method: 1) Diffusion-based simulation pipeline with stereo consistency for synthetic data generation. 2) Robust feature encoder combining ConvNet and denoising transformer.

Result: RobuSTereo improves robustness and generalization of stereo matching models in diverse adverse weather conditions.

Conclusion: The framework effectively addresses data and feature challenges, enhancing stereo matching performance in unseen weather scenarios.

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [99] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SPoT introduces subpixel token placement to overcome grid-based tokenization limits in Vision Transformers, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Standard tokenization restricts Vision Transformers to grid-based features, limiting sparsity exploitation.

Method: Proposes Subpixel Placement of Tokens (SPoT) with oracle-guided search for optimal token positioning.

Result: Achieves significant performance gains with fewer tokens, enhancing efficiency.

Conclusion: SPoT redefines sparsity as an advantage, enabling flexible and interpretable ViT architectures.

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [100] [What does really matter in image goal navigation?](https://arxiv.org/abs/2507.01667)
*Gianluca Monaci,Philippe Weinzaepfel,Christian Wolf*

Main category: cs.CV

TL;DR: The paper explores whether end-to-end RL training can efficiently solve image goal navigation, analyzing architectural choices and simulator impacts.


<details>
  <summary>Details</summary>
Motivation: To determine if end-to-end RL training can replace dedicated methods for image goal navigation and enable relative pose estimation from navigation rewards.

Method: Investigates architectural choices (late fusion, channel stacking, etc.) and their impact on emergent relative pose estimators through large-scale RL training.

Result: Simulator settings influence method success, but capabilities can transfer to realistic settings. Navigation performance correlates with emergent relative pose estimation.

Conclusion: End-to-end RL training shows promise for image goal navigation, though simulator shortcuts exist, and emergent skills correlate with navigation success.

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


### [101] [Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2507.01673)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: FACET-VLM, a vision-language framework for 3D/4D facial expression recognition, integrates multiview learning with semantic guidance, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Advancing applications in human behavior understanding, healthcare monitoring, and human-computer interaction by addressing the complexity of spatial and temporal facial dynamics.

Method: Proposes FACET-VLM with Cross-View Semantic Aggregation (CVSA), Multiview Text-Guided Fusion (MTGF), and a multiview consistency loss for coherent facial emotion recognition.

Result: Achieves top accuracy on benchmarks like BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous, and extends successfully to 4D micro-expression recognition.

Conclusion: FACET-VLM is a robust, extensible, and high-performing solution for multimodal facial expression recognition in posed and spontaneous settings.

Abstract: Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompts. FACET-VLM introduces three key
components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,
Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,
and a multiview consistency loss to enforce structural coherence across views.
Our model achieves state-of-the-art accuracy across multiple benchmarks,
including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend
FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,
demonstrating strong performance in capturing subtle, short-lived emotional
cues. The extensive experimental results confirm the effectiveness and
substantial contributions of each individual component within the framework.
Overall, FACET-VLM offers a robust, extensible, and high-performing solution
for multimodal FER in both posed and spontaneous settings.

</details>


### [102] [Component Adaptive Clustering for Generalized Category Discovery](https://arxiv.org/abs/2507.01711)
*Mingfu Yan,Jiancheng Huang,Yifan Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: AdaGCD introduces Adaptive Slot Attention (AdaSlot) for Generalized Category Discovery (GCD), dynamically determining class counts without predefined assumptions, improving flexibility and accuracy in categorizing unlabeled images.


<details>
  <summary>Details</summary>
Motivation: Traditional GCD methods rely on rigid assumptions like predefined class counts, limiting adaptability to real-world data variability. AdaGCD aims to overcome this by dynamically adapting to data complexity.

Method: AdaGCD uses a cluster-centric contrastive learning framework with AdaSlot, which dynamically allocates slots based on data complexity, enabling flexible clustering of known and novel classes.

Result: Experiments on public and fine-grained datasets show AdaGCD's effectiveness, particularly in leveraging spatial local information for improved category discovery.

Conclusion: AdaGCD's adaptive approach enhances GCD by removing rigid assumptions, offering a more flexible and accurate solution for open-world scenarios.

Abstract: Generalized Category Discovery (GCD) tackles the challenging problem of
categorizing unlabeled images into both known and novel classes within a
partially labeled dataset, without prior knowledge of the number of unknown
categories. Traditional methods often rely on rigid assumptions, such as
predefining the number of classes, which limits their ability to handle the
inherent variability and complexity of real-world data. To address these
shortcomings, we propose AdaGCD, a cluster-centric contrastive learning
framework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD
framework. AdaSlot dynamically determines the optimal number of slots based on
data complexity, removing the need for predefined slot counts. This adaptive
mechanism facilitates the flexible clustering of unlabeled data into known and
novel categories by dynamically allocating representational capacity. By
integrating adaptive representation with dynamic slot allocation, our method
captures both instance-specific and spatially clustered features, improving
class discovery in open-world scenarios. Extensive experiments on public and
fine-grained datasets validate the effectiveness of our framework, emphasizing
the advantages of leveraging spatial local information for category discovery
in unlabeled image datasets.

</details>


### [103] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: Proposes a wavelet domain fingerprint for SPN extraction, bypassing image inversion and improving accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Enhance efficiency and accuracy in camera fingerprint detection by avoiding the inversion step in wavelet denoising.

Method: Modifies wavelet-based SPN extraction by introducing wavelet domain fingerprints, enabling direct comparisons in the wavelet domain.

Result: Higher detection accuracy and significantly improved processing speed on real-world datasets.

Conclusion: The wavelet domain fingerprint method streamlines SPN extraction and comparison, offering better performance.

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [104] [Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation](https://arxiv.org/abs/2507.01721)
*Zhongwen Zhang,Yuri Boykov*

Main category: cs.CV

TL;DR: Soft self-labeling improves weakly supervised segmentation (WSSS) by optimizing CRF/Potts loss with relaxations, outperforming complex systems and even full supervision.


<details>
  <summary>Details</summary>
Motivation: Hard pseudo-labels in WSSS lack uncertainty representation, motivating soft self-labeling for better performance.

Method: Derives an auxiliary loss, evaluates CRF relaxations, and proposes a continuous sub-problem solver for soft self-labeling.

Result: Soft self-labeling enhances scribble-based training, surpassing specialized WSSS systems and full supervision.

Conclusion: The approach is effective for WSSS and applicable to other weakly-supervised problems.

Abstract: We consider weakly supervised segmentation where only a fraction of pixels
have ground truth labels (scribbles) and focus on a self-labeling approach
optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled
pixels. While WSSS methods can directly optimize such losses via gradient
descent, prior work suggests that higher-order optimization can improve network
training by introducing hidden pseudo-labels and powerful CRF sub-problem
solvers, e.g. graph cut. However, previously used hard pseudo-labels can not
represent class uncertainty or errors, which motivates soft self-labeling. We
derive a principled auxiliary loss and systematically evaluate standard and new
CRF relaxations (convex and non-convex), neighborhood systems, and terms
connecting network predictions with soft pseudo-labels. We also propose a
general continuous sub-problem solver. Using only standard architectures, soft
self-labeling consistently improves scribble-based training and outperforms
significantly more complex specialized WSSS systems. It can outperform full
pixel-precise supervision. Our general ideas apply to other weakly-supervised
problems/systems.

</details>


### [105] [When Does Pruning Benefit Vision Representations?](https://arxiv.org/abs/2507.01722)
*Enrico Cassano,Riccardo Renzulli,Andrea Bragagnolo,Marco Grangetto*

Main category: cs.CV

TL;DR: The paper explores how pruning affects vision models in interpretability, unsupervised object discovery, and alignment with human perception, identifying 'sweet spots' where sparsity improves performance.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of pruning on interpretability, representation learning, and human-aligned features in vision models.

Method: Analyzed different vision network architectures under varying sparsity levels, focusing on feature attribution, unsupervised object discovery, and human perception alignment.

Result: Found 'sweet spots' where sparse models show higher interpretability, generalization, and human alignment, though results vary by architecture and size.

Conclusion: Pruning's benefits are complex and context-dependent, emphasizing the need for further investigation into its effects on vision representations.

Abstract: Pruning is widely used to reduce the complexity of deep learning models, but
its effects on interpretability and representation learning remain poorly
understood. This paper investigates how pruning influences vision models across
three key dimensions: (i) interpretability, (ii) unsupervised object discovery,
and (iii) alignment with human perception. We first analyze different vision
network architectures to examine how varying sparsity levels affect feature
attribution interpretability methods. Additionally, we explore whether pruning
promotes more succinct and structured representations, potentially improving
unsupervised object discovery by discarding redundant information while
preserving essential features. Finally, we assess whether pruning enhances the
alignment between model representations and human perception, investigating
whether sparser models focus on more discriminative features similarly to
humans. Our findings also reveal the presence of sweet spots, where sparse
models exhibit higher interpretability, downstream generalization and human
alignment. However, these spots highly depend on the network architectures and
their size in terms of trainable parameters. Our results suggest a complex
interplay between these three dimensions, highlighting the importance of
investigating when and how pruning benefits vision representations.

</details>


### [106] [HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion](https://arxiv.org/abs/2507.01737)
*Lin Wu,Zhixiang Chen,Jianglin Lan*

Main category: cs.CV

TL;DR: HOI-Dyn is a framework for generating realistic 3D human-object interactions by modeling them as a driver-responder system, using a transformer-based dynamics model and a residual-based loss for consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model detailed interaction dynamics, leading to implausible behaviors.

Method: HOI-Dyn uses a transformer-based interaction dynamics model to predict object responses to human motion, with a residual-based dynamics loss for consistency.

Result: The approach improves HOI generation quality and provides a feasible evaluation metric.

Conclusion: HOI-Dyn effectively addresses the challenge of generating realistic 3D human-object interactions.

Abstract: Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-based interaction dynamics
model that explicitly predicts how objects should react to human motion. To
further enforce consistency, we introduce a residual-based dynamics loss that
mitigates the impact of dynamics prediction errors and prevents misleading
optimization signals. The dynamics model is used only during training,
preserving inference efficiency. Through extensive qualitative and quantitative
experiments, we demonstrate that our approach not only enhances the quality of
HOI generation but also establishes a feasible metric for evaluating the
quality of generated interactions.

</details>


### [107] [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/abs/2507.01738)
*Ming Dai,Wenxuan Cheng,Jiang-jiang Liu,Sen Yang,Wenxiao Cai,Yanpeng Sun,Wankou Yang*

Main category: cs.CV

TL;DR: DeRIS decomposes RIS into perception and cognition, identifies cognitive limitations as the main bottleneck, and introduces Loopback Synergy and data augmentation to improve performance.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze and address the fundamental bottlenecks in existing RIS frameworks, which are underexplored.

Method: DeRIS decomposes RIS into perception and cognition, introduces Loopback Synergy for better module synergy, and uses non-referent sample conversion for data augmentation.

Result: DeRIS improves segmentation precision and robust image-text comprehension, adapting to non- and multi-referents scenarios without architectural changes.

Conclusion: DeRIS effectively addresses cognitive limitations in RIS, enhancing performance and general applicability.

Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment
objects in an image based on natural language expressions. While prior studies
have predominantly concentrated on improving vision-language interactions and
achieving fine-grained localization, a systematic analysis of the fundamental
bottlenecks in existing RIS frameworks remains underexplored. To bridge this
gap, we propose DeRIS, a novel framework that decomposes RIS into two key
components: perception and cognition. This modular decomposition facilitates a
systematic analysis of the primary bottlenecks impeding RIS performance. Our
findings reveal that the predominant limitation lies not in perceptual
deficiencies, but in the insufficient multi-modal cognitive capacity of current
models. To mitigate this, we propose a Loopback Synergy mechanism, which
enhances the synergy between the perception and cognition modules, thereby
enabling precise segmentation while simultaneously improving robust image-text
comprehension. Additionally, we analyze and introduce a simple non-referent
sample conversion data augmentation to address the long-tail distribution issue
related to target existence judgement in general scenarios. Notably, DeRIS
demonstrates inherent adaptability to both non- and multi-referents scenarios
without requiring specialized architectural modifications, enhancing its
general applicability. The codes and models are available at
https://github.com/Dmmm1997/DeRIS.

</details>


### [108] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. Valdés Hernández*

Main category: cs.CV

TL;DR: ViTs pre-trained with MAE outperform supervised nnU-Net in 3D medical image segmentation for IAC, showing improved robustness and clinical utility.


<details>
  <summary>Details</summary>
Motivation: ViTs are underutilized in 3D medical imaging despite their potential for self-supervised training, which avoids costly annotations. IAC quantification can aid in large-scale risk assessment for neurovascular diseases.

Method: Pre-train ViTs using MAE, fine-tune for IAC segmentation on heterogeneous data from IST-3. Evaluate key aspects like patch size and upsampling methods.

Result: MAE pre-trained ViTs outperform nnU-Net by 3.2 Dice points, show robustness to higher slice thicknesses, and improve risk classification by 46%.

Conclusion: Self-supervised ViTs are effective for 3D medical image segmentation, offering clinical benefits and robustness, with low patch sizes and interpolation upsampling being optimal.

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to
neurovascular diseases such as stroke and dementia, and automated IAC
quantification could enable their large-scale risk assessment. We pre-train
ViTs with MAE and fine-tune them for IAC segmentation for the first time. To
develop our models, we use highly heterogeneous data from a large clinical
trial, the third International Stroke Trial (IST-3). We evaluate key aspects of
MAE pre-trained ViTs in IAC segmentation, and analyse the clinical
implications. We show: 1) our calibrated self-supervised ViT beats a strong
supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial
for ViTs for IAC segmentation and interpolation upsampling with regular
convolutions is preferable to transposed convolutions for ViT-based models, and
3) our ViTs increase robustness to higher slice thicknesses and improve risk
group classification in a clinical scenario by 46%. Our code is available
online.

</details>


### [109] [SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery](https://arxiv.org/abs/2507.01747)
*Nora Gourmelon,Marcel Dreier,Martin Mayr,Thorsten Seehaus,Dakota Pyles,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: The paper introduces self-supervised pretraining techniques and a hybrid model to improve glacier calving front monitoring using SAR imagery, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Accurate monitoring of glacier ice loss is critical, but existing deep learning models are suboptimal due to domain shifts between natural and remote sensing imagery.

Method: Proposes two self-supervised pretraining techniques and a hybrid Swin Transformer-CNN model, pretrained on a new dataset (SSL4SAR).

Result: Achieves a mean distance error of 293 m on the CaFFe benchmark, outperforming prior models by 67 m, and 75 m in a multi-annotator study.

Conclusion: The advancements enable precise seasonal monitoring of glacier calving fronts, nearing human performance.

Abstract: Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the domain shift between the natural images in
ImageNet and the specialized characteristics of remote sensing imagery, in
particular for Synthetic Aperture Radar imagery. To address this challenge, we
propose two novel self-supervised multimodal pretraining techniques that
leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14
Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the
dataset. Additionally, we introduce a novel hybrid model architecture that
combines a Swin Transformer encoder with a residual Convolutional Neural
Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean
distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe)
benchmark dataset, outperforming the prior best model by 67 m. Evaluating an
ensemble of the proposed model on a multi-annotator study of the benchmark
dataset reveals a mean distance error of 75 m, approaching the human
performance of 38 m. This advancement enables precise monitoring of seasonal
changes in glacier calving fronts.

</details>


### [110] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: DisCon improves image generation by using discrete tokens as conditional signals for continuous autoregressive modeling, avoiding quantization loss and outperforming AR methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the information loss in AR-based visual generation due to quantization and the challenges of continuous token modeling.

Method: Introduces DisCon, a framework that models continuous representations conditioned on discrete tokens, avoiding direct quantization.

Result: Achieves a gFID score of 1.38 on ImageNet 256×256, surpassing state-of-the-art AR methods.

Conclusion: DisCon effectively balances the trade-off between quantization loss and continuous modeling challenges, advancing visual generation.

Abstract: Recent advances in large language models (LLMs) have spurred interests in
encoding images as discrete tokens and leveraging autoregressive (AR)
frameworks for visual generation. However, the quantization process in AR-based
visual generation models inherently introduces information loss that degrades
image fidelity. To mitigate this limitation, recent studies have explored to
autoregressively predict continuous tokens. Unlike discrete tokens that reside
in a structured and bounded space, continuous representations exist in an
unbounded, high-dimensional space, making density estimation more challenging
and increasing the risk of generating out-of-distribution artifacts. Based on
the above findings, this work introduces DisCon (Discrete-Conditioned
Continuous Autoregressive Model), a novel framework that reinterprets discrete
tokens as conditional signals rather than generation targets. By modeling the
conditional probability of continuous representations conditioned on discrete
tokens, DisCon circumvents the optimization challenges of continuous token
modeling while avoiding the information loss caused by quantization. DisCon
achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation,
outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [111] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: Vision transformers (ViTs) in medical imaging lack semantic meaningfulness in representations, making them vulnerable to small changes and unreliable for classification.


<details>
  <summary>Details</summary>
Motivation: To investigate whether ViT representations are semantically meaningful and robust for medical imaging tasks.

Method: Used a projected gradient-based algorithm to analyze ViT representations and their vulnerability to small changes.

Result: ViT representations are not semantically meaningful; imperceptible changes cause significant accuracy drops (over 60%).

Conclusion: ViTs pose a critical challenge for safety-critical medical systems due to unreliable representations.

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [112] [Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation](https://arxiv.org/abs/2507.01791)
*Zihong Guo,Chen Wan,Yayin Zheng,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: The paper introduces a Segmented Gaussian Pyramid (SGP) attack method to improve adversarial example transferability against defense models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the security challenge posed by adversarial example transferability in deep neural networks, especially against defense models.

Method: Uses Gaussian filtering and three types of downsampling to create multi-scale examples, averaging gradients of the loss function to determine perturbations.

Result: SGP significantly boosts attack success rates against black-box defense models, with improvements ranging from 2.3% to 32.6%.

Conclusion: SGP is a highly extensible and effective method for enhancing adversarial attack transferability.

Abstract: The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale examples. Then, the gradients of the loss
function with respect to each scale are computed, and their average is used to
determine the adversarial perturbations. The proposed SGP can be considered an
input transformation with high extensibility that is easily integrated into
most existing adversarial attacks. Extensive experiments demonstrate that in
contrast to the state-of-the-art methods, SGP significantly enhances attack
success rates against black-box defense models, with average attack success
rates increasing by 2.3% to 32.6%, based only on transferability.

</details>


### [113] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: FreeLoRA is a framework for training-free fusion of subject-specific LoRA modules, enabling multi-subject personalization in image generation without complex re-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-subject personalization due to the need for complex re-tuning or joint optimization.

Method: FreeLoRA uses Full Token Tuning for subject-specific LoRA modules and Subject-Aware Inference to activate modules only on corresponding subject tokens.

Result: FreeLoRA achieves strong performance in subject fidelity and prompt consistency.

Conclusion: FreeLoRA provides a simple and generalizable solution for multi-subject personalization in image generation.

Abstract: Subject-driven image generation plays a crucial role in applications such as
virtual try-on and poster design. Existing approaches typically fine-tune
pretrained generative models or apply LoRA-based adaptations for individual
subjects. However, these methods struggle with multi-subject personalization,
as combining independently adapted modules often requires complex re-tuning or
joint optimization. We present FreeLoRA, a simple and generalizable framework
that enables training-free fusion of subject-specific LoRA modules for
multi-subject personalization. Each LoRA module is adapted on a few images of a
specific subject using a Full Token Tuning strategy, where it is applied across
all tokens in the prompt to encourage weakly supervised token-content
alignment. At inference, we adopt Subject-Aware Inference, activating each
module only on its corresponding subject tokens. This enables training-free
fusion of multiple personalized subjects within a single image, while
mitigating overfitting and mutual interference between subjects. Extensive
experiments show that FreeLoRA achieves strong performance in both subject
fidelity and prompt consistency.

</details>


### [114] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: The paper introduces HCNQA, a 3D VQA model using hierarchical concentration narrowing supervision to ensure rational reasoning pathways, outperforming answer-centric methods.


<details>
  <summary>Details</summary>
Motivation: Answer-centric supervision in 3D VQA lacks reasoning pathway oversight, leading to superficial shortcuts. Slow-thinking methods also suffer from underthinking.

Method: HCNQA mimics human focus narrowing, supervising key reasoning checkpoints hierarchically to guide rational reasoning.

Result: Experiments show HCNQA ensures effective reasoning pathways and better performance.

Conclusion: HCNQA addresses reasoning pathway supervision gaps, improving 3D VQA model performance.

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [115] [AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](https://arxiv.org/abs/2507.01801)
*Bin Rao,Haicheng Liao,Yanchen Guan,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: AMD framework improves trajectory prediction by addressing long-tail data imbalance using adaptive momentum and decoupled contrastive learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle the diversity and uncertainty of long-tail trajectory patterns, crucial for complex and hazardous scenarios in autonomous driving.

Method: Proposes AMD framework with MoCo-DT and DCL modules, trajectory augmentation, and online iterative clustering for dynamic pseudo-label updates.

Result: AMD achieves optimal performance in long-tail trajectory prediction and superior overall accuracy on nuScenes and ETH/UCY datasets.

Conclusion: AMD effectively addresses long-tail data challenges, enhancing prediction accuracy for rare and complex trajectories.

Abstract: Accurately predicting the future trajectories of traffic agents is essential
in autonomous driving. However, due to the inherent imbalance in trajectory
distributions, tail data in natural datasets often represents more complex and
hazardous scenarios. Existing studies typically rely solely on a base model's
prediction error, without considering the diversity and uncertainty of
long-tail trajectory patterns. We propose an adaptive momentum and decoupled
contrastive learning framework (AMD), which integrates unsupervised and
supervised contrastive learning strategies. By leveraging an improved momentum
contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,
our framework enhances the model's ability to recognize rare and complex
trajectories. Additionally, we design four types of trajectory random
augmentation methods and introduce an online iterative clustering strategy,
allowing the model to dynamically update pseudo-labels and better adapt to the
distributional shifts in long-tail data. We propose three different criteria to
define long-tail trajectories and conduct extensive comparative experiments on
the nuScenes and ETH$/$UCY datasets. The results show that AMD not only
achieves optimal performance in long-tail trajectory prediction but also
demonstrates outstanding overall prediction accuracy.

</details>


### [116] [Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views](https://arxiv.org/abs/2507.01835)
*Daniil Reutsky,Daniil Vladimirov,Yasin Mamedov,Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: A multi-image-to-hyperspectral reconstruction (MI-HSR) framework using a triple-camera smartphone system improves accuracy by 30% over single-camera methods.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral reconstruction (HSR) from RGB images is ill-posed due to spectral information loss; existing single-image methods are limited.

Method: Proposes MI-HSR with a triple-camera smartphone system (two lenses with spectral filters) and introduces the Doomer dataset for validation.

Result: Achieves 30% more accurate spectra estimation compared to conventional RGB cameras.

Conclusion: Multi-view spectral filtering with commodity hardware enables more accurate and practical hyperspectral imaging.

Abstract: Hyperspectral reconstruction (HSR) from RGB images is a fundamentally
ill-posed problem due to severe spectral information loss. Existing approaches
typically rely on a single RGB image, limiting reconstruction accuracy. In this
work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)
framework that leverages a triple-camera smartphone system, where two lenses
are equipped with carefully selected spectral filters. Our configuration,
grounded in theoretical and empirical analysis, enables richer and more diverse
spectral observations than conventional single-camera setups. To support this
new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising
aligned images from three smartphone cameras and a hyperspectral reference
camera across diverse scenes. We show that the proposed HSR model achieves
consistent improvements over existing methods on the newly proposed benchmark.
In a nutshell, our setup allows 30% towards more accurately estimated spectra
compared to an ordinary RGB camera. Our findings suggest that multi-view
spectral filtering with commodity hardware can unlock more accurate and
practical hyperspectral imaging solutions.

</details>


### [117] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: A lightweight CNN framework with 4K parameters achieves real-time image enhancement (IE) at 1,100 FPS, balancing speed and performance.


<details>
  <summary>Details</summary>
Motivation: Deploying deep learning models on resource-constrained platforms like mobile devices is challenging due to high computation and memory demands.

Method: Integrates reparameterization, Incremental Weight Optimization, Feature Self-Transform module, Hierarchical Dual-Path Attention, and Local Variance-Weighted loss.

Result: Achieves real-time IE at 1,100 FPS with competitive image quality, offering the best speed-performance trade-off.

Conclusion: The framework enables efficient, real-time IE on mobile devices, with code available for public use.

Abstract: Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incremental Weight Optimization
strategy to ensure efficiency. Additionally, we enhance performance with a
Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,
optimized with a Local Variance-Weighted loss. With this efficient framework,
we are the first to achieve real-time IE inference at up to 1,100 frames per
second (FPS) while delivering competitive image quality, achieving the best
trade-off between speed and performance across multiple IE tasks. The code will
be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [118] [Future Slot Prediction for Unsupervised Object Discovery in Surgical Video](https://arxiv.org/abs/2507.01882)
*Guiqiu Liao,Matjaz Jogan,Marcel Hussing,Edward Zhang,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: A dynamic temporal slot transformer (DTST) improves unsupervised object-centric learning for surgical videos, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous scenes in real-world applications like surgery are hard to parse into meaningful slots, and current adaptive slot count methods underperform on surgical videos.

Method: Proposes a DTST module trained for temporal reasoning and predicting optimal future slot initialization.

Result: Achieves state-of-the-art performance on multiple surgical databases.

Conclusion: Unsupervised object-centric methods can be effectively applied to real-world healthcare data like surgical videos.

Abstract: Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.

</details>


### [119] [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/abs/2507.01884)
*Kunlun Xu,Fan Zhuo,Jiangmeng Li,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: The paper introduces SPRED, a framework for Semi-Supervised Lifelong Person Re-Identification (Semi-LReID), addressing performance degradation in unlabeled data scenarios by combining dynamic prototype-guided pseudo-label generation and dual-knowledge purification.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios often lack labeled data, causing existing LReID methods to degrade. The paper aims to improve Semi-LReID by tackling noisy knowledge in unlabeled data utilization.

Method: SPRED uses learnable identity prototypes for pseudo-label generation and a dual-knowledge cooperation scheme to refine labels, creating a self-reinforcing cycle for better unlabeled data use.

Result: SPRED achieves state-of-the-art performance on Semi-LReID benchmarks.

Conclusion: The proposed SPRED framework effectively enhances Semi-LReID by improving pseudo-label quality and knowledge propagation over long-term learning.

Abstract: Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED

</details>


### [120] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: The paper introduces Reason50K, a dataset for hypothetical instruction reasoning in image editing, and ReasonBrain, a framework leveraging MLLMs and diffusion models for complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex implicit instructions and lack datasets or mechanisms for reasoning-aware editing.

Method: Proposes ReasonBrain with a Fine-grained Reasoning Cue Extraction module and Cross-Modal Enhancer for detailed semantics and interaction.

Result: ReasonBrain outperforms baselines in reasoning scenarios and generalizes well to conventional tasks.

Conclusion: The work advances instruction-based image editing by addressing reasoning gaps and introduces a publicly available dataset and framework.

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.

</details>


### [121] [Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion](https://arxiv.org/abs/2507.01909)
*Jorge Tapias Gomez,Nishant Nadkarni,Lando S. Bosma,Jue Jiang,Ergys D. Subashi,William P. Segars,James M. Balter,Mert R Sabuncu,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: The paper introduces a pipeline for creating patient-specific digital twins (DTs) to assess deformable image registration (DIR) accuracy in gastrointestinal (GI) organs, validated against real patient data.


<details>
  <summary>Details</summary>
Motivation: Clinical DIR implementation lacks reliable voxel-based accuracy metrics for highly mobile GI organs, necessitating a robust validation method.

Method: A semi-automated pipeline generated 21 motion phases from static 3D scans using analytical GI motion models. Six DIR methods were evaluated using DTs, with metrics like target registration error and dose warping.

Result: DTs replicated realistic GI motion, closely matching real-patient data. The pipeline provided detailed DIR performance metrics and validated dose mapping accuracy.

Conclusion: The pipeline offers a rigorous method to test DIR tools for dynamic, complex regions, improving spatial and dosimetric accuracy assessments.

Abstract: Objective: Clinical implementation of deformable image registration (DIR)
requires voxel-based spatial accuracy metrics such as manually identified
landmarks, which are challenging to implement for highly mobile
gastrointestinal (GI) organs. To address this, patient-specific digital twins
(DT) modeling temporally varying motion were created to assess the accuracy of
DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D
sequences were generated from static 3D patient scans using published
analytical GI motion models through a semi-automated pipeline. Eleven datasets,
including six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,
and three contrast-enhanced CT scans. The motion amplitudes of the DTs were
assessed against real patient stomach motion amplitudes extracted from
independent 4D MRI datasets. The generated DTs were then used to assess six
different DIR methods using target registration error, Dice similarity
coefficient, and the 95th percentile Hausdorff distance using summary metrics
and voxel-level granular visualizations. Finally, for a subset of T2w MRI scans
from patients treated with MR-guided radiation therapy, dose distributions were
warped and accumulated to assess dose warping errors, including evaluations of
DIR performance in both low- and high-dose regions for patient-specific error
estimation. Main results: Our proposed pipeline synthesized DTs modeling
realistic GI motion, achieving mean and maximum motion amplitudes and a mean
log Jacobian determinant within 0.8 mm and 0.01, respectively, similar to
published real-patient gastric motion data. It also enables the extraction of
detailed quantitative DIR performance metrics and rigorous validation of dose
mapping accuracy. Significance: The pipeline enables rigorously testing DIR
tools for dynamic, anatomically complex regions enabling granular spatial and
dosimetric accuracies.

</details>


### [122] [3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP](https://arxiv.org/abs/2507.01912)
*Ranjan Sapkota,Zhichao Meng,Martin Churuvija,Xiaoqiang Du,Zenghong Ma,Manoj Karkee*

Main category: cs.CV

TL;DR: The paper presents a framework for multi-seasonal data fusion in orchard automation to overcome foliage occlusion, using RGB-D imagery, YOLOv9-Seg, Kinect Fusion, and Fast GICP for accurate 3D reconstruction and alignment.


<details>
  <summary>Details</summary>
Motivation: Dense foliage during the canopy season occludes tree structures, limiting machine vision. The dormant season offers better visibility, motivating a fusion of multi-seasonal data for year-round automation.

Method: The framework combines RGB-D imagery from dormant and canopy seasons, using YOLOv9-Seg for segmentation, Kinect Fusion for 3D reconstruction, and Fast GICP for model alignment.

Result: YOLOv9-Seg achieved an MSE of 0.0047 and mAP@50 of 0.78. Kinect Fusion had RMSEs of 5.23 mm (trunk), 4.50 mm (branch), and 13.72 mm (branch spacing). Fast GICP achieved a fitness score of 0.00197.

Conclusion: The fused structural model enables robotic systems to access obscured tree information, improving precision in pruning and thinning operations.

Abstract: In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the entire growing season. The framework
combines high-resolution RGB-D imagery from both dormant and canopy periods
using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D
reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for
model alignment. Segmentation outputs from YOLOv9-Seg were used to extract
depth-informed masks, which enabled accurate 3D point cloud reconstruction via
Kinect Fusion; these reconstructed models from each season were subsequently
aligned using Fast GICP to achieve spatially coherent multi-season fusion. The
YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared
error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in
dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree
geometry, validated with field measurements resulting in root mean square
errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and
13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal
registration with a minimum fitness score of 0.00197, allowing integrated,
comprehensive tree structure modeling despite heavy occlusions during the
growing season. This fused structural representation enables robotic systems to
access otherwise obscured architectural information, improving the precision of
pruning, thinning, and other automated orchard operations.

</details>


### [123] [IC-Custom: Diverse Image Customization via In-Context Learning](https://arxiv.org/abs/2507.01926)
*Yaowei Li,Xiaoyu Li,Zhaoyang Zhang,Yuxuan Bian,Gan Liu,Xinyuan Li,Jiale Xu,Wenbo Hu,Yating Liu,Lingen Li,Jing Cai,Yuexian Zou,Yancheng He,Ying Shan*

Main category: cs.CV

TL;DR: IC-Custom is a unified framework for image customization, integrating position-aware and position-free paradigms via in-context learning, outperforming existing methods with minimal parameter training.


<details>
  <summary>Details</summary>
Motivation: Current image customization methods lack a universal framework, limiting versatility. IC-Custom aims to unify diverse customization tasks.

Method: Uses in-context learning with DiT's attention mechanism, ICMA for task handling, and a curated dataset of 12k samples.

Result: Outperforms competitors with 73% higher human preference, training only 0.4% of original parameters.

Conclusion: IC-Custom is a versatile, efficient solution for industrial image customization, validated by extensive benchmarks.

Abstract: Image customization, a crucial technique for industrial media production,
aims to generate content that is consistent with reference images. However,
current approaches conventionally separate image customization into
position-aware and position-free customization paradigms and lack a universal
framework for diverse customization, limiting their applications across various
scenarios. To overcome these limitations, we propose IC-Custom, a unified
framework that seamlessly integrates position-aware and position-free image
customization through in-context learning. IC-Custom concatenates reference
images with target images to a polyptych, leveraging DiT's multi-modal
attention mechanism for fine-grained token-level interactions. We introduce the
In-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented
register tokens and boundary-aware positional embeddings to enable the model to
correctly handle different task types and distinguish various inputs in
polyptych configurations. To bridge the data gap, we carefully curated a
high-quality dataset of 12k identity-consistent samples with 8k from real-world
sources and 4k from high-quality synthetic data, avoiding the overly glossy and
over-saturated synthetic appearance. IC-Custom supports various industrial
applications, including try-on, accessory placement, furniture arrangement, and
creative IP customization. Extensive evaluations on our proposed ProductBench
and the publicly available DreamBench demonstrate that IC-Custom significantly
outperforms community workflows, closed-source models, and state-of-the-art
open-source approaches. IC-Custom achieves approximately 73% higher human
preference across identity consistency, harmonicity, and text alignment
metrics, while training only 0.4% of the original model parameters. Project
page: https://liyaowei-stu.github.io/project/IC_Custom

</details>


### [124] [evMLP: An Efficient Event-Driven MLP Architecture for Vision](https://arxiv.org/abs/2507.01927)
*Zhentan Zheng*

Main category: cs.CV

TL;DR: The paper introduces evMLP, an MLP-based vision model with an event-driven local update mechanism for efficient processing of sequential image data like videos, achieving competitive accuracy and reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency in vision tasks, especially for sequential data, by leveraging MLPs and an event-driven approach to avoid redundant computations.

Method: Proposes evMLP, which processes image patches via MLPs and selectively updates patches where changes (events) occur between frames.

Result: evMLP achieves competitive accuracy on ImageNet and reduces computational costs on video datasets while maintaining output consistency.

Conclusion: evMLP offers an efficient alternative for vision tasks, particularly for sequential data, by combining MLPs with an event-driven update mechanism.

Abstract: Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed evMLP can independently
process patches on images or feature maps via MLPs. We define changes between
consecutive frames as "events". Under the event-driven local update mechanism,
evMLP selectively processes patches where events occur. For sequential image
data (e.g., video processing), this approach improves computational performance
by avoiding redundant computations. Through ImageNet image classification
experiments, evMLP attains accuracy competitive with state-of-the-art models.
More significantly, experimental results on multiple video datasets demonstrate
that evMLP reduces computational cost via its event-driven local update
mechanism while maintaining output consistency with its non-event-driven
baseline. The code and trained models are available at
https://github.com/i-evi/evMLP.

</details>


### [125] [CI-VID: A Coherent Interleaved Text-Video Dataset](https://arxiv.org/abs/2507.01938)
*Yiming Ju,Jijin Hu,Zhengxiong Luo,Haoge Deng,hanyu Zhao,Li Du,Chengwei Wu,Donglin Hao,Xinlong Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: CI-VID is a new dataset for coherent multi-scene video generation, addressing limitations of isolated text-video pairs in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack support for modeling coherent multi-clip video sequences, limiting the quality of generated content.

Method: Introduces CI-VID, a dataset with 340,000 samples of coherent video sequences and captions, enabling text-and-video-to-video generation.

Result: Models trained on CI-VID show improved accuracy and content consistency in generating coherent video sequences.

Conclusion: CI-VID facilitates story-driven content creation with smooth transitions and temporal coherence, proving its practical utility.

Abstract: Text-to-video (T2V) generation has recently attracted considerable attention,
resulting in the development of numerous high-quality datasets that have
propelled progress in this area. However, existing public datasets are
primarily composed of isolated text-video (T-V) pairs and thus fail to support
the modeling of coherent multi-clip video sequences. To address this
limitation, we introduce CI-VID, a dataset that moves beyond isolated
text-to-video (T2V) generation toward text-and-video-to-video (TV2V)
generation, enabling models to produce coherent, multi-scene video sequences.
CI-VID contains over 340,000 samples, each featuring a coherent sequence of
video clips with text captions that capture both the individual content of each
clip and the transitions between them, enabling visually and textually grounded
generation. To further validate the effectiveness of CI-VID, we design a
comprehensive, multi-dimensional benchmark incorporating human evaluation,
VLM-based assessment, and similarity-based metrics. Experimental results
demonstrate that models trained on CI-VID exhibit significant improvements in
both accuracy and content consistency when generating video sequences. This
facilitates the creation of story-driven content with smooth visual transitions
and strong temporal coherence, underscoring the quality and practical utility
of the CI-VID dataset We release the CI-VID dataset and the accompanying code
for data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID

</details>


### [126] [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](https://arxiv.org/abs/2507.01945)
*Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao*

Main category: cs.CV

TL;DR: LongAnimation framework addresses long-term color consistency in animation colorization using a dynamic global-local paradigm, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High labor costs in long animation colorization and limitations of existing short-term methods neglecting global consistency.

Method: Proposes LongAnimation with SketchDiT, Dynamic Global-Local Memory (DGLM), and Color Consistency Reward for dynamic global feature fusion.

Result: Effective in maintaining color consistency for both short-term (14 frames) and long-term (average 500 frames) animations.

Conclusion: LongAnimation offers a robust solution for open-domain animation colorization with improved consistency.

Abstract: Animation colorization is a crucial part of real animation industry
production. Long animation colorization has high labor costs. Therefore,
automated long animation colorization based on the video generation model has
significant research value. Existing studies are limited to short-term
colorization. These studies adopt a local paradigm, fusing overlapping features
to achieve smooth transitions between local segments. However, the local
paradigm neglects global information, failing to maintain long-term color
consistency. In this study, we argue that ideal long-term color consistency can
be achieved through a dynamic global-local paradigm, i.e., dynamically
extracting global color-consistent features relevant to the current generation.
Specifically, we propose LongAnimation, a novel framework, which mainly
includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color
Consistency Reward. The SketchDiT captures hybrid reference features to support
the DGLM module. The DGLM module employs a long video understanding model to
dynamically compress global historical features and adaptively fuse them with
the current generation features. To refine the color consistency, we introduce
a Color Consistency Reward. During inference, we propose a color consistency
fusion to smooth the video segment transition. Extensive experiments on both
short-term (14 frames) and long-term (average 500 frames) animations show the
effectiveness of LongAnimation in maintaining short-term and long-term color
consistency for open-domain animation colorization task. The code can be found
at https://cn-makers.github.io/long_animation_web/.

</details>


### [127] [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949)
*Kwai Keye Team,Biao Yang,Bin Wen,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Hao Peng,Haojie Ding,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Jin Ouyang,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yang Zhou,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zhenhua Wu,Zhenyu Li,Zhixin Ling,Ziming Li,Dehua Ma,Di Xu,Haixuan Gao,Hang Li,Jiawei Guo,Jing Wang,Lejian Ren,Muhao Wei,Qianqian Wang,Qigen Hu,Shiyao Wang,Tao Yu,Xinchen Luo,Yan Li,Yiming Liang,Yuhang Hu,Zeyi Lu,Zhuoran Yang,Zixing Zhang*

Main category: cs.CV

TL;DR: Kwai Keye-VL is an 8B-parameter MLLM designed for short-video understanding, leveraging a massive dataset and innovative training to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with dynamic, information-dense short videos, a key digital medium. Keye-VL aims to bridge this gap.

Method: Four-stage pre-training for vision-language alignment, followed by a two-phase post-training process with a unique five-mode data mixture and RL.

Result: State-of-the-art on video benchmarks, competitive on image tasks, and strong performance on the new KC-MMBench.

Conclusion: Keye-VL effectively addresses short-video comprehension while maintaining general vision-language capabilities.

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.

</details>


### [128] [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/abs/2507.01953)
*Yukang Cao,Chenyang Si,Jinghao Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: FreeMorph is a tuning-free image morphing method that handles inputs with different semantics/layouts, outperforming existing methods in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods require finetuning and struggle with semantic/layout discrepancies and time constraints. FreeMorph aims to eliminate these limitations.

Method: Integrates guidance-aware spherical interpolation and step-oriented variation trends to modify self-attention modules for controlled transitions.

Result: FreeMorph is 10x~50x faster and achieves higher fidelity than existing methods, setting a new benchmark.

Conclusion: FreeMorph successfully addresses challenges in tuning-free image morphing, offering superior performance and efficiency.

Abstract: We present FreeMorph, the first tuning-free method for image morphing that
accommodates inputs with different semantics or layouts. Unlike existing
methods that rely on finetuning pre-trained diffusion models and are limited by
time constraints and semantic/layout discrepancies, FreeMorph delivers
high-fidelity image morphing without requiring per-instance training. Despite
their efficiency and potential, tuning-free methods face challenges in
maintaining high-quality results due to the non-linear nature of the multi-step
denoising process and biases inherited from the pre-trained diffusion model. In
this paper, we introduce FreeMorph to address these challenges by integrating
two key innovations. 1) We first propose a guidance-aware spherical
interpolation design that incorporates explicit guidance from the input images
by modifying the self-attention modules, thereby addressing identity loss and
ensuring directional transitions throughout the generated sequence. 2) We
further introduce a step-oriented variation trend that blends self-attention
modules derived from each input image to achieve controlled and consistent
transitions that respect both inputs. Our extensive evaluations demonstrate
that FreeMorph outperforms existing methods, being 10x ~ 50x faster and
establishing a new state-of-the-art for image morphing.

</details>


### [129] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: Benchmarking multimodal foundation models (e.g., GPT-4o, Gemini, Claude) on standard vision tasks reveals they lag behind specialist models but are strong generalists, with better performance on semantic than geometric tasks. GPT-4o leads among non-reasoning models.


<details>
  <summary>Details</summary>
Motivation: To assess the vision understanding capabilities of multimodal foundation models, which are primarily trained for text tasks, and benchmark their performance on standard computer vision tasks.

Method: Translate vision tasks into text-promptable and API-compatible formats using prompt chaining to create a standardized benchmarking framework.

Result: Models perform below specialist models but are respectable generalists. GPT-4o leads in 4/6 tasks. Semantic tasks outperform geometric ones, and reasoning models improve geometric performance.

Conclusion: Multimodal foundation models show promise as generalists but lack specialist-level performance. GPT-4o excels among non-reasoning models, while reasoning models aid geometric tasks.

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [130] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: Locality-aware Parallel Decoding (LPD) accelerates autoregressive image generation by enabling flexible parallelization and locality-aware ordering, reducing steps significantly without quality loss.


<details>
  <summary>Details</summary>
Motivation: Autoregressive image generation is memory-bound and slow; existing parallelization methods are limited. LPD aims to achieve high parallelization while maintaining quality.

Method: Introduces Flexible Parallelized Autoregressive Modeling (learnable position queries) and Locality-aware Generation Ordering (dependency-minimizing grouping).

Result: Reduces generation steps from 256 to 20 (256x256) and 1024 to 48 (512x512), with 3.4x lower latency than prior methods.

Conclusion: LPD successfully balances speed and quality in autoregressive image generation.

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [131] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: The paper proposes using natural language for multi-agent collaborative driving to improve intent communication, efficiency, and safety.


<details>
  <summary>Details</summary>
Motivation: Existing communication methods (sensor data, neural features, perception results) lack bandwidth efficiency, completeness, and interoperability, and ignore decision-level fusion.

Method: Transition from perception-oriented data exchanges to natural language for intent and reasoning communication.

Result: Natural language balances semantic density and bandwidth, adapts to real-time conditions, and bridges heterogeneous platforms.

Conclusion: Natural language transforms collaborative driving into proactive coordination, enhancing safety, efficiency, and transparency.

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [132] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
*Kumarmanas Nethil,Vaibhav Mishra,Kriti Anandan,Kavya Manohar*

Main category: eess.AS

TL;DR: An open-source framework for efficient command-style dictation using VAD and parallel Whisper models, outperforming batch processing in latency and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap between resource-heavy online systems and high-latency batch processing for dictation.

Method: Uses Voice Activity Detection (VAD) to segment audio and transcribes segments in parallel with Whisper models, supporting various ASR architectures.

Result: Deployed in 15% of India's courtrooms, showing reduced latency with increased user concurrency.

Conclusion: The framework offers scalable, low-latency dictation and is open-source for broader adoption.

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [133] [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
*Harsh Joshi,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Niharika Jain,Sarthak Jain,Jiechao Gao,Usman Naseem*

Main category: cs.IR

TL;DR: Benchmarking CLIP, BLIP, and LXMERT reveals trade-offs between generalization and specialization in Vision-Language Models (VLMs). CLIP generalizes best, BLIP performs well on curated data, and LXMERT leads in structured reasoning.


<details>
  <summary>Details</summary>
Motivation: To evaluate the consistency and performance of VLMs across diverse tasks and datasets, addressing gaps in current understanding.

Method: Benchmarked CLIP, BLIP, and LXMERT on retrieval, captioning, and reasoning tasks using accuracy, generation quality, efficiency, and a novel Cross-Dataset Consistency (CDC) metric.

Result: CLIP showed strongest generalization (CDC: 0.92), BLIP excelled on curated data, and LXMERT led in structured reasoning.

Conclusion: The study highlights trade-offs between generalization and specialization in VLMs, guiding future development and industrial deployment.

Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their
performance consistency across tasks is underexamined. We benchmark CLIP, BLIP,
and LXMERT across diverse datasets spanning retrieval, captioning, and
reasoning. Our evaluation includes task accuracy, generation quality,
efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows
strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT
leads in structured reasoning. These results expose trade-offs between
generalization and specialization, informing industrial deployment of VLMs and
guiding development toward robust, task-flexible architectures.

</details>


### [134] [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
*Pranav Jadhav*

Main category: cs.IR

TL;DR: The paper proposes using Dense Passage Retrieval (DPR) for patient cohort retrieval in echocardiography, transforming unstructured EHR data into a Query-Passage dataset and introducing custom evaluation metrics. Their custom-trained DPR model outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Patient cohort retrieval from EHRs is crucial for medical research and practice, but unstructured data in echocardiography poses challenges. The work aims to address this gap.

Method: The authors apply DPR to transform unstructured echocardiographic EHR data into a Query-Passage dataset, design clinical scenario-inspired evaluation metrics, and train a custom DPR embedding model.

Result: The custom-trained DPR model achieves superior performance compared to traditional and off-the-shelf state-of-the-art methods.

Conclusion: This is the first application of DPR for cohort retrieval in echocardiography, offering a adaptable framework for other medical domains.

Abstract: Patient cohort retrieval is a pivotal task in medical research and clinical
practice, enabling the identification of specific patient groups from extensive
electronic health records (EHRs). In this work, we address the challenge of
cohort retrieval in the echocardiography domain by applying Dense Passage
Retrieval (DPR), a prominent methodology in semantic search. We propose a
systematic approach to transform an echocardiographic EHR dataset of
unstructured nature into a Query-Passage dataset, framing the problem as a
Cohort Retrieval task. Additionally, we design and implement evaluation metrics
inspired by real-world clinical scenarios to rigorously test the models across
diverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding
model that demonstrates superior performance compared to traditional and
off-the-shelf SOTA methods.To our knowledge, this is the first work to apply
DPR for patient cohort retrieval in the echocardiography domain, establishing a
framework that can be adapted to other medical domains.

</details>


### [135] [Embedding-based Retrieval in Multimodal Content Moderation](https://arxiv.org/abs/2507.01066)
*Hanzhong Liang,Jinghao Shi,Xiang Shen,Zixuan Wang,Vera Wen,Ardalan Mehrani,Zhiqian Chen,Yifan Wu,Zhixin Zhang*

Main category: cs.IR

TL;DR: The paper introduces an Embedding-Based Retrieval (EBR) method for video content moderation, outperforming traditional classification with improved efficiency, cost savings, and performance metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional classification struggles with rapid, cost-efficient responses in content moderation, especially for trend adaptation and urgent escalations.

Method: Uses Supervised Contrastive Learning (SCL) to train foundation embedding models (single/multi-modal), then integrates them into an EBR system for efficient video retrieval.

Result: EBR improves ROC-AUC to 0.99 and PR-AUC to 0.95 in offline tests, and boosts action rates by 10.32% while cutting costs by 80% online.

Conclusion: EBR enhances interpretability, flexibility, and performance in content moderation, offering a superior alternative to classification-based methods.

Abstract: Video understanding plays a fundamental role for content moderation on short
video platforms, enabling the detection of inappropriate content. While
classification remains the dominant approach for content moderation, it often
struggles in scenarios requiring rapid and cost-efficient responses, such as
trend adaptation and urgent escalations. To address this issue, we introduce an
Embedding-Based Retrieval (EBR) method designed to complement traditional
classification approaches. We first leverage a Supervised Contrastive Learning
(SCL) framework to train a suite of foundation embedding models, including both
single-modal and multi-modal architectures. Our models demonstrate superior
performance over established contrastive learning methods such as CLIP and
MoCo. Building on these embedding models, we design and implement the
embedding-based retrieval system that integrates embedding generation and video
retrieval to enable efficient and effective trend handling. Comprehensive
offline experiments on 25 diverse emerging trends show that EBR improves
ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online
experiments reveal that EBR increases action rates by 10.32% and reduces
operational costs by over 80%, while also enhancing interpretability and
flexibility compared to classification-based solutions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [136] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Main category: eess.IV

TL;DR: Prompt-based methodologies enhance deep learning in medical imaging by improving adaptability and performance without extensive retraining, though challenges like data heterogeneity and scalability remain.


<details>
  <summary>Details</summary>
Motivation: To address challenges like data scarcity and distribution shifts in medical imaging, prompt-based methods are explored for their potential to improve model performance and adaptability.

Method: The review examines diverse prompt modalities (textual, visual, learnable embeddings) and their integration in tasks like image generation, segmentation, and classification.

Result: Prompt mechanisms enhance accuracy, robustness, and data efficiency while reducing manual feature engineering and improving interpretability.

Conclusion: Despite advancements, challenges like prompt design optimization persist. Future directions include multimodal prompting and clinical integration to revolutionize diagnostics and treatment planning.

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [137] [MID-INFRARED (MIR) OCT-based inspection in industry](https://arxiv.org/abs/2507.01074)
*N. P. García-de-la-Puente,Rocío del Amor,Fernando García-Torres,Niels Møller Israelsen,Coraline Lapre,Christian Rosenberg Petersen,Ole Bang,Dominik Brouczek,Martin Schwentenwein,Kevin Neumann,Niels Benson,Valery Naranjo*

Main category: eess.IV

TL;DR: The paper explores MIR OCT systems for detecting sub-surface irregularities in materials like composites and ceramics, using preprocessing and AI-enhanced vision for anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To enhance non-destructive inspection techniques for industrial production monitoring.

Method: Exploratory study involving acquisitions on composites and ceramics, evaluating preprocessing and AI algorithms for anomaly detection.

Result: Identifies capabilities, limitations, and optimal parameters for MIR OCT systems.

Conclusion: Highlights strengths and weaknesses of the system, providing insights for industrial applications.

Abstract: This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography
(OCT) systems as a tool to penetrate different materials and detect sub-surface
irregularities. This is useful for monitoring production processes, allowing
Non-Destructive Inspection Techniques of great value to the industry. In this
exploratory study, several acquisitions are made on composite and ceramics to
know the capabilities of the system. In addition, it is assessed which
preprocessing and AI-enhanced vision algorithms can be anomaly-detection
methodologies capable of detecting abnormal zones in the analyzed objects.
Limitations and criteria for the selection of optimal parameters will be
discussed, as well as strengths and weaknesses will be highlighted.

</details>


### [138] [Classification based deep learning models for lung cancer and disease using medical images](https://arxiv.org/abs/2507.01279)
*Ahmad Chaddad,Jihao Peng,Yihang Wu*

Main category: eess.IV

TL;DR: A novel CNN model, ResNet+, improves lung cancer and disease prediction by integrating ResNet-D and attention modules, achieving high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance lung cancer and disease prediction in medical images by addressing feature loss during downsampling and improving generalization.

Method: ResNet+ integrates ResNet-D for better feature extraction and a convolutional attention module for focusing on relevant image regions, evaluated on five public datasets with data augmentation.

Result: Achieved 98.14% accuracy/F1 on LC2500 and 99.25%/99.13% on IQ-OTH/NCCD, with reduced computational costs.

Conclusion: ResNet+ outperforms baseline models, offering improved performance and efficiency for medical image analysis.

Abstract: The use of deep learning (DL) in medical image analysis has significantly
improved the ability to predict lung cancer. In this study, we introduce a
novel deep convolutional neural network (CNN) model, named ResNet+, which is
based on the established ResNet framework. This model is specifically designed
to improve the prediction of lung cancer and diseases using the images. To
address the challenge of missing feature information that occurs during the
downsampling process in CNNs, we integrate the ResNet-D module, a variant
designed to enhance feature extraction capabilities by modifying the
downsampling layers, into the traditional ResNet model. Furthermore, a
convolutional attention module was incorporated into the bottleneck layers to
enhance model generalization by allowing the network to focus on relevant
regions of the input images. We evaluated the proposed model using five public
datasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and
LCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT
$n$=425024 images). To address class imbalance, we used data augmentation
techniques to artificially increase the representation of underrepresented
classes in the training dataset. The experimental results show that ResNet+
model demonstrated remarkable accuracy/F1, reaching 98.14/98.14\% on the
LC25000 dataset and 99.25/99.13\% on the IQ-OTH/NCCD dataset. Furthermore, the
ResNet+ model saved computational cost compared to the original ResNet series
in predicting lung cancer images. The proposed model outperformed the baseline
models on publicly available datasets, achieving better performance metrics.
Our codes are publicly available at
https://github.com/AIPMLab/Graduation-2024/tree/main/Peng.

</details>


### [139] [PanTS: The Pancreatic Tumor Segmentation Dataset](https://arxiv.org/abs/2507.01291)
*Wenxuan Li,Xinze Zhou,Qi Chen,Tianyu Lin,Pedro R. A. S. Bassi,Szymon Plotka,Jaroslaw B. Cwikla,Xiaoxi Chen,Chen Ye,Zheren Zhu,Kai Ding,Heng Li,Kang Wang,Yang Yang,Yucheng Tang,Daguang Xu,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: PanTS is a large-scale dataset for pancreatic CT analysis, improving AI model performance with extensive annotations and metadata.


<details>
  <summary>Details</summary>
Motivation: To advance research in pancreatic CT analysis by providing a comprehensive, expert-validated dataset.

Method: Curated 36,390 CT scans from 145 centers with voxel-wise annotations of tumors and 24 surrounding structures, plus metadata.

Result: AI models trained on PanTS outperform those on existing datasets due to larger-scale annotations and additional anatomical structures.

Conclusion: PanTS sets a new benchmark for AI development in pancreatic CT analysis.

Abstract: PanTS is a large-scale, multi-institutional dataset curated to advance
research in pancreatic CT analysis. It contains 36,390 CT scans from 145
medical centers, with expert-validated, voxel-wise annotations of over 993,000
anatomical structures, covering pancreatic tumors, pancreas head, body, and
tail, and 24 surrounding anatomical structures such as vascular/skeletal
structures and abdominal/thoracic organs. Each scan includes metadata such as
patient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,
etc. AI models trained on PanTS achieve significantly better performance in
pancreatic tumor detection, localization, and segmentation compared to those
trained on existing public datasets. Our analysis indicates that these gains
are directly attributable to the 16x larger-scale tumor annotations and
indirectly supported by the 24 additional surrounding anatomical structures. As
the largest and most comprehensive resource of its kind, PanTS offers a new
benchmark for developing and evaluating AI models in pancreatic CT analysis.

</details>


### [140] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/abs/2507.01323)
*Rongchang Zhao,Huanchi Liu,Jian Zhang*

Main category: eess.IV

TL;DR: SWinMamba, a novel method for vascular segmentation, uses serpentine window sequences and bidirectional state space models to ensure continuity in slender vascular structures, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Vascular segmentation often results in discontinuous structures due to the slender nature of vessels and inadequate prior modeling, necessitating a more accurate approach.

Method: SWinMamba incorporates serpentine window sequences (SWToken) for adaptive feature capturing and bidirectional aggregation (BAM) for continuity. It also uses dual-domain learning (SFFU) for enhanced feature representation.

Result: Extensive experiments on three datasets show SWinMamba achieves superior performance with complete and connected vessel segmentation.

Conclusion: SWinMamba effectively addresses discontinuity in vascular segmentation, offering a robust solution for medical imaging applications.

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [141] [Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction](https://arxiv.org/abs/2507.01326)
*Dong Liang,Xingyu Qiu,Yuzhen Li,Wei Wang,Kuanquan Wang,Suyu Dong,Gongning Luo*

Main category: eess.IV

TL;DR: S2DNets, a novel self-supervised deep learning model, improves MR image quality by addressing intensity inhomogeneity with structural and smoothness constraints, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Intensity inhomogeneity in MR images hampers medical analysis. Existing deep learning models lack structural and smoothness constraints, leading to distorted corrections.

Method: Proposes S2DNets, dual networks with piece-wise structural constraints and bias field smoothness for self-supervised correction.

Result: Outperforms conventional and deep learning models on clinical and simulated datasets, improving visual metrics and downstream segmentation tasks.

Conclusion: S2DNets effectively corrects intensity inhomogeneity while preserving structural details, enhancing MR image quality for medical analysis.

Abstract: MR imaging techniques are of great benefit to disease diagnosis. However, due
to the limitation of MR devices, significant intensity inhomogeneity often
exists in imaging results, which impedes both qualitative and quantitative
medical analysis. Recently, several unsupervised deep learning-based models
have been proposed for MR image improvement. However, these models merely
concentrate on global appearance learning, and neglect constraints from image
structures and smoothness of bias field, leading to distorted corrected
results. In this paper, novel structure and smoothness constrained dual
networks, named S2DNets, are proposed aiming to self-supervised bias field
correction. S2DNets introduce piece-wise structural constraints and smoothness
of bias field for network training to effectively remove non-uniform intensity
and retain much more structural details. Extensive experiments executed on both
clinical and simulated MR datasets show that the proposed model outperforms
other conventional and deep learning-based models. In addition to comparison on
visual metrics, downstream MR image segmentation tasks are also used to
evaluate the impact of the proposed model. The source code is available at:
https://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.

</details>


### [142] [BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy](https://arxiv.org/abs/2507.01387)
*Ahmad Soliman,Ron Keuth,Marian Himstedt*

Main category: eess.IV

TL;DR: BronchoGAN introduces anatomical constraints in a conditional GAN for robust image-to-image translation across bronchoscopy domains, improving realism and dataset scalability.


<details>
  <summary>Details</summary>
Motivation: Limited bronchoscopy image availability hinders deep learning training; robust translation across domains is needed for clinical applications.

Method: Integrates anatomical constraints (bronchial orifice matching) and foundation model-generated depth images into a conditional GAN for domain-agnostic translation.

Result: Successfully translates diverse input domains to realistic human airway images, improving FID, SSIM, and Dice scores (up to 0.43).

Conclusion: BronchoGAN bridges the gap in public bronchoscopy datasets by leveraging CT scans and anatomical constraints for scalable, realistic image synthesis.

Abstract: The limited availability of bronchoscopy images makes image synthesis
particularly interesting for training deep learning models. Robust image
translation across different domains -- virtual bronchoscopy, phantom as well
as in-vivo and ex-vivo image data -- is pivotal for clinical applications. This
paper proposes BronchoGAN introducing anatomical constraints for image-to-image
translation being integrated into a conditional GAN. In particular, we force
bronchial orifices to match across input and output images. We further propose
to use foundation model-generated depth images as intermediate representation
ensuring robustness across a variety of input domains establishing models with
substantially less reliance on individual training datasets. Moreover our
intermediate depth image representation allows to easily construct paired image
data for training. Our experiments showed that input images from different
domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to
images mimicking realistic human airway appearance. We demonstrated that
anatomical settings (i.e. bronchial orifices) can be robustly preserved with
our approach which is shown qualitatively and quantitatively by means of
improved FID, SSIM and dice coefficients scores. Our anatomical constraints
enabled an improvement in the Dice coefficient of up to 0.43 for synthetic
images. Through foundation models for intermediate depth representations,
bronchial orifice segmentation integrated as anatomical constraints into
conditional GANs we are able to robustly translate images from different
bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan
data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image
datasets with realistic appearance. BronchoGAN enables to bridge the gap of
missing public bronchoscopy images.

</details>


### [143] [Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling](https://arxiv.org/abs/2507.01564)
*Chia-Ming Lee,Bo-Cheng Qiu,Ting-Yao Chen,Ming-Han Sun,Fang-Ying Lin,Jung-Tse Tsai,I-An Tsai,Yu-Fan Lin,Chih-Chung Hsu*

Main category: eess.IV

TL;DR: The paper presents a solution for COVID-19 detection from multi-source chest CT scans using SSFL and KDS, achieving high F1-scores with EfficientNet and Swin Transformer.


<details>
  <summary>Details</summary>
Motivation: To address variability in multi-source CT scans for COVID-19 detection and improve classification accuracy.

Method: Uses SSFL with KDS for preprocessing, including lung extraction and adaptive slice sampling. Compares EfficientNet and Swin Transformer architectures.

Result: EfficientNet achieves 94.68% F1-score, outperforming Swin Transformer (93.34%). Demonstrates pipeline effectiveness on multi-source data.

Conclusion: The KDS-based pipeline is effective for multi-source COVID-19 detection, emphasizing dataset balance in medical imaging.

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which classifies chest CT scans from four distinct medical centers. To address
multi-source variability, we employ the Spatial-Slice Feature Learning (SSFL)
framework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing
pipeline combines lung region extraction, quality control, and adaptive slice
sampling to select eight representative slices per scan. We compare
EfficientNet and Swin Transformer architectures on the validation set. The
EfficientNet model achieves an F1-score of 94.68%, compared to the Swin
Transformer's 93.34%. The results demonstrate the effectiveness of our
KDS-based pipeline on multi-source data and highlight the importance of dataset
balance in multi-institutional medical imaging evaluation.

</details>


### [144] [Robust brain age estimation from structural MRI with contrastive learning](https://arxiv.org/abs/2507.01794)
*Carlo Alberto Barbano,Benoit Dufumier,Edouard Duchesnay,Marco Grangetto,Pietro Gori*

Main category: eess.IV

TL;DR: Contrastive learning improves brain age estimation from MRI, offering better generalization, robustness to scanner differences, and clinical relevance compared to supervised methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of supervised approaches for brain age estimation by leveraging contrastive learning for scalability and robustness.

Method: Introduces a novel contrastive loss function, $\mathcal{L}^{exp}$, and evaluates it on multi-site neuroimaging datasets with over 20,000 scans.

Result: Scaling pre-training improves generalization, $\mathcal{L}^{exp}$ handles scanner confounds, captures accelerated aging in diseases, and correlates well with diagnostic performance.

Conclusion: Contrastive learning is a promising approach for generalizable and clinically meaningful brain age estimation.

Abstract: Estimating brain age from structural MRI has emerged as a powerful tool for
characterizing normative and pathological aging. In this work, we explore
contrastive learning as a scalable and robust alternative to supervised
approaches for brain age estimation. We introduce a novel contrastive loss
function, $\mathcal{L}^{exp}$, and evaluate it across multiple public
neuroimaging datasets comprising over 20,000 scans. Our experiments reveal four
key findings. First, scaling pre-training on diverse, multi-site data
consistently improves generalization performance, cutting external mean
absolute error (MAE) nearly in half. Second, $\mathcal{L}^{exp}$ is robust to
site-related confounds, maintaining low scanner-predictability as training size
increases. Third, contrastive models reliably capture accelerated aging in
patients with cognitive impairment and Alzheimer's disease, as shown through
brain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike
supervised baselines, $\mathcal{L}^{exp}$ maintains a strong correlation
between brain age accuracy and downstream diagnostic performance, supporting
its potential as a foundation model for neuroimaging. These results position
contrastive learning as a promising direction for building generalizable and
clinically meaningful brain representations.

</details>


### [145] [Autoadaptive Medical Segment Anything Model](https://arxiv.org/abs/2507.01828)
*Tyler Ward,Meredith K. Owen,O'Kira Coleman,Brian Noehren,Abdullah-Al-Zubaer Imran*

Main category: eess.IV

TL;DR: ADA-SAM is a multitask learning framework for medical image segmentation that uses class activation maps and a gradient feedback mechanism to improve accuracy in limited label settings.


<details>
  <summary>Details</summary>
Motivation: Traditional fully-supervised segmentation models require expensive manual annotations, prompting the need for annotation-efficient methods.

Method: ADA-SAM combines class activation maps from an auxiliary classifier with the SAM framework and introduces a gradient feedback mechanism to link segmentation and classification tasks.

Result: ADA-SAM outperforms fully-supervised and semi-supervised baselines by double digits in limited label settings on real-world clinical data.

Conclusion: ADA-SAM provides an accurate, automatic, and annotation-efficient solution for medical image segmentation.

Abstract: Medical image segmentation is a key task in the imaging workflow, influencing
many image-based decisions. Traditional, fully-supervised segmentation models
rely on large amounts of labeled training data, typically obtained through
manual annotation, which can be an expensive, time-consuming, and error-prone
process. This signals a need for accurate, automatic, and annotation-efficient
methods of training these models. We propose ADA-SAM (automated,
domain-specific, and adaptive segment anything model), a novel multitask
learning framework for medical image segmentation that leverages class
activation maps from an auxiliary classifier to guide the predictions of the
semi-supervised segmentation branch, which is based on the Segment Anything
(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient
feedback mechanism to create a learnable connection between the segmentation
and classification branches by using the segmentation gradients to guide and
improve the classification predictions. We validate ADA-SAM on real-world
clinical data collected during rehabilitation trials, and demonstrate that our
proposed method outperforms both fully-supervised and semi-supervised baselines
by double digits in limited label settings. Our code is available at:
https://github.com/tbwa233/ADA-SAM.

</details>


### [146] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/abs/2507.01881)
*Niccolò McConnell,Pardeep Vasudev,Daisuke Yamada,Daryl Cheng,Mehran Azimbagirad,John McCabe,Shahab Aslani,Ahmed H. Shahin,Yukun Zhou,The SUMMIT Consortium,Andre Altmann,Yipeng Hu,Paul Taylor,Sam M. Janes,Daniel C. Alexander,Joseph Jacob*

Main category: eess.IV

TL;DR: TANGERINE is an open-source, lightweight vision foundation model for LDCT analysis, designed for accessibility and efficiency in detecting lung diseases, including cancer, with minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: The shortage of radiologists for interpreting LDCT scans in lung cancer screening programs necessitates a scalable, efficient solution for early disease detection.

Method: TANGERINE uses a masked autoencoder framework for 3D imaging, pretrained self-supervised on 98,000 LDCTs, and fine-tuned for disease-specific tasks with limited data and resources.

Result: Achieves state-of-the-art performance across 14 disease classification tasks, including lung cancer, with fast convergence and strong label efficiency.

Conclusion: TANGERINE's open-source, lightweight design enables rapid integration into medical imaging tools, expanding lung cancer screening to comprehensive respiratory disease management.

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [147] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Main category: cs.HC

TL;DR: Older migrants in urban China used AI-assisted co-creation to express fragmented narratives through Hanzi reconstruction and storytelling.


<details>
  <summary>Details</summary>
Motivation: To address underrepresented and hard-to-verbalize personal narratives of aging migrants.

Method: Pilot workshop combining oral storytelling, Hanzi reconstruction with LLM-suggested Xiaozhuan glyphs, and physical materials.

Result: Participants transformed lived experiences into visual/tactile expressions without digital literacy, supported by human and soft AI facilitation.

Conclusion: Repositions AI as a supportive mechanism, enhancing narrative agency in sociotechnical systems for aging populations.

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [148] [Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping](https://arxiv.org/abs/2507.01411)
*Yifei Sun,Marshall A. Dalton,Robert D. Sanders,Yixuan Yuan,Xiang Li,Sharon L. Naismith,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: A deep learning framework predicts brain age from hippocampal functional connectivity, revealing age-sensitive hippocampal-cortical connections and distinct anterior-posterior hippocampal patterns.


<details>
  <summary>Details</summary>
Motivation: To understand functional connectivity changes in the hippocampus during aging, which remains limited despite known grey matter loss.

Method: Used a 3D CNN with LayerCAM saliency mapping to predict brain age from hippocampal FC, analyzing key hippocampal-cortical connections.

Result: Identified age-sensitive hippocampal-cortical connections (e.g., precuneus, cuneus) and distinct anterior-posterior hippocampal FC patterns.

Conclusion: The study offers insights into hippocampal aging mechanisms and highlights explainable deep learning's potential in neuroimaging.

Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimensional convolutional neural network (3D
CNN) combined with LayerCAM saliency mapping. This approach maps key
hippocampal-cortical connections, particularly with the precuneus, cuneus,
posterior cingulate cortex, parahippocampal cortex, left superior parietal
lobule, and right superior temporal sulcus, that are highly sensitive to age.
Critically, disaggregating anterior and posterior hippocampal FC reveals
distinct mapping aligned with their known functional specializations. These
findings provide new insights into the functional mechanisms of hippocampal
aging and demonstrate the power of explainable deep learning to uncover
biologically meaningful patterns in neuroimaging data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [149] [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513)
*Beitao Chen,Xinyu Lyu,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CR

TL;DR: The paper analyzes vulnerabilities in Multimodal Large Language Models (MLLMs) and proposes SafePTR, a training-free defense framework to mitigate jailbreak risks by pruning harmful tokens.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods for MLLMs fail to address root causes of vulnerabilities, leading to overdefensive behaviors and inefficiency. The study aims to uncover how harmful tokens trigger jailbreaks and develop a lightweight solution.

Method: The authors identify harmful tokens in early-middle layers and propose SafePTR, which selectively prunes these tokens while restoring benign features, without additional training.

Result: SafePTR significantly improves MLLM safety against jailbreaks, with evaluations showing state-of-the-art performance across benchmarks.

Conclusion: SafePTR offers an efficient, training-free solution to enhance MLLM safety by targeting a small subset of harmful tokens, preserving model utility.

Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs)
extend LLMs to support visual reasoning. However, this integration also
introduces new vulnerabilities, making MLLMs susceptible to multimodal
jailbreak attacks and hindering their safe deployment.Existing defense methods,
including Image-to-Text Translation, Safe Prompting, and Multimodal Safety
Tuning, attempt to address this by aligning multimodal inputs with LLMs'
built-in safeguards.Yet, they fall short in uncovering root causes of
multimodal vulnerabilities, particularly how harmful multimodal tokens trigger
jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven
multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing
heavy training overhead.To bridge this gap, we present an comprehensive
analysis of where, how and which harmful multimodal tokens bypass safeguards in
MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers
are responsible for inducing unsafe behaviors, highlighting the potential of
precisely removing a small subset of harmful tokens, without requiring safety
tuning, can still effectively improve safety against jailbreaks. Motivated by
this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense
framework that selectively prunes harmful tokens at vulnerable layers while
restoring benign features at subsequent layers.Without incurring additional
computational overhead, SafePTR significantly enhances the safety of MLLMs
while preserving efficiency. Extensive evaluations across three MLLMs and five
benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating
jailbreak risks without compromising utility.

</details>


### [150] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: A privacy-preserving platform enables small- and medium-sized manufacturers to securely share data with researchers, who develop tools like an automated crystal analysis app, ensuring data confidentiality.


<details>
  <summary>Details</summary>
Motivation: Manufacturers hesitate to share proprietary data due to competition and privacy concerns, hindering innovation.

Method: A secure platform allows data sharing; researchers develop tools (e.g., ML-based crystal analysis) and deploy them back for factory use.

Result: An automated tool for counting food crystals from microscope images was created, improving speed and accuracy over manual methods.

Conclusion: The platform successfully bridges data privacy and innovation, with potential for broader applications in manufacturing.

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [151] [A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification](https://arxiv.org/abs/2507.01778)
*Vivek Tetarwal,Sandeep Kumar*

Main category: cs.IT

TL;DR: A novel Dual Ensemble Neural Network (DENN) is proposed for classifying clean and dirty solar panels, outperforming existing methods with state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Automated discrimination between clean and dirty solar panels is crucial for maintaining solar energy system performance.

Method: The DENN integrates multiple ensemble models into a dual framework to enhance classification accuracy and robustness.

Result: DENN achieves superior performance on the Deep Solar Eye dataset, surpassing current ensemble methods.

Conclusion: Hybrid ensemble learning like DENN can advance automated solar panel inspections for scalable predictive maintenance.

Abstract: The installation of solar energy systems is on the rise, and therefore,
appropriate maintenance techniques are required to be used in order to maintain
maximum performance levels. One of the major challenges is the automated
discrimination between clean and dirty solar panels. This paper presents a
novel Dual Ensemble Neural Network (DENN) to classify solar panels using
image-based features. The suggested approach utilizes the advantages offered by
various ensemble models by integrating them into a dual framework, aimed at
improving both classification accuracy and robustness. The DENN model is
evaluated in comparison to current ensemble methods, showcasing its superior
performance across a range of assessment metrics. The proposed approach
performs the best compared to other methods and reaches state-of-the-art
accuracy on experimental results for the Deep Solar Eye dataset, effectively
serving predictive maintenance purposes in solar energy systems. It reveals the
potential of hybrid ensemble learning techniques to further advance the
prospects of automated solar panel inspections as a scalable solution to
real-world challenges.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [152] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: PathCoT improves pathology visual reasoning in MLLMs by integrating expert knowledge and self-evaluation to reduce errors and hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with pathology tasks due to lack of domain knowledge and errors in CoT reasoning steps.

Method: Proposes PathCoT, a zero-shot CoT prompting method integrating pathology expert knowledge and self-evaluation.

Result: PathCoT outperforms on the PathMMU dataset, enhancing pathology visual understanding.

Conclusion: PathCoT effectively addresses domain-specific challenges in MLLMs for pathology tasks.

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [153] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: A two-stage training framework for detoxifying social media text achieves strong detoxification, semantic preservation, and generalization with minimal annotated data.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing detoxification methods, which struggle with performance, semantic preservation, and data efficiency.

Method: A two-stage approach: supervised fine-tuning on filtered parallel data, followed by training with unlabeled toxic inputs and a custom reward model using Group Relative Policy Optimization.

Result: State-of-the-art performance with improved generalization and reduced reliance on annotated data.

Conclusion: The proposed framework effectively balances detoxification, semantic preservation, and data efficiency, outperforming existing methods.

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [154] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: SPRO is a novel framework for process-aware reinforcement learning in LLMs, eliminating the need for external reward models and improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational overhead and lack of theoretical framework in process-level advantage estimation for PRL.

Method: SPRO derives process rewards intrinsically from the policy model and introduces cumulative process rewards and Masked Step Advantage (MSA) for step-wise action advantage estimation.

Result: SPRO outperforms GRPO with 3.4x higher training efficiency, 17.5% test accuracy improvement, stable policy entropy, and reduced response length.

Conclusion: SPRO is a computationally efficient and effective solution for process-aware RL, suitable for industrial implementation.

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [155] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: Prefix-RFT combines SFT and RFT to improve LLM post-training, outperforming standalone methods and being easy to integrate.


<details>
  <summary>Details</summary>
Motivation: Address the trade-offs of SFT (behavior cloning issues) and RFT (unexpected behaviors, sensitivity to initial policy) by unifying them.

Method: Introduces Prefix-RFT, a hybrid approach blending demonstration (SFT) and exploration (RFT), tested on mathematical reasoning.

Result: Prefix-RFT outperforms standalone SFT and RFT, and mixed-policy RFT, with robustness to data variations.

Conclusion: Prefix-RFT harmonizes SFT and RFT, suggesting a unified paradigm for future LLM post-training research.

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [156] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER is an evolutionary black-box method for LLM post-training, addressing privacy and security concerns of gradient-based optimization while overcoming scalability and computational challenges of black-box methods.


<details>
  <summary>Details</summary>
Motivation: Address privacy, security, and overfitting issues in gradient-based optimization for deep learning, especially in restricted or adversarial scenarios.

Method: Introduces BBoxER, an evolutionary black-box method that induces an information bottleneck via implicit data compression, providing theoretical guarantees on generalization and robustness.

Result: BBoxER improves performance and generalization on reasoning benchmarks for LLMs, offering lightweight and modular enhancements.

Conclusion: BBoxER is a promising add-on to gradient-based optimization, suitable for privacy-sensitive environments with non-vacuous guarantees.

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [157] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: Proposes a CPU-friendly LoRA fine-tuning method for LLMs, leveraging pre-trained adapters to create lightweight combinations, offering a practical alternative to GPU-based training.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of LoRA's reliance on GPU-based training, making fine-tuning accessible for users with limited computational resources like standard laptop CPUs.

Method: Learns a meta-operator to map input datasets to LoRA weights by combining pre-trained adapters for Mistral-7B-Instruct-v0.2, avoiding gradient updates and working directly on CPU.

Result: The CPU-trained adapters outperform the base Mistral model on downstream tasks, though they don't match GPU-trained LoRA performance.

Conclusion: Provides a viable, resource-efficient alternative to GPU-based LoRA fine-tuning, democratizing access to parameter-efficient LLM updates.

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [158] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1 is a reflective generative model achieving OpenAI o3's performance via SPRM, integrating policy and reward models efficiently with minimal parameters. It supports test-time scaling and open-sourced for research.


<details>
  <summary>Details</summary>
Motivation: To create an efficient, unified model combining policy and reward models without extra annotations, reducing parameters and enabling scalable reasoning.

Method: Uses SPRM with shared backbone and task-specific heads for next token prediction and process scoring, enabling efficient reasoning and test-time scaling.

Result: Achieves OpenAI o3-mini performance with only 32B parameters and establishes a scaling law for thinking computation.

Conclusion: MetaStone-S1 is a scalable, efficient model open-sourced for community use, demonstrating competitive performance with reduced parameters.

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [159] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: The paper explores aligning disjoint vision and language models into a shared representation using the Joint Autoencoder Modulator (JAM) framework, optimizing for mutual coherence while preserving modality-specific structures.


<details>
  <summary>Details</summary>
Motivation: The Platonic Representation Hypothesis suggests that independently trained models may converge to a shared reality model. The study aims to explicitly optimize alignment between such disjoint representations.

Method: The JAM framework jointly trains modality-specific autoencoders on pre-trained models' latent representations, using reconstruction and cross-modal objectives. It evaluates alignment objectives, layer depth, and foundation model scale.

Result: The lightweight Pareto-efficient JAM framework reliably induces alignment, even across frozen, independently trained representations.

Conclusion: The study provides theoretical and practical insights for transforming unimodal foundations into specialist multimodal models.

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [160] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: Zapping (resampling weights in the last layer) improves continual learning, but its mechanisms are unclear. This study explores its effects on learning and forgetting in neural networks, showing faster recovery in new domains and optimizer-dependent task dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind zapping's effectiveness in continual learning and its impact on learning/forgetting patterns in neural networks.

Method: Experiments with convolutional neural networks in continual and few-shot transfer learning, using handwritten characters and natural images. Analyzed task-specific effects and optimizer influence.

Result: Zapping accelerates recovery in new domains. Optimizer choice significantly affects learning/forgetting dynamics, creating complex task synergy/interference patterns.

Conclusion: Zapping and optimizer selection critically influence continual learning performance, revealing intricate task interactions during sequential learning.

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [161] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: VLAD integrates a fine-tuned VLM with VAD to enhance autonomous driving, improving spatial reasoning and reducing collision rates by 31.82%.


<details>
  <summary>Details</summary>
Motivation: Leverage open-source VLMs' general knowledge to improve autonomous driving systems' perception, prediction, and planning.

Method: Fine-tune a VLM with custom QA datasets for spatial reasoning, integrate it with VAD, and generate navigational commands and explanations.

Result: 31.82% reduction in collision rates on the nuScenes dataset.

Conclusion: VLAD sets a new benchmark for VLM-augmented autonomous driving systems with improved performance and transparency.

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [162] [LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction](https://arxiv.org/abs/2507.01308)
*Muhammad Atta ur Rahman,Dooseop Choi,KyoungWook Min*

Main category: cs.RO

TL;DR: Proposes an enhanced motion forecasting model using multiple vector map elements for richer road environment representation, with a feature fusion strategy and pruning mechanism for efficiency.


<details>
  <summary>Details</summary>
Motivation: Current motion prediction models rely on lane centerlines, limiting their ability to capture road environments and traffic rules effectively.

Method: Uses multiple vector map elements (lane boundaries, road edges) and a feature fusion strategy. Includes a pruning mechanism to filter relevant map connections for efficiency.

Result: Achieves improved performance on the Argoverse 2 dataset, maintaining competitiveness while enhancing accuracy.

Conclusion: The method provides a more informative and efficient representation of driving environments, advancing motion forecasting for autonomous vehicles.

Abstract: Accurate motion forecasting is critical for safe and efficient autonomous
driving, enabling vehicles to predict future trajectories and make informed
decisions in complex traffic scenarios. Most of the current designs of motion
prediction models are based on the major representation of lane centerlines,
which limits their capability to capture critical road environments and traffic
rules and constraints. In this work, we propose an enhanced motion forecasting
model informed by multiple vector map elements, including lane boundaries and
road edges, that facilitates a richer and more complete representation of
driving environments. An effective feature fusion strategy is developed to
merge information in different vector map components, where the model learns
holistic information on road structures and their interactions with agents.
Since encoding more information about the road environment increases memory
usage and is computationally expensive, we developed an effective pruning
mechanism that filters the most relevant map connections to the target agent,
ensuring computational efficiency while maintaining essential spatial and
semantic relationships for accurate trajectory prediction. Overcoming the
limitations of lane centerline-based models, our method provides a more
informative and efficient representation of the driving environment and
advances the state of the art for autonomous vehicle motion forecasting. We
verify our approach with extensive experiments on the Argoverse 2 motion
forecasting dataset, where our method maintains competitiveness on AV2 while
achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements,
road topology, connection pruning, Argoverse 2.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [163] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: The paper proposes 'Data Agents' to automate and enhance Data+AI systems by leveraging LLMs for semantic understanding, reasoning, and planning, addressing current limitations in pipeline orchestration.


<details>
  <summary>Details</summary>
Motivation: Existing Data+AI systems rely on human experts for pipeline orchestration due to limited semantic understanding and planning capabilities. LLMs offer potential to overcome these limitations.

Method: Introduces 'Data Agents,' an architecture integrating LLMs for knowledge comprehension, reasoning, and planning to automate data-related tasks and pipeline orchestration.

Result: Examples of Data Agent systems (e.g., data science agent, DBA agent) demonstrate feasibility, but challenges like pipeline optimization and self-reflection remain.

Conclusion: Data Agents represent a promising direction for revolutionizing Data+AI ecosystems, though open challenges in design and implementation persist.

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [164] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve is an AI-assisted grading platform using LLMs to streamline grading handwritten STEM responses, reducing time by 65% with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Handwritten grading in large STEM courses is time-consuming; Pensieve aims to automate and improve efficiency.

Method: Leverages LLMs for transcription and evaluation, integrating a human-in-the-loop interface for the entire grading pipeline.

Result: Deployed in 20+ institutions, graded 300,000+ responses, reducing grading time by 65% with 95.4% agreement rate for high-confidence predictions.

Conclusion: Pensieve effectively automates grading while maintaining accuracy, offering significant time savings for instructors.

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [165] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: The paper introduces T3DM, a method for Temporal Knowledge Graph Reasoning (TKGR) that addresses distribution shift and improves negative sampling.


<details>
  <summary>Details</summary>
Motivation: Existing TKGR methods struggle with event distribution shifts between training and test samples and rely on low-quality random negative sampling.

Method: Proposes T3DM for distribution shift modeling and an adversarial training-based negative-sampling strategy.

Result: T3DM outperforms state-of-the-art baselines, providing more robust results.

Conclusion: T3DM effectively addresses key challenges in TKGR, improving performance and robustness.

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 79]
- [cs.CV](#cs.CV) [Total: 117]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.IR](#cs.IR) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [eess.IV](#eess.IV) [Total: 20]
- [cs.AI](#cs.AI) [Total: 9]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-Fran√ßois Godbout,Reihaneh Rabbany,Kellin Pelrine*

Main category: cs.CL

TL;DR: Veracity is an open-source AI system using LLMs and web retrieval to fact-check claims with explanations, supporting multilingual and interactive features.


<details>
  <summary>Details</summary>
Motivation: Combat misinformation exacerbated by generative AI through transparent, accessible fact-checking.

Method: Leverages LLMs and web retrieval to analyze claims, providing veracity assessments with explanations.

Result: Features include multilingual support, veracity scoring, and an interactive interface for user engagement.

Conclusion: Veracity aims to detect misinformation, explain reasoning, and foster media literacy for a more informed society.

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [2] [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
*Riccardo Di Sipio*

Main category: cs.CL

TL;DR: The paper explores optimization in LLMs using information geometry, highlighting natural gradient descent, curvature-aware methods, and potential quantum analogies for efficient training.


<details>
  <summary>Details</summary>
Motivation: To understand and improve optimization in high-dimensional LLM parameter spaces by leveraging geometric principles like the Fisher information metric.

Method: Uses information geometry, particularly the Fisher information metric and natural gradient descent, to analyze LLM training dynamics. Also explores quantum analogies for optimization.

Result: Provides insights into phenomena like sharp minima, generalization, and scaling laws in LLMs, suggesting curvature-aware methods enhance understanding.

Conclusion: Geometric approaches, including potential quantum-inspired methods, offer deeper insights and more efficient optimization for LLMs.

Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional
parameter spaces with non-Euclidean structure. Information geometry frames this
landscape using the Fisher information metric, enabling more principled
learning via natural gradient descent. Though often impractical, this geometric
lens clarifies phenomena such as sharp minima, generalization, and observed
scaling laws. We argue that curvature-aware approaches deepen our understanding
of LLM training. Finally, we speculate on quantum analogies based on the
Fubini-Study metric and Quantum Fisher Information, hinting at efficient
optimization in quantum-enhanced systems.

</details>


### [3] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
*Zijian Zhou,Ao Qu,Zhaoxuan Wu,Sunghwan Kim,Alok Prakash,Daniela Rus,Jinhua Zhao,Bryan Kian Hsiang Low,Paul Pu Liang*

Main category: cs.CL

TL;DR: MEM1 is a reinforcement learning framework for language agents that uses constant memory for long multi-turn tasks, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiencies of full-context prompting in LLM systems, which leads to unbounded memory growth and degraded reasoning.

Method: MEM1 updates a compact internal state at each turn, integrating prior memory with new observations while discarding irrelevant information. Training uses composed datasets for realistic multi-turn environments.

Result: MEM1-7B improves performance by 3.5x and reduces memory usage by 3.7x compared to Qwen2.5-14B-Instruct, generalizing beyond training horizons.

Conclusion: MEM1 demonstrates scalable reasoning-driven memory consolidation for efficient and high-performance long-horizon interactive agents.

Abstract: Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.

</details>


### [4] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
*Glenn Matlin,Mika Okamoto,Huzaifa Pardawala,Yang Yang,Sudheer Chava*

Main category: cs.CL

TL;DR: The paper introduces FLaME, a benchmarking suite to evaluate Language Models (LMs) for finance-specific NLP tasks, addressing gaps in existing frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation frameworks underestimate LMs' performance on finance NLP tasks due to methodological gaps.

Method: The study evaluates 23 foundation LMs and 'reasoning-reinforced' LMs across 20 core finance NLP tasks using the FLaME framework.

Result: The paper demonstrates LMs' potential for finance NLP tasks and provides open-source data and results.

Conclusion: FLaME offers a comprehensive tool to assess LMs in finance, correcting misconceptions about their lower performance bounds.

Abstract: Language Models (LMs) have demonstrated impressive capabilities with core
Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly
specialized knowledge-intensive tasks in finance remains difficult to assess
due to major gaps in the methodologies of existing evaluation frameworks, which
have caused an erroneous belief in a far lower bound of LMs' performance on
common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for
these FinNLP tasks, we present the first holistic benchmarking suite for
Financial Language Model Evaluation (FLaME). We are the first research paper to
comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical
study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source
our framework software along with all data and results.

</details>


### [5] [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
*Yifan Hu,Frank Liang,Dachuan Zhao,Jonathan Geuter,Varshini Reddy,Craig W. Schmidt,Chris Tanner*

Main category: cs.CL

TL;DR: The paper proposes entropy-informed pre-tokenization strategies to improve BPE for unsegmented languages like Chinese, showing better segmentation metrics than standard BPE.


<details>
  <summary>Details</summary>
Motivation: BPE struggles with unsegmented languages like Chinese due to its frequency-driven approach ignoring linguistic boundaries.

Method: Two strategies: one uses pointwise mutual information and entropy, the other leverages GPT-2's predictive entropy to guide BPE.

Result: Improved segmentation precision, recall, and F1 score on the PKU dataset compared to standard BPE.

Conclusion: Entropy-guided pre-tokenization enhances linguistic alignment and tokenization quality, especially in low-resource settings.

Abstract: Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization
method in modern language models due to its simplicity and strong empirical
performance across downstream tasks. However, applying BPE to unsegmented
languages such as Chinese presents significant challenges, as its
frequency-driven merge operation is agnostic to linguistic boundaries. To
address this, we propose two entropy-informed pre-tokenization strategies that
guide BPE segmentation using unsupervised information-theoretic cues. The first
approach uses pointwise mutual information and left/right entropy to identify
coherent character spans, while the second leverages predictive entropy derived
from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both
methods on a subset of the PKU dataset and demonstrate substantial improvements
in segmentation precision, recall, and F1 score compared to standard BPE. Our
results suggest that entropy-guided pre-tokenization not only enhances
alignment with gold-standard linguistic units but also offers a promising
direction for improving tokenization quality in low-resource and multilingual
settings.

</details>


### [6] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
*Sam Silver,Jimin Sun,Ivan Zhang,Sara Hooker,Eddie Kim*

Main category: cs.CL

TL;DR: LLMs show robust intrinsic self-correction in reasoning, even without fine-tuning for long CoT, suggesting their capabilities are underappreciated.


<details>
  <summary>Details</summary>
Motivation: To understand the self-correction capabilities of LLMs in mathematical reasoning, especially under minor perturbations.

Method: Experiments measuring models' ability to self-correct synthetic perturbations in Chain of Thought reasoning.

Result: LLMs exhibit strong intrinsic self-correction, ranging from subtle to explicit corrections.

Conclusion: Recent reasoning models amplify existing traits in LLMs, indicating underestimated self-correction abilities.

Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical
reasoning capabilities, yet their performance remains brittle to minor
variations in problem description and prompting strategy. Furthermore,
reasoning is vulnerable to sampling-induced errors which autoregressive models
must primarily address using self-correction via additionally-generated tokens.
To better understand self-correction capabilities of recent models, we conduct
experiments measuring models' ability to self-correct synthetic perturbations
introduced into their Chain of Thought (CoT) reasoning. We observe robust
single-utterance intrinsic self-correction behavior across a range of
open-weight models and datasets, ranging from subtle, implicit corrections to
explicit acknowledgments and corrections of errors. Our findings suggest that
LLMs, including those not finetuned for long CoT, may possess stronger
intrinsic self-correction capabilities than commonly shown in the literature.
The presence of this ability suggests that recent "reasoning" model work
involves amplification of traits already meaningfully present in models.

</details>


### [7] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
*Mohammad Amaan Sayeed,Mohammed Talha Alam,Raza Imam,Shahab Saquib Sohail,Amir Hussain*

Main category: cs.CL

TL;DR: The paper introduces Tibbe-AG, a pipeline to evaluate LLMs using Islamic medical texts, showing retrieval and self-critique improve accuracy and cultural sensitivity.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in culturally grounded medical guidance by leveraging Islamic texts and modern AI, addressing underutilization and inaccessibility.

Method: A unified evaluation pipeline (Tibbe-AG) tests three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) with 30 curated questions, using direct generation, retrieval-augmented generation, and a self-critique filter, assessed by an agentic judge.

Result: Retrieval boosts factual accuracy by 13%, and the agentic prompt adds 10% more improvement through deeper insights and safety.

Conclusion: Combining classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical QA.

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [8] [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
*Narutatsu Ri,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: The paper addresses gaps in evaluating and improving unbiased summaries in political perspective summarization using LLMs, proposing reliable metrics and methods like reranking and preference tuning.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation frameworks for perspective summarization lack reliable metrics, and efforts to improve summarizers are underdeveloped.

Method: The study identifies reliable metrics using human annotations, compares traditional and language model-based metrics, and tests reranking and preference tuning methods.

Result: Language model-based metrics outperform traditional ones, and reranking with preference tuning enhances summarization performance.

Conclusion: The findings contribute to better evaluation and development of perspective summarization methods.

Abstract: Generating unbiased summaries in real-world settings such as political
perspective summarization remains a crucial application of Large Language
Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics
for measuring key attributes such as coverage and faithfulness without
verifying their applicability, and efforts to develop improved summarizers are
still nascent. We address these gaps by (1) identifying reliable metrics for
measuring perspective summary quality, and (2) investigating the efficacy of
LLM-based methods beyond zero-shot inference. Namely, we build a test set for
benchmarking metric reliability using human annotations and show that
traditional metrics underperform compared to language model-based metrics,
which prove to be strong evaluators. Using these metrics, we show that
reranking-based methods yield strong results, and preference tuning with
synthetically generated and reranking-labeled data further boosts performance.
Our findings aim to contribute to the reliable evaluation and development of
perspective summarization methods.

</details>


### [9] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
*Toan Nguyen Hai,Ha Nguyen Viet,Truong Quan Xuan,Duc Do Minh*

Main category: cs.CL

TL;DR: The paper introduces VSMRC, a dataset for Vietnamese text segmentation and MRC, sourced from Wikipedia, with human-verified QA pairs. mBERT outperforms monolingual models, highlighting multilingual models' effectiveness for under-resourced languages.


<details>
  <summary>Details</summary>
Motivation: Vietnamese lacks robust NLP resources, prompting the creation of VSMRC to address gaps in text segmentation and MRC.

Method: The dataset includes 15,942 documents for segmentation and 16,347 synthetic QA pairs from Vietnamese Wikipedia, with human quality assurance.

Result: mBERT achieves 88.01% accuracy on MRC and 63.15% F1 on segmentation, outperforming monolingual models.

Conclusion: Multilingual models like mBERT excel in Vietnamese NLP tasks, suggesting broader applications for under-resourced languages. VSMRC is publicly available.

Abstract: Vietnamese, the 20th most spoken language with over 102 million native
speakers, lacks robust resources for key natural language processing tasks such
as text segmentation and machine reading comprehension (MRC). To address this
gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice
Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset
includes 15,942 documents for text segmentation and 16,347 synthetic
multiple-choice question-answer pairs generated with human quality assurance,
ensuring a reliable and diverse resource. Experiments show that mBERT
consistently outperforms monolingual models on both tasks, achieving an
accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text
segmentation test set. Our analysis reveals that multilingual models excel in
NLP tasks for Vietnamese, suggesting potential applications to other
under-resourced languages. VSMRC is available at HuggingFace

</details>


### [10] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
*Markus Frohmann,Gabriel Meseguer-Brocal,Markus Schedl,Elena V. Epure*

Main category: cs.CL

TL;DR: A novel multimodal approach, DE-detect, combines transcribed sung lyrics and speech features to detect AI-generated music, outperforming existing methods and enhancing robustness.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated music challenges artists and copyright holders, necessitating reliable detection methods beyond current audio or lyrics-based limitations.

Method: Proposes a multimodal, modular late-fusion pipeline integrating transcribed sung lyrics and speech features from audio to improve detection robustness.

Result: DE-detect outperforms existing lyrics-based detectors and shows greater resilience to audio perturbations.

Conclusion: The method provides a practical, robust solution for detecting AI-generated music in real-world scenarios, with code publicly available.

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [11] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
*Zhihan Guo,Jiele Wu,Wenqian Cui,Yifei Zhang,Minda Hu,Yufei Wang,Irwin King*

Main category: cs.CL

TL;DR: ProxyReward is a new RL-based framework for improving Open-LTG in LLMs, outperforming GPT-4-Turbo and enhancing performance by 20%.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in Open-LTG research due to lack of gold standard data and limited reward signals.

Method: Introduces ProxyReward, a framework with an auto-generated dataset and targeted reward signals for accuracy and comprehensiveness.

Result: ProxyReward outperforms GPT-4-Turbo and improves Open-LTG performance by 20%.

Conclusion: ProxyReward effectively enhances LLMs' ability to handle complex open-ended questions.

Abstract: Current research on long-form context in Large Language Models (LLMs)
primarily focuses on the understanding of long-contexts, the Open-ended Long
Text Generation (Open-LTG) remains insufficiently explored. Training a
long-context generation model requires curation of gold standard reference
data, which is typically nonexistent for informative Open-LTG tasks. However,
previous methods only utilize general assessments as reward signals, which
limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative
reinforcement learning (RL) based framework, which includes a dataset and a
reward signal computation method. Firstly, ProxyReward Dataset generation is
accomplished through simple prompts that enables the model to create
automatically, obviating extensive labeled data or significant manual effort.
Secondly, ProxyReward Signal offers a targeted evaluation of information
comprehensiveness and accuracy for specific questions. The experimental results
indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can
significantly enhance performance by 20% on the Open-LTG task when training
widely used open-source models, while also surpassing the LLM-as-a-Judge
approach. Our work presents effective methods to enhance the ability of LLMs to
address complex open-ended questions posed by human.

</details>


### [12] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Main category: cs.CL

TL;DR: EvoLM is a model suite for analyzing LM training dynamics across stages, revealing insights like diminishing returns from excessive training and the importance of mitigating forgetting.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency in evaluating LM design choices across training stages.

Method: Training over 100 LMs (1B and 4B parameters) and evaluating upstream/downstream capabilities.

Result: Key insights include diminishing returns from excessive training and the role of continued pre-training.

Conclusion: EvoLM provides a framework for transparent LM analysis, with released models and pipelines for reproducibility.

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [13] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Main category: cs.CL

TL;DR: A novel RAG framework for complex QA tasks, outperforming baselines with improved multi-hop reasoning and contextual understanding.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multi-hop reasoning and contextual understanding for complex QA tasks.

Method: Integrates dense retrieval with context fusion and multi-hop reasoning, using LLaMA 3 and joint optimization of retrieval likelihood and generation cross-entropy.

Result: Outperforms existing retrieval-augmented and generative baselines in accuracy and coherence.

Conclusion: The framework effectively delivers precise, contextually grounded answers for complex QA tasks.

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [14] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan √ñ. Arƒ±k*

Main category: cs.CL

TL;DR: DynScaling improves LLM performance under resource constraints by integrating parallel-sequential sampling and dynamic budget allocation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Practical application of inference-time scaling is limited by reliance on external verifiers or lack of optimization for computational constraints.

Method: Uses an integrated parallel-sequential sampling strategy and a bandit-based dynamic budget allocation framework.

Result: DynScaling outperforms verifier-free baselines in task performance and computational cost.

Conclusion: DynScaling effectively enhances LLM performance without external verifiers, adapting to practical constraints.

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [15] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
*Devesh Kumar*

Main category: cs.CL

TL;DR: A hybrid model combining transformer-based contextual understanding and broad learning systems for cyberbullying detection achieves high accuracy on benchmark datasets and includes explainability mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address the growing issue of cyberbullying by improving detection accuracy and transparency in automated content moderation.

Method: Combines a modified DeBERTa model with Squeeze-and-Excitation blocks and sentiment analysis, integrated with a Gated Broad Learning System (GBLS) classifier.

Result: Achieved accuracies of 79.3% (HateXplain), 95.41% (SOSNet), 91.37% (Mendeley-I), and 94.67% (Mendeley-II).

Conclusion: The hybrid framework outperforms existing methods, provides explainability, and identifies challenges like implicit bias and sarcasm for future improvements.

Abstract: The proliferation of online communication platforms has created unprecedented
opportunities for global connectivity while simultaneously enabling harmful
behaviors such as cyberbullying, which affects approximately 54.4\% of
teenagers according to recent research. This paper presents a hybrid
architecture that combines the contextual understanding capabilities of
transformer-based models with the pattern recognition strengths of broad
learning systems for effective cyberbullying detection. This approach
integrates a modified DeBERTa model augmented with Squeeze-and-Excitation
blocks and sentiment analysis capabilities with a Gated Broad Learning System
(GBLS) classifier, creating a synergistic framework that outperforms existing
approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +
GBLS model achieved good performance on four English datasets: 79.3\% accuracy
on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and
94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework
incorporates comprehensive explainability mechanisms including token-level
attribution analysis, LIME-based local interpretations, and confidence
calibration, addressing critical transparency requirements in automated content
moderation. Ablation studies confirm the meaningful contribution of each
architectural component, while failure case analysis reveals specific
challenges in detecting implicit bias and sarcastic content, providing valuable
insights for future improvements in cyberbullying detection systems.

</details>


### [16] [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
*Andy Yang,Micha√´l Cadilhac,David Chiang*

Main category: cs.CL

TL;DR: The paper explores how deeper transformers gain expressive capabilities, linking them to C-RASP programs and temporal logic, with empirical validation.


<details>
  <summary>Details</summary>
Motivation: To formally establish which capabilities are gained with greater depth in transformers.

Method: Theoretical proof linking transformers to C-RASP programs, followed by empirical study on sequential dependency tasks.

Result: Deeper transformers are more expressive than shallower ones, as proven by equivalence to C-RASP and temporal logic.

Conclusion: The theory predicts the depth needed for transformers to generalize on sequential tasks, validated empirically.

Abstract: It has been observed that transformers with greater depth (that is, more
layers) have more capabilities, but can we establish formally which
capabilities are gained with greater depth? We answer this question with a
theoretical proof followed by an empirical study. First, we consider
transformers that round to fixed precision except inside attention. We show
that this subclass of transformers is expressively equivalent to the
programming language C-RASP and this equivalence preserves depth. Second, we
prove that deeper C-RASP programs are more expressive than shallower C-RASP
programs, implying that deeper transformers are more expressive than shallower
transformers (within the subclass mentioned above). These results are
established by studying a form of temporal logic with counting operators, which
was shown equivalent to C-RASP in previous work. Finally, we provide empirical
evidence that our theory predicts the depth required for transformers without
positional encodings to length-generalize on a family of sequential dependency
tasks.

</details>


### [17] [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
*Duc Hieu Ho,Chenglin Fan*

Main category: cs.CL

TL;DR: The paper evaluates LLMs for honesty and helpfulness, proposes a self-critique-guided prompting strategy, and shows improved results on the HONESET dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ensuring LLMs produce consistently honest and helpful outputs without additional training.

Method: Proposes self-critique-guided curiosity refinement prompting, incorporating self-critique and refinement steps.

Result: Improves H¬≤ scores by 1.4% to 4.3%, reducing poor-quality and increasing high-quality responses.

Conclusion: Structured self-refinement is a scalable, training-free strategy to enhance LLM trustworthiness.

Abstract: Large language models (LLMs) have demonstrated robust capabilities across
various natural language tasks. However, producing outputs that are
consistently honest and helpful remains an open challenge. To overcome this
challenge, this paper tackles the problem through two complementary directions.
It conducts a comprehensive benchmark evaluation of ten widely used large
language models, including both proprietary and open-weight models from OpenAI,
Meta, and Google. In parallel, it proposes a novel prompting strategy,
self-critique-guided curiosity refinement prompting. The key idea behind this
strategy is enabling models to self-critique and refine their responses without
additional training. The proposed method extends the curiosity-driven prompting
strategy by incorporating two lightweight in-context steps including
self-critique step and refinement step.
  The experiment results on the HONESET dataset evaluated using the framework
$\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a
judge of honesty and helpfulness, show consistent improvements across all
models. The approach reduces the number of poor-quality responses, increases
high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores
ranging from 1.4% to 4.3% compared to curiosity-driven prompting across
evaluated models. These results highlight the effectiveness of structured
self-refinement as a scalable and training-free strategy to improve the
trustworthiness of LLMs outputs.

</details>


### [18] [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
*Devesh Kumar*

Main category: cs.CL

TL;DR: A framework for detecting cyberbullying in Hinglish text using MURIL architecture outperforms existing multilingual models, achieving high accuracy across six datasets and incorporating explainability features.


<details>
  <summary>Details</summary>
Motivation: The rise of code-mixed Hindi-English (Hinglish) communication on digital platforms challenges existing monolingual cyberbullying detection systems, necessitating a tailored solution.

Method: The paper employs the MURIL architecture, with selective layer freezing, specialized preprocessing, and a designed classification head, evaluated on six benchmark datasets.

Result: The MURIL-based approach outperforms models like RoBERTa and IndicBERT, with accuracy improvements of 1.36 to 13.07 percentage points across datasets.

Conclusion: The framework addresses current limitations but highlights challenges like context-dependent interpretation and sarcasm detection, guiding future multilingual cyberbullying research.

Abstract: The growth of digital communication platforms has led to increased
cyberbullying incidents worldwide, creating a need for automated detection
systems to protect users. The rise of code-mixed Hindi-English (Hinglish)
communication on digital platforms poses challenges for existing cyberbullying
detection systems, which were designed primarily for monolingual text. This
paper presents a framework for cyberbullying detection in Hinglish text using
the Multilingual Representations for Indian Languages (MURIL) architecture to
address limitations in current approaches. Evaluation across six benchmark
datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et
al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based
approach outperforms existing multilingual models including RoBERTa and
IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies
of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\%
on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The
framework includes explainability features through attribution analysis and
cross-linguistic pattern recognition. Ablation studies show that selective
layer freezing, appropriate classification head design, and specialized
preprocessing for code-mixed content improve detection performance, while
failure analysis identifies challenges including context-dependent
interpretation, cultural understanding, and cross-linguistic sarcasm detection,
providing directions for future research in multilingual cyberbullying
detection.

</details>


### [19] [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
*Natapong Nitarach,Warit Sirichotedumrong,Panop Pitchayarthorn,Pittawat Taveekitworachai,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: FinCoT introduces a structured CoT prompting method for financial reasoning, outperforming standard and unstructured CoT methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the lack of domain-specific structured reasoning in FinNLP and improve model performance and interpretability.

Method: Evaluates three prompting styles (standard, unstructured CoT, structured CoT) and FinCoT on CFA-style questions across ten financial domains.

Result: FinCoT boosts accuracy (63.2% to 80.5%) and reduces token usage by eight-fold compared to structured CoT.

Conclusion: Domain-aligned structured prompts enhance performance, reduce costs, and improve interpretability.

Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting
approach that incorporates insights from domain-specific expert financial
reasoning to guide the reasoning traces of large language models. We
investigate that there are three main prompting styles in FinNLP: (1) standard
prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an
explicit reasoning structure, such as the use of tags; and (3) structured CoT
prompting--CoT prompting with explicit instructions or examples that define
structured reasoning steps. Previously, FinNLP has primarily focused on prompt
engineering with either standard or unstructured CoT prompting. However,
structured CoT prompting has received limited attention in prior work.
Furthermore, the design of reasoning structures in structured CoT prompting is
often based on heuristics from non-domain experts. In this study, we
investigate each prompting approach in FinNLP. We evaluate the three main
prompting styles and FinCoT on CFA-style questions spanning ten financial
domains. We observe that FinCoT improves performance from 63.2% to 80.5% and
Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens
eight-fold compared to structured CoT prompting. Our findings show that
domain-aligned structured prompts not only improve performance and reduce
inference costs but also yield more interpretable and expert-aligned reasoning
traces.

</details>


### [20] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
*Chenxi Wang,Yixuan Zhang,Lang Gao,Zixiang Xu,Zirui Song,Yanbo Wang,Xiuying Chen*

Main category: cs.CL

TL;DR: LLMs internalize language-specific reasoning biases, as shown by attention patterns and performance on a bilingual causal reasoning dataset (BICAUSE).


<details>
  <summary>Details</summary>
Motivation: To test if LLMs internalize habitual logical structures from language, as suggested by linguistic relativity.

Method: Used BICAUSE, a bilingual dataset with aligned Chinese and English samples, to analyze LLM attention and performance.

Result: LLMs show language-specific attention patterns, rigidly apply causal word order preferences, and converge on shared abstractions when reasoning succeeds.

Conclusion: LLMs mimic and internalize language-shaped reasoning biases, empirically verified through structural analysis.

Abstract: Language is not only a tool for communication but also a medium for human
cognition and reasoning. If, as linguistic relativity suggests, the structure
of language shapes cognitive patterns, then large language models (LLMs)
trained on human language may also internalize the habitual logical structures
embedded in different languages. To examine this hypothesis, we introduce
BICAUSE, a structured bilingual dataset for causal reasoning, which includes
semantically aligned Chinese and English samples in both forward and reversed
causal forms. Our study reveals three key findings: (1) LLMs exhibit
typologically aligned attention patterns, focusing more on causes and
sentence-initial connectives in Chinese, while showing a more balanced
distribution in English. (2) Models internalize language-specific preferences
for causal word order and often rigidly apply them to atypical inputs, leading
to degraded performance, especially in Chinese. (3) When causal reasoning
succeeds, model representations converge toward semantically aligned
abstractions across languages, indicating a shared understanding beyond surface
form. Overall, these results suggest that LLMs not only mimic surface
linguistic forms but also internalize the reasoning biases shaped by language.
Rooted in cognitive linguistic theory, this phenomenon is for the first time
empirically verified through structural analysis of model internals.

</details>


### [21] [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
*Guanhua Chen,Yutong Yao,Lidia S. Chao,Xuebo Liu,Derek F. Wong*

Main category: cs.CL

TL;DR: The paper introduces SGIC, a Self-Guided Iterative Calibration Framework, to improve LLMs' calibration by using uncertainty scores for multi-round refinements.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods often overlook LLMs' calibration capabilities, which can enhance in-context reasoning.

Method: SGIC uses uncertainty scores to assess document relevance and LLM confidence, iteratively refining calibration. It also introduces a self-calibration training set.

Result: The framework boosts performance on both closed-source and open-weight LLMs.

Conclusion: SGIC effectively leverages uncertainty scores to enhance LLM calibration and response accuracy.

Abstract: Recent research in retrieval-augmented generation (RAG) has concentrated on
retrieving useful information from candidate documents. However, numerous
methodologies frequently neglect the calibration capabilities of large language
models (LLMs), which capitalize on their robust in-context reasoning prowess.
This work illustrates that providing LLMs with specific cues substantially
improves their calibration efficacy, especially in multi-round calibrations. We
present a new SGIC: Self-Guided Iterative Calibration Framework that employs
uncertainty scores as a tool. Initially, this framework calculates uncertainty
scores to determine both the relevance of each document to the query and the
confidence level in the responses produced by the LLMs. Subsequently, it
reevaluates these scores iteratively, amalgamating them with prior responses to
refine calibration. Furthermore, we introduce an innovative approach for
constructing an iterative self-calibration training set, which optimizes LLMs
to efficiently harness uncertainty scores for capturing critical information
and enhancing response accuracy. Our proposed framework significantly improves
performance on both closed-source and open-weight LLMs.

</details>


### [22] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
*Masashi Takeshita,Rafal Rzepka*

Main category: cs.CL

TL;DR: JETHICS is a Japanese dataset for evaluating AI ethics understanding, with 78K examples based on normative theories and commonsense morality. Evaluations show GPT-4o scores ~0.7 and the best Japanese LLM ~0.5, indicating significant room for improvement.


<details>
  <summary>Details</summary>
Motivation: To assess the ethics understanding of AI models in Japanese, addressing gaps in non-English datasets.

Method: Constructed JETHICS using methods from the English ETHICS dataset, covering four normative theory categories and commonsense morality.

Result: GPT-4o scored ~0.7, while the best Japanese LLM scored ~0.5, highlighting performance gaps.

Conclusion: Current LLMs, including GPT-4o, have substantial room for improvement in Japanese ethics understanding.

Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics
understanding of AI models. JETHICS contains 78K examples and is built by
following the construction methods of the existing English ETHICS dataset. It
includes four categories based normative theories and concepts from ethics and
political philosophy; and one representing commonsense morality. Our evaluation
experiments on non-proprietary large language models (LLMs) and on GPT-4o
reveal that even GPT-4o achieves only an average score of about 0.7, while the
best-performing Japanese LLM attains around 0.5, indicating a relatively large
room for improvement in current LLMs.

</details>


### [23] [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
*Luna Wang,Andrew Caines,Alice Hutchings*

Main category: cs.CL

TL;DR: The paper critiques hate speech dataset curation, advocating for reflexive, transparent methods to improve reliability.


<details>
  <summary>Details</summary>
Motivation: To highlight the methodological challenges and value judgments in hate speech dataset creation, emphasizing the need for transparency.

Method: Examines diverse datasets using Max Weber's ideal types, proposing a reflexive approach to acknowledge researcher biases.

Result: Identifies common themes and practices in dataset curation, stressing their impact on reliability.

Conclusion: Researchers should adopt reflexive, transparent methods to enhance dataset rigor and acknowledge value judgments.

Abstract: The curation of hate speech datasets involves complex design decisions that
balance competing priorities. This paper critically examines these
methodological choices in a diverse range of datasets, highlighting common
themes and practices, and their implications for dataset reliability. Drawing
on Max Weber's notion of ideal types, we argue for a reflexive approach in
dataset creation, urging researchers to acknowledge their own value judgments
during dataset construction, fostering transparency and methodological rigour.

</details>


### [24] [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
*Anindita Bhattacharya,Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Main category: cs.CL

TL;DR: This research evaluates advanced abstractive summarization models to generate concise radiology report impressions from detailed findings, using the MIMIC-CXR dataset and multiple metrics for assessment.


<details>
  <summary>Details</summary>
Motivation: To address the need for automated, concise summarization of lengthy radiology findings into key diagnostic impressions.

Method: Comparative analysis of pre-trained and open-source models (T5-base, BART-base, PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network) using metrics like ROUGE, METEOR, and BERTScore.

Result: Identifies strengths and limitations of each model in medical text summarization.

Conclusion: Provides valuable insights for healthcare professionals seeking automated summarization tools.

Abstract: The findings section of a radiology report is often detailed and lengthy,
whereas the impression section is comparatively more compact and captures key
diagnostic conclusions. This research explores the use of advanced abstractive
summarization models to generate the concise impression from the findings
section of a radiology report. We have used the publicly available MIMIC-CXR
dataset. A comparative analysis is conducted on leading pre-trained and
open-source large language models, including T5-base, BART-base,
PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network
with a coverage mechanism. To ensure a thorough assessment, multiple evaluation
metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and
BERTScore. By analyzing the performance of these models, this study identifies
their respective strengths and limitations in the summarization of medical
text. The findings of this paper provide helpful information for medical
professionals who need automated summarization solutions in the healthcare
sector.

</details>


### [25] [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
*Aishwarya Pothula,Bhavana Akkiraju,Srihari Bandarupalli,Charan D,Santosh Kesiraju,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: The paper investigates using weakly labeled data to build speech-to-text translation systems for low-resource languages, achieving performance comparable to strong baselines.


<details>
  <summary>Details</summary>
Motivation: High-quality annotated data is scarce for low-resource languages, prompting exploration of weakly labeled data as an alternative.

Method: Bitext mining using state-of-the-art sentence encoders was employed to create datasets (Shrutilipi-anuvaad) for Bengali-Hindi, Malayalam-Hindi, Odia-Hindi, and Telugu-Hindi. Training data with varying quality and quantity was tested.

Result: ST systems built with weakly labeled data performed comparably to massive multi-modal multilingual baselines like SONAR and SeamlessM4T.

Conclusion: Weakly labeled data is a viable alternative for building effective ST systems for low-resource languages.

Abstract: The scarcity of high-quality annotated data presents a significant challenge
in developing effective end-to-end speech-to-text translation (ST) systems,
particularly for low-resource languages. This paper explores the hypothesis
that weakly labeled data can be used to build ST models for low-resource
language pairs. We constructed speech-to-text translation datasets with the
help of bitext mining using state-of-the-art sentence encoders. We mined the
multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset
comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,
Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data
with varying degrees of quality and quantity to investigate the effect of
quality versus quantity of weakly labeled data on ST model performance. Results
demonstrate that ST systems can be built using weakly labeled data, with
performance comparable to massive multi-modal multilingual baselines such as
SONAR and SeamlessM4T.

</details>


### [26] [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
*Hao-Chien Lu,Jhen-Ke Lin,Hong-Yun Lin,Chung-Chun Wang,Berlin Chen*

Main category: cs.CL

TL;DR: The paper introduces a hybrid scoring model for automated speaking assessment (ASA) with two enhancements: a multifaceted relevance module and fine-grained grammar error features, improving content relevance and language use evaluation.


<details>
  <summary>Details</summary>
Motivation: Current ASA systems lack comprehensive content relevance analysis and detailed grammar error identification, limiting their effectiveness in multi-aspect evaluations.

Method: Proposes a hybrid scoring model with (1) a relevance module integrating question, image, exemplar, and spoken response, and (2) fine-grained grammar error features from GEC and annotation.

Result: Experiments show significant improvements in evaluating content relevance, language use, and overall ASA performance.

Conclusion: Richer, nuanced feature sets enhance holistic speaking assessment, addressing current system deficiencies.

Abstract: Current automated speaking assessment (ASA) systems for use in multi-aspect
evaluations often fail to make full use of content relevance, overlooking image
or exemplar cues, and employ superficial grammar analysis that lacks detailed
error types. This paper ameliorates these deficiencies by introducing two novel
enhancements to construct a hybrid scoring model. First, a multifaceted
relevance module integrates question and the associated image content,
exemplar, and spoken response of an L2 speaker for a comprehensive assessment
of content relevance. Second, fine-grained grammar error features are derived
using advanced grammar error correction (GEC) and detailed annotation to
identify specific error categories. Experiments and ablation studies
demonstrate that these components significantly improve the evaluation of
content relevance, language use, and overall ASA performance, highlighting the
benefits of using richer, more nuanced feature sets for holistic speaking
assessment.

</details>


### [27] [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
*Aleksandra Krasnodƒôbska,Karolina Seweryn,Szymon ≈Åukasik,Wojciech Kusa*

Main category: cs.CL

TL;DR: A benchmark dataset for LLM safety in Polish is introduced, along with adversarial samples. Experiments show a HerBERT-based classifier outperforms others, especially under adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: Existing safety assessments for LLMs are biased toward high-resource languages, leaving many global languages like Polish underexamined.

Method: A manually annotated benchmark dataset for Polish is created, including adversarial variants. Three models (Llama-Guard-3-8B, HerBERT-based classifier, PLLuM) are fine-tuned and evaluated.

Result: The HerBERT-based classifier achieves the highest performance, particularly in adversarial scenarios.

Conclusion: The study highlights the need for language-specific safety tools and demonstrates the effectiveness of the HerBERT-based approach for Polish.

Abstract: Despite increasing efforts to ensure the safety of large language models
(LLMs), most existing safety assessments and moderation tools remain heavily
biased toward English and other high-resource languages, leaving majority of
global languages underexamined. To address this gap, we introduce a manually
annotated benchmark dataset for language model safety classification in Polish.
We also create adversarially perturbed variants of these samples designed to
challenge model robustness. We conduct a series of experiments to evaluate
LLM-based and classifier-based models of varying sizes and architectures.
Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based
classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B
model. We train these models using different combinations of annotated data and
evaluate their performance, comparing it against publicly available guard
models. Results demonstrate that the HerBERT-based classifier achieves the
highest overall performance, particularly under adversarial conditions.

</details>


### [28] [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
*Agnese Daffara,Sourabh Dattawad,Sebastian Pad√≥,Tanise Ceron*

Main category: cs.CL

TL;DR: The study evaluates the applicability of the Media Frame Corpus (MFC) to Brazilian news, introducing FrameNews-PT and testing MFC frames' generalization.


<details>
  <summary>Details</summary>
Motivation: To assess if MFC frames, designed for U.S. news, can effectively capture frames in Brazilian political and economic news.

Method: Annotated FrameNews-PT dataset using MFC framework, tested fine-tuned and zero-shot models on out-of-domain data.

Result: MFC frames mostly apply but require minor guideline adjustments; some frames are rarely used, and fall-back frames handle novel issues.

Conclusion: Cross-cultural frame analysis needs careful adaptation of existing frameworks.

Abstract: Frames capture aspects of an issue that are emphasized in a debate by
interlocutors and can help us understand how political language conveys
different perspectives and ultimately shapes people's opinions. The Media Frame
Corpus (MFC) is the most commonly used framework with categories and detailed
guidelines for operationalizing frames. It is, however, focused on a few
salient U.S. news issues, making it unclear how well these frames can capture
news issues in other cultural contexts. To explore this, we introduce
FrameNews-PT, a dataset of Brazilian Portuguese news articles covering
political and economic news and annotate it within the MFC framework. Through
several annotation rounds, we evaluate the extent to which MFC frames
generalize to the Brazilian debate issues. We further evaluate how fine-tuned
and zero-shot models perform on out-of-domain data. Results show that the 15
MFC frames remain broadly applicable with minor revisions of the guidelines.
However, some MFC frames are rarely used, and novel news issues are analyzed
using general 'fall-back' frames. We conclude that cross-cultural frame use
requires careful consideration.

</details>


### [29] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
*Cedric M√∂ller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: Incorporating knowledge graph information improves relation extraction models, especially in imbalanced datasets, using graph-aware Neural Bellman-Ford networks.


<details>
  <summary>Details</summary>
Motivation: To test if entity positions in knowledge graphs enhance relation extraction performance.

Method: Combined established relation extraction methods with graph-aware Neural Bellman-Ford networks, tested in supervised and zero-shot settings.

Result: Significant performance improvement, particularly with imbalanced training examples.

Conclusion: Knowledge graph integration boosts relation extraction effectiveness across diverse datasets.

Abstract: We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.

</details>


### [30] [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
*Cedric M√∂ller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: A discriminative method for closed information extraction improves accuracy, especially for long-tail relations, outperforming generative models and emphasizing efficiency with smaller models.


<details>
  <summary>Details</summary>
Motivation: To enhance relation extraction accuracy, particularly for long-tail relations, and address the challenges of large-scale closed information extraction with millions of entities and hundreds of relations.

Method: Employs a discriminative approach incorporating type and entity-specific information, leveraging smaller models for efficiency.

Result: Superior performance compared to state-of-the-art generative models, with type-information integration matching or surpassing larger models.

Conclusion: The method advances accurate and efficient information extraction, promising better performance in large-scale scenarios.

Abstract: This paper introduces a novel method for closed information extraction. The
method employs a discriminative approach that incorporates type and
entity-specific information to improve relation extraction accuracy,
particularly benefiting long-tail relations. Notably, this method demonstrates
superior performance compared to state-of-the-art end-to-end generative models.
This is especially evident for the problem of large-scale closed information
extraction where we are confronted with millions of entities and hundreds of
relations. Furthermore, we emphasize the efficiency aspect by leveraging
smaller models. In particular, the integration of type-information proves
instrumental in achieving performance levels on par with or surpassing those of
a larger generative model. This advancement holds promise for more accurate and
efficient information extraction techniques.

</details>


### [31] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Main category: cs.CL

TL;DR: The paper examines whether LLMs like GPT-4 can represent real-world entities, arguing that structural correspondences alone are insufficient unless they explain successful task performance.


<details>
  <summary>Details</summary>
Motivation: To address uncertainties about LLMs' representational capacities, especially given their text-only inputs and outputs.

Method: Uses a structural-correspondence based account to analyze evidence and explore conditions for grounding representation.

Result: Structural correspondences alone don't ground representation; they must explain task success. LLMs' text-boundedness poses a challenge.

Conclusion: LLMs could represent real-world contents if structural correspondences play an appropriate role, but overcoming text-boundedness is key.

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [32] [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
*Kexin Huang,Qian Tu,Liwei Fan,Chenchen Yang,Dong Zhang,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: The paper introduces InstructTTSEval, a benchmark for evaluating instruction-driven TTS systems' ability to handle complex natural-language style control, highlighting current limitations and future potential.


<details>
  <summary>Details</summary>
Motivation: Traditional TTS systems lack flexibility in controlling paralinguistic features, and there's a shortage of benchmarks for evaluating instruction-based TTS models.

Method: The authors propose InstructTTSEval, a benchmark with three tasks (Acoustic-Parameter Specification, Descriptive-Style Directive, Role-Play) and 6k test cases, using Gemini as an automatic judge.

Result: Evaluation reveals significant room for improvement in current instruction-following TTS systems.

Conclusion: InstructTTSEval aims to advance more flexible and accurate instruction-driven TTS models.

Abstract: In modern speech synthesis, paralinguistic information--such as a speaker's
vocal timbre, emotional state, and dynamic prosody--plays a critical role in
conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)
systems rely on fixed style labels or inserting a speech prompt to control
these cues, which severely limits flexibility. Recent attempts seek to employ
natural-language instructions to modulate paralinguistic features,
substantially improving the generalization of instruction-driven TTS models.
Although many TTS systems now support customized synthesis via textual
description, their actual ability to interpret and execute complex instructions
remains largely unexplored. In addition, there is still a shortage of
high-quality benchmarks and automated evaluation metrics specifically designed
for instruction-based TTS, which hinders accurate assessment and iterative
optimization of these models. To address these limitations, we introduce
InstructTTSEval, a benchmark for measuring the capability of complex
natural-language style control. We introduce three tasks, namely
Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,
including English and Chinese subsets, each with 1k test cases (6k in total)
paired with reference audio. We leverage Gemini as an automatic judge to assess
their instruction-following abilities. Our evaluation of accessible
instruction-following TTS systems highlights substantial room for further
improvement. We anticipate that InstructTTSEval will drive progress toward more
powerful, flexible, and accurate instruction-following TTS.

</details>


### [33] [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
*Hao Li,Viktor Schlegel,Yizheng Sun,Riza Batista-Navarro,Goran Nenadic*

Main category: cs.CL

TL;DR: A survey on how Large Language Models (LLMs) have advanced Argument Mining (AM) in NLP, covering theories, datasets, methodologies, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To synthesize recent advancements in LLM-driven AM and guide researchers in this evolving field.

Method: Systematic review of foundational theories, datasets, LLM techniques (e.g., prompting, chain-of-thought reasoning), architectures, and evaluation practices.

Result: Comprehensive taxonomy of AM subtasks, insights into LLM-driven advancements, and identification of key challenges like long-context reasoning and interpretability.

Conclusion: Highlights emerging trends and proposes a research agenda to strategically advance LLM-based computational argumentation.

Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing
(NLP), focuses on extracting argumentative structures from text. The advent of
Large Language Models (LLMs) has profoundly transformed AM, enabling advanced
in-context learning, prompt-based generation, and robust cross-domain
adaptability. This survey systematically synthesizes recent advancements in
LLM-driven AM. We provide a concise review of foundational theories and
annotation frameworks, alongside a meticulously curated catalog of datasets. A
key contribution is our comprehensive taxonomy of AM subtasks, elucidating how
contemporary LLM techniques -- such as prompting, chain-of-thought reasoning,
and retrieval augmentation -- have reconfigured their execution. We further
detail current LLM architectures and methodologies, critically assess
evaluation practices, and delineate pivotal challenges including long-context
reasoning, interpretability, and annotation bottlenecks. Conclusively, we
highlight emerging trends and propose a forward-looking research agenda for
LLM-based computational argumentation, aiming to strategically guide
researchers in this rapidly evolving domain.

</details>


### [34] [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
*Sani Abdullahi Sani,Salim Abubakar,Falalu Ibrahim Lawan,Abdulhamid Abubakar,Maryam Bala*

Main category: cs.CL

TL;DR: The paper presents a method for multi-label emotion detection in Hausa using AfriBERTa, achieving 74% accuracy and 73.5% F1-score.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of emotion detection in Hausa, a low-resource African language, leveraging transformer-based models.

Method: Fine-tuned AfriBERTa on Hausa text, involving preprocessing, tokenization, and Hugging Face Trainer API for classification into six emotions.

Result: Achieved 74.00% validation accuracy and 73.50% F1-score.

Conclusion: Transformer-based models like AfriBERTa are effective for emotion detection in low-resource languages.

Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a
low-resource African language, as part of SemEval Track A. We fine-tuned
AfriBERTa, a transformer-based model pre-trained on African languages, to
classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and
surprise. Our methodology involved data preprocessing, tokenization, and model
fine-tuning using the Hugging Face Trainer API. The system achieved a
validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the
effectiveness of transformer-based models for emotion detection in low-resource
languages.

</details>


### [35] [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
*Chenyi Zhou,Zhengyan Shi,Yuan Yao,Lei Liang,Huajun Chen,Qiang Zhang*

Main category: cs.CL

TL;DR: RiOT is a novel framework for automatic prompt optimization in LLMs, addressing diversity and semantic drift issues by iteratively refining prompts with text gradients and residual connections.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods lack diversity and suffer from semantic drift, limiting their effectiveness.

Method: RiOT uses text gradients to generate diverse prompt candidates, selects the best via perplexity, and employs residual connections to retain beneficial content. A tree structure ensures scalability.

Result: RiOT outperforms previous methods and manual prompting across five benchmarks, including commonsense and mathematical reasoning.

Conclusion: RiOT effectively addresses diversity and semantic drift, demonstrating superior performance in prompt optimization for LLMs.

Abstract: Recent advancements in large language models (LLMs) have highlighted their
potential across a variety of tasks, but their performance still heavily relies
on the design of effective prompts. Existing methods for automatic prompt
optimization face two challenges: lack of diversity, limiting the exploration
of valuable and innovative directions and semantic drift, where optimizations
for one task can degrade performance in others. To address these issues, we
propose Residual Optimization Tree (RiOT), a novel framework for automatic
prompt optimization. RiOT iteratively refines prompts through text gradients,
generating multiple semantically diverse candidates at each step, and selects
the best prompt using perplexity. Additionally, RiOT incorporates the text
residual connection to mitigate semantic drift by selectively retaining
beneficial content across optimization iterations. A tree structure efficiently
manages the optimization process, ensuring scalability and flexibility.
Extensive experiments across five benchmarks, covering commonsense,
mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT
outperforms both previous prompt optimization methods and manual prompting.

</details>


### [36] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
*Yao Lu,Zhaiyuan Ji,Jiawei Du,Yu Shanqing,Qi Xuan,Tianyi Zhou*

Main category: cs.CL

TL;DR: AutoAnnotator is a multi-model cooperative annotation framework that reduces costs and improves accuracy by combining LLMs and SLMs.


<details>
  <summary>Details</summary>
Motivation: Address the high cost and low accuracy of LLMs in fine-grained semantic tasks like sentiment and toxicity classification.

Method: A two-layer framework: meta-controller (LLMs for selection, code generation, and verification) and task-specialist (SLMs for voting). Uses continual learning to fine-tune SLMs.

Result: Outperforms LLMs in zero-shot, one-shot, CoT, and majority voting, reducing costs by 74.15% and improving accuracy by 6.21%.

Conclusion: AutoAnnotator offers a cost-effective and accurate alternative to LLM-based annotation, leveraging SLMs and continual learning.

Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has
made significant breakthroughs in recent years, its actual deployment still has
two core bottlenecks: first, the cost of calling commercial APIs in large-scale
annotation is very expensive; second, in scenarios that require fine-grained
semantic understanding, such as sentiment classification and toxicity
classification, the annotation accuracy of LLMs is even lower than that of
Small Language Models (SLMs) dedicated to this field. To address these
problems, we propose a new paradigm of multi-model cooperative annotation and
design a fully automatic annotation framework AutoAnnotator based on this.
Specifically, AutoAnnotator consists of two layers. The upper-level
meta-controller layer uses the generation and reasoning capabilities of LLMs to
select SLMs for annotation, automatically generate annotation code and verify
difficult samples; the lower-level task-specialist layer consists of multiple
SLMs that perform annotation through multi-model voting. In addition, we use
the difficult samples obtained by the secondary review of the meta-controller
layer as the reinforcement learning set and fine-tune the SLMs in stages
through a continual learning strategy, thereby improving the generalization of
SLMs. Extensive experiments show that AutoAnnotator outperforms existing
open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.
Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to
directly annotating with GPT-3.5-turbo, while still improving the accuracy by
6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [37] [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
*Zhexu Wang,Yiping Liu,Yejie Wang,Wenyang He,Bofei Gao,Muxi Diao,Yanxu Chen,Kelin Fu,Flood Sung,Zhilin Yang,Tianyu Liu,Weiran Xu*

Main category: cs.CL

TL;DR: OJBench is a new benchmark for evaluating competitive-level code reasoning in LLMs, revealing limitations even in top models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack the rigor to assess competitive-level code reasoning in LLMs.

Method: OJBench includes 232 programming competition problems from NOI and ICPC, tested on 37 diverse LLMs.

Result: State-of-the-art models struggle with highly challenging competition-level problems.

Conclusion: Competitive-level code reasoning remains a significant challenge for LLMs.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
significant progress in math and code reasoning capabilities. However, existing
code benchmark are limited in their ability to evaluate the full spectrum of
these capabilities, particularly at the competitive level. To bridge this gap,
we introduce OJBench, a novel and challenging benchmark designed to assess the
competitive-level code reasoning abilities of LLMs. OJBench comprises 232
programming competition problems from NOI and ICPC, providing a more rigorous
test of models' reasoning skills. We conducted a comprehensive evaluation using
OJBench on 37 models, including both closed-source and open-source models,
reasoning-oriented and non-reasoning-oriented models. Our results indicate that
even state-of-the-art reasoning-oriented models, such as o4-mini and
Gemini-2.5-pro-exp, struggle with highly challenging competition-level
problems. This highlights the significant challenges that models face in
competitive-level code reasoning.

</details>


### [38] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
*Shushanta Pudasaini,Aman Shakya,Siddhartha Shrestha,Sahil Bhatta,Sunil Thapa,Sushmita Palikhe*

Main category: cs.CL

TL;DR: The paper introduces NepaliGPT, the first generative LLM for the Nepali language, addressing a gap in NLP research. It includes a new corpus and benchmark dataset, achieving strong performance metrics.


<details>
  <summary>Details</summary>
Motivation: The lack of a generative language model for Nepali hinders downstream tasks. This research fills that gap by developing NepaliGPT.

Method: The study collects a Devanagari Corpus and creates a benchmark dataset of 4,296 question-answer pairs. NepaliGPT is then developed and evaluated.

Result: NepaliGPT achieves a perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25%, and causal consistency of 85.41%.

Conclusion: NepaliGPT successfully addresses the research gap, providing a foundation for future Nepali NLP tasks.

Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge
popularity in recent days and thousands of variants of LLMs have been released.
However, there is no generative language model for the Nepali language, due to
which other downstream tasks, including fine-tuning, have not been explored
yet. To fill this research gap in the Nepali NLP space, this research proposes
\textit{NepaliGPT}, a generative large language model tailored specifically for
the Nepali language. This research introduces an advanced corpus for the Nepali
language collected from several sources, called the Devanagari Corpus.
Likewise, the research introduces the first NepaliGPT benchmark dataset
comprised of 4,296 question-answer pairs in the Nepali language. The proposed
LLM NepaliGPT achieves the following metrics in text generation: Perplexity of
26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal
consistency of 85.41\%.

</details>


### [39] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Main category: cs.CL

TL;DR: The paper proposes a framework to address LLM challenges with long texts, identifying three failure modes and advocating multi-agent chunking for effective processing.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the challenges of applying LLMs to long texts by analyzing failure modes and proposing solutions.

Method: Theoretical framework categorizing failure modes (task, model, aggregator noise) and experimental validation of multi-agent chunking on tasks like retrieval, QA, and summarization.

Result: Multi-agent chunking is effective under certain conditions, and weaker models with chunking can outperform advanced models like GPT4o for large inputs.

Conclusion: A principled framework and chunking strategies offer a viable pathway for handling long contexts in LLMs.

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [40] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: REIS is an In-Storage Processing (ISP) system designed to optimize Retrieval-Augmented Generation (RAG) by addressing bottlenecks in retrieval, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: LLMs' static knowledge limits their effectiveness; RAG supplements this with external data, but retrieval is slow due to data movement and inefficient ANNS.

Method: REIS introduces a tailored database layout, ISP-friendly data placement, and leverages existing storage resources for ANNS.

Result: REIS improves retrieval performance by 13x and energy efficiency by 55x compared to server-grade systems.

Conclusion: REIS effectively addresses RAG's retrieval bottlenecks, offering significant performance and efficiency gains.

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [41] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: StoryWriter is a multi-agent framework for long story generation, addressing coherence and complexity through outline, planning, and writing agents. It outperforms baselines and generates a high-quality dataset.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with long story generation due to discourse coherence and narrative complexity.

Method: Uses three agents: outline (event plots), planning (chapter details), and writing (dynamic compression). Evaluated via human and automated metrics.

Result: Significantly outperforms baselines in quality and length. Generates 6,000 high-quality stories (~8,000 words each).

Conclusion: StoryWriter advances long story generation, with fine-tuned models (Llama3.1-8B, GLM4-9B) showing superior performance.

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [42] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau B√∂l√∂ni*

Main category: cs.CL

TL;DR: The paper proposes a method to detect implicit hate speech in existing harmful speech datasets using lexicon analysis, influential sample identification, reannotation, and augmentation with Llama-3 70B and GPT-4o, achieving a 12.9-point F1 score improvement.


<details>
  <summary>Details</summary>
Motivation: Implicit hate speech is a growing challenge for social media platforms, often overlooked in traditional harmful speech datasets due to its subtle nature and annotator subjectivity.

Method: The approach involves lexicon analysis, influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o to enhance detection.

Result: The method improves implicit hate detection by +12.9 F1 score compared to baseline.

Conclusion: The proposed approach effectively addresses implicit hate speech detection and generalizability across datasets.

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [43] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
*Soumya Suvra Ghosal,Vaibhav Singh,Akash Ghosh,Soumyabrata Pal,Subhadip Baidya,Sriparna Saha,Dinesh Manocha*

Main category: cs.CL

TL;DR: RELIC is an in-context learning framework for reward modeling in low-resource Indic languages, improving accuracy by leveraging auxiliary high-resource language examples.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual reward models are unreliable for low-resource Indic languages due to lack of preference data, making traditional training impractical.

Method: RELIC trains a retriever with a pairwise ranking objective to select effective in-context examples from high-resource languages.

Result: RELIC improves reward model accuracy by 12.81% and 10.13% over zero-shot and state-of-the-art methods, respectively, for low-resource languages like Bodo.

Conclusion: RELIC effectively addresses the challenge of reward modeling in low-resource languages by leveraging high-resource language examples.

Abstract: Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.

</details>


### [44] [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
*Dana Serditova,Kevin Tang,Jochen Steffens*

Main category: cs.CL

TL;DR: ASR systems perform poorly on regional dialects like Newcastle English due to biased training. This study identifies phonological, lexical, and morphosyntactic errors in ASR, emphasizing the need for dialectal diversity in training data.


<details>
  <summary>Details</summary>
Motivation: To address the underexamined issue of regional bias in ASR systems, particularly for challenging dialects like Newcastle English.

Method: A two-stage analysis: manual error analysis on a subsample and a case study on regional pronouns.

Result: ASR errors correlate with dialectal features, with social factors playing a minor role.

Conclusion: Greater dialectal diversity in training data and sociolinguistic analysis are needed to mitigate regional biases in ASR.

Abstract: Automatic Speech Recognition (ASR) systems struggle with regional dialects
due to biased training which favours mainstream varieties. While previous
research has identified racial, age, and gender biases in ASR, regional bias
remains underexamined. This study investigates ASR performance on Newcastle
English, a well-documented regional dialect known to be challenging for ASR. A
two-stage analysis was conducted: first, a manual error analysis on a subsample
identified key phonological, lexical, and morphosyntactic errors behind ASR
misrecognitions; second, a case study focused on the systematic analysis of ASR
recognition of the regional pronouns ``yous'' and ``wor''. Results show that
ASR errors directly correlate with regional dialectal features, while social
factors play a lesser role in ASR mismatches. We advocate for greater dialectal
diversity in ASR training data and highlight the value of sociolinguistic
analysis in diagnosing and addressing regional biases.

</details>


### [45] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: A continual learning approach for neural speech recognition models prevents catastrophic forgetting by using factorization and centralization phases, inspired by human learning cycles.


<details>
  <summary>Details</summary>
Motivation: To enable neural networks to absorb new data without re-training, avoiding catastrophic forgetting in rehearsal-free, multilingual settings.

Method: Proposes a two-phase approach: factorization for learning and centralization for merging knowledge, using low-rank adapters.

Result: Effective prevention of catastrophic forgetting in experiments with varied code-switching datasets.

Conclusion: The centralization phase successfully preserves knowledge by accumulating it in low-rank adapters.

Abstract: Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

</details>


### [46] [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
*Tuan-Nam Nguyen,Ngoc-Quan Pham,Seymanur Akti,Alexander Waibel*

Main category: cs.CL

TL;DR: A streaming accent conversion model transforms non-native speech into native-like accents while preserving speaker identity and prosody, using an Emformer encoder and optimized inference for real-time processing.


<details>
  <summary>Details</summary>
Motivation: To enable real-time accent conversion while maintaining speaker identity and prosody, addressing the lack of streaming-capable AC models.

Method: Modifies a prior AC architecture with an Emformer encoder and optimized inference, integrating a native TTS model for training data.

Result: Achieves performance comparable to top AC models with stable latency, enabling the first streaming AC system.

Conclusion: The proposed model successfully enables streaming accent conversion without sacrificing quality or speaker identity.

Abstract: We propose a first streaming accent conversion (AC) model that transforms
non-native speech into a native-like accent while preserving speaker identity,
prosody and improving pronunciation. Our approach enables stream processing by
modifying a previous AC architecture with an Emformer encoder and an optimized
inference mechanism. Additionally, we integrate a native text-to-speech (TTS)
model to generate ideal ground-truth data for efficient training. Our streaming
AC model achieves comparable performance to the top AC models while maintaining
stable latency, making it the first AC system capable of streaming.

</details>


### [47] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: The paper proposes a framework to evaluate if LLMs have a robust world model by measuring response consistency across semantically equivalent prompts. Results show larger models perform better but not uniformly across domains.


<details>
  <summary>Details</summary>
Motivation: Assessing LLM reliability in high-stakes applications by determining if they have a structured world model.

Method: Introduces an evaluation approach decomposing response variability into user purpose, articulation, and model instability.

Result: Larger models show more robustness, but improvements are modest and not uniform across domains.

Conclusion: Semantic diagnostics are crucial for assessing LLM world models beyond accuracy benchmarks.

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [48] [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
*Hanshu Rao,Weisi Liu,Haohan Wang,I-Chan Huang,Zhe He,Xiaolei Huang*

Main category: cs.CL

TL;DR: A scoping review synthesizes 59 studies (2020-2025) on synthetic data generation in biomedical fields, focusing on clinical applications, methods, and evaluations, while addressing current limitations and challenges.


<details>
  <summary>Details</summary>
Motivation: To mitigate data scarcity, privacy concerns, and quality challenges in biomedical research using synthetic data generated by LLMs.

Method: Followed PRISMA-ScR guidelines to review 59 studies, analyzing data modalities (text, tabular, multimodal), generation methods (prompting, fine-tuning, specialized models), and evaluation metrics (intrinsic, human-in-the-loop, LLM-based).

Result: Identified trends in synthetic data generation, with unstructured texts (78.0%) as the dominant modality, prompting (72.9%) as the primary method, and human-in-the-loop assessments (55.9%) as the main evaluation approach.

Conclusion: Highlights challenges in clinical domain adaptation, resource accessibility, and evaluation standardization, emphasizing the need for further research and standardization.

Abstract: Synthetic data generation--mitigating data scarcity, privacy concerns, and
data quality challenges in biomedical fields--has been facilitated by rapid
advances of large language models (LLMs). This scoping review follows
PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and
2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The
review systematically examines biomedical research and application trends in
synthetic data generation, emphasizing clinical applications, methodologies,
and evaluations. Our analysis identifies data modalities of unstructured texts
(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation
methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model
(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),
human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The
analysis addresses current limitations in what, where, and how health
professionals can leverage synthetic data generation for biomedical domains.
Our review also highlights challenges in adaption across clinical domains,
resource and model accessibility, and evaluation standardizations.

</details>


### [49] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Main category: cs.CL

TL;DR: A computational framework models public perception of science news across 12 dimensions, using a large-scale dataset and NLP models to predict engagement patterns.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of anticipating public perception and interaction with scientific news in an information-saturated environment.

Method: Developed a computational framework and NLP models to analyze public perception, using a dataset of 10,489 annotations from 2,101 participants.

Result: Science news consumption frequency drives perception, and estimated perception scores predict engagement (comments/upvotes) on platforms like Reddit.

Conclusion: Nuanced perception modeling is crucial for predicting public interest and engagement with scientific content.

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


### [50] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Main category: cs.CL

TL;DR: The paper proposes using LLMs to automate the development of rule-based NLP systems, achieving high recall in identifying clinical text snippets and perfect accuracy in extracting key terms for NER.


<details>
  <summary>Details</summary>
Motivation: Rule-based NLP systems are preferred in clinical settings for interpretability and efficiency, but their manual development is labor-intensive. The study aims to address this by leveraging LLMs.

Method: The approach uses LLMs during the development phase of rule-based NLP systems, focusing on identifying relevant text snippets and extracting keywords for NER.

Result: Experiments showed high recall (Deepseek: 0.98, Qwen: 0.99) in snippet identification and perfect accuracy (1.0) in key term extraction.

Conclusion: The study highlights a promising direction for semi-automated or automated development of rule-based NLP systems, offering faster, cost-effective, and transparent solutions compared to deep learning.

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [51] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Main category: cs.CL

TL;DR: The paper introduces GeoGuess, a novel multimodal reasoning task involving hierarchical visual clues and geographic knowledge, and presents the SightSense method for solving it.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal reasoning tasks lack focus on hierarchical visual clues (local details and global context), which are crucial in real-world scenarios like location identification.

Method: The authors propose SightSense, a multimodal and multilevel reasoning method, and introduce the GeoExplain dataset (panoramas-geocoordinates-explanation tuples) for benchmarking.

Result: SightSense demonstrates outstanding performance in the GeoGuess task, effectively integrating hierarchical visual information and external knowledge.

Conclusion: GeoGuess and SightSense address a gap in multimodal reasoning, offering a challenging benchmark and a robust solution for hierarchical visual and geographic reasoning.

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [52] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
*Pavlo Vasylenko,Marcos Treviso,Andr√© F. T. Martins*

Main category: cs.CL

TL;DR: The paper introduces ASEntmax, a sparse attention mechanism using Œ±-entmax, to improve focus on fixed-size patterns in transformers, outperforming traditional softmax methods.


<details>
  <summary>Details</summary>
Motivation: Traditional softmax attention disperses focus over long sequences, harming tasks requiring precise attention on fixed-size patterns.

Method: Proposes ASEntmax, a learnable sparse attention mechanism, and emphasizes the role of position encodings.

Result: ASEntmax outperforms softmax and other baselines in long-context generalization.

Conclusion: Sparse attention with ASEntmax and proper position encodings enhances transformer performance for tasks needing precise focus.

Abstract: Transformer-based architectures traditionally employ softmax to compute
attention weights, which produces dense distributions over all tokens in a
sequence. While effective in many settings, this density has been shown to be
detrimental for tasks that demand precise focus on fixed-size patterns: as
sequence length increases, non-informative tokens accumulate attention
probability mass, leading to dispersion and representational collapse. We show
in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid
these issues, due to their ability to assign exact zeros to irrelevant tokens.
Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows
$\alpha$-entmax with a learnable temperature parameter, allowing the attention
distribution to interpolate between sparse (pattern-focused) and dense
(softmax-like) regimes. Finally, we show that the ability to locate and
generalize fixed-size patterns can be further improved through a careful design
of position encodings, which impacts both dense and sparse attention methods.
By integrating ASEntmax into standard transformer layers alongside proper
positional encodings, we show that our models greatly outperform softmax,
scalable softmax, and fixed-temperature $\alpha$-entmax baselines on
long-context generalization.

</details>


### [53] [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
*Co Tran,Salman Paracha,Adil Hafeez,Shuguang Chen*

Main category: cs.CL

TL;DR: The paper introduces Arch-Router, a 1.5B model for preference-aligned routing of LLMs, addressing limitations in existing methods by better capturing human preferences and supporting flexible model additions.


<details>
  <summary>Details</summary>
Motivation: Existing LLM routing approaches fail to align with human preferences and lack flexibility in model selection.

Method: Proposes Arch-Router, a compact model that maps queries to domain-action preferences for routing, enabling seamless integration of new models without retraining.

Result: Achieves SOTA in matching queries with human preferences, outperforming proprietary models, and enhances transparency and flexibility in routing.

Conclusion: Arch-Router effectively addresses subjective evaluation criteria and improves routing decisions, with the model publicly available for use.

Abstract: With the rapid proliferation of large language models (LLMs) -- each
optimized for different strengths, style, or latency/cost profile -- routing
has become an essential technique to operationalize the use of different
models. However, existing LLM routing approaches are limited in two key ways:
they evaluate performance using benchmarks that often fail to capture human
preferences driven by subjective evaluation criteria, and they typically select
from a limited pool of models. In this work, we propose a preference-aligned
routing framework that guides model selection by matching queries to
user-defined domains (e.g., travel) or action types (e.g., image editing) --
offering a practical mechanism to encode preferences in routing decisions.
Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that
learns to map queries to domain-action preferences for model routing decisions.
Our approach also supports seamlessly adding new models for routing without
requiring retraining or architectural modifications. Experiments on
conversational datasets demonstrate that our approach achieves state-of-the-art
(SOTA) results in matching queries with human preferences, outperforming top
proprietary models. Our approach captures subjective evaluation criteria and
makes routing decisions more transparent and flexible. Our model is available
at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

</details>


### [54] [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
*Ananth Agarwal,Jasper Jian,Christopher D. Manning,Shikhar Murty*

Main category: cs.CL

TL;DR: Probing LLMs for syntactic features doesn't reliably predict their downstream syntactic performance, revealing a disconnect between latent representations and observable behaviors.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs internally represent syntax and whether probing accurately reflects their syntactic capabilities in practice.

Method: Evaluated 32 transformer models using probing for syntactic features and compared results with downstream syntactic task performance.

Result: Syntactic features from probing did not predict downstream syntactic performance, showing a disconnect.

Conclusion: Probing alone is insufficient for understanding LLMs' syntactic behaviors; alternative methods are needed.

Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when
processing and generating text. While this suggests internalized understanding
of hierarchical syntax and dependency relations, the precise mechanism by which
they represent syntactic structure is an open area within interpretability
research. Probing provides one way to identify the mechanism of syntax being
linearly encoded in activations, however, no comprehensive study has yet
established whether a model's probing accuracy reliably predicts its downstream
syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we
evaluate 32 open-weight transformer models and find that syntactic features
extracted via probing fail to predict outcomes of targeted syntax evaluations
across English linguistic phenomena. Our results highlight a substantial
disconnect between latent syntactic representations found via probing and
observable syntactic behaviors in downstream tasks.

</details>


### [55] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
*Hyunsoo Yun,Eun Hak Lee*

Main category: cs.CL

TL;DR: LegiGPT combines LLM and XAI to analyze transportation bills, revealing how political ideologies and sponsor traits influence South Korea's legislative outcomes.


<details>
  <summary>Details</summary>
Motivation: Understanding the impact of lawmakers' political ideologies on transportation policymaking is crucial for legislative dynamics and future policy development.

Method: LegiGPT uses GPT-4 for zero-shot classification of bills and XAI to analyze relationships between party affiliation and attributes.

Result: Conservative and progressive sponsors, district size, and electoral population significantly shape legislative outcomes.

Conclusion: LegiGPT offers insights into bipartisan engagement and aids infrastructure planning, with broader governance implications.

Abstract: Given the significant influence of lawmakers' political ideologies on
legislative decision-making, understanding their impact on policymaking is
critically important. We introduce a novel framework, LegiGPT, which integrates
a large language model (LLM) with explainable artificial intelligence (XAI) to
analyze transportation-related legislative proposals. LegiGPT employs a
multi-stage filtering and classification pipeline using zero-shot prompting
with GPT-4. Using legislative data from South Korea's 21st National Assembly,
we identify key factors - including sponsor characteristics, political
affiliations, and geographic variables - that significantly influence
transportation policymaking. The LLM was used to classify
transportation-related bill proposals through a stepwise filtering process
based on keywords, phrases, and contextual relevance. XAI techniques were then
applied to examine relationships between party affiliation and associated
attributes. The results reveal that the number and proportion of conservative
and progressive sponsors, along with district size and electoral population,
are critical determinants shaping legislative outcomes. These findings suggest
that both parties contributed to bipartisan legislation through different forms
of engagement, such as initiating or supporting proposals. This integrated
approach provides a valuable tool for understanding legislative dynamics and
guiding future policy development, with broader implications for infrastructure
planning and governance.

</details>


### [56] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
*Bin Chen,Xinzge Gao,Chuanrui Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: ReasonGRM is a three-stage framework improving Generative Reward Models (GRMs) by enhancing reasoning paths, reducing hallucinations, and refining preference discrimination, outperforming prior GRMs and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: GRMs lack robust reasoning, leading to incomplete or speculative paths and hallucinations. ReasonGRM addresses this to improve preference modeling.

Method: Three-stage framework: 1) Zero-RL for concise reasoning paths, 2) $R^\star$ metric to score paths, 3) RL refinement on challenging examples.

Result: Outperforms prior GRMs by 1.8% and GPT-4o by up to 5.6% on benchmarks.

Conclusion: ReasonGRM demonstrates the value of reasoning-aware training and high-quality rationale selection for reliable preference modeling.

Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

</details>


### [57] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
*Xinyi Liu,Weiguang Wang,Hangfeng He*

Main category: cs.CL

TL;DR: The paper explores how bias affects epistemic and aleatoric uncertainty in LLMs like GPT-4o and Qwen2-VL, finding that bias mitigation improves uncertainty quantification, especially when model confidence is low.


<details>
  <summary>Details</summary>
Motivation: Accurately assessing epistemic uncertainty in LLMs is crucial for reliable outcomes, but it's complicated by aleatoric uncertainty and bias. This study investigates the trade-off between bias and uncertainty quantification.

Method: Experiments on Visual Question Answering (VQA) tasks with GPT-4o and Qwen2-VL, analyzing how prompt biases affect epistemic and aleatoric uncertainty at varying confidence levels.

Result: Bias mitigation improves uncertainty quantification, with greater effects at lower confidence levels. Bias leads to underestimation of epistemic uncertainty (overconfidence) but doesn't significantly alter aleatoric uncertainty estimation.

Conclusion: The findings enhance understanding of bias's role in uncertainty quantification, offering insights for developing advanced techniques to mitigate bias and improve LLM reliability.

Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended
tasks, accurately assessing epistemic uncertainty, which reflects a model's
lack of knowledge, has become crucial to ensuring reliable outcomes. However,
quantifying epistemic uncertainty in such tasks is challenging due to the
presence of aleatoric uncertainty, which arises from multiple valid answers.
While bias can introduce noise into epistemic uncertainty estimation, it may
also reduce noise from aleatoric uncertainty. To investigate this trade-off, we
conduct experiments on Visual Question Answering (VQA) tasks and find that
mitigating prompt-introduced bias improves uncertainty quantification in
GPT-4o. Building on prior work showing that LLMs tend to copy input information
when model confidence is low, we further analyze how these prompt biases affect
measured epistemic and aleatoric uncertainty across varying bias-free
confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases
induce greater changes in both uncertainties when bias-free model confidence is
lower. Moreover, lower bias-free model confidence leads to greater
underestimation of epistemic uncertainty (i.e. overconfidence) due to bias,
whereas it has no significant effect on the direction of changes in aleatoric
uncertainty estimation. These distinct effects deepen our understanding of bias
mitigation for uncertainty quantification and potentially inform the
development of more advanced techniques.

</details>


### [58] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
*Daejin Jo,Jeeyoung Yun,Byungseok Roh,Sungwoong Kim*

Main category: cs.CL

TL;DR: LM-SPT introduces a novel speech tokenization method with semantic distillation to align speech tokens with language models, improving efficiency and performance in speech-language tasks.


<details>
  <summary>Details</summary>
Motivation: Existing speech tokenization methods produce long sequences and struggle to align with language models due to acoustic redundancy and semantic misalignment.

Method: LM-SPT uses semantic distillation by reconstructing speech from semantic tokens and minimizing discrepancy with original waveforms via a frozen ASR encoder. It also includes architectural improvements and supports multiple frame rates.

Result: LM-SPT achieves superior reconstruction fidelity and outperforms baselines in speech-to-text and text-to-speech tasks.

Conclusion: LM-SPT effectively addresses the challenges of speech tokenization, offering better alignment with language models and improved performance in multimodal tasks.

Abstract: With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

</details>


### [59] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
*Lance Ying,Ryan Truong,Katherine M. Collins,Cedegao E. Zhang,Megan Wei,Tyler Brooke-Wilson,Tan Zhi-Xuan,Lionel Wong,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: LIRAS integrates linguistic and visual inputs for social reasoning, outperforming state-of-the-art models in capturing human judgments.


<details>
  <summary>Details</summary>
Motivation: Social inferences require multimodal inputs, with language providing abstract and concrete details not easily observed visually.

Method: LIRAS combines multimodal language models for parsing inputs into symbolic representations and a Bayesian inverse planning engine for probabilistic judgments.

Result: LIRAS outperforms ablations and state-of-the-art models on social reasoning tasks, matching human judgments.

Conclusion: LIRAS effectively integrates language and vision for context-specific social reasoning, demonstrating superior performance.

Abstract: Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

</details>


### [60] [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
*Zhuang Chen,Yaru Cao,Guanqun Bi,Jincenzi Wu,Jinfeng Zhou,Xiyao Xiao,Si Chen,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: SocialSim is a framework for simulating emotional support conversations by integrating social disclosure and awareness, outperforming crowdsourced data in quality.


<details>
  <summary>Details</summary>
Motivation: High costs of crowdsourcing ESC data and the oversight of social dynamics in existing methods necessitate a more effective simulation approach.

Method: SocialSim uses a persona bank for social disclosure and cognitive reasoning for social awareness, creating the SSConv corpus.

Result: SSConv surpasses crowdsourced ESC data in quality, and a chatbot trained on it achieves state-of-the-art performance.

Conclusion: SocialSim provides a scalable solution for synthesizing high-quality ESC, enhancing accessibility to emotional care.

Abstract: Emotional support conversation (ESC) helps reduce people's psychological
stress and provide emotional value through interactive dialogues. Due to the
high cost of crowdsourcing a large ESC corpus, recent attempts use large
language models for dialogue augmentation. However, existing approaches largely
overlook the social dynamics inherent in ESC, leading to less effective
simulations. In this paper, we introduce SocialSim, a novel framework that
simulates ESC by integrating key aspects of social interactions: social
disclosure and social awareness. On the seeker side, we facilitate social
disclosure by constructing a comprehensive persona bank that captures diverse
and authentic help-seeking scenarios. On the supporter side, we enhance social
awareness by eliciting cognitive reasoning to generate logical and supportive
responses. Building upon SocialSim, we construct SSConv, a large-scale
synthetic ESC corpus of which quality can even surpass crowdsourced ESC data.
We further train a chatbot on SSConv and demonstrate its state-of-the-art
performance in both automatic and human evaluations. We believe SocialSim
offers a scalable way to synthesize ESC, making emotional care more accessible
and practical.

</details>


### [61] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
*Lei Jiang,Zixun Zhang,Zizhou Wang,Xiaobing Sun,Zhen Li,Liangli Zhen,Xiaohua Xu*

Main category: cs.CL

TL;DR: CAMO is a stealthy and efficient black-box jailbreak attack framework for LVLMs, using cross-modal reasoning to bypass safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods are detectable and inefficient; CAMO aims to overcome these limitations.

Method: Decomposes malicious prompts into benign visual/textual fragments, leveraging LVLMs' cross-modal reasoning.

Result: CAMO evades detection, requires fewer queries, and shows strong cross-model transferability.

Conclusion: Highlights vulnerabilities in LVLM safety mechanisms, calling for advanced security solutions.

Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

</details>


### [62] [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
*Heloisa Oss Boll,Antonio Oss Boll,Leticia Puttlitz Boll,Ameen Abu Hanna,Iacer Calixto*

Main category: cs.CL

TL;DR: Distillnote is an LLM-based framework for clinical note summarization, achieving high compression and improved predictive performance for heart failure compared to original notes. Clinicians prefer one-step summaries for relevance, while distilled summaries excel in efficiency and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: To alleviate the burden of clinical documentation by generating concise summaries of patient information using LLMs.

Method: Three techniques: one-step summarization, structured summarization (divide-and-conquer), and distilled summarization (further condensing structured summaries).

Result: Distilled summaries achieve 79% text compression and 18.2% improvement in AUPRC for heart failure prediction. Clinicians favor one-step summaries for relevance, while distilled summaries are efficient and reduce hallucinations.

Conclusion: Distillnote demonstrates the potential of LLMs in clinical note summarization, balancing efficiency and quality, with summaries released for future research.

Abstract: Large language models (LLMs) offer unprecedented opportunities to generate
concise summaries of patient information and alleviate the burden of clinical
documentation that overwhelms healthcare providers. We present Distillnote, a
framework for LLM-based clinical note summarization, and generate over 64,000
admission note summaries through three techniques: (1) One-step, direct
summarization, and a divide-and-conquer approach involving (2) Structured
summarization focused on independent clinical insights, and (3) Distilled
summarization that further condenses the Structured summaries. We test how
useful are the summaries by using them to predict heart failure compared to a
model trained on the original notes. Distilled summaries achieve 79% text
compression and up to 18.2% improvement in AUPRC compared to an LLM trained on
the full notes. We also evaluate the quality of the generated summaries in an
LLM-as-judge evaluation as well as through blinded pairwise comparisons with
clinicians. Evaluations indicate that one-step summaries are favoured by
clinicians according to relevance and clinical actionability, while distilled
summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)
and significantly reduce hallucinations. We release our summaries on PhysioNet
to encourage future research.

</details>


### [63] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
*Muyang Zheng,Yuanzhi Yao,Changting Lin,Rui Wang,Meng Han*

Main category: cs.CL

TL;DR: MIST is a method for jailbreaking black-box LLMs via iterative semantic tuning, achieving high success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs remain vulnerable to jailbreak attacks despite alignment efforts, posing challenges due to discrete inputs, restricted access, and limited queries.

Method: MIST uses iterative semantic tuning with sequential synonym search and order-determining optimization to refine prompts for harmful content.

Result: MIST achieves competitive attack success rates and transferability across multiple models, validated by efficiency experiments.

Conclusion: MIST is a practical and effective solution for jailbreaking black-box LLMs, outperforming existing methods.

Abstract: Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

</details>


### [64] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: The paper investigates how language models of varying architectures and sizes perform on learning and recalling facts of different frequencies, highlighting differences in handling rare information.


<details>
  <summary>Details</summary>
Motivation: Sample efficiency is critical for training language models, especially given the long-tailed distribution of information in real-world text. The study aims to understand how models handle rare facts.

Method: Multiple models of different architectures and sizes were trained on the same data. Relational facts were annotated by frequency, and model performance was analyzed across frequency ranges.

Result: Models perform similarly on high-frequency facts but differ significantly on low-frequency facts, revealing variability in learning efficiency for rare information.

Conclusion: The study offers insights into how model architecture and size influence factual learning efficiency, particularly for rare facts.

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [65] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: The paper introduces a Language Bottleneck Model (LBM) for Knowledge Tracing (KT), using interpretable natural-language summaries to improve accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: Traditional KT methods lack interpretability, and LLM-based approaches may produce unreliable predictions. The goal is to create a method that ensures accuracy while being human-interpretable.

Method: LBM uses an encoder LLM to generate interpretable summaries and a frozen decoder LLM to predict student responses. Training involves group-relative policy optimization.

Result: LBMs match state-of-the-art KT and LLM methods in accuracy but require fewer student trajectories.

Conclusion: LBMs offer a promising balance of accuracy and interpretability for KT, validated on synthetic and real-world datasets.

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [66] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: TeXpert is a benchmark dataset for evaluating LLMs' ability to generate LaTeX code from natural language prompts, revealing performance gaps and common errors.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack evaluation of LLMs' ability to generate LaTeX for scientific documents, despite its importance.

Method: Introduces TeXpert, a dataset with natural language prompts for LaTeX generation across difficulty levels, and evaluates LLM performance.

Result: LLMs perform poorly in LaTeX tasks, with accuracy dropping as complexity increases; open-source models rival closed-source ones; formatting and package errors are common.

Conclusion: LLMs need more diverse LaTeX training data to improve performance, and TeXpert provides a foundation for future research.

Abstract: LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [67] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
*Mikhail Menschikov,Dmitry Evseev,Ruslan Kostoev,Ilya Perepechkin,Ilnaz Salimov,Victoria Dochkina,Petr Anokhin,Evgeny Burnaev,Nikita Semenov*

Main category: cs.CL

TL;DR: The paper proposes using knowledge graphs with hyperedges for personalized language models, showing robustness in benchmarks like TriviaQA and DiaASQ.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of personalizing language models by retaining and utilizing extensive personal information.

Method: Utilizing external memory via knowledge graphs (including hyperedges), inspired by AriGraph architecture, and testing on TriviaQA, HotpotQA, and DiaASQ benchmarks.

Result: The approach unified graph construction and knowledge extraction, maintaining robust performance even with added temporal and contradictory dialogue parameters.

Conclusion: The proposed architecture effectively handles temporal dependencies and personalization in language models.

Abstract: The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

</details>


### [68] [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
*Danielle R. Thomas,Conrad Borchers,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: LLM-generated feedback shows moderate learning benefits in tutor training, with effectiveness depending on learners' tendency to seek support. It's scalable and well-received without increasing completion time.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of LLM-generated feedback on learning compared to existing methods, especially in tutor training.

Method: Analyzed 2,600 lesson completions from 885 learners across three groups (LLM feedback, declined, no access) with propensity scoring to address selection bias.

Result: Two out of seven lessons showed significant learning benefits (effect sizes 0.28, 0.33). LLM feedback was rated helpful and didn't increase completion time.

Conclusion: LLM feedback is a scalable, low-cost tool for improving learning in open-ended tasks, particularly in systems already providing feedback.

Abstract: Large language models (LLMs) are increasingly used to generate feedback, yet
their impact on learning remains underexplored, especially compared to existing
feedback methods. This study investigates how on-demand LLM-generated
explanatory feedback influences learning in seven scenario-based tutor training
lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we
compare posttest performance among learners across three groups: learners who
received feedback generated by gpt-3.5-turbo, those who declined it, and those
without access. All groups received non-LLM corrective feedback. To address
potential selection bias-where higher-performing learners may be more inclined
to use LLM feedback-we applied propensity scoring. Learners with a higher
predicted likelihood of engaging with LLM feedback scored significantly higher
at posttest than those with lower propensity. After adjusting for this effect,
two out of seven lessons showed statistically significant learning benefits
from LLM feedback with standardized effect sizes of 0.28 and 0.33. These
moderate effects suggest that the effectiveness of LLM feedback depends on the
learners' tendency to seek support. Importantly, LLM feedback did not
significantly increase completion time, and learners overwhelmingly rated it as
helpful. These findings highlight LLM feedback's potential as a low-cost and
scalable way to improve learning on open-ended tasks, particularly in existing
systems already providing feedback without LLMs. This work contributes open
datasets, LLM prompts, and rubrics to support reproducibility.

</details>


### [69] [Instituto de Telecomunica√ß√µes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
*Giuseppe Attanasio,Sonal Sannigrahi,Ben Peters,Andr√© F. T. Martins*

Main category: cs.CL

TL;DR: The paper describes IT-IST's submission to IWSLT 2025's Shared Task on Instruction Following Speech Processing, focusing on speech recognition, translation, and spoken question answering using a unified speech-to-text model with small-scale language models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of instruction following in speech processing tasks with limited resources, leveraging small-scale models and high-quality data.

Method: A unified speech-to-text model combining a pre-trained speech encoder and text decoder, trained in two phases: modality alignment and instruction fine-tuning. Uses small-scale language models (<2B) and high-quality CC-BY data, supplemented by synthetic data.

Result: Submitted results for the Short Track (speech recognition, translation, and spoken question answering) at IWSLT 2025.

Conclusion: The approach demonstrates the feasibility of effective instruction following in speech processing with constrained resources and high-quality data.

Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on
Instruction Following Speech Processing. We submit results for the Short Track,
i.e., speech recognition, translation, and spoken question answering. Our model
is a unified speech-to-text model that integrates a pre-trained continuous
speech encoder and text decoder through a first phase of modality alignment and
a second phase of instruction fine-tuning. Crucially, we focus on using
small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY
data along with synthetic data generation to supplement existing resources.

</details>


### [70] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: MUCAR is a new benchmark for evaluating multimodal ambiguity resolution in multilingual and cross-modal scenarios, revealing gaps in current models compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook linguistic and visual ambiguities, failing to leverage cross-modal clarification. MUCAR addresses this gap.

Method: MUCAR includes a multilingual dataset and a dual-ambiguity dataset, pairing ambiguous images and texts for mutual disambiguation.

Result: Tests on 19 state-of-the-art models show significant performance gaps versus humans.

Conclusion: Future research needs to improve cross-modal ambiguity comprehension to advance multimodal reasoning.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


### [71] [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
*Dominik Mach√°ƒçek,Peter Pol√°k*

Main category: cs.CL

TL;DR: Charles University's IWSLT 2025 submission uses Whisper and AlignAtt for simultaneous speech translation, improving BLEU scores and proposing a new latency measure.


<details>
  <summary>Details</summary>
Motivation: To enhance simultaneous speech translation performance across multiple language pairs using advanced models and policies.

Method: Utilizes Whisper for translation/transcription with AlignAtt policy, in-domain prompting, and EuroLLM for cascaded systems.

Result: BLEU improvements: +2 (Czech-English), +13-22 (English-German/Chinese/Japanese). New latency measure introduced.

Conclusion: The approach significantly outperforms baselines and introduces a novel latency metric.

Abstract: This paper describes Charles University submission to the Simultaneous Speech
Translation Task of the IWSLT 2025. We cover all four language pairs with a
direct or cascade approach. The backbone of our systems is the offline Whisper
speech model, which we use for both translation and transcription in
simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We
further improve the performance by prompting to inject in-domain terminology,
and we accommodate context. Our cascaded systems further use EuroLLM for
unbounded simultaneous translation. Compared to the Organizers' baseline, our
systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on
English to German, Chinese and Japanese on the development sets. Additionally,
we also propose a new enhanced measure of speech recognition latency.

</details>


### [72] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
*Ricardo Rei,Nuno M. Guerreiro,Jos√© Pombal,Jo√£o Alves,Pedro Teixeirinha,Amin Farajian,Andr√© F. T. Martins*

Main category: cs.CL

TL;DR: Tower+ is a suite of models balancing translation specialization and multilingual general-purpose capabilities, outperforming larger models like Llama 3.3 70B and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between fine-tuning for specific tasks (e.g., translation) and retaining general-purpose capabilities in LLMs.

Method: A novel training recipe involving continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards.

Result: Smaller Tower+ models outperform larger general-purpose LLMs, and the largest model achieves top translation and multilingual task performance.

Conclusion: It's possible to optimize for specific domains (e.g., translation) while rivaling frontier models in general capabilities.

Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for
reaching state-of-the-art performance on specific tasks like machine
translation. However, this process of adaptation often implies sacrificing
general-purpose capabilities, such as conversational reasoning and
instruction-following, hampering the utility of the system in real-world
applications that require a mixture of skills. In this paper, we introduce
Tower+, a suite of models designed to deliver strong performance across both
translation and multilingual general-purpose text capabilities. We achieve a
Pareto frontier between translation specialization and multilingual
general-purpose capabilities by introducing a novel training recipe that builds
on Tower (Alves et al., 2024), comprising continued pretraining, supervised
fine-tuning, preference optimization, and reinforcement learning with
verifiable rewards. At each stage of training, we carefully generate and curate
data to strengthen performance on translation as well as general-purpose tasks
involving code generation, mathematics problem solving, and general
instruction-following. We develop models at multiple scales: 2B, 9B, and 72B.
Our smaller models often outperform larger general-purpose open-weight and
proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers
best-in-class translation performance for high-resource languages and top
results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we
introduce for evaluating both translation and instruction-following. Our
findings highlight that it is possible to rival frontier models in general
capabilities, while optimizing for specific business domains, such as
translation and localization.

</details>


### [73] [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
*Jiahao Cheng,Tiancheng Su,Jia Yuan,Guoxiu He,Jiawei Liu,Xinqi Tao,Jingwen Xie,Huaxia Li*

Main category: cs.CL

TL;DR: CoT prompting reduces LLM hallucinations but weakens detection methods by obscuring key signals.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored impact of CoT prompting on hallucination detection in LLMs.

Method: Systematic empirical evaluation, including pilot experiments and analysis of CoT's effects on hallucination detection across LLMs.

Result: CoT reduces hallucination frequency but impairs detection accuracy and confidence by masking critical signals.

Conclusion: CoT introduces a trade-off between reducing hallucinations and maintaining effective detection.

Abstract: Large Language Models (LLMs) often exhibit \textit{hallucinations},
generating factually incorrect or semantically irrelevant content in response
to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by
encouraging step-by-step reasoning, but its impact on hallucination detection
remains underexplored. To bridge this gap, we conduct a systematic empirical
evaluation. We begin with a pilot experiment, revealing that CoT reasoning
significantly affects the LLM's internal states and token probability
distributions. Building on this, we evaluate the impact of various CoT
prompting methods on mainstream hallucination detection methods across both
instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three
key dimensions: changes in hallucination score distributions, variations in
detection accuracy, and shifts in detection confidence. Our findings show that
while CoT prompting helps reduce hallucination frequency, it also tends to
obscure critical signals used for detection, impairing the effectiveness of
various detection methods. Our study highlights an overlooked trade-off in the
use of reasoning. Code is publicly available at:
https://anonymous.4open.science/r/cot-hallu-detect.

</details>


### [74] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
*Murtaza Nazir,Matthew Finlayson,John X. Morris,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: PILS method recovers hidden prompts from language model outputs using next-token probabilities, achieving 2--3.5x higher recovery rates than prior methods.


<details>
  <summary>Details</summary>
Motivation: Address security risks like private data leaks in API-protected language models by improving prompt inversion techniques.

Method: Uses next-token probabilities and a linear map to compress and analyze model outputs for hidden prompt recovery.

Result: Achieves 2--3.5x higher exact recovery rates, e.g., from 17% to 60%, and generalizes well to more generation steps.

Conclusion: Next-token probabilities are a vulnerable attack surface, and PILS significantly advances prompt inversion capabilities.

Abstract: Language model inversion seeks to recover hidden prompts using only language
model outputs. This capability has implications for security and accountability
in language model deployments, such as leaking private information from an
API-protected language model's system message. We propose a new method --
prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts
by gleaning clues from the model's next-token probabilities over the course of
multiple generation steps. Our method is enabled by a key insight: The
vector-valued outputs of a language model occupy a low-dimensional subspace.
This enables us to losslessly compress the full next-token probability
distribution over multiple generation steps using a linear map, allowing more
output information to be used for inversion. Our approach yields massive gains
over previous state-of-the-art methods for recovering hidden prompts, achieving
2--3.5 times higher exact recovery rates across test sets, in one case
increasing the recovery rate from 17% to 60%. Our method also exhibits
surprisingly good generalization behavior; for instance, an inverter trained on
16 generations steps gets 5--27 points higher prompt recovery when we increase
the number of steps to 32 at test time. Furthermore, we demonstrate strong
performance of our method on the more challenging task of recovering hidden
system messages. We also analyze the role of verbatim repetition in prompt
recovery and propose a new method for cross-family model transfer for
logit-based inverters. Our findings show that next-token probabilities are a
considerably more vulnerable attack surface for inversion attacks than
previously known.

</details>


### [75] [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
*Adithya Bhaskar,Alexander Wettig,Tianyu Gao,Yihe Dong,Danqi Chen*

Main category: cs.CL

TL;DR: The paper introduces the *KV footprint* metric to evaluate KV cache methods, revealing high peak memory in prior approaches. It adapts post-fill eviction and proposes PruLong for recency eviction, achieving lower footprints without performance loss.


<details>
  <summary>Details</summary>
Motivation: Address the growing memory costs of KV caches in long-context language models, highlighting limitations of prior methods and the need for a unified evaluation metric.

Method: Proposes *KV footprint* as a metric, adapts post-fill eviction for pre-filling, and introduces PruLong for optimized recency eviction.

Result: PruLong achieves a 12% smaller KV footprint than prior methods while maintaining performance in recall tasks.

Conclusion: The paper provides a clear framework for evaluating KV cache methods and advances memory-efficient long-context inference.

Abstract: Language models handle increasingly long contexts for tasks such as book
summarization, but this leads to growing memory costs for the key-value (KV)
cache. Many prior works have proposed ways of discarding KVs from memory, but
their approaches are tailored to favorable settings, obscuring caveats like
high peak memory and performance degradation, and a fair comparison between
methods is difficult. In this paper, we propose the *KV footprint* as a unified
metric, which accounts for both the amount of KV entries stored and their
lifespan in memory. We evaluate methods based on the smallest footprint they
attain while preserving performance in both long-context understanding and
generation, with context lengths of up to 128K tokens. This metric reveals the
high peak memory of prior KV eviction methods. One class of methods --
*post-fill eviction* -- has a high footprint due to being incompatible with
eviction during pre-filling. We adapt these methods to be able to evict KVs
during pre-filling, achieving substantially lower KV footprints. We then turn
to *recency eviction* methods, wherein we propose PruLong, an end-to-end
optimization method for learning which attention heads need to retain the full
KV cache and which do not. PruLong saves memory while preserving long-context
performance, achieving 12% smaller KV footprint than prior methods while
retaining performance in challenging recall tasks. Our paper clarifies the
complex tangle of long-context inference methods and paves the way for future
development to minimize the KV footprint.

</details>


### [76] [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Main category: cs.CL

TL;DR: CLEAR-3K is a dataset of 3,000 assertion-reasoning questions to test language models' ability to distinguish causal explanations from semantic relatedness. Evaluations reveal models confuse similarity with causality, and performance plateaus at 0.55 MCC.


<details>
  <summary>Details</summary>
Motivation: To assess and improve language models' capability in genuine causal reasoning, crucial for applications requiring accurate causal relationship evaluation.

Method: Created CLEAR-3K dataset and evaluated 21 state-of-the-art language models (0.5B to 72B parameters) on their ability to distinguish causal relationships.

Result: Models confuse semantic similarity with causality; performance plateaus at 0.55 MCC, with larger models shifting from skepticism to permissiveness.

Conclusion: CLEAR-3K serves as a vital benchmark for advancing causal reasoning in language models, highlighting current limitations.

Abstract: We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions
designed to evaluate whether language models can determine if one statement
causally explains another. Each question present an assertion-reason pair and
challenge language models to distinguish between semantic relatedness and
genuine causal explanatory relationships. Through comprehensive evaluation of
21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we
identify two fundamental findings. First, language models frequently confuse
semantic similarity with causality, relying on lexical and semantic overlap
instead of inferring actual causal explanatory relationships. Second, as
parameter size increases, models tend to shift from being overly skeptical
about causal relationships to being excessively permissive in accepting them.
Despite this shift, performance measured by the Matthews Correlation
Coefficient plateaus at just 0.55, even for the best-performing models.Hence,
CLEAR-3K provides a crucial benchmark for developing and evaluating genuine
causal reasoning in language models, which is an essential capability for
applications that require accurate assessment of causal relationships.

</details>


### [77] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
*Yuchen Li,Hengyi Cai,Rui Kong,Xinran Chen,Jiamin Chen,Jun Yang,Haojie Zhang,Jiayi Li,Jiayi Wu,Yiqun Chen,Changle Qu,Keyi Kong,Wenwen Ye,Lixin Su,Xinyu Ma,Long Xia,Daiting Shi,Jiashu Zhao,Haoyi Xiong,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: The paper introduces the AI Search Paradigm, a modular system using four LLM-powered agents to handle diverse search tasks through dynamic collaboration and workflow coordination.


<details>
  <summary>Details</summary>
Motivation: To develop next-generation search systems that emulate human information processing and decision-making, addressing a wide range of query complexities.

Method: Employs a modular architecture with four LLM agents (Master, Planner, Executor, Writer) for task decomposition, planning, execution, and synthesis, supported by algorithmic and infrastructure optimizations.

Result: A comprehensive framework for adaptive, scalable, and trustworthy AI search systems, integrating task planning, tool usage, and retrieval-augmented generation.

Conclusion: The work provides foundational methodologies for building advanced AI search systems, emphasizing adaptability, robustness, and efficiency.

Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

</details>


### [78] [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
*Kathleen C. Fraser,Hillary Dawkins,Isar Nejadgholi,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: Fine-tuning LLMs can unintentionally remove safety features, posing risks even with benign data. Safety evaluations show high variance, highlighting the need for standardized reporting.


<details>
  <summary>Details</summary>
Motivation: To address the unintended removal of safety features in LLMs during fine-tuning and the lack of reliable safety evaluations.

Method: Investigates the robustness of safety benchmarks by testing trivial variations in fine-tuning setups and evaluating stochastic LLM behavior.

Result: High variance in safety evaluation results due to minor changes in fine-tuning, indicating unreliable benchmarks.

Conclusion: Standardized reporting is crucial for meaningful comparisons and mitigating safety risks in fine-tuned LLMs.

Abstract: Fine-tuning a general-purpose large language model (LLM) for a specific
domain or task has become a routine procedure for ordinary users. However,
fine-tuning is known to remove the safety alignment features of the model, even
when the fine-tuning data does not contain any harmful content. We consider
this to be a critical failure mode of LLMs due to the widespread uptake of
fine-tuning, combined with the benign nature of the "attack". Most
well-intentioned developers are likely unaware that they are deploying an LLM
with reduced safety. On the other hand, this known vulnerability can be easily
exploited by malicious actors intending to bypass safety guardrails. To make
any meaningful progress in mitigating this issue, we first need reliable and
reproducible safety evaluations. In this work, we investigate how robust a
safety benchmark is to trivial variations in the experimental procedure, and
the stochastic nature of LLMs. Our initial experiments expose surprising
variance in the results of the safety evaluation, even when seemingly
inconsequential changes are made to the fine-tuning setup. Our observations
have serious implications for how researchers in this field should report
results to enable meaningful comparisons in the future.

</details>


### [79] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles, improving caption quality by leveraging context from related figures.


<details>
  <summary>Details</summary>
Motivation: Existing caption generation models lack personalization for authors' styles and domain-specific needs, especially in multimodal settings.

Method: LaMP-Cap provides multimodal profiles (images, captions, and paragraphs) for figures, tested with four LLMs to assess caption quality.

Result: Using profile information improves caption alignment with author-written ones, with images proving more helpful than text.

Conclusion: Multimodal profiles enhance personalized caption generation, outperforming text-only approaches.

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [80] [A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion](https://arxiv.org/abs/2506.15747)
*Fangzhou Lin,Zilin Dai,Rigved Sanku,Songlin Hou,Kazunori D Yamada,Haichong K. Zhang,Ziming Zhang*

Main category: cs.CV

TL;DR: A view-free, attention-based multi-branch encoder-decoder network is proposed for single-view image guided point cloud completion, outperforming state-of-the-art methods without relying on image guidance.


<details>
  <summary>Details</summary>
Motivation: To investigate the necessity of image guidance in single-view image guided point cloud completion (SVIPC) and propose a strong baseline method.

Method: An attention-based multi-branch encoder-decoder network with hierarchical self-fusion (cross-attention and self-attention layers) for integrating information from partial point clouds.

Result: The view-free framework outperforms existing SVIPC methods on the ShapeNet-ViPC dataset.

Conclusion: The findings challenge the necessity of image guidance in SVIPC and offer insights for multimodal learning advancements.

Abstract: The single-view image guided point cloud completion (SVIPC) task aims to
reconstruct a complete point cloud from a partial input with the help of a
single-view image. While previous works have demonstrated the effectiveness of
this multimodal approach, the fundamental necessity of image guidance remains
largely unexamined. To explore this, we propose a strong baseline approach for
SVIPC based on an attention-based multi-branch encoder-decoder network that
only takes partial point clouds as input, view-free. Our hierarchical
self-fusion mechanism, driven by cross-attention and self-attention layers,
effectively integrates information across multiple streams, enriching feature
representations and strengthening the networks ability to capture geometric
structures. Extensive experiments and ablation studies on the ShapeNet-ViPC
dataset demonstrate that our view-free framework performs superiorly to
state-of-the-art SVIPC methods. We hope our findings provide new insights into
the development of multimodal learning in SVIPC. Our demo code will be
available at https://github.com/Zhang-VISLab.

</details>


### [81] [VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service](https://arxiv.org/abs/2506.15755)
*Xiasi Wang,Tianliang Yao,Simin Chen,Runqi Wang,Lei YE,Kuofeng Gao,Yi Huang,Yuan Yao*

Main category: cs.CV

TL;DR: VLMInferSlow evaluates VLM efficiency robustness in a black-box setting, revealing vulnerabilities to adversarial examples that increase computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing VLM research overlooks efficiency robustness, which is critical for real-time applications and ML-as-a-service deployments.

Method: Proposes VLMInferSlow, using fine-grained efficiency modeling and zero-order optimization to find adversarial examples.

Result: Generates adversarial images with imperceptible perturbations, increasing computational cost by up to 128.47%.

Conclusion: Highlights the need for awareness and research on VLM efficiency robustness.

Abstract: Vision-Language Models (VLMs) have demonstrated great potential in real-world
applications. While existing research primarily focuses on improving their
accuracy, the efficiency remains underexplored. Given the real-time demands of
many applications and the high inference overhead of VLMs, efficiency
robustness is a critical issue. However, previous studies evaluate efficiency
robustness under unrealistic assumptions, requiring access to the model
architecture and parameters -- an impractical scenario in ML-as-a-service
settings, where VLMs are deployed via inference APIs. To address this gap, we
propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness
in a realistic black-box setting. VLMInferSlow incorporates fine-grained
efficiency modeling tailored to VLM inference and leverages zero-order
optimization to search for adversarial examples. Experimental results show that
VLMInferSlow generates adversarial images with imperceptible perturbations,
increasing the computational cost by up to 128.47%. We hope this research
raises the community's awareness about the efficiency robustness of VLMs.

</details>


### [82] [Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation](https://arxiv.org/abs/2506.15757)
*Ruoyu Wang,Tong Yu,Junda Wu,Yao Liu,Julian McAuley,Lina Yao*

Main category: cs.CV

TL;DR: Proposes Weakly-supervised Partial Contrastive Learning (WPCL) to improve Visual Language Navigation (VLN) by integrating pre-trained VLM knowledge without fine-tuning, addressing dynamic viewpoint and computational cost challenges.


<details>
  <summary>Details</summary>
Motivation: Existing VLN methods struggle with dynamic viewpoints, lack domain knowledge in pre-trained models, and face high computational costs when fine-tuning.

Method: WPCL integrates pre-trained VLM knowledge into perception without fine-tuning, enhancing object identification from dynamic viewpoints.

Result: Outperforms baseline methods on benchmarks, showing effectiveness, robustness, and generalizability.

Conclusion: WPCL offers a computationally efficient solution to improve VLN performance without fine-tuning pre-trained models.

Abstract: Visual Language Navigation (VLN) is a fundamental task within the field of
Embodied AI, focusing on the ability of agents to navigate complex environments
based on natural language instructions. Despite the progress made by existing
methods, these methods often present some common challenges. First, they rely
on pre-trained backbone models for visual perception, which struggle with the
dynamic viewpoints in VLN scenarios. Second, the performance is limited when
using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN
domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results,
their computational costs are higher than those without fine-tuning. To address
these limitations, we propose Weakly-supervised Partial Contrastive Learning
(WPCL), a method that enhances an agent's ability to identify objects from
dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM
knowledge into the perception process, without requiring VLM fine-tuning. Our
method enhances the agent's ability to interpret and respond to environmental
cues while ensuring computational efficiency. Experimental results have shown
that our method outperforms the baseline methods on multiple benchmarks, which
validate the effectiveness, robustness and generalizability of our method.

</details>


### [83] [Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving](https://arxiv.org/abs/2506.15806)
*Akarshani Ramanayake,Nihal Kodikara*

Main category: cs.CV

TL;DR: A learning-based 3D scene reconstruction method using LiDAR and deep neural networks improves boundary-level accuracy for autonomous vehicles in dense traffic.


<details>
  <summary>Details</summary>
Motivation: Current technologies lack precise 3D scene reconstruction for dense urban traffic, necessitating better boundary-level accuracy for safe navigation.

Method: Uses LiDAR data and a deep neural network to create static Signed Distance Function (SDF) maps, replacing traditional polygonal representations.

Result: Preliminary results show enhanced collision detection in congested, dynamic environments.

Conclusion: The proposed SDF-based method offers a promising solution for detailed 3D obstacle mapping in autonomous driving.

Abstract: In crowded urban environments where traffic is dense, current technologies
struggle to oversee tight navigation, but surface-level understanding allows
autonomous vehicles to safely assess proximity to surrounding obstacles. 3D or
2D scene mapping of the surrounding objects is an essential task in addressing
the above problem. Despite its importance in dense vehicle traffic conditions,
3D scene reconstruction of object shapes with higher boundary level accuracy is
not yet entirely considered in current literature. The sign distance function
represents any shape through parameters that calculate the distance from any
point in space to the closest obstacle surface, making it more efficient in
terms of storage. In recent studies, researchers have started to formulate
problems with Implicit 3D reconstruction methods in the autonomous driving
domain, highlighting the possibility of using sign distance function to map
obstacles effectively. This research addresses this gap by developing a
learning-based 3D scene reconstruction methodology that leverages LiDAR data
and a deep neural network to build a the static Signed Distance Function (SDF)
maps. Unlike traditional polygonal representations, this approach has the
potential to map 3D obstacle shapes with more boundary-level details. Our
preliminary results demonstrate that this method would significantly enhance
collision detection performance, particularly in congested and dynamic
environments.

</details>


### [84] [ADAM-Dehaze: Adaptive Density-Aware Multi-Stage Dehazing for Improved Object Detection in Foggy Conditions](https://arxiv.org/abs/2506.15837)
*Fatmah AlHindaassi,Mohammed Talha Alam,Fakhri Karray*

Main category: cs.CV

TL;DR: ADAM-Dehaze is an adaptive dehazing framework that improves image restoration and object detection under varying fog intensities, outperforming benchmarks in PSNR, FADE, and mAP while reducing inference time.


<details>
  <summary>Details</summary>
Motivation: Fog degrades visual information, posing challenges for autonomous vehicles and surveillance systems.

Method: Uses a Haze Density Estimation Network (HDEN) to classify fog intensity and routes images through tailored CORUN branches (Light, Medium, Complex) with an adaptive loss for balance.

Result: Improves PSNR by 2.1 dB, reduces FADE by 30%, increases mAP by 13 points, and cuts inference time by 20%.

Conclusion: Intensity-specific processing and integration with vision tasks are crucial for effective dehazing.

Abstract: Adverse weather conditions, particularly fog, pose a significant challenge to
autonomous vehicles, surveillance systems, and other safety-critical
applications by severely degrading visual information. We introduce
ADAM-Dehaze, an adaptive, density-aware dehazing framework that jointly
optimizes image restoration and object detection under varying fog intensities.
A lightweight Haze Density Estimation Network (HDEN) classifies each input as
light, medium, or heavy fog. Based on this score, the system dynamically routes
the image through one of three CORUN branches: Light, Medium, or Complex, each
tailored to its haze regime. A novel adaptive loss balances physical-model
coherence and perceptual fidelity, ensuring both accurate defogging and
preservation of fine details. On Cityscapes and the real-world RTTS benchmark,
ADAM-Dehaze improves PSNR by up to 2.1 dB, reduces FADE by 30 percent, and
increases object detection mAP by up to 13 points, while cutting inference time
by 20 percent. These results highlight the importance of intensity-specific
processing and seamless integration with downstream vision tasks. Code
available at: https://github.com/talha-alam/ADAM-Dehaze.

</details>


### [85] [EchoShot: Multi-Shot Portrait Video Generation](https://arxiv.org/abs/2506.15838)
*Jiahao Wang,Hualian Sheng,Sijia Cai,Weizhan Zhang,Caixia Yan,Yachuang Feng,Bing Deng,Jieping Ye*

Main category: cs.CV

TL;DR: EchoShot is a multi-shot framework for portrait video generation, ensuring identity consistency and content control, built on a video diffusion model.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require multi-shot video generation with identity consistency and flexible control, which existing single-shot pipelines lack.

Method: Proposes shot-aware position embedding in a video diffusion transformer and trains on the PortraitGala dataset for multi-shot consistency and controllability.

Result: Achieves superior identity consistency and attribute-level controllability in multi-shot portrait video generation.

Conclusion: EchoShot is a scalable and effective framework for multi-shot video modeling, with potential as a foundational paradigm.

Abstract: Video diffusion models substantially boost the productivity of artistic
workflows with high-quality portrait video generative capacity. However,
prevailing pipelines are primarily constrained to single-shot creation, while
real-world applications urge for multiple shots with identity consistency and
flexible content controllability. In this work, we propose EchoShot, a native
and scalable multi-shot framework for portrait customization built upon a
foundation video diffusion model. To start with, we propose shot-aware position
embedding mechanisms within video diffusion transformer architecture to model
inter-shot variations and establish intricate correspondence between multi-shot
visual content and their textual descriptions. This simple yet effective design
enables direct training on multi-shot video data without introducing additional
computational overhead. To facilitate model training within multi-shot
scenario, we construct PortraitGala, a large-scale and high-fidelity
human-centric video dataset featuring cross-shot identity consistency and
fine-grained captions such as facial attributes, outfits, and dynamic motions.
To further enhance applicability, we extend EchoShot to perform reference
image-based personalized multi-shot generation and long video synthesis with
infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves
superior identity consistency as well as attribute-level controllability in
multi-shot portrait video generation. Notably, the proposed framework
demonstrates potential as a foundational paradigm for general multi-shot video
modeling.

</details>


### [86] [Assessing the impact of Binarization for Writer Identification in Greek Papyrus](https://arxiv.org/abs/2506.15852)
*Dominic Akt,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: The paper evaluates binarization methods for Greek papyri, comparing traditional and DL approaches, and assesses their impact on writer identification performance.


<details>
  <summary>Details</summary>
Motivation: Writer identification in historical Greek papyri is challenging due to non-uniform, fragmented backgrounds. Binarization is crucial to avoid learning irrelevant background features.

Method: Traditional and DL binarization methods are compared, with DL models trained using custom data augmentation and different selection criteria. Performance is evaluated on DIBCO 2019 and its impact on writer identification is assessed.

Result: Data augmentation significantly improves DL methods. Binarization effectiveness on DIBCO 2019 strongly correlates with writer identification performance.

Conclusion: Binarization quality, especially with DL and augmentation, critically impacts writer identification in historical documents.

Abstract: This paper tackles the task of writer identification for Greek papyri. A
common preprocessing step in writer identification pipelines is image
binarization, which prevents the model from learning background features. This
is challenging in historical documents, in our case Greek papyri, as background
is often non-uniform, fragmented, and discolored with visible fiber structures.
We compare traditional binarization methods to state-of-the-art Deep Learning
(DL) models, evaluating the impact of binarization quality on subsequent writer
identification performance. DL models are trained with and without a custom
data augmentation technique, as well as different model selection criteria are
applied. The performance of these binarization methods, is then systematically
evaluated on the DIBCO 2019 dataset. The impact of binarization on writer
identification is subsequently evaluated using a state-of-the-art approach for
writer identification. The results of this analysis highlight the influence of
data augmentation for DL methods. Furthermore, findings indicate a strong
correlation between binarization effectiveness on papyri documents of DIBCO
2019 and downstream writer identification performance.

</details>


### [87] [Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation](https://arxiv.org/abs/2506.15854)
*Abdolazim Rezaei,Mehdi Sookhak,Ahmad Patooghy*

Main category: cs.CV

TL;DR: A novel framework uses reinforcement learning and vision-language models to convert AIE camera images into text, preserving privacy while retaining scene details.


<details>
  <summary>Details</summary>
Motivation: Address privacy risks in CAVs' AIE cameras, where traditional methods like blurring fail to fully protect against misuse of visual data.

Method: Feedback-based reinforcement learning and vision-language models transform images into semantically equivalent text, refined hierarchically for accuracy and privacy.

Result: Improves privacy protection and textual quality, with a 77% increase in Unique Word Count and 50% in Detail Density over existing methods.

Conclusion: The framework effectively balances privacy and information retention, offering a robust solution for AIE camera applications.

Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that
often process privacy-sensitive data. Among these, roadside units play a
critical role particularly through the use of AI-equipped (AIE) cameras for
applications such as violation detection. However, the privacy risks associated
with captured imagery remain a major concern, as such data can be misused for
identity theft, profiling, or unauthorized commercial purposes. While
traditional techniques such as face blurring and obfuscation have been applied
to mitigate privacy risks, individual privacy remains at risk, as individuals
can still be tracked using other features such as their clothing. This paper
introduces a novel privacy-preserving framework that leverages feedback-based
reinforcement learning (RL) and vision-language models (VLMs) to protect
sensitive visual information captured by AIE cameras. The main idea is to
convert images into semantically equivalent textual descriptions, ensuring that
scene-relevant information is retained while visual privacy is preserved. A
hierarchical RL strategy is employed to iteratively refine the generated text,
enhancing both semantic accuracy and privacy. Evaluation results demonstrate
significant improvements in both privacy protection and textual quality, with
the Unique Word Count increasing by approximately 77\% and Detail Density by
around 50\% compared to existing approaches.

</details>


### [88] [Visual symbolic mechanisms: Emergent symbol processing in vision language models](https://arxiv.org/abs/2506.15871)
*Rim Assouel,Declan Campbell,Taylor Webb*

Main category: cs.CV

TL;DR: The paper explores how vision language models (VLMs) solve the 'binding problem' using symbolic, content-independent mechanisms, similar to language models, and links binding errors to failures in these mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand whether VLMs employ symbolic mechanisms for feature binding, given their frequent failures in binding tasks.

Method: Identifies emergent symbolic mechanisms in VLMs that support binding via a spatial indexing scheme and traces binding errors to these mechanisms.

Result: VLMs use content-independent, spatial indexing for binding, and binding errors arise from failures in these mechanisms.

Conclusion: The findings reveal symbolic processing in VLMs and suggest ways to address their binding failures.

Abstract: To accurately process a visual scene, observers must bind features together
to represent individual objects. This capacity is necessary, for instance, to
distinguish an image containing a red square and a blue circle from an image
containing a blue square and a red circle. Recent work has found that language
models solve this 'binding problem' via a set of symbol-like,
content-independent indices, but it is unclear whether similar mechanisms are
employed by vision language models (VLMs). This question is especially
relevant, given the persistent failures of VLMs on tasks that require binding.
Here, we identify a set of emergent symbolic mechanisms that support binding in
VLMs via a content-independent, spatial indexing scheme. Moreover, we find that
binding errors can be traced directly to failures in these mechanisms. Taken
together, these results shed light on the mechanisms that support symbol-like
processing in VLMs, and suggest possible avenues for addressing the persistent
binding failures exhibited by these models.

</details>


### [89] [Pediatric Pancreas Segmentation from MRI Scans with Deep Learning](https://arxiv.org/abs/2506.15908)
*Elif Keles,Merve Yazol,Gorkem Durak,Ziliang Hong,Halil Ertugrul Aktas,Zheyuan Zhang,Linkai Peng,Onkar Susladkar,Necati Guzelyel,Oznur Leman Boyunaga,Cemal Yazici,Mark Lowe,Aliye Uc,Ulas Bagci*

Main category: cs.CV

TL;DR: PanSegNet, a deep learning algorithm, was validated for pediatric pancreas segmentation on MRI, showing high accuracy and reliability in healthy and diseased states.


<details>
  <summary>Details</summary>
Motivation: To develop and validate a reliable, radiation-free tool for pediatric pancreas segmentation on MRI, addressing a gap in accessible imaging solutions for children with pancreatitis.

Method: Retrospective analysis of 84 MRI scans from children (healthy and with AP/CP) using PanSegNet, with manual segmentation by radiologists and evaluation via DSC, HD95, and Cohen's kappa.

Result: PanSegNet achieved DSC scores of 88% (controls), 81% (AP), and 80% (CP), with strong inter- and intra-observer agreement, demonstrating clinical reliability.

Conclusion: PanSegNet is the first validated DL solution for pediatric pancreatic MRI segmentation, offering expert-level performance and open-source availability to advance research.

Abstract: Objective: Our study aimed to evaluate and validate PanSegNet, a deep
learning (DL) algorithm for pediatric pancreas segmentation on MRI in children
with acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls.
Methods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T
Siemens Aera/Verio) from children aged 2-19 years at Gazi University
(2015-2024). The dataset includes healthy children as well as patients
diagnosed with AP or CP based on clinical criteria. Pediatric and general
radiologists manually segmented the pancreas, then confirmed by a senior
pediatric radiologist. PanSegNet-generated segmentations were assessed using
Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance
(HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W
scans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years)
and 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved
DSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98
mm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86
(controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and
0.81. Strong agreement was observed between automated and manual volumes (R^2 =
0.85 in controls, 0.77 in diseased), demonstrating clinical reliability.
Conclusion: PanSegNet represents the first validated deep learning solution for
pancreatic MRI segmentation, achieving expert-level performance across healthy
and diseased states. This tool, algorithm, along with our annotated dataset,
are freely available on GitHub and OSF, advancing accessible, radiation-free
pediatric pancreatic imaging and fostering collaborative research in this
underserved domain.

</details>


### [90] [Moir√©XNet: Adaptive Multi-Scale Demoir√©ing with Linear Attention Test-Time Training and Truncated Flow Matching Prior](https://arxiv.org/abs/2506.15929)
*Liangyan Li,Yimo Ning,Kevin Le,Wei Dong,Yunzhe Li,Jun Chen,Xiaohong Liu*

Main category: cs.CV

TL;DR: A hybrid MAP-based framework combines supervised learning with generative priors for effective image and video demoir√©ing, addressing nonlinear degradation challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with nonlinear moir√© patterns due to limited model capacity and training data, leading to incomplete removal or overly smooth results.

Method: The framework integrates a supervised learning model with linear attention TTT modules and a TFMP prior to refine outputs, balancing efficiency and detail restoration.

Result: The approach improves restoration performance by effectively removing moir√© patterns and preserving high-frequency details.

Conclusion: The hybrid framework successfully addresses nonlinear demoir√©ing challenges, outperforming traditional and generative methods.

Abstract: This paper introduces a novel framework for image and video demoir\'eing by
integrating Maximum A Posteriori (MAP) estimation with advanced deep learning
techniques. Demoir\'eing addresses inherently nonlinear degradation processes,
which pose significant challenges for existing methods.
  Traditional supervised learning approaches either fail to remove moir\'e
patterns completely or produce overly smooth results. This stems from
constrained model capacity and scarce training data, which inadequately
represent the clean image distribution and hinder accurate reconstruction of
ground-truth images. While generative models excel in image restoration for
linear degradations, they struggle with nonlinear cases such as demoir\'eing
and often introduce artifacts.
  To address these limitations, we propose a hybrid MAP-based framework that
integrates two complementary components. The first is a supervised learning
model enhanced with efficient linear attention Test-Time Training (TTT)
modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\'eing.
The second is a Truncated Flow Matching Prior (TFMP) that further refines the
outputs by aligning them with the clean image distribution, effectively
restoring high-frequency details and suppressing artifacts. These two
components combine the computational efficiency of linear attention with the
refinement abilities of generative models, resulting in improved restoration
performance.

</details>


### [91] [Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization](https://arxiv.org/abs/2506.15937)
*Yosub Shin,Igor Molybog*

Main category: cs.CV

TL;DR: VideoSync is a framework for video synchronization that avoids reliance on specific features, outperforms prior methods, and introduces reproducible benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing video synchronization methods depend on unreliable audio or visual cues, and benchmarks lack generality and reproducibility.

Method: VideoSync operates independently of specific feature extraction methods and is evaluated on diverse datasets. Biases in prior work are corrected.

Result: VideoSync outperforms existing methods, including SeSyn-Net, under fair conditions. A CNN-based model is identified as the most effective for synchronization.

Conclusion: VideoSync advances video synchronization by improving generalizability and robustness for real-world applications.

Abstract: Video synchronization-aligning multiple video streams capturing the same
event from different angles-is crucial for applications such as reality TV show
production, sports analysis, surveillance, and autonomous systems. Prior work
has heavily relied on audio cues or specific visual events, limiting
applicability in diverse settings where such signals may be unreliable or
absent. Additionally, existing benchmarks for video synchronization lack
generality and reproducibility, restricting progress in the field. In this
work, we introduce VideoSync, a video synchronization framework that operates
independently of specific feature extraction methods, such as human pose
estimation, enabling broader applicability across different content types. We
evaluate our system on newly composed datasets covering single-human,
multi-human, and non-human scenarios, providing both the methodology and code
for dataset creation to establish reproducible benchmarks. Our analysis reveals
biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,
leading to inflated performance claims. We correct these biases and propose a
more rigorous evaluation framework, demonstrating that VideoSync outperforms
existing approaches, including SeSyn-Net, under fair experimental conditions.
Additionally, we explore various synchronization offset prediction methods,
identifying a convolutional neural network (CNN)-based model as the most
effective. Our findings advance video synchronization beyond domain-specific
constraints, making it more generalizable and robust for real-world
applications.

</details>


### [92] [Polyline Path Masked Attention for Vision Transformer](https://arxiv.org/abs/2506.15940)
*Zhongchen Zhao,Chaodong Xiao,Hui Lin,Qi Xie,Lei Zhang,Deyu Meng*

Main category: cs.CV

TL;DR: The paper introduces Polyline Path Masked Attention (PPMA), combining ViTs' self-attention with Mamba2's structured mask to improve spatial adjacency modeling in vision tasks.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of Vision Transformers (ViTs) and Mamba2 by integrating their global dependency and spatial adjacency modeling capabilities.

Method: Enhances Mamba2's structured mask with a 2D polyline path scanning strategy, embeds it into ViTs' self-attention, and provides theoretical analysis and efficient computation.

Result: PPMA outperforms state-of-the-art models in image classification, object detection, and segmentation, e.g., achieving 48.7%/51.1%/52.3% mIoU on ADE20K.

Conclusion: PPMA effectively combines ViTs and Mamba2 strengths, demonstrating superior performance in vision tasks.

Abstract: Global dependency modeling and spatial position modeling are two core issues
of the foundational architecture design in current deep learning frameworks.
Recently, Vision Transformers (ViTs) have achieved remarkable success in
computer vision, leveraging the powerful global dependency modeling capability
of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its
significant potential in natural language processing tasks by explicitly
modeling the spatial adjacency prior through the structured mask. In this
paper, we propose Polyline Path Masked Attention (PPMA) that integrates the
self-attention mechanism of ViTs with an enhanced structured mask of Mamba2,
harnessing the complementary strengths of both architectures. Specifically, we
first ameliorate the traditional structured mask of Mamba2 by introducing a 2D
polyline path scanning strategy and derive its corresponding structured mask,
polyline path mask, which better preserves the adjacency relationships among
image tokens. Notably, we conduct a thorough theoretical analysis on the
structural characteristics of the proposed polyline path mask and design an
efficient algorithm for the computation of the polyline path mask. Next, we
embed the polyline path mask into the self-attention mechanism of ViTs,
enabling explicit modeling of spatial adjacency prior. Extensive experiments on
standard benchmarks, including image classification, object detection, and
segmentation, demonstrate that our model outperforms previous state-of-the-art
approaches based on both state-space models and Transformers. For example, our
proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K
semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%,
respectively. Code is available at https://github.com/zhongchenzhao/PPMA.

</details>


### [93] [Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging](https://arxiv.org/abs/2506.15971)
*Jiawen Yang,Shuhao Chen,Yucong Duan,Ke Tang,Yu Zhang*

Main category: cs.CV

TL;DR: Proposes HMUDA for domain adaptation between distinct modalities using a bridge domain, and introduces LSB framework for semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of domain adaptation when source and target domains are entirely different modalities.

Method: Uses a dual-branch architecture with feature consistency and domain alignment losses to align representations and reduce discrepancies.

Result: Achieves state-of-the-art performance on six benchmark datasets.

Conclusion: LSB effectively enables knowledge transfer across completely different modalities.

Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps
but become struggled when the source and target domains belong to entirely
distinct modalities. To address this limitation, we propose a novel setting
called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which
enables knowledge transfer between completely different modalities by
leveraging a bridge domain containing unlabeled samples from both modalities.
To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a
specialized framework designed for the semantic segmentation task.
Specifically, LSB utilizes a dual-branch architecture, incorporating a feature
consistency loss to align representations across modalities and a domain
alignment loss to reduce discrepancies between class centroids across domains.
Extensive experiments conducted on six benchmark datasets demonstrate that LSB
achieves state-of-the-art performance.

</details>


### [94] [LBMamba: Locally Bi-directional Mamba](https://arxiv.org/abs/2506.15976)
*Jingwei Zhang,Xi Han,Hong Qin,Mahdi S. Hosseini,Dimitris Samaras*

Main category: cs.CV

TL;DR: LBMamba introduces a locally bi-directional SSM block to avoid extra scans, improving efficiency. LBVim, built on LBMamba, alternates scan directions for global receptive field without added cost, outperforming in various tasks.


<details>
  <summary>Details</summary>
Motivation: Current Mamba-based methods use bi-directional scans, doubling computation and reducing efficiency. LBMamba aims to eliminate this inefficiency.

Method: LBMamba embeds a lightweight locally backward scan inside the forward selective scan. LBVim alternates scan directions every two layers.

Result: LBVim achieves higher accuracy and efficiency in ImageNet-1K, ADE20K, COCO, and WSI tasks.

Conclusion: LBMamba and LBVim offer a superior performance-throughput trade-off, validating their effectiveness in vision tasks.

Abstract: Mamba, a State Space Model (SSM) that accelerates training by recasting
recurrence as a parallel selective scan, has recently emerged as a
linearly-scaling, efficient alternative to self-attention. Because of its
unidirectional nature, each state in Mamba only has information of its previous
states and is blind to states after. Current Mamba-based computer-vision
methods typically overcome this limitation by augmenting Mamba's global forward
scan with a global backward scan, forming a bi-directional scan that restores a
full receptive field. However, this operation doubles the computational load,
eroding much of the efficiency advantage that originally Mamba have. To
eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM
block that embeds a lightweight locally backward scan inside the forward
selective scan and executes it entirely in per-thread registers. Building on
LBMamba, we present LBVim, a scalable vision backbone that alternates scan
directions every two layers to recover a global receptive field without extra
backward sweeps. We validate the versatility of our approach on both natural
images and whole slide images (WSIs). We show that our LBVim constantly offers
a superior performance-throughput trade-off. That is under the same throughput,
LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K
classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic
segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection
dataset. We also integrate LBMamba into the SOTA pathology multiple instance
learning (MIL) approach, MambaMIL, which uses single directional scan.
Experiments on 3 public WSI classification datasets for show that our method
achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1,
1.67% better accuracy.

</details>


### [95] [Towards Classifying Histopathological Microscope Images as Time Series Data](https://arxiv.org/abs/2506.15977)
*Sungrae Hong,Hyeongmin Park,Youngsin Ko,Sol Lee,Bryan Wong,Mun Yong Yi*

Main category: cs.CV

TL;DR: A novel method classifies microscopy images as time series using Dynamic Time-series Warping (DTW) and attention-based pooling, achieving reliable performance in medical image analysis.


<details>
  <summary>Details</summary>
Motivation: Microscopic pathology images are crucial for cancer diagnosis but underutilized in deep learning. The paper addresses challenges like manual acquisition and weak labels.

Method: Proposes treating microscopy images as time series, using DTW to fit varying-length sequences to fixed targets and attention-based pooling for classification.

Result: Outperforms baselines, with ablation studies validating component contributions. Stable and reliable results are achieved through varied inference strategies.

Conclusion: The approach advances medical image analysis by effectively leveraging microscopy images and ensuring trustworthy performance.

Abstract: As the frontline data for cancer diagnosis, microscopic pathology images are
fundamental for providing patients with rapid and accurate treatment. However,
despite their practical value, the deep learning community has largely
overlooked their usage. This paper proposes a novel approach to classifying
microscopy images as time series data, addressing the unique challenges posed
by their manual acquisition and weakly labeled nature. The proposed method fits
image sequences of varying lengths to a fixed-length target by leveraging
Dynamic Time-series Warping (DTW). Attention-based pooling is employed to
predict the class of the case simultaneously. We demonstrate the effectiveness
of our approach by comparing performance with various baselines and showcasing
the benefits of using various inference strategies in achieving stable and
reliable results. Ablation studies further validate the contribution of each
component. Our approach contributes to medical image analysis by not only
embracing microscopic images but also lifting them to a trustworthy level of
performance.

</details>


### [96] [Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization](https://arxiv.org/abs/2506.15980)
*Cong Wang,Zexuan Deng,Zhiwei Jiang,Fei Shen,Yafeng Yin,Shiwei Gan,Zifeng Cheng,Shiping Ge,Qing Gu*

Main category: cs.CV

TL;DR: SignViP is a novel SLVG framework using multiple fine-grained conditions (poses, 3D hands) for high-fidelity sign language video generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing SLVG methods rely on single coarse conditions, limiting video naturalness and expressiveness. SignViP addresses this by incorporating fine-grained conditions.

Method: SignViP uses a diffusion model, FSQ Autoencoder, and token translator to integrate and represent fine-grained conditions, translating text to tokens for video generation.

Result: SignViP achieves state-of-the-art performance in video quality, temporal coherence, and semantic fidelity.

Conclusion: SignViP advances SLVG by leveraging fine-grained conditions and discrete tokenization, improving generation fidelity.

Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving
sign language videos from spoken language texts. Existing methods primarily
rely on the single coarse condition (\eg, skeleton sequences) as the
intermediary to bridge the translation model and the video generation model,
which limits both the naturalness and expressiveness of the generated videos.
To overcome these limitations, we propose SignViP, a novel SLVG framework that
incorporates multiple fine-grained conditions for improved generation fidelity.
Rather than directly translating error-prone high-dimensional conditions,
SignViP adopts a discrete tokenization paradigm to integrate and represent
fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP
contains three core components. (1) Sign Video Diffusion Model is jointly
trained with a multi-condition encoder to learn continuous embeddings that
encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization
(FSQ) Autoencoder is further trained to compress and quantize these embeddings
into discrete tokens for compact representation of the conditions. (3)
Multi-Condition Token Translator is trained to translate spoken language text
to discrete multi-condition tokens. During inference, Multi-Condition Token
Translator first translates the spoken language text into discrete
multi-condition tokens. These tokens are then decoded to continuous embeddings
by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion
Model to guide video generation. Experimental results show that SignViP
achieves state-of-the-art performance across metrics, including video quality,
temporal coherence, and semantic fidelity. The code is available at
https://github.com/umnooob/signvip/.

</details>


### [97] [Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation](https://arxiv.org/abs/2506.15988)
*Connor Malone,Owen Claxton,Iman Shames,Michael Milford*

Main category: cs.CV

TL;DR: The paper analyzes adversarial attacks on Visual Place Recognition (VPR) systems, proposes an Adversarial Attack Detector (AAD) framework, and demonstrates its performance benefits in robot navigation.


<details>
  <summary>Details</summary>
Motivation: VPR systems are vulnerable to adversarial attacks, posing risks in robot navigation. This work aims to address this gap by evaluating attacks and proposing a defense mechanism.

Method: The study tests four common and four novel VPR-specific adversarial attacks, introduces an AAD framework, and evaluates it using metrics like Along-Track Error and detection rates.

Result: AADs improve performance, e.g., reducing mean along-track error by ~50% with 75% True Positive and up to 25% False Positive rates.

Conclusion: The work underscores the necessity of AADs for trustworthy navigation and provides design insights for real-world systems.

Abstract: Stand-alone Visual Place Recognition (VPR) systems have little defence
against a well-designed adversarial attack, which can lead to disastrous
consequences when deployed for robot navigation. This paper extensively
analyzes the effect of four adversarial attacks common in other perception
tasks and four novel VPR-specific attacks on VPR localization performance. We
then propose how to close the loop between VPR, an Adversarial Attack Detector
(AAD), and active navigation decisions by demonstrating the performance benefit
of simulated AADs in a novel experiment paradigm -- which we detail for the
robotics community to use as a system framework. In the proposed experiment
paradigm, we see the addition of AADs across a range of detection accuracies
can improve performance over baseline; demonstrating a significant improvement
-- such as a ~50% reduction in the mean along-track localization error -- can
be achieved with True Positive and False Positive detection rates of only 75%
and up to 25% respectively. We examine a variety of metrics including:
Along-Track Error, Percentage of Time Attacked, Percentage of Time in an
`Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on
these results, we provide the first investigation into the efficacy of the Fast
Gradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this
work highlights the need for AADs in real-world systems for trustworthy
navigation, and informs quantitative requirements for system design.

</details>


### [98] [DIGMAPPER: A Modular System for Automated Geologic Map Digitization](https://arxiv.org/abs/2506.16006)
*Weiwei Duan,Michael P. Gerlek,Steven N. Minton,Craig A. Knoblock,Fandel Lin,Theresa Chen,Leeje Jang,Sofia Kirsanova,Zekun Li,Yijun Lin,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: DIGMAPPER is a scalable, automated system for digitizing geologic maps, using deep learning and innovative techniques to overcome data and complexity challenges, achieving high accuracy and deployment success at USGS.


<details>
  <summary>Details</summary>
Motivation: Historical geologic maps are vital for renewable energy and national security, but manual digitization is labor-intensive and slow.

Method: DIGMAPPER employs a dockerized, workflow-orchestrated system with deep learning models for layout analysis, feature extraction, and georeferencing, using in-context learning, synthetic data, and transformers.

Result: Evaluations on 100+ maps show high accuracy in feature extraction and georeferencing.

Conclusion: DIGMAPPER accelerates geospatial dataset creation, aiding critical mineral assessments and geoscientific applications.

Abstract: Historical geologic maps contain rich geospatial information, such as rock
units, faults, folds, and bedding planes, that is critical for assessing
mineral resources essential to renewable energy, electric vehicles, and
national security. However, digitizing maps remains a labor-intensive and
time-consuming task. We present DIGMAPPER, a modular, scalable system developed
in collaboration with the United States Geological Survey (USGS) to automate
the digitization of geologic maps. DIGMAPPER features a fully dockerized,
workflow-orchestrated architecture that integrates state-of-the-art deep
learning models for map layout analysis, feature extraction, and
georeferencing. To overcome challenges such as limited training data and
complex visual content, our system employs innovative techniques, including
in-context learning with large language models, synthetic data generation, and
transformer-based models. Evaluations on over 100 annotated maps from the
DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point
feature extraction, and reliable georeferencing performance. Deployed at USGS,
DIGMAPPER significantly accelerates the creation of analysis-ready geospatial
datasets, supporting national-scale critical mineral assessments and broader
geoscientific applications.

</details>


### [99] [EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training](https://arxiv.org/abs/2506.16017)
*Liangjing Shao,Linxin Bai,Chenkang Du,Xinrong Chen*

Main category: cs.CV

TL;DR: A novel framework with multistep efficient finetuning improves self-supervised depth estimation in endoscopy, achieving state-of-the-art performance with lower error.


<details>
  <summary>Details</summary>
Motivation: Address lighting variations and sparse textures in endoscopic scenes for better depth and ego-motion estimation.

Method: Three-step training: optical flow registration, multiscale image decomposition, and multiple transformation alignments, with parameter-efficient finetuning.

Result: Achieves 4%‚àº10% lower error on SCARED and Hamlyn datasets.

Conclusion: The proposed method effectively handles illumination issues and information interference, outperforming existing techniques.

Abstract: Monocular depth estimation and ego-motion estimation are significant tasks
for scene perception and navigation in stable, accurate and efficient
robot-assisted endoscopy. To tackle lighting variations and sparse textures in
endoscopic scenes, multiple techniques including optical flow, appearance flow
and intrinsic image decomposition have been introduced into the existing
methods. However, the effective training strategy for multiple modules are
still critical to deal with both illumination issues and information
interference for self-supervised depth estimation in endoscopy. Therefore, a
novel framework with multistep efficient finetuning is proposed in this work.
In each epoch of end-to-end training, the process is divided into three steps,
including optical flow registration, multiscale image decomposition and
multiple transformation alignments. At each step, only the related networks are
trained without interference of irrelevant information. Based on
parameter-efficient finetuning on the foundation model, the proposed method
achieves state-of-the-art performance on self-supervised depth estimation on
SCARED dataset and zero-shot depth estimation on Hamlyn dataset, with
4\%$\sim$10\% lower error. The evaluation code of this work has been published
on https://github.com/BaymaxShao/EndoMUST.

</details>


### [100] [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054)
*Tianchen Zhao,Ke Hong,Xinhao Yang,Xuefeng Xiao,Huixia Li,Feng Ling,Ruiqi Xie,Siqi Chen,Hongyu Zhu,Yichong Zhang,Yu Wang*

Main category: cs.CV

TL;DR: PAROAttention reorganizes visual attention patterns into block-wise forms, improving efficiency in high-resolution image and video generation without loss of quality.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of attention mechanisms in visual generation leads to high costs, and existing solutions like sparsification and quantization struggle with low density and reduced bitwidths.

Method: Proposes PARO, a technique to reorganize attention patterns into hardware-friendly block-wise forms, simplifying sparsification and quantization.

Result: Achieves lossless metrics in video/image generation with ~20%-30% density and INT8/INT4 bitwidth, yielding 1.9x-2.7x speedup.

Conclusion: PAROAttention effectively addresses efficiency challenges in visual generation while maintaining performance.

Abstract: In visual generation, the quadratic complexity of attention mechanisms
results in high memory and computational costs, especially for longer token
sequences required in high-resolution image or multi-frame video generation. To
address this, prior research has explored techniques such as sparsification and
quantization. However, these techniques face significant challenges under low
density and reduced bitwidths. Through systematic analysis, we identify that
the core difficulty stems from the dispersed and irregular characteristics of
visual attention patterns. Therefore, instead of introducing specialized
sparsification and quantization design to accommodate such patterns, we propose
an alternative strategy: *reorganizing* the attention pattern to alleviate the
challenges. Inspired by the local aggregation nature of visual feature
extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)**
technique, which unifies the diverse attention patterns into a
hardware-friendly block-wise pattern. This unification substantially simplifies
and enhances both sparsification and quantization. We evaluate the
performance-efficiency trade-offs of various design choices and finalize a
methodology tailored for the unified pattern. Our approach, **PAROAttention**,
achieves video and image generation with lossless metrics, and nearly identical
results from full-precision (FP) baselines, while operating at notably lower
density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to
**2.7x** end-to-end latency speedup.

</details>


### [101] [Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation](https://arxiv.org/abs/2506.16058)
*Yong Liu,SongLi Wu,Sule Bai,Jiahao Wang,Yitong Wang,Yansong Tang*

Main category: cs.CV

TL;DR: The paper introduces OpenBench, a new benchmark for open-vocabulary segmentation, and OVSNet, a method to improve segmentation performance in diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to measure models' comprehension of open-vocabulary concepts due to semantic overlap with training data.

Method: Proposes OVSNet, which fuses heterogeneous features and expands the training space cost-free.

Result: OVSNet achieves state-of-the-art results on both existing datasets and OpenBench.

Conclusion: OpenBench and OVSNet effectively address the limitations of current benchmarks and methods, demonstrating their soundness and effectiveness.

Abstract: Open-vocabulary segmentation aims to achieve segmentation of arbitrary
categories given unlimited text inputs as guidance. To achieve this, recent
works have focused on developing various technical routes to exploit the
potential of large-scale pre-trained vision-language models and have made
significant progress on existing benchmarks. However, we find that existing
test sets are limited in measuring the models' comprehension of
``open-vocabulary" concepts, as their semantic space closely resembles the
training space, even with many overlapping categories. To this end, we present
a new benchmark named OpenBench that differs significantly from the training
semantics. It is designed to better assess the model's ability to understand
and segment a wide range of real-world concepts. When testing existing methods
on OpenBench, we find that their performance diverges from the conclusions
drawn on existing test sets. In addition, we propose a method named OVSNet to
improve the segmentation performance for diverse and open scenarios. Through
elaborate fusion of heterogeneous features and cost-free expansion of the
training space, OVSNet achieves state-of-the-art results on both existing
datasets and our proposed OpenBench. Corresponding analysis demonstrate the
soundness and effectiveness of our proposed benchmark and method.

</details>


### [102] [STAR-Pose: Efficient Low-Resolution Video Human Pose Estimation via Spatial-Temporal Adaptive Super-Resolution](https://arxiv.org/abs/2506.16061)
*Yucheng Jin,Jinyan Chen,Ziyue He,Baojun Han,Furan An*

Main category: cs.CV

TL;DR: STAR-Pose is a spatial-temporal adaptive super-resolution framework for low-resolution video human pose estimation, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of human pose estimation in low-resolution videos, where conventional methods are either inefficient or assume high-quality inputs.

Method: Proposes a spatial-temporal Transformer with LeakyReLU-modified linear attention and an adaptive fusion module integrating CNN for local texture enhancement. Uses a pose-aware compound loss for task-oriented super-resolution.

Result: Achieves up to 5.2% mAP improvement under 64x48 resolution and is 2.8x to 4.4x faster than cascaded approaches.

Conclusion: STAR-Pose effectively balances accuracy and efficiency for low-resolution video pose estimation.

Abstract: Human pose estimation in low-resolution videos presents a fundamental
challenge in computer vision. Conventional methods either assume high-quality
inputs or employ computationally expensive cascaded processing, which limits
their deployment in resource-constrained environments. We propose STAR-Pose, a
spatial-temporal adaptive super-resolution framework specifically designed for
video-based human pose estimation. Our method features a novel spatial-temporal
Transformer with LeakyReLU-modified linear attention, which efficiently
captures long-range temporal dependencies. Moreover, it is complemented by an
adaptive fusion module that integrates parallel CNN branch for local texture
enhancement. We also design a pose-aware compound loss to achieve task-oriented
super-resolution. This loss guides the network to reconstruct structural
features that are most beneficial for keypoint localization, rather than
optimizing purely for visual quality. Extensive experiments on several
mainstream video HPE datasets demonstrate that STAR-Pose outperforms existing
approaches. It achieves up to 5.2% mAP improvement under extremely
low-resolution (64x48) conditions while delivering 2.8x to 4.4x faster
inference than cascaded approaches.

</details>


### [103] [TD3Net: A Temporal Densely Connected Multi-Dilated Convolutional Network for Lipreading](https://arxiv.org/abs/2506.16073)
*Byung Hoon Lee,Wooseok Shin,Sung Won Han*

Main category: cs.CV

TL;DR: The paper proposes TD3Net, a backend architecture for word-level lipreading that combines dense skip connections and multi-dilated temporal convolutions to address blind spots in the receptive field, achieving state-of-the-art performance with fewer parameters and lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing TCN-based backend architectures for lipreading suffer from blind spots in the receptive field, leading to information loss about continuous lip movements.

Method: TD3Net integrates dense skip connections and multi-dilated temporal convolutions to create a wide, dense receptive field without blind spots.

Result: TD3Net outperforms existing TCN-based methods in accuracy while using fewer parameters and lower computational resources, as validated on LRW and LRW-1000 datasets.

Conclusion: TD3Net effectively models complex temporal representations in lipreading, preserving temporal continuity and offering practical advantages for lipreading systems.

Abstract: The word-level lipreading approach typically employs a two-stage framework
with separate frontend and backend architectures to model dynamic lip
movements. Each component has been extensively studied, and in the backend
architecture, temporal convolutional networks (TCNs) have been widely adopted
in state-of-the-art methods. Recently, dense skip connections have been
introduced in TCNs to mitigate the limited density of the receptive field,
thereby improving the modeling of complex temporal representations. However,
their performance remains constrained owing to potential information loss
regarding the continuous nature of lip movements, caused by blind spots in the
receptive field. To address this limitation, we propose TD3Net, a temporal
densely connected multi-dilated convolutional network that combines dense skip
connections and multi-dilated temporal convolutions as the backend
architecture. TD3Net covers a wide and dense receptive field without blind
spots by applying different dilation factors to skip-connected features.
Experimental results on a word-level lipreading task using two large publicly
available datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that
the proposed method achieves performance comparable to state-of-the-art
methods. It achieved higher accuracy with fewer parameters and lower
floating-point operations compared to existing TCN-based backend architectures.
Moreover, visualization results suggest that our approach effectively utilizes
diverse temporal features while preserving temporal continuity, presenting
notable advantages in lipreading systems. The code is available at our GitHub
repository:
https://github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading

</details>


### [104] [PR-DETR: Injecting Position and Relation Prior for Dense Video Captioning](https://arxiv.org/abs/2506.16082)
*Yizhe Li,Sanping Zhou,Zheng Qin,Le Wang*

Main category: cs.CV

TL;DR: PR-DETR improves dense video captioning by injecting explicit position and relation priors into a detection transformer, enhancing localization and caption quality.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based methods implicitly learn event locations and semantics, requiring large datasets and limiting performance. PR-DETR addresses this by introducing explicit priors.

Method: PR-DETR uses position-anchored queries for scene-specific position/semantic info and an event relation encoder for boundary relationships, improving event localization and caption coherence.

Result: Ablation studies confirm the effectiveness of the priors. PR-DETR achieves competitive performance on ActivityNet Captions and YouCook2 datasets.

Conclusion: PR-DETR demonstrates that explicit position and relation priors significantly enhance dense video captioning performance.

Abstract: Dense video captioning is a challenging task that aims to localize and
caption multiple events in an untrimmed video. Recent studies mainly follow the
transformer-based architecture to jointly perform the two sub-tasks, i.e.,
event localization and caption generation, in an end-to-end manner. Based on
the general philosophy of detection transformer, these methods implicitly learn
the event locations and event semantics, which requires a large amount of
training data and limits the model's performance in practice. In this paper, we
propose a novel dense video captioning framework, named PR-DETR, which injects
the explicit position and relation prior into the detection transformer to
improve the localization accuracy and caption quality, simultaneously. On the
one hand, we first generate a set of position-anchored queries to provide the
scene-specific position and semantic information about potential events as
position prior, which serves as the initial event search regions to eliminate
the implausible event proposals. On the other hand, we further design an event
relation encoder to explicitly calculate the relationship between event
boundaries as relation prior to guide the event interaction to improve the
semantic coherence of the captions. Extensive ablation studies are conducted to
verify the effectiveness of the position and relation prior. Experimental
results also show the competitive performance of our method on ActivityNet
Captions and YouCook2 datasets.

</details>


### [105] [AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models](https://arxiv.org/abs/2506.16112)
*Yuan Zhang,Chun-Kai Fan,Tao Huang,Ming Lu,Sicheng Yu,Junwen Pan,Kuan Cheng,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: AutoV automates visual prompt selection for LVLMs, improving performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Manual visual prompt design is challenging and sub-optimal; automation is needed.

Method: AutoV learns to select optimal visual prompts using a ranking-based training pipeline.

Result: AutoV boosts LVLM performance, e.g., 1.7% gain on LLaVA-OV and 1.9% on Qwen2.5-VL.

Conclusion: AutoV is an effective automated visual prompting method for LVLMs.

Abstract: Inspired by text prompts in large language models (LLMs), visual prompts have
been explored to enhance the reasoning capabilities of large vision-language
models (LVLMs). Current methods design heuristic visual prompts, such as
overlaying a text-query-guided attention heatmap on the original input image.
However, designing effective prompts manually is challenging and
time-consuming, and it often fails to explore the benefits of different visual
prompts, leading to sub-optimal performance. To this end, we propose
\textbf{AutoV} that learns to automatically select the optimal visual prompt
from various candidates based on given textual queries and the input image. To
train AutoV, we developed an automatic data collection and labeling pipeline
that evaluates various visual prompts with a pre-trained LVLM. We input a set
of visual prompts into the LVLM and rank them according to the prediction
losses generated by the model. Using the ranking as a supervision signal, we
train AutoV to automatically choose the optimal visual prompt from various
visual prompts for LVLMs. Experimental results indicate that AutoV enhances the
performance of various LVLMs across multiple popular image understanding tasks.
For instance, LLaVA-OV with AutoV achieves $\textbf{1.7}\%$ accuracy gain on
LLaVA$^{\text{Wild}}$, and AutoV boosts Qwen2.5-VL by $\textbf{1.9}\%$ on MMMU,
highlighting its potential as an optimal visual prompting method for LVLMs.

</details>


### [106] [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](https://arxiv.org/abs/2506.16119)
*Chengyu Bai,Yuming Li,Zhongyu Zhao,Jintao Chen,Peidong Jia,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: FastInit introduces a fast noise initialization method for video generation, eliminating iterative refinement and improving efficiency while maintaining high temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational cost and inefficiency of iterative refinement in video generation while ensuring temporal consistency.

Method: FastInit uses a Video Noise Prediction Network (VNPNet) to generate refined noise in a single forward pass, trained on a large-scale dataset of text prompts, random noise, and refined noise pairs.

Result: FastInit enhances video generation efficiency and quality, achieving high temporal consistency across frames in various text-to-video models.

Conclusion: FastInit offers a practical, efficient solution for video generation, improving both quality and consistency, with plans to release the code and dataset.

Abstract: Video generation has made significant strides with the development of
diffusion models; however, achieving high temporal consistency remains a
challenging task. Recently, FreeInit identified a training-inference gap and
introduced a method to iteratively refine the initial noise during inference.
However, iterative refinement significantly increases the computational cost
associated with video generation. In this paper, we introduce FastInit, a fast
noise initialization method that eliminates the need for iterative refinement.
FastInit learns a Video Noise Prediction Network (VNPNet) that takes random
noise and a text prompt as input, generating refined noise in a single forward
pass. Therefore, FastInit greatly enhances the efficiency of video generation
while achieving high temporal consistency across frames. To train the VNPNet,
we create a large-scale dataset consisting of pairs of text prompts, random
noise, and refined noise. Extensive experiments with various text-to-video
models show that our method consistently improves the quality and temporal
consistency of the generated videos. FastInit not only provides a substantial
improvement in video generation but also offers a practical solution that can
be applied directly during inference. The code and dataset will be released.

</details>


### [107] [Neurosymbolic Object-Centric Learning with Distant Supervision](https://arxiv.org/abs/2506.16129)
*Stefano Colamonaco,David Debot,Giuseppe Marra*

Main category: cs.CV

TL;DR: DeepObjectLog, a neurosymbolic model, learns object-centric representations from raw data using distant supervision, outperforming baselines in generalization tasks.


<details>
  <summary>Details</summary>
Motivation: Existing systems require object-level supervision or predefined decompositions, limiting flexibility. The goal is to learn object-centric representations directly from unstructured data.

Method: DeepObjectLog combines a perceptual module for object extraction with a symbolic reasoning layer using probabilistic logic programming, enabling guided object discovery.

Result: The model outperforms neural and neurosymbolic baselines in tasks involving unseen object compositions, tasks, and object counts.

Conclusion: DeepObjectLog demonstrates effective learning of object-centric representations without explicit supervision, advancing relational learning capabilities.

Abstract: Relational learning enables models to generalize across structured domains by
reasoning over objects and their interactions. While recent advances in
neurosymbolic reasoning and object-centric learning bring us closer to this
goal, existing systems rely either on object-level supervision or on a
predefined decomposition of the input into objects. In this work, we propose a
neurosymbolic formulation for learning object-centric representations directly
from raw unstructured perceptual data and using only distant supervision. We
instantiate this approach in DeepObjectLog, a neurosymbolic model that
integrates a perceptual module, which extracts relevant object representations,
with a symbolic reasoning layer based on probabilistic logic programming. By
enabling sound probabilistic logical inference, the symbolic component
introduces a novel learning signal that further guides the discovery of
meaningful objects in the input. We evaluate our model across a diverse range
of generalization settings, including unseen object compositions, unseen tasks,
and unseen number of objects. Experimental results show that our method
outperforms neural and neurosymbolic baselines across the tested settings.

</details>


### [108] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: The paper introduces SEED-Bench-R1 for evaluating multimodal LLMs (MLLMs) and proposes GRPO-CARE, a consistency-aware RL framework, to improve reasoning coherence and answer accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods like GRPO lack rigorous evaluation for MLLMs and often sacrifice logical coherence for answer correctness.

Method: GRPO-CARE uses a two-tiered reward system: a base reward for answer correctness and an adaptive consistency bonus for logical coherence, replacing KL penalties.

Result: GRPO-CARE outperforms standard GRPO, achieving a 6.7% performance gain on hard tasks and a 24.5% improvement in consistency, with strong transferability.

Conclusion: The work provides a benchmark (SEED-Bench-R1) and a framework (GRPO-CARE) to advance interpretable and robust MLLMs.

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [109] [MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models](https://arxiv.org/abs/2506.16157)
*Xingbai Chen,Tingchao Fu,Renyang Liu,Wei Zhou,Chao Yi*

Main category: cs.CV

TL;DR: The paper introduces a novel adversarial attack strategy, Multimodal Bidirectional Attack, for Referring Expression Segmentation (RES) models to address robustness against adversarial examples and improve cross-text transferability.


<details>
  <summary>Details</summary>
Motivation: The robustness of RES models against adversarial examples is unexplored, and existing attack methods fail to expose vulnerabilities in RES's multimodal structure. Practical scenarios require adversarial examples that generalize across diverse textual inputs.

Method: The proposed method involves learnable proxy textual embedding perturbation and joint visual-aligned and textual-adversarial optimization during attack generation.

Result: Extensive experiments show the method's superior effectiveness in generating adversarial examples with high cross-text transferability compared to existing methods.

Conclusion: The Multimodal Bidirectional Attack effectively addresses the multimodal challenges of RES, enhancing adversarial robustness and cross-text transferability.

Abstract: Referring Expression Segmentation (RES) enables precise object segmentation
in images based on natural language descriptions, offering high flexibility and
broad applicability in real-world vision tasks. Despite its impressive
performance, the robustness of RES models against adversarial examples remains
largely unexplored. While prior adversarial attack methods have explored
adversarial robustness on conventional segmentation models, they perform poorly
when directly applied to RES, failing to expose vulnerabilities in its
multimodal structure. Moreover, in practical open-world scenarios, users
typically issue multiple, diverse referring expressions to interact with the
same image, highlighting the need for adversarial examples that generalize
across varied textual inputs. To address these multimodal challenges, we
propose a novel adversarial attack strategy termed \textbf{Multimodal
Bidirectional Attack}, tailored for RES models. Our method introduces learnable
proxy textual embedding perturbation and jointly performs visual-aligned
optimization on the image modality and textual-adversarial optimization on the
textual modality during attack generation. This dual optimization framework
encourages adversarial images to actively adapt to more challenging text
embedding during optimization, thereby enhancing their cross-text
transferability, which refers to the ability of adversarial examples to remain
effective under a variety of unseen or semantically diverse textual inputs.
Extensive experiments conducted on multiple RES models and benchmark datasets
demonstrate the superior effectiveness of our method compared to existing
methods.

</details>


### [110] [Co-Speech Gesture and Facial Expression Generation for Non-Photorealistic 3D Characters](https://arxiv.org/abs/2506.16159)
*Taisei Omine,Naoyuki Kawabata,Fuminori Homma*

Main category: cs.CV

TL;DR: Proposes methods for expressing emotions in non-photorealistic characters using comics and semantic gestures, showing improvements over existing research.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on photorealistic avatars, neglecting non-photorealistic characters like anime.

Method: Utilizes expression data from comics and dialogue-specific semantic gestures.

Result: User study showed significant improvements in multiple aspects.

Conclusion: The proposed methods effectively enhance emotional expression for non-photorealistic characters.

Abstract: With the advancement of conversational AI, research on bodily expressions,
including gestures and facial expressions, has also progressed. However, many
existing studies focus on photorealistic avatars, making them unsuitable for
non-photorealistic characters, such as those found in anime. This study
proposes methods for expressing emotions, including exaggerated expressions
unique to non-photorealistic characters, by utilizing expression data extracted
from comics and dialogue-specific semantic gestures. A user study demonstrated
significant improvements across multiple aspects when compared to existing
research.

</details>


### [111] [Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization](https://arxiv.org/abs/2506.16160)
*Jiyao Wang,Xiao Yang,Hao Lu,Dengbo He,Kaishun Wu*

Main category: cs.CV

TL;DR: The paper proposes a unified framework (GAP) for multi-source synsemantic domain generalization (MSSDG) and test-time personalized adaptation (TTPA) in remote physiological measurement, addressing challenges like partial labeling and noise.


<details>
  <summary>Details</summary>
Motivation: To enhance generalizability and bridge the gap between generalization and personalization methods in remote physiological measurement.

Method: Disentangles face video data into invariant semantics, individual bias, and noise, then applies multiple modules with priors for MSSDG and TTPA.

Result: Validated on six datasets and a new real-world driving dataset, showing effectiveness with minimal adjustments.

Conclusion: The framework successfully unifies MSSDG and TTPA, with plans to release codes and datasets.

Abstract: Multi-source synsemantic domain generalization (MSSDG) for multi-task remote
physiological measurement seeks to enhance the generalizability of these
metrics and attracts increasing attention. However, challenges like partial
labeling and environmental noise may disrupt task-specific accuracy. Meanwhile,
given that real-time adaptation is necessary for personalized products, the
test-time personalized adaptation (TTPA) after MSSDG is also worth exploring,
while the gap between previous generalization and personalization methods is
significant and hard to fuse. Thus, we proposed a unified framework for
MSSD\textbf{G} and TTP\textbf{A} employing \textbf{P}riors (\textbf{GAP}) in
biometrics and remote photoplethysmography (rPPG). We first disentangled
information from face videos into invariant semantics, individual bias, and
noise. Then, multiple modules incorporating priors and our observations were
applied in different stages and for different facial information. Then, based
on the different principles of achieving generalization and personalization,
our framework could simultaneously address MSSDG and TTPA under multi-task
remote physiological estimation with minimal adjustments. We expanded the MSSDG
benchmark to the TTPA protocol on six publicly available datasets and
introduced a new real-world driving dataset with complete labeling. Extensive
experiments that validated our approach, and the codes along with the new
dataset will be released.

</details>


### [112] [Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis](https://arxiv.org/abs/2506.16186)
*Zhenghao Xi,Xiang Liu,Yaqi Liu,Yitong Cai,Yangyu Zheng*

Main category: cs.CV

TL;DR: The paper proposes a deep learning framework using GANs and CNNs for accident detection in CCTV footage, achieving high accuracy with models like FTCNN and VIT.


<details>
  <summary>Details</summary>
Motivation: The rising global car accident rates necessitate automated, efficient accident detection systems to enhance transport safety.

Method: The framework uses GANs for data synthesis and CNNs for training, with preprocessing steps like resizing and normalization. Models tested include CNN, FTCNN, and VIT.

Result: FTCNN and VIT achieved 94% and 95% accuracy, respectively, outperforming CNN (88%).

Conclusion: The framework is effective for real-time accident detection and has potential for smart city and emergency management applications.

Abstract: Accident detection using Closed Circuit Television (CCTV) footage is one of
the most imperative features for enhancing transport safety and efficient
traffic control. To this end, this research addresses the issues of supervised
monitoring and data deficiency in accident detection systems by adapting
excellent deep learning technologies. The motivation arises from rising
statistics in the number of car accidents worldwide; this calls for innovation
and the establishment of a smart, efficient and automated way of identifying
accidents and calling for help to save lives. Addressing the problem of the
scarcity of data, the presented framework joins Generative Adversarial Networks
(GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model
training. Video frames for accidents and non-accidents are collected from
YouTube videos, and we perform resizing, image enhancement and image
normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned
Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best
for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%,
while the CNN model obtained 88%. Such results show that the proposed framework
suits traffic safety applications due to its high real-time accident detection
capabilities and broad-scale applicability. This work lays the foundation for
intelligent surveillance systems in the future for real-time traffic
monitoring, smart city framework, and integration of intelligent surveillance
systems into emergency management systems.

</details>


### [113] [VideoGAN-based Trajectory Proposal for Automated Vehicles](https://arxiv.org/abs/2506.16209)
*Annajoyce Mariani,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: A GAN-based pipeline generates realistic traffic trajectories from BEV videos, achieving fast training and inference with high physical realism.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture complex, multimodal trajectory distributions in traffic scenarios.

Method: Uses a GAN trained on low-resolution BEV occupancy grid videos, extracting trajectories via object detection and matching.

Result: Achieves realistic trajectories with 100 GPU hours training and <20ms inference, aligning well with ground truth data.

Conclusion: GANs are effective for generating accurate, fast traffic trajectories from BEV videos.

Abstract: Being able to generate realistic trajectory options is at the core of
increasing the degree of automation of road vehicles. While model-driven,
rule-based, and classical learning-based methods are widely used to tackle
these tasks at present, they can struggle to effectively capture the complex,
multimodal distributions of future trajectories. In this paper we investigate
whether a generative adversarial network (GAN) trained on videos of bird's-eye
view (BEV) traffic scenarios can generate statistically accurate trajectories
that correctly capture spatial relationships between the agents. To this end,
we propose a pipeline that uses low-resolution BEV occupancy grid videos as
training data for a video generative model. From the generated videos of
traffic scenarios we extract abstract trajectory data using single-frame object
detection and frame-to-frame object matching. We particularly choose a GAN
architecture for the fast training and inference times with respect to
diffusion models. We obtain our best results within 100 GPU hours of training,
with inference times under 20\,ms. We demonstrate the physical realism of the
proposed trajectories in terms of distribution alignment of spatial and dynamic
parameters with respect to the ground truth videos from the Waymo Open Motion
Dataset.

</details>


### [114] [FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2506.16218)
*Xinting Liao,Weiming Liu,Jiaming Qian,Pengyang Zhou,Jiahe Xu,Wenjie Wang,Chaochao Chen,Xiaolin Zheng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: FOCoOp introduces a federated OOD-aware framework to improve performance and robustness in vision-language models by leveraging ID and OOD prompts with bi-level optimization.


<details>
  <summary>Details</summary>
Motivation: Existing federated prompt learning approaches struggle with balancing performance and robustness, especially under OOD shifts, due to data heterogeneity among clients.

Method: FOCoOp uses ID global prompts, local prompts, and OOD prompts for class- and distribution-level separations, optimized via bi-level distributionally robust optimization and semi-unbalanced optimal transport.

Result: Experiments show FOCoOp effectively handles decentralized heterogeneous distributions and enhances robustness against OOD shifts.

Conclusion: FOCoOp successfully addresses the trade-off in federated prompt learning, improving reliability in real-world scenarios.

Abstract: Federated prompt learning (FPL) for vision-language models is a powerful
approach to collaboratively adapt models across distributed clients while
preserving data privacy. However, existing FPL approaches suffer from a
trade-off between performance and robustness, particularly in
out-of-distribution (OOD) shifts, limiting their reliability in real-world
scenarios. The inherent in-distribution (ID) data heterogeneity among different
clients makes it more challenging to maintain this trade-off. To fill this gap,
we introduce a Federated OOD-aware Context Optimization (FOCoOp) framework,
which captures diverse distributions among clients using ID global prompts,
local prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of
prompts to create both class-level and distribution-level separations, which
adapt to OOD shifts through bi-level distributionally robust optimization.
Additionally, FOCoOp improves the discrimination consistency among clients,
i.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by
semi-unbalanced optimal transport. The extensive experiments on real-world
datasets demonstrate that FOCoOp effectively captures decentralized
heterogeneous distributions and enhances robustness of different OOD shifts.
The project is available at GitHub.

</details>


### [115] [R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision](https://arxiv.org/abs/2506.16262)
*Weeyoung Kwon,Jeahun Sung,Minkyu Jeon,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: The paper surveys 3D Low-Level Vision (3D LLV) for robust rendering, restoration, and enhancement in degraded conditions, addressing challenges in neural rendering methods like NeRF and 3DGS.


<details>
  <summary>Details</summary>
Motivation: Existing neural rendering models assume clean, high-resolution inputs, limiting their robustness to real-world degradations like noise, blur, and weather artifacts.

Method: The survey formalizes degradation-aware rendering, categorizes recent methods integrating LLV into neural rendering, and reviews datasets and evaluation protocols.

Result: 3D LLV enables high-fidelity 3D reconstruction under adverse conditions, with applications in autonomous driving, AR/VR, and robotics.

Conclusion: 3D LLV is a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments.

Abstract: Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have achieved significant progress in photorealistic
3D scene reconstruction and novel view synthesis. However, most existing models
assume clean and high-resolution (HR) multi-view inputs, which limits their
robustness under real-world degradations such as noise, blur, low-resolution
(LR), and weather-induced artifacts. To address these limitations, the emerging
field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision
tasks including super-resolution (SR), deblurring, weather degradation removal,
restoration, and enhancement into the 3D spatial domain. This survey, referred
to as R\textsuperscript{3}eVision, provides a comprehensive overview of robust
rendering, restoration, and enhancement for 3D LLV by formalizing the
degradation-aware rendering problem and identifying key challenges related to
spatio-temporal consistency and ill-posed optimization. Recent methods that
integrate LLV into neural rendering frameworks are categorized to illustrate
how they enable high-fidelity 3D reconstruction under adverse conditions.
Application domains such as autonomous driving, AR/VR, and robotics are also
discussed, where reliable 3D perception from degraded inputs is critical. By
reviewing representative methods, datasets, and evaluation protocols, this work
positions 3D LLV as a fundamental direction for robust 3D content generation
and scene-level reconstruction in real-world environments.

</details>


### [116] [Dense 3D Displacement Estimation for Landslide Monitoring via Fusion of TLS Point Clouds and Embedded RGB Images](https://arxiv.org/abs/2506.16265)
*Zhaoyi Wang,Jemil Avers Butt,Shengyu Huang,Tomislav Medic,Andreas Wieser*

Main category: cs.CV

TL;DR: A hierarchical coarse-to-fine method fuses 3D point clouds and RGB images for dense 3D landslide displacement estimation, outperforming state-of-the-art in coverage and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for landslide monitoring rely on sparse or non-3D displacement estimates, lacking spatial coverage and accuracy.

Method: Proposes a hierarchical partition-based approach combining 3D point clouds and RGB images, using patch-level matches and geometric consistency checks.

Result: Achieves high spatial coverage (79% and 97%) and accuracy (deviations of 0.15 m and 0.25 m), outperforming F2S3.

Conclusion: The method provides a practical, adaptable solution for TLS-based landslide monitoring, extensible to other point clouds and tasks.

Abstract: Landslide monitoring is essential for understanding geohazards and mitigating
associated risks. However, existing point cloud-based methods typically rely on
either geometric or radiometric information and often yield sparse or non-3D
displacement estimates. In this paper, we propose a hierarchical
partition-based coarse-to-fine approach that fuses 3D point clouds and
co-registered RGB images to estimate dense 3D displacement vector fields. We
construct patch-level matches using both 3D geometry and 2D image features.
These matches are refined via geometric consistency checks, followed by rigid
transformation estimation per match. Experimental results on two real-world
landslide datasets demonstrate that our method produces 3D displacement
estimates with high spatial coverage (79% and 97%) and high accuracy.
Deviations in displacement magnitude with respect to external measurements
(total station or GNSS observations) are 0.15 m and 0.25 m on the two datasets,
respectively, and only 0.07 m and 0.20 m compared to manually derived
references. These values are below the average scan resolutions (0.08 m and
0.30 m). Our method outperforms the state-of-the-art method F2S3 in spatial
coverage while maintaining comparable accuracy. Our approach offers a practical
and adaptable solution for TLS-based landslide monitoring and is extensible to
other types of point clouds and monitoring tasks. Our example data and source
code are publicly available at https://github.com/zhaoyiww/fusion4landslide.

</details>


### [117] [Fine-grained Image Retrieval via Dual-Vision Adaptation](https://arxiv.org/abs/2506.16273)
*Xin Jiang,Meiqi Cao,Hao Tang,Fei Shen,Zechao Li*

Main category: cs.CV

TL;DR: DVA improves FGIR by adapting pre-trained models with sample and feature adjustments, avoiding overfitting and enhancing generalization.


<details>
  <summary>Details</summary>
Motivation: Current FGIR methods overfit training data and lose pre-training knowledge, reducing generalization.

Method: DVA uses Object-Perceptual Adaptation (modifies samples) and In-Context Adaptation (adjusts features) without altering pre-trained parameters. Discrimination Perception Transfer balances efficiency and performance.

Result: DVA performs well on six datasets with fewer parameters.

Conclusion: DVA effectively adapts pre-trained models for FGIR, improving performance and generalization.

Abstract: Fine-Grained Image Retrieval~(FGIR) faces challenges in learning
discriminative visual representations to retrieve images with similar
fine-grained features. Current leading FGIR solutions typically follow two
regimes: enforce pairwise similarity constraints in the semantic embedding
space, or incorporate a localization sub-network to fine-tune the entire model.
However, such two regimes tend to overfit the training data while forgetting
the knowledge gained from large-scale pre-training, thus reducing their
generalization ability. In this paper, we propose a Dual-Vision Adaptation
(DVA) approach for FGIR, which guides the frozen pre-trained model to perform
FGIR through collaborative sample and feature adaptation. Specifically, we
design Object-Perceptual Adaptation, which modifies input samples to help the
pre-trained model perceive critical objects and elements within objects that
are helpful for category prediction. Meanwhile, we propose In-Context
Adaptation, which introduces a small set of parameters for feature adaptation
without modifying the pre-trained parameters. This makes the FGIR task using
these adjusted features closer to the task solved during the pre-training.
Additionally, to balance retrieval efficiency and performance, we propose
Discrimination Perception Transfer to transfer the discriminative knowledge in
the object-perceptual adaptation to the image encoder using the knowledge
distillation mechanism. Extensive experiments show that DVA has fewer learnable
parameters and performs well on three in-distribution and three
out-of-distribution fine-grained datasets.

</details>


### [118] [SycnMapV2: Robust and Adaptive Unsupervised Segmentation](https://arxiv.org/abs/2506.16297)
*Heng Zhang,Zikang Wan,Danilo Vasconcellos Vargas*

Main category: cs.CV

TL;DR: SyncMapV2 is an unsupervised segmentation algorithm with unmatched robustness, outperforming SOTA methods under various corruptions without robust training or supervision.


<details>
  <summary>Details</summary>
Motivation: Human vision's robustness to noise contrasts with AI's struggles, motivating the development of SyncMapV2 for unsupervised, adaptable segmentation.

Method: SyncMapV2 uses self-organizing dynamical equations and random network concepts, adapting online without re-initialization.

Result: SyncMapV2 shows minimal mIoU drop (0.01%) under corruption, significantly outperforming SOTA methods in noise, weather, and blur scenarios.

Conclusion: SyncMapV2 pioneers online adaptability and robustness, paving the way for future adaptive AI systems.

Abstract: Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods.This superior performance extends across various types
of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%
vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,
supervision, or loss functions. It is based on a learning paradigm that uses
self-organizing dynamical equations combined with concepts from random
networks. Moreover,unlike conventional methods that require re-initialization
for each new input, SyncMapV2 adapts online, mimicking the continuous
adaptability of human vision. Thus, we go beyond the accurate and robust
results, and present the first algorithm that can do all the above online,
adapting to input rather than re-initializing. In adaptability tests, SyncMapV2
demonstrates near-zero performance degradation, which motivates and fosters a
new generation of robust and adaptive intelligence in the near future.

</details>


### [119] [Learning Multi-scale Spatial-frequency Features for Image Denoising](https://arxiv.org/abs/2506.16307)
*Xu Zhao,Chen Zhao,Xiantao Hu,Hongliang Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: MADNet introduces a multi-scale adaptive dual-domain network for image denoising, addressing limitations of fixed Unet architectures and uniform frequency domain treatment.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on fixed single-input single-output Unet architectures and treat frequency domains uniformly, ignoring multi-scale representations and noise characteristics.

Method: Proposes MADNet with image pyramid inputs and an adaptive spatial-frequency learning unit (ASFU) for high/low-frequency separation. Includes a global feature fusion block in skip connections.

Result: MADNet outperforms state-of-the-art denoising methods on synthetic and real noisy datasets.

Conclusion: MADNet effectively addresses multi-scale and frequency domain challenges in image denoising, demonstrating superior performance.

Abstract: Recent advancements in multi-scale architectures have demonstrated
exceptional performance in image denoising tasks. However, existing
architectures mainly depends on a fixed single-input single-output Unet
architecture, ignoring the multi-scale representations of pixel level. In
addition, previous methods treat the frequency domain uniformly, ignoring the
different characteristics of high-frequency and low-frequency noise. In this
paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for
image denoising. We use image pyramid inputs to restore noise-free results from
low-resolution images. In order to realize the interaction of high-frequency
and low-frequency information, we design an adaptive spatial-frequency learning
unit (ASFU), where a learnable mask is used to separate the information into
high-frequency and low-frequency components. In the skip connections, we design
a global feature fusion block to enhance the features at different scales.
Extensive experiments on both synthetic and real noisy image datasets verify
the effectiveness of MADNet compared with current state-of-the-art denoising
approaches.

</details>


### [120] [Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation](https://arxiv.org/abs/2506.16318)
*Carmelo Scribano,Elena Govi,Paolo bertellini,Simone Parisi,Giorgia Franchini,Marko Bertogna*

Main category: cs.CV

TL;DR: A pipeline for agricultural field boundary mapping using a fine-tuned Segment Anything Model (SAM) is introduced, with a new regional dataset (ERAS) to enhance coverage and accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate field boundary mapping is crucial for efficient agriculture, and automated methods can reduce reliance on costly ground surveys.

Method: The paper proposes a fine-tuning strategy for SAM to adapt it to field delineation, alongside a method for acquiring a complementary regional dataset (ERAS).

Result: Extensive experiments show the approach's segmentation accuracy and generalization capabilities, establishing a robust baseline for automated delineation.

Conclusion: The fine-tuned SAM and ERAS dataset provide an effective solution for field boundary mapping, with the dataset now publicly available.

Abstract: Accurate mapping of agricultural field boundaries is essential for the
efficient operation of agriculture. Automatic extraction from high-resolution
satellite imagery, supported by computer vision techniques, can avoid costly
ground surveys. In this paper, we present a pipeline for field delineation
based on the Segment Anything Model (SAM), introducing a fine-tuning strategy
to adapt SAM to this task. In addition to using published datasets, we describe
a method for acquiring a complementary regional dataset that covers areas
beyond current sources. Extensive experiments assess segmentation accuracy and
evaluate the generalization capabilities. Our approach provides a robust
baseline for automated field delineation. The new regional dataset, known as
ERAS, is now publicly available.

</details>


### [121] [RealDriveSim: A Realistic Multi-Modal Multi-Task Synthetic Dataset for Autonomous Driving](https://arxiv.org/abs/2506.16319)
*Arpit Jadon,Haoran Wang,Phillip Thomas,Michael Stanley,S. Nathaniel Cibik,Rachel Laurat,Omar Maher,Lukas Hoyer,Ozan Unal,Dengxin Dai*

Main category: cs.CV

TL;DR: RealDriveSim is a realistic multi-modal synthetic dataset for autonomous driving, supporting 2D and LiDAR applications with fine-grained annotations for 64 classes, outperforming existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: The high cost and limited scope of current synthetic datasets hinder scalability for perception models, necessitating a more versatile and realistic solution.

Method: Developed RealDriveSim, a multi-modal synthetic dataset with fine-grained annotations for 64 classes, evaluated across various applications.

Result: Achieved state-of-the-art performance compared to existing synthetic benchmarks.

Conclusion: RealDriveSim addresses scalability and realism gaps in synthetic datasets, offering a publicly available resource for autonomous driving research.

Abstract: As perception models continue to develop, the need for large-scale datasets
increases. However, data annotation remains far too expensive to effectively
scale and meet the demand. Synthetic datasets provide a solution to boost model
performance with substantially reduced costs. However, current synthetic
datasets remain limited in their scope, realism, and are designed for specific
tasks and applications. In this work, we present RealDriveSim, a realistic
multi-modal synthetic dataset for autonomous driving that not only supports
popular 2D computer vision applications but also their LiDAR counterparts,
providing fine-grained annotations for up to 64 classes. We extensively
evaluate our dataset for a wide range of applications and domains,
demonstrating state-of-the-art results compared to existing synthetic
benchmarks. The dataset is publicly available at
https://realdrivesim.github.io/.

</details>


### [122] [Reliable Few-shot Learning under Dual Noises](https://arxiv.org/abs/2506.16330)
*Ji Zhang,Jingkuan Song,Lianli Gao,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: DETA++ is a noise-robust few-shot learning method that addresses ID and OOD noise in task adaptation using contrastive relevance aggregation, a memory bank, and intra-class region swapping.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot learning methods struggle with ID and OOD noise in support and query samples, leading to unreliable predictions. DETA++ aims to mitigate these issues.

Method: DETA++ employs a CoRA module for support sample weighting, a clean prototype loss, noise entropy maximization, a memory bank for clean region storage, LocalNCC for query predictions, and IntraSwap for prototype rectification.

Result: Extensive experiments show DETA++ effectively handles dual noises, improving reliability in few-shot learning.

Conclusion: DETA++ provides a robust and flexible solution for noise-affected few-shot learning tasks.

Abstract: Recent advances in model pre-training give rise to task adaptation-based
few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic
model for capturing task-specific knowledge with a few-labeled support samples
of the target task.Nevertheless, existing approaches may still fail in the open
world due to the inevitable in-distribution (ID) and out-of-distribution (OOD)
noise from both support and query samples of the target task. With limited
support samples available, i) the adverse effect of the dual noises can be
severely amplified during task adaptation, and ii) the adapted model can
produce unreliable predictions on query samples in the presence of the dual
noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable
FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate
image and region weights for support samples, based on which a clean prototype
loss and a noise entropy maximization loss are proposed to achieve noise-robust
task adaptation. Additionally,DETA++ employs a memory bank to store and refine
clean regions for each inner-task class, based on which a Local Nearest
Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on
query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping
(IntraSwap) strategy to rectify ID class prototypes during task adaptation,
enhancing the model's robustness to the dual noises. Extensive experiments
demonstrate the effectiveness and flexibility of DETA++.

</details>


### [123] [Transparency Techniques for Neural Networks trained on Writer Identification and Writer Verification](https://arxiv.org/abs/2506.16331)
*Viktoria Pundy,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: The paper applies two transparency techniques to neural networks for Writer Identification and Verification, finding pixel-wise saliency maps more effective than point-specific ones for forensic analysis.


<details>
  <summary>Details</summary>
Motivation: To improve the transparency of neural networks in Writer Identification and Verification, aiding forensic experts by revealing how networks identify similarities in handwritten text.

Method: Two transparency techniques are used: pixel-level saliency maps and point-specific saliency maps, evaluated with deletion and insertion score metrics.

Result: Pixel-wise saliency maps outperform point-specific ones and align well with areas forensic experts focus on.

Conclusion: Pixel-wise saliency maps are effective for supporting forensic experts in analyzing handwritten text similarities.

Abstract: Neural Networks are the state of the art for many tasks in the computer
vision domain, including Writer Identification (WI) and Writer Verification
(WV). The transparency of these "black box" systems is important for
improvements of performance and reliability. For this work, two transparency
techniques are applied to neural networks trained on WI and WV for the first
time in this domain. The first technique provides pixel-level saliency maps,
while the point-specific saliency maps of the second technique provide
information on similarities between two images. The transparency techniques are
evaluated using deletion and insertion score metrics. The goal is to support
forensic experts with information on similarities in handwritten text and to
explore the characteristics selected by a neural network for the identification
process. For the qualitative evaluation, the highlights of the maps are
compared to the areas forensic experts consider during the identification
process. The evaluation results show that the pixel-wise saliency maps
outperform the point-specific saliency maps and are suitable for the support of
forensic experts.

</details>


### [124] [MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval](https://arxiv.org/abs/2506.16353)
*Chao He,Hongxi Wei*

Main category: cs.CV

TL;DR: MambaHash, a visual state space hashing model, leverages Vision Mamba for efficient large-scale image retrieval, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Explore the suitability of Vision Mamba for large-scale image retrieval tasks, given its linear time complexity and strong performance in computer vision.

Method: Proposes a backbone network with grouped Mamba operations for local/global modeling, channel interaction attention for cross-channel communication, and adaptive feature enhancement for diversity.

Result: Outperforms state-of-the-art deep hashing methods on CIFAR-10, NUS-WIDE, and IMAGENET datasets, demonstrating efficiency and superior performance.

Conclusion: MambaHash effectively addresses large-scale image retrieval with improved efficiency and performance, validated by comprehensive experiments.

Abstract: Deep image hashing aims to enable effective large-scale image retrieval by
mapping the input images into simple binary hash codes through deep neural
networks. More recently, Vision Mamba with linear time complexity has attracted
extensive attention from researchers by achieving outstanding performance on
various computer tasks. Nevertheless, the suitability of Mamba for large-scale
image retrieval tasks still needs to be explored. Towards this end, we propose
a visual state space hashing model, called MambaHash. Concretely, we propose a
backbone network with stage-wise architecture, in which grouped Mamba operation
is introduced to model local and global information by utilizing Mamba to
perform multi-directional scanning along different groups of the channel.
Subsequently, the proposed channel interaction attention module is used to
enhance information communication across channels. Finally, we meticulously
design an adaptive feature enhancement module to increase feature diversity and
enhance the visual representation capability of the model. We have conducted
comprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and
IMAGENET. The experimental results demonstrate that compared with the
state-of-the-art deep hashing methods, our proposed MambaHash has well
efficiency and superior performance to effectively accomplish large-scale image
retrieval tasks. Source code is available
https://github.com/shuaichaochao/MambaHash.git

</details>


### [125] [Prompt-based Dynamic Token Pruning to Guide Transformer Attention in Efficient Segmentation](https://arxiv.org/abs/2506.16369)
*Pallabi Dutta,Anubhab Maity,Sushmita Mitra*

Main category: cs.CV

TL;DR: Proposes an adaptive prompt-guided pruning method for Vision Transformers (ViTs) to reduce irrelevant token processing in medical image segmentation, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: High computational demands of ViTs limit their practical use in medical image analysis, necessitating a method to reduce irrelevant token processing.

Method: Uses prompt-based spatial prior to rank and prune low-relevance tokens, integrating with state-of-the-art models for end-to-end training and gradient flow.

Result: Reduces tokens by ~35-55%, lowering computational costs while maintaining segmentation accuracy.

Conclusion: The framework enables cost-effective, real-time medical image processing, expanding applicability in resource-constrained environments.

Abstract: The high computational demands of Vision Transformers (ViTs), in processing a
huge number of tokens, often constrain their practical application in analyzing
medical images. This research proposes an adaptive prompt-guided pruning method
to selectively reduce the processing of irrelevant tokens in the segmentation
pipeline. The prompt-based spatial prior helps to rank the tokens according to
their relevance. Tokens with low-relevance scores are down-weighted, ensuring
that only the relevant ones are propagated for processing across subsequent
stages. This data-driven pruning strategy facilitates end-to-end training,
maintains gradient flow, and improves segmentation accuracy by focusing
computational resources on essential regions. The proposed framework is
integrated with several state-of-the-art models to facilitate the elimination
of irrelevant tokens; thereby, enhancing computational efficiency while
preserving segmentation accuracy. The experimental results show a reduction of
$\sim$ 35-55\% tokens; thus reducing the computational costs relative to the
baselines. Cost-effective medical image processing, using our framework,
facilitates real-time diagnosis by expanding its applicability in
resource-constrained environments.

</details>


### [126] [AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios](https://arxiv.org/abs/2506.16371)
*Yunhao Hou,Bochao Zou,Min Zhang,Ran Chen,Shangdong Yang,Yanmei Zhang,Junbao Zhuo,Siheng Chen,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: AGC-Drive is the first large-scale real-world dataset for aerial-ground cooperative 3D perception, addressing the lack of high-quality data in this domain. It includes 120K LiDAR frames, 440K images, and covers diverse driving scenarios with dynamic interactions.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack aerial perspectives (UAVs) in collaborative perception, which can provide dynamic, top-down views to mitigate occlusions and monitor large-scale environments.

Method: The dataset is collected using two vehicles (each with five cameras and one LiDAR) and one UAV (with a camera and LiDAR). It includes 14 diverse driving scenarios, 400 scenes, and fully annotated 3D bounding boxes for 13 object categories.

Result: AGC-Drive provides benchmarks for vehicle-to-vehicle and vehicle-to-UAV collaborative perception tasks. It also includes an open-source toolkit for alignment, visualization, and annotation.

Conclusion: AGC-Drive fills a critical gap in aerial-ground collaborative perception research and offers valuable resources for advancing multi-agent perception systems.

Abstract: By sharing information across multiple agents, collaborative perception helps
autonomous vehicles mitigate occlusions and improve overall perception
accuracy. While most previous work focus on vehicle-to-vehicle and
vehicle-to-infrastructure collaboration, with limited attention to aerial
perspectives provided by UAVs, which uniquely offer dynamic, top-down views to
alleviate occlusions and monitor large-scale interactive environments. A major
reason for this is the lack of high-quality datasets for aerial-ground
collaborative scenarios. To bridge this gap, we present AGC-Drive, the first
large-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The
data collection platform consists of two vehicles, each equipped with five
cameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and
a LiDAR sensor, enabling comprehensive multi-view and multi-agent perception.
Consisting of approximately 120K LiDAR frames and 440K images, the dataset
covers 14 diverse real-world driving scenarios, including urban roundabouts,
highway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic
interaction events, including vehicle cut-ins, cut-outs, and frequent lane
changes. AGC-Drive contains 400 scenes, each with approximately 100 frames and
fully annotated 3D bounding boxes covering 13 object categories. We provide
benchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative
perception and vehicle-to-UAV collaborative perception. Additionally, we
release an open-source toolkit, including spatiotemporal alignment verification
tools, multi-agent visualization systems, and collaborative annotation
utilities. The dataset and code are available at
https://github.com/PercepX/AGC-Drive.

</details>


### [127] [CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset](https://arxiv.org/abs/2506.16385)
*Santosh Patapati,Trisanth Srinivasan,Amith Adiraju*

Main category: cs.CV

TL;DR: The paper introduces CLIP-MG, a modified CLIP model for micro-gesture recognition, achieving 61.82% Top-1 accuracy on the iMiGUE dataset.


<details>
  <summary>Details</summary>
Motivation: Micro-gestures are subtle and involuntary, making recognition challenging in affective computing.

Method: A Pose-Guided Semantics-Aware CLIP-based architecture (CLIP-MG) integrates human pose data via semantic query generation and gated multi-modal fusion.

Result: CLIP-MG achieves a Top-1 accuracy of 61.82%.

Conclusion: The approach shows promise but highlights the difficulty in adapting vision-language models like CLIP for micro-gesture recognition.

Abstract: Micro-gesture recognition is a challenging task in affective computing due to
the subtle, involuntary nature of the gestures and their low movement
amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based
architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP
model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG
integrates human pose (skeleton) information into the CLIP-based recognition
pipeline through pose-guided semantic query generation and a gated multi-modal
fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These
results demonstrate both the potential of our approach and the remaining
difficulty in fully adapting vision-language models like CLIP for micro-gesture
recognition.

</details>


### [128] [HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis](https://arxiv.org/abs/2506.16398)
*Peixiang Huang,Yanyan Huang,Weiqin Zhao,Junjun He,Lequan Yu*

Main category: cs.CV

TL;DR: HyperPath leverages hyperbolic space and textual descriptions to model semantic hierarchies in WSIs, improving classification via geometry-aware methods.


<details>
  <summary>Details</summary>
Motivation: Existing MIL methods for WSI analysis rely on Euclidean embeddings, which inadequately capture semantic hierarchies.

Method: HyperPath integrates visual and textual features in hyperbolic space, using Angular Modality Alignment Loss and Semantic Hierarchy Consistency Loss for robust alignment and coherence.

Result: Superior performance in WSI classification tasks compared to existing methods.

Conclusion: Hyperbolic embeddings enhance WSI analysis, demonstrating the potential of geometry-aware approaches.

Abstract: Pathology is essential for cancer diagnosis, with multiple instance learning
(MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural
hierarchy -- patches, regions, and slides -- with distinct semantic
associations. While some methods attempt to leverage this hierarchy for
improved representation, they predominantly rely on Euclidean embeddings, which
struggle to fully capture semantic hierarchies. To address this limitation, we
propose HyperPath, a novel method that integrates knowledge from textual
descriptions to guide the modeling of semantic hierarchies of WSIs in
hyperbolic space, thereby enhancing WSI classification. Our approach adapts
both visual and textual features extracted by pathology vision-language
foundation models to the hyperbolic space. We design an Angular Modality
Alignment Loss to ensure robust cross-modal alignment, while a Semantic
Hierarchy Consistency Loss further refines feature hierarchies through
entailment and contradiction relationships and thus enhance semantic coherence.
The classification is performed with geodesic distance, which measures the
similarity between entities in the hyperbolic semantic hierarchy. This
eliminates the need for linear classifiers and enables a geometry-aware
approach to WSI analysis. Extensive experiments show that our method achieves
superior performance across tasks compared to existing methods, highlighting
the potential of hyperbolic embeddings for WSI analysis.

</details>


### [129] [Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks](https://arxiv.org/abs/2506.16407)
*Dong Nguyen Tien,Dung D. Le*

Main category: cs.CV

TL;DR: The paper introduces a unified framework for generating and evaluating multi-modal adversarial attacks on OCR-based Visual Document Understanding (VDU) models, demonstrating their vulnerability to line-level and compound perturbations.


<details>
  <summary>Details</summary>
Motivation: To explore the robustness of VDU systems under realistic adversarial perturbations, which remains understudied.

Method: A framework covering six gradient-based layout attack scenarios, manipulating OCR bounding boxes, pixels, and texts at word and line granularities with layout constraints.

Result: Line-level attacks and compound perturbations cause the most severe performance degradation; PGD-based BBox perturbations outperform random-shift baselines.

Conclusion: The study highlights the vulnerability of VDU models to adversarial attacks and validates the impact of layout budget, text modification, and adversarial transferability.

Abstract: Visual Document Understanding (VDU) systems have achieved strong performance
in information extraction by integrating textual, layout, and visual signals.
However, their robustness under realistic adversarial perturbations remains
insufficiently explored. We introduce the first unified framework for
generating and evaluating multi-modal adversarial attacks on OCR-based VDU
models. Our method covers six gradient-based layout attack scenarios,
incorporating manipulations of OCR bounding boxes, pixels, and texts across
both word and line granularities, with constraints on layout perturbation
budget (e.g., IoU >= 0.6) to preserve plausibility.
  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and
six model families demonstrate that line-level attacks and compound
perturbations (BBox + Pixel + Text) yield the most severe performance
degradation. Projected Gradient Descent (PGD)-based BBox perturbations
outperform random-shift baselines in all investigated models. Ablation studies
further validate the impact of layout budget, text modification, and
adversarial transferability.

</details>


### [130] [Efficient Transformations in Deep Learning Convolutional Neural Networks](https://arxiv.org/abs/2506.16418)
*Berk Yilmaz,Daniel Fidel Harvey,Prajit Dhuri*

Main category: cs.CV

TL;DR: Integration of WHT in ResNet50 improves accuracy (79%) and reduces energy consumption (39 kJ) compared to baseline (66%, 25,606 kJ).


<details>
  <summary>Details</summary>
Motivation: To assess trade-offs between computational efficiency, energy consumption, and accuracy in CNNs using signal processing transforms.

Method: Modified ResNet50 with WHT in early/late layers, tested on CIFAR-100.

Result: WHT-enhanced models achieved higher accuracy (74%-79%) with drastically lower energy use (39 kJ vs. 25,606 kJ).

Conclusion: WHT is efficient and effective for energy-constrained CNN applications.

Abstract: This study investigates the integration of signal processing transformations
-- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete
Cosine Transform (DCT) -- within the ResNet50 convolutional neural network
(CNN) model for image classification. The primary objective is to assess the
trade-offs between computational efficiency, energy consumption, and
classification accuracy during training and inference. Using the CIFAR-100
dataset (100 classes, 60,000 images), experiments demonstrated that
incorporating WHT significantly reduced energy consumption while improving
accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy
of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified
ResNet50 incorporating WHT in the early convolutional layers achieved 74%
accuracy, and an enhanced version with WHT applied to both early and late
layers achieved 79% accuracy, with an average energy consumption of only 39 kJ
per model. These results demonstrate the potential of WHT as a highly efficient
and effective approach for energy-constrained CNN applications.

</details>


### [131] [Structured Semantic 3D Reconstruction (S23DR) Challenge 2025 -- Winning solution](https://arxiv.org/abs/2506.16421)
*Jan Skvrna,Lukas Neumann*

Main category: cs.CV

TL;DR: Winning solution for S23DR Challenge 2025: a 3D deep learning method for predicting house roof wireframes from sparse point clouds and semantic segmentations, achieving a top HSS of 0.43.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting accurate 3D roof wireframes from limited input data (sparse point clouds and semantic segmentations).

Method: A two-stage 3D deep learning approach: (1) identifies vertex candidates from COLMAP point cloud using Gestalt segmentations, (2) refines and classifies vertices and predicts edges using two PointNet-like models.

Result: Achieved a winning Hybrid Structure Score (HSS) of 0.43 on the private leaderboard.

Conclusion: The proposed method effectively predicts 3D roof wireframes, demonstrating the potential of 3D deep learning for such tasks.

Abstract: This paper presents the winning solution for the S23DR Challenge 2025, which
involves predicting a house's 3D roof wireframe from a sparse point cloud and
semantic segmentations. Our method operates directly in 3D, first identifying
vertex candidates from the COLMAP point cloud using Gestalt segmentations. We
then employ two PointNet-like models: one to refine and classify these
candidates by analyzing local cubic patches, and a second to predict edges by
processing the cylindrical regions connecting vertex pairs. This two-stage, 3D
deep learning approach achieved a winning Hybrid Structure Score (HSS) of 0.43
on the private leaderboard.

</details>


### [132] [How Far Can Off-the-Shelf Multimodal Large Language Models Go in Online Episodic Memory Question Answering?](https://arxiv.org/abs/2506.16450)
*Giuseppe Lando,Rosario Forte,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: Off-the-shelf MLLMs achieve competitive performance in OEM-VQA without training, using lightweight textual memory and minimal storage.


<details>
  <summary>Details</summary>
Motivation: To explore if MLLMs can handle OEM-VQA efficiently without additional training, leveraging streaming video and minimal memory.

Method: Converts streaming video into small textual memory via MLLM descriptor, answers questions using LLM reasoner.

Result: 56.0% accuracy on QAEgo4D-Closed benchmark with 3.6 kB/min storage, matching SOTAs while being highly memory-efficient.

Conclusion: Demonstrates MLLMs' potential for OEM-VQA, with insights for future improvements.

Abstract: We investigate whether off-the-shelf Multimodal Large Language Models (MLLMs)
can tackle Online Episodic-Memory Video Question Answering (OEM-VQA) without
additional training. Our pipeline converts a streaming egocentric video into a
lightweight textual memory, only a few kilobytes per minute, via an MLLM
descriptor module, and answers multiple-choice questions by querying this
memory with an LLM reasoner module. On the QAEgo4D-Closed benchmark, our best
configuration attains 56.0% accuracy with 3.6 kB per minute storage, matching
the performance of dedicated state-of-the-art systems while being 10**4/10**5
times more memory-efficient. Extensive ablations provides insights into the
role of each component and design choice, and highlight directions of
improvement for future research.

</details>


### [133] [Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors](https://arxiv.org/abs/2506.16497)
*Riccardo Ziglio,Cecilia Pasquini,Silvio Ranise*

Main category: cs.CV

TL;DR: The paper evaluates CNN-based models for detecting face-swapping in videos, focusing on occlusion artifacts. While models perform well within the same dataset, they struggle with generalization across sources.


<details>
  <summary>Details</summary>
Motivation: Face-swapping in videos poses a growing threat, and existing methods rely on visual artifacts like occlusions. This study tests their robustness.

Method: Benchmarked CNN-based models on two datasets (one new) to analyze generalization across sources and swapping algorithms.

Result: Models excel within the same dataset but fail to generalize occlusion-based cues across different sources.

Conclusion: Specialized detection strategies are needed to handle occlusion artifacts robustly across diverse datasets.

Abstract: Face swapping manipulations in video streams represents an increasing threat
in remote video communications, due to advances
  in automated and real-time tools. Recent literature proposes to characterize
and exploit visual artifacts introduced in video frames
  by swapping algorithms when dealing with challenging physical scenes, such as
face occlusions. This paper investigates the
  effectiveness of this approach by benchmarking CNN-based data-driven models
on two data corpora (including a newly collected
  one) and analyzing generalization capabilities with respect to different
acquisition sources and swapping algorithms. The results
  confirm excellent performance of general-purpose CNN architectures when
operating within the same data source, but a significant
  difficulty in robustly characterizing occlusion-based visual cues across
datasets. This highlights the need for specialized detection
  strategies to deal with such artifacts.

</details>


### [134] [Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details](https://arxiv.org/abs/2506.16504)
*Zeqiang Lai,Yunfei Zhao,Haolin Liu,Zibo Zhao,Qingxiang Lin,Huiwen Shi,Xianghui Yang,Mingxin Yang,Shuhui Yang,Yifei Feng,Sheng Zhang,Xin Huang,Di Luo,Fan Yang,Fang Yang,Lifu Wang,Sicong Liu,Yixuan Tang,Yulin Cai,Zebin He,Tian Liu,Yuhong Liu,Jie Jiang,Linus,Jingwei Huang,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D 2.5 introduces a 3D diffusion model suite with improved shape and texture generation, featuring a 10B-parameter shape model (LATTICE) and PBR-based texture upgrades.


<details>
  <summary>Details</summary>
Motivation: To advance 3D asset generation by closing the gap between generated and handcrafted 3D shapes and textures.

Method: Two-stage pipeline with a new shape foundation model (LATTICE) and PBR-based multi-view texture generation.

Result: Outperforms previous methods in shape and texture generation, producing high-fidelity 3D assets.

Conclusion: Hunyuan3D 2.5 sets a new benchmark for 3D asset generation with its scalable and detailed approach.

Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion
models aimed at generating high-fidelity and detailed textured 3D assets.
Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D
2.0, while demonstrating substantial advancements in both shape and texture
generation. In terms of shape generation, we introduce a new shape foundation
model -- LATTICE, which is trained with scaled high-quality datasets,
model-size, and compute. Our largest model reaches 10B parameters and generates
sharp and detailed 3D shape with precise image-3D following while keeping mesh
surface clean and smooth, significantly closing the gap between generated and
handcrafted 3D shapes. In terms of texture generation, it is upgraded with
phyiscal-based rendering (PBR) via a novel multi-view architecture extended
from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D
2.5 significantly outperforms previous methods in both shape and end-to-end
texture generation.

</details>


### [135] [How Hard Is Snow? A Paired Domain Adaptation Dataset for Clear and Snowy Weather: CADC+](https://arxiv.org/abs/2506.16531)
*Mei Qi Tang,Sean Sedwards,Chengjie Huang,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: CADC+ is a paired weather domain adaptation dataset for evaluating 3D object detection in snowy vs. clear conditions, minimizing domain shift.


<details>
  <summary>Details</summary>
Motivation: The lack of realistic paired datasets for snowy and clear weather conditions hinders accurate evaluation of snow's impact on 3D object detection.

Method: CADC+ extends CADC by pairing snowy sequences with clear weather sequences from the same roads and period, reducing unrelated domain shifts.

Result: Snow introduces aleatoric and epistemic uncertainties, acting as noise and a distinct domain, affecting detection performance.

Conclusion: CADC+ enables better evaluation of snow's impact on 3D object detection, highlighting its dual role as noise and a unique domain.

Abstract: The impact of snowfall on 3D object detection performance remains
underexplored. Conducting such an evaluation requires a dataset with sufficient
labelled data from both weather conditions, ideally captured in the same
driving environment. Current driving datasets with LiDAR point clouds either do
not provide enough labelled data in both snowy and clear weather conditions, or
rely on de-snowing methods to generate synthetic clear weather. Synthetic data
often lacks realism and introduces an additional domain shift that confounds
accurate evaluations. To address these challenges, we present CADC+, the first
paired weather domain adaptation dataset for autonomous driving in winter
conditions. CADC+ extends the Canadian Adverse Driving Conditions dataset
(CADC) using clear weather data that was recorded on the same roads and in the
same period as CADC. To create CADC+, we pair each CADC sequence with a clear
weather sequence that matches the snowy sequence as closely as possible. CADC+
thus minimizes the domain shift resulting from factors unrelated to the
presence of snow. We also present some preliminary results using CADC+ to
evaluate the effect of snow on 3D object detection performance. We observe that
snow introduces a combination of aleatoric and epistemic uncertainties, acting
as both noise and a distinct data domain.

</details>


### [136] [From Semantic To Instance: A Semi-Self-Supervised Learning Approach](https://arxiv.org/abs/2506.16563)
*Keyhan Najafian,Farhad Maleki,Lingling Jin,Ian Stavness*

Main category: cs.CV

TL;DR: Proposes a semi-self-supervised learning approach (GLMask) for instance segmentation with minimal manual annotation, achieving high performance in agriculture and general datasets.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of creating large-scale pixel-level annotated datasets for instance segmentation, especially in densely packed, self-occluded agricultural images.

Method: Uses GLMask, an image-mask representation focusing on shape, texture, and pattern, and a pipeline to convert semantic to instance segmentation.

Result: Achieves 98.5% mAP@50 for wheat head segmentation and 12.6% improvement on COCO dataset.

Conclusion: The approach is effective for precision agriculture and general domains with similar data characteristics.

Abstract: Instance segmentation is essential for applications such as automated
monitoring of plant health, growth, and yield. However, extensive effort is
required to create large-scale datasets with pixel-level annotations of each
object instance for developing instance segmentation models that restrict the
use of deep learning in these areas. This challenge is more significant in
images with densely packed, self-occluded objects, which are common in
agriculture. To address this challenge, we propose a semi-self-supervised
learning approach that requires minimal manual annotation to develop a
high-performing instance segmentation model. We design GLMask, an image-mask
representation for the model to focus on shape, texture, and pattern while
minimizing its dependence on color features. We develop a pipeline to generate
semantic segmentation and then transform it into instance-level segmentation.
The proposed approach substantially outperforms the conventional instance
segmentation models, establishing a state-of-the-art wheat head instance
segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed
methodology on the general-purpose Microsoft COCO dataset, achieving a
significant performance improvement of over 12.6% mAP@50. This highlights that
the utility of our proposed approach extends beyond precision agriculture and
applies to other domains, specifically those with similar data characteristics.

</details>


### [137] [SafeTriage: Facial Video De-identification for Privacy-Preserving Stroke Triage](https://arxiv.org/abs/2506.16578)
*Tongan Cai,Haomiao Ni,Wenchao Ma,Yuan Xue,Qian Ma,Rachel Leicht,Kelvin Wong,John Volpi,Stephen T. C. Wong,James Z. Wang,Sharon X. Huang*

Main category: cs.CV

TL;DR: SafeTriage de-identifies patient facial videos for stroke diagnosis using synthetic identities and motion transfer, ensuring privacy and diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Address ethical and privacy challenges in AI-based stroke triage by avoiding reliance on real patient data.

Method: Uses a pretrained video motion transfer model to map patient facial motion onto synthetic identities, with a conditional generative model for adaptation.

Result: Synthetic videos preserve stroke-relevant facial patterns, enabling reliable AI triage while protecting privacy.

Conclusion: SafeTriage offers a secure, ethical solution for data sharing and AI-driven clinical analysis in neurological disorders.

Abstract: Effective stroke triage in emergency settings often relies on clinicians'
ability to identify subtle abnormalities in facial muscle coordination. While
recent AI models have shown promise in detecting such patterns from patient
facial videos, their reliance on real patient data raises significant ethical
and privacy challenges -- especially when training robust and generalizable
models across institutions. To address these concerns, we propose SafeTriage, a
novel method designed to de-identify patient facial videos while preserving
essential motion cues crucial for stroke diagnosis. SafeTriage leverages a
pretrained video motion transfer (VMT) model to map the motion characteristics
of real patient faces onto synthetic identities. This approach retains
diagnostically relevant facial dynamics without revealing the patients'
identities. To mitigate the distribution shift between normal population
pre-training videos and patient population test videos, we introduce a
conditional generative model for visual prompt tuning, which adapts the input
space of the VMT model to ensure accurate motion transfer without needing to
fine-tune the VMT model backbone. Comprehensive evaluation, including
quantitative metrics and clinical expert assessments, demonstrates that
SafeTriage-produced synthetic videos effectively preserve stroke-relevant
facial patterns, enabling reliable AI-based triage. Our evaluations also show
that SafeTriage provides robust privacy protection while maintaining diagnostic
accuracy, offering a secure and ethically sound foundation for data sharing and
AI-driven clinical analysis in neurological disorders.

</details>


### [138] [Spatially-Aware Evaluation of Segmentation Uncertainty](https://arxiv.org/abs/2506.16589)
*Tal Zeevi,El√©onore V. Lieffrig,Lawrence H. Staib,John A. Onofrey*

Main category: cs.CV

TL;DR: Proposed spatially aware metrics for uncertainty evaluation in segmentation, improving alignment with clinical factors and discrimination of uncertainty patterns.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty metrics ignore spatial context and anatomical structure, leading to identical scores for distinct uncertainty patterns.

Method: Three spatially aware metrics incorporating structural and boundary information, validated on prostate zonal segmentation data.

Result: Improved alignment with clinically important factors and better discrimination between meaningful and spurious uncertainty patterns.

Conclusion: Spatially aware metrics enhance uncertainty evaluation in medical segmentation.

Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions.
However, most uncertainty evaluation metrics treat voxels independently,
ignoring spatial context and anatomical structure. As a result, they may assign
identical scores to qualitatively distinct patterns (e.g., scattered vs.
boundary-aligned uncertainty). We propose three spatially aware metrics that
incorporate structural and boundary information and conduct a thorough
validation on medical imaging data from the prostate zonal segmentation
challenge within the Medical Segmentation Decathlon. Our results demonstrate
improved alignment with clinically important factors and better discrimination
between meaningful and spurious uncertainty patterns.

</details>


### [139] [MetaQAP -- A Meta-Learning Approach for Quality-Aware Pretraining in Image Quality Assessment](https://arxiv.org/abs/2506.16601)
*Muhammad Azeem Aslam,Muhammad Hamza,Nisar Ahmed,Gulshan Saleem,Zhu Shuangtong,Hu Hongfei,Xu Wei,Saba Aslam,Wang Jun*

Main category: cs.CV

TL;DR: MetaQAP is a no-reference IQA model using quality-aware pre-training and meta-learning, outperforming benchmarks on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of subjective human perception and complex image distortions in IQA.

Method: Pre-training CNNs on quality-aware data, using a quality-aware loss function, and integrating a meta-learner for ensemble predictions.

Result: Achieved high PLCC/SROCC scores (e.g., 0.9885/0.9812 on LiveCD) and demonstrated generalizability in cross-dataset evaluations.

Conclusion: MetaQAP provides a robust, generalizable framework for practical IQA, advancing no-reference IQA methodologies.

Abstract: Image Quality Assessment (IQA) is a critical task in a wide range of
applications but remains challenging due to the subjective nature of human
perception and the complexity of real-world image distortions. This study
proposes MetaQAP, a novel no-reference IQA model designed to address these
challenges by leveraging quality-aware pre-training and meta-learning. The
model performs three key contributions: pre-training Convolutional Neural
Networks (CNNs) on a quality-aware dataset, implementing a quality-aware loss
function to optimize predictions, and integrating a meta-learner to form an
ensemble model that effectively combines predictions from multiple base models.
Experimental evaluations were conducted on three benchmark datasets: LiveCD,
KonIQ-10K, and BIQ2021. The proposed MetaQAP model achieved exceptional
performance with Pearson Linear Correlation Coefficient (PLCC) and Spearman
Rank Order Correlation Coefficient (SROCC) scores of 0.9885/0.9812 on LiveCD,
0.9702/0.9658 on KonIQ-10K, and 0.884/0.8765 on BIQ2021, outperforming existing
IQA methods. Cross-dataset evaluations further demonstrated the
generalizability of the model, with PLCC and SROCC scores ranging from 0.6721
to 0.8023 and 0.6515 to 0.7805, respectively, across diverse datasets. The
ablation study confirmed the significance of each model component, revealing
substantial performance degradation when critical elements such as the
meta-learner or quality-aware loss function were omitted. MetaQAP not only
addresses the complexities of authentic distortions but also establishes a
robust and generalizable framework for practical IQA applications. By advancing
the state-of-the-art in no-reference IQA, this research provides valuable
insights and methodologies for future improvements and extensions in the field.

</details>


### [140] [Leveraging CNN and IoT for Effective E-Waste Management](https://arxiv.org/abs/2506.16647)
*Ajesh Thangaraj Nadar,Gabriel Nixon Raj,Soham Chandane,Sushant Bhat*

Main category: cs.CV

TL;DR: An IoT and CNN-based system for automated e-waste classification to improve recycling efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the environmental and health risks of improper e-waste disposal by enhancing identification and recycling processes.

Method: Combines IoT with a lightweight CNN for visual and weight-based classification of e-waste components.

Result: Real-time detection of e-waste items like circuit boards and wires improves smart recycling workflows.

Conclusion: The system enhances e-waste processing efficiency and supports better recycling practices.

Abstract: The increasing proliferation of electronic devices in the modern era has led
to a significant surge in electronic waste (e-waste). Improper disposal and
insufficient recycling of e-waste pose serious environmental and health risks.
This paper proposes an IoT-enabled system combined with a lightweight CNN-based
classification pipeline to enhance the identification, categorization, and
routing of e-waste materials. By integrating a camera system and a digital
weighing scale, the framework automates the classification of electronic items
based on visual and weight-based attributes. The system demonstrates how
real-time detection of e-waste components such as circuit boards, sensors, and
wires can facilitate smart recycling workflows and improve overall waste
processing efficiency.

</details>


### [141] [A Comparative Analysis of Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) as Dimensionality Reduction Techniques](https://arxiv.org/abs/2506.16663)
*Michael Gyimadu,Gregory Bell*

Main category: cs.CV

TL;DR: The paper compares PCA and SVD for dimensionality reduction, focusing on interpretability, stability, and matrix suitability, offering guidelines for choosing between them without empirical testing.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical comparison of PCA and SVD for high-dimensional image data, aiding in algorithm selection without empirical benchmarking.

Method: Derived PCA and SVD from first principles, analyzed their interpretability, numerical stability, and suitability for different matrix shapes.

Result: Synthesized guidelines for choosing PCA or SVD based on theoretical properties, without empirical validation.

Conclusion: Highlights the need for future experimental work to validate the guidelines and address limitations.

Abstract: High-dimensional image data often require dimensionality reduction before
further analysis. This paper provides a purely analytical comparison of two
linear techniques-Principal Component Analysis (PCA) and Singular Value
Decomposition (SVD). After the derivation of each algorithm from first
principles, we assess their interpretability, numerical stability, and
suitability for differing matrix shapes. building on classical and recent
numerical literature, We synthesize rule-of-thumb guidelines for choosing one
out of the two algorithms without empirical benchmarking, building on classical
and recent numerical literature. Limitations and directions for future
experimental work are outlined at the end.

</details>


### [142] [Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge](https://arxiv.org/abs/2506.16673)
*Ruiming Chen,Junming Yang,Shiyu Xia,Xu Yang,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: MM-LG extracts multimodal generalizable knowledge from CLIP, reducing pre-training costs and storage while improving performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of pre-training CLIP at different scales and the lack of multimodal generalizable knowledge extraction in existing Learngene paradigms.

Method: Proposes MM-LG, using multimodal and unimodal blocks to extract knowledge, then initializing descendant models with these components.

Result: Achieves performance gains (e.g., +3.1% on Oxford-IIIT PET) and reduces pre-training costs by ~2.8x with 25% parameter storage.

Conclusion: MM-LG is efficient for deploying diverse downstream tasks, outperforming existing methods and matching fine-tuning results.

Abstract: CLIP (Contrastive Language-Image Pre-training) has attracted widespread
attention for its multimodal generalizable knowledge, which is significant for
downstream tasks. However, the computational overhead of a large number of
parameters and large-scale pre-training poses challenges of pre-training a
different scale of CLIP. Learngene extracts the generalizable components termed
as learngene from an ancestry model and initializes diverse descendant models
with it. Previous Learngene paradigms fail to handle the generalizable
knowledge in multimodal scenarios. In this paper, we put forward the idea of
utilizing a multimodal block to extract the multimodal generalizable knowledge,
which inspires us to propose MM-LG (Multimodal Learngene), a novel framework
designed to extract and leverage generalizable components from CLIP.
Specifically, we first establish multimodal and unimodal blocks to extract the
multimodal and unimodal generalizable knowledge in a weighted-sum manner.
Subsequently, we employ these components to numerically initialize descendant
models of varying scales and modalities. Extensive experiments demonstrate
MM-LG's effectiveness, which achieves performance gains over existing learngene
approaches (e.g.,+3.1% on Oxford-IIIT PET and +4.13% on Flickr30k) and
comparable or superior results to the pre-training and fine-tuning paradigm
(e.g.,+1.9% on Oxford-IIIT PET and +3.65% on Flickr30k). Notably, MM-LG
requires only around 25% of the parameter storage while reducing around 2.8
times pre-training costs for diverse model scales compared to the pre-training
and fine-tuning paradigm, making it particularly suitable for efficient
deployment across diverse downstream tasks.

</details>


### [143] [How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions](https://arxiv.org/abs/2506.16679)
*Manuel Brack,Sudeep Katakol,Felix Friedrich,Patrick Schramowski,Hareesh Ravi,Kristian Kersting,Ajinkya Kale*

Main category: cs.CV

TL;DR: The study explores how synthetic captioning strategies affect text-to-image models, finding that dense captions improve alignment but may reduce aesthetics and diversity, while randomized-length captions balance these aspects.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about how synthetic captioning design choices impact text-to-image model performance.

Method: Systematically investigating different synthetic captioning strategies and their effects on model performance.

Result: Dense captions enhance text alignment but may hurt aesthetics and diversity; randomized-length captions balance these trade-offs. Caption distributions also influence output bias.

Conclusion: Caption design is critical for optimal model performance, and the study provides practical insights for better training data strategies.

Abstract: Training data is at the core of any successful text-to-image models. The
quality and descriptiveness of image text are crucial to a model's performance.
Given the noisiness and inconsistency in web-scraped datasets, recent works
shifted towards synthetic training captions. While this setup is generally
believed to produce more capable models, current literature does not provide
any insights into its design choices. This study closes this gap by
systematically investigating how different synthetic captioning strategies
impact the downstream performance of text-to-image models. Our experiments
demonstrate that dense, high-quality captions enhance text alignment but may
introduce trade-offs in output aesthetics and diversity. Conversely, captions
of randomized lengths yield balanced improvements across aesthetics and
alignment without compromising sample diversity. We also demonstrate that
varying caption distributions introduce significant shifts in the output bias
of a trained model. Our findings underscore the importance of caption design in
achieving optimal model performance and provide practical insights for more
effective training data strategies in text-to-image generation.

</details>


### [144] [DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches](https://arxiv.org/abs/2506.16690)
*Yun Xing,Yue Cao,Nhat Chung,Jie Zhang,Ivor Tsang,Ming-Ming Cheng,Yang Liu,Lei Ma,Qing Guo*

Main category: cs.CV

TL;DR: Introducing striped structures in adversarial patches improves stereo depth estimation attacks, outperforming naive repeated textures in both digital and physical settings.


<details>
  <summary>Details</summary>
Motivation: To address the poor performance of naively repeated textures in physical-world adversarial attacks on stereo depth estimation, aiming to enhance practical utility for security testing.

Method: Developed a novel attack by optimizing striped structures and textures jointly, tested on state-of-the-art methods (RAFT-Stereo, STTR) and commercial RGB-D cameras.

Result: The striped structure significantly boosts attack effectiveness, successfully misleading stereo depth estimation in real-world conditions.

Conclusion: The optimized adversarial patches are practical tools for assessing stereo system vulnerabilities, demonstrating real-world applicability.

Abstract: Stereo Depth estimation is a critical task in autonomous driving and
robotics, where inaccuracies (such as misidentifying nearby objects as distant)
can lead to dangerous situations. Adversarial attacks against stereo depth
estimation can help reveal vulnerabilities before deployment. Previous work has
shown that repeating optimized textures can effectively mislead stereo depth
estimation in digital settings. However, our research reveals that these
naively repeated texture structures perform poorly in physical-world
implementations, i.e., when deployed as patches, limiting their practical
utility for testing stereo depth estimation systems. In this work, for the
first time, we discover that introducing regular intervals between repeated
textures, creating a striped structure, significantly enhances the patch attack
effectiveness. Through extensive experimentation, we analyze how variations of
this novel structure influence the performance. Based on these insights, we
develop a novel stereo depth attack that jointly optimizes both the striped
structure and texture elements. Our generated adversarial patches can be
inserted into any scenes and successfully attack state-of-the-art stereo depth
estimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can
also attack commercial RGB-D cameras (Intel RealSense) in real-world
conditions, demonstrating their practical relevance for security assessment of
stereo systems.

</details>


### [145] [LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation](https://arxiv.org/abs/2506.16691)
*Tongtian Yue,Longteng Guo,Yepeng Tang,Zijia Zhao,Xinxin Zhu,Hua Huang,Jing Liu*

Main category: cs.CV

TL;DR: LaVi introduces efficient vision-language fusion in LVLMs by modulating LLM features, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in current LVLMs' visual-language integration, which disrupt structure or increase computational burden.

Method: Uses internal feature modulation via token-wise vision-conditioned deltas in LLM layer normalization, avoiding long-context expansion.

Result: Achieves SOTA performance on 15 benchmarks, reduces FLOPs by 94%, speeds inference 3.1x, and halves memory usage.

Conclusion: LaVi offers a scalable, efficient solution for real-time multimodal reasoning, outperforming existing methods.

Abstract: Despite the impressive advancements of Large Vision-Language Models (LVLMs),
existing approaches suffer from a fundamental bottleneck: inefficient
visual-language integration. Current methods either disrupt the model's
inherent structure or introduce severe long-context computational burden,
severely limiting scalability and efficiency. In this paper, we rethink
multimodal integration and present LaVi, a novel LVLM that enables seamless and
efficient vision-language fusion through internal feature modulation within the
Large Language Models (LLMs). Unlike dominant LVLMs that rely on visual token
concatenation, LaVi bypasses long-context expansion by introducing a
lightweight and adaptive transformation, which incorporates visual context by
injecting token-wise vision-conditioned deltas into the affine parameters of
layer normalization. This mechanism directly modulates linguistic hidden states
based on visual input, ensuring precise vision-language alignment while
preserving the LLM's linguistic priors and drastically reducing computational
costs. Extensive evaluations across 15 image and video benchmarks demonstrate
that LaVi not only achieves state-of-the-art multimodal performance but also
dramatically enhances efficiency. Compared to LLaVA-OV-7B, LaVi reduces FLOPs
by 94.0%, improves inference speed by 3.1 times, and cuts memory usage in half
- establishing LaVi as a scalable and practical solution for real-time
multimodal reasoning. The code and models will be released soon.

</details>


### [146] [Language-driven Description Generation and Common Sense Reasoning for Video Action Recognition](https://arxiv.org/abs/2506.16701)
*Xiaodan Hu,Chuhang Zou,Suchen Wang,Jaechul Kim,Narendra Ahuja*

Main category: cs.CV

TL;DR: A framework leverages language-driven common sense priors to improve video action recognition in cluttered, occluded scenes by combining visual and textual cues.


<details>
  <summary>Details</summary>
Motivation: Existing methods underutilize the rich common sense priors in language models for understanding cluttered video scenes.

Method: Proposes a framework with three components: video context summary, description generation, and multi-modal activity recognition.

Result: Demonstrated effectiveness on Action Genome and Charades datasets.

Conclusion: Incorporating language-driven common sense priors enhances video action recognition in challenging scenarios.

Abstract: Recent video action recognition methods have shown excellent performance by
adapting large-scale pre-trained language-image models to the video domain.
However, language models contain rich common sense priors - the scene contexts
that humans use to constitute an understanding of objects, human-object
interactions, and activities - that have not been fully exploited. In this
paper, we introduce a framework incorporating language-driven common sense
priors to identify cluttered video action sequences from monocular views that
are often heavily occluded. We propose: (1) A video context summary component
that generates candidate objects, activities, and the interactions between
objects and activities; (2) A description generation module that describes the
current scene given the context and infers subsequent activities, through
auxiliary prompts and common sense reasoning; (3) A multi-modal activity
recognition head that combines visual and textual cues to recognize video
actions. We demonstrate the effectiveness of our approach on the challenging
Action Genome and Charades datasets.

</details>


### [147] [Few-Shot Generalized Category Discovery With Retrieval-Guided Decision Boundary Enhancement](https://arxiv.org/abs/2506.16728)
*Yunhan Ren,Feng Luo,Siyu Huang*

Main category: cs.CV

TL;DR: The paper introduces Few-shot Generalized Category Discovery (FSGCD) to address performance limitations in GCD tasks with scarce labeled samples and known categories. It proposes a decision boundary enhancement framework with affinity-based retrieval, achieving superior results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GCD models lack exploration of performance under limited labeled samples and known categories, prompting the need for FSGCD.

Method: A decision boundary enhancement framework with pre-training and two-stage retrieval-guided optimization to refine boundaries using pseudo-labeled samples.

Result: The method outperforms existing approaches on six public GCD benchmarks in the FSGCD setting.

Conclusion: The proposed framework effectively addresses the challenges of FSGCD, demonstrating improved performance and robustness.

Abstract: While existing Generalized Category Discovery (GCD) models have achieved
significant success, their performance with limited labeled samples and a small
number of known categories remains largely unexplored. In this work, we
introduce the task of Few-shot Generalized Category Discovery (FSGCD), aiming
to achieve competitive performance in GCD tasks under conditions of known
information scarcity. To tackle this challenge, we propose a decision boundary
enhancement framework with affinity-based retrieval. Our framework is designed
to learn the decision boundaries of known categories and transfer these
boundaries to unknown categories. First, we use a decision boundary
pre-training module to mitigate the overfitting of pre-trained information on
known category boundaries and improve the learning of these decision boundaries
using labeled samples. Second, we implement a two-stage retrieval-guided
decision boundary optimization strategy. Specifically, this strategy further
enhances the severely limited known boundaries by using affinity-retrieved
pseudo-labeled samples. Then, these refined boundaries are applied to unknown
clusters via guidance from affinity-based feature retrieval. Experimental
results demonstrate that our proposed method outperforms existing methods on
six public GCD benchmarks under the FSGCD setting. The codes are available at:
https://github.com/Ryh1218/FSGCD

</details>


### [148] [TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.16730)
*Mingrui Zhu,Xiru Chen,Xin Wei,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: The paper introduces TeSG, a method for text-guided infrared and visible image fusion, leveraging textual semantics from VLMs to enhance downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Current text-guided IVF lacks effective integration of textual semantic information, limiting its utility for tasks like detection and segmentation.

Method: TeSG uses a Semantic Information Generator (SIG), Mask-Guided Cross-Attention (MGCA), and Text-Driven Attentional Fusion (TDAF) to fuse images guided by mask and text semantics.

Result: Experiments show TeSG outperforms state-of-the-art methods, especially in downstream tasks.

Conclusion: TeSG effectively integrates textual semantics for improved IVF, demonstrating strong performance in practical applications.

Abstract: Infrared and visible image fusion (IVF) aims to combine complementary
information from both image modalities, producing more informative and
comprehensive outputs. Recently, text-guided IVF has shown great potential due
to its flexibility and versatility. However, the effective integration and
utilization of textual semantic information remains insufficiently studied. To
tackle these challenges, we introduce textual semantics at two levels: the mask
semantic level and the text semantic level, both derived from textual
descriptions extracted by large Vision-Language Models (VLMs). Building on
this, we propose Textual Semantic Guidance for infrared and visible image
fusion, termed TeSG, which guides the image synthesis process in a way that is
optimized for downstream tasks such as detection and segmentation.
Specifically, TeSG consists of three core components: a Semantic Information
Generator (SIG), a Mask-Guided Cross-Attention (MGCA) module, and a Text-Driven
Attentional Fusion (TDAF) module. The SIG generates mask and text semantics
based on textual descriptions. The MGCA module performs initial attention-based
fusion of visual features from both infrared and visible images, guided by mask
semantics. Finally, the TDAF module refines the fusion process with gated
attention driven by text semantics. Extensive experiments demonstrate the
competitiveness of our approach, particularly in terms of performance on
downstream tasks, compared to existing state-of-the-art methods.

</details>


### [149] [3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting](https://arxiv.org/abs/2506.16735)
*Yunshan Li,Wenwu Gong,Qianqian Wang,Chao Wang,Lili Yang*

Main category: cs.CV

TL;DR: The paper introduces 3DeepRep, a novel 3-directional deep low-rank tensor representation model for HSI inpainting, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on spectral mode transforms, neglecting low-rank properties in other tensor modes, limiting effectiveness.

Method: 3DeepRep performs deep nonlinear transforms along all three HSI tensor modes, using 3-directional TNN regularization and a learnable aggregation module.

Result: The method achieves superior inpainting performance on real-world HSI datasets, both qualitatively and quantitatively.

Conclusion: 3DeepRep effectively addresses limitations of prior methods by leveraging multi-directional low-rank structures, demonstrating significant improvements in HSI inpainting.

Abstract: Recent approaches based on transform-based tensor nuclear norm (TNN) have
demonstrated notable effectiveness in hyperspectral image (HSI) inpainting by
leveraging low-rank structures in latent representations. Recent developments
incorporate deep transforms to improve low-rank tensor representation; however,
existing approaches typically restrict the transform to the spectral mode,
neglecting low-rank properties along other tensor modes. In this paper, we
propose a novel 3-directional deep low-rank tensor representation (3DeepRep)
model, which performs deep nonlinear transforms along all three modes of the
HSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of
mode-i frontal slices in the corresponding latent space for each direction
(i=1,2,3), forming a 3-directional TNN regularization. The outputs from the
three directional branches are subsequently fused via a learnable aggregation
module to produce the final result. An efficient gradient-based optimization
algorithm is developed to solve the model in a self-supervised manner.
Extensive experiments on real-world HSI datasets demonstrate that the proposed
method achieves superior inpainting performance compared to existing
state-of-the-art techniques, both qualitatively and quantitatively.

</details>


### [150] [Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection](https://arxiv.org/abs/2506.16737)
*Liu Zongzhen,Luo Hui,Wang Zhixing,Wei Yuxing,Zuo Haorui,Zhang Jianlin*

Main category: cs.CV

TL;DR: Proposes CoDAF, a unified framework for weakly aligned UAV object detection, addressing semantic inconsistency and modality conflict via offset-guided alignment and dynamic fusion.


<details>
  <summary>Details</summary>
Motivation: Improve robustness in UAV object detection by addressing spatial misalignment and modality conflict in multimodal (RGB-IR) imagery.

Method: CoDAF integrates Offset-guided Semantic Alignment (OSA) for precise feature alignment and Dynamic Attention-guided Fusion Module (DAFM) for adaptive modality fusion.

Result: Achieves 78.6% mAP on the DroneVehicle dataset, outperforming existing methods.

Conclusion: CoDAF effectively tackles alignment and fusion challenges, enhancing UAV object detection performance.

Abstract: Unmanned aerial vehicle (UAV) object detection plays a vital role in
applications such as environmental monitoring and urban security. To improve
robustness, recent studies have explored multimodal detection by fusing visible
(RGB) and infrared (IR) imagery. However, due to UAV platform motion and
asynchronous imaging, spatial misalignment frequently occurs between
modalities, leading to weak alignment. This introduces two major challenges:
semantic inconsistency at corresponding spatial locations and modality conflict
during feature fusion. Existing methods often address these issues in
isolation, limiting their effectiveness. In this paper, we propose Cross-modal
Offset-guided Dynamic Alignment and Fusion (CoDAF), a unified framework that
jointly tackles both challenges in weakly aligned UAV-based object detection.
CoDAF comprises two novel modules: the Offset-guided Semantic Alignment (OSA),
which estimates attention-based spatial offsets and uses deformable convolution
guided by a shared semantic space to align features more precisely; and the
Dynamic Attention-guided Fusion Module (DAFM), which adaptively balances
modality contributions through gating and refines fused features via
spatial-channel dual attention. By integrating alignment and fusion in a
unified design, CoDAF enables robust UAV object detection. Experiments on
standard benchmarks validate the effectiveness of our approach, with CoDAF
achieving a mAP of 78.6% on the DroneVehicle dataset.

</details>


### [151] [Uncertainty-Aware Variational Information Pursuit for Interpretable Medical Image Analysis](https://arxiv.org/abs/2506.16742)
*Md Nahiduzzaman,Ruwan Tennakoon,Steven Korevaar,Zongyuan Ge,Alireza Bab-Hadiashar*

Main category: cs.CV

TL;DR: UAV-IP improves V-IP by integrating uncertainty quantification, boosting AUC by 3.2% and providing 20% more concise explanations in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Addressing overlooked instance-level uncertainties in V-IP methods to enhance interpretability and trust in AI decision-support systems for medical imaging.

Method: Introduces Uncertainty-Aware V-IP (UAV-IP), integrating uncertainty quantification into V-IP for better query-answer generation.

Result: UAV-IP achieves a 3.2% average AUC improvement and 20% more concise explanations across four datasets (PH2, Derm7pt, BrEaST, SkinCon).

Conclusion: Uncertainty-aware reasoning is crucial for robust, interpretable AI models in medical decision-making.

Abstract: In medical imaging, AI decision-support systems must balance accuracy and
interpretability to build user trust and support effective clinical
decision-making. Recently, Variational Information Pursuit (V-IP) and its
variants have emerged as interpretable-by-design modeling techniques, aiming to
explain AI decisions in terms of human-understandable, clinically relevant
concepts. However, existing V-IP methods overlook instance-level uncertainties
in query-answer generation, which can arise from model limitations (epistemic
uncertainty) or variability in expert responses (aleatoric uncertainty).
  This paper introduces Uncertainty-Aware V-IP (UAV-IP), a novel framework that
integrates uncertainty quantification into the V-IP process. We evaluate UAV-IP
across four medical imaging datasets, PH2, Derm7pt, BrEaST, and SkinCon,
demonstrating an average AUC improvement of approximately 3.2% while generating
20% more concise explanations compared to baseline V-IP, without sacrificing
informativeness. These findings highlight the importance of uncertainty-aware
reasoning in interpretable by design models for robust and reliable medical
decision-making.

</details>


### [152] [Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention](https://arxiv.org/abs/2506.16743)
*Weinan Guan,Wei Wang,Bo Peng,Ziwen He,Jing Dong,Haonan Cheng*

Main category: cs.CV

TL;DR: The paper introduces a Noise-Aware Self-Attention (NASA) module to detect diffusion-generated images by focusing on noise patterns, achieving state-of-the-art performance even for unseen models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of generalizing forgery detection to unseen diffusion models by leveraging shared noise patterns in generated images.

Method: Propose NASA-Swin, integrating NASA into Swin Transformer, using cross-modality fusion of RGB and noise images, and a channel mask strategy.

Result: Demonstrates superior detection performance, especially for unseen generation methods.

Conclusion: The NASA-Swin approach effectively enhances detection of diffusion-generated images, setting a new benchmark for generalization.

Abstract: With the rapid development of image generation technologies, especially the
advancement of Diffusion Models, the quality of synthesized images has
significantly improved, raising concerns among researchers about information
security. To mitigate the malicious abuse of diffusion models,
diffusion-generated image detection has proven to be an effective
countermeasure.However, a key challenge for forgery detection is generalising
to diffusion models not seen during training. In this paper, we address this
problem by focusing on image noise. We observe that images from different
diffusion models share similar noise patterns, distinct from genuine images.
Building upon this insight, we introduce a novel Noise-Aware Self-Attention
(NASA) module that focuses on noise regions to capture anomalous patterns. To
implement a SOTA detection model, we incorporate NASA into Swin Transformer,
forming an novel detection architecture NASA-Swin. Additionally, we employ a
cross-modality fusion embedding to combine RGB and noise images, along with a
channel mask strategy to enhance feature learning from both modalities.
Extensive experiments demonstrate the effectiveness of our approach in
enhancing detection capabilities for diffusion-generated images. When
encountering unseen generation methods, our approach achieves the
state-of-the-art performance.Our code is available at
https://github.com/WeinanGuan/NASA-Swin.

</details>


### [153] [Class Agnostic Instance-level Descriptor for Visual Instance Search](https://arxiv.org/abs/2506.16745)
*Qi-Ying Sun,Wan-Lei Zhao,Yi-Bo Miao,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: The paper proposes a hierarchical feature decomposition method using self-supervised ViT to improve instance-level image retrieval, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in visual instance search due to ineffective instance-level feature representation and poor performance of supervised methods on unknown categories.

Method: Hierarchical decomposition of feature sets from self-supervised ViT to detect compact feature subsets, addressing object embedding and occlusions.

Result: Outperforms state-of-the-art methods on three instance search benchmarks, effective for known and unknown object categories.

Conclusion: The hierarchical feature decomposition provides a comprehensive representation for latent instances, enhancing instance-level retrieval.

Abstract: Despite the great success of the deep features in content-based image
retrieval, the visual instance search remains challenging due to the lack of
effective instance level feature representation. Supervised or weakly
supervised object detection methods are not among the options due to their poor
performance on the unknown object categories. In this paper, based on the
feature set output from self-supervised ViT, the instance level region
discovery is modeled as detecting the compact feature subsets in a hierarchical
fashion. The hierarchical decomposition results in a hierarchy of feature
subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the
various instance regions in an image of different semantic scales. The
hierarchical decomposition well addresses the problem of object embedding and
occlusions, which are widely observed in the real scenarios. The features
derived from the nodes on the hierarchy make up a comprehensive representation
for the latent instances in the image. Our instance-level descriptor remains
effective on both the known and unknown object categories. Empirical studies on
three instance search benchmarks show that it outperforms state-of-the-art
methods considerably.

</details>


### [154] [Infrared and Visible Image Fusion Based on Implicit Neural Representations](https://arxiv.org/abs/2506.16773)
*Shuchen Sun,Ligen Shi,Chang Liu,Lina Wu,Jun Qiu*

Main category: cs.CV

TL;DR: The paper proposes INRFuse, an image fusion method using Implicit Neural Representations (INR) to combine infrared and visible light images, achieving superior results without training data.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional fusion methods by leveraging continuous function parameterization for richer, resolution-independent fused images.

Method: Uses INR with multi-layer perceptrons to fuse features, optimizing via multiple loss functions for thermal and texture preservation.

Result: INRFuse outperforms existing methods in visual quality and metrics, producing clear, detailed, and information-rich fused images.

Conclusion: INRFuse is a robust, training-free solution for high-quality infrared and visible light image fusion.

Abstract: Infrared and visible light image fusion aims to combine the strengths of both
modalities to generate images that are rich in information and fulfill visual
or computational requirements. This paper proposes an image fusion method based
on Implicit Neural Representations (INR), referred to as INRFuse. This method
parameterizes a continuous function through a neural network to implicitly
represent the multimodal information of the image, breaking through the
traditional reliance on discrete pixels or explicit features. The normalized
spatial coordinates of the infrared and visible light images serve as inputs,
and multi-layer perceptrons is utilized to adaptively fuse the features of both
modalities, resulting in the output of the fused image. By designing multiple
loss functions, the method jointly optimizes the similarity between the fused
image and the original images, effectively preserving the thermal radiation
information of the infrared image while maintaining the texture details of the
visible light image. Furthermore, the resolution-independent characteristic of
INR allows for the direct fusion of images with varying resolutions and
achieves super-resolution reconstruction through high-density coordinate
queries. Experimental results indicate that INRFuse outperforms existing
methods in both subjective visual quality and objective evaluation metrics,
producing fused images with clear structures, natural details, and rich
information without the necessity for a training dataset.

</details>


### [155] [PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model](https://arxiv.org/abs/2506.16776)
*Beomseok Ko,Hyeryung Jang*

Main category: cs.CV

TL;DR: PQCAD-DM combines Progressive Quantization and Calibration-Assisted Distillation to compress diffusion models, reducing inference time while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are resource-intensive and suffer from error accumulation, limiting compression effectiveness.

Method: PQCAD-DM uses Progressive Quantization (adaptive bit-width transitions) and Calibration-Assisted Distillation (full-precision calibration datasets).

Result: PQCAD-DM halves inference time and maintains competitive performance, outperforming fixed-bit quantization.

Conclusion: PQCAD-DM effectively balances efficiency and generative quality, validated by extensive experiments.

Abstract: Diffusion models excel in image generation but are computational and
resource-intensive due to their reliance on iterative Markov chain processes,
leading to error accumulation and limiting the effectiveness of naive
compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid
compression framework combining Progressive Quantization (PQ) and
Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs
a two-stage quantization with adaptive bit-width transitions guided by a
momentum-based mechanism, reducing excessive weight perturbations in
low-precision. CAD leverages full-precision calibration datasets during
distillation, enabling the student to match full-precision performance even
with a quantized teacher. As a result, PQCAD-DM achieves a balance between
computational efficiency and generative quality, halving inference time while
maintaining competitive performance. Extensive experiments validate PQCAD-DM's
superior generative capabilities and efficiency across diverse datasets,
outperforming fixed-bit quantization methods.

</details>


### [156] [TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration](https://arxiv.org/abs/2506.16784)
*Xiaoyu Shi,Rahul Kumar Jain,Yinhao Li,Ruibo Hou,Jingliang Cheng,Jie Bai,Guohua Zhao,Lanfen Lin,Rui Xu,Yen-wei Chen*

Main category: cs.CV

TL;DR: The paper introduces TextBraTS, a multimodal dataset combining MRI volumes and textual annotations for brain tumor segmentation, and proposes a novel framework with cross-attention for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing brain tumor segmentation lacks multimodal datasets integrating images and text, limiting exploration of text-guided methods.

Method: A novel baseline framework and sequential cross-attention method for text-guided volumetric segmentation, tested with various fusion strategies.

Result: Significant improvements in segmentation accuracy, demonstrating effective multimodal integration.

Conclusion: TextBraTS and the proposed framework advance multimodal brain tumor analysis, with publicly available resources for further research.

Abstract: Deep learning has demonstrated remarkable success in medical image
segmentation and computer-aided diagnosis. In particular, numerous advanced
methods have achieved state-of-the-art performance in brain tumor segmentation
from MRI scans. While recent studies in other medical imaging domains have
revealed that integrating textual reports with visual data can enhance
segmentation accuracy, the field of brain tumor analysis lacks a comprehensive
dataset that combines radiological images with corresponding textual
annotations. This limitation has hindered the exploration of multimodal
approaches that leverage both imaging and textual data.
  To bridge this critical gap, we introduce the TextBraTS dataset, the first
publicly available volume-level multimodal dataset that contains paired MRI
volumes and rich textual annotations, derived from the widely adopted BraTS2020
benchmark. Building upon this novel dataset, we propose a novel baseline
framework and sequential cross-attention method for text-guided volumetric
medical image segmentation. Through extensive experiments with various
text-image fusion strategies and templated text formulations, our approach
demonstrates significant improvements in brain tumor segmentation accuracy,
offering valuable insights into effective multimodal integration techniques.
  Our dataset, implementation code, and pre-trained models are publicly
available at https://github.com/Jupitern52/TextBraTS.

</details>


### [157] [RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought](https://arxiv.org/abs/2506.16796)
*Junbo Qiao,Miaomiao Cai,Wei Li,Yutong Liu,Xudong Huang,Gaoqi He,Jiao Xie,Jie Hu,Xinghao Chen,Shaohui Lin*

Main category: cs.CV

TL;DR: RealSR-R1 introduces VLCoT-GRPO, a framework combining vision and language reasoning for Real-World Image Super-Resolution, outperforming existing methods with realistic detail generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Real-World Image Super-Resolution struggle with understanding degraded content, leading to low-fidelity results. The paper aims to improve this by integrating reasoning capabilities inspired by Chain of Thought (CoT).

Method: Proposes VLCoT-GRPO, a framework simulating human image handling with four reward functions: Format, Degradation, Understanding, and Generation rewards, using Group Relative Policy Optimization (GRPO).

Result: RealSR-R1 generates realistic details and accurately understands image content, especially in semantically rich or severely degraded scenes.

Conclusion: The VLCoT-GRPO framework effectively enhances Real-World Image Super-Resolution by combining vision and language reasoning, validated by extensive experiments.

Abstract: Real-World Image Super-Resolution is one of the most challenging task in
image restoration. However, existing methods struggle with an accurate
understanding of degraded image content, leading to reconstructed results that
are both low-fidelity and unnatural. We present RealSR-R1 in this work, which
empowers the RealSR models with understanding and reasoning capabilities.
Inspired by the success of Chain of Thought (CoT) in large language models
(LLMs), we simulate the human process of handling degraded images and propose
the VLCoT framework, which integrates vision and language reasoning. The
framework aims to precisely restore image details by progressively generating
more comprehensive text and higher-resolution images. To overcome the challenge
of traditional supervised learning CoT failing to generalize to real-world
scenarios, we introduce, for the first time, Group Relative Policy Optimization
(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO
as a solution, which designs four reward functions: (1) Format reward, used to
standardize the CoT process; (2) Degradation reward, to incentivize accurate
degradation estimation; (3) Understanding reward, to ensure the accuracy of the
generated content; and (4) Generation reward, where we propose using a visual
expert model to evaluate the quality of generated images, encouraging the model
to generate more realistic images. Extensive experiments demonstrate that our
proposed RealSR-R1 can generate realistic details and accurately understand
image content, particularly in semantically rich scenes or images with severe
degradation.

</details>


### [158] [Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation](https://arxiv.org/abs/2506.16802)
*Riccardo Corvi,Davide Cozzolino,Ekta Prashnani,Shalini De Mello,Koki Nagano,Luisa Verdoliva*

Main category: cs.CV

TL;DR: The paper introduces a novel forensic-oriented data augmentation strategy to improve the generalizability of AI-generated video detectors by focusing on intrinsic low-level artifacts rather than high-level semantic flaws.


<details>
  <summary>Details</summary>
Motivation: Existing video forensic detectors exhibit poor generalization, limiting their real-world applicability. The goal is to guide detectors to focus on intrinsic artifacts rather than model-specific flaws.

Method: The study identifies discriminative features across generative architectures and introduces a wavelet-based data augmentation strategy to enhance forensic cues. The detector is trained on data from one model and tested against multiple others.

Result: The method significantly improves accuracy over state-of-the-art detectors, even on recent models like NOVA and FLUX, without complex algorithms or large datasets.

Conclusion: The proposed training paradigm enhances detector generalizability, demonstrating effectiveness across diverse generative models. Code and data will be publicly available.

Abstract: Synthetic video generation is progressing very rapidly. The latest models can
produce very realistic high-resolution videos that are virtually
indistinguishable from real ones. Although several video forensic detectors
have been recently proposed, they often exhibit poor generalization, which
limits their applicability in a real-world scenario. Our key insight to
overcome this issue is to guide the detector towards seeing what really
matters. In fact, a well-designed forensic classifier should focus on
identifying intrinsic low-level artifacts introduced by a generative
architecture rather than relying on high-level semantic flaws that characterize
a specific model. In this work, first, we study different generative
architectures, searching and identifying discriminative features that are
unbiased, robust to impairments, and shared across models. Then, we introduce a
novel forensic-oriented data augmentation strategy based on the wavelet
decomposition and replace specific frequency-related bands to drive the model
to exploit more relevant forensic cues. Our novel training paradigm improves
the generalizability of AI-generated video detectors, without the need for
complex algorithms and large datasets that include multiple synthetic
generators. To evaluate our approach, we train the detector using data from a
single generative model and test it against videos produced by a wide range of
other models. Despite its simplicity, our method achieves a significant
accuracy improvement over state-of-the-art detectors and obtains excellent
results even on very recent generative models, such as NOVA and FLUX. Code and
data will be made publicly available.

</details>


### [159] [Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes](https://arxiv.org/abs/2506.16805)
*Chao Chen,Nobel Dang,Juexiao Zhang,Wenkai Sun,Pengfei Zheng,Xuhang He,Yimeng Ye,Taarun Srinivas,Chen Feng*

Main category: cs.CV

TL;DR: The paper introduces the Co-VisiON benchmark to evaluate co-visibility reasoning in sparse image sets, revealing a gap between current vision models and human performance. A proposed multi-view baseline, Covis, narrows this gap.


<details>
  <summary>Details</summary>
Motivation: To assess whether current vision models match human-level proficiency in co-visibility analysis, a critical skill in 3D vision and robotic perception.

Method: The Co-VisiON benchmark evaluates co-visibility reasoning on sparse image sets across 1000+ indoor scenarios. A novel multi-view baseline, Covis, is proposed.

Result: Existing vision models struggle with co-visibility under sparse conditions. A proprietary vision-language model outperforms vision-only approaches, but all lag behind humans. Covis narrows the gap.

Conclusion: The benchmark highlights the need for high-level reasoning in vision models. Covis shows promise, but further advancements are needed to match human performance in sparse environments.

Abstract: Humans exhibit a remarkable ability to recognize co-visibility-the
overlapping regions visible in multiple images-even when these images are
sparsely distributed across a complex scene. This capability is foundational in
3D vision and robotic perception. Despite significant progress in vision
learning, it remains unclear whether current vision models have reached
human-level proficiency in co-visibility analysis. In this work, we introduce
the Co-Visibility reasONing (Co-VisiON) benchmark, designed to directly
evaluate co-visibility reasoning on sparse image sets across over 1000 indoor
scenarios. Our experiments reveal that while co-visibility is typically treated
as a low-level feature matching task, it poses a significant challenge for
existing vision models under sparse conditions. Notably, a proprietary
vision-language model outperforms all purely vision-based approaches, with all
models lagging substantially behind human performance. This gap underscores the
need for more than basic pairwise vision processing-it calls for a
comprehensive spatial understanding through high-level reasoning across
multiple views. Inspired by human visual cognition, we propose a novel
multi-view baseline, Covis, which achieves top performance among pure vision
models and narrows the gap to the proprietary VLM. We hope our benchmark and
findings will spur further advancements in developing vision models capable of
robust, high-level reasoning in challenging, sparse environments. Our dataset
and source code can be found at: https://ai4ce.github.io/CoVISION

</details>


### [160] [FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation](https://arxiv.org/abs/2506.16806)
*Fan Yang,Yousong Zhu,Xin Li,Yufei Zhan,Hongyin Zhao,Shurong Zheng,Yaowei Wang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: FOCUS is a unified LVLM integrating segmentation-aware perception and controllable object-centric generation, outperforming disjointed models in multimodal understanding, segmentation, and image generation.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs treat visual understanding and editing separately, relying on disjointed models, which FOCUS aims to unify for better performance.

Method: FOCUS uses a dual-branch visual encoder, MoVQGAN-based tokenizer, and a progressive multi-stage training pipeline to align segmentation and generation.

Result: FOCUS achieves strong performance in multimodal understanding, referring segmentation, and controllable image generation.

Conclusion: FOCUS effectively bridges segmentation-aware perception with fine-grained visual synthesis, demonstrating superior joint optimization.

Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising
capabilities in unifying visual understanding and generative modeling, enabling
both accurate content understanding and flexible editing. However, current
approaches treat "what to see" and "how to edit" separately: they either
perform isolated object segmentation or utilize segmentation masks merely as
conditional prompts for local edit generation tasks, often relying on multiple
disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM
that integrates segmentation-aware perception and controllable object-centric
generation within an end-to-end framework. FOCUS employs a dual-branch visual
encoder to simultaneously capture global semantic context and fine-grained
spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to
produce discrete visual tokens that enhance generation quality. To enable
accurate and controllable image editing, we propose a progressive multi-stage
training pipeline, where segmentation masks are jointly optimized and used as
spatial condition prompts to guide the diffusion decoder. This strategy aligns
visual encoding, segmentation, and generation modules, effectively bridging
segmentation-aware perception with fine-grained visual synthesis. Extensive
experiments across three core tasks, including multimodal understanding,
referring segmentation accuracy, and controllable image generation, demonstrate
that FOCUS achieves strong performance by jointly optimizing visual perception
and generative capabilities.

</details>


### [161] [Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection](https://arxiv.org/abs/2506.16819)
*Yuchu Jiang,Jiaming Chu,Jian Zhao,Xin Zhang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Loupe is a lightweight framework for joint deepfake detection and localization, combining patch-aware classification and segmentation with conditional queries. It achieves state-of-the-art performance with a test-time adaptation mechanism.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of existing deepfake detection methods, such as poor generalization and complex architectures, by proposing a unified solution.

Method: Integrates patch-aware classifier and segmentation module with conditional queries, using pseudo-label-guided test-time adaptation for robustness.

Result: Achieves 0.846 overall score on DDL dataset, winning IJCAI 2025 challenge.

Conclusion: Loupe's patch-level fusion and conditional query design improve accuracy and localization across diverse forgery patterns.

Abstract: The proliferation of generative models has raised serious concerns about
visual content forgery. Existing deepfake detection methods primarily target
either image-level classification or pixel-wise localization. While some
achieve high accuracy, they often suffer from limited generalization across
manipulation types or rely on complex architectures. In this paper, we propose
Loupe, a lightweight yet effective framework for joint deepfake detection and
localization. Loupe integrates a patch-aware classifier and a segmentation
module with conditional queries, allowing simultaneous global authenticity
classification and fine-grained mask prediction. To enhance robustness against
distribution shifts of test set, Loupe introduces a pseudo-label-guided
test-time adaptation mechanism by leveraging patch-level predictions to
supervise the segmentation head. Extensive experiments on the DDL dataset
demonstrate that Loupe achieves state-of-the-art performance, securing the
first place in the IJCAI 2025 Deepfake Detection and Localization Challenge
with an overall score of 0.846. Our results validate the effectiveness of the
proposed patch-level fusion and conditional query design in improving both
classification accuracy and spatial localization under diverse forgery
patterns. The code is available at https://github.com/Kamichanw/Loupe.

</details>


### [162] [Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots](https://arxiv.org/abs/2506.16821)
*Can Lin,Daniele Affinita,Marco E. P. Zimmatore,Daniele Nardi,Domenico D. Bloisi,Vincenzo Suriani*

Main category: cs.CV

TL;DR: A self-supervised learning framework for ball detection in RoboCup soccer robots, reducing reliance on manual annotations and improving performance.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised methods for ball detection require costly manual annotations; this work aims to reduce dependency on labeled data.

Method: Uses a pretrained model for pseudo-labeling and self-supervised pretext tasks (colorization, edge detection, triplet loss) for feature learning. Incorporates MAML for quick adaptation.

Result: Outperforms baseline models in accuracy, F1 score, and IoU, with faster convergence. Introduces a new 10,000-image dataset.

Conclusion: The proposed framework is effective for robust ball detection in dynamic environments, reducing annotation costs.

Abstract: Robust and accurate ball detection is a critical component for autonomous
humanoid soccer robots, particularly in dynamic and challenging environments
such as RoboCup outdoor fields. However, traditional supervised approaches
require extensive manual annotation, which is costly and time-intensive. To
overcome this problem, we present a self-supervised learning framework for
domain-adaptive feature extraction to enhance ball detection performance. The
proposed approach leverages a general-purpose pretrained model to generate
pseudo-labels, which are then used in a suite of self-supervised pretext tasks
-- including colorization, edge detection, and triplet loss -- to learn robust
visual features without relying on manual annotations. Additionally, a
model-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid
adaptation to new deployment scenarios with minimal supervision. A new dataset
comprising 10,000 labeled images from outdoor RoboCup SPL matches is
introduced, used to validate the method, and made available to the community.
Experimental results demonstrate that the proposed pipeline outperforms
baseline models in terms of accuracy, F1 score, and IoU, while also exhibiting
faster convergence.

</details>


### [163] [AnyTraverse: An off-road traversability framework with VLM and human operator in the loop](https://arxiv.org/abs/2506.16826)
*Sattwik Sahu,Agamdeep Singh,Karthik Nambiar,Srikanth Saripalli,P. B. Sujit*

Main category: cs.CV

TL;DR: AnyTraverse is a framework for off-road traversability segmentation that combines natural language prompts and human-operator assistance to adapt to diverse robotic vehicles and unstructured environments, reducing supervision load.


<details>
  <summary>Details</summary>
Motivation: Current frameworks struggle with variations in unstructured environments and lack adaptability for different robot types, limiting autonomous navigation applications.

Method: AnyTraverse uses natural language-based prompts and human-operator assistance to segment scenes, calling operators only for unexplored or unknown regions, and employs zero-shot learning to avoid extensive retraining.

Result: AnyTraverse outperforms GA-NAV and Off-seg in experimental validation on RELLIS-3D, Freiburg Forest, and RUGD datasets, demonstrating real-world deployment on multiple robot platforms.

Conclusion: AnyTraverse offers a vehicle-agnostic, adaptive solution for off-road traversability, balancing automation with targeted human supervision.

Abstract: Off-road traversability segmentation enables autonomous navigation with
applications in search-and-rescue, military operations, wildlife exploration,
and agriculture. Current frameworks struggle due to significant variations in
unstructured environments and uncertain scene changes, and are not adaptive to
be used for different robot types. We present AnyTraverse, a framework
combining natural language-based prompts with human-operator assistance to
determine navigable regions for diverse robotic vehicles. The system segments
scenes for a given set of prompts and calls the operator only when encountering
previously unexplored scenery or unknown class not part of the prompt in its
region-of-interest, thus reducing active supervision load while adapting to
varying outdoor scenes. Our zero-shot learning approach eliminates the need for
extensive data collection or retraining. Our experimental validation includes
testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate
real-world deployment on multiple robot platforms. The results show that
AnyTraverse performs better than GA-NAV and Off-seg while offering a
vehicle-agnostic approach to off-road traversability that balances automation
with targeted human supervision.

</details>


### [164] [Camera Calibration via Circular Patterns: A Comprehensive Framework with Measurement Uncertainty and Unbiased Projection Model](https://arxiv.org/abs/2506.16842)
*Chaehyeon Song,Dongjae Lee,Jongwoo Lim,Ayoung Kim*

Main category: cs.CV

TL;DR: The paper proposes an unbiased projection model for circular patterns in camera calibration, addressing bias under lens distortion. It introduces centroid uncertainty to improve robustness and provides guidelines for better calibration.


<details>
  <summary>Details</summary>
Motivation: Existing projection models for circle centroids are biased under lens distortion, leading to low performance. The study aims to overcome this limitation.

Method: The authors propose an unbiased projection model for circular patterns and introduce centroid uncertainty. They model boundary points as a Markov random field and propagate shape distribution to centroid uncertainty using Green theorem.

Result: The framework achieves significant improvements in calibration accuracy and robustness compared to checkerboard methods.

Conclusion: The proposed method enhances camera calibration by addressing bias and uncertainty, offering superior performance and practical guidelines.

Abstract: Camera calibration using planar targets has been widely favored, and two
types of control points have been mainly considered as measurements: the
corners of the checkerboard and the centroid of circles. Since a centroid is
derived from numerous pixels, the circular pattern provides more precise
measurements than the checkerboard. However, the existing projection model of
circle centroids is biased under lens distortion, resulting in low performance.
To surmount this limitation, we propose an unbiased projection model of the
circular pattern and demonstrate its superior accuracy compared to the
checkerboard. Complementing this, we introduce uncertainty into circular
patterns to enhance calibration robustness and completeness. Defining centroid
uncertainty improves the performance of calibration components, including
pattern detection, optimization, and evaluation metrics. We also provide
guidelines for performing good camera calibration based on the evaluation
metric. The core concept of this approach is to model the boundary points of a
two-dimensional shape as a Markov random field, considering its connectivity.
The shape distribution is propagated to the centroid uncertainty through an
appropriate shape representation based on the Green theorem. Consequently, the
resulting framework achieves marked gains in calibration accuracy and
robustness. The complete source code and demonstration video are available at
https://github.com/chaehyeonsong/discocal.

</details>


### [165] [Controllable and Expressive One-Shot Video Head Swapping](https://arxiv.org/abs/2506.16852)
*Chaonan Ji,Jinwei Qi,Peng Zhang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: A diffusion-based framework for video head swapping that preserves identity and allows expression editing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack holistic head preservation, struggle with hairstyles/backgrounds, and don't support post-swap expression edits.

Method: Uses latent diffusion with identity-preserving context fusion and expression-aware landmark retargeting/editing.

Result: Achieves seamless integration, identity preservation, and superior expression transfer for real/virtual characters.

Conclusion: The method effectively addresses limitations of current head-swapping techniques.

Abstract: In this paper, we propose a novel diffusion-based multi-condition
controllable framework for video head swapping, which seamlessly transplant a
human head from a static image into a dynamic video, while preserving the
original body and background of target video, and further allowing to tweak
head expressions and movements during swapping as needed. Existing
face-swapping methods mainly focus on localized facial replacement neglecting
holistic head morphology, while head-swapping approaches struggling with
hairstyle diversity and complex backgrounds, and none of these methods allow
users to modify the transplanted head expressions after swapping. To tackle
these challenges, our method incorporates several innovative strategies through
a unified latent diffusion paradigm. 1) Identity-preserving context fusion: We
propose a shape-agnostic mask strategy to explicitly disentangle foreground
head identity features from background/body contexts, combining hair
enhancement strategy to achieve robust holistic head identity preservation
across diverse hair types and complex backgrounds. 2) Expression-aware landmark
retargeting and editing: We propose a disentangled 3DMM-driven retargeting
module that decouples identity, expression, and head poses, minimizing the
impact of original expressions in input images and supporting expression
editing. While a scale-aware retargeting strategy is further employed to
minimize cross-identity expression distortion for higher transfer precision.
Experimental results demonstrate that our method excels in seamless background
integration while preserving the identity of the source portrait, as well as
showcasing superior expression transfer capabilities applicable to both real
and virtual characters.

</details>


### [166] [ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control](https://arxiv.org/abs/2506.16856)
*Jun Fu,Bin Tian,Haonan Chen,Shi Meng,Tingting Yao*

Main category: cs.CV

TL;DR: A Transformer-based end-to-end framework for autonomous parking learns from expert demonstrations, achieving high success rates in simulated environments.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based parking systems struggle with adaptability in dynamic environments, while human drivers park intuitively. This inspires a learning-based approach.

Method: The framework uses surround-view camera images, goal-point representations, ego motion, and pedestrian trajectories. It employs a cross-attention module and GRU-based pedestrian predictor.

Result: The model achieves a 96.57% success rate with low positional (0.21m) and orientation (0.41¬∞) errors in CARLA simulations.

Conclusion: The proposed method is effective for autonomous parking, with key modules like pedestrian prediction enhancing performance. Code and dataset will be released.

Abstract: Autonomous parking plays a vital role in intelligent vehicle systems,
particularly in constrained urban environments where high-precision control is
required. While traditional rule-based parking systems struggle with
environmental uncertainties and lack adaptability in crowded or dynamic scenes,
human drivers demonstrate the ability to park intuitively without explicit
modeling. Inspired by this observation, we propose a Transformer-based
end-to-end framework for autonomous parking that learns from expert
demonstrations. The network takes as input surround-view camera images,
goal-point representations, ego vehicle motion, and pedestrian trajectories. It
outputs discrete control sequences including throttle, braking, steering, and
gear selection. A novel cross-attention module integrates BEV features with
target points, and a GRU-based pedestrian predictor enhances safety by modeling
dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both
vertical and parallel parking scenarios. Experiments show our model achieves a
high success rate of 96.57\%, with average positional and orientation errors of
0.21 meters and 0.41 degrees, respectively. The ablation studies further
demonstrate the effectiveness of key modules such as pedestrian prediction and
goal-point attention fusion. The code and dataset will be released at:
https://github.com/little-snail-f/ParkFormer.

</details>


### [167] [With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You](https://arxiv.org/abs/2506.16895)
*Fabian Gr√∂ger,Shuo Wen,Huyen Le,Maria Brbiƒá*

Main category: cs.CV

TL;DR: The paper proposes STRUCTURE, a method for aligning pretrained unimodal models with limited paired data, achieving high performance in multimodal tasks using minimal samples.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models require large paired datasets, which are costly or unavailable in many domains. This work explores alignment with limited data.

Method: Aligns pretrained unimodal models using STRUCTURE, a regularization technique preserving latent space geometry, and optimizes layer alignment based on representational similarity.

Result: Achieves significant improvements (51.6% in classification, 91.8% in retrieval) across 24 benchmarks with minimal paired samples.

Conclusion: The framework is effective for resource-constrained domains, offering a scalable solution for limited-sample multimodal learning.

Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks
requiring multimodal alignment including zero-shot classification and
cross-modal retrieval. However, existing models typically rely on millions of
paired multimodal samples, which are prohibitively expensive or infeasible to
obtain in many domains. In this work, we explore the feasibility of building
multimodal models with limited amount of paired data by aligning pretrained
unimodal foundation models. We show that high-quality alignment is possible
with as few as tens of thousands of paired samples$\unicode{x2013}$less than
$1\%$ of the data typically used in the field. To achieve this, we introduce
STRUCTURE, an effective regularization technique that preserves the
neighborhood geometry of the latent space of unimodal encoders. Additionally,
we show that aligning last layers is often suboptimal and demonstrate the
benefits of aligning the layers with the highest representational similarity
across modalities. These two components can be readily incorporated into
existing alignment methods, yielding substantial gains across 24 zero-shot
image classification and retrieval benchmarks, with average relative
improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our
results highlight the effectiveness and broad applicability of our framework
for limited-sample multimodal learning and offer a promising path forward for
resource-constrained domains.

</details>


### [168] [LunarLoc: Segment-Based Global Localization on the Moon](https://arxiv.org/abs/2506.16940)
*Annika Thomas,Robaire Galliath,Aleksander Garbuz,Luke Anger,Cormac O'Neill,Trevor Johst,Dami Thomas,George Lordos,Jonathan P. How*

Main category: cs.CV

TL;DR: LunarLoc is a global localization method for lunar surface operations, using instance segmentation and graph-based terrain alignment to achieve drift-free, sub-cm accuracy.


<details>
  <summary>Details</summary>
Motivation: Autonomous lunar operations lack Earth-based navigation like GPS, requiring precise pose estimation for tasks like excavation and transport, where odometry drift is a challenge.

Method: LunarLoc uses instance segmentation to extract boulder landmarks from stereo imagery, constructs a terrain graph, and aligns it with a reference map using graph-theoretic data association.

Result: The method achieves sub-cm accuracy in multi-session global localization, outperforming existing lunar localization techniques.

Conclusion: LunarLoc provides a robust solution for lunar global localization, with publicly released datasets to foster further development.

Abstract: Global localization is necessary for autonomous operations on the lunar
surface where traditional Earth-based navigation infrastructure, such as GPS,
is unavailable. As NASA advances toward sustained lunar presence under the
Artemis program, autonomous operations will be an essential component of tasks
such as robotic exploration and infrastructure deployment. Tasks such as
excavation and transport of regolith require precise pose estimation, but
proposed approaches such as visual-inertial odometry (VIO) accumulate odometry
drift over long traverses. Precise pose estimation is particularly important
for upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on
autonomous agents to operate over extended timescales and varied terrain. To
help overcome odometry drift over long traverses, we propose LunarLoc, an
approach to global localization that leverages instance segmentation for
zero-shot extraction of boulder landmarks from onboard stereo imagery. Segment
detections are used to construct a graph-based representation of the terrain,
which is then aligned with a reference map of the environment captured during a
previous session using graph-theoretic data association. This method enables
accurate and drift-free global localization in visually ambiguous settings.
LunarLoc achieves sub-cm level accuracy in multi-session global localization
experiments, significantly outperforming the state of the art in lunar global
localization. To encourage the development of further methods for global
localization on the Moon, we release our datasets publicly with a playback
module: https://github.com/mit-acl/lunarloc-data.

</details>


### [169] [LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models](https://arxiv.org/abs/2506.16950)
*Fanfei Li,Thomas Klein,Wieland Brendel,Robert Geirhos,Roland S. Zimmermann*

Main category: cs.CV

TL;DR: LAION-C is introduced as a new benchmark for OOD robustness, addressing limitations of older benchmarks like ImageNet-C by including novel distortions not found in web-scale datasets.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like ImageNet-C are no longer effective for evaluating OOD robustness due to their corruptions being common in modern web-scale datasets.

Method: LAION-C is created with six novel distortion types designed to be OOD for web-scale datasets. State-of-the-art models are evaluated, and human robustness is compared via psychophysical experiments.

Result: LAION-C challenges contemporary models, with top models now matching or surpassing human performance in OOD generalization.

Conclusion: LAION-C provides a more relevant benchmark for OOD robustness, revealing a shift where models now rival or exceed human performance.

Abstract: Out-of-distribution (OOD) robustness is a desired property of computer vision
models. Improving model robustness requires high-quality signals from
robustness benchmarks to quantify progress. While various benchmark datasets
such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C
corruption types are no longer OOD relative to today's large, web-scraped
datasets, which already contain common corruptions such as blur or JPEG
compression artifacts. Consequently, these benchmarks are no longer well-suited
for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent
models show saturating scores on ImageNet-era OOD benchmarks, indicating that
it is unclear whether models trained on web-scale datasets truly become better
at OOD generalization or whether they have simply been exposed to the test
distortions during training. To address this, we introduce LAION-C as a
benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion
types specifically designed to be OOD, even for web-scale datasets such as
LAION. In a comprehensive evaluation of state-of-the-art models, we find that
the LAION-C dataset poses significant challenges to contemporary models,
including MLLMs such as Gemini and GPT-4o. We additionally conducted a
psychophysical experiment to evaluate the difficulty of our corruptions for
human observers, enabling a comparison of models to lab-quality human
robustness data. We observe a paradigm shift in OOD generalization: from humans
outperforming models, to the best models now matching or outperforming the best
human observers.

</details>


### [170] [Visual-Instructed Degradation Diffusion for All-in-One Image Restoration](https://arxiv.org/abs/2506.16960)
*Wenyang Luo,Haina Qin,Zewen Chen,Libin Wang,Dandan Zheng,Yuming Li,Yufan Liu,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: Defusion is an all-in-one image restoration framework using visual instruction-guided degradation diffusion, outperforming task-specific models for mixed or unknown degradations.


<details>
  <summary>Details</summary>
Motivation: Existing models are limited to specific degradation types, lacking generalization for real-world mixed or unknown degradations.

Method: Defusion uses visual instructions aligned with degradation patterns to guide a diffusion-based model operating in degradation space.

Result: Defusion outperforms state-of-the-art methods across diverse restoration tasks, including complex and real-world degradations.

Conclusion: Defusion offers a generalized, stable solution for image restoration, adaptable to various degradation types without task-specific models.

Abstract: Image restoration tasks like deblurring, denoising, and dehazing usually need
distinct models for each degradation type, restricting their generalization in
real-world scenarios with mixed or unknown degradations. In this work, we
propose \textbf{Defusion}, a novel all-in-one image restoration framework that
utilizes visual instruction-guided degradation diffusion. Unlike existing
methods that rely on task-specific models or ambiguous text-based priors,
Defusion constructs explicit \textbf{visual instructions} that align with the
visual degradation patterns. These instructions are grounded by applying
degradations to standardized visual elements, capturing intrinsic degradation
features while agnostic to image semantics. Defusion then uses these visual
instructions to guide a diffusion-based model that operates directly in the
degradation space, where it reconstructs high-quality images by denoising the
degradation effects with enhanced stability and generalizability. Comprehensive
experiments demonstrate that Defusion outperforms state-of-the-art methods
across diverse image restoration tasks, including complex and real-world
degradations.

</details>


### [171] [Reversing Flow for Image Restoration](https://arxiv.org/abs/2506.16961)
*Haina Qin,Wenyang Luo,Libin Wang,Dandan Zheng,Jingdong Chen,Ming Yang,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: ResFlow is a novel image restoration framework using deterministic continuous normalizing flows for efficient and high-quality results.


<details>
  <summary>Details</summary>
Motivation: Existing generative models treat degradation as stochastic, leading to inefficiency and complexity. ResFlow addresses this by modeling degradation deterministically.

Method: ResFlow uses continuous normalizing flows to model degradation as a deterministic path, augmented with an auxiliary process for reversible modeling. It employs entropy-preserving flow paths and velocity field matching.

Result: ResFlow achieves state-of-the-art performance, completing restoration in fewer than four steps, and outperforms benchmarks.

Conclusion: ResFlow offers a practical, efficient solution for image restoration, improving speed and quality over existing methods.

Abstract: Image restoration aims to recover high-quality (HQ) images from degraded
low-quality (LQ) ones by reversing the effects of degradation. Existing
generative models for image restoration, including diffusion and score-based
models, often treat the degradation process as a stochastic transformation,
which introduces inefficiency and complexity. In this work, we propose ResFlow,
a novel image restoration framework that models the degradation process as a
deterministic path using continuous normalizing flows. ResFlow augments the
degradation process with an auxiliary process that disambiguates the
uncertainty in HQ prediction to enable reversible modeling of the degradation
process. ResFlow adopts entropy-preserving flow paths and learns the augmented
degradation flow by matching the velocity field. ResFlow significantly improves
the performance and speed of image restoration, completing the task in fewer
than four sampling steps. Extensive experiments demonstrate that ResFlow
achieves state-of-the-art results across various image restoration benchmarks,
offering a practical and efficient solution for real-world applications.

</details>


### [172] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Main category: cs.CV

TL;DR: The paper introduces Mentor-Intern Collaborative Search (MICS) to improve reasoning in medical MLLMs by generating high-quality chain-of-thought data, resulting in state-of-the-art performance for medical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack a comprehensive framework for generating and evaluating reasoning paths in medical MLLMs, limiting their diagnostic capabilities.

Method: MICS uses mentor models to initialize reasoning paths, intern models to extend them, and an MICS-Score to select optimal paths, creating a rigorous CoT dataset.

Result: The method produces Chiron-o1, a medical MLLM with superior visual question-answering and reasoning performance on benchmarks.

Conclusion: MICS effectively enhances medical MLLM reasoning, validated by Chiron-o1's state-of-the-art results.

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [173] [ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](https://arxiv.org/abs/2506.16991)
*Binbin Xiang,Maciej Wielgosz,Stefano Puliti,Kamil Kr√°l,Martin Kr≈Øƒçek,Azim Missarov,Rasmus Astrup*

Main category: cs.CV

TL;DR: ForestFormer3D is a new framework for precise forest LiDAR point cloud segmentation, outperforming existing methods on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with the complexity of natural forests, necessitating a more robust solution.

Method: ForestFormer3D uses ISA-guided query points, score-based block merging, and one-to-many association for training.

Result: Achieves state-of-the-art performance on FOR-instanceV2 and generalizes well to unseen datasets.

Conclusion: ForestFormer3D is effective and robust, with plans to release the dataset and code.

Abstract: The segmentation of forest LiDAR 3D point clouds, including both individual
tree and semantic segmentation, is fundamental for advancing forest management
and ecological research. However, current approaches often struggle with the
complexity and variability of natural forest environments. We present
ForestFormer3D, a new unified and end-to-end framework designed for precise
individual tree and semantic segmentation. ForestFormer3D incorporates
ISA-guided query point selection, a score-based block merging strategy during
inference, and a one-to-many association mechanism for effective training. By
combining these new components, our model achieves state-of-the-art performance
for individual tree segmentation on the newly introduced FOR-instanceV2
dataset, which spans diverse forest types and regions. Additionally,
ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),
showcasing its robustness across different forest conditions and sensor
modalities. The FOR-instanceV2 dataset and the ForestFormer3D code will be
released soon.

</details>


### [174] [Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments](https://arxiv.org/abs/2506.16994)
*Yasir Ali Farrukh,Syed Wali,Irfan Khan,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: Prmpt2Adpt is a lightweight, efficient zero-shot domain adaptation framework using prompt-driven feature alignment, achieving fast adaptation and inference with minimal source data.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing UDA methods, which rely on large models and full source data, for resource-constrained environments like drones.

Method: Uses a teacher-student paradigm with a distilled CLIP model, prompt-driven feature alignment, and pseudo-labels for adaptation.

Result: Achieves competitive performance with 7x faster adaptation and 5x faster inference on the MDS-A dataset.

Conclusion: Prmpt2Adpt is a practical, scalable solution for real-time adaptation in low-resource domains.

Abstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world
vision systems, especially in resource-constrained environments like drones,
where memory and computation are limited. Existing prompt-driven UDA methods
typically rely on large vision-language models and require full access to
source-domain data during adaptation, limiting their applicability. In this
work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain
adaptation framework built around a teacher-student paradigm guided by
prompt-based feature alignment. At the core of our method is a distilled and
fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A
small set of low-level source features is aligned to the target domain
semantics-specified only through a natural language prompt-via Prompt-driven
Instance Normalization (PIN). These semantically steered features are used to
briefly fine-tune the detection head of the teacher model. The adapted teacher
then generates high-quality pseudo-labels, which guide the on-the-fly
adaptation of a compact student model. Experiments on the MDS-A dataset
demonstrate that Prmpt2Adpt achieves competitive detection performance compared
to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x
faster inference speed using few source images-making it a practical and
scalable solution for real-time adaptation in low-resource domains.

</details>


### [175] [A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving](https://arxiv.org/abs/2506.17004)
*Hanlin Wu,Pengfei Lin,Ehsan Javanmardi,Naren Bao,Bo Qian,Hao Si,Manabu Tsukada*

Main category: cs.CV

TL;DR: The paper introduces collaborative 3D semantic occupancy prediction to overcome single-vehicle limitations like occlusion and narrow viewpoints, using augmented dataset and a baseline model for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Single-vehicle perception is limited by occlusion, sensor range, and viewpoints; collaboration can enhance completeness and accuracy.

Method: Augmented an existing dataset with CARLA for dense occupancy annotations, developed benchmarks for spatial impact, and created a baseline model with inter-agent feature fusion.

Result: The baseline model outperforms single-agent models, with greater gains as prediction range increases.

Conclusion: Collaborative perception improves 3D semantic occupancy prediction, especially over larger spatial extents.

Abstract: 3D semantic occupancy prediction is an emerging perception paradigm in
autonomous driving, providing a voxel-level representation of both geometric
details and semantic categories. However, the perception capability of a single
vehicle is inherently constrained by occlusion, restricted sensor range, and
narrow viewpoints. To address these limitations, collaborative perception
enables the exchange of complementary information, thereby enhancing the
completeness and accuracy. In the absence of a dedicated dataset for
collaborative 3D semantic occupancy prediction, we augment an existing
collaborative perception dataset by replaying it in CARLA with a
high-resolution semantic voxel sensor to provide dense and comprehensive
occupancy annotations. In addition, we establish benchmarks with varying
prediction ranges designed to systematically assess the impact of spatial
extent on collaborative prediction. We further develop a baseline model that
performs inter-agent feature fusion via spatial alignment and attention
aggregation. Experimental results demonstrate that our baseline model
consistently outperforms single-agent models, with increasing gains observed as
the prediction range expands.

</details>


### [176] [Unsupervised Image Super-Resolution Reconstruction Based on Real-World Degradation Patterns](https://arxiv.org/abs/2506.17027)
*Yiyang Tie,Hong Zhu,Yunyun Luo,Jing Shi*

Main category: cs.CV

TL;DR: The paper proposes a TripleGAN framework to address challenges in modeling real-world degradation patterns for super-resolution (SR) reconstruction, outperforming existing methods on RealSR and DRealSR datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture diverse real-world degradation patterns (e.g., blur, noise, color shifts) from low-resolution (LR) images, limiting SR model performance.

Method: A TripleGAN framework with three components: FirstGAN narrows blur domain gaps, SecondGAN approximates target-domain blur and learns additional degradations, and ThirdGAN reconstructs LR images from pseudo-real data.

Result: The method achieves superior quantitative metrics and sharp reconstructions without over-smoothing on RealSR and DRealSR datasets.

Conclusion: The TripleGAN framework effectively learns and synthesizes real-world degradation patterns, enabling high-quality SR reconstruction from real-world LR inputs.

Abstract: The training of real-world super-resolution reconstruction models heavily
relies on datasets that reflect real-world degradation patterns. Extracting and
modeling degradation patterns for super-resolution reconstruction using only
real-world low-resolution (LR) images remains a challenging task. When
synthesizing datasets to simulate real-world degradation, relying solely on
degradation extraction methods fails to capture both blur and diverse noise
characteristics across varying LR distributions, as well as more implicit
degradations such as color gamut shifts. Conversely, domain translation alone
cannot accurately approximate real-world blur characteristics due to the
significant degradation domain gap between synthetic and real data. To address
these challenges, we propose a novel TripleGAN framework comprising two
strategically designed components: The FirstGAN primarily focuses on narrowing
the domain gap in blur characteristics, while the SecondGAN performs
domain-specific translation to approximate target-domain blur properties and
learn additional degradation patterns. The ThirdGAN is trained on pseudo-real
data generated by the FirstGAN and SecondGAN to reconstruct real-world LR
images. Extensive experiments on the RealSR and DRealSR datasets demonstrate
that our method exhibits clear advantages in quantitative metrics while
maintaining sharp reconstructions without over-smoothing artifacts. The
proposed framework effectively learns real-world degradation patterns from LR
observations and synthesizes aligned datasets with corresponding degradation
characteristics, thereby enabling the trained network to achieve superior
performance in reconstructing high-quality SR images from real-world LR inputs.

</details>


### [177] [Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance](https://arxiv.org/abs/2506.17040)
*Lorenzo Tausani,Paolo Muratore,Morgan B. Talbot,Giacomo Amerio,Gabriel Kreiman,Davide Zoccolan*

Main category: cs.CV

TL;DR: The paper introduces Stretch-and-Squeeze (SnS), a framework to study feature invariance and adversarial sensitivity in visual systems, revealing insights into CNNs and human vision.


<details>
  <summary>Details</summary>
Motivation: To understand how visual units encode feature combinations and generalize, beyond just identifying exciting images.

Method: SnS uses bi-objective optimization to probe invariance (preserving activation) and adversarial sensitivity (suppressing activation) in visual systems.

Result: SnS identified image variations preserving unit responses better than affine transformations, with differences based on representation layers. Robust CNNs produced more recognizable invariant images.

Conclusion: SnS provides a systematic way to study invariance and adversarial effects, highlighting robust CNNs as better models of human vision.

Abstract: Uncovering which features' combinations high-level visual units encode is
critical to understand how images are transformed into representations that
support recognition. While existing feature visualization approaches typically
infer a unit's most exciting images, this is insufficient to reveal the
manifold of transformations under which responses remain invariant, which is
key to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS),
an unbiased, model-agnostic, and gradient-free framework to systematically
characterize a unit's invariance landscape and its vulnerability to adversarial
perturbations in both biological and artificial visual systems. SnS frames
these transformations as bi-objective optimization problems. To probe
invariance, SnS seeks image perturbations that maximally alter the
representation of a reference stimulus in a given processing stage while
preserving unit activation. To probe adversarial sensitivity, SnS seeks
perturbations that minimally alter the stimulus while suppressing unit
activation. Applied to convolutional neural networks (CNNs), SnS revealed image
variations that were further from a reference image in pixel-space than those
produced by affine transformations, while more strongly preserving the target
unit's response. The discovered invariant images differed dramatically
depending on the choice of image representation used for optimization:
pixel-level changes primarily affected luminance and contrast, while stretching
mid- and late-layer CNN representations altered texture and pose respectively.
Notably, the invariant images from robust networks were more recognizable by
human subjects than those from standard networks, supporting the higher
fidelity of robust CNNs as models of the visual system.

</details>


### [178] [Relaxed syntax modeling in Transformers for future-proof license plate recognition](https://arxiv.org/abs/2506.17051)
*Florent Meyer,Laurent Guichard,Denis Coquenet,Guillaume Gravier,Yann Soullard,Bertrand Co√ºasnon*

Main category: cs.CV

TL;DR: The paper addresses the performance drop of Transformer-based license plate recognition systems over time due to syntax shifts, proposing SaLT, a Syntax-Less Transformer, to maintain accuracy on future plates.


<details>
  <summary>Details</summary>
Motivation: Transformers perform poorly on new license plate syntaxes, making them unreliable for production. The study aims to create a system resilient to syntax changes.

Method: Analyzes Transformer flaws, proposes architectural changes, and introduces SaLT for syntax-agnostic modeling. Tests on real and synthetic datasets.

Result: SaLT achieves top accuracy on known syntaxes and maintains performance on future plates, outperforming traditional Transformers.

Conclusion: SaLT's architectural enhancements effectively address syntax dependency, proving robust for evolving license plate recognition.

Abstract: Effective license plate recognition systems are required to be resilient to
constant change, as new license plates are released into traffic daily. While
Transformer-based networks excel in their recognition at first sight, we
observe significant performance drop over time which proves them unsuitable for
tense production environments. Indeed, such systems obtain state-of-the-art
results on plates whose syntax is seen during training. Yet, we show they
perform similarly to random guessing on future plates where legible characters
are wrongly recognized due to a shift in their syntax. After highlighting the
flows of positional and contextual information in Transformer encoder-decoders,
we identify several causes for their over-reliance on past syntax. Following,
we devise architectural cut-offs and replacements which we integrate into SaLT,
an attempt at a Syntax-Less Transformer for syntax-agnostic modeling of license
plate representations. Experiments on both real and synthetic datasets show
that our approach reaches top accuracy on past syntax and most importantly
nearly maintains performance on future license plates. We further demonstrate
the robustness of our architecture enhancements by way of various ablations.

</details>


### [179] [Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion](https://arxiv.org/abs/2506.17074)
*Wang Zhao,Yan-Pei Cao,Jiale Xu,Yuejiang Dong,Ying Shan*

Main category: cs.CV

TL;DR: Assembler is a scalable framework for 3D part assembly using diffusion models and sparse anchor point clouds, achieving state-of-the-art results on diverse objects.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of prior methods, which rely on deterministic pose prediction and category-specific training, by handling diverse, real-world objects with varying part geometries and structures.

Method: Formulates part assembly as a generative problem using diffusion models, introduces a shape-centric representation with sparse anchor point clouds, and constructs a large-scale dataset of 320K part-object assemblies.

Result: Achieves state-of-the-art performance on PartNet and demonstrates high-quality assembly for complex, real-world objects.

Conclusion: Assembler advances 3D part assembly and enables a part-aware 3D modeling system for interactive and compositional design.

Abstract: We present Assembler, a scalable and generalizable framework for 3D part
assembly that reconstructs complete objects from input part meshes and a
reference image. Unlike prior approaches that mostly rely on deterministic part
pose prediction and category-specific training, Assembler is designed to handle
diverse, in-the-wild objects with varying part counts, geometries, and
structures. It addresses the core challenges of scaling to general 3D part
assembly through innovations in task formulation, representation, and data.
First, Assembler casts part assembly as a generative problem and employs
diffusion models to sample plausible configurations, effectively capturing
ambiguities arising from symmetry, repeated parts, and multiple valid
assemblies. Second, we introduce a novel shape-centric representation based on
sparse anchor point clouds, enabling scalable generation in Euclidean space
rather than SE(3) pose prediction. Third, we construct a large-scale dataset of
over 320K diverse part-object assemblies using a synthesis and filtering
pipeline built on existing 3D shape repositories. Assembler achieves
state-of-the-art performance on PartNet and is the first to demonstrate
high-quality assembly for complex, real-world objects. Based on Assembler, we
further introduce an interesting part-aware 3D modeling system that generates
high-resolution, editable objects from images, demonstrating potential for
interactive and compositional design. Project page:
https://assembler3d.github.io

</details>


### [180] [Acquiring and Accumulating Knowledge from Diverse Datasets for Multi-label Driving Scene Classification](https://arxiv.org/abs/2506.17101)
*Ke Li,Chenyu Zhang,Yuxin Ding,Xianbiao Hu,Ruwen Qin*

Main category: cs.CV

TL;DR: The paper introduces KAA-CAL, a system combining knowledge acquisition and accumulation (KAA) with consistency-based active learning (CAL) to improve multi-label driving scene identification, achieving a 56.1% performance boost over baselines.


<details>
  <summary>Details</summary>
Motivation: Enhancing autonomous vehicles' contextual awareness by addressing challenges in multi-label classification, such as dataset imbalance and task learning balance.

Method: Uses KAA to learn from single-label datasets and CAL to align knowledge gaps between attribute distributions.

Result: 56.1% performance increase over baseline, with KAA contributing 31.3% and CAL 24.8%. Outperforms SOTA models with 85% less data.

Conclusion: KAA-CAL effectively addresses multi-label classification challenges, improving driving scene identification performance and efficiency.

Abstract: Driving scene identification, which assigns multiple non-exclusive class
labels to a scene, provides the contextual awareness necessary for enhancing
autonomous vehicles' ability to understand, reason about, and interact with the
complex driving environment. As a multi-label classification problem, it is
better tackled via multitasking learning. However, directly training a
multi-label classification model for driving scene identification through
multitask learning presents two main challenges: acquiring a balanced,
comprehensively annotated multi-label dataset and balancing learning across
different tasks. This paper introduces a novel learning system that synergizes
knowledge acquisition and accumulation (KAA) with consistency-based active
learning (CAL) to address those challenges. KAA acquires and accumulates
knowledge about scene identification from various single-label datasets via
monotask learning. Subsequently, CAL effectively resolves the knowledge gap
caused by the discrepancy between the marginal distributions of individual
attributes and their joint distribution. An ablation study on our Driving Scene
Identification (DSI) dataset demonstrates a 56.1% performance increase over the
baseline model pretrained on ImageNet. Of this, KAA accounts for 31.3% of the
gain, and CAL contributes 24.8%. Moreover, KAA-CAL stands out as the best
performer when compared to state-of-the-art (SOTA) multi-label models on two
public datasets, BDD100K and HSD, achieving this while using 85% less data. The
DSI dataset and the implementation code for KAA-CAL are available at
https://github.com/KELISBU/KAA-CAL .

</details>


### [181] [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
*Shoubin Yu,Yue Zhang,Ziyang Wang,Jaehong Yoon,Mohit Bansal*

Main category: cs.CV

TL;DR: MEXA is a training-free framework for multimodal reasoning that dynamically selects and aggregates expert models based on input modality and task demands, improving performance across diverse domains.


<details>
  <summary>Details</summary>
Motivation: The challenge of unifying diverse input modalities and task complexities in multimodal reasoning, such as medical diagnosis and financial forecasting, motivates the need for a scalable framework.

Method: MEXA selects modality- and task-aware expert models, generates interpretable textual reasoning outputs, and aggregates them using a Large Reasoning Model (LRM) for final answers.

Result: MEXA outperforms strong baselines in benchmarks like Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA.

Conclusion: MEXA's modular, training-free design enables effective and transparent multimodal reasoning across diverse domains.

Abstract: Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.

</details>


### [182] [RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking](https://arxiv.org/abs/2506.17119)
*Teng Guo,Jingjin Yu*

Main category: cs.CV

TL;DR: RGBTrack is a robust, real-time 6D pose estimation and tracking framework using only RGB data, eliminating depth input. It combines a binary search strategy, render-and-compare, 2D tracking, and scale recovery for dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: To enable precise object pose tracking without depth input, addressing challenges like rapid movements and occlusions.

Method: Uses a binary search strategy with render-and-compare, integrates XMem for 2D tracking, a Kalman filter, and a state machine for recovery. Includes a scale recovery module for unknown CAD model scales.

Result: Achieves competitive accuracy and real-time performance on benchmark datasets.

Conclusion: RGBTrack is a practical solution for robotics, AR, and computer vision, with publicly available source code.

Abstract: We introduce a robust framework, RGBTrack, for real-time 6D pose estimation
and tracking that operates solely on RGB data, thereby eliminating the need for
depth input for such dynamic and precise object pose tracking tasks. Building
on the FoundationPose architecture, we devise a novel binary search strategy
combined with a render-and-compare mechanism to efficiently infer depth and
generate robust pose hypotheses from true-scale CAD models. To maintain stable
tracking in dynamic scenarios, including rapid movements and occlusions,
RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman
filter and a state machine for proactive object pose recovery. In addition,
RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale
using an initial depth estimate, enabling seamless integration with modern
generative reconstruction techniques. Extensive evaluations on benchmark
datasets demonstrate that RGBTrack's novel depth-free approach achieves
competitive accuracy and real-time performance, making it a promising practical
solution candidate for application areas including robotics, augmented reality,
and computer vision.
  The source code for our implementation will be made publicly available at
https://github.com/GreatenAnoymous/RGBTrack.git.

</details>


### [183] [Dynamic Watermark Generation for Digital Images using Perimeter Gated SPAD Imager PUFs](https://arxiv.org/abs/2506.17134)
*Md Sakibur Sajal,Marc Dandin*

Main category: cs.CV

TL;DR: A novel watermarking technique using pgSPAD imagers is proposed, leveraging DSNU for source identification and tamper detection with controllable sensitivity-robustness trade-off.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of SPAD imagers for digital watermarking, unlike previous work focused on CIS and APS.

Method: Utilized DSNU of three 64x64 pgSPAD imager chips fabricated in a 0.35Œºm CMOS process, analyzing simulated watermarks on standard test images.

Result: Achieved source identification and tamper detection using dynamic watermarks with adjustable sensitivity-robustness.

Conclusion: pgSPAD imagers are viable for watermarking, offering unique advantages in security applications.

Abstract: Digital image watermarks as a security feature can be derived from the
imager's physically unclonable functions (PUFs) by utilizing the manufacturing
variations, i.e., the dark signal non-uniformity (DSNU). While a few
demonstrations focused on the CMOS image sensors (CIS) and active pixel sensors
(APS), single photon avalanche diode (SPAD) imagers have never been
investigated for this purpose. In this work, we have proposed a novel
watermarking technique using perimeter gated SPAD (pgSPAD) imagers. We utilized
the DSNU of three 64 x 64 pgSPAD imager chips, fabricated in a 0.35 {\mu}m
standard CMOS process and analyzed the simulated watermarks for standard test
images from publicly available database. Our observation shows that both source
identification and tamper detection can be achieved using the proposed
source-scene-specific dynamic watermarks with a controllable
sensitivity-robustness trade-off.

</details>


### [184] [Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations](https://arxiv.org/abs/2506.17136)
*Dongdong Meng,Sheng Li,Hao Wu,Guoping Wang,Xueqing Yan*

Main category: cs.CV

TL;DR: A novel semi-supervised multi-modal medical image segmentation method is proposed, leveraging complementary multi-modal data and contrastive mutual learning to improve performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of effectively utilizing unlabeled data in semi-supervised multi-modal medical image segmentation, where existing methods struggle with complex backgrounds and tasks.

Method: Multi-stage multi-modal fusion and enhancement strategy to reduce feature discrepancies and improve alignment, combined with contrastive mutual learning for prediction consistency.

Result: Superior performance and robustness demonstrated on two multi-modal datasets.

Conclusion: The proposed framework shows significant potential for medical image segmentation in complex scenarios with limited labeled data.

Abstract: Semi-supervised learning addresses the issue of limited annotations in
medical images effectively, but its performance is often inadequate for complex
backgrounds and challenging tasks. Multi-modal fusion methods can significantly
improve the accuracy of medical image segmentation by providing complementary
information. However, they face challenges in achieving significant
improvements under semi-supervised conditions due to the challenge of
effectively leveraging unlabeled data. There is a significant need to create an
effective and reliable multi-modal learning strategy for leveraging unlabeled
data in semi-supervised segmentation. To address these issues, we propose a
novel semi-supervised multi-modal medical image segmentation approach, which
leverages complementary multi-modal information to enhance performance with
limited labeled data. Our approach employs a multi-stage multi-modal fusion and
enhancement strategy to fully utilize complementary multi-modal information,
while reducing feature discrepancies and enhancing feature sharing and
alignment. Furthermore, we effectively introduce contrastive mutual learning to
constrain prediction consistency across modalities, thereby facilitating the
robustness of segmentation results in semi-supervised tasks. Experimental
results on two multi-modal datasets demonstrate the superior performance and
robustness of the proposed framework, establishing its valuable potential for
solving medical image segmentation tasks in complex scenarios.

</details>


### [185] [On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting](https://arxiv.org/abs/2506.17137)
*Zhuonan Liang,Dongnan Liu,Jianan Fan,Yaxuan Song,Qiang Qu,Yu Yao,Peng Fu,Weidong Cai*

Main category: cs.CV

TL;DR: The paper proposes a theoretical framework for conditional feature alignment to address domain adaptation challenges in object counting, showing improved performance over unconditional methods.


<details>
  <summary>Details</summary>
Motivation: Standard domain adaptation fails in object counting due to density shifts, which are task-relevant. The paper aims to address this by aligning features conditionally.

Method: The authors formalize conditional divergence, partition domains into subsets, and derive a joint error bound. They propose a practical adaptation strategy for unsupervised domain-adaptive counting.

Result: Experiments on multiple datasets show the method outperforms existing unsupervised domain adaptation techniques.

Conclusion: Conditional feature alignment improves cross-domain generalization for counting by preserving task-relevant variations and filtering nuisance shifts.

Abstract: Object counting models suffer when deployed across domains with differing
density variety, since density shifts are inherently task-relevant and violate
standard domain adaptation assumptions. To address this, we propose a
theoretical framework of conditional feature alignment. We first formalize the
notion of conditional divergence by partitioning each domain into subsets
(e.g., object vs. background) and measuring divergences per condition. We then
derive a joint error bound showing that, under discrete label spaces treated as
condition sets, aligning distributions conditionally leads to tighter bounds on
the combined source-target decision error than unconditional alignment. These
insights motivate a general conditional adaptation principle: by preserving
task-relevant variations while filtering out nuisance shifts, one can achieve
superior cross-domain generalization for counting. We provide both defining
conditional divergence then proving its benefit in lowering joint error and a
practical adaptation strategy that preserves task-relevant information in
unsupervised domain-adaptive counting. We demonstrate the effectiveness of our
approach through extensive experiments on multiple counting datasets with
varying density distributions. The results show that our method outperforms
existing unsupervised domain adaptation methods, empirically validating the
theoretical insights on conditional feature alignment.

</details>


### [186] [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/abs/2506.17144)
*Ritabrata Chakraborty,Rajatsubhra Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: The paper proposes a text-based approach using LLMs for soccer action spotting, replacing traditional video-centric methods, and demonstrates its effectiveness on the SoccerNet Echoes dataset.


<details>
  <summary>Details</summary>
Motivation: To create a lightweight, scalable alternative to computationally expensive video-based action spotting by leveraging expert commentary and LLMs.

Method: Uses three specialized LLMs to evaluate timestamped commentary for identifying key actions like goals and cards, generating accurate event timestamps.

Result: The language-centric approach effectively detects critical match events without training, offering a viable alternative to video-based methods.

Conclusion: Text-based action spotting with LLMs is a lightweight, training-free solution that performs comparably to traditional video-based approaches.

Abstract: Traditional video-based tasks like soccer action spotting rely heavily on
visual inputs, often requiring complex and computationally expensive models to
process dense video data. In this work, we propose a shift from this
video-centric approach to a text-based task, making it lightweight and scalable
by utilizing Large Language Models (LLMs) instead of Vision-Language Models
(VLMs). We posit that expert commentary, which provides rich, fine-grained
descriptions and contextual cues such as excitement and tactical insights,
contains enough information to reliably spot key actions in a match. To
demonstrate this, we use the SoccerNet Echoes dataset, which provides
timestamped commentary, and employ a system of three LLMs acting as judges
specializing in outcome, excitement, and tactics. Each LLM evaluates sliding
windows of commentary to identify actions like goals, cards, and substitutions,
generating accurate timestamps for these events. Our experiments show that this
language-centric approach performs effectively in detecting critical match
events, providing a lightweight and training-free alternative to traditional
video-based methods for action spotting.

</details>


### [187] [Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation](https://arxiv.org/abs/2506.17159)
*Qing Xu,Yuxiang Luo,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: The paper proposes Co-Seg++, a framework for medical image segmentation that jointly addresses semantic and instance segmentation tasks, improving performance by leveraging their interdependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat segmentation tasks in isolation, ignoring interdependencies, leading to suboptimal results. The goal is to enhance segmentation performance and medical image understanding.

Method: Co-Seg++ introduces a co-segmentation paradigm with a spatio-temporal prompt encoder (STP-Encoder) for spatial constraints and a multi-task collaborative decoder (MTC-Decoder) for contextual consistency.

Result: Experiments on CT and histopathology datasets show Co-Seg++ outperforms state-of-the-art methods in semantic, instance, and panoptic segmentation.

Conclusion: Co-Seg++ effectively integrates semantic and instance segmentation, demonstrating superior performance and versatility in medical image analysis.

Abstract: Medical image analysis is critical yet challenged by the need of jointly
segmenting organs or tissues, and numerous instances for anatomical structures
and tumor microenvironment analysis. Existing studies typically formulated
different segmentation tasks in isolation, which overlooks the fundamental
interdependencies between these tasks, leading to suboptimal segmentation
performance and insufficient medical image understanding. To address this
issue, we propose a Co-Seg++ framework for versatile medical segmentation.
Specifically, we introduce a novel co-segmentation paradigm, allowing semantic
and instance segmentation tasks to mutually enhance each other. We first devise
a spatio-temporal prompt encoder (STP-Encoder) to capture long-range spatial
and temporal relationships between segmentation regions and image embeddings as
prior spatial constraints. Moreover, we devise a multi-task collaborative
decoder (MTC-Decoder) that leverages cross-guidance to strengthen the
contextual consistency of both tasks, jointly computing semantic and instance
segmentation masks. Extensive experiments on diverse CT and histopathology
datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts
in the semantic, instance, and panoptic segmentation of dental anatomical
structures, histopathology tissues, and nuclei instances. The source code is
available at https://github.com/xq141839/Co-Seg-Plus.

</details>


### [188] [YASMOT: Yet another stereo image multi-object tracker](https://arxiv.org/abs/2506.17186)
*Ketil Malde*

Main category: cs.CV

TL;DR: yasmot is a lightweight, flexible object tracker for video or image sequences, compatible with monoscopic or stereoscopic cameras, and supports ensemble detector outputs.


<details>
  <summary>Details</summary>
Motivation: Tracking objects over time improves detection performance and is essential for tasks like behavior classification and abundance estimation.

Method: Processes outputs from popular object detectors to track objects, including generating consensus detections from ensembles.

Result: A versatile tracker (yasmot) that works with various camera setups and detector outputs.

Conclusion: yasmot offers a practical solution for object tracking in image time series, enhancing downstream applications.

Abstract: There now exists many popular object detectors based on deep learning that
can analyze images and extract locations and class labels for occurrences of
objects. For image time series (i.e., video or sequences of stills), tracking
objects over time and preserving object identity can help to improve object
detection performance, and is necessary for many downstream tasks, including
classifying and predicting behaviors, and estimating total abundances. Here we
present yasmot, a lightweight and flexible object tracker that can process the
output from popular object detectors and track objects over time from either
monoscopic or stereoscopic camera configurations. In addition, it includes
functionality to generate consensus detections from ensembles of object
detectors.

</details>


### [189] [Facial Landmark Visualization and Emotion Recognition Through Neural Networks](https://arxiv.org/abs/2506.17191)
*Israel Ju√°rez-Jim√©nez,Tiffany Guadalupe Mart√≠nez Paredes,Jes√∫s Garc√≠a-Ram√≠rez,Eric Ramos Aguilar*

Main category: cs.CV

TL;DR: Proposes facial landmark box plots for dataset analysis and compares two facial landmark feature sets, finding neural networks outperform random forests.


<details>
  <summary>Details</summary>
Motivation: Improving emotion recognition from facial images by addressing dataset analysis gaps and visualizing facial landmarks.

Method: Introduces facial landmark box plots for outlier detection and compares absolute positions vs. displacements of landmarks.

Result: Neural networks perform better than random forest classifiers in emotion recognition.

Conclusion: Facial landmark box plots and neural networks enhance emotion recognition accuracy.

Abstract: Emotion recognition from facial images is a crucial task in human-computer
interaction, enabling machines to learn human emotions through facial
expressions. Previous studies have shown that facial images can be used to
train deep learning models; however, most of these studies do not include a
through dataset analysis. Visualizing facial landmarks can be challenging when
extracting meaningful dataset insights; to address this issue, we propose
facial landmark box plots, a visualization technique designed to identify
outliers in facial datasets. Additionally, we compare two sets of facial
landmark features: (i) the landmarks' absolute positions and (ii) their
displacements from a neutral expression to the peak of an emotional expression.
Our results indicate that a neural network achieves better performance than a
random forest classifier.

</details>


### [190] [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](https://arxiv.org/abs/2506.17201)
*Jiaqi Li,Junshu Tang,Zhiyong Xu,Longhuang Wu,Yuan Zhou,Shuai Shao,Tianbao Yu,Zhiguo Cao,Qinglin Lu*

Main category: cs.CV

TL;DR: Hunyuan-GameCraft is a framework for high-dynamic interactive video generation in games, addressing limitations in dynamics, consistency, and efficiency. It unifies inputs, uses hybrid training, and employs model distillation for real-time deployment, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods for games lack dynamics, generality, long-term consistency, and efficiency, limiting gameplay video creation.

Method: Unifies keyboard/mouse inputs into a shared camera space, uses hybrid history-conditioned training, and applies model distillation for efficiency. Trained on a large dataset of gameplay recordings and fine-tuned on synthetic data.

Result: Outperforms existing models in realism and playability, with improved visual fidelity and controllability.

Conclusion: Hunyuan-GameCraft advances interactive game video generation, offering high realism and playability for real-time deployment.

Abstract: Recent advances in diffusion-based and controllable video generation have
enabled high-quality and temporally coherent video synthesis, laying the
groundwork for immersive interactive gaming experiences. However, current
methods face limitations in dynamics, generality, long-term consistency, and
efficiency, which limit the ability to create various gameplay videos. To
address these gaps, we introduce Hunyuan-GameCraft, a novel framework for
high-dynamic interactive video generation in game environments. To achieve
fine-grained action control, we unify standard keyboard and mouse inputs into a
shared camera representation space, facilitating smooth interpolation between
various camera and movement operations. Then we propose a hybrid
history-conditioned training strategy that extends video sequences
autoregressively while preserving game scene information. Additionally, to
enhance inference efficiency and playability, we achieve model distillation to
reduce computational overhead while maintaining consistency across long
temporal sequences, making it suitable for real-time deployment in complex
interactive environments. The model is trained on a large-scale dataset
comprising over one million gameplay recordings across over 100 AAA games,
ensuring broad coverage and diversity, then fine-tuned on a carefully annotated
synthetic dataset to enhance precision and control. The curated game scene data
significantly improves the visual fidelity, realism and action controllability.
Extensive experiments demonstrate that Hunyuan-GameCraft significantly
outperforms existing models, advancing the realism and playability of
interactive game video generation.

</details>


### [191] [UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.17202)
*Teng Li,Quanfeng Lu,Lirui Zhao,Hao Li,Xizhou Zhu,Yu Qiao,Jun Zhang,Wenqi Shao*

Main category: cs.CV

TL;DR: UniFork, a Y-shaped architecture, balances shared learning and task specialization for unified image understanding and generation, outperforming fully shared Transformer models.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of designing optimal architectures for unified image understanding and generation, identifying divergent modality alignment patterns in understanding and generation tasks.

Method: Analyzes modality alignment in expert and unified models, then proposes UniFork, a Y-shaped architecture with shared shallow layers and task-specific deeper branches.

Result: UniFork outperforms fully shared Transformer architectures and matches or exceeds task-specific models.

Conclusion: UniFork effectively resolves the conflict between understanding and generation tasks by balancing shared and task-specific learning.

Abstract: Unified image understanding and generation has emerged as a promising
paradigm in multimodal artificial intelligence. Despite recent progress, the
optimal architectural design for such unified models remains an open challenge.
In this work, we start by analyzing the modality alignment behaviors of
task-specific expert models for understanding and generation, as well as
current unified models. Our analysis reveals a crucial observation:
understanding tasks benefit from a progressively increasing modality alignment
across network depth, which helps build up semantic information for better
comprehension; In contrast, generation tasks follow a different trend: modality
alignment increases in the early layers but decreases in the deep layers to
recover spatial details. These divergent alignment patterns create a
fundamental conflict in fully shared Transformer backbones, where a uniform
representational flow often leads to performance compromises across two tasks.
Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture
that shares the shallow layers for cross-task representation learning, while
employing task-specific branches in deeper layers to avoid task interference.
This design effectively balances shared learning and task specialization.
Through extensive ablation experiments, we demonstrate that Unifork
consistently outperforms conventional fully shared Transformer architectures,
and achieves performance on par with or better than task-specific models.

</details>


### [192] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: Part$^{2}$GS is a novel framework for modeling articulated objects with high-fidelity geometry and physically consistent motion, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Articulated objects are common but challenging to model accurately in 3D reconstruction.

Method: Uses a part-aware 3D Gaussian representation with learnable attributes and physics-based constraints for motion consistency.

Result: Outperforms baselines by up to 10x in Chamfer Distance for movable parts.

Conclusion: Part$^{2}$GS advances articulated object modeling with high-fidelity and physically consistent results.

Abstract: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.

</details>


### [193] [Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation](https://arxiv.org/abs/2506.17213)
*Xiuyu Yang,Shuhan Tan,Philipp Kr√§henb√ºhl*

Main category: cs.CV

TL;DR: InfGen is a next-token prediction model for traffic simulation, excelling in both short-term (9s) and long-term (30s) scenarios by interleaving closed-loop motion simulation and scene generation.


<details>
  <summary>Details</summary>
Motivation: Current traffic simulators focus on closed-loop motion for initial agents, failing to handle long-term simulations where agents enter and exit dynamically.

Method: InfGen uses a unified next-token prediction model to switch between closed-loop motion simulation and scene generation, enabling stable long-term rollouts.

Result: InfGen achieves state-of-the-art performance in short-term simulation and outperforms others in long-term simulation.

Conclusion: InfGen addresses the limitations of prior models, offering a robust solution for realistic long-term traffic simulation.

Abstract: An ideal traffic simulator replicates the realistic long-term point-to-point
trip that a self-driving system experiences during deployment. Prior models and
benchmarks focus on closed-loop motion simulation for initial agents in a
scene. This is problematic for long-term simulation. Agents enter and exit the
scene as the ego vehicle enters new regions. We propose InfGen, a unified
next-token prediction model that performs interleaved closed-loop motion
simulation and scene generation. InfGen automatically switches between
closed-loop motion simulation and scene generation mode. It enables stable
long-term rollout simulation. InfGen performs at the state-of-the-art in
short-term (9s) traffic simulation, and significantly outperforms all other
methods in long-term (30s) simulation. The code and model of InfGen will be
released at https://orangesodahub.github.io/InfGen

</details>


### [194] [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens](https://arxiv.org/abs/2506.17218)
*Zeyuan Yang,Xueyang Yu,Delin Chen,Maohao Shen,Chuang Gan*

Main category: cs.CV

TL;DR: Mirage enhances VLMs by using latent visual tokens for multimodal reasoning without explicit image generation, improving performance.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with tasks requiring visual imagination due to text-only decoding. Mirage aims to mimic human mental imagery for better reasoning.

Method: Mirage augments VLM decoding with latent visual tokens, supervised via distillation and reinforcement learning, avoiding explicit image generation.

Result: Mirage improves multimodal reasoning on benchmarks without generating pixel-level images.

Conclusion: Mirage demonstrates the potential of latent visual tokens for enhancing VLM reasoning, avoiding the drawbacks of explicit image generation.

Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their
text-only decoding forces them to verbalize visual reasoning, limiting
performance on tasks that demand visual imagination. Recent attempts train VLMs
to render explicit images, but the heavy image-generation pre-training often
hinders the reasoning ability. Inspired by the way humans reason with mental
imagery-the internal construction and manipulation of visual cues-we
investigate whether VLMs can reason through interleaved multimodal trajectories
without producing explicit images. To this end, we present a Machine Mental
Imagery framework, dubbed as Mirage, which augments VLM decoding with latent
visual tokens alongside ordinary text. Concretely, whenever the model chooses
to ``think visually'', it recasts its hidden states as next tokens, thereby
continuing a multimodal trajectory without generating pixel-level images. Begin
by supervising the latent tokens through distillation from ground-truth image
embeddings, we then switch to text-only supervision to make the latent
trajectory align tightly with the task objective. A subsequent reinforcement
learning stage further enhances the multimodal reasoning capability.
Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger
multimodal reasoning without explicit image generation.

</details>


### [195] [Emergent Temporal Correspondences from Video Diffusion Transformers](https://arxiv.org/abs/2506.17220)
*Jisu Nam,Soowon Son,Dahyun Chung,Jiyoung Kim,Siyoon Jin,Junhwa Hur,Seungryong Kim*

Main category: cs.CV

TL;DR: DiffTrack is a framework analyzing how video diffusion models (DiTs) establish temporal correspondences, revealing key insights into their mechanisms and enabling practical applications like zero-shot tracking and motion-enhanced video generation.


<details>
  <summary>Details</summary>
Motivation: To understand how video diffusion models internally represent temporal correspondences across frames, a question not yet addressed in existing research.

Method: DiffTrack constructs a dataset with pseudo ground-truth tracking annotations and introduces evaluation metrics to analyze the 3D attention mechanism of DiTs, focusing on representations, layers, and timesteps.

Result: Query-key similarities in specific layers are critical for temporal matching, becoming more prominent during denoising. DiffTrack achieves state-of-the-art performance in zero-shot point tracking and improves video generation consistency.

Conclusion: DiffTrack provides insights into video DiTs' temporal understanding, laying groundwork for future research and applications.

Abstract: Recent advancements in video diffusion models based on Diffusion Transformers
(DiTs) have achieved remarkable success in generating temporally coherent
videos. Yet, a fundamental question persists: how do these models internally
establish and represent temporal correspondences across frames? We introduce
DiffTrack, the first quantitative analysis framework designed to answer this
question. DiffTrack constructs a dataset of prompt-generated video with pseudo
ground-truth tracking annotations and proposes novel evaluation metrics to
systematically analyze how each component within the full 3D attention
mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to
establishing temporal correspondences. Our analysis reveals that query-key
similarities in specific, but not all, layers play a critical role in temporal
matching, and that this matching becomes increasingly prominent during the
denoising process. We demonstrate practical applications of DiffTrack in
zero-shot point tracking, where it achieves state-of-the-art performance
compared to existing vision foundation and self-supervised video models.
Further, we extend our findings to motion-enhanced video generation with a
novel guidance method that improves temporal consistency of generated videos
without additional training. We believe our work offers crucial insights into
the inner workings of video DiTs and establishes a foundation for further
research and applications leveraging their temporal understanding.

</details>


### [196] [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.17221)
*Zhangyang Qi,Zhixiong Zhang,Yizhou Yu,Jiaqi Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: VLN-R1 is an end-to-end framework using Large Vision-Language Models (LVLM) for continuous navigation, trained with GRPO-based methods and a two-stage approach (SFT and RFT), achieving strong results on the VLN-CE benchmark.


<details>
  <summary>Details</summary>
Motivation: Current navigation systems rely on discrete topological graphs, limiting flexibility. VLN-R1 aims to enable continuous navigation using LVLMs for more natural and adaptable path planning.

Method: Proposes VLN-R1, leveraging LVLMs to translate egocentric video into actions. Uses GRPO-based training, constructs the VLN-Ego dataset, and employs a two-stage training approach: SFT for alignment with expert demonstrations and RFT with Time-Decayed Reward for multi-step action weighting.

Result: VLN-R1 achieves strong performance on the VLN-CE benchmark, demonstrating LVLMs' potential for embodied navigation and task-specific reasoning.

Conclusion: VLN-R1 successfully integrates LVLMs for continuous navigation, proving their effectiveness in embodied AI through data-efficient, reward-driven training.

Abstract: Vision-Language Navigation (VLN) is a core challenge in embodied AI,
requiring agents to navigate real-world environments using natural language
instructions. Current language model-based navigation systems operate on
discrete topological graphs, limiting path planning to predefined node
connections. We propose VLN-R1, an end-to-end framework that leverages Large
Vision-Language Models (LVLM) to directly translate egocentric video streams
into continuous navigation actions, adopting GRPO-based training inspired by
DeepSeek-R1. To enable effective training, we first construct the VLN-Ego
dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling
to balance historical and current observations. While large language models can
supervise complete textual instructions, they lack fine-grained action-level
control. Our framework employs a two-stage training approach: a) Supervised
fine-tuning (SFT) to align the model's action sequence text predictions with
expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced
with a Time-Decayed Reward (TDR) mechanism that strategically weights
multi-step future actions. Experimental results show VLN-R1 achieves strong
performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied
navigation and enhance task-specific reasoning through data-efficient,
reward-driven post-training.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [197] [PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps](https://arxiv.org/abs/2506.15849)
*Kirill Muravyev,Vasily Yuryev,Oleg Bulichev,Dmitry Yudin,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: PRISM-Loc is a topological map-based localization method for large environments, combining global place recognition and local pose estimation with a novel lidar scan matching algorithm. It outperforms competitors in quality and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Localization in large environments using dense global lidar maps is memory-intensive and computationally challenging. Topological maps offer a viable alternative.

Method: PRISM-Loc uses a twofold pipeline: global place recognition and local pose estimation with a 2D feature-based lidar scan matching algorithm.

Result: Evaluated on a 3 km route, PRISM-Loc outperforms state-of-the-art metric map-based and place recognition-based methods in quality and computational efficiency.

Conclusion: PRISM-Loc is an effective solution for localization in large environments, offering superior performance and efficiency.

Abstract: Localization in the environment is one of the crucial tasks of navigation of
a mobile robot or a self-driving vehicle. For long-range routes, performing
localization within a dense global lidar map in real time may be difficult, and
the creation of such a map may require much memory. To this end, leveraging
topological maps may be useful. In this work, we propose PRISM-Loc -- a
topological map-based approach for localization in large environments. The
proposed approach leverages a twofold localization pipeline, which consists of
global place recognition and estimation of the local pose inside the found
location. For local pose estimation, we introduce an original lidar scan
matching algorithm, which is based on 2D features and point-based optimization.
We evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and
compare it against the state-of-the-art metric map-based and place
recognition-based competitors. The results of the experiments show that the
proposed method outperforms its competitors both quality-wise and
computationally-wise.

</details>


### [198] [Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles](https://arxiv.org/abs/2506.15851)
*Qiyuan Wu,Mark Campbell*

Main category: cs.RO

TL;DR: The paper proposes a method for uncertainty quantification in visual localization for autonomous driving, using a lightweight sensor error model to predict non-Gaussian measurement errors under varying conditions.


<details>
  <summary>Details</summary>
Motivation: Accurate uncertainty quantification is essential for safety-critical applications like self-driving cars, especially under diverse environmental conditions.

Method: A lightweight sensor error model maps image features and semantic information to a 2D error distribution, capturing contextual factors implicitly.

Result: The method outperforms Gaussian models, especially in poor weather and lighting, as validated on the Ithaca365 dataset.

Conclusion: The proposed Gaussian Mixture model effectively quantifies uncertainty, improving localization accuracy in challenging conditions.

Abstract: The uncertainty quantification of sensor measurements coupled with deep
learning networks is crucial for many robotics systems, especially for
safety-critical applications such as self-driving cars. This paper develops an
uncertainty quantification approach in the context of visual localization for
autonomous driving, where locations are selected based on images. Key to our
approach is to learn the measurement uncertainty using light-weight sensor
error model, which maps both image feature and semantic information to
2-dimensional error distribution. Our approach enables uncertainty estimation
conditioned on the specific context of the matched image pair, implicitly
capturing other critical, unannotated factors (e.g., city vs highway, dynamic
vs static scenes, winter vs summer) in a latent manner. We demonstrate the
accuracy of our uncertainty prediction framework using the Ithaca365 dataset,
which includes variations in lighting and weather (sunny, night, snowy). Both
the uncertainty quantification of the sensor+network is evaluated, along with
Bayesian localization filters using unique sensor gating method. Results show
that the measurement error does not follow a Gaussian distribution with poor
weather and lighting conditions, and is better predicted by our Gaussian
Mixture model.

</details>


### [199] [Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments](https://arxiv.org/abs/2506.16050)
*Jiawen Yu,Jieji Ren,Yang Chang,Qiaojun Yu,Xuan Tong,Boyang Wang,Yan Song,You Li,Xinji Mai,Wenqiang Zhang*

Main category: cs.RO

TL;DR: A novel anomaly detection method (HetNet) improves industrial defect detection in complex environments, outperforming existing methods by ~10%.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with defect detection in unstructured industrial settings due to varying conditions like illumination and poses.

Method: Proposes HetNet, combining a collaborative distillation heterogeneous teacher network, adaptive feature fusion, and Gaussian noise generation.

Result: HetNet achieves ~10% better performance on MSC-AD and state-of-the-art results on other datasets, proving resilience to environmental changes.

Conclusion: HetNet is effective for real-time, robust anomaly detection in industrial production, validated by real-world tests.

Abstract: Anomaly detection and localization in automated industrial manufacturing can
significantly enhance production efficiency and product quality. Existing
methods are capable of detecting surface defects in pre-defined or controlled
imaging environments. However, accurately detecting workpiece defects in
complex and unstructured industrial environments with varying views, poses and
illumination remains challenging. We propose a novel anomaly detection and
localization method specifically designed to handle inputs with perturbative
patterns. Our approach introduces a new framework based on a collaborative
distillation heterogeneous teacher network (HetNet), an adaptive local-global
feature fusion module, and a local multivariate Gaussian noise generation
module. HetNet can learn to model the complex feature distribution of normal
patterns using limited information about local disruptive changes. We conducted
extensive experiments on mainstream benchmarks. HetNet demonstrates superior
performance with approximately 10% improvement across all evaluation metrics on
MSC-AD under industrial conditions, while achieving state-of-the-art results on
other datasets, validating its resilience to environmental fluctuations and its
capability to enhance the reliability of industrial anomaly detection systems
across diverse scenarios. Tests in real-world environments further confirm that
HetNet can be effectively integrated into production lines to achieve robust
and real-time anomaly detection. Codes, images and videos are published on the
project website at: https://zihuatanejoyu.github.io/HetNet/

</details>


### [200] [FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation](https://arxiv.org/abs/2506.16201)
*Sen Wang,Le Wang,Sanping Zhou,Jingyi Tian,Jiayi Li,Haowen Sun,Wei Tang*

Main category: cs.RO

TL;DR: FlowRAM is a novel framework using generative models for efficient robotic manipulation, achieving state-of-the-art performance in high-precision tasks with faster inference.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based methods are computationally inefficient and lack exploration of generative models for 3D environments.

Method: FlowRAM employs Dynamic Radius Schedule for adaptive perception, state space models for multimodal integration, and conditional flow matching for action pose learning.

Result: FlowRAM outperforms previous methods by 12.0% in success rate and achieves faster inference (under 4 time steps).

Conclusion: FlowRAM enhances robotic manipulation efficiency and precision, making it suitable for real-world applications.

Abstract: Robotic manipulation in high-precision tasks is essential for numerous
industrial and real-world applications where accuracy and speed are required.
Yet current diffusion-based policy learning methods generally suffer from low
computational efficiency due to the iterative denoising process during
inference. Moreover, these methods do not fully explore the potential of
generative models for enhancing information exploration in 3D environments. In
response, we propose FlowRAM, a novel framework that leverages generative
models to achieve region-aware perception, enabling efficient multimodal
information processing. Specifically, we devise a Dynamic Radius Schedule,
which allows adaptive perception, facilitating transitions from global scene
comprehension to fine-grained geometric details. Furthermore, we integrate
state space models to integrate multimodal information, while preserving linear
computational complexity. In addition, we employ conditional flow matching to
learn action poses by regressing deterministic vector fields, simplifying the
learning process while maintaining performance. We verify the effectiveness of
the FlowRAM in the RLBench, an established manipulation benchmark, and achieve
state-of-the-art performance. The results demonstrate that FlowRAM achieves a
remarkable improvement, particularly in high-precision tasks, where it
outperforms previous methods by 12.0% in average success rate. Additionally,
FlowRAM is able to generate physically plausible actions for a variety of
real-world tasks in less than 4 time steps, significantly increasing inference
speed.

</details>


### [201] [Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control](https://arxiv.org/abs/2506.16565)
*Yuxin Chen,Jianglan Wei,Chenfeng Xu,Boyi Li,Masayoshi Tomizuka,Andrea Bajcsy,Ran Tian*

Main category: cs.RO

TL;DR: Reimagination with Observation Intervention (ReOI) improves world model robustness by detecting and removing visual distractors, enhancing action outcome predictions in novel scenarios.


<details>
  <summary>Details</summary>
Motivation: World models struggle with novel visual distractors, leading to unreliable predictions. ReOI addresses this by intervening at test time to improve robustness.

Method: ReOI detects distractors, modifies observations to remove them, reimagines outcomes, and reintroduces distractors post-hoc for visual consistency.

Result: ReOI boosts task success rates by up to 3x in novel distractor scenarios, outperforming baseline methods.

Conclusion: ReOI effectively enhances world model reliability in open-world settings with visual distractors.

Abstract: World models enable robots to "imagine" future observations given current
observations and planned actions, and have been increasingly adopted as
generalized dynamics models to facilitate robot learning. Despite their
promise, these models remain brittle when encountering novel visual distractors
such as objects and background elements rarely seen during training.
Specifically, novel distractors can corrupt action outcome predictions, causing
downstream failures when robots rely on the world model imaginations for
planning or action verification. In this work, we propose Reimagination with
Observation Intervention (ReOI), a simple yet effective test-time strategy that
enables world models to predict more reliable action outcomes in open-world
scenarios where novel and unanticipated visual distractors are inevitable.
Given the current robot observation, ReOI first detects visual distractors by
identifying which elements of the scene degrade in physically implausible ways
during world model prediction. Then, it modifies the current observation to
remove these distractors and bring the observation closer to the training
distribution. Finally, ReOI "reimagines" future outcomes with the modified
observation and reintroduces the distractors post-hoc to preserve visual
consistency for downstream planning and verification. We validate our approach
on a suite of robotic manipulation tasks in the context of action verification,
where the verifier needs to select desired action plans based on predictions
from a world model. Our results show that ReOI is robust to both
in-distribution and out-of-distribution visual distractors. Notably, it
improves task success rates by up to 3x in the presence of novel distractors,
significantly outperforming action verification that relies on world model
predictions without imagination interventions.

</details>


### [202] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: A framework using Vision-Language Models (VLMs) to resolve ambiguities in robotic manipulation tasks by generating interpretable code and 3D attention maps.


<details>
  <summary>Details</summary>
Motivation: Address ambiguity and vagueness in natural language instructions for robotic tasks, overcoming limitations of end-to-end models.

Method: Uses a VLM to interpret instructions, generates task-specific code, and integrates spatial-semantic info via 3D attention maps.

Result: Outperforms in tasks with language ambiguity, contact-rich manipulation, and multi-object interactions.

Conclusion: Proposed framework enhances interpretability and performance in ambiguous robotic manipulation tasks.

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


### [203] [Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping](https://arxiv.org/abs/2506.17110)
*Teng Guo,Baichuan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: MOMA is a novel framework for accurate metric depth estimation from a single RGB image, addressing limitations of current depth sensors and monocular depth models.


<details>
  <summary>Details</summary>
Motivation: Current 6D pose estimation methods rely on expensive, noisy depth sensors or fail to generalize with monocular depth models. MOMA aims to provide accurate metric depth without additional data or retraining.

Method: MOMA uses one-shot adaptation of monocular depth estimation models, performing scale-rotation-shift alignments during calibration with sparse ground-truth depth points.

Result: MOMA achieves high success rates in real-world robotic tasks like grasping and bin-picking, demonstrating strong generalization.

Conclusion: MOMA effectively recovers metric depth from RGB images, enabling robust robotic manipulation without costly sensors or extensive retraining.

Abstract: Accurate 6D object pose estimation is a prerequisite for successfully
completing robotic prehensile and non-prehensile manipulation tasks. At
present, 6D pose estimation for robotic manipulation generally relies on depth
sensors based on, e.g., structured light, time-of-flight, and stereo-vision,
which can be expensive, produce noisy output (as compared with RGB cameras),
and fail to handle transparent objects. On the other hand, state-of-the-art
monocular depth estimation models (MDEMs) provide only affine-invariant depths
up to an unknown scale and shift. Metric MDEMs achieve some successful
zero-shot results on public datasets, but fail to generalize. We propose a
novel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover
metric depth from a single RGB image, through a one-shot adaptation building on
MDEM techniques. MOMA performs scale-rotation-shift alignments during camera
calibration, guided by sparse ground-truth depth points, enabling accurate
depth estimation without additional data collection or model retraining on the
testing setup. MOMA supports fine-tuning the MDEM on transparent objects,
demonstrating strong generalization capabilities. Real-world experiments on
tabletop 2-finger grasping and suction-based bin-picking applications show MOMA
achieves high success rates in diverse tasks, confirming its effectiveness.

</details>


### [204] [Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation](https://arxiv.org/abs/2506.17198)
*Jianglong Ye,Keyi Wang,Chengjing Yuan,Ruihan Yang,Yiquan Li,Jiyue Zhu,Yuzhe Qin,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: Dex1B is a large-scale, diverse, and high-quality dataset for dexterous hand manipulation tasks, generated using a novel generative model with geometric constraints.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of creating large-scale demonstrations for dexterous hand manipulation, which is crucial for advancing robotic capabilities.

Method: Proposes a generative model integrating geometric constraints for feasibility and additional conditions for diversity, applied to produce the Dex1B dataset.

Result: Outperforms prior state-of-the-art methods in simulation benchmarks and demonstrates effectiveness in real-world robot experiments.

Conclusion: Dex1B and the proposed generative model significantly advance the field of dexterous hand manipulation by providing scalable, diverse, and high-quality demonstrations.

Abstract: Generating large-scale demonstrations for dexterous hand manipulation remains
challenging, and several approaches have been proposed in recent years to
address this. Among them, generative models have emerged as a promising
paradigm, enabling the efficient creation of diverse and physically plausible
demonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and
high-quality demonstration dataset produced with generative models. The dataset
contains one billion demonstrations for two fundamental tasks: grasping and
articulation. To construct it, we propose a generative model that integrates
geometric constraints to improve feasibility and applies additional conditions
to enhance diversity. We validate the model on both established and newly
introduced simulation benchmarks, where it significantly outperforms prior
state-of-the-art methods. Furthermore, we demonstrate its effectiveness and
robustness through real-world robot experiments. Our project page is at
https://jianglongye.com/dex1b

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [205] [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
*Zihao Fu,Chris Russell*

Main category: cs.CR

TL;DR: Dual Watermarking reduces false positives in detecting LLM-generated text by separately encoding detection and identification watermarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the false detection problem in digital watermarking when the same embedding is used for both detection and user identification.

Method: Proposes Dual Watermarking, which jointly encodes detection and identification watermarks.

Result: Significantly reduces false positives while maintaining high detection accuracy.

Conclusion: Dual Watermarking is an effective solution for mitigating misuse of generated text.

Abstract: Digital watermarking is a promising solution for mitigating some of the risks
arising from the misuse of automatically generated text. These approaches
either embed non-specific watermarks to allow for the detection of any text
generated by a particular sampler, or embed specific keys that allow the
identification of the LLM user. However, simultaneously using the same
embedding for both detection and user identification leads to a false detection
problem, whereby, as user capacity grows, unwatermarked text is increasingly
likely to be falsely detected as watermarked. Through theoretical analysis, we
identify the underlying causes of this phenomenon. Building on these insights,
we propose Dual Watermarking which jointly encodes detection and identification
watermarks into generated text, significantly reducing false positives while
maintaining high detection accuracy. Our experimental results validate our
theoretical findings and demonstrate the effectiveness of our approach.

</details>


### [206] [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
*Biao Yi,Tiansheng Huang,Sishuo Chen,Tong Li,Zheli Liu,Zhixuan Chu,Yiming Li*

Main category: cs.CR

TL;DR: BEAT is a black-box defense against backdoor unalignment attacks in LLMs, detecting triggered samples by measuring output distribution distortion after concatenation with a probe.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks compromise LLM safety alignment stealthily, posing threats in LLMaaS settings. BEAT addresses this by leveraging the probe concatenate effect.

Method: BEAT detects triggered samples by analyzing refusal rate changes when concatenated with a probe, using multiple sampling to approximate output distributions.

Result: BEAT effectively defends against various backdoor attacks and LLMs, including GPT-3.5-turbo, and shows promise against jailbreak attacks.

Conclusion: BEAT offers a robust, sample-independent defense for black-box LLMs, addressing backdoor and jailbreak threats efficiently.

Abstract: Backdoor unalignment attacks against Large Language Models (LLMs) enable the
stealthy compromise of safety alignment using a hidden trigger while evading
normal safety auditing. These attacks pose significant threats to the
applications of LLMs in the real-world Large Language Model as a Service
(LLMaaS) setting, where the deployed model is a fully black-box system that can
only interact through text. Furthermore, the sample-dependent nature of the
attack target exacerbates the threat. Instead of outputting a fixed label, the
backdoored LLM follows the semantics of any malicious command with the hidden
trigger, significantly expanding the target space. In this paper, we introduce
BEAT, a black-box defense that detects triggered samples during inference to
deactivate the backdoor. It is motivated by an intriguing observation (dubbed
the probe concatenate effect), where concatenated triggered samples
significantly reduce the refusal rate of the backdoored LLM towards a malicious
probe, while non-triggered samples have little effect. Specifically, BEAT
identifies whether an input is triggered by measuring the degree of distortion
in the output distribution of the probe before and after concatenation with the
input. Our method addresses the challenges of sample-dependent targets from an
opposite perspective. It captures the impact of the trigger on the refusal
signal (which is sample-independent) instead of sample-specific successful
attack behaviors. It overcomes black-box access limitations by using multiple
sampling to approximate the output distribution. Extensive experiments are
conducted on various backdoor attacks and LLMs (including the closed-source
GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.
Besides, we also preliminarily verify that BEAT can effectively defend against
popular jailbreak attacks, as they can be regarded as 'natural backdoors'.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [207] [GratNet: A Photorealistic Neural Shader for Diffractive Surfaces](https://arxiv.org/abs/2506.15815)
*Narayan Kandel,Daljit Singh J. S. Dhillon*

Main category: cs.GR

TL;DR: A neural network-based method for efficient and accurate rendering of diffractive surfaces, reducing memory usage significantly.


<details>
  <summary>Details</summary>
Motivation: Addressing the heavy data dependency in current wave optics models for structural coloration by introducing a data-driven approach.

Method: Uses a multi-layer perceptron (MLP) for data compression and nuanced training, avoiding overfitting and ensuring robust resampling.

Result: Achieves high-quality reconstruction of ground-truth data, outperforming state-of-the-art methods in performance and memory efficiency.

Conclusion: The MLP-based method offers a scalable and efficient solution for rendering diffractive surfaces with minimal memory footprint.

Abstract: Structural coloration is commonly modeled using wave optics for reliable and
photorealistic rendering of natural, quasi-periodic and complex nanostructures.
Such models often rely on dense, preliminary or preprocessed data to accurately
capture the nuanced variations in diffractive surface reflectances. This heavy
data dependency warrants implicit neural representation which has not been
addressed comprehensively in the current literature. In this paper, we present
a multi-layer perceptron (MLP) based method for data-driven rendering of
diffractive surfaces with high accuracy and efficiency. We primarily approach
this problem from a data compression perspective to devise a nuanced training
and modeling method which is attuned to the domain and range characteristics of
diffractive reflectance datasets. Importantly, our approach avoids over-fitting
and has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR),
Structural Similarity Index Measure (SSIM) and a flipping difference evaluator
(FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of
the ground-truth. In comparison to a recent state-of-the-art offline,
wave-optical, forward modeling approach, our method reproduces subjectively
similar results with significant performance gains. We reduce the memory
footprint of the raw datasets by two orders of magnitude in general. Lastly, we
depict the working of our method with actual surface renderings.

</details>


### [208] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Main category: cs.GR

TL;DR: VEIGAR is a novel framework for Novel View Synthesis and 3D generation that avoids initial 3D reconstruction, using pixel-space priors and scale-invariant depth loss for efficiency and superior results.


<details>
  <summary>Details</summary>
Motivation: Current methods for cross-view consistency in NVS and 3D generation are computationally expensive and often yield suboptimal reconstruction quality due to reliance on initial 3D reconstruction.

Method: VEIGAR employs a lightweight foundation model for pixel-space alignment and introduces scale-invariant depth loss to eliminate traditional depth regularization steps.

Result: VEIGAR achieves state-of-the-art reconstruction quality and cross-view consistency while reducing training time by threefold compared to existing methods.

Conclusion: VEIGAR offers a more efficient and effective solution for NVS and 3D generation, outperforming prior methods without the computational overhead of initial reconstruction.

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have
significantly improved editing tasks, with a primary emphasis on maintaining
cross-view consistency throughout the generative process. Contemporary methods
typically address this challenge using a dual-strategy framework: performing
consistent 2D inpainting across all views guided by embedded priors either
explicitly in pixel space or implicitly in latent space; and conducting 3D
reconstruction with additional consistency guidance. Previous strategies, in
particular, often require an initial 3D reconstruction phase to establish
geometric structure, introducing considerable computational overhead. Even with
the added cost, the resulting reconstruction quality often remains suboptimal.
In this paper, we present VEIGAR, a computationally efficient framework that
outperforms existing methods without relying on an initial reconstruction
phase. VEIGAR leverages a lightweight foundation model to reliably align priors
explicitly in the pixel space. In addition, we introduce a novel supervision
strategy based on scale-invariant depth loss, which removes the need for
traditional scale-and-shift operations in monocular depth regularization.
Through extensive experimentation, VEIGAR establishes a new state-of-the-art
benchmark in reconstruction quality and cross-view consistency, while achieving
a threefold reduction in training time compared to the fastest existing method,
highlighting its superior balance of efficiency and effectiveness.

</details>


### [209] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: A new curvature proxy for neural SDFs reduces computational costs while maintaining geometric accuracy by focusing on mixed second-order terms.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for enforcing developable behavior in neural SDFs rely on costly full Hessian evaluation and second-order autodiff. This work aims to reduce memory and runtime overhead.

Method: Two proxies are introduced: (i) a finite-difference proxy using forward SDF evaluations and gradients, and (ii) an autodiff proxy using Hessian-vector products. Both avoid full Hessian assembly.

Result: On ABC benchmarks, the proxies match or exceed Hessian-based baselines in reconstruction fidelity while halving GPU memory use and wall-clock time.

Conclusion: The method offers a practical, scalable solution for curvature-aware SDF learning, suitable for engineering-grade shape reconstruction.

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [210] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: A novel PDE-driven corruption process for generative image synthesis, combining advection-diffusion and Gaussian noise, implemented via a GPU-accelerated solver.


<details>
  <summary>Details</summary>
Motivation: To generalize existing PDE-based approaches and improve image diversity/quality while preserving color palette.

Method: Uses a physically motivated PDE (advection-diffusion + noise) with a custom Lattice Boltzmann solver and stochastic velocity fields for turbulence.

Result: Demonstrates improved image diversity and quality, generalizing prior PDE-based techniques.

Conclusion: Bridges fluid dynamics and deep generative modeling, offering a new perspective for diffusion-based synthesis.

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [211] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: DreamCube extends 2D foundation models to 3D panorama generation using multi-plane synchronization, achieving diverse and accurate results.


<details>
  <summary>Details</summary>
Motivation: Overcome the incompatibility between 2D single views and 3D panoramas by leveraging 2D foundation models.

Method: Introduces DreamCube, a multi-plane RGB-D diffusion model, applying multi-plane synchronization to 2D operators.

Result: Effective in panoramic image generation, depth estimation, and 3D scene generation.

Conclusion: DreamCube successfully bridges 2D and 3D domains for high-quality panorama synthesis.

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [212] [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
*Jushaan Singh Kalra,Xinran Zhao,To Eun Kim,Fengyu Cai,Fernando Diaz,Tongshuang Wu*

Main category: cs.IR

TL;DR: The paper introduces a zero-shot, weighted combination of heterogeneous retrievers (mixture of retrievers) to dynamically select and integrate multiple retrievers per query, outperforming individual retrievers and larger models.


<details>
  <summary>Details</summary>
Motivation: Current retrieval-augmented generation (RAG) systems rely on a single retriever, limiting generalization across diverse information needs. The paper explores dynamic selection of multiple retrievers without manual intervention.

Method: Proposes a mixture of retrievers, combining signals from different retrievers (e.g., BM25 for lexical matches, dense retrievers for semantic similarity) in a zero-shot, weighted manner.

Result: The mixture outperforms individual retrievers (+10.8%) and larger models (+3.9%) with just 0.8B parameters. It also improves collaboration with human information sources by 58.9%.

Conclusion: Dynamic integration of multiple retrievers is effective and efficient, enhancing RAG performance without manual selection.

Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.

</details>


### [213] [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
*Fengyu Cai,Tong Chen,Xinran Zhao,Sihao Chen,Hongming Zhang,Sherry Tongshuang Wu,Iryna Gurevych,Heinz Koeppl*

Main category: cs.IR

TL;DR: Revela is a self-supervised framework for training dense retrievers by modeling semantic dependencies among documents, outperforming prior methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Specialized domains lack annotated query-document pairs, motivating self-supervised retriever learning inspired by language modeling.

Method: Revela trains retrievers by conditioning next-token prediction on local and cross-document context, using retriever-computed similarity scores for optimization.

Result: Revela achieves absolute improvements of 5.2% and 5.6% on NDCG@10, outperforming prior methods and scaling with model size.

Conclusion: Revela demonstrates effectiveness and scalability for self-supervised retriever learning, promising for specialized domains.

Abstract: Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [214] [Exoplanet Classification through Vision Transformers with Temporal Image Analysis](https://arxiv.org/abs/2506.16597)
*Anupma Choudhary,Sohith Bandari,B. S. Kushvah,C. Swastik*

Main category: astro-ph.EP

TL;DR: The paper proposes a machine learning method using Vision Transformer (ViT) with transformed light curve data (GAFs and RPs) for efficient exoplanet classification, achieving high recall and precision.


<details>
  <summary>Details</summary>
Motivation: Traditional exoplanet classification methods are resource-intensive, necessitating advanced techniques like machine learning for efficiency.

Method: Light curve data is transformed into GAFs and RPs, then fed into a ViT model, evaluated via 5-fold cross-validation.

Result: RPs outperform GAFs, with ViT achieving 89.46% recall and 85.09% precision.

Conclusion: The study highlights the potential of ViT for exoplanet classification but notes limitations like dataset size, calling for further research.

Abstract: The classification of exoplanets has been a longstanding challenge in
astronomy, requiring significant computational and observational resources.
Traditional methods demand substantial effort, time, and cost, highlighting the
need for advanced machine learning techniques to enhance classification
efficiency. In this study, we propose a methodology that transforms raw light
curve data from NASA's Kepler mission into Gramian Angular Fields (GAFs) and
Recurrence Plots (RPs) using the Gramian Angular Difference Field and
recurrence plot techniques. These transformed images serve as inputs to the
Vision Transformer (ViT) model, leveraging its ability to capture intricate
temporal dependencies. We assess the performance of the model through recall,
precision, and F1 score metrics, using a 5-fold cross-validation approach to
obtain a robust estimate of the model's performance and reduce evaluation bias.
Our comparative analysis reveals that RPs outperform GAFs, with the ViT model
achieving an 89.46$\%$ recall and an 85.09$\%$ precision rate, demonstrating
its significant capability in accurately identifying exoplanetary transits.
Despite using under-sampling techniques to address class imbalance, dataset
size reduction remains a limitation. This study underscores the importance of
further research into optimizing model architectures to enhance automation,
performance, and generalization of the model.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [215] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: The paper introduces a structure-aware chunking method for Retrieval-Augmented Generation (RAG) pipelines using Abstract Syntax Trees (ASTs) to improve code generation quality by preserving semantic coherence.


<details>
  <summary>Details</summary>
Motivation: Existing line-based chunking heuristics in RAG pipelines often disrupt semantic structures, degrading code generation quality. The paper addresses this gap by proposing a more effective chunking method.

Method: The proposed method, chunking via ASTs, recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits, ensuring self-contained and semantically coherent units.

Result: The method improves performance on code generation tasks, increasing Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.

Conclusion: The study underscores the importance of structure-aware chunking for enhancing retrieval-augmented code intelligence, demonstrating significant performance gains.

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


### [216] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: The paper analyzes submissions to SWE-Bench Lite and Verified leaderboards, revealing trends like the dominance of proprietary LLMs and diverse contributor involvement.


<details>
  <summary>Details</summary>
Motivation: To clarify the unclear architectural designs and origins of solutions in SWE-Bench submissions due to lack of detailed documentation.

Method: Comprehensive study of 68 SWE-Bench Lite and 79 Verified submissions, analyzing 67 unique approaches across dimensions like submitter type, LLM usage, and system architecture.

Result: Dominance of proprietary LLMs (e.g., Claude 3.5/3.7), mix of agentic and non-agentic designs, and diverse contributor base from individuals to large tech companies.

Conclusion: The study provides insights into the current state of APR solutions, highlighting trends and gaps in documentation and design transparency.

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [217] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
*Sophie Chiang,Guy Laban,Hatice Gunes*

Main category: cs.HC

TL;DR: The study compares emotionally supportive conversations between robots and humans with human-to-human therapy sessions, finding strong thematic and semantic alignment.


<details>
  <summary>Details</summary>
Motivation: To understand if robot-led support conversations resemble traditional therapy and if robot responses mirror human therapist responses.

Method: Analyzed datasets of human therapy sessions and robot interactions using sentence embeddings, K-means clustering, and distance-based methods.

Result: 90.88% of robot disclosures mapped to human therapy clusters, showing strong thematic and semantic overlap in responses.

Conclusion: Robot-led support conversations share parallels with human therapy, suggesting potential for augmenting mental health interventions.

Abstract: As conversational agents increasingly engage in emotionally supportive
dialogue, it is important to understand how closely their interactions resemble
those in traditional therapy settings. This study investigates whether the
concerns shared with a robot align with those shared in human-to-human (H2H)
therapy sessions, and whether robot responses semantically mirror those of
human therapists. We analyzed two datasets: one of interactions between users
and professional therapists (Hugging Face's NLP Mental Health Conversations),
and another involving supportive conversations with a social robot (QTrobot
from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence
embeddings and K-means clustering, we assessed cross-agent thematic alignment
by applying a distance-based cluster-fitting method that evaluates whether
responses from one agent type map to clusters derived from the other, and
validated it using Euclidean distances. Results showed that 90.88% of robot
conversation disclosures could be mapped to clusters from the human therapy
dataset, suggesting shared topical structure. For matched clusters, we compared
the subjects as well as therapist and robot responses using Transformer,
Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'
disclosures in both datasets, as well as in the responses given to similar
human disclosure themes across agent types (robot vs. human therapist). These
findings highlight both the parallels and boundaries of robot-led support
conversations and their potential for augmenting mental health interventions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [218] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Main category: cs.LG

TL;DR: BASE-Q improves rotational quantization for LLMs by addressing alignment and distribution issues, reducing errors, and enabling blockwise optimization.


<details>
  <summary>Details</summary>
Motivation: Current rotational quantization methods have limitations in channel alignment and Gaussian-like distributions, leading to higher errors and memory overhead.

Method: Introduces BASE-Q, combining bias correction and asymmetric scaling to reduce rounding and clipping errors, and supports blockwise optimization.

Result: BASE-Q reduces accuracy gaps to full-precision models by 50.5%, 42.9%, and 29.2% compared to QuaRot, SpinQuant, and OSTQuant.

Conclusion: BASE-Q is a practical and effective solution for rotational quantization in LLMs, addressing key limitations and improving performance.

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [219] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Main category: cs.LG

TL;DR: LFPS is a method to accelerate sparse indexing in LLMs by leveraging historical attention patterns, achieving significant speedups while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The memory demand for KV caches in LLMs grows rapidly with longer contexts, creating bottlenecks in GPU memory and PCIe bandwidth. Existing sparse attention methods inefficiently retrieve indices by ignoring temporal correlations.

Method: LFPS dynamically constructs sparse indexing candidates using historical attention patterns (vertical and slash patterns) and a positional expansion strategy to predict Top-k indices.

Result: LFPS achieves up to 22.8√ó speedup over full attention and 9.6√ó speedup over exact Top-k retrieval on benchmarks like LongBench-RULER, with preserved accuracy.

Conclusion: LFPS provides an efficient and practical solution for optimizing decoding in long-context LLM inference.

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [220] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: A learnable two-sided short-time Laplace transform (STLT) replaces self-attention in transformers, offering dynamic adaptation of token relevance and frequency responses with scalable complexity.


<details>
  <summary>Details</summary>
Motivation: To address the computational bottleneck of self-attention in transformers while maintaining or improving performance for ultra-long-sequence tasks.

Method: Introduces trainable Laplace nodes for decay rates, frequencies, and window bandwidth, using recursive convolution and FFT-based relevance matrix computation. Adaptive node allocation dynamically adjusts active nodes.

Result: Achieves competitive perplexities and scores on language modeling, translation, and QA tasks, scaling to 100k+ tokens. Ablations confirm the importance of learnable parameters and adaptive nodes.

Conclusion: The STLT combines interpretability, scalability, and robustness, enabling efficient ultra-long-sequence modeling without self-attention limitations.

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [221] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Main category: cs.LG

TL;DR: The paper introduces daDPO, a method combining preference optimization and distribution-based distillation to enhance smaller LLMs, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Smaller LLMs lose conversational ability as size decreases, limiting deployment in resource-constrained settings. Current methods ignore teacher model output distributions.

Method: Proposes daDPO (Distribution-Aware DPO), a unified approach for preference optimization and distribution-based distillation.

Result: daDPO improves performance, enabling pruned models to nearly match teacher performance and smaller models to occasionally surpass larger ones.

Conclusion: daDPO effectively bridges the gap in smaller LLM performance, offering a practical solution for resource-limited environments.

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [222] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: MadaKV is a modality-adaptive KV cache eviction strategy for MLLMs, improving efficiency in long-context inference by addressing modality disparities.


<details>
  <summary>Details</summary>
Motivation: Traditional KV cache eviction methods are unimodal and fail to handle modality-specific information, leading to suboptimal performance in multimodal scenarios.

Method: MadaKV uses modality preference adaptation and hierarchical compression compensation to dynamically retain critical tokens.

Result: Achieves 1.3-1.5x improvement in KV cache memory and decoding latency while maintaining accuracy.

Conclusion: MadaKV outperforms existing methods, as shown by experiments on MLLMs and the MileBench benchmark.

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [223] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Main category: cs.LG

TL;DR: Fractional Reasoning is a framework for dynamically adjusting reasoning intensity in LLMs at inference time, improving performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods apply uniform reasoning across inputs, ignoring varying problem complexities. Fractional Reasoning addresses this by allowing tailored reasoning depth.

Method: Extracts a latent steering vector for deeper reasoning and scales it dynamically, supporting breadth- and depth-based strategies.

Result: Improves performance on GSM8K, MATH500, and GPQA benchmarks.

Conclusion: Fractional Reasoning offers a flexible, training-free way to enhance LLM reasoning, outperforming fixed methods.

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [224] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Main category: cs.LG

TL;DR: Accelerating neural speech transcription via time-domain signal sparsification in transformer encoders, achieving up to 1.6x speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Speech signals are highly compressible, and transformer-based models like Whisper can benefit from early sparsification to improve efficiency.

Method: Systematic architecture search over sparsification stage (encoder layer) and compression ratio (sparsity) in Whisper models.

Result: Best solutions sparsify hidden states to 40-60% sparsity early, achieving 1.6x runtime acceleration with <1% accuracy loss.

Conclusion: Early sparsification in transformer encoders efficiently accelerates speech transcription without fine-tuning.

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [225] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: The paper identifies vulnerabilities in safety-aligned AI models due to shallow alignment methods, proposes a probing method to detect latent shifts, introduces an attack (ASA), and suggests a robust training strategy (LAPT).


<details>
  <summary>Details</summary>
Motivation: Current safety alignment methods are shallow, focusing on surface-level behaviors, making models vulnerable to latent shifts that trigger unsafe responses.

Method: The authors introduce a probing method to measure latent sensitivity, develop the Activation Steering Attack (ASA), and propose Layer-wise Adversarial Patch Training (LAPT) for robust alignment.

Result: LAPT improves alignment robustness without degrading general model capabilities, while ASA exposes vulnerabilities in current methods.

Conclusion: The work highlights flaws in surface-level alignment and advocates for representation-level training strategies to enhance robustness.

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [226] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Main category: cs.LG

TL;DR: Transformers in LLMs can identify and use latent concepts in ICL tasks, showing localized structures for concept disentanglement.


<details>
  <summary>Details</summary>
Motivation: To understand if transformers represent latent structures or take shortcuts in ICL tasks, especially in multi-step reasoning.

Method: Examined transformers in 2-hop reasoning tasks with discrete latent concepts and continuous latent parameterizations.

Result: Models identify latent concepts and perform step-by-step composition; continuous concepts show low-dimensional subspaces mimicking underlying parameterization.

Conclusion: Transformers exhibit localized structures for disentangling latent concepts in ICL, refining understanding of their representations.

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [227] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Main category: cs.LG

TL;DR: The paper introduces SAMD and SAMI for interpreting and controlling transformer models by mapping complex concepts to attention heads and adjusting their effects with a scalar parameter.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and control of transformers by addressing gaps in current attribution research, particularly for complex concepts and attention mechanisms.

Method: SAMD maps concepts to attention heads via cosine similarity, while SAMI adjusts module effects using a scalar.

Result: Demonstrated stability of module locations, improved performance on GSM8K (+1.6%), and facilitated jailbreaking on HarmBench (+72.7%).

Conclusion: The approach is domain-agnostic, effective for complex concepts, and enhances both performance and behavioral control.

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


### [228] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2506.15698)
*Yunhak Oh,Junseok Lee,Yeongmin Kim,Sangwoo Seo,Namkyeong Lee,Chanyoung Park*

Main category: cs.LG

TL;DR: Spotscape is a novel framework for Spatially Resolved Transcriptomics (SRT) that improves spot representations by capturing global relationships and regulating distances between spots, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based methods for SRT fail to provide meaningful spot representations, especially near boundaries, due to overemphasis on adjacent spots with minimal feature differences.

Method: Spotscape introduces a Similarity Telescope module for global spot relationships and a similarity scaling strategy for multi-slice integration.

Result: Spotscape outperforms existing methods in single-slice and multi-slice downstream tasks.

Conclusion: Spotscape effectively addresses limitations of current SRT methods, offering improved performance and multi-slice integration.

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [229] [Shadow defense against gradient inversion attack in federated learning](https://arxiv.org/abs/2506.15711)
*Le Jiang,Liyan Ma,Guang Yang*

Main category: cs.LG

TL;DR: A framework for targeted noise injection in federated learning to defend against gradient inversion attacks, improving privacy without significantly degrading model performance.


<details>
  <summary>Details</summary>
Motivation: Address privacy leakage in federated learning, especially in healthcare, where gradient inversion attacks can reconstruct sensitive data from model updates.

Method: Uses a shadow model with interpretability to identify sensitive areas for targeted noise injection, minimizing impact on model accuracy.

Result: Achieves PSNR discrepancies of 3.73 (ChestXRay) and 2.78 (EyePACS), with less than 1% F1 reduction, and stable defense improvements over 1.5% in LPIPS and SSIM.

Conclusion: The framework effectively defends against gradient inversion attacks while preserving model performance, validated across diverse medical datasets.

Abstract: Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.

</details>


### [230] [Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.15720)
*Juntae Lee,Munawar Hayat,Sungrack Yun*

Main category: cs.LG

TL;DR: A novel FSCIL method, Tri-WE, addresses catastrophic forgetting and overfitting by interpolating base, previous, and current models in weight-space, and uses amplified data distillation for better generalization.


<details>
  <summary>Details</summary>
Motivation: Fixed feature extractors in FSCIL limit adaptability to new classes, leading to forgetting and overfitting.

Method: Proposes Tri-WE for weight-space interpolation and amplified data knowledge distillation.

Result: Achieves state-of-the-art performance on miniImageNet, CUB200, and CIFAR100.

Conclusion: Tri-WE effectively updates models with few examples while mitigating forgetting and overfitting.

Abstract: Few-shot class incremental learning (FSCIL) enables the continual learning of
new concepts with only a few training examples. In FSCIL, the model undergoes
substantial updates, making it prone to forgetting previous concepts and
overfitting to the limited new examples. Most recent trend is typically to
disentangle the learning of the representation from the classification head of
the model. A well-generalized feature extractor on the base classes (many
examples and many classes) is learned, and then fixed during incremental
learning. Arguing that the fixed feature extractor restricts the model's
adaptability to new classes, we introduce a novel FSCIL method to effectively
address catastrophic forgetting and overfitting issues. Our method enables to
seamlessly update the entire model with a few examples. We mainly propose a
tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,
immediately previous, and current models in weight-space, especially for the
classification heads of the models. Then, it collaboratively maintains
knowledge from the base and previous models. In addition, we recognize the
challenges of distilling generalized representations from the previous model
from scarce data. Hence, we suggest a regularization loss term using amplified
data knowledge distillation. Simply intermixing the few-shot data, we can
produce richer data enabling the distillation of critical knowledge from the
previous model. Consequently, we attain state-of-the-art results on the
miniImageNet, CUB200, and CIFAR100 datasets.

</details>


### [231] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanoviƒá,Ismail Labiad,Tom√°≈° Souƒçek,Martin Vechev,Pierre Fernandez*

Main category: cs.LG

TL;DR: First token-level watermarking for autoregressive image generation models, addressing reverse cycle-consistency and robustness to attacks.


<details>
  <summary>Details</summary>
Motivation: Track provenance of generative model outputs, especially for autoregressive image models, which lack prior watermarking solutions.

Method: Adapt language model watermarking, improve reverse cycle-consistency via tokenizer-detokenizer finetuning, and add a watermark synchronization layer.

Result: Reliable and robust watermark detection with theoretically grounded p-values.

Conclusion: The approach effectively watermarks autoregressive image model outputs, addressing key challenges and ensuring robustness.

Abstract: Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values.

</details>


### [232] [Subspace-Boosted Model Merging](https://arxiv.org/abs/2506.16506)
*Ronald Skorobogat,Karsten Roth,Mariana-Iuliana Georgescu,Zeynep Akata*

Main category: cs.LG

TL;DR: Model merging combines expert models but faces diminishing returns due to rank collapse in task vector space. Subspace Boosting mitigates this, improving merging efficacy by over 10% for up to 20 models.


<details>
  <summary>Details</summary>
Motivation: To address the diminishing returns and performance degradation in merging multiple expert models, caused by rank collapse in task vector space.

Method: Introduces Subspace Boosting, which maintains task vector ranks using singular value decomposition, and employs Higher-Order Generalized Singular Value Decomposition to quantify task similarity.

Result: Subspace Boosting improves merging efficacy by over 10% for up to 20 expert models on vision benchmarks.

Conclusion: Subspace Boosting effectively mitigates rank collapse, enhancing model merging performance and providing interpretable insights into task similarity.

Abstract: Model merging enables the combination of multiple specialized expert models
into a single model capable of performing multiple tasks. However, the benefits
of merging an increasing amount of specialized experts generally lead to
diminishing returns and reduced overall performance gains. In this work, we
offer an explanation and analysis from a task arithmetic perspective; revealing
that as the merging process (across numerous existing merging methods)
continues for more and more experts, the associated task vector space
experiences rank collapse. To mitigate this issue, we introduce Subspace
Boosting, which operates on the singular value decomposed task vector space and
maintains task vector ranks. Subspace Boosting raises merging efficacy for up
to 20 expert models by large margins of more than 10% when evaluated on vision
benchmarks. Moreover, we propose employing Higher-Order Generalized Singular
Value Decomposition to further quantify task similarity, offering a new
interpretable perspective on model merging.

</details>


### [233] [From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images](https://arxiv.org/abs/2506.16890)
*Sebastian H√∂nel,Jonas Nordqvist*

Main category: cs.LG

TL;DR: The paper addresses challenges in unsupervised machine learning for defect detection in industrial settings, focusing on robustness and practical applicability.


<details>
  <summary>Details</summary>
Motivation: Manual inspection is costly and error-prone; unsupervised ML offers a solution but struggles with real-world data quality and robustness.

Method: Evaluates two state-of-the-art models for anomaly detection on low-quality RGB images of metal parts, focusing on identifying and improving data issues without new data.

Result: Highlights pitfalls of likelihood-based methods and proposes a framework for better empirical risk estimation in real-world scenarios.

Conclusion: Provides practical guardrails for practitioners to address robustness and invariance issues in similar industrial applications.

Abstract: The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [234] [Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse](https://arxiv.org/abs/2506.16412)
*Paulina DeVito,Akhil Vallala,Sean Mcmahon,Yaroslav Hinda,Benjamin Thaw,Hanqi Zhuang,Hari Kalva*

Main category: cs.SI

TL;DR: The study analyzes stakeholder perceptions of Generative AI (GAI) in education using social media data, proposing a modular LLM-based framework that outperforms classical NLP models. Findings reveal contrasting student and teacher concerns, emphasizing the need for clearer policies and support mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand how students and educators perceive GAI tools in education, leveraging social media discourse for comprehensive insights.

Method: Analyzed 1,199 Reddit posts and 13,959 comments using sentiment analysis, topic modeling, and author classification. Proposed a modular LLM-based framework (GPT-4o) and compared it to classical NLP models.

Result: GPT-4o achieved 90.6% accuracy in sentiment analysis. Identified 12 latent topics with varying sentiment and author distributions. Students and teachers expressed optimism but differed in concerns (e.g., cheating accusations vs. job security).

Conclusion: The study highlights tensions in GAI adoption, advocating for clearer policies and transparent practices. It demonstrates the potential of LLM-based frameworks for analyzing online discourse.

Abstract: Generative AI (GAI) technologies are quickly reshaping the educational
landscape. As adoption accelerates, understanding how students and educators
perceive these tools is essential. This study presents one of the most
comprehensive analyses to date of stakeholder discourse dynamics on GAI in
education using social media data. Our dataset includes 1,199 Reddit posts and
13,959 corresponding top-level comments. We apply sentiment analysis, topic
modeling, and author classification. To support this, we propose and validate a
modular framework that leverages prompt-based large language models (LLMs) for
analysis of online social discourse, and we evaluate this framework against
classical natural language processing (NLP) models. Our GPT-4o pipeline
consistently outperforms prior approaches across all tasks. For example, it
achieved 90.6% accuracy in sentiment analysis against gold-standard human
annotations. Topic extraction uncovered 12 latent topics in the public
discourse with varying sentiment and author distributions. Teachers and
students convey optimism about GAI's potential for personalized learning and
productivity in higher education. However, key differences emerged: students
often voice distress over false accusations of cheating by AI detectors, while
teachers generally express concern about job security, academic integrity, and
institutional pressures to adopt GAI tools. These contrasting perspectives
highlight the tension between innovation and oversight in GAI-enabled learning
environments. Our findings suggest a need for clearer institutional policies,
more transparent GAI integration practices, and support mechanisms for both
educators and students. More broadly, this study demonstrates the potential of
LLM-based frameworks for modeling stakeholder discourse within online
communities.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [235] [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: The paper discusses the challenges of applying human psychological measurement tools to LLMs, proposing a dual-validity framework to ensure reliable measurement and causal inference in AI psychology.


<details>
  <summary>Details</summary>
Motivation: Address concerns about contradictory results and measurement phantoms when using human tools on LLMs, aiming to build a robust science of AI psychology.

Method: Introduces a dual-validity framework integrating reliable measurement principles and causal inference standards, scaling evidence requirements based on claim ambition.

Result: Current practices often fail to meet rigorous validation needs, treating pattern matching as psychological evidence without proper differentiation.

Conclusion: Advocates for computational analogues of psychological constructs and scalable evidence standards, moving beyond uncritical use of human tools.

Abstract: Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

</details>


### [236] [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: The paper provides a framework for using LLMs in psychological research, focusing on simulating roles/personas and cognitive modeling, while addressing challenges like prompt sensitivity and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methodological guidance for using LLMs in psychological and behavioral research, offering a structured approach to leverage their capabilities.

Method: Presents methods for developing psychologically grounded personas and synthesizes approaches for cognitive modeling, including validation against human data and causal interventions.

Result: A framework that integrates empirical evidence about LLM performance, addressing biases and limitations, to aid researchers in psychological studies.

Conclusion: The framework helps researchers navigate challenges and utilize LLMs effectively in psychological research, emphasizing transparency about model constraints.

Abstract: Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

</details>


### [237] [TrajSceneLLM: A Multimodal Perspective on Semantic GPS Trajectory Analysis](https://arxiv.org/abs/2506.16401)
*Chunhou Ji,Qiumeng Li*

Main category: cs.CY

TL;DR: TrajSceneLLM enhances GPS trajectory analysis by combining map images and LLM-generated text for rich semantic embeddings, improving performance in tasks like Travel Mode Identification.


<details>
  <summary>Details</summary>
Motivation: Traditional methods lack deep semantic understanding and contextual map integration in GPS trajectory analysis.

Method: TrajSceneLLM integrates map images and LLM-generated text, creating multimodal embeddings for semantic enhancement, paired with an MLP classifier.

Result: The framework achieves significant performance improvements in Travel Mode Identification, capturing spatio-temporal dependencies.

Conclusion: TrajSceneLLM shows promise for geospatial AI applications, reducing reliance on handcrafted features and enabling diverse downstream uses.

Abstract: GPS trajectory data reveals valuable patterns of human mobility and urban
dynamics, supporting a variety of spatial applications. However, traditional
methods often struggle to extract deep semantic representations and incorporate
contextual map information. We propose TrajSceneLLM, a multimodal perspective
for enhancing semantic understanding of GPS trajectories. The framework
integrates visualized map images (encoding spatial context) and textual
descriptions generated through LLM reasoning (capturing temporal sequences and
movement dynamics). Separate embeddings are generated for each modality and
then concatenated to produce trajectory scene embeddings with rich semantic
content which are further paired with a simple MLP classifier. We validate the
proposed framework on Travel Mode Identification (TMI), a critical task for
analyzing travel choices and understanding mobility behavior. Our experiments
show that these embeddings achieve significant performance improvement,
highlighting the advantage of our LLM-driven method in capturing deep
spatio-temporal dependencies and reducing reliance on handcrafted features.
This semantic enhancement promises significant potential for diverse downstream
applications and future research in geospatial artificial intelligence. The
source code and dataset are publicly available at:
https://github.com/februarysea/TrajSceneLLM.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [238] [DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation](https://arxiv.org/abs/2506.16495)
*Changsheng Gao,Zijie Liu,Li Li,Dong Liu,Xiaoyan Sun,Weisi Lin*

Main category: cs.MM

TL;DR: The paper introduces a universal feature coding method for large models, addressing distributional heterogeneity in features by proposing a learned peaky-to-balanced transformation, improving compression and cross-model generalization.


<details>
  <summary>Details</summary>
Motivation: Prior studies focused on task- or model-specific feature coding, leaving universal feature coding for diverse large models unexplored. The challenge lies in the incompatible distributions of features from different models.

Method: Proposes a learned peaky-to-balanced distribution transformation to reshape skewed feature distributions into a balanced target space, enabling alignment without modifying downstream codecs.

Result: Validated on LLaMA3, DINOv2, and SD3, the method shows notable improvements in compression efficiency and cross-model generalization over task-specific baselines.

Conclusion: The approach effectively addresses distributional heterogeneity, enabling universal feature coding for diverse large models, with source code released for future research.

Abstract: Like image coding in visual data transmission, feature coding is essential
for the distributed deployment of large models by significantly reducing
transmission and storage overhead. However, prior studies have mostly targeted
task- or model-specific scenarios, leaving the challenge of universal feature
coding across diverse large models largely unaddressed. In this paper, we
present the first systematic study on universal feature coding for large
models. The key challenge lies in the inherently diverse and distributionally
incompatible nature of features extracted from different models. For example,
features from DINOv2 exhibit highly peaky, concentrated distributions, while
those from Stable Diffusion 3 (SD3) are more dispersed and uniform. This
distributional heterogeneity severely hampers both compression efficiency and
cross-model generalization. To address this, we propose a learned
peaky-to-balanced distribution transformation, which reshapes highly skewed
feature distributions into a common, balanced target space. This transformation
is non-uniform, data-driven, and plug-and-play, enabling effective alignment of
heterogeneous distributions without modifying downstream codecs. With this
alignment, a universal codec trained on the balanced target distribution can
effectively generalize to features from different models and tasks. We validate
our approach on three representative large models-LLaMA3, DINOv2, and
SD3-across multiple tasks and modalities. Extensive experiments show that our
method achieves notable improvements in both compression efficiency and
cross-model generalization over task-specific baselines. All source code will
be released for future research.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [239] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: DeepRTL2 is a versatile LLM addressing both generation- and embedding-based tasks in EDA, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Previous work focused on generation tasks in EDA, neglecting embedding-based tasks critical for hardware design.

Method: DeepRTL2, a family of LLMs, unifies generation and embedding tasks for RTL code.

Result: State-of-the-art performance across all evaluated tasks.

Conclusion: DeepRTL2 provides a comprehensive solution for diverse EDA challenges.

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [240] [Bias Variation Compensation in Perimeter-Gated SPAD TRNGs](https://arxiv.org/abs/2506.15888)
*Md Sakibur Sajal,Hunter Guthrie,Marc Dandin*

Main category: physics.ins-det

TL;DR: A 64x64 array of pgSPADs in 0.35Œºm CMOS is used for entropy, achieving <1% BV with Von Neumann debiasing, passing NIST tests.


<details>
  <summary>Details</summary>
Motivation: Address bias variation (BV) in entropy sources by optimizing hardware-friendly debiasing for wide BV tolerance.

Method: Utilize pgSPAD array with gate voltage tuning for BV compensation; apply Von Neumann debiasing.

Result: Achieved <1% BV at 2 kHz/pixel; debiased bits passed all 16 NIST tests.

Conclusion: The pgSPAD array with BV compensation and Von Neumann debiasing is effective for high-quality random bit generation.

Abstract: Random number generators that utilize arrays of entropy source elements
suffer from bias variation (BV). Despite the availability of efficient
debiasing algorithms, optimized implementations of hardware friendly options
depend on the bit bias in the raw bit streams and cannot accommodate a wide BV.
In this work, we present a 64 x 64 array of perimeter gated single photon
avalanche diodes (pgSPADs), fabricated in a 0.35 {\mu}m standard CMOS
technology, as a source of entropy to generate random binary strings with a BV
compensation technique. By applying proper gate voltages based on the devices'
native dark count rates, we demonstrate less than 1% BV for a raw-bit
generation rate of 2 kHz/pixel at room temperature. The raw bits were debiased
using the classical iterative Von Neumann's algorithm and the debiased bits
were found to pass all of the 16 tests from NIST's Statistical Test Suite.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [241] [InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding](https://arxiv.org/abs/2506.15745)
*Minsoo Kim,Kyuhong Shim,Jungwook Choi,Simyung Chang*

Main category: eess.IV

TL;DR: InfiniPot-V is a training-free, query-agnostic framework for MLLMs that enforces a fixed memory cap for streaming video understanding, reducing GPU memory by up to 94% without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the linear growth of KV cache in MLLMs, which exceeds memory limits of edge devices, by proposing a memory-efficient solution.

Method: Uses Temporal-axis Redundancy (TaR) to remove redundant tokens and Value-Norm (VaN) ranking to retain significant tokens during video encoding.

Result: Achieves up to 94% reduction in peak GPU memory, sustains real-time generation, and matches full-cache accuracy.

Conclusion: InfiniPot-V enables efficient on-device streaming video assistants by eliminating the KV cache bottleneck without retraining.

Abstract: Modern multimodal large language models (MLLMs) can reason over hour-long
video, yet their key-value (KV) cache grows linearly with time--quickly
exceeding the fixed memory of phones, AR glasses, and edge robots. Prior
compression schemes either assume the whole video and user query are available
offline or must first build the full cache, so memory still scales with stream
length. InfiniPot-V is the first training-free, query-agnostic framework that
enforces a hard, length-independent memory cap for streaming video
understanding. During video encoding it monitors the cache and, once a user-set
threshold is reached, runs a lightweight compression pass that (i) removes
temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)
keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four
open-source MLLMs and four long-video and two streaming-video benchmarks,
InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,
and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By
dissolving the KV cache bottleneck without retraining or query knowledge,
InfiniPot-V closes the gap for on-device streaming video assistants.

</details>


### [242] [Pixel-wise Modulated Dice Loss for Medical Image Segmentation](https://arxiv.org/abs/2506.15744)
*Seyed Mohsen Hosseini*

Main category: eess.IV

TL;DR: The paper proposes a modified Dice loss (PM Dice loss) to address both class and difficulty imbalances in medical segmentation tasks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Class and difficulty imbalances in medical segmentation tasks hinder neural network performance, with existing solutions being costly or ineffective.

Method: Introduces a pixel-wise modulating term to the Dice loss, leveraging its class imbalance handling to also address difficulty imbalance with minimal computational cost.

Result: PM Dice loss outperforms other methods on three medical segmentation tasks.

Conclusion: The proposed PM Dice loss effectively tackles both class and difficulty imbalances, offering a simple and efficient solution.

Abstract: Class imbalance and the difficulty imbalance are the two types of data
imbalance that affect the performance of neural networks in medical
segmentation tasks. In class imbalance the loss is dominated by the majority
classes and in difficulty imbalance the loss is dominated by easy to classify
pixels. This leads to an ineffective training. Dice loss, which is based on a
geometrical metric, is very effective in addressing the class imbalance
compared to the cross entropy (CE) loss, which is adopted directly from
classification tasks. To address the difficulty imbalance, the common approach
is employing a re-weighted CE loss or a modified Dice loss to focus the
training on difficult to classify areas. The existing modification methods are
computationally costly and with limited success. In this study we propose a
simple modification to the Dice loss with minimal computational cost. With a
pixel level modulating term, we take advantage of the effectiveness of Dice
loss in handling the class imbalance to also handle the difficulty imbalance.
Results on three commonly used medical segmentation tasks show that the
proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other
methods, which are designed to tackle the difficulty imbalance problem.

</details>


### [243] [Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2506.15748)
*Zhe Wang,Yuhua Ru,Aladine Chetouani,Tina Shiang,Fang Chen,Fabian Bauer,Liping Zhang,Didier Hans,Rachid Jennane,William Ewing Palmer,Mohamed Jarraya,Yung Hsin Chen*

Main category: eess.IV

TL;DR: The paper introduces Diffusion-based Counterfactual Augmentation (DCA) to improve automated grading of Knee Osteoarthritis (KOA) by enhancing model robustness and interpretability through targeted counterfactual examples.


<details>
  <summary>Details</summary>
Motivation: Addressing inter-observer variability and deep learning model limitations in KOA grading, especially near critical decision boundaries.

Method: Uses a diffusion model with a Stochastic Differential Equation (SDE) to generate counterfactuals, combined with a self-corrective learning strategy.

Result: Significantly improves classification accuracy on OAI and MOST datasets and aligns latent space topology with clinical KOA progression.

Conclusion: DCA converts model uncertainty into a robust training signal, advancing accurate and trustworthy automated diagnostics.

Abstract: Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged
by significant inter-observer variability and the limited robustness of deep
learning models, particularly near critical decision boundaries. To address
these limitations, this paper proposes a novel framework, Diffusion-based
Counterfactual Augmentation (DCA), which enhances model robustness and
interpretability by generating targeted counterfactual examples. The method
navigates the latent space of a diffusion model using a Stochastic Differential
Equation (SDE), governed by balancing a classifier-informed boundary drive with
a manifold constraint. The resulting counterfactuals are then used within a
self-corrective learning strategy to improve the classifier by focusing on its
specific areas of uncertainty. Extensive experiments on the public
Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST)
datasets demonstrate that this approach significantly improves classification
accuracy across multiple model architectures. Furthermore, the method provides
interpretability by visualizing minimal pathological changes and revealing that
the learned latent space topology aligns with clinical knowledge of KOA
progression. The DCA framework effectively converts model uncertainty into a
robust training signal, offering a promising pathway to developing more
accurate and trustworthy automated diagnostic systems. Our code is available at
https://github.com/ZWang78/DCA.

</details>


### [244] [MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction](https://arxiv.org/abs/2506.15835)
*Mingyuan Luo,Xin Yang,Zhongnuo Yan,Yan Cao,Yuanji Zhang,Xindi Hu,Jin Wang,Haoxuan Ding,Wei Han,Litao Sun,Dong Ni*

Main category: eess.IV

TL;DR: MoNetV2 improves 3D ultrasound reconstruction by fusing image and motion data, using multi-level consistency constraints and self-supervised strategies to reduce errors.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning-based 3D ultrasound reconstruction struggles with cumulative drift and accuracy, especially in complex motion scenarios.

Method: Proposes MoNetV2 with sensor-based fusion, online multi-level consistency constraints, and self-supervised strategies to enhance reconstruction.

Result: MoNetV2 outperforms existing methods in accuracy and generalizability across three datasets.

Conclusion: MoNetV2 effectively addresses challenges in 3D ultrasound reconstruction, improving performance in diverse scanning conditions.

Abstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the
spatial relationships of anatomical structures, playing a crucial role in
clinical diagnosis. Recently, deep-learning-based freehand 3D US has made
significant advancements. It reconstructs volumes by estimating transformations
between images without external tracking. However, image-only reconstruction
poses difficulties in reducing cumulative drift and further improving
reconstruction accuracy, particularly in scenarios involving complex motion
trajectories. In this context, we propose an enhanced motion network (MoNetV2)
to enhance the accuracy and generalizability of reconstruction under diverse
scanning velocities and tactics. First, we propose a sensor-based temporal and
multi-branch structure that fuses image and motion information from a velocity
perspective to improve image-only reconstruction accuracy. Second, we devise an
online multi-level consistency constraint that exploits the inherent
consistency of scans to handle various scanning velocities and tactics. This
constraint exploits both scan-level velocity consistency, path-level appearance
consistency, and patch-level motion consistency to supervise inter-frame
transformation estimation. Third, we distill an online multi-modal
self-supervised strategy that leverages the correlation between network
estimation and motion information to further reduce cumulative errors.
Extensive experiments clearly demonstrate that MoNetV2 surpasses existing
methods in both reconstruction quality and generalizability performance across
three large datasets.

</details>


### [245] [Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images](https://arxiv.org/abs/2506.15853)
*Amit Das,Naofumi Tomita,Kyle J. Syme,Weijie Ma,Paige O'Connor,Kristin N. Corbett,Bing Ren,Xiaoying Liu,Saeed Hassanpour*

Main category: eess.IV

TL;DR: HistoStainAlign predicts IHC staining from H&E images using deep learning, improving efficiency by reducing reliance on costly IHC staining.


<details>
  <summary>Details</summary>
Motivation: IHC staining is expensive and time-consuming; this study aims to provide a computational alternative using H&E images.

Method: A deep learning framework (HistoStainAlign) learns joint representations of H&E and IHC features via contrastive training, tested on gastrointestinal and lung tissues.

Result: Achieved weighted F1 scores of 0.735, 0.830, and 0.723 for P53, PD-L1, and Ki-67 stains, respectively, outperforming baseline models.

Conclusion: HistoStainAlign shows promise as a pre-screening tool to prioritize cases for IHC, enhancing workflow efficiency.

Abstract: Hematoxylin and Eosin (H&E) staining is a cornerstone of pathological
analysis, offering reliable visualization of cellular morphology and tissue
architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry
(IHC) staining provides molecular insights by detecting specific proteins
within tissues, enhancing diagnostic accuracy, and improving treatment
planning. However, IHC staining is costly, time-consuming, and
resource-intensive, requiring specialized expertise. To address these
limitations, this study proposes HistoStainAlign, a novel deep learning
framework that predicts IHC staining patterns directly from H&E whole-slide
images (WSIs) by learning joint representations of morphological and molecular
features. The framework integrates paired H&E and IHC embeddings through a
contrastive training strategy, capturing complementary features across staining
modalities without patch-level annotations or tissue registration. The model
was evaluated on gastrointestinal and lung tissue WSIs with three commonly used
IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores
of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI:
0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC
stains. Embedding analyses demonstrated the robustness of the contrastive
alignment in capturing meaningful cross-stain relationships. Comparisons with a
baseline model further highlight the advantage of incorporating contrastive
learning for improved stain pattern prediction. This study demonstrates the
potential of computational approaches to serve as a pre-screening tool, helping
prioritize cases for IHC staining and improving workflow efficiency.

</details>


### [246] [Fast Training-free Perceptual Image Compression](https://arxiv.org/abs/2506.16102)
*Ziran Zhu,Tongda Xu,Minye Huang,Dailan He,Xingtong Ge,Xinjie Zhang,Ling Li,Yan Wang*

Main category: eess.IV

TL;DR: A training-free algorithm improves perceptual quality of image codecs with fast decoding, outperforming previous methods in FID and time efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing training-free perceptual image codecs are slow due to reliance on diffusion inversion or sample communication. This paper aims to improve decoding speed while maintaining perceptual quality.

Method: Proposes a training-free algorithm with implementations for different decoding time budgets (‚âà0.1s, 0.1-10s, ‚â•10s). It enhances existing codecs like ELIC, VTM, and MS-ILLM.

Result: Reduces decoding time from 1 min to 0.1-10s with comparable perceptual quality. Outperforms HiFiC and MS-ILLM in FID.

Conclusion: The approach offers fast, high-quality decoding for perceptual image codecs, with practical applications and perception-distortion trade-off flexibility.

Abstract: Training-free perceptual image codec adopt pre-trained unconditional
generative model during decoding to avoid training new conditional generative
model. However, they heavily rely on diffusion inversion or sample
communication, which take 1 min to intractable amount of time to decode a
single image. In this paper, we propose a training-free algorithm that improves
the perceptual quality of any existing codec with theoretical guarantee. We
further propose different implementations for optimal perceptual quality when
decoding time budget is $\approx 0.1$s, $0.1-10$s and $\ge 10$s. Our approach:
1). improves the decoding time of training-free codec from 1 min to $0.1-10$s
with comparable perceptual quality. 2). can be applied to non-differentiable
codec such as VTM. 3). can be used to improve previous perceptual codecs, such
as MS-ILLM. 4). can easily achieve perception-distortion trade-off.
Empirically, we show that our approach successfully improves the perceptual
quality of ELIC, VTM and MS-ILLM with fast decoding. Our approach achieves
comparable FID to previous training-free codec with significantly less decoding
time. And our approach still outperforms previous conditional generative model
based codecs such as HiFiC and MS-ILLM in terms of FID. The source code is
provided in the supplementary material.

</details>


### [247] [Enhanced Dermatology Image Quality Assessment via Cross-Domain Training](https://arxiv.org/abs/2506.16116)
*Ignacio Hern√°ndez Montilla,Alfonso Medela,Paola Pasquali,Andy Aguilar,Taig Mac Carthy,Gerardo Fern√°ndez,Antonio Martorell,Enrique Onieva*

Main category: eess.IV

TL;DR: The paper proposes cross-domain training of Image Quality Assessment (IQA) models for teledermatology, combining dermatology and non-dermatology datasets to improve image quality management.


<details>
  <summary>Details</summary>
Motivation: Poor image quality in teledermatology reduces consultation effectiveness, and existing dermatology IQA research lacks leverage of recent advances in non-dermatology IQA.

Method: Created a new dermatology IQA database (Legit.Health-DIQA-Artificial) and trained IQA models using cross-domain datasets to address data scarcity.

Result: Cross-domain training improves performance across domains, manages image distortions better, and enhances teledermatology image quality.

Conclusion: Cross-domain IQA training overcomes data limitations in dermatology, leading to better image quality management in teledermatology.

Abstract: Teledermatology has become a widely accepted communication method in daily
clinical practice, enabling remote care while showing strong agreement with
in-person visits. Poor image quality remains an unsolved problem in
teledermatology and is a major concern to practitioners, as bad-quality images
reduce the usefulness of the remote consultation process. However, research on
Image Quality Assessment (IQA) in dermatology is sparse, and does not leverage
the latest advances in non-dermatology IQA, such as using larger image
databases with ratings from large groups of human observers. In this work, we
propose cross-domain training of IQA models, combining dermatology and
non-dermatology IQA datasets. For this purpose, we created a novel dermatology
IQA database, Legit.Health-DIQA-Artificial, using dermatology images from
several sources and having them annotated by a group of human observers. We
demonstrate that cross-domain training yields optimal performance across
domains and overcomes one of the biggest limitations in dermatology IQA, which
is the small scale of data, and leads to models trained on a larger pool of
image distortions, resulting in a better management of image quality in the
teledermatology process.

</details>


### [248] [From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction](https://arxiv.org/abs/2506.16210)
*Zhenxuan Zhang,Lipei Zhang,Yanqi Cheng,Zi Wang,Fanwen Wang,Haosen Zhang,Yue Yang,Yinzhe Wu,Jiahao Huang,Angelica I Aviles-Rivero,Zhifan Gao,Guang Yang,Peter J. Lally*

Main category: eess.IV

TL;DR: PR-INR framework improves MRI slice-to-volume reconstruction by addressing motion artifacts, structural disruptions, and volumetric anisotropy through progressive refinement and implicit neural representation.


<details>
  <summary>Details</summary>
Motivation: Challenges in MRI slice-to-volume reconstruction include local detail loss, global structural aliasing, and volumetric anisotropy due to motion and undersampling.

Method: PR-INR combines motion-aware diffusion for coarse reconstruction, implicit detail restoration for local refinement, and voxel continuous-aware representation for inter-slice completion.

Result: PR-INR outperforms state-of-the-art methods in reconstruction metrics and visual quality, showing robustness across diverse conditions.

Conclusion: PR-INR effectively unifies motion correction and structural refinement, demonstrating superior performance and generalization in MRI reconstruction.

Abstract: In motion-robust magnetic resonance imaging (MRI), slice-to-volume
reconstruction is critical for recovering anatomically consistent 3D brain
volumes from 2D slices, especially under accelerated acquisitions or patient
motion. However, this task remains challenging due to hierarchical structural
disruptions. It includes local detail loss from k-space undersampling, global
structural aliasing caused by motion, and volumetric anisotropy. Therefore, we
propose a progressive refinement implicit neural representation (PR-INR)
framework. Our PR-INR unifies motion correction, structural refinement, and
volumetric synthesis within a geometry-aware coordinate space. Specifically, a
motion-aware diffusion module is first employed to generate coarse volumetric
reconstructions that suppress motion artifacts and preserve global anatomical
structures. Then, we introduce an implicit detail restoration module that
performs residual refinement by aligning spatial coordinates with visual
features. It corrects local structures and enhances boundary precision.
Further, a voxel continuous-aware representation module represents the image as
a continuous function over 3D coordinates. It enables accurate inter-slice
completion and high-frequency detail recovery. We evaluate PR-INR on five
public MRI datasets under various motion conditions (3% and 5% displacement),
undersampling rates (4x and 8x) and slice resolutions (scale = 5). Experimental
results demonstrate that PR-INR outperforms state-of-the-art methods in both
quantitative reconstruction metrics and visual quality. It further shows
generalization and robustness across diverse unseen domains.

</details>


### [249] [CF-Seg: Counterfactuals meet Segmentation](https://arxiv.org/abs/2506.16213)
*Raghav Mehta,Fabio De Sousa Ribeiro,Tian Xia,Melanie Roschewitz,Ainkaran Santhirasekaram,Dominic C. Marshall,Ben Glocker*

Main category: eess.IV

TL;DR: Using counterfactual images to improve anatomical segmentation in medical images by simulating disease-free appearances.


<details>
  <summary>Details</summary>
Motivation: Disease patterns complicate accurate segmentation, risking misdiagnosis. Counterfactual images simulate healthy anatomy to address this.

Method: Generate counterfactual images to show disease-free anatomy, then segment structures using these images without modifying the segmentation model.

Result: Experiments on chest X-ray datasets show improved anatomical segmentation with counterfactual images.

Conclusion: Counterfactual images enhance segmentation accuracy, supporting better clinical decisions.

Abstract: Segmenting anatomical structures in medical images plays an important role in
the quantitative assessment of various diseases. However, accurate segmentation
becomes significantly more challenging in the presence of disease. Disease
patterns can alter the appearance of surrounding healthy tissues, introduce
ambiguous boundaries, or even obscure critical anatomical structures. As such,
segmentation models trained on real-world datasets may struggle to provide good
anatomical segmentation, leading to potential misdiagnosis. In this paper, we
generate counterfactual (CF) images to simulate how the same anatomy would
appear in the absence of disease without altering the underlying structure. We
then use these CF images to segment structures of interest, without requiring
any changes to the underlying segmentation model. Our experiments on two
real-world clinical chest X-ray datasets show that the use of counterfactual
images improves anatomical segmentation, thereby aiding downstream clinical
decision-making.

</details>


### [250] [AGE-US: automated gestational age estimation based on fetal ultrasound images](https://arxiv.org/abs/2506.16256)
*C√©sar D√≠az-Parga,Marta Nu√±ez-Garcia,Maria J. Carreira,Gabriel Bernardino,Nicol√°s Vila-Blanco*

Main category: eess.IV

TL;DR: An interpretable deep learning method for automated gestational age calculation, using segmentation and distance maps, performs comparably to state-of-the-art models with reduced complexity.


<details>
  <summary>Details</summary>
Motivation: Accurate gestational age estimation is vital for fetal health, but traditional methods (e.g., last menstrual period) are unreliable, and ultrasound-based approaches suffer from manual variability.

Method: Proposes a deep learning-based approach with a novel segmentation architecture and distance maps to address dataset limitations and scarce segmentation masks.

Result: Achieves performance comparable to top models while simplifying complexity, suitable for resource-limited settings. Distance maps excel in femur endpoint estimation.

Conclusion: The method offers a reliable, automated solution for gestational age calculation, especially in settings with limited resources or annotated data.

Abstract: Being born small carries significant health risks, including increased
neonatal mortality and a higher likelihood of future cardiac diseases. Accurate
estimation of gestational age is critical for monitoring fetal growth, but
traditional methods, such as estimation based on the last menstrual period, are
in some situations difficult to obtain. While ultrasound-based approaches offer
greater reliability, they rely on manual measurements that introduce
variability. This study presents an interpretable deep learning-based method
for automated gestational age calculation, leveraging a novel segmentation
architecture and distance maps to overcome dataset limitations and the scarcity
of segmentation masks. Our approach achieves performance comparable to
state-of-the-art models while reducing complexity, making it particularly
suitable for resource-constrained settings and with limited annotated data.
Furthermore, our results demonstrate that the use of distance maps is
particularly suitable for estimating femur endpoints.

</details>


### [251] [VesselSDF: Distance Field Priors for Vascular Network Reconstruction](https://arxiv.org/abs/2506.16556)
*Salvatore Esposito,Daniel Rebain,Arno Onken,Changjian Li,Oisin Mac Aodha*

Main category: eess.IV

TL;DR: VesselSDF introduces a novel framework using signed distance fields (SDFs) for robust vessel segmentation in sparse CT scans, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of vascular networks from sparse CT scans is challenging due to thin, branching vessels and sparsity between imaging planes.

Method: Reformulates vessel segmentation as a continuous SDF regression problem, using an adaptive Gaussian regularizer to ensure smoothness and precision.

Result: VesselSDF outperforms existing methods, preserving vessel geometry and connectivity.

Conclusion: The framework enables more reliable vascular analysis in clinical settings.

Abstract: Accurate segmentation of vascular networks from sparse CT scan slices remains
a significant challenge in medical imaging, particularly due to the thin,
branching nature of vessels and the inherent sparsity between imaging planes.
Existing deep learning approaches, based on binary voxel classification, often
struggle with structural continuity and geometric fidelity. To address this
challenge, we present VesselSDF, a novel framework that leverages signed
distance fields (SDFs) for robust vessel reconstruction. Our method
reformulates vessel segmentation as a continuous SDF regression problem, where
each point in the volume is represented by its signed distance to the nearest
vessel surface. This continuous representation inherently captures the smooth,
tubular geometry of blood vessels and their branching patterns. We obtain
accurate vessel reconstructions while eliminating common SDF artifacts such as
floating segments, thanks to our adaptive Gaussian regularizer which ensures
smoothness in regions far from vessel surfaces while producing precise geometry
near the surface boundaries. Our experimental results demonstrate that
VesselSDF significantly outperforms existing methods and preserves vessel
geometry and connectivity, enabling more reliable vascular analysis in clinical
settings.

</details>


### [252] [DiffO: Single-step Diffusion for Image Compression at Ultra-Low Bitrates](https://arxiv.org/abs/2506.16572)
*Chanung Park,Joo Chan Lee,Jong Hwan Ko*

Main category: eess.IV

TL;DR: DiffO is a single-step diffusion model for image compression, offering high perceptual quality and fast decoding at ultra-low bitrates by using VQ Residual training and rate-adaptive noise modulation.


<details>
  <summary>Details</summary>
Motivation: Existing image compression methods degrade in quality at low bitrates, and diffusion-based models suffer from slow decoding and limited perceptual quality.

Method: DiffO combines VQ Residual training (for global geometry and high-frequency details) and rate-adaptive noise modulation (to adjust denoising strength for desired bitrates).

Result: DiffO outperforms state-of-the-art methods in compression performance and improves decoding speed by 50x.

Conclusion: DiffO enhances the practicality of generative codecs by balancing quality and speed at ultra-low bitrates.

Abstract: Although image compression is fundamental to visual data processing and has
inspired numerous standard and learned codecs, these methods still suffer
severe quality degradation at extremely low bits per pixel. While recent
diffusion based models provided enhanced generative performance at low
bitrates, they still yields limited perceptual quality and prohibitive decoding
latency due to multiple denoising steps. In this paper, we propose the first
single step diffusion model for image compression (DiffO) that delivers high
perceptual quality and fast decoding at ultra low bitrates. DiffO achieves
these goals by coupling two key innovations: (i) VQ Residual training, which
factorizes a structural base code and a learned residual in latent space,
capturing both global geometry and high frequency details; and (ii) rate
adaptive noise modulation, which tunes denoising strength on the fly to match
the desired bitrate. Extensive experiments show that DiffO surpasses state of
the art compression performance while improving decoding speed by about 50x
compared to prior diffusion-based methods, greatly improving the practicality
of generative codecs. The code will be available at
https://github.com/Freemasti/DiffO.

</details>


### [253] [Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images](https://arxiv.org/abs/2506.16592)
*Muhammad Azeem Aslam,Asim Naveed,Nisar Ahmed*

Main category: eess.IV

TL;DR: A hybrid attention-based network for breast ultrasound lesion segmentation improves accuracy by integrating DenseNet121, attention mechanisms, and a hybrid loss function.


<details>
  <summary>Details</summary>
Motivation: Automated tumor segmentation in breast ultrasound is difficult due to noise, scale variations, and fuzzy boundaries, necessitating a robust solution.

Method: The proposed network combines DenseNet121 for feature extraction with an attention-enhanced decoder, incorporating GSA, PE, SDPA, and SFEB for refined spatial features and a hybrid loss function (BCE + Jaccard).

Result: The method outperforms existing approaches on public datasets, showing promise for early breast cancer diagnosis.

Conclusion: The hybrid attention-based network effectively addresses segmentation challenges, aiding radiologists in accurate diagnosis.

Abstract: Breast ultrasound imaging is a valuable tool for early breast cancer
detection, but automated tumor segmentation is challenging due to inherent
noise, variations in scale of lesions, and fuzzy boundaries. To address these
challenges, we propose a novel hybrid attention-based network for lesion
segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in
the encoder part for robust feature extraction with a multi-branch
attention-enhanced decoder tailored for breast ultrasound images. The
bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE),
and Scaled Dot-Product Attention (SDPA) to learn global context, spatial
relationships, and relative positional features. The Spatial Feature
Enhancement Block (SFEB) is embedded at skip connections to refine and enhance
spatial features, enabling the network to focus more effectively on tumor
regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and
Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap
metrics, enhancing robustness to class imbalance and irregular tumor shapes.
Experiments on public datasets demonstrate that our method outperforms existing
approaches, highlighting its potential to assist radiologists in early and
accurate breast cancer diagnosis.

</details>


### [254] [Overfitting in Histopathology Model Training: The Need for Customized Architectures](https://arxiv.org/abs/2506.16631)
*Saghir Alfasly,Ghazal Alabtah,H. R. Tizhoosh*

Main category: eess.IV

TL;DR: The study highlights overfitting issues in deep learning models for histopathology image analysis, showing that large-scale natural image models perform poorly. Customized, simpler architectures outperform them.


<details>
  <summary>Details</summary>
Motivation: To address overfitting and suboptimal performance when applying natural image models to histopathology tasks.

Method: Experiments with ResNet variants and Vision Transformers (ViT) on histopathology datasets, focusing on the Oesophageal Adenocarcinomas dataset.

Result: Increasing model capacity doesn't improve performance; simpler, domain-specific architectures perform better and reduce overfitting.

Conclusion: Customized architectures are essential for histopathology image analysis, especially with limited datasets.

Abstract: This study investigates the critical problem of overfitting in deep learning
models applied to histopathology image analysis. We show that simply adopting
and fine-tuning large-scale models designed for natural image analysis often
leads to suboptimal performance and significant overfitting when applied to
histopathology tasks. Through extensive experiments with various model
architectures, including ResNet variants and Vision Transformers (ViT), we show
that increasing model capacity does not necessarily improve performance on
histopathology datasets. Our findings emphasize the need for customized
architectures specifically designed for histopathology image analysis,
particularly when working with limited datasets. Using Oesophageal
Adenocarcinomas public dataset, we demonstrate that simpler, domain-specific
architectures can achieve comparable or better performance while minimizing
overfitting.

</details>


### [255] [A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer Conversion](https://arxiv.org/abs/2506.16733)
*Fang Chen,Weifeng Zhang,Xingyu Ai,BingXuan Li,An Li,Qiegen Liu*

Main category: eess.IV

TL;DR: The paper proposes a prior-guided joint diffusion model (PJDM) to transform 18F-FDG PET images into 18F-DOPA PET images in the projection domain, improving sinogram quality and synthetic outcomes.


<details>
  <summary>Details</summary>
Motivation: The limitations of 18F-FDG PET for certain tumors and the challenges in 18F-DOPA synthesis and use motivate the need for a method to simulate 18F-DOPA PET images from 18F-FDG data.

Method: A two-stage model (coarse estimation and prior refinement) is trained independently. During inference, a higher-order hybrid sampler generates an initial synthetic 18F-DOPA PET sinogram, which is refined iteratively using learned prior.

Result: PJDM effectively improves sinogram quality and synthetic outcomes, demonstrating its potential for clinical use.

Conclusion: The PJDM framework offers a promising solution for generating 18F-DOPA PET images from 18F-FDG data, addressing limitations of current radiotracers.

Abstract: Positron emission tomography (PET) is widely used to assess metabolic
activity, but its application is limited by the availability of radiotracers.
18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but
shows limited effectiveness for certain tumors. In contrast,
6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity
for neuroendocrine tumors and neurological disorders. However, its complex
synthesis and limitations in transportation and clinical use hinder widespread
adoption. During PET imaging, the sinogram represents a form of raw data
acquired by the scanner. Therefore, modeling in projection domain enables more
direct utilization of the original information, potentially reducing the
accumulation of errors introduced during the image reconstruction process.
Inspired by these factors, this study proposes a prior-guided joint diffusion
model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in
projection domain. Specifically, a coarse estimation model and a prior
refinement model are trained independently. During inference, an initial
synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid
sampler. This sinogram is then degraded and serves as an additional condition
to guide the iterative refinement process using learned prior. Experimental
results demonstrated that PJDM effectively improved both sinogram quality and
synthetic outcomes. The code is available at: https://github.com/yqx7150/PJDM.

</details>


### [256] [Temperature calibration of surface emissivities with an improved thermal image enhancement network](https://arxiv.org/abs/2506.16803)
*Ning Chu,Siya Zheng,Shanqing Zhang,Li Li,Caifang Cai,Ali Mohammad-Djafari,Feng Zhao,Yuanbo Song*

Main category: eess.IV

TL;DR: A neural framework combines temperature correction and image enhancement for infrared thermography, addressing emissivity variations and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack joint optimization of radiometric calibration and image degradation, leading to temperature inaccuracies.

Method: Uses a symmetric skip-CNN and emissivity-aware attention module, with ROI segmentation, dual-constrained loss, and dynamic feature fusion.

Result: Achieves accurate calibration and structural detail recovery in industrial conditions.

Conclusion: The framework effectively unifies temperature correction and image enhancement, improving infrared thermography accuracy.

Abstract: Infrared thermography faces persistent challenges in temperature accuracy due
to material emissivity variations, where existing methods often neglect the
joint optimization of radiometric calibration and image degradation. This study
introduces a physically guided neural framework that unifies temperature
correction and image enhancement through a symmetric skip-CNN architecture and
an emissivity-aware attention module. The pre-processing stage segments the
ROIs of the image and and initially corrected the firing rate. A novel
dual-constrained loss function strengthens the statistical consistency between
the target and reference regions through mean-variance alignment and histogram
matching based on Kullback-Leibler dispersion. The method works by dynamically
fusing thermal radiation features and spatial context, and the model suppresses
emissivity artifacts while recovering structural details. After validating the
industrial blower system under different conditions, the improved network
realizes the dynamic fusion of thermal radiation characteristics and spatial
background, with accurate calibration results in various industrial conditions.

</details>


### [257] [PET Tracer Separation Using Conditional Diffusion Transformer with Multi-latent Space Learning](https://arxiv.org/abs/2506.16934)
*Bin Huang,Feihong Xu,Xinchong Shi,Shan Huang,Binxuan Li,Fei Li,Qiegen Liu*

Main category: eess.IV

TL;DR: Proposes MS-CDT, a multi-latent space guided texture conditional diffusion transformer model, for separating PET tracer signals in multi-tracer PET imaging, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Multi-tracer PET imaging provides supplementary information but faces challenges in distinguishing tracer signals due to identical gamma-photon energies.

Method: Integrates diffusion and transformer architectures with texture masks as conditional inputs and multi-latent space prior for feature representation.

Result: MS-CDT outperforms advanced methods in image quality and preservation of clinically relevant information on brain and chest PET datasets.

Conclusion: MS-CDT effectively addresses tracer separation in PET imaging, balancing computational efficiency and detail preservation.

Abstract: In clinical practice, single-radiotracer positron emission tomography (PET)
is commonly used for imaging. Although multi-tracer PET imaging can provide
supplementary information of radiotracers that are sensitive to physiological
function changes, enabling a more comprehensive characterization of
physiological and pathological states, the gamma-photon pairs generated by
positron annihilation reactions of different tracers in PET imaging have the
same energy, making it difficult to distinguish the tracer signals. In this
study, a multi-latent space guided texture conditional diffusion transformer
model (MS-CDT) is proposed for PET tracer separation. To the best of our
knowledge, this is the first attempt to use texture condition and multi-latent
space for tracer separation in PET imaging. The proposed model integrates
diffusion and transformer architectures into a unified optimization framework,
with the novel addition of texture masks as conditional inputs to enhance image
details. By leveraging multi-latent space prior derived from different tracers,
the model captures multi-level feature representations, aiming to balance
computational efficiency and detail preservation. The texture masks, serving as
conditional guidance, help the model focus on salient structural patterns,
thereby improving the extraction and utilization of fine-grained image
textures. When combined with the diffusion transformer backbone, this
conditioning mechanism contributes to more accurate and robust tracer
separation. To evaluate its effectiveness, the proposed MS-CDT is compared with
several advanced methods on two types of 3D PET datasets: brain and chest
scans. Experimental results indicate that MS-CDT achieved competitive
performance in terms of image quality and preservation of clinically relevant
information. Code is available at: https://github.com/yqx7150/MS-CDT.

</details>


### [258] [Robust Training with Data Augmentation for Medical Imaging Classification](https://arxiv.org/abs/2506.17133)
*Josu√© Mart√≠nez-Mart√≠nez,Olivia Brown,Mostafa Karami,Sheida Nabavi*

Main category: eess.IV

TL;DR: Proposes RTDA, a robust training algorithm with data augmentation, to enhance the resilience of medical image classifiers against adversarial attacks and distribution shifts, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks in medical imaging are vulnerable to adversarial attacks and distribution shifts, risking diagnostic reliability and trust.

Method: Introduces RTDA, combining robust training and data augmentation, and benchmarks it against six baseline techniques on datasets with mammograms, X-rays, and ultrasound.

Result: RTDA shows superior robustness against adversarial attacks and better generalization under distribution shifts while maintaining high clean accuracy.

Conclusion: RTDA is effective for improving the reliability of medical image classifiers in adversarial and varied conditions.

Abstract: Deep neural networks are increasingly being used to detect and diagnose
medical conditions using medical imaging. Despite their utility, these models
are highly vulnerable to adversarial attacks and distribution shifts, which can
affect diagnostic reliability and undermine trust among healthcare
professionals. In this study, we propose a robust training algorithm with data
augmentation (RTDA) to mitigate these vulnerabilities in medical image
classification. We benchmark classifier robustness against adversarial
perturbations and natural variations of RTDA and six competing baseline
techniques, including adversarial training and data augmentation approaches in
isolation and combination, using experimental data sets with three different
imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that
RTDA achieves superior robustness against adversarial attacks and improved
generalization performance in the presence of distribution shift in each image
classification task while maintaining high clean accuracy.

</details>


### [259] [MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification](https://arxiv.org/abs/2506.17140)
*David Jacob Drexlin,Jonas Dippel,Julius Hense,Niklas Preni√ül,Gr√©goire Montavon,Frederick Klauschen,Klaus-Robert M√ºller*

Main category: eess.IV

TL;DR: A novel Metadata-guided generative Diffusion model (MeDi) is proposed to mitigate biases in deep learning models for histopathology by augmenting underrepresented subpopulations with synthetic data.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for histopathology lack robustness to varying conditions (e.g., staining, demographics), leading to biased predictions due to shortcut learning. Existing foundation models haven't fully addressed this issue.

Method: MeDi explicitly models metadata to generate synthetic data for underrepresented subpopulations, balancing training data and reducing biases in downstream models.

Result: MeDi produces high-quality histopathology images for unseen subpopulations, improves image fidelity, and enhances downstream classifier performance on datasets with subpopulation shifts.

Conclusion: MeDi is a proof-of-concept for mitigating data biases in histopathology using generative models, demonstrating potential for clinical adaptation.

Abstract: Deep learning models have made significant advances in histological
prediction tasks in recent years. However, for adaptation in clinical practice,
their lack of robustness to varying conditions such as staining, scanner,
hospital, and demographics is still a limiting factor: if trained on
overrepresented subpopulations, models regularly struggle with less frequent
patterns, leading to shortcut learning and biased predictions. Large-scale
foundation models have not fully eliminated this issue. Therefore, we propose a
novel approach explicitly modeling such metadata into a Metadata-guided
generative Diffusion model framework (MeDi). MeDi allows for a targeted
augmentation of underrepresented subpopulations with synthetic data, which
balances limited training data and mitigates biases in downstream models. We
experimentally show that MeDi generates high-quality histopathology images for
unseen subpopulations in TCGA, boosts the overall fidelity of the generated
images, and enables improvements in performance for downstream classifiers on
datasets with subpopulation shifts. Our work is a proof-of-concept towards
better mitigating data biases with generative models.

</details>


### [260] [Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network](https://arxiv.org/abs/2506.17165)
*Mahin Montasir Afif,Abdullah Al Noman,K. M. Tahsin Kabir,Md. Mortuza Ahmmed,Md. Mostafizur Rahman,Mufti Mahmud,Md. Ashraful Babu*

Main category: eess.IV

TL;DR: GAN-generated brain tumor MRI images can augment limited datasets, improving CNN performance, but excessive synthetic data may reduce model generalization.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of varying ratios of GAN-generated and real brain tumor MRI images on CNN classification performance.

Method: Used a DCGAN to create synthetic images, mixed them with real images at different ratios, and trained a custom CNN. Evaluated performance on a real-world test set.

Result: High sensitivity and precision were maintained even with mostly synthetic data. Best performance (95.2% accuracy) was achieved with a small GAN data addition (e.g., 100 GAN images + 900 real images). Performance declined with higher GAN ratios.

Conclusion: GANs are effective for dataset augmentation when real data is scarce, but excessive synthetic data can harm generalization due to artifacts.

Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding
limited medical imaging datasets. This study explores how different ratios of
GAN-generated and real brain tumor MRI images impact the performance of a CNN
in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic
images which were mixed with real ones at various ratios to train a custom CNN.
The CNN was then evaluated on a separate real-world test set. Our results
indicate that the model maintains high sensitivity and precision in tumor
classification, even when trained predominantly on synthetic data. When only a
small portion of GAN data was added, such as 900 real images and 100 GAN
images, the model achieved excellent performance, with test accuracy reaching
95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the
proportion of GAN images increased further, performance gradually declined.
This study suggests that while GANs are useful for augmenting limited datasets
especially when real data is scarce, too much synthetic data can introduce
artifacts that affect the model's ability to generalize to real world cases.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [261] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Main category: cs.AI

TL;DR: The paper highlights the lack of standardization in Agentic AI research, introduces a robust evaluation protocol, and presents OAgents, a modular framework achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current agent research lacks standardization and rigor, hindering fair comparisons and progress measurement.

Method: A systematic empirical study on GAIA benchmark and BrowseComp to evaluate design choices in agent components.

Result: Identified critical and redundant components, introduced a stable evaluation protocol, and developed OAgents, a top-performing modular framework.

Conclusion: OAgents provides a standardized, modular foundation for future Agentic AI research, addressing reproducibility and performance gaps.

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [262] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia W√ºst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: SLR is an automated framework for evaluating and training LLMs via scalable logical reasoning, creating tasks with controlled difficulty and synthesizing rules, validation programs, and prompts. It benchmarks LLMs, revealing their struggles with logical inference, and improves performance through logic-tuning.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate and enhance LLMs' logical reasoning capabilities without human annotation, ensuring scalability and novelty.

Method: SLR synthesizes tasks with ground-truth rules, validation programs, and prompts, creating SLR-Bench for evaluation. It logic-tunes models like Llama-3-8B.

Result: LLMs often fail at logical inference despite valid syntax. Logic-tuning via SLR doubles Llama-3-8B's accuracy, matching Gemini-Flash-Thinking at lower cost.

Conclusion: SLR provides a scalable, automated solution for advancing LLMs' reasoning, demonstrating significant performance improvements with minimal human intervention.

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [263] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: The paper introduces a framework to evaluate AI agents in mission-critical negotiations, focusing on personality traits and AI characteristics. Experiments show their impact on negotiation outcomes and trustworthiness, providing a methodology for reliable AI systems in high-stakes scenarios.


<details>
  <summary>Details</summary>
Motivation: The need for AI agents that adapt to diverse human operators in critical negotiations, ensuring reliability and effectiveness in complex social dynamics.

Method: Uses Sotopia as a simulation testbed with two experiments: one analyzing personality traits' effects on bargaining, and another evaluating human-AI job negotiations by manipulating traits and AI characteristics.

Result: Agreeableness and Extraversion significantly impact negotiation outcomes. AI trustworthiness (transparency, competence, adaptability) affects mission effectiveness. Sociocognitive measures reveal empathic communication differences.

Conclusion: The framework advances AI evaluation by incorporating social dynamics, providing actionable insights for reliable AI systems in high-stakes operations.

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [264] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: BEWA is a Bayesian-based architecture for structured scientific belief modeling, integrating replication scores, citation weighting, and temporal decay for dynamic belief updates.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of processing vast scientific literature by formalizing belief as a probabilistically coherent function over structured claims.

Method: Uses Bayesian inference, replication scores, citation weighting, temporal decay, and graph-based claim propagation.

Result: Enables machine reasoning with truth utility, rational belief convergence, and audit-resilient integrity.

Conclusion: BEWA provides a foundation for verifiable epistemic networks in dynamic scientific domains.

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [265] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: IS-Bench is a benchmark for evaluating interactive safety in embodied AI agents, revealing current VLMs' lack of safety awareness.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods fail to assess dynamic risks in interactive environments, posing safety hazards for real-world deployment.

Method: IS-Bench introduces a multi-modal benchmark with 161 scenarios and 388 risks, enabling process-oriented evaluation of risk mitigation.

Result: Current VLMs, including GPT-4o and Gemini-2.5, lack interactive safety awareness; safety-aware Chain-of-Thought improves performance but hinders task completion.

Conclusion: IS-Bench lays the groundwork for developing safer embodied AI systems by addressing critical safety limitations.

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [266] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Main category: cs.AI

TL;DR: The paper introduces an Elo rating-based method to improve LLM performance for analyzing harmful content, outperforming traditional methods in accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: LLMs' built-in moderation systems hinder analysis of harmful content like microaggressions or hate speech, affecting research validity.

Method: An Elo rating-based method is proposed to enhance LLM performance for harmful content analysis.

Result: The method outperforms traditional LLM prompting and machine learning models in accuracy, precision, and F1 scores on microaggression and hate speech datasets.

Conclusion: The approach improves reliability, reduces false positives, and supports organizational applications like workplace harassment detection and fostering inclusive environments.

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [267] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garc√©s-Erice,Ioana Giurgiu*

Main category: cs.AI

TL;DR: Different bias evaluation methods for LLMs yield inconsistent model rankings, highlighting the need for standardized benchmarks.


<details>
  <summary>Details</summary>
Motivation: To assess the robustness of independent benchmarks evaluating LLM safety, particularly for bias.

Method: Compare rankings of representative models using various widely used bias evaluation methods.

Result: Disparate model rankings emerge from different evaluation methods.

Conclusion: Recommendations are provided for the community on using such benchmarks effectively.

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


### [268] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: The paper introduces a method called 'The Safety Reminder' to enhance safety in Vision-Language Models (VLMs) by addressing delayed safety awareness through proactive prompt tuning.


<details>
  <summary>Details</summary>
Motivation: VLMs face unique vulnerabilities due to their multimodal nature, allowing adversaries to bypass safety measures and generate harmful content. The study aims to mitigate this by leveraging VLMs' retained safety awareness.

Method: The authors propose 'The Safety Reminder,' a soft prompt tuning approach that injects learnable prompt tokens during text generation to reactivate safety awareness when harmful content is detected.

Result: The method significantly reduces attack success rates across safety benchmarks and adversarial attacks while preserving model utility for benign tasks.

Conclusion: The Safety Reminder offers a practical solution for deploying safer VLMs in real-world applications by proactively enhancing safety awareness without compromising performance.

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [269] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Main category: cs.AI

TL;DR: The paper explores geographic biases in image generation models like FLUX 1 and Stable Diffusion 3.5, revealing a preference for metropolis-like areas and issues with European-sounding names.


<details>
  <summary>Details</summary>
Motivation: To investigate the geographic knowledge and biases embedded in state-of-the-art image generation models when applied to urban analysis and design.

Method: Generated 150 synthetic images per US state and capital using FLUX 1 and Stable Diffusion 3.5, analyzed similarity with DINO-v2 ViT-S/14 and Fr√©chet Inception Distances.

Result: Models show implicit geographic learning but exhibit bias toward metropolis-like areas and struggle with European-sounding names.

Conclusion: Image generation models have geographic biases and disambiguation issues, highlighting the need for more balanced training data.

Abstract: Image generation models are revolutionizing many domains, and urban analysis
and design is no exception. While such models are widely adopted, there is a
limited literature exploring their geographic knowledge, along with the biases
they embed. In this work, we generated 150 synthetic images for each state in
the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two
state-of-the-art models for image generation. We embed each image using DINO-v2
ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity
between the generated images. We found that while these models have implicitly
learned aspects of USA geography, if we prompt the models to generate an image
for "United States" instead of specific cities or states, the models exhibit a
strong representative bias toward metropolis-like areas, excluding rural states
and smaller cities. {\color{black} In addition, we found that models
systematically exhibit some entity-disambiguation issues with European-sounding
names like Frankfort or Devon.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [270] [Smartphone-integrated RPA-CRISPR-Cas12a Detection System with Microneedle Sampling for Point-of-Care Diagnosis of Potato Late Blight in Early Stage](https://arxiv.org/abs/2506.15728)
*Jiangnan Zhao,Hanbo Xu,Cifu Xu,Wenlong Yin,Laixin Luo,Gang Liu,Yan Wang*

Main category: q-bio.QM

TL;DR: A portable RPA-CRISPR system with smartphone integration enables rapid, sensitive, and specific detection of potato late blight, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of conventional lab-based detection methods for plant diseases in field settings.

Method: Uses a PVA microneedle patch for quick sample extraction, followed by an RPA-CRISPR-Cas12a isothermal assay for specific pathogen detection, with smartphone-based image analysis.

Result: Achieves high DNA extraction efficiency (56 ¬µg/mg), a detection limit of 2 pg/¬µL, and early-stage diagnosis (80-100% detection before visible symptoms).

Conclusion: The system offers a promising, portable solution for early plant disease detection in the field, reducing reliance on specialized lab equipment.

Abstract: Potato late blight, caused by the oomycete pathogen Phytophthora infestans,
is one of the most devastating diseases affecting potato crops in the history.
Although conventional detection methods of plant diseases such as PCR and LAMP
are highly sensitive and specific, they rely on bulky and expensive laboratory
equipment and involve complex operations, making them impracticable for
point-of care diagnosis in the field. Here in this study, we report a portable
RPA-CRISPR based diagnosis system for plant disease, integrating smartphone for
acquisition and analysis of fluorescent images. A polyvinyl alcohol (PVA)
microneedle patch was employed for sample extraction on the plant leaves within
one minute, the DNA extraction efficiency achieved 56 ug/mg, which is
approximately 3 times to the traditional CTAB methods (18 ug/mg). The system of
RPA-CRISPR-Cas12a isothermal assay was established to specifically target P.
infestans with no cross-reactivity observed against closely-related species (P.
sojae, P. capsici). The system demonstrated a detection limit of 2 pg/uL for P.
infestans genomic DNA, offering sensitivity comparable to that of benchtop
laboratory equipment. The system demonstrates the early-stage diagnosis
capability by achieving a approximately 80% and 100% detection rate on the
third and fourth day post-inoculation respectively, before visible symptoms
observed on the leaves. The smartphone-based "sample-to-result" system
decouples the limitations of traditional methods that rely heavily on
specialized equipment, offering a promising way for early-stage plant disease
detection and control in the field.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [271] [Wavelet-based Global Orientation and Surface Reconstruction for Point Clouds](https://arxiv.org/abs/2506.16299)
*Yueji Ma,Yanzun Meng,Dong Xiao,Zuoqiang Shi,Bin Wang*

Main category: cs.CG

TL;DR: A wavelet-based method for unoriented surface reconstruction is proposed, addressing sparse point clouds and improving orientation and reconstruction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods like iWSR perform poorly on sparse point clouds, and classic wavelet reconstruction only handles oriented points.

Method: Uses wavelet-based mollified indicator function, modifying kernel for smoothing, and divergence-free function field for constraints.

Result: Achieves state-of-the-art performance in orientation and reconstruction for sparse models, with efficient CPU performance.

Conclusion: The method effectively combines wavelet properties and novel constraints for superior results in unoriented surface reconstruction.

Abstract: Unoriented surface reconstruction is an important task in computer graphics
and has extensive applications. Based on the compact support of wavelet and
orthogonality properties, classic wavelet surface reconstruction achieves good
and fast reconstruction. However, this method can only handle oriented points.
Despite some improved attempts for unoriented points, such as iWSR, these
methods perform poorly on sparse point clouds. To address these shortcomings,
we propose a wavelet-based method to represent the mollified indicator function
and complete both the orientation and surface reconstruction tasks. We use the
modifying kernel function to smoothen out discontinuities on the surface,
aligning with the continuity of the wavelet basis function. During the
calculation of coefficient, we fully utilize the properties of the
convolutional kernel function to shift the modifying computation onto wavelet
basis to accelerate. In addition, we propose a novel method for constructing
the divergence-free function field and using them to construct the additional
homogeneous constraints to improve the effectiveness and stability. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
in both orientation and reconstruction for sparse models. We align the matrix
construction with the compact support property of wavelet basis functions to
further accelerate our method, resulting in efficient performance on CPU. Our
source codes will be released on GitHub.

</details>

{"id": "2507.10577", "pdf": "https://arxiv.org/pdf/2507.10577", "abs": "https://arxiv.org/abs/2507.10577", "authors": ["Log\u00e9 C\u00e9cile", "Ghori Rehan"], "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.", "AI": {"tldr": "The paper introduces an AI-powered system with two agents, Truth Sleuth and Trend Bender, to fact-check YouTube videos and engage users in comments to combat misinformation.", "motivation": "Misinformation spreads rapidly on platforms like YouTube, threatening digital discourse. The paper aims to address this by leveraging AI for fact-checking and user engagement.", "method": "Truth Sleuth extracts claims and verifies them using RAG (Wikipedia, Google Search, Google FactCheck). Trend Bender generates persuasive comments for user engagement, improving iteratively via self-evaluation.", "result": "Experiments on benchmark datasets and real-world YouTube deployment show high fact-checking accuracy and potential to influence user perspectives.", "conclusion": "The system effectively combats misinformation and fosters informed online discussions, demonstrating the promise of AI-driven interventions."}}
{"id": "2507.10580", "pdf": "https://arxiv.org/pdf/2507.10580", "abs": "https://arxiv.org/abs/2507.10580", "authors": ["Vimaleswar A", "Prabhu Nandan Sahu", "Nilesh Kumar Sahu", "Haroon R Lone"], "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions.", "AI": {"tldr": "EmoSApp is an offline, smartphone-based mental health app using fine-tuned LLMs for empathetic, coherent support without internet.", "motivation": "Addressing challenges like accessibility, connectivity, and privacy in digital mental health support.", "method": "Developed EmoSApp using fine-tuned, quantized LLaMA-3.2-1B-Instruct model on a custom mental-health QA dataset, deployed offline via Torchtune and Executorch.", "result": "Qualitative evaluation showed coherent, empathetic responses; quantitative tests confirmed efficacy in low-resource settings.", "conclusion": "EmoSApp exemplifies secure, portable AI-driven mental health solutions, setting a blueprint for future innovations."}}
{"id": "2507.10582", "pdf": "https://arxiv.org/pdf/2507.10582", "abs": "https://arxiv.org/abs/2507.10582", "authors": ["Anders Ledberg", "Anna Thal\u00e9n"], "title": "Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis", "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "Unstructured text from legal, medical, and administrative sources offers a\nrich but underutilized resource for research in public health and the social\nsciences. However, large-scale analysis is hampered by two key challenges: the\npresence of sensitive, personally identifiable information, and significant\nheterogeneity in structure and language. We present a modular toolchain that\nprepares such text data for embedding-based analysis, relying entirely on\nopen-weight models that run on local hardware, requiring only a\nworkstation-level GPU and supporting privacy-sensitive research.\n  The toolchain employs large language model (LLM) prompting to standardize,\nsummarize, and, when needed, translate texts to English for greater\ncomparability. Anonymization is achieved via LLM-based redaction, supplemented\nwith named entity recognition and rule-based methods to minimize the risk of\ndisclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court\ndecisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.\nEach document is processed into an anonymized, standardized summary and\ntransformed into a document-level embedding. Validation, including manual\nreview, automated scanning, and predictive evaluation shows the toolchain\neffectively removes identifying information while retaining semantic content.\nAs an illustrative application, we train a predictive model using embedding\nvectors derived from a small set of manually labeled summaries, demonstrating\nthe toolchain's capacity for semi-automated content analysis at scale.\n  By enabling structured, privacy-conscious analysis of sensitive documents,\nour toolchain opens new possibilities for large-scale research in domains where\ntextual data was previously inaccessible due to privacy and heterogeneity\nconstraints.", "AI": {"tldr": "A modular toolchain using open-weight LLMs processes sensitive, unstructured text (e.g., legal, medical) for embedding-based analysis, ensuring privacy and standardization. Demonstrated on Swedish court decisions, it anonymizes and summarizes documents effectively.", "motivation": "To enable large-scale analysis of sensitive, heterogeneous text data (e.g., legal, medical) while addressing privacy and structural challenges.", "method": "Uses LLM prompting for standardization, summarization, and translation, plus LLM-based redaction and rule-based methods for anonymization. Validated via manual review and predictive evaluation.", "result": "Successfully processes 10,842 Swedish court decisions into anonymized, standardized summaries and embeddings, retaining semantic content while removing sensitive info.", "conclusion": "The toolchain facilitates privacy-conscious, large-scale text analysis, expanding research possibilities in previously inaccessible domains."}}
{"id": "2507.10585", "pdf": "https://arxiv.org/pdf/2507.10585", "abs": "https://arxiv.org/abs/2507.10585", "authors": ["Isar Nejadgholi", "Mona Omidyeganeh", "Marc-Antoine Drouin", "Jonathan Boisvert"], "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Workshop of Technical AI Governance, 5 pages 2\n  figures", "summary": "Effective AI governance requires structured approaches for stakeholders to\naccess and verify AI system behavior. With the rise of large language models,\nNatural Language Explanations (NLEs) are now key to articulating model\nbehavior, which necessitates a focused examination of their characteristics and\ngovernance implications. We draw on Explainable AI (XAI) literature to create\nan updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:\n(1) Context, including task, data, audience, and goals; (2) Generation and\nPresentation, covering generation methods, inputs, interactivity, outputs, and\nforms; and (3) Evaluation, focusing on content, presentation, and user-centered\nproperties, as well as the setting of the evaluation. This taxonomy provides a\nframework for researchers, auditors, and policymakers to characterize, design,\nand enhance NLEs for transparent AI systems.", "AI": {"tldr": "The paper proposes an updated taxonomy for Explainable AI (XAI) focused on Natural Language Explanations (NLEs) to improve AI governance by structuring their creation, presentation, and evaluation.", "motivation": "The rise of large language models necessitates clear NLEs for transparency, requiring a systematic approach to their design and governance.", "method": "The authors develop an XAI taxonomy for NLEs across three dimensions: Context, Generation and Presentation, and Evaluation.", "result": "The taxonomy offers a structured framework for stakeholders to analyze and improve NLEs in AI systems.", "conclusion": "This framework aids researchers, auditors, and policymakers in enhancing transparency and governance of AI systems through better NLEs."}}
{"id": "2507.10689", "pdf": "https://arxiv.org/pdf/2507.10689", "abs": "https://arxiv.org/abs/2507.10689", "authors": ["Tongshun Zhang", "Pingping Liu", "Yubing Lu", "Mengen Cai", "Zijian Zhang", "Zhe Zhang", "Qiuzhan Zhou"], "title": "CWNet: Causal Wavelet Network for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on\nuniform brightness adjustment, often neglecting instance-level semantic\ninformation and the inherent characteristics of different features. To address\nthese limitations, we propose CWNet (Causal Wavelet Network), a novel\narchitecture that leverages wavelet transforms for causal reasoning.\nSpecifically, our approach comprises two key components: 1) Inspired by the\nconcept of intervention in causality, we adopt a causal reasoning perspective\nto reveal the underlying causal relationships in low-light enhancement. From a\nglobal perspective, we employ a metric learning strategy to ensure causal\nembeddings adhere to causal principles, separating them from non-causal\nconfounding factors while focusing on the invariance of causal factors. At the\nlocal level, we introduce an instance-level CLIP semantic loss to precisely\nmaintain causal factor consistency. 2) Based on our causal analysis, we present\na wavelet transform-based backbone network that effectively optimizes the\nrecovery of frequency information, ensuring precise enhancement tailored to the\nspecific attributes of wavelet transforms. Extensive experiments demonstrate\nthat CWNet significantly outperforms current state-of-the-art methods across\nmultiple datasets, showcasing its robust performance across diverse scenes.\nCode is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.", "AI": {"tldr": "CWNet introduces a causal wavelet network for low-light image enhancement, leveraging wavelet transforms and causal reasoning to outperform existing methods.", "motivation": "Traditional LLIE methods lack instance-level semantic understanding and feature-specific adjustments. CWNet addresses this by incorporating causal reasoning and wavelet transforms.", "method": "CWNet uses causal reasoning with global metric learning and local CLIP semantic loss, combined with a wavelet transform-based backbone for frequency optimization.", "result": "CWNet outperforms state-of-the-art methods across multiple datasets, demonstrating robust performance in diverse scenes.", "conclusion": "CWNet effectively enhances low-light images by integrating causal reasoning and wavelet transforms, offering superior performance and precision."}}
{"id": "2507.10586", "pdf": "https://arxiv.org/pdf/2507.10586", "abs": "https://arxiv.org/abs/2507.10586", "authors": ["Kaushik Dwivedi", "Padmanabh Patanjali Mishra"], "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable fluency across a\nrange of natural language tasks, yet remain vulnerable to hallucinations -\nfactual inaccuracies that undermine trust in real world deployment. We present\nAutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that\ntackles hallucination in large language models through lightweight LoRA-based\nadapters and KL-regularized training. Our pipeline integrates automated prompt\nrewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in\nretrieved evidence. A hallucination detection module, using both\nclassifier-based and self-evaluation techniques, assigns confidence scores to\ngenerated outputs, triggering an optional feedback correction loop. This loop\nenforces factual alignment via contrastive KL loss and adapter fine tuning. We\ndemonstrate that AutoRAG-LoRA significantly reduces the factual drift while\npreserving the efficiency and modularity of the model.", "AI": {"tldr": "AutoRAG-LoRA is a framework to reduce hallucinations in LLMs using LoRA adapters, KL-regularized training, and retrieval-augmented generation, with a feedback loop for factual alignment.", "motivation": "LLMs often produce factual inaccuracies (hallucinations), which undermine trust in real-world applications.", "method": "The framework uses LoRA-based adapters, KL-regularized training, automated prompt rewriting, hybrid retrieval, and a hallucination detection module with feedback correction.", "result": "AutoRAG-LoRA significantly reduces factual drift while maintaining model efficiency and modularity.", "conclusion": "The proposed framework effectively mitigates hallucinations in LLMs, enhancing their reliability for practical use."}}
{"id": "2507.10737", "pdf": "https://arxiv.org/pdf/2507.10737", "abs": "https://arxiv.org/abs/2507.10737", "authors": ["Jiayuan Chen", "Thai-Hoang Pham", "Yuanlong Wang", "Ping Zhang"], "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "High-throughput screening techniques, such as microscopy imaging of cellular\nresponses to genetic and chemical perturbations, play a crucial role in drug\ndiscovery and biomedical research. However, robust perturbation screening for\n\\textit{de novo} cell lines remains challenging due to the significant\nmorphological and biological heterogeneity across cell lines. To address this,\nwe propose a novel framework that integrates external biological knowledge into\nexisting pretraining strategies to enhance microscopy image profiling models.\nOur approach explicitly disentangles perturbation-specific and cell\nline-specific representations using external biological information.\nSpecifically, we construct a knowledge graph leveraging protein interaction\ndata from STRING and Hetionet databases to guide models toward\nperturbation-specific features during pretraining. Additionally, we incorporate\ntranscriptomic features from single-cell foundation models to capture cell\nline-specific representations. By learning these disentangled features, our\nmethod improves the generalization of imaging models to \\textit{de novo} cell\nlines. We evaluate our framework on the RxRx database through one-shot\nfine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from\nthe RxRx19a dataset. Experimental results demonstrate that our method enhances\nmicroscopy image profiling for \\textit{de novo} cell lines, highlighting its\neffectiveness in real-world phenotype-based drug discovery applications.", "AI": {"tldr": "A novel framework integrates biological knowledge to improve microscopy image profiling for de novo cell lines by disentangling perturbation-specific and cell line-specific features.", "motivation": "Addressing the challenge of robust perturbation screening for de novo cell lines due to morphological and biological heterogeneity.", "method": "Integrates external biological knowledge (protein interaction data and transcriptomic features) to disentangle representations during pretraining.", "result": "Improves generalization of imaging models to de novo cell lines, validated on RxRx datasets.", "conclusion": "The framework enhances microscopy image profiling, proving effective for phenotype-based drug discovery."}}
{"id": "2507.10587", "pdf": "https://arxiv.org/pdf/2507.10587", "abs": "https://arxiv.org/abs/2507.10587", "authors": ["Dennis Ulmer", "Alexandra Lorson", "Ivan Titov", "Christian Hardmeier"], "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Human users increasingly rely on natural language interactions with large\nlanguage models (LLMs) in order to receive help on a large variety of tasks and\nproblems. However, the trustworthiness and perceived legitimacy of LLMs is\nundermined by the fact that their output is frequently stated in very confident\nterms, even when its accuracy is questionable. Therefore, there is a need to\nsignal the confidence of the language model to a user in order to reap the\nbenefits of human-machine collaboration and mitigate potential harms.\nVerbalized uncertainty is the expression of confidence with linguistic means,\nan approach that integrates perfectly into language-based interfaces.\nNevertheless, most recent research in natural language processing (NLP)\noverlooks the nuances surrounding human uncertainty communication and the data\nbiases that influence machine uncertainty communication. We argue for\nanthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty\ncommunication requires a degree of linguistic authenticity and personalization\nto the user, which could be achieved by emulating human communication. We\npresent a thorough overview over the research in human uncertainty\ncommunication, survey ongoing research, and perform additional analyses to\ndemonstrate so-far overlooked biases in verbalized uncertainty. We conclude by\npointing out unique factors in human-machine communication of uncertainty and\ndeconstruct anthropomimetic uncertainty into future research directions for\nNLP.", "AI": {"tldr": "The paper discusses the need for LLMs to communicate uncertainty to users to enhance trust and collaboration, proposing anthropomimetic uncertainty as a solution.", "motivation": "LLMs often provide overly confident outputs, undermining trust. Effective uncertainty communication is needed for better human-machine collaboration.", "method": "The paper reviews human uncertainty communication, surveys NLP research, and analyzes biases in verbalized uncertainty.", "result": "Highlights overlooked biases and proposes anthropomimetic uncertainty for more authentic and personalized communication.", "conclusion": "Future NLP research should focus on human-like uncertainty communication to improve trust and effectiveness in human-machine interactions."}}
{"id": "2507.10755", "pdf": "https://arxiv.org/pdf/2507.10755", "abs": "https://arxiv.org/abs/2507.10755", "authors": ["Rina Khan", "Catherine Stinson"], "title": "Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Facial expression recognition (FER) algorithms classify facial expressions\ninto emotions such as happy, sad, or angry. An evaluative challenge facing FER\nalgorithms is the fall in performance when detecting spontaneous expressions\ncompared to posed expressions. An ethical (and evaluative) challenge facing FER\nalgorithms is that they tend to perform poorly for people of some races and\nskin colors. These challenges are linked to the data collection practices\nemployed in the creation of FER datasets. In this study, we audit two\nstate-of-the-art FER datasets. We take random samples from each dataset and\nexamine whether images are spontaneous or posed. In doing so, we propose a\nmethodology for identifying spontaneous or posed images. We discover a\nsignificant number of images that were posed in the datasets purporting to\nconsist of in-the-wild images. Since performance of FER models vary between\nspontaneous and posed images, the performance of models trained on these\ndatasets will not represent the true performance if such models were to be\ndeployed in in-the-wild applications. We also observe the skin color of\nindividuals in the samples, and test three models trained on each of the\ndatasets to predict facial expressions of people from various races and skin\ntones. We find that the FER models audited were more likely to predict people\nlabeled as not white or determined to have dark skin as showing a negative\nemotion such as anger or sadness even when they were smiling. This bias makes\nsuch models prone to perpetuate harm in real life applications.", "AI": {"tldr": "The paper audits FER datasets, revealing biases in performance for spontaneous expressions and racial/skin color groups, highlighting ethical and evaluative challenges.", "motivation": "To address performance drops in FER algorithms for spontaneous expressions and racial/skin color biases, linked to dataset collection practices.", "method": "Audit two FER datasets by sampling images to classify them as spontaneous or posed, and evaluate model performance across races and skin tones.", "result": "Found posed images in 'in-the-wild' datasets and racial/skin color biases in FER models, leading to incorrect negative emotion predictions for non-white/dark-skinned individuals.", "conclusion": "FER datasets and models exhibit biases, risking harm in real-world applications; improved data collection and model evaluation are needed."}}
{"id": "2507.10596", "pdf": "https://arxiv.org/pdf/2507.10596", "abs": "https://arxiv.org/abs/2507.10596", "authors": ["Yogachandran Rahulamathavan", "Misbah Farooq", "Varuna De Silva"], "title": "PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) excel in text classification, but their\ncomplexity hinders interpretability, making it difficult to understand the\nreasoning behind their predictions. Explainable AI (XAI) methods like LIME and\nSHAP offer local explanations by identifying influential words, but they rely\non computationally expensive perturbations. These methods typically generate\nthousands of perturbed sentences and perform inferences on each, incurring a\nsubstantial computational burden, especially with LLMs. To address this, we\npropose \\underline{P}erturbation-free \\underline{L}ocal \\underline{Ex}planation\n(PLEX), a novel method that leverages the contextual embeddings extracted from\nthe LLM and a ``Siamese network\" style neural network trained to align with\nfeature importance scores. This one-off training eliminates the need for\nsubsequent perturbations, enabling efficient explanations for any new sentence.\nWe demonstrate PLEX's effectiveness on four different classification tasks\n(sentiment, fake news, fake COVID-19 news and depression), showing more than\n92\\% agreement with LIME and SHAP. Our evaluation using a ``stress test\"\nreveals that PLEX accurately identifies influential words, leading to a similar\ndecline in classification accuracy as observed with LIME and SHAP when these\nwords are removed. Notably, in some cases, PLEX demonstrates superior\nperformance in capturing the impact of key features. PLEX dramatically\naccelerates explanation, reducing time and computational overhead by two and\nfour orders of magnitude, respectively. This work offers a promising solution\nfor explainable LLM-based text classification.", "AI": {"tldr": "PLEX is a perturbation-free method for explainable AI in LLMs, using contextual embeddings and a Siamese network to match feature importance scores, offering faster and more efficient explanations than LIME and SHAP.", "motivation": "The complexity of LLMs makes their predictions hard to interpret, and existing XAI methods like LIME and SHAP are computationally expensive due to reliance on perturbations.", "method": "PLEX leverages LLM embeddings and a Siamese network to align with feature importance scores, eliminating the need for perturbations.", "result": "PLEX achieves over 92% agreement with LIME and SHAP, identifies influential words accurately, and reduces explanation time and computational overhead significantly.", "conclusion": "PLEX provides an efficient and effective solution for explainable LLM-based text classification, outperforming traditional methods in speed and computational cost."}}
{"id": "2507.10770", "pdf": "https://arxiv.org/pdf/2507.10770", "abs": "https://arxiv.org/abs/2507.10770", "authors": ["Ionu\u0163 Grigore", "C\u0103lin-Adrian Popa", "Claudiu Leoveanu-Condrei"], "title": "FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching", "categories": ["cs.CV"], "comment": null, "summary": "The extraction and matching of interest points are fundamental to many\ngeometric computer vision tasks. Traditionally, matching is performed by\nassigning descriptors to interest points and identifying correspondences based\non descriptor similarity. This work introduces a technique where interest\npoints are inherently associated during detection, eliminating the need for\ncomputing, storing, transmitting, or matching descriptors. Although the\nmatching accuracy is marginally lower than that of conventional approaches, our\nmethod completely eliminates the need for descriptors, leading to a drastic\nreduction in memory usage for localization systems. We assess its effectiveness\nby comparing it against both classical handcrafted methods and modern learned\napproaches.", "AI": {"tldr": "A new technique eliminates descriptor usage in interest point matching, reducing memory while slightly lowering accuracy.", "motivation": "Traditional methods rely on descriptors for matching, which increases memory usage. This work aims to remove the need for descriptors.", "method": "Interest points are inherently associated during detection, bypassing descriptor computation and matching.", "result": "Matching accuracy is slightly lower than conventional methods, but memory usage is drastically reduced.", "conclusion": "The method offers a memory-efficient alternative to descriptor-based matching, suitable for localization systems."}}
{"id": "2507.10599", "pdf": "https://arxiv.org/pdf/2507.10599", "abs": "https://arxiv.org/abs/2507.10599", "authors": ["Bo Zhao", "Maya Okawa", "Eric J. Bigelow", "Rose Yu", "Tomer Ullman", "Ekdeep Singh Lubana", "Hidenori Tanaka"], "title": "Emergence of Hierarchical Emotion Organization in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) increasingly power conversational agents,\nunderstanding how they model users' emotional states is critical for ethical\ndeployment. Inspired by emotion wheels -- a psychological framework that argues\nemotions organize hierarchically -- we analyze probabilistic dependencies\nbetween emotional states in model outputs. We find that LLMs naturally form\nhierarchical emotion trees that align with human psychological models, and\nlarger models develop more complex hierarchies. We also uncover systematic\nbiases in emotion recognition across socioeconomic personas, with compounding\nmisclassifications for intersectional, underrepresented groups. Human studies\nreveal striking parallels, suggesting that LLMs internalize aspects of social\nperception. Beyond highlighting emergent emotional reasoning in LLMs, our\nresults hint at the potential of using cognitively-grounded theories for\ndeveloping better model evaluations.", "AI": {"tldr": "LLMs form hierarchical emotion trees aligning with human psychology, with larger models showing more complexity. Biases in emotion recognition exist, especially for underrepresented groups, mirroring human social perceptions.", "motivation": "To understand how LLMs model users' emotional states for ethical deployment, inspired by psychological emotion wheels.", "method": "Analyzed probabilistic dependencies between emotional states in LLM outputs, comparing hierarchical structures and biases across socioeconomic personas.", "result": "LLMs naturally form hierarchical emotion trees resembling human models, with larger models exhibiting more complexity. Systematic biases in emotion recognition were found, particularly for underrepresented groups.", "conclusion": "LLMs internalize aspects of social perception, and cognitively-grounded theories could improve model evaluations."}}
{"id": "2507.10775", "pdf": "https://arxiv.org/pdf/2507.10775", "abs": "https://arxiv.org/abs/2507.10775", "authors": ["Jeffrey Joan Sam", "Janhavi Sathe", "Nikhil Chigali", "Naman Gupta", "Radhey Ruparel", "Yicheng Jiang", "Janmajay Singh", "James W. Berck", "Arko Barman"], "title": "A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Spacecraft deployed in outer space are routinely subjected to various forms\nof damage due to exposure to hazardous environments. In addition, there are\nsignificant risks to the subsequent process of in-space repairs through human\nextravehicular activity or robotic manipulation, incurring substantial\noperational costs. Recent developments in image segmentation could enable the\ndevelopment of reliable and cost-effective autonomous inspection systems. While\nthese models often require large amounts of training data to achieve\nsatisfactory results, publicly available annotated spacecraft segmentation data\nare very scarce. Here, we present a new dataset of nearly 64k annotated\nspacecraft images that was created using real spacecraft models, superimposed\non a mixture of real and synthetic backgrounds generated using NASA's TTALOS\npipeline. To mimic camera distortions and noise in real-world image\nacquisition, we also added different types of noise and distortion to the\nimages. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to\ngenerate performance benchmarks for the dataset under well-defined hardware and\ninference time constraints to mimic real-world image segmentation challenges\nfor real-time onboard applications in space on NASA's inspector spacecraft. The\nresulting models, when tested under these constraints, achieved a Dice score of\n0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.\nThe dataset and models for performance benchmark are available at\nhttps://github.com/RiceD2KLab/SWiM.", "AI": {"tldr": "A new dataset of 64k annotated spacecraft images was created for autonomous inspection, with benchmarks for YOLOv8 and YOLOv11 models achieving high accuracy under real-time constraints.", "motivation": "Spacecraft face damage risks in outer space, and repairs are costly. Autonomous inspection systems could mitigate these issues, but lack of annotated data is a barrier.", "method": "Created a dataset using real spacecraft models and synthetic backgrounds, added noise/distortion, and finetuned YOLOv8/YOLOv11 models for benchmarking.", "result": "Models achieved a Dice score of 0.92, Hausdorff distance of 0.69, and inference time of ~0.5s under real-time constraints.", "conclusion": "The dataset and models provide a reliable benchmark for real-time spacecraft inspection, addressing data scarcity and enabling cost-effective solutions."}}
{"id": "2507.10743", "pdf": "https://arxiv.org/pdf/2507.10743", "abs": "https://arxiv.org/abs/2507.10743", "authors": ["Nickolas Freeman", "Thanh Nguyen", "Gregory Bott", "Jason Parton", "Collin Francel"], "title": "Language Models for Adult Service Website Text Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "32 pages, 12 figures, 1 table", "summary": "Sex trafficking refers to the use of force, fraud, or coercion to compel an\nindividual to perform in commercial sex acts against their will. Adult service\nwebsites (ASWs) have and continue to be linked to sex trafficking, offering a\nplatform for traffickers to advertise their victims. Thus, organizations\ninvolved in the fight against sex trafficking often use ASW data when\nattempting to identify potential sex trafficking victims. A critical challenge\nin transforming ASW data into actionable insight is text analysis. Previous\nresearch using ASW data has shown that ASW ad text is important for linking\nads. However, working with this text is challenging due to its extensive use of\nemojis, poor grammar, and deliberate obfuscation to evade law enforcement\nscrutiny. We conduct a comprehensive study of language modeling approaches for\nthis application area, including simple information retrieval methods,\npre-trained transformers, and custom transformer models. We demonstrate that\ncharacteristics of ASW text data allow efficient custom transformer models to\nbe trained with relatively small GPU resources and used efficiently for\ninference on consumer hardware. Our custom models outperform fine-tuned\nvariants of well-known encoder-only transformer models, including BERT-base,\nRoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We\ndemonstrate the use of our best-performing custom configuration on three tasks\nrelated to ASW data analysis: (i) decomposing the giant component in a graph\nrepresentation of ASW data, (ii) clustering ASW ad text, and (iii) using the\nlearned token embeddings to understand the use of emojis in the illicit context\nwe study. The models we develop represent a significant advancement in ASW text\nanalysis, which can be leveraged in a variety of downstream applications and\nresearch.", "AI": {"tldr": "The paper explores language modeling for analyzing Adult Service Website (ASW) ad text to combat sex trafficking, proposing custom transformer models that outperform existing methods.", "motivation": "ASWs are linked to sex trafficking, but analyzing their ad text is challenging due to obfuscation and poor grammar. Effective text analysis is needed to identify victims.", "method": "The study evaluates various language modeling approaches, including custom transformers, and tests them on ASW text data.", "result": "Custom transformer models outperform pre-trained models like BERT and RoBERTa in accuracy, recall, and other metrics. They also enable efficient inference on consumer hardware.", "conclusion": "The custom models advance ASW text analysis, aiding tasks like graph decomposition, clustering, and emoji analysis, with potential for broader applications."}}
{"id": "2507.10778", "pdf": "https://arxiv.org/pdf/2507.10778", "abs": "https://arxiv.org/abs/2507.10778", "authors": ["Hsiang-Wei Huang", "Jen-Hao Cheng", "Kuang-Ming Chen", "Cheng-Yen Yang", "Bahaa Alattar", "Yi-Ru Lin", "Pyongkun Kim", "Sangwon Kim", "Kwangju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "title": "Warehouse Spatial Question Answering with LLM Agent", "categories": ["cs.CV", "cs.AI"], "comment": "1st Place Solution of the 9th AI City Challenge Track 3", "summary": "Spatial understanding has been a challenging task for existing Multi-modal\nLarge Language Models~(MLLMs). Previous methods leverage large-scale MLLM\nfinetuning to enhance MLLM's spatial understanding ability. In this paper, we\npresent a data-efficient approach. We propose a LLM agent system with strong\nand advanced spatial reasoning ability, which can be used to solve the\nchallenging spatial question answering task in complex indoor warehouse\nscenarios. Our system integrates multiple tools that allow the LLM agent to\nconduct spatial reasoning and API tools interaction to answer the given\ncomplicated spatial question. Extensive evaluations on the 2025 AI City\nChallenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that\nour system achieves high accuracy and efficiency in tasks such as object\nretrieval, counting, and distance estimation. The code is available at:\nhttps://github.com/hsiangwei0903/SpatialAgent", "AI": {"tldr": "A data-efficient LLM agent system is proposed for spatial reasoning in complex indoor warehouse scenarios, outperforming previous methods.", "motivation": "Existing MLLMs struggle with spatial understanding, prompting the need for a more efficient solution.", "method": "The system integrates LLM agents with spatial reasoning tools and API interactions for complex spatial tasks.", "result": "Achieves high accuracy in object retrieval, counting, and distance estimation on the AI City Challenge dataset.", "conclusion": "The proposed system is effective and efficient for spatial reasoning in warehouse environments."}}
{"id": "2507.10772", "pdf": "https://arxiv.org/pdf/2507.10772", "abs": "https://arxiv.org/abs/2507.10772", "authors": ["Michal Podstawski"], "title": "Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis.", "AI": {"tldr": "The paper explores using pretrained text embedding models to enhance semantic analysis in labeled property graphs, improving tasks like node classification and relation prediction without altering the graph structure.", "motivation": "To leverage rich textual attributes in property graphs for better analytical tasks by integrating pretrained text embeddings.", "method": "Embed textual node and edge properties using pretrained language models and integrate them into the graph pipeline.", "result": "The approach enhances accuracy and interpretability of property graph analysis by incorporating textual semantics.", "conclusion": "Textual semantics from embeddings significantly improve property graph analysis without structural changes."}}
{"id": "2507.10800", "pdf": "https://arxiv.org/pdf/2507.10800", "abs": "https://arxiv.org/abs/2507.10800", "authors": ["Ali Hojjat", "Janek Haberer", "Soren Pirk", "Olaf Landsiedel"], "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Vision Transformers deliver state-of-the-art performance, yet their fixed\ncomputational budget prevents scalable deployment across heterogeneous\nhardware. Recent nested Transformer architectures mitigate this by embedding\nnested subnetworks within a single model to enable scalable inference. However,\nthese models allocate the same amount of compute to all inputs, regardless of\ntheir complexity, which leads to inefficiencies. To address this, we introduce\nThinkingViT, a nested ViT architecture that employs progressive thinking stages\nto dynamically adjust inference computation based on input difficulty.\nThinkingViT initiates inference by activating a small subset of the most\nimportant attention heads and terminates early if predictions reach sufficient\ncertainty. Otherwise, it activates additional attention heads and re-evaluates\nthe input. At the core of ThinkingViT is our Token Recycling mechanism, which\nconditions each subsequent inference stage on the embeddings from the previous\nstage, enabling progressive improvement. Due to its backbone-preserving design,\nThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show\nthat ThinkingViT surpasses nested baselines by up to 2.0 percentage points\n(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs\non ImageNet-1K. The source code is available at\nhttps://github.com/ds-kiel/ThinkingViT.", "AI": {"tldr": "ThinkingViT is a nested Vision Transformer (ViT) that dynamically adjusts computation based on input difficulty, improving efficiency and accuracy.", "motivation": "Fixed computational budgets in Vision Transformers limit scalability and efficiency, especially for inputs of varying complexity.", "method": "ThinkingViT uses progressive thinking stages and Token Recycling to dynamically activate attention heads and terminate early if predictions are certain.", "result": "ThinkingViT outperforms nested baselines by up to 2.9 p.p. in accuracy on ImageNet-1K at equal computational cost.", "conclusion": "ThinkingViT offers a scalable, efficient, and accurate solution for Vision Transformers, with potential as a plugin upgrade."}}
{"id": "2507.10787", "pdf": "https://arxiv.org/pdf/2507.10787", "abs": "https://arxiv.org/abs/2507.10787", "authors": ["Yilun Zhao", "Chengye Wang", "Chuhan Li", "Arman Cohan"], "title": "Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers", "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "This paper introduces MISS-QA, the first benchmark specifically designed to\nevaluate the ability of models to interpret schematic diagrams within\nscientific literature. MISS-QA comprises 1,500 expert-annotated examples over\n465 scientific papers. In this benchmark, models are tasked with interpreting\nschematic diagrams that illustrate research overviews and answering\ncorresponding information-seeking questions based on the broader context of the\npaper. We assess the performance of 18 frontier multimodal foundation models,\nincluding o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant\nperformance gap between these models and human experts on MISS-QA. Our analysis\nof model performance on unanswerable questions and our detailed error analysis\nfurther highlight the strengths and limitations of current models, offering key\ninsights to enhance models in comprehending multimodal scientific literature.", "AI": {"tldr": "MISS-QA is a new benchmark for evaluating models' ability to interpret schematic diagrams in scientific papers, revealing a performance gap between models and humans.", "motivation": "To assess and improve models' comprehension of multimodal scientific literature, particularly schematic diagrams.", "method": "Created a benchmark with 1,500 expert-annotated examples from 465 papers, testing 18 models on diagram interpretation and question-answering.", "result": "Significant performance gap between models and humans; detailed error analysis provided insights into model limitations.", "conclusion": "MISS-QA highlights current model shortcomings and offers directions for improving multimodal scientific literature comprehension."}}
{"id": "2507.10844", "pdf": "https://arxiv.org/pdf/2507.10844", "abs": "https://arxiv.org/abs/2507.10844", "authors": ["Furkan Mumcu", "Michael J. Jones", "Anoop Cherian", "Yasin Yilmaz"], "title": "LLM-Guided Agentic Object Detection for Open-World Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Object detection traditionally relies on fixed category sets, requiring\ncostly re-training to handle novel objects. While Open-World and\nOpen-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD\nlacks semantic labels for unknowns, and OVOD depends on user prompts, limiting\nautonomy. We propose an LLM-guided agentic object detection (LAOD) framework\nthat enables fully label-free, zero-shot detection by prompting a Large\nLanguage Model (LLM) to generate scene-specific object names. These are passed\nto an open-vocabulary detector for localization, allowing the system to adapt\nits goals dynamically. We introduce two new metrics, Class-Agnostic Average\nPrecision (CAAP) and Semantic Naming Average Precision (SNAP), to separately\nevaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD\nvalidate our approach, showing strong performance in detecting and naming novel\nobjects. Our method offers enhanced autonomy and adaptability for open-world\nunderstanding.", "AI": {"tldr": "Proposes LLM-guided agentic object detection (LAOD) for label-free, zero-shot detection by using LLMs to generate scene-specific object names, improving autonomy and adaptability in open-world object detection.", "motivation": "Traditional object detection requires fixed category sets and costly re-training for novel objects. Existing methods like OWOD and OVOD lack semantic labels or depend on user prompts, limiting autonomy.", "method": "LAOD framework uses an LLM to generate scene-specific object names, which are passed to an open-vocabulary detector for localization. Introduces new metrics (CAAP and SNAP) to evaluate localization and naming separately.", "result": "Validated on LVIS, COCO, and COCO-OOD datasets, showing strong performance in detecting and naming novel objects.", "conclusion": "LAOD enhances autonomy and adaptability for open-world understanding, offering a flexible and label-free solution."}}
{"id": "2507.10810", "pdf": "https://arxiv.org/pdf/2507.10810", "abs": "https://arxiv.org/abs/2507.10810", "authors": ["David M. Markowitz", "Samuel Hardman Taylor"], "title": "Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "In this paper, we explored how online hate is motivated by receiving social\napproval from others. We specifically examined two central tenets of Walther's\n(2024) social approval theory of online hate: (H1a) more signals of social\napproval on hate messages predicts more subsequent hate messages, and (H1b) as\nsocial approval increases, hate speech messages become more extreme. Using over\n110 million posts from Parler (2018-2021), we observed that the number of\nupvotes a person received on a hate speech post was unassociated with the\namount of hate speech in their next post and posts during the next week, month,\nthree months, and six months. Between-person effects revealed an average\nnegative relationship between social approval and hate speech production at the\npost level, but this relationship was mixed at other time intervals. Social\napproval reinforcement mechanisms of online hate may operate differently on\nniche social media platforms.", "AI": {"tldr": "The study investigates whether social approval (upvotes) on hate speech posts leads to more or more extreme hate speech. Analyzing 110M Parler posts, it found no consistent positive relationship, suggesting social approval may not reinforce online hate as expected.", "motivation": "To test Walther's (2024) social approval theory of online hate, specifically whether social approval (e.g., upvotes) motivates more or more extreme hate speech.", "method": "Analyzed over 110 million posts from Parler (2018-2021), examining the relationship between upvotes on hate speech posts and subsequent hate speech production at various time intervals.", "result": "No consistent positive relationship found; upvotes on hate speech posts did not predict more or more extreme hate speech. Between-person effects showed mixed or negative relationships.", "conclusion": "Social approval may not reinforce online hate as theorized, especially on niche platforms like Parler."}}
{"id": "2507.10846", "pdf": "https://arxiv.org/pdf/2507.10846", "abs": "https://arxiv.org/abs/2507.10846", "authors": ["Casey Wall", "Longwei Wang", "Rodrigue Rizk", "KC Santosh"], "title": "Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 10 figures, 7 tables. Submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "summary": "Interpreting the decision-making process of Convolutional Neural Networks\n(CNNs) is critical for deploying models in high-stakes domains.\nGradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method\nfor visual explanations, yet it typically focuses on the final convolutional\nlayer or na\\\"ively averages across layers, strategies that can obscure\nimportant semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a\nnovel, human-tunable extension of Grad-CAM that generates robust and coherent\nsaliency maps by aggregating information across all convolutional layers. To\nmitigate the influence of noisy or extreme attribution values, Winsor-CAM\napplies Winsorization, a percentile-based outlier attenuation technique. A\nuser-controllable threshold allows for semantic-level tuning, enabling flexible\nexploration of model behavior across representational hierarchies. Evaluations\non standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the\nPASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable\nheatmaps and achieves superior performance in localization metrics, including\nintersection-over-union and center-of-mass alignment, when compared to Grad-CAM\nand uniform layer-averaging baselines. Winsor-CAM advances the goal of\ntrustworthy AI by offering interpretable, multi-layer insights with\nhuman-in-the-loop control.", "AI": {"tldr": "Winsor-CAM improves Grad-CAM by aggregating info across all CNN layers using Winsorization, offering tunable, interpretable saliency maps.", "motivation": "Enhancing CNN interpretability for high-stakes applications by addressing Grad-CAM's limitations in layer aggregation and noise handling.", "method": "Winsor-CAM applies Winsorization to attenuate outliers and aggregates attributions across all convolutional layers with a user-tunable threshold.", "result": "Outperforms Grad-CAM in interpretability and localization metrics (e.g., IoU, center-of-mass alignment) on PASCAL VOC 2012.", "conclusion": "Winsor-CAM advances trustworthy AI with multi-layer insights and human-in-the-loop control."}}
{"id": "2507.10852", "pdf": "https://arxiv.org/pdf/2507.10852", "abs": "https://arxiv.org/abs/2507.10852", "authors": ["Yiran Hu", "Zongyue Xue", "Haitao Li", "Siyuan Zheng", "Qingjing Chen", "Shaochun Wang", "Xihan Zhang", "Ning Zheng", "Yun Liu", "Qingyao Ai", "Yiqun Liu", "Charles L. A. Clarke", "Weixing Shen"], "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness.", "AI": {"tldr": "The paper investigates judicial fairness in LLMs, revealing pervasive inconsistency, bias, and inaccuracy, with notable demographic biases. It introduces a framework, dataset (JudiFair), metrics, and a toolkit for future research.", "motivation": "To address the underexplored judicial fairness of LLMs in high-stakes fields, ensuring their trustworthiness in judicial applications.", "method": "Develops a framework with 65 labels and 161 values, compiles JudiFair dataset (177,100 cases), and introduces three metrics (inconsistency, bias, imbalanced inaccuracy) to evaluate 16 LLMs.", "result": "LLMs exhibit severe judicial unfairness, with pronounced demographic biases. Adjusting temperature affects fairness, but model size, release date, and origin do not.", "conclusion": "The study highlights LLM fairness issues and provides tools for future research to evaluate and improve fairness."}}
{"id": "2507.10855", "pdf": "https://arxiv.org/pdf/2507.10855", "abs": "https://arxiv.org/abs/2507.10855", "authors": ["Wei Chen", "Jingxi Yu", "Zichen Miao", "Qiang Qiu"], "title": "Sparse Fine-Tuning of Transformers for Generative Tasks", "categories": ["cs.CV"], "comment": "Accepted by International Conference on Computer Vision 2025", "summary": "Large pre-trained transformers have revolutionized artificial intelligence\nacross various domains, and fine-tuning remains the dominant approach for\nadapting these models to downstream tasks due to the cost of training from\nscratch. However, in existing fine-tuning methods, the updated representations\nare formed as a dense combination of modified parameters, making it challenging\nto interpret their contributions and understand how the model adapts to new\ntasks. In this work, we introduce a fine-tuning framework inspired by sparse\ncoding, where fine-tuned features are represented as a sparse combination of\nbasic elements, i.e., feature dictionary atoms. The feature dictionary atoms\nfunction as fundamental building blocks of the representation, and tuning atoms\nallows for seamless adaptation to downstream tasks. Sparse coefficients then\nserve as indicators of atom importance, identifying the contribution of each\natom to the updated representation. Leveraging the atom selection capability of\nsparse coefficients, we first demonstrate that our method enhances image\nediting performance by improving text alignment through the removal of\nunimportant feature dictionary atoms. Additionally, we validate the\neffectiveness of our approach in the text-to-image concept customization task,\nwhere our method efficiently constructs the target concept using a sparse\ncombination of feature dictionary atoms, outperforming various baseline\nfine-tuning methods.", "AI": {"tldr": "A sparse coding-inspired fine-tuning framework for transformers improves interpretability and performance in downstream tasks like image editing and concept customization.", "motivation": "Existing fine-tuning methods lack interpretability in how models adapt to new tasks due to dense parameter updates.", "method": "Introduces a sparse coding-based framework where fine-tuned features are sparse combinations of feature dictionary atoms, with coefficients indicating atom importance.", "result": "Enhances image editing by improving text alignment and outperforms baselines in text-to-image concept customization.", "conclusion": "The sparse fine-tuning framework offers interpretable and efficient adaptation of pre-trained models to downstream tasks."}}
{"id": "2507.10918", "pdf": "https://arxiv.org/pdf/2507.10918", "abs": "https://arxiv.org/abs/2507.10918", "authors": ["Ikumi Numaya", "Shoji Moriya", "Shiki Sato", "Reina Akama", "Jun Suzuki"], "title": "How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations", "categories": ["cs.CL"], "comment": "Accepted to SIGDIAL 2025 (long)", "summary": "Recent advancements in dialogue generation have broadened the scope of\nhuman-bot interactions, enabling not only contextually appropriate responses\nbut also the analysis of human affect and sensitivity. While prior work has\nsuggested that stylistic similarity between user and system may enhance user\nimpressions, the distinction between subjective and objective similarity is\noften overlooked. To investigate this issue, we introduce a novel dataset that\nincludes users' preferences, subjective stylistic similarity based on users'\nown perceptions, and objective stylistic similarity annotated by third party\nevaluators in open-domain dialogue settings. Analysis using the constructed\ndataset reveals a strong positive correlation between subjective stylistic\nsimilarity and user preference. Furthermore, our analysis suggests an important\nfinding: users' subjective stylistic similarity differs from third party\nobjective similarity. This underscores the importance of distinguishing between\nsubjective and objective evaluations and understanding the distinct aspects\neach captures when analyzing the relationship between stylistic similarity and\nuser preferences. The dataset presented in this paper is available online.", "AI": {"tldr": "The paper explores the impact of stylistic similarity in dialogue systems, distinguishing between subjective (user-perceived) and objective (third-party annotated) similarity, and finds a strong correlation between subjective similarity and user preference.", "motivation": "Prior work overlooks the distinction between subjective and objective stylistic similarity in dialogue systems, which may affect user impressions.", "method": "A novel dataset is introduced, capturing user preferences, subjective stylistic similarity, and objective stylistic similarity in open-domain dialogues.", "result": "Subjective stylistic similarity strongly correlates with user preference, and it differs from objective similarity.", "conclusion": "The study highlights the need to distinguish between subjective and objective evaluations in analyzing stylistic similarity's impact on user preferences."}}
{"id": "2507.10864", "pdf": "https://arxiv.org/pdf/2507.10864", "abs": "https://arxiv.org/abs/2507.10864", "authors": ["Saadat Behzadi", "Danial Sharifrazi", "Bita Mesbahzadeh", "Javad Hassannataj Joloudarid", "Roohallah Alizadehsani"], "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Objectives: Timely and accurate detection of colorectal polyps plays a\ncrucial role in diagnosing and preventing colorectal cancer, a major cause of\nmortality worldwide. This study introduces a new, lightweight, and efficient\nframework for polyp detection that combines the Local Outlier Factor (LOF)\nalgorithm for filtering noisy data with the YOLO-v11n deep learning model.\n  Study design: An experimental study leveraging deep learning and outlier\nremoval techniques across multiple public datasets.\n  Methods: The proposed approach was tested on five diverse and publicly\navailable datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.\nSince these datasets originally lacked bounding box annotations, we converted\ntheir segmentation masks into suitable detection labels. To enhance the\nrobustness and generalizability of our model, we apply 5-fold cross-validation\nand remove anomalous samples using the LOF method configured with 30 neighbors\nand a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a\nfast and resource-efficient object detection architecture optimized for\nreal-time applications. We train the model using a combination of modern\naugmentation strategies to improve detection accuracy under diverse conditions.\n  Results: Our approach significantly improves polyp localization performance,\nachieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5\nof 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,\nour model demonstrates enhanced accuracy and efficiency.\n  Conclusions: These results suggest that the proposed method is well-suited\nfor real-time colonoscopy support in clinical settings. Overall, the study\nunderscores how crucial data preprocessing and model efficiency are when\ndesigning effective AI systems for medical imaging.", "AI": {"tldr": "A lightweight framework combining LOF for noise filtering and YOLO-v11n for polyp detection achieves high accuracy and efficiency in real-time colonoscopy support.", "motivation": "Improving polyp detection accuracy to aid in colorectal cancer diagnosis and prevention.", "method": "Uses LOF for outlier removal and YOLO-v11n for detection, tested on five datasets with 5-fold cross-validation and data augmentation.", "result": "Achieves precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5 of 96.48%, and mAP@0.5:0.95 of 77.75%.", "conclusion": "The method is effective for real-time clinical use, highlighting the importance of data preprocessing and model efficiency."}}
{"id": "2507.10920", "pdf": "https://arxiv.org/pdf/2507.10920", "abs": "https://arxiv.org/abs/2507.10920", "authors": ["Seungho Choi"], "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often show poor performance in low-resource\nlanguages like Korean, partly due to unique linguistic challenges such as\nhomophonous Sino-Korean words that are indistinguishable in Hangul script. To\naddress this semantic ambiguity, we propose HanjaBridge, a novel\nmeaning-injection technique integrated into a continual pre-training (CPT)\nframework. Instead of deterministically mapping a word to a single Hanja\n(Chinese character), HanjaBridge presents the model with all possible Hanja\ncandidates for a given homograph, encouraging the model to learn contextual\ndisambiguation. This process is paired with token-level knowledge distillation\nto prevent catastrophic forgetting. Experimental results show that HanjaBridge\nsignificantly improves Korean language understanding, achieving a 21\\% relative\nimprovement on the KoBALT benchmark. Notably, by reinforcing semantic alignment\nbetween Korean and Chinese through shared Hanja, we observe a strong positive\ncross-lingual transfer. Furthermore, these gains persist even when Hanja\naugmentation is omitted at inference time, ensuring practical efficiency with\nno additional run-time cost.", "AI": {"tldr": "HanjaBridge improves Korean LLM performance by addressing semantic ambiguity through Hanja-based meaning injection and continual pre-training, achieving a 21% boost on KoBALT.", "motivation": "LLMs underperform in low-resource languages like Korean due to linguistic challenges like homophonous Sino-Korean words.", "method": "Proposes HanjaBridge, injecting all possible Hanja candidates for homographs during continual pre-training, paired with token-level knowledge distillation.", "result": "21% relative improvement on KoBALT; strong cross-lingual transfer between Korean and Chinese.", "conclusion": "HanjaBridge enhances Korean understanding without runtime cost, maintaining gains even without Hanja at inference."}}
{"id": "2507.10881", "pdf": "https://arxiv.org/pdf/2507.10881", "abs": "https://arxiv.org/abs/2507.10881", "authors": ["Roman Naeem", "David Hagerman", "Jennifer Alv\u00e9n", "Lennart Svensson", "Fredrik Kahl"], "title": "Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes", "categories": ["cs.CV"], "comment": "Submitted Version. Accepted at MICCAI 2025", "summary": "Tubular tree structures, such as blood vessels and airways, are essential in\nhuman anatomy and accurately tracking them while preserving their topology is\ncrucial for various downstream tasks. Trexplorer is a recurrent model designed\nfor centerline tracking in 3D medical images but it struggles with predicting\nduplicate branches and terminating tracking prematurely. To address these\nissues, we present Trexplorer Super, an enhanced version that notably improves\nperformance through novel advancements. However, evaluating centerline tracking\nmodels is challenging due to the lack of public datasets. To enable thorough\nevaluation, we develop three centerline datasets, one synthetic and two real,\neach with increasing difficulty. Using these datasets, we conduct a\ncomprehensive evaluation of existing state-of-the-art (SOTA) models and compare\nthem with our approach. Trexplorer Super outperforms previous SOTA models on\nevery dataset. Our results also highlight that strong performance on synthetic\ndata does not necessarily translate to real datasets. The code and datasets are\navailable at https://github.com/RomStriker/Trexplorer-Super.", "AI": {"tldr": "Trexplorer Super improves upon Trexplorer for 3D centerline tracking in medical images, addressing duplicate branches and premature termination. It outperforms SOTA models on new synthetic and real datasets.", "motivation": "Accurate tracking of tubular tree structures (e.g., blood vessels) is vital for medical tasks, but existing models like Trexplorer have limitations.", "method": "Trexplorer Super introduces novel advancements to enhance performance. Three datasets (one synthetic, two real) are developed for evaluation.", "result": "Trexplorer Super outperforms SOTA models on all datasets, though synthetic data performance doesn't always generalize to real data.", "conclusion": "Trexplorer Super advances centerline tracking, with datasets enabling robust evaluation. The code and datasets are publicly available."}}
{"id": "2507.10957", "pdf": "https://arxiv.org/pdf/2507.10957", "abs": "https://arxiv.org/abs/2507.10957", "authors": ["Kalit Inani", "Keshav Kabra", "Vijay Marupudi", "Sashank Varma"], "title": "Modeling Understanding of Story-Based Analogies Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "To appear at CogSci 2025", "summary": "Recent advancements in Large Language Models (LLMs) have brought them closer\nto matching human cognition across a variety of tasks. How well do these models\nalign with human performance in detecting and mapping analogies? Prior research\nhas shown that LLMs can extract similarities from analogy problems but lack\nrobust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the\ncurrent study focused on a story-based analogical mapping task and conducted a\nfine-grained evaluation of LLM reasoning abilities compared to human\nperformance. First, it explored the semantic representation of analogies in\nLLMs, using sentence embeddings to assess whether they capture the similarity\nbetween the source and target texts of an analogy, and the dissimilarity\nbetween the source and distractor texts. Second, it investigated the\neffectiveness of explicitly prompting LLMs to explain analogies. Throughout, we\nexamine whether LLMs exhibit similar performance profiles to those observed in\nhumans by evaluating their reasoning at the level of individual analogies, and\nnot just at the level of overall accuracy (as prior studies have done). Our\nexperiments include evaluating the impact of model size (8B vs. 70B parameters)\nand performance variation across state-of-the-art model architectures such as\nGPT-4 and LLaMA3. This work advances our understanding of the analogical\nreasoning abilities of LLMs and their potential as models of human reasoning.", "AI": {"tldr": "The study evaluates LLMs' analogical reasoning abilities compared to humans, focusing on semantic representation and prompting for explanations, while assessing model size and architecture impacts.", "motivation": "To determine how well LLMs align with human performance in detecting and mapping analogies, addressing gaps in prior research.", "method": "Used a story-based analogical mapping task, analyzed semantic representations via sentence embeddings, and tested explicit prompting for explanations. Evaluated model size (8B vs. 70B) and architectures (GPT-4, LLaMA3).", "result": "Assessed LLMs' ability to capture analogy similarities and dissimilarities, and the impact of prompting. Examined performance profiles at individual analogy levels.", "conclusion": "Advances understanding of LLMs' analogical reasoning and their potential as models of human cognition."}}
{"id": "2507.10893", "pdf": "https://arxiv.org/pdf/2507.10893", "abs": "https://arxiv.org/abs/2507.10893", "authors": ["Minjong Cheon", "Eunhan Goo", "Su-Hyeon Shin", "Muhammad Ahmed", "Hyungjun Kim"], "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": "26pages, 9 Figures", "summary": "Recently, AI-based weather forecast models have achieved impressive advances.\nThese models have reached accuracy levels comparable to traditional NWP\nsystems, marking a significant milestone in data-driven weather prediction.\nHowever, they mostly leverage Transformer-based architectures, which often\nleads to high training complexity and resource demands due to the massive\nparameter sizes. In this study, we introduce a modernized CNN-based model for\nglobal weather forecasting that delivers competitive accuracy while\nsignificantly reducing computational requirements. To present a systematic\nmodernization roadmap, we highlight key architectural enhancements across\nmultiple design scales from an earlier CNN-based approach. KAI-a incorporates a\nscale-invariant architecture and InceptionNeXt-based blocks within a\ngeophysically-aware design, tailored to the structure of Earth system data.\nTrained on the ERA5 daily dataset with 67 atmospheric variables, the model\ncontains about 7 million parameters and completes training in just 12 hours on\na single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the\nperformance of state-of-the-art models in medium-range weather forecasting,\nwhile offering a significantly lightweight design. Furthermore, case studies on\nthe 2018 European heatwave and the East Asian summer monsoon demonstrate\nKAI-a's robust skill in capturing extreme events, reinforcing its practical\nutility.", "AI": {"tldr": "A lightweight CNN-based model (KAI-a) for global weather forecasting matches state-of-the-art accuracy while reducing computational demands.", "motivation": "AI-based weather models often use Transformer architectures, which are resource-intensive. This study aims to provide a more efficient alternative.", "method": "KAI-a uses a scale-invariant architecture with InceptionNeXt blocks, tailored for Earth system data, and is trained on ERA5 dataset with 7M parameters.", "result": "KAI-a achieves competitive accuracy in medium-range forecasting and excels in capturing extreme events like heatwaves and monsoons.", "conclusion": "KAI-a offers a practical, resource-efficient solution for accurate weather forecasting, especially for extreme events."}}
{"id": "2507.10958", "pdf": "https://arxiv.org/pdf/2507.10958", "abs": "https://arxiv.org/abs/2507.10958", "authors": ["Anthony Miyaguchi", "David Guecha", "Yuwen Chiu", "Sidharth Gaur"], "title": "DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models", "categories": ["cs.CL"], "comment": null, "summary": "This Working Note summarizes the participation of the DS@GT team in two eRisk\n2025 challenges. For the Pilot Task on conversational depression detection with\nlarge language-models (LLMs), we adopted a prompt-engineering strategy in which\ndiverse LLMs conducted BDI-II-based assessments and produced structured JSON\noutputs. Because ground-truth labels were unavailable, we evaluated cross-model\nagreement and internal consistency. Our prompt design methodology aligned model\noutputs with BDI-II criteria and enabled the analysis of conversational cues\nthat influenced the prediction of symptoms. Our best submission, second on the\nofficial leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.", "AI": {"tldr": "The DS@GT team participated in eRisk 2025 challenges, focusing on conversational depression detection using LLMs with prompt-engineering and BDI-II assessments. Their best submission ranked second.", "motivation": "To explore the use of large language models (LLMs) for detecting depression in conversational contexts, leveraging BDI-II criteria.", "method": "Adopted a prompt-engineering strategy with diverse LLMs to generate structured JSON outputs and evaluated cross-model agreement and internal consistency.", "result": "Achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27, ranking second on the leaderboard.", "conclusion": "The prompt design effectively aligned LLM outputs with BDI-II criteria, enabling analysis of conversational cues for symptom prediction."}}
{"id": "2507.10895", "pdf": "https://arxiv.org/pdf/2507.10895", "abs": "https://arxiv.org/abs/2507.10895", "authors": ["Xiaocong Zeng", "Craig Michoski", "Yan Pang", "Dongyang Kuang"], "title": "Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "comment": null, "summary": "In this work, we address the often-overlooked issue of Timescale Dependent\nLabel Inconsistency (TsDLI) in training neural network models for EEG-based\nhuman emotion recognition. To mitigate TsDLI and enhance model generalization\nand explainability, we propose two novel regularization strategies: Local\nVariation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods\nincorporate classical mathematical principles--specifically, functions of\nbounded variation and commute-time distances--within a graph theoretic\nframework. Complementing our regularizers, we introduce a suite of new\nevaluation metrics that better capture the alignment between temporally local\npredictions and their associated global emotion labels. We validate our\napproach through comprehensive experiments on two widely used EEG emotion\ndatasets, DREAMER and DEAP, across a range of neural architectures including\nLSTM and transformer-based models. Performance is assessed using five distinct\nmetrics encompassing both quantitative accuracy and qualitative consistency.\nResults consistently show that our proposed methods outperform state-of-the-art\nbaselines, delivering superior aggregate performance and offering a principled\ntrade-off between interpretability and predictive power under label\ninconsistency. Notably, LVL achieves the best aggregate rank across all\nbenchmarked backbones and metrics, while LGCL frequently ranks the second,\nhighlighting the effectiveness of our framework.", "AI": {"tldr": "The paper addresses Timescale Dependent Label Inconsistency (TsDLI) in EEG-based emotion recognition, proposing two regularization strategies (LVL and LGCL) to improve model generalization and explainability. The methods outperform baselines on DREAMER and DEAP datasets.", "motivation": "To mitigate TsDLI and enhance model generalization and explainability in EEG-based emotion recognition.", "method": "Proposes Local Variation Loss (LVL) and Local-Global Consistency Loss (LGCL), incorporating bounded variation and commute-time distances in a graph theoretic framework. Introduces new evaluation metrics for temporal alignment.", "result": "LVL and LGCL outperform state-of-the-art baselines on DREAMER and DEAP datasets, with LVL achieving the best aggregate rank.", "conclusion": "The proposed methods offer a principled trade-off between interpretability and predictive power, effectively addressing TsDLI."}}
{"id": "2507.10972", "pdf": "https://arxiv.org/pdf/2507.10972", "abs": "https://arxiv.org/abs/2507.10972", "authors": ["Zhaoyi An", "Rei Kawakami"], "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": "Accepted by IEEE ICIP 2025", "summary": "Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language.", "AI": {"tldr": "TEAM-Sign fine-tunes an LLM to treat sign language as a natural language, using stepwise prompting to bridge the gap between sign and spoken language, showing effectiveness on benchmark datasets.", "motivation": "Sign language generation is complex and underutilized by LLMs despite their capabilities, prompting the need for a tailored approach.", "method": "Fine-tuning an LLM with stepwise prompting to align sign and spoken language distributions and rules.", "result": "Effective alignment of sign and spoken language distributions and rules on How2Sign and Phoenix14T datasets.", "conclusion": "TEAM-Sign successfully leverages LLMs for sign language generation by treating it as a natural language and using tailored prompting."}}
{"id": "2507.10935", "pdf": "https://arxiv.org/pdf/2507.10935", "abs": "https://arxiv.org/abs/2507.10935", "authors": ["Shaowen Tong", "Zimin Xia", "Alexandre Alahi", "Xuming He", "Yujiao Shi"], "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization", "categories": ["cs.CV"], "comment": "accepted by ICCV2025", "summary": "Cross-view localization, the task of estimating a camera's\n3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with\nsatellite images, is crucial for large-scale outdoor applications like\nautonomous navigation and augmented reality. Existing methods often rely on\nfully supervised learning, which requires costly ground-truth pose annotations.\nIn this work, we propose GeoDistill, a Geometry guided weakly supervised self\ndistillation framework that uses teacher-student learning with Field-of-View\n(FoV)-based masking to enhance local feature learning for robust cross-view\nlocalization. In GeoDistill, the teacher model localizes a panoramic image,\nwhile the student model predicts locations from a limited FoV counterpart\ncreated by FoV-based masking. By aligning the student's predictions with those\nof the teacher, the student focuses on key features like lane lines and ignores\ntextureless regions, such as roads. This results in more accurate predictions\nand reduced uncertainty, regardless of whether the query images are panoramas\nor limited FoV images. Our experiments show that GeoDistill significantly\nimproves localization performance across different frameworks. Additionally, we\nintroduce a novel orientation estimation network that predicts relative\norientation without requiring precise planar position ground truth. GeoDistill\nprovides a scalable and efficient solution for real-world cross-view\nlocalization challenges. Code and model can be found at\nhttps://github.com/tongshw/GeoDistill.", "AI": {"tldr": "GeoDistill is a weakly supervised framework for cross-view localization using teacher-student learning with FoV-based masking, improving accuracy and reducing uncertainty.", "motivation": "Existing methods rely on costly ground-truth pose annotations; GeoDistill aims to reduce this dependency by using weakly supervised learning.", "method": "Uses teacher-student learning with FoV-based masking to align predictions, focusing on key features like lane lines.", "result": "Significantly improves localization performance and introduces a novel orientation estimation network.", "conclusion": "GeoDistill offers a scalable and efficient solution for real-world cross-view localization challenges."}}
{"id": "2507.10996", "pdf": "https://arxiv.org/pdf/2507.10996", "abs": "https://arxiv.org/abs/2507.10996", "authors": ["Lin Tian", "Johanne R. Trippas", "Marian-Andrei Rizoiu"], "title": "Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection", "categories": ["cs.CL"], "comment": "12 pages, 5 tables, CLEF 2025", "summary": "This paper presents our approach to EXIST 2025 Task 1, addressing text-based\nsexism detection in English and Spanish tweets through hierarchical Low-Rank\nAdaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter\nrouting that explicitly models label dependencies across three hierarchically\nstructured subtasks: binary sexism identification, source intention detection,\nand multilabel sexism categorization. Unlike conventional LoRA applications\nthat target only attention layers, we apply adaptation to all linear\ntransformations, enhancing the model's capacity to capture task-specific\npatterns. In contrast to complex data processing and ensemble approaches, we\nshow that straightforward parameter-efficient fine-tuning achieves strong\nperformance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each\nsubtask using unified multilingual training that leverages Llama 3.1's native\nbilingual capabilities. The method requires minimal preprocessing and uses\nstandard supervised learning. Our multilingual training strategy eliminates the\nneed for separate language-specific models, achieving 1.7-2.4\\% F1 improvements\nthrough cross-lingual transfer. With only 1.67\\% trainable parameters compared\nto full fine-tuning, our approach reduces training time by 75\\% and model\nstorage by 98\\%, while achieving competitive performance across all subtasks\n(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,\n0.6519 for multilabel categorization).", "AI": {"tldr": "The paper introduces a hierarchical LoRA adaptation method for Llama 3.1 8B to detect text-based sexism in English and Spanish tweets, achieving strong performance with minimal preprocessing and parameter-efficient fine-tuning.", "motivation": "To address sexism detection in multilingual tweets efficiently, leveraging hierarchical subtasks and parameter-efficient fine-tuning to reduce computational costs.", "method": "Hierarchical LoRA adaptation applied to all linear transformations, with conditional adapter routing for label dependencies across subtasks (binary sexism, intention detection, multilabel categorization). Uses QLoRA 4-bit and multilingual training.", "result": "Achieves competitive performance (ICM-Hard: 0.6774 binary, 0.4991 intention, 0.6519 multilabel) with 1.67% trainable parameters, 75% faster training, and 98% storage reduction.", "conclusion": "The method demonstrates efficient and effective sexism detection, outperforming complex approaches while minimizing resource usage."}}
{"id": "2507.10938", "pdf": "https://arxiv.org/pdf/2507.10938", "abs": "https://arxiv.org/abs/2507.10938", "authors": ["Zhengyi Xu", "Haoran Wu", "Wen Jiang", "Jie Geng"], "title": "Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Semantic change detection (SCD) extends the binary change detection task to\nprovide not only the change locations but also the detailed \"from-to\"\ncategories in multi-temporal remote sensing data. Such detailed semantic\ninsights into changes offer considerable advantages for a wide array of\napplications. However, since SCD involves the simultaneous optimization of\nmultiple tasks, the model is prone to negative transfer due to task-specific\nlearning difficulties and conflicting gradient flows. To address this issue, we\npropose Graph Aggregation Prototype Learning for Semantic Change Detection in\nremote sensing(GAPL-SCD). In this framework, a multi-task joint optimization\nmethod is designed to optimize the primary task of semantic segmentation and\nchange detection, along with the auxiliary task of graph aggregation prototype\nlearning. Adaptive weight allocation and gradient rotation methods are used to\nalleviate the conflict between training tasks and improve multi-task learning\ncapabilities. Specifically, the graph aggregation prototype learning module\nconstructs an interaction graph using high-level features. Prototypes serve as\nclass proxies, enabling category-level domain alignment across time points and\nreducing interference from irrelevant changes. Additionally, the proposed\nself-query multi-level feature interaction and bi-temporal feature fusion\nmodules further enhance multi-scale feature representation, improving\nperformance in complex scenes. Experimental results on the SECOND and\nLandsat-SCD datasets demonstrate that our method achieves state-of-the-art\nperformance, with significant improvements in accuracy and robustness for SCD\ntask.", "AI": {"tldr": "The paper proposes GAPL-SCD, a framework for semantic change detection in remote sensing, addressing multi-task optimization challenges with adaptive weight allocation and gradient rotation.", "motivation": "Semantic change detection (SCD) provides detailed 'from-to' category insights but faces challenges like negative transfer due to conflicting tasks.", "method": "GAPL-SCD uses multi-task joint optimization, graph aggregation prototype learning, and adaptive weight allocation to improve performance.", "result": "The method achieves state-of-the-art performance on SECOND and Landsat-SCD datasets, enhancing accuracy and robustness.", "conclusion": "GAPL-SCD effectively addresses SCD challenges, offering improved multi-task learning and feature representation for complex scenes."}}
{"id": "2507.11004", "pdf": "https://arxiv.org/pdf/2507.11004", "abs": "https://arxiv.org/abs/2507.11004", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "title": "Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification", "categories": ["cs.CL"], "comment": "ACL 2025 Workshop (FEVER)", "summary": "This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task\nat the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the\nbest-performing open-source model from the previous year's challenge. It\nimproves evidence quality through document summarization and answer\nreformulation, optimizes veracity prediction via post-training quantization\nunder computational constraints, and enhances overall system performance by\nintegrating updated language model (LM) backbones. HerO 2 ranked second on the\nleaderboard while achieving the shortest runtime among the top three systems,\ndemonstrating both high efficiency and strong potential for real-world fact\nverification. The code is available at https://github.com/ssu-humane/HerO2.", "AI": {"tldr": "HerO 2 is an improved system for fact verification, ranking second in AVeriTeC with optimized efficiency and performance.", "motivation": "To enhance evidence quality, veracity prediction, and system performance for real-world fact verification.", "method": "Uses document summarization, answer reformulation, post-training quantization, and updated LM backbones.", "result": "Ranked second on the leaderboard with the shortest runtime among top systems.", "conclusion": "HerO 2 demonstrates high efficiency and strong potential for practical fact verification."}}
{"id": "2507.10943", "pdf": "https://arxiv.org/pdf/2507.10943", "abs": "https://arxiv.org/abs/2507.10943", "authors": ["Yushun Fang", "Lu Liu", "Xiang Gao", "Qiang Hu", "Ning Cao", "Jianghe Cui", "Gang Chen", "Xiaoyun Zhang"], "title": "Robust ID-Specific Face Restoration via Alignment Learning", "categories": ["cs.CV"], "comment": "17 pages, 8 figures", "summary": "The latest developments in Face Restoration have yielded significant\nadvancements in visual quality through the utilization of diverse diffusion\npriors. Nevertheless, the uncertainty of face identity introduced by\nidentity-obscure inputs and stochastic generative processes remains unresolved.\nTo address this challenge, we present Robust ID-Specific Face Restoration\n(RIDFR), a novel ID-specific face restoration framework based on diffusion\nmodels. Specifically, RIDFR leverages a pre-trained diffusion model in\nconjunction with two parallel conditioning modules. The Content Injection\nModule inputs the severely degraded image, while the Identity Injection Module\nintegrates the specific identity from a given image. Subsequently, RIDFR\nincorporates Alignment Learning, which aligns the restoration results from\nmultiple references with the same identity in order to suppress the\ninterference of ID-irrelevant face semantics (e.g. pose, expression, make-up,\nhair style). Experiments demonstrate that our framework outperforms the\nstate-of-the-art methods, reconstructing high-quality ID-specific results with\nhigh identity fidelity and demonstrating strong robustness.", "AI": {"tldr": "RIDFR is a novel ID-specific face restoration framework using diffusion models to address identity uncertainty, outperforming state-of-the-art methods.", "motivation": "Current face restoration methods struggle with identity uncertainty due to obscure inputs and stochastic processes.", "method": "RIDFR uses a pre-trained diffusion model with two parallel conditioning modules (Content and Identity Injection) and Alignment Learning to align results.", "result": "RIDFR reconstructs high-quality ID-specific results with high fidelity and robustness, surpassing existing methods.", "conclusion": "RIDFR effectively resolves identity uncertainty in face restoration, achieving superior performance."}}
{"id": "2507.11049", "pdf": "https://arxiv.org/pdf/2507.11049", "abs": "https://arxiv.org/abs/2507.11049", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "categories": ["cs.CL"], "comment": "Preprint. 24 pages", "summary": "As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias.", "AI": {"tldr": "The paper introduces K-News-Stance, a Korean dataset for article-level stance detection, and JoA-ICL, a framework for stance detection in long-form news, outperforming existing methods.", "motivation": "To address gaps in stance detection research, which is limited to short texts and high-resource languages, and to mitigate filter bubbles and polarization in news recommendations.", "method": "Proposes JoA-ICL, a journalism-guided framework using a language model agent to predict stances of key segments, aggregated for overall article stance.", "result": "JoA-ICL outperforms existing methods, demonstrating effectiveness in capturing long-form article stances and promoting viewpoint diversity.", "conclusion": "The work advances stance detection for long-form content and supports diverse news recommendations and media bias analysis."}}
{"id": "2507.10969", "pdf": "https://arxiv.org/pdf/2507.10969", "abs": "https://arxiv.org/abs/2507.10969", "authors": ["Palash Ray", "Mahuya Sasmal", "Asish Bera"], "title": "Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data", "categories": ["cs.CV"], "comment": null, "summary": "Sports action classification representing complex body postures and\nplayer-object interactions is an emerging area in image-based sports analysis.\nSome works have contributed to automated sports action recognition using\nmachine learning techniques over the past decades. However, sufficient image\ndatasets representing women sports actions with enough intra- and inter-class\nvariations are not available to the researchers. To overcome this limitation,\nthis work presents a new dataset named WomenSports for women sports\nclassification using small-scale training data. This dataset includes a variety\nof sports activities, covering wide variations in movements, environments, and\ninteractions among players. In addition, this study proposes a convolutional\nneural network (CNN) for deep feature extraction. A channel attention scheme\nupon local contextual regions is applied to refine and enhance feature\nrepresentation. The experiments are carried out on three different sports\ndatasets and one dance dataset for generalizing the proposed algorithm, and the\nperformances on these datasets are noteworthy. The deep learning method\nachieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed\nWomenSports dataset, which is publicly available for research at Mendeley Data.", "AI": {"tldr": "The paper introduces a new dataset, WomenSports, for classifying women's sports actions using small-scale training data and proposes a CNN with channel attention for improved feature extraction, achieving 89.15% accuracy.", "motivation": "Existing datasets lack sufficient representation of women's sports actions with intra- and inter-class variations, limiting research in automated sports action recognition.", "method": "A new dataset (WomenSports) is created, and a CNN with channel attention is proposed for deep feature extraction.", "result": "The method achieves 89.15% top-1 classification accuracy on the WomenSports dataset using ResNet-50.", "conclusion": "The WomenSports dataset and proposed CNN with channel attention effectively address the gap in women's sports action classification, achieving high accuracy."}}
{"id": "2507.11052", "pdf": "https://arxiv.org/pdf/2507.11052", "abs": "https://arxiv.org/abs/2507.11052", "authors": ["Haowei Yang", "Ziyu Shen", "Junli Shao", "Luyao Men", "Xinyue Han", "Jing Dong"], "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments.", "AI": {"tldr": "A novel LLM-augmented NLP pipeline improves CVD risk prediction by extracting and analyzing unstructured clinical notes, outperforming traditional methods.", "motivation": "Timely and accurate CVD risk stratification is crucial for reducing mortality. Unstructured clinical notes contain early indicators not captured by structured data.", "method": "The study uses domain-adapted LLMs for symptom extraction, contextual reasoning, and correlation from free-text reports, with cardiovascular-specific fine-tuning and prompt-based inference.", "result": "Evaluations show improved precision, recall, F1-score, and AUROC, with high clinical relevance (kappa = 0.82). Challenges like contextual hallucination and temporal ambiguity are mitigated.", "conclusion": "The approach highlights LLMs' potential in clinical decision support, enhancing early warning systems and translating patient narratives into actionable risk assessments."}}
{"id": "2507.10977", "pdf": "https://arxiv.org/pdf/2507.10977", "abs": "https://arxiv.org/abs/2507.10977", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "Human-object interaction (HOI) detection is essential for accurately\nlocalizing and characterizing interactions between humans and objects,\nproviding a comprehensive understanding of complex visual scenes across various\ndomains. However, existing HOI detectors often struggle to deliver reliable\npredictions efficiently, relying on resource-intensive training methods and\ninefficient architectures. To address these challenges, we conceptualize a\nwavelet attention-like backbone and a novel ray-based encoder architecture\ntailored for HOI detection. Our wavelet backbone addresses the limitations of\nexpressing middle-order interactions by aggregating discriminative features\nfrom the low- and high-order interactions extracted from diverse convolutional\nfilters. Concurrently, the ray-based encoder facilitates multi-scale attention\nby optimizing the focus of the decoder on relevant regions of interest and\nmitigating computational overhead. As a result of harnessing the attenuated\nintensity of learnable ray origins, our decoder aligns query embeddings with\nemphasized regions of interest for accurate predictions. Experimental results\non benchmark datasets, including ImageNet and HICO-DET, showcase the potential\nof our proposed architecture. The code is publicly available at\n[https://github.com/henry-pay/RayEncoder].", "AI": {"tldr": "A novel wavelet attention-like backbone and ray-based encoder architecture for efficient and accurate human-object interaction (HOI) detection.", "motivation": "Existing HOI detectors are inefficient and resource-intensive, limiting reliable predictions.", "method": "Proposes a wavelet backbone for feature aggregation and a ray-based encoder for multi-scale attention.", "result": "Improved performance on benchmark datasets like ImageNet and HICO-DET.", "conclusion": "The architecture effectively addresses efficiency and accuracy challenges in HOI detection."}}
{"id": "2507.11084", "pdf": "https://arxiv.org/pdf/2507.11084", "abs": "https://arxiv.org/abs/2507.11084", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha"], "title": "Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach", "categories": ["cs.CL"], "comment": "This paper has been accepted and presented at the IEEE ECAI 2025. The\n  final version will be available in the IEEE Xplore Digital Library", "summary": "The July Revolution in Bangladesh marked a significant student-led mass\nuprising, uniting people across the nation to demand justice, accountability,\nand systemic reform. Social media platforms played a pivotal role in amplifying\npublic sentiment and shaping discourse during this historic mass uprising. In\nthis study, we present a hybrid transformer-based sentiment analysis framework\nto decode public opinion expressed in social media comments during and after\nthe revolution. We used a brand new dataset of 4,200 Bangla comments collected\nfrom social media. The framework employs advanced transformer-based feature\nextraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the\nproposed hybrid XMB-BERT, to capture nuanced patterns in textual data.\nPrinciple Component Analysis (PCA) were utilized for dimensionality reduction\nto enhance computational efficiency. We explored eleven traditional and\nadvanced machine learning classifiers for identifying sentiments. The proposed\nhybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of\n83.7% and outperform other model classifier combinations. This study\nunderscores the potential of machine learning techniques to analyze social\nsentiment in low-resource languages like Bangla.", "AI": {"tldr": "A hybrid transformer-based sentiment analysis framework was developed to analyze Bangla social media comments during the July Revolution in Bangladesh, achieving 83.7% accuracy with the proposed XMB-BERT and voting classifier.", "motivation": "To decode public opinion on social media during the July Revolution in Bangladesh, addressing the need for sentiment analysis in low-resource languages like Bangla.", "method": "Used a dataset of 4,200 Bangla comments, employed transformer-based models (BanglaBERT, mBERT, XLM-RoBERTa, hybrid XMB-BERT), PCA for dimensionality reduction, and tested eleven classifiers.", "result": "The hybrid XMB-BERT with voting classifier achieved the highest accuracy of 83.7%, outperforming other models.", "conclusion": "Machine learning techniques are effective for sentiment analysis in low-resource languages, as demonstrated by the study's success with Bangla social media data."}}
{"id": "2507.10978", "pdf": "https://arxiv.org/pdf/2507.10978", "abs": "https://arxiv.org/abs/2507.10978", "authors": ["Ayush Gupta", "Siyuan Huang", "Rama Chellappa"], "title": "Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction", "categories": ["cs.CV"], "comment": "Accepted at IJCB 2025", "summary": "Gait is becoming popular as a method of person re-identification because of\nits ability to identify people at a distance. However, most current works in\ngait recognition do not address the practical problem of occlusions. Among\nthose which do, some require paired tuples of occluded and holistic sequences,\nwhich are impractical to collect in the real world. Further, these approaches\nwork on occlusions but fail to retain performance on holistic inputs. To\naddress these challenges, we propose RG-Gait, a method for residual correction\nfor occluded gait recognition with holistic retention. We model the problem as\na residual learning task, conceptualizing the occluded gait signature as a\nresidual deviation from the holistic gait representation. Our proposed network\nadaptively integrates the learned residual, significantly improving performance\non occluded gait sequences without compromising the holistic recognition\naccuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR\ndatasets and show that learning the residual can be an effective technique to\ntackle occluded gait recognition with holistic retention.", "AI": {"tldr": "RG-Gait proposes residual correction for occluded gait recognition, improving performance on occluded sequences without losing accuracy on holistic inputs.", "motivation": "Current gait recognition methods often ignore occlusions or require impractical paired data, and fail to retain performance on holistic inputs.", "method": "Models occluded gait as a residual deviation from holistic gait, using a network to adaptively integrate the learned residual.", "result": "Significantly improves occluded gait recognition on Gait3D, GREW, and BRIAR datasets while maintaining holistic accuracy.", "conclusion": "Residual learning is effective for occluded gait recognition with holistic retention."}}
{"id": "2507.11086", "pdf": "https://arxiv.org/pdf/2507.11086", "abs": "https://arxiv.org/abs/2507.11086", "authors": ["Andres Azqueta-Gavald\u00f3n", "Joaquin Ramos Cosgrove"], "title": "Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification", "categories": ["cs.CL"], "comment": null, "summary": "The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%).", "AI": {"tldr": "The paper explores using Large Language Models (LLMs) to improve entity-matching in cross-border financial activities, outperforming traditional methods with higher accuracy and lower false positives.", "motivation": "The need for accurate foreign entity classification in the Spanish financial system to enhance risk management and regulatory compliance, addressing challenges like linguistic variations and outdated names.", "method": "Evaluation of traditional methods (Jaccard, cosine, Levenshtein) and LLMs (Hugging Face, Microsoft Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.", "result": "Traditional methods achieve >92% accuracy but have high false positives (20-40%). Interface-based LLMs achieve >93% accuracy, >96% F1 scores, and lower false positives (40-80%).", "conclusion": "LLMs, especially interface-based ones, offer superior performance for entity-matching in financial systems, addressing limitations of traditional methods."}}
{"id": "2507.10999", "pdf": "https://arxiv.org/pdf/2507.10999", "abs": "https://arxiv.org/abs/2507.10999", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "The resurgence of convolutional neural networks (CNNs) in visual recognition\ntasks, exemplified by ConvNeXt, has demonstrated their capability to rival\ntransformer-based architectures through advanced training methodologies and\nViT-inspired design principles. However, both CNNs and transformers exhibit a\nsimplicity bias, favoring straightforward features over complex structural\nrepresentations. Furthermore, modern CNNs often integrate MLP-like blocks akin\nto those in transformers, but these blocks suffer from significant information\nredundancies, necessitating high expansion ratios to sustain competitive\nperformance. To address these limitations, we propose SpaRTAN, a lightweight\narchitectural design that enhances spatial and channel-wise information\nprocessing. SpaRTAN employs kernels with varying receptive fields, controlled\nby kernel size and dilation factor, to capture discriminative multi-order\nspatial features effectively. A wave-based channel aggregation module further\nmodulates and reinforces pixel interactions, mitigating channel-wise\nredundancies. Combining the two modules, the proposed network can efficiently\ngather and dynamically contextualize discriminative features. Experimental\nresults in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable\nparameter efficiency while maintaining competitive performance. In particular,\non the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M\nparameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver\nstrong performance through an efficient design. On the COCO benchmark, it\nachieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M\nparameters. The code is publicly available at\n[https://github.com/henry-pay/SpaRTAN].", "AI": {"tldr": "SpaRTAN is a lightweight CNN architecture designed to improve spatial and channel-wise information processing, achieving competitive performance with high efficiency.", "motivation": "Address limitations of CNNs and transformers, such as simplicity bias and information redundancies in MLP-like blocks.", "method": "Uses kernels with varying receptive fields and a wave-based channel aggregation module to capture multi-order spatial features and reduce redundancies.", "result": "Achieves 77.7% accuracy on ImageNet-1k with 3.8M parameters and 50.0% AP on COCO with 21.5M parameters.", "conclusion": "SpaRTAN offers an efficient and effective design for visual recognition tasks."}}
{"id": "2507.11097", "pdf": "https://arxiv.org/pdf/2507.11097", "abs": "https://arxiv.org/abs/2507.11097", "authors": ["Zichen Wen", "Jiashu Qu", "Dongrui Liu", "Zhiyuan Liu", "Ruixi Wu", "Yicun Yang", "Xiangqi Jin", "Haoyun Xu", "Xuyang Liu", "Weijia Li", "Chaochao Lu", "Jing Shao", "Conghui He", "Linfeng Zhang"], "title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs", "categories": ["cs.CL"], "comment": "21 pages, 9 figures, work in progress", "summary": "Diffusion-based large language models (dLLMs) have recently emerged as a\npowerful alternative to autoregressive LLMs, offering faster inference and\ngreater interactivity via parallel decoding and bidirectional modeling.\nHowever, despite strong performance in code generation and text infilling, we\nidentify a fundamental safety concern: existing alignment mechanisms fail to\nsafeguard dLLMs against context-aware, masked-input adversarial prompts,\nexposing novel vulnerabilities. To this end, we present DIJA, the first\nsystematic study and jailbreak attack framework that exploits unique safety\nweaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial\ninterleaved mask-text prompts that exploit the text generation mechanisms of\ndLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional\nmodeling drives the model to produce contextually consistent outputs for masked\nspans, even when harmful, while parallel decoding limits model dynamic\nfiltering and rejection sampling of unsafe content. This causes standard\nalignment mechanisms to fail, enabling harmful completions in alignment-tuned\ndLLMs, even when harmful behaviors or unsafe instructions are directly exposed\nin the prompt. Through comprehensive experiments, we demonstrate that DIJA\nsignificantly outperforms existing jailbreak methods, exposing a previously\noverlooked threat surface in dLLM architectures. Notably, our method achieves\nup to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior\nbaseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and\nby 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of\nharmful content in the jailbreak prompt. Our findings underscore the urgent\nneed for rethinking safety alignment in this emerging class of language models.\nCode is available at https://github.com/ZichenWen1/DIJA.", "AI": {"tldr": "DIJA is a jailbreak attack framework targeting diffusion-based LLMs (dLLMs), exploiting their bidirectional modeling and parallel decoding to bypass alignment mechanisms, achieving high success rates in generating harmful completions.", "motivation": "Existing alignment mechanisms for dLLMs fail against context-aware adversarial prompts, exposing safety vulnerabilities.", "method": "DIJA constructs adversarial interleaved mask-text prompts to exploit dLLMs' bidirectional modeling and parallel decoding, bypassing alignment safeguards.", "result": "DIJA outperforms prior jailbreak methods, achieving up to 100% keyword-based ASR and surpassing baselines by significant margins.", "conclusion": "The study highlights critical safety gaps in dLLMs, urging a rethink of alignment strategies for this model class."}}
{"id": "2507.11003", "pdf": "https://arxiv.org/pdf/2507.11003", "abs": "https://arxiv.org/abs/2507.11003", "authors": ["Yuhu Bai", "Jiangning Zhang", "Yunkang Cao", "Guangyuan Lu", "Qingdong He", "Xiangtai Li", "Guanzhong Tian"], "title": "Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "With the advent of vision-language models (e.g., CLIP) in zero- and few-shot\nsettings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in\nrecent research, where the rare classes are essential and expected in many\napplications. This study introduces \\textbf{FiSeCLIP} for ZSAD with\ntraining-free \\textbf{CLIP}, combining the feature matching with the\ncross-modal alignment. Testing with the entire dataset is impractical, while\nbatch-based testing better aligns with real industrial needs, and images within\na batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes\nother images in the same batch as reference information for the current image.\nHowever, the lack of labels for these references can introduce ambiguity, we\napply text information to \\textbf{fi}lter out noisy features. In addition, we\nfurther explore CLIP's inherent potential to restore its local\n\\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection\ntasks to enable a more accurate filtering process. Our approach exhibits\nsuperior performance for both anomaly classification and segmentation on\nanomaly detection benchmarks, building a stronger baseline for the direction,\ne.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by\n+4.6\\%$\\uparrow$/+5.7\\%$\\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.", "AI": {"tldr": "FiSeCLIP enhances zero-shot anomaly detection (ZSAD) by combining feature matching and cross-modal alignment with CLIP, outperforming SOTA methods.", "motivation": "To address the challenges of zero-shot anomaly detection (ZSAD) by leveraging CLIP's capabilities without additional training, while improving accuracy through feature filtering and local semantic restoration.", "method": "FiSeCLIP uses batch-based testing with mutual reference images, filters noisy features using text information, and restores CLIP's local semantic correlation for fine-grained anomaly detection.", "result": "FiSeCLIP achieves superior performance in anomaly classification and segmentation, outperforming AdaCLIP by +4.6%/+5.7% in AU-ROC/F1-max on MVTec-AD.", "conclusion": "FiSeCLIP sets a stronger baseline for ZSAD by effectively utilizing CLIP's potential and addressing ambiguity in reference images, demonstrating significant improvements over existing methods."}}
{"id": "2507.11112", "pdf": "https://arxiv.org/pdf/2507.11112", "abs": "https://arxiv.org/abs/2507.11112", "authors": ["Sanhanat Sivapiromrat", "Caiqi Zhang", "Marco Basaldella", "Nigel Collier"], "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.", "AI": {"tldr": "The paper explores multi-trigger data poisoning in LLMs, showing triggers can coexist without interference and remain robust. It proposes a selective retraining defense.", "motivation": "To understand trigger mechanisms and interactions in poisoned LLMs, addressing gaps in existing research.", "method": "A framework for studying poisoning, testing trigger coexistence and robustness, and a post hoc recovery method via selective retraining.", "result": "Multiple triggers can coexist robustly in LLMs, exposing a persistent vulnerability. The proposed defense removes triggers efficiently.", "conclusion": "LLMs are vulnerable to multi-trigger poisoning, but selective retraining offers an effective mitigation strategy."}}
{"id": "2507.11015", "pdf": "https://arxiv.org/pdf/2507.11015", "abs": "https://arxiv.org/abs/2507.11015", "authors": ["Zeyi Hou", "Zeqiang Wei", "Ruixin Yan", "Ning Lang", "Xiuzhuang Zhou"], "title": "Semantically Informed Salient Regions Guided Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in automated radiology report generation from chest X-rays\nusing deep learning algorithms have the potential to significantly reduce the\narduous workload of radiologists. However, due to the inherent massive data\nbias in radiology images, where abnormalities are typically subtle and sparsely\ndistributed, existing methods often produce fluent yet medically inaccurate\nreports, limiting their applicability in clinical practice. To address this\nissue effectively, we propose a Semantically Informed Salient Regions-guided\n(SISRNet) report generation method. Specifically, our approach explicitly\nidentifies salient regions with medically critical characteristics using\nfine-grained cross-modal semantics. Then, SISRNet systematically focuses on\nthese high-information regions during both image modeling and report\ngeneration, effectively capturing subtle abnormal findings, mitigating the\nnegative impact of data bias, and ultimately generating clinically accurate\nreports. Compared to its peers, SISRNet demonstrates superior performance on\nwidely used IU-Xray and MIMIC-CXR datasets.", "AI": {"tldr": "SISRNet improves radiology report generation by focusing on medically critical regions, addressing data bias and enhancing accuracy.", "motivation": "Existing methods produce fluent but inaccurate reports due to data bias in radiology images.", "method": "SISRNet identifies salient regions using fine-grained cross-modal semantics and focuses on them during image modeling and report generation.", "result": "SISRNet outperforms peers on IU-Xray and MIMIC-CXR datasets.", "conclusion": "SISRNet generates clinically accurate reports by mitigating data bias and focusing on critical regions."}}
{"id": "2507.11114", "pdf": "https://arxiv.org/pdf/2507.11114", "abs": "https://arxiv.org/abs/2507.11114", "authors": ["Seif Ahmed", "Mohamed T. Younes", "Abdelrahman Moustafa", "Abdelrahman Allam", "Hamza Moustafa"], "title": "MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We present a robust ensemble-based system for multilingual multimodal\nreasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach\nintegrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption\nrefinement and consistency checks, and Gemini 2.5 Pro as a reasoner which\nhandles final answer selection, all coordinated through carefully engineered\nfew-shot and zero-shot prompts. We conducted an extensive ablation study,\ntraining several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,\nMistral) on an English dataset and its multilingual augmented version.\nAdditionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for\ncomparison and found it to substantially outperform the trained models. Prompt\ndesign also proved critical: enforcing concise, language-normalized formats and\nprohibiting explanatory text boosted model accuracy on the English validation\nset from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)\nachieved first place overall in the multilingual track with 81.4% accuracy, and\nled 11 out of 13 individual language tracks, with top results such as 95.07%\nfor Croatian and 92.12% for Italian. These findings highlight that lightweight\nOCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual\naugmentation, can outperform heavier end-to-end models in high-stakes,\nmultilingual educational settings.", "AI": {"tldr": "A robust ensemble system for multilingual multimodal reasoning achieved top performance in the ImageCLEF 2025 EXAMS V challenge, leveraging Gemini models and precise prompts.", "motivation": "To address the challenge of multilingual multimodal reasoning in educational settings, aiming for high accuracy across diverse languages.", "method": "Integrated Gemini models (2.5 Flash, 1.5 Pro, 2.5 Pro) with few-shot and zero-shot prompts, conducted ablation studies, and evaluated zero-shot performance.", "result": "Achieved 81.4% accuracy overall, leading 11/13 language tracks (e.g., 95.07% for Croatian). Prompt optimization boosted accuracy from 55.9% to 61.7%.", "conclusion": "Lightweight OCR-VLM ensembles with precise prompts and cross-lingual augmentation outperform heavier models in multilingual educational tasks."}}
{"id": "2507.11025", "pdf": "https://arxiv.org/pdf/2507.11025", "abs": "https://arxiv.org/abs/2507.11025", "authors": ["Sung Ho Kang", "Hyun-Cheol Park"], "title": "Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schr\u00f6dinger Bridge with Conditional Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel framework for CBCT-to-MDCT translation, grounded in the\nSchrodinger Bridge (SB) formulation, which integrates GAN-derived priors with\nhuman-guided conditional diffusion. Unlike conventional GANs or diffusion\nmodels, our approach explicitly enforces boundary consistency between CBCT\ninputs and pseudo targets, ensuring both anatomical fidelity and perceptual\ncontrollability. Binary human feedback is incorporated via classifier-free\nguidance (CFG), effectively steering the generative process toward clinically\npreferred outcomes. Through iterative refinement and tournament-based\npreference selection, the model internalizes human preferences without relying\non a reward model. Subtraction image visualizations reveal that the proposed\nmethod selectively attenuates shade artifacts in key anatomical regions while\npreserving fine structural detail. Quantitative evaluations further demonstrate\nsuperior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical\ndatasets -- outperforming prior GAN- and fine-tuning-based feedback methods --\nwhile requiring only 10 sampling steps. These findings underscore the\neffectiveness and efficiency of our framework for real-time, preference-aligned\nmedical image translation.", "AI": {"tldr": "A novel CBCT-to-MDCT translation framework using Schrodinger Bridge (SB) with GAN priors and human-guided diffusion, ensuring anatomical fidelity and clinical preference alignment.", "motivation": "To improve CBCT-to-MDCT translation by integrating human feedback and enforcing boundary consistency for better anatomical accuracy and clinical relevance.", "method": "Combines GAN-derived priors with human-guided conditional diffusion, using classifier-free guidance (CFG) and iterative refinement with tournament-based preference selection.", "result": "Outperforms prior methods in RMSE, SSIM, LPIPS, and Dice metrics, with only 10 sampling steps, reducing artifacts while preserving detail.", "conclusion": "The framework is effective and efficient for real-time, preference-aligned medical image translation."}}
{"id": "2507.11128", "pdf": "https://arxiv.org/pdf/2507.11128", "abs": "https://arxiv.org/abs/2507.11128", "authors": ["Dimitri Staufer"], "title": "What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.6; H.2.8"], "comment": "16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable\n  Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto,\n  Portugal", "summary": "Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests.", "AI": {"tldr": "The paper introduces WikiMem, a dataset and method to identify and quantify personal data memorization in LLMs, aiding GDPR compliance like the Right to Be Forgotten.", "motivation": "Address concerns about LLMs memorizing and revealing personal data, especially under GDPR's RTBF, by identifying individual-fact associations.", "method": "Uses WikiMem dataset (5,000+ canaries) and a model-agnostic metric to rank ground-truth values against counterfactuals using calibrated negative log-likelihood.", "result": "Shows memorization correlates with subject web presence and model scale, evaluated across 15 LLMs (410M-70B parameters).", "conclusion": "Provides a foundation for identifying memorized personal data at the individual level, supporting dynamic forget sets for unlearning and RTBF requests."}}
{"id": "2507.11030", "pdf": "https://arxiv.org/pdf/2507.11030", "abs": "https://arxiv.org/abs/2507.11030", "authors": ["Sunghyun Park", "Jungsoo Lee", "Shubhankar Borse", "Munawar Hayat", "Sungha Choi", "Kyuwoong Hwang", "Fatih Porikli"], "title": "Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025; 15 pages", "summary": "While open-vocabulary semantic segmentation (OVSS) can segment an image into\nsemantic regions based on arbitrarily given text descriptions even for classes\nunseen during training, it fails to understand personal texts (e.g., `my mug\ncup') for segmenting regions of specific interest to users. This paper\naddresses challenges like recognizing `my mug cup' among `multiple mug cups'.\nTo overcome this challenge, we introduce a novel task termed\n\\textit{personalized open-vocabulary semantic segmentation} and propose a text\nprompt tuning-based plug-in method designed to recognize personal visual\nconcepts using a few pairs of images and masks, while maintaining the\nperformance of the original OVSS. Based on the observation that reducing false\npredictions is essential when applying text prompt tuning to this task, our\nproposed method employs `negative mask proposal' that captures visual concepts\nother than the personalized concept. We further improve the performance by\nenriching the representation of text prompts by injecting visual embeddings of\nthe personal concept into them. This approach enhances personalized OVSS\nwithout compromising the original OVSS performance. We demonstrate the\nsuperiority of our method on our newly established benchmarks for this task,\nincluding FSS$^\\text{per}$, CUB$^\\text{per}$, and ADE$^\\text{per}$.", "AI": {"tldr": "The paper introduces a novel task called personalized open-vocabulary semantic segmentation (OVSS) to recognize personal visual concepts (e.g., 'my mug cup') using text prompt tuning and negative mask proposals, while preserving original OVSS performance.", "motivation": "Current OVSS fails to segment regions based on personal text descriptions (e.g., 'my mug cup') among similar objects, limiting user-specific applications.", "method": "Proposes a plug-in method with text prompt tuning and 'negative mask proposal' to reduce false predictions, and enhances text prompts by injecting visual embeddings of personal concepts.", "result": "Demonstrates superior performance on new benchmarks (FSS$^\\text{per}$, CUB$^\\text{per}$, ADE$^\\text{per}$) without compromising original OVSS capabilities.", "conclusion": "The method effectively addresses personalized OVSS challenges, improving segmentation for user-specific concepts while maintaining general OVSS performance."}}
{"id": "2507.11198", "pdf": "https://arxiv.org/pdf/2507.11198", "abs": "https://arxiv.org/abs/2507.11198", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "categories": ["cs.CL", "cs.AI"], "comment": "Manuscript submitted for review", "summary": "Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code.", "AI": {"tldr": "The study explores how multi-agent systems (MAS) with varied personas and temperature settings affect consensus-building and coding accuracy in qualitative research using LLMs. Results show limited benefits of MAS over single-agent coding, with only specific conditions yielding improvements.", "motivation": "To understand the benefits of multi-agent systems (MAS) over single-agent coding in qualitative research using LLMs, particularly how agent personas and temperature influence consensus and accuracy.", "method": "Experimental study using six open-source LLMs (3B to 32B parameters) and 18 configurations to analyze over 77,000 coding decisions against human-annotated transcripts. MAS mirrored human coding workflows with structured discussion and consensus arbitration.", "result": "Temperature impacted consensus across all LLMs. MAS with diverse personas delayed consensus in most cases, and neither temperature nor personas robustly improved accuracy. Only one model (OpenHermesV2:7B) showed gains under specific conditions.", "conclusion": "MAS with diverse personas does not consistently outperform single-agent coding, challenging assumptions about their benefits. However, MAS may help refine ambiguous code applications, suggesting potential for improving codebooks and human-AI collaboration."}}
{"id": "2507.11035", "pdf": "https://arxiv.org/pdf/2507.11035", "abs": "https://arxiv.org/abs/2507.11035", "authors": ["Lirong Zheng", "Yanshan Li", "Rui Yu", "Kaihao Zhang"], "title": "Efficient Dual-domain Image Dehazing with Haze Prior Perception", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Transformer-based models exhibit strong global modeling capabilities in\nsingle-image dehazing, but their high computational cost limits real-time\napplicability. Existing methods predominantly rely on spatial-domain features\nto capture long-range dependencies, which are computationally expensive and\noften inadequate under complex haze conditions. While some approaches introduce\nfrequency-domain cues, the weak coupling between spatial and frequency branches\nlimits the overall performance. To overcome these limitations, we propose the\nDark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel\ndual-domain framework that performs physically guided degradation alignment\nacross spatial and frequency domains. At its core, the DGFDBlock comprises two\nkey modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a\npixel-level haze confidence map from dark channel priors to adaptively enhance\nhaze-relevant frequency components, thereby achieving global degradation-aware\nspectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which\nfuses multi-scale features through diverse convolutional kernels and hybrid\ngating mechanisms to recover fine structural details. Additionally, a Prior\nCorrection Guidance Branch (PCGB) incorporates a closed-loop feedback\nmechanism, enabling iterative refinement of the prior by intermediate dehazed\nfeatures and significantly improving haze localization accuracy, especially in\nchallenging outdoor scenes. Extensive experiments on four benchmark haze\ndatasets demonstrate that DGFDNet achieves state-of-the-art performance with\nsuperior robustness and real-time efficiency. Code is available at:\nhttps://github.com/Dilizlr/DGFDNet.", "AI": {"tldr": "DGFDNet is a dual-domain dehazing network combining spatial and frequency domains, guided by dark channel priors, achieving state-of-the-art performance with real-time efficiency.", "motivation": "Transformer-based models are computationally expensive for dehazing, and existing methods struggle with complex haze conditions due to weak coupling between spatial and frequency domains.", "method": "Proposes DGFDNet with HAFM for adaptive frequency modulation and MGAM for multi-scale feature fusion, along with PCGB for iterative prior refinement.", "result": "Achieves superior performance on benchmark datasets with robustness and real-time efficiency.", "conclusion": "DGFDNet effectively addresses dehazing challenges by integrating spatial and frequency domains, outperforming existing methods."}}
{"id": "2507.11216", "pdf": "https://arxiv.org/pdf/2507.11216", "abs": "https://arxiv.org/abs/2507.11216", "authors": ["Valle Ruiz-Fern\u00e1ndez", "Mario Mina", "J\u00falia Falc\u00e3o", "Luis Vasquez-Reina", "Anna Sall\u00e9s", "Aitor Gonzalez-Agirre", "Olatz Perez-de-Vi\u00f1aspre"], "title": "EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Previous literature has largely shown that Large Language Models (LLMs)\nperpetuate social biases learnt from their pre-training data. Given the notable\nlack of resources for social bias evaluation in languages other than English,\nand for social contexts outside of the United States, this paper introduces the\nSpanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and\nCaBBQ). Based on the original BBQ, these two parallel datasets are designed to\nassess social bias across 10 categories using a multiple-choice QA setting, now\nadapted to the Spanish and Catalan languages and to the social context of\nSpain. We report evaluation results on different LLMs, factoring in model\nfamily, size and variant. Our results show that models tend to fail to choose\nthe correct answer in ambiguous scenarios, and that high QA accuracy often\ncorrelates with greater reliance on social biases.", "AI": {"tldr": "The paper introduces Spanish and Catalan Bias Benchmarks (EsBBQ and CaBBQ) to evaluate social biases in LLMs, adapted from BBQ for Spain's context. Results show models struggle with ambiguous scenarios and high accuracy often aligns with bias reliance.", "motivation": "Address the lack of non-English and non-US social bias evaluation resources by creating Spanish and Catalan benchmarks.", "method": "Develop parallel datasets (EsBBQ and CaBBQ) based on BBQ, assessing bias in 10 categories via multiple-choice QA. Evaluate LLMs by model family, size, and variant.", "result": "Models often fail in ambiguous scenarios, and higher QA accuracy correlates with increased reliance on social biases.", "conclusion": "The benchmarks highlight LLMs' bias issues in Spanish and Catalan, emphasizing the need for context-specific evaluations."}}
{"id": "2507.11037", "pdf": "https://arxiv.org/pdf/2507.11037", "abs": "https://arxiv.org/abs/2507.11037", "authors": ["Jie-Wen Li", "Zi-Han Ye", "Qingyuan Zhou", "Jiayi Song", "Ying He", "Ben Fei", "Wen-Ming Chen"], "title": "A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion", "categories": ["cs.CV"], "comment": "15 pages, 10 figures, 2 tables", "summary": "The kinematics analysis of foot-ankle complex during gait is essential for\nadvancing biomechanical research and clinical assessment. Collecting accurate\nsurface geometry data from the foot and ankle during dynamic gait conditions is\ninherently challenging due to swing foot occlusions and viewing limitations.\nThus, this paper introduces FootGait3D, a novel multi-view dataset of\nhigh-resolution ankle-foot surface point clouds captured during natural gait.\nDifferent from existing gait datasets that typically target whole-body or\nlower-limb motion, FootGait3D focuses specifically on the detailed modeling of\nthe ankle-foot region, offering a finer granularity of motion data. To address\nthis, FootGait3D consists of 8,403 point cloud frames collected from 46\nsubjects using a custom five-camera depth sensing system. Each frame includes a\ncomplete 5-view reconstruction of the foot and ankle (serving as ground truth)\nalong with partial point clouds obtained from only four, three, or two views.\nThis structured variation enables rigorous evaluation of 3D point cloud\ncompletion methods under varying occlusion levels and viewpoints. Our dataset\nis designed for shape completion tasks, facilitating the benchmarking of\nstate-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and\nmulti-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the\nchallenge of recovering the full foot geometry from occluded inputs. FootGait3D\nhas significant potential to advance research in biomechanics and multi-segment\nfoot modeling, offering a valuable testbed for clinical gait analysis,\nprosthetic design, and robotics applications requiring detailed 3D models of\nthe foot during motion. The dataset is now available at\nhttps://huggingface.co/datasets/ljw285/FootGait3D.", "AI": {"tldr": "FootGait3D is a multi-view dataset of high-resolution ankle-foot point clouds for gait analysis, addressing occlusion challenges and enabling shape completion benchmarking.", "motivation": "Accurate foot-ankle kinematics during gait is challenging due to occlusions and viewing limitations, necessitating detailed datasets like FootGait3D.", "method": "FootGait3D collects 8,403 point cloud frames from 46 subjects using a five-camera system, providing complete and partial views for evaluation.", "result": "The dataset supports benchmarking of 3D point cloud completion methods and advances biomechanics research.", "conclusion": "FootGait3D is a valuable resource for gait analysis, prosthetic design, and robotics, available for public use."}}
{"id": "2507.11222", "pdf": "https://arxiv.org/pdf/2507.11222", "abs": "https://arxiv.org/abs/2507.11222", "authors": ["Fares Wael", "Youssef Maklad", "Ali Hamdi", "Wael Elsersy"], "title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining", "categories": ["cs.CL", "cs.AI", "cs.NI"], "comment": null, "summary": "Finite-State Machines (FSMs) are critical for modeling the operational logic\nof network protocols, enabling verification, analysis, and vulnerability\ndiscovery. However, existing FSM extraction techniques face limitations such as\nscalability, incomplete coverage, and ambiguity in natural language\nspecifications. In this paper, we propose FlowFSM, a novel agentic framework\nthat leverages Large Language Models (LLMs) combined with prompt chaining and\nchain-of-thought reasoning to extract accurate FSMs from raw RFC documents.\nFlowFSM systematically processes protocol specifications, identifies state\ntransitions, and constructs structured rule-books by chaining agent outputs.\nExperimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM\nachieves high extraction precision while minimizing hallucinated transitions,\nshowing promising results. Our findings highlight the potential of agent-based\nLLM systems in the advancement of protocol analysis and FSM inference for\ncybersecurity and reverse engineering applications.", "AI": {"tldr": "FlowFSM uses LLMs with prompt chaining to extract accurate FSMs from RFC documents, outperforming existing methods in precision and scalability.", "motivation": "Existing FSM extraction techniques lack scalability, coverage, and clarity, especially with natural language specifications.", "method": "FlowFSM combines LLMs, prompt chaining, and chain-of-thought reasoning to systematically process RFCs and construct FSMs.", "result": "FlowFSM achieves high precision in FSM extraction for FTP and RTSP protocols, reducing hallucinated transitions.", "conclusion": "Agent-based LLM systems like FlowFSM show promise for advancing protocol analysis and cybersecurity applications."}}
{"id": "2507.11040", "pdf": "https://arxiv.org/pdf/2507.11040", "abs": "https://arxiv.org/abs/2507.11040", "authors": ["Nicolas Drapier", "Aladine Chetouani", "Aur\u00e9lien Chateigner"], "title": "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery", "categories": ["cs.CV"], "comment": "11 pages, 9 figures", "summary": "We present GLOD, a transformer-first architecture for object detection in\nhigh-resolution satellite imagery. GLOD replaces CNN backbones with a Swin\nTransformer for end-to-end feature extraction, combined with novel UpConvMixer\nblocks for robust upsampling and Fusion Blocks for multi-scale feature\nintegration. Our approach achieves 32.95\\% on xView, outperforming SOTA methods\nby 11.46\\%. Key innovations include asymmetric fusion with CBAM attention and a\nmulti-path head design capturing objects across scales. The architecture is\noptimized for satellite imagery challenges, leveraging spatial priors while\nmaintaining computational efficiency.", "AI": {"tldr": "GLOD introduces a transformer-first architecture for satellite object detection, outperforming SOTA by 11.46% on xView with innovations like Swin Transformer, UpConvMixer, and Fusion Blocks.", "motivation": "To address the challenges of object detection in high-resolution satellite imagery by leveraging transformer-based architectures for better performance.", "method": "Uses Swin Transformer for feature extraction, UpConvMixer for upsampling, and Fusion Blocks for multi-scale integration, with CBAM attention and multi-path head design.", "result": "Achieves 32.95% on xView, surpassing SOTA methods by 11.46%.", "conclusion": "GLOD is optimized for satellite imagery, combining efficiency and performance through novel architectural choices."}}
{"id": "2507.11230", "pdf": "https://arxiv.org/pdf/2507.11230", "abs": "https://arxiv.org/abs/2507.11230", "authors": ["Lyzander Marciano Andrylie", "Inaya Rahmanisa", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages", "categories": ["cs.CL", "68T50"], "comment": null, "summary": "Understanding the multilingual mechanisms of large language models (LLMs)\nprovides insight into how they process different languages, yet this remains\nchallenging. Existing studies often focus on individual neurons, but their\npolysemantic nature makes it difficult to isolate language-specific units from\ncross-lingual representations. To address this, we explore sparse autoencoders\n(SAEs) for their ability to learn monosemantic features that represent concrete\nand abstract concepts across languages in LLMs. While some of these features\nare language-independent, the presence of language-specific features remains\nunderexplored. In this work, we introduce SAE-LAPE, a method based on feature\nactivation probability, to identify language-specific features within the\nfeed-forward network. We find that many such features predominantly appear in\nthe middle to final layers of the model and are interpretable. These features\ninfluence the model's multilingual performance and language output and can be\nused for language identification with performance comparable to fastText along\nwith more interpretability. Our code is available at\nhttps://github.com/LyzanderAndrylie/language-specific-features .", "AI": {"tldr": "The paper explores language-specific features in LLMs using sparse autoencoders (SAEs) and introduces SAE-LAPE, a method to identify these features, revealing their impact on multilingual performance.", "motivation": "Understanding how LLMs process multiple languages is challenging due to the polysemantic nature of neurons. The study aims to isolate language-specific features for better interpretability.", "method": "The authors use sparse autoencoders (SAEs) and introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features in LLMs.", "result": "Language-specific features are found in middle to final layers, are interpretable, and influence multilingual performance. They also enable language identification comparable to fastText.", "conclusion": "SAE-LAPE effectively identifies language-specific features in LLMs, enhancing interpretability and performance in multilingual tasks."}}
{"id": "2507.11055", "pdf": "https://arxiv.org/pdf/2507.11055", "abs": "https://arxiv.org/abs/2507.11055", "authors": ["Shuchang Ye", "Usman Naseem", "Mingyuan Meng", "Jinman Kim"], "title": "Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Medical language-guided segmentation, integrating textual clinical reports as\nauxiliary guidance to enhance image segmentation, has demonstrated significant\nimprovements over unimodal approaches. However, its inherent reliance on paired\nimage-text input, which we refer to as ``textual reliance\", presents two\nfundamental limitations: 1) many medical segmentation datasets lack paired\nreports, leaving a substantial portion of image-only data underutilized for\ntraining; and 2) inference is limited to retrospective analysis of cases with\npaired reports, limiting its applicability in most clinical scenarios where\nsegmentation typically precedes reporting. To address these limitations, we\npropose ProLearn, the first Prototype-driven Learning framework for\nlanguage-guided segmentation that fundamentally alleviates textual reliance. At\nits core, in ProLearn, we introduce a novel Prototype-driven Semantic\nApproximation (PSA) module to enable approximation of semantic guidance from\ntextual input. PSA initializes a discrete and compact prototype space by\ndistilling segmentation-relevant semantics from textual reports. Once\ninitialized, it supports a query-and-respond mechanism which approximates\nsemantic guidance for images without textual input, thereby alleviating textual\nreliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG\ndemonstrate that ProLearn outperforms state-of-the-art language-guided methods\nwhen limited text is available.", "AI": {"tldr": "ProLearn is a prototype-driven framework for language-guided segmentation that reduces reliance on paired image-text data, enabling broader clinical application.", "motivation": "Current language-guided segmentation methods depend on paired image-text data, limiting their use in datasets without reports and clinical scenarios where segmentation precedes reporting.", "method": "ProLearn introduces a Prototype-driven Semantic Approximation (PSA) module to approximate semantic guidance from text, allowing segmentation without paired reports.", "result": "ProLearn outperforms state-of-the-art methods on datasets like QaTa-COV19, MosMedData+, and Kvasir-SEG when text is scarce.", "conclusion": "ProLearn effectively reduces textual reliance, enhancing the practicality of language-guided segmentation in clinical settings."}}
{"id": "2507.11273", "pdf": "https://arxiv.org/pdf/2507.11273", "abs": "https://arxiv.org/abs/2507.11273", "authors": ["Luohe Shi", "Zuchao Li", "Lefei Zhang", "Guoming Liu", "Baoyuan Qi", "Hai Zhao"], "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding", "categories": ["cs.CL"], "comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.", "AI": {"tldr": "KV-Latent reduces Key-Value cache footprint in LLMs by down-sampling dimensions into a latent space, improving efficiency with minimal extra training.", "motivation": "The increasing Key-Value cache during inference in LLMs causes memory and bandwidth inefficiencies.", "method": "Proposes KV-Latent, down-sampling KV vectors into a latent space, and modifies Rotary Positional Embedding for stability.", "result": "Significantly reduces KV Cache footprint and improves inference speed with minimal training overhead.", "conclusion": "KV-Latent enables more efficient LLMs and opens new possibilities for KV Cache optimization."}}
{"id": "2507.11061", "pdf": "https://arxiv.org/pdf/2507.11061", "abs": "https://arxiv.org/abs/2507.11061", "authors": ["Hayeon Kim", "Ji Ha Jang", "Se Young Chun"], "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing.", "AI": {"tldr": "RoMaP is a novel framework for precise local 3D Gaussian editing, addressing challenges in inconsistent multi-view segmentations and SDS loss ambiguity. It introduces 3D-GALP for robust mask generation and a regularized SDS loss with additional constraints, achieving state-of-the-art results.", "motivation": "Precise local 3D edits are challenging due to inconsistent multi-view 2D part segmentations and ambiguous SDS loss. Existing methods struggle with accuracy and flexibility in Gaussian Splatting.", "method": "RoMaP combines 3D-GALP for consistent part segmentation using SH coefficients and a regularized SDS loss with L1 anchor loss (via SLaMP) and other regularizers like Gaussian prior removal.", "result": "RoMaP achieves state-of-the-art local 3D editing on reconstructed and generated Gaussian scenes, enabling robust and flexible part-level modifications.", "conclusion": "RoMaP advances local 3D Gaussian editing by improving precision and flexibility, making it suitable for high-quality 3D content creation."}}
{"id": "2507.11275", "pdf": "https://arxiv.org/pdf/2507.11275", "abs": "https://arxiv.org/abs/2507.11275", "authors": ["Jiaxuan Xie", "Chengwu Liu", "Ye Yuan", "Siqi Li", "Zhiping Xiao", "Ming Zhang"], "title": "FMC: Formalization of Natural Language Mathematical Competition Problems", "categories": ["cs.CL"], "comment": "Accepted in ICML 2025 AI4MATH Workshop", "summary": "Efficient and accurate autoformalization methods, which leverage large-scale\ndatasets of extensive natural language mathematical problems to construct\nformal language datasets, are key to advancing formal mathematical reasoning.\nIn this paper, we propose an autoformalization pipeline based on large language\nmodels with error feedback, achieving a fully automatic and training-free\nformalization approach. Using this pipeline, we curate an Olympiad-level\ndataset aligning natural language problems with Lean formalizations. The\ndataset comprises $3,922$ mathematical problems in natural language and $9,787$\nin Lean, of which $64.46\\%$ were assessed as at least above-average quality,\nmaking it suitable as a benchmark for automated theorem provers. Additionally,\nwe investigate the formalization and reasoning capabilities of various LLMs and\nempirically demonstrate that few-shot learning, error feedback, and increasing\nsampling numbers enhance the autoformalization process. Experiments of three\nautomated theorem provers on the \\dataset\\ dataset also highlight its\nchallenging nature and its value as a benchmark for formal reasoning tasks.", "AI": {"tldr": "The paper introduces an autoformalization pipeline using large language models with error feedback to create a high-quality Olympiad-level dataset of natural language and Lean formalizations. It demonstrates the effectiveness of few-shot learning and error feedback, and validates the dataset as a benchmark for automated theorem provers.", "motivation": "Advancing formal mathematical reasoning by developing efficient and accurate autoformalization methods to bridge natural language and formal language datasets.", "method": "Proposes an autoformalization pipeline using large language models with error feedback, curating a dataset of 3,922 natural language problems and 9,787 Lean formalizations. Evaluates the quality and benchmarks automated theorem provers.", "result": "64.46% of the dataset was rated as above-average quality. Few-shot learning, error feedback, and increased sampling improved autoformalization. The dataset proved challenging for theorem provers.", "conclusion": "The autoformalization pipeline and dataset are effective for formal reasoning tasks, providing a valuable benchmark for automated theorem provers."}}
{"id": "2507.11075", "pdf": "https://arxiv.org/pdf/2507.11075", "abs": "https://arxiv.org/abs/2507.11075", "authors": ["Chang Peng", "Yifei Zhou", "Huifeng Xi", "Shiqing Huang", "Chuangye Chen", "Jianming Yang", "Bao Yang", "Zhenyu Jiang"], "title": "Joint angle model based learning to refine kinematic human pose estimation", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.3"], "comment": null, "summary": "Marker-free human pose estimation (HPE) has found increasing applications in\nvarious fields. Current HPE suffers from occasional errors in keypoint\nrecognition and random fluctuation in keypoint trajectories when analyzing\nkinematic human poses. The performance of existing deep learning-based models\nfor HPE refinement is considerably limited by inaccurate training datasets in\nwhich the keypoints are manually annotated. This paper proposed a novel method\nto overcome the difficulty through joint angle-based modeling. The key\ntechniques include: (i) A joint angle-based model of human pose, which is\nrobust to describe kinematic human poses; (ii) Approximating temporal variation\nof joint angles through high order Fourier series to get reliable \"ground\ntruth\"; (iii) A bidirectional recurrent network is designed as a\npost-processing module to refine the estimation of well-established HRNet.\nTrained with the high-quality dataset constructed using our method, the network\ndemonstrates outstanding performance to correct wrongly recognized joints and\nsmooth their spatiotemporal trajectories. Tests show that joint angle-based\nrefinement (JAR) outperforms the state-of-the-art HPE refinement network in\nchallenging cases like figure skating and breaking.", "AI": {"tldr": "Proposes a joint angle-based method to refine human pose estimation, improving accuracy and smoothing trajectories by using high-quality datasets and a bidirectional recurrent network.", "motivation": "Current HPE methods suffer from errors in keypoint recognition and trajectory fluctuations due to inaccurate manually annotated datasets.", "method": "Uses joint angle-based modeling, approximates joint angle variations with Fourier series, and employs a bidirectional recurrent network for refinement.", "result": "Outperforms state-of-the-art HPE refinement networks, especially in challenging cases like figure skating and breaking.", "conclusion": "Joint angle-based refinement (JAR) effectively improves HPE accuracy and trajectory smoothness."}}
{"id": "2507.11292", "pdf": "https://arxiv.org/pdf/2507.11292", "abs": "https://arxiv.org/abs/2507.11292", "authors": ["Zewen Bai", "Liang Yang", "Shengdi Yin", "Yuanyuan Sun", "Hongfei Lin"], "title": "Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of hate speech has inflicted significant societal harm,\nwith its intensity and directionality closely tied to specific targets and\narguments. In recent years, numerous machine learning-based methods have been\ndeveloped to detect hateful comments on online platforms automatically.\nHowever, research on Chinese hate speech detection lags behind, and\ninterpretability studies face two major challenges: first, the scarcity of\nspan-level fine-grained annotated datasets limits models' deep semantic\nunderstanding of hate speech; second, insufficient research on identifying and\ninterpreting coded hate speech restricts model explainability in complex\nreal-world scenarios. To address these, we make the following contributions:\n(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE\nToxiCN), the first span-level Chinese hate speech dataset, and evaluate the\nhate semantic understanding of existing models using it. (2) We conduct the\nfirst comprehensive study on Chinese coded hate terms, LLMs' ability to\ninterpret hate semantics. (3) We propose a method to integrate an annotated\nlexicon into models, significantly enhancing hate speech detection performance.\nOur work provides valuable resources and insights to advance the\ninterpretability of Chinese hate speech detection research.", "AI": {"tldr": "The paper addresses challenges in Chinese hate speech detection by introducing a span-level dataset (STATE ToxiCN), studying coded hate terms, and proposing a lexicon-integrated method to improve detection and interpretability.", "motivation": "The rise of hate speech and the lack of fine-grained datasets and interpretability research in Chinese contexts motivate this work.", "method": "The authors introduce a span-level dataset, study coded hate terms, and propose integrating an annotated lexicon into models.", "result": "The proposed method enhances hate speech detection performance and provides insights into interpretability.", "conclusion": "This work advances Chinese hate speech detection by offering resources and methods for better understanding and detecting hate speech."}}
{"id": "2507.11077", "pdf": "https://arxiv.org/pdf/2507.11077", "abs": "https://arxiv.org/abs/2507.11077", "authors": ["Weizhao Ma", "Dong Zhou", "Yuhui Hu", "Zipeng He"], "title": "GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft", "categories": ["cs.CV"], "comment": null, "summary": "Monocular pose estimation of non-cooperative spacecraft is significant for\non-orbit service (OOS) tasks, such as satellite maintenance, space debris\nremoval, and station assembly. Considering the high demands on pose estimation\naccuracy, mainstream monocular pose estimation methods typically consist of\nkeypoint detectors and PnP solver. However, current keypoint detectors remain\nvulnerable to structural symmetry and partial occlusion of non-cooperative\nspacecraft. To this end, we propose a graph-based keypoints network for the\nmonocular pose estimation of non-cooperative spacecraft, GKNet, which leverages\nthe geometric constraint of keypoints graph. In order to better validate\nkeypoint detectors, we present a moderate-scale dataset for the spacecraft\nkeypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000\nsimulated images, and corresponding high-precise keypoint annotations.\nExtensive experiments and an ablation study have demonstrated the high accuracy\nand effectiveness of our GKNet, compared to the state-of-the-art spacecraft\nkeypoint detectors. The code for GKNet and the SKD dataset is available at\nhttps://github.com/Dongzhou-1996/GKNet.", "AI": {"tldr": "A graph-based keypoints network (GKNet) is proposed for accurate monocular pose estimation of non-cooperative spacecraft, addressing challenges like structural symmetry and occlusion. A dataset (SKD) is introduced for validation.", "motivation": "High accuracy in monocular pose estimation is crucial for on-orbit service tasks, but current keypoint detectors struggle with structural symmetry and occlusion.", "method": "GKNet leverages geometric constraints of keypoints graphs to improve detection. A dataset (SKD) with 90,000 simulated images is created for validation.", "result": "GKNet outperforms state-of-the-art keypoint detectors in accuracy and effectiveness.", "conclusion": "GKNet and the SKD dataset provide a robust solution for spacecraft pose estimation, with potential applications in satellite maintenance and debris removal."}}
{"id": "2507.11299", "pdf": "https://arxiv.org/pdf/2507.11299", "abs": "https://arxiv.org/abs/2507.11299", "authors": ["Andrei Niculae", "Adrian Cosma", "Cosmin Dumitrache", "Emilian R\u01cedoi"], "title": "Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian", "categories": ["cs.CL"], "comment": "10 figures, 2 tables, 2 listings", "summary": "Text-based telemedicine has become increasingly common, yet the quality of\nmedical advice in doctor-patient interactions is often judged more on how\nadvice is communicated rather than its clinical accuracy. To address this, we\nintroduce Dr.Copilot , a multi-agent large language model (LLM) system that\nsupports Romanian-speaking doctors by evaluating and enhancing the presentation\nquality of their written responses. Rather than assessing medical correctness,\nDr.Copilot provides feedback along 17 interpretable axes. The system comprises\nof three LLM agents with prompts automatically optimized via DSPy. Designed\nwith low-resource Romanian data and deployed using open-weight models, it\ndelivers real-time specific feedback to doctors within a telemedicine platform.\nEmpirical evaluations and live deployment with 41 doctors show measurable\nimprovements in user reviews and response quality, marking one of the first\nreal-world deployments of LLMs in Romanian medical settings.", "AI": {"tldr": "Dr.Copilot is a multi-agent LLM system for Romanian-speaking doctors, improving the presentation quality of their telemedicine responses without assessing medical accuracy. It provides feedback on 17 interpretable axes, using three LLM agents with optimized prompts. Deployed with open-weight models, it enhances response quality and user reviews.", "motivation": "The quality of medical advice in text-based telemedicine is often judged by communication rather than clinical accuracy. This gap motivates the need for a tool like Dr.Copilot to enhance the presentation quality of doctors' responses.", "method": "Dr.Copilot uses three LLM agents with prompts optimized via DSPy, designed for low-resource Romanian data. It provides real-time feedback on 17 interpretable axes, deployed using open-weight models.", "result": "Empirical evaluations and live deployment with 41 doctors show measurable improvements in user reviews and response quality.", "conclusion": "Dr.Copilot successfully enhances the presentation quality of telemedicine responses, marking a pioneering deployment of LLMs in Romanian medical settings."}}
{"id": "2507.11081", "pdf": "https://arxiv.org/pdf/2507.11081", "abs": "https://arxiv.org/abs/2507.11081", "authors": ["Chang Peng", "Bao Yang", "Meiqi Li", "Ge Zhang", "Hui Sun", "Zhenyu Jiang"], "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.2"], "comment": null, "summary": "Ground penetrating radar (GPR) has become a rapid and non-destructive\nsolution for road subsurface distress (RSD) detection. However, RSD recognition\nfrom GPR images is labor-intensive and heavily relies on inspectors' expertise.\nDeep learning offers the possibility for automatic RSD recognition, but its\ncurrent performance is limited by two factors: Scarcity of high-quality dataset\nfor network training and insufficient capability of network to distinguish RSD.\nIn this study, a rigorously validated 3D GPR dataset containing 2134 samples of\ndiverse types was constructed through field scanning. Based on the finding that\nthe YOLO model trained with one of the three scans of GPR images exhibits\nvarying sensitivity to specific type of RSD, we proposed a novel\ncross-verification strategy with outstanding accuracy in RSD recognition,\nachieving recall over 98.6% in field tests. The approach, integrated into an\nonline RSD detection system, can reduce the labor of inspection by around 90%.", "AI": {"tldr": "A novel cross-verification strategy using YOLO models and a rigorously validated 3D GPR dataset improves RSD recognition accuracy, reducing inspection labor by 90%.", "motivation": "Manual RSD recognition from GPR images is labor-intensive and expertise-dependent. Deep learning's potential is hindered by dataset scarcity and network limitations.", "method": "Constructed a 3D GPR dataset with 2134 samples and proposed a cross-verification strategy using YOLO models trained on different GPR scans.", "result": "Achieved over 98.6% recall in RSD recognition during field tests.", "conclusion": "The approach significantly reduces inspection labor and enhances accuracy, demonstrating practical utility in RSD detection."}}
{"id": "2507.11316", "pdf": "https://arxiv.org/pdf/2507.11316", "abs": "https://arxiv.org/abs/2507.11316", "authors": ["Haoran Jin", "Meng Li", "Xiting Wang", "Zhihao Xu", "Minlie Huang", "Yantao Jia", "Defu Lian"], "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 14 figures. Accepted by ACL 2025 (main conference)", "summary": "Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.", "AI": {"tldr": "The paper introduces ConVA, a method to align LLMs with human values by modifying latent representations, ensuring consistency without performance loss.", "motivation": "Aligning LLMs with human values is crucial for clarity, transparency, and adaptability in evolving scenarios.", "method": "Proposes Controlled Value Vector Activation (ConVA) to interpret and modify latent value representations, using context-controlled identification and gated activation for minimal interference.", "result": "Achieves highest control success rate across 10 values, maintains performance/fluency, and resists malicious inputs.", "conclusion": "ConVA effectively aligns LLMs with human values, ensuring consistency and robustness."}}
{"id": "2507.11085", "pdf": "https://arxiv.org/pdf/2507.11085", "abs": "https://arxiv.org/abs/2507.11085", "authors": ["Tianchi Xu"], "title": "Atmos-Bench: 3D Atmospheric Structures for Climate Insight", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Atmospheric structure, represented by backscatter coefficients (BC) recovered\nfrom satellite LiDAR attenuated backscatter (ATB), provides a volumetric view\nof clouds, aerosols, and molecules, playing a critical role in human\nactivities, climate understanding, and extreme weather forecasting. Existing\nmethods often rely on auxiliary inputs and simplified physics-based\napproximations, and lack a standardized 3D benchmark for fair evaluation.\nHowever, such approaches may introduce additional uncertainties and\ninsufficiently capture realistic radiative transfer and atmospheric\nscattering-absorption effects. To bridge these gaps, we present Atmos-Bench:\nthe first 3D atmospheric benchmark, along with a novel FourCastX:\nFrequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)\ngenerates 921,600 image slices from 3D scattering volumes simulated at 532 nm\nand 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean\ntime steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC\nphysical constraints into the model architecture, promoting energy consistency\nduring restoration; (c) achieves consistent improvements on the Atmos-Bench\ndataset across both 355 nm and 532 nm bands, outperforming state-of-the-art\nbaseline models without relying on auxiliary inputs. Atmos-Bench establishes a\nnew standard for satellite-based 3D atmospheric structure recovery and paves\nthe way for deeper climate insight.", "AI": {"tldr": "The paper introduces Atmos-Bench, the first 3D atmospheric benchmark, and FourCastX, a novel network for atmospheric structure recovery, outperforming existing methods without auxiliary inputs.", "motivation": "Current methods for atmospheric structure recovery rely on simplified physics and lack standardized benchmarks, leading to uncertainties and incomplete radiative transfer modeling.", "method": "The authors propose Atmos-Bench, a 3D benchmark, and FourCastX, a frequency-enhanced spatio-temporal network embedding physical constraints, trained on simulated scattering volumes.", "result": "FourCastX achieves consistent improvements over baselines on Atmos-Bench, excelling in both 355 nm and 532 nm bands without auxiliary inputs.", "conclusion": "Atmos-Bench sets a new standard for 3D atmospheric recovery, enhancing climate understanding and forecasting."}}
{"id": "2507.11330", "pdf": "https://arxiv.org/pdf/2507.11330", "abs": "https://arxiv.org/abs/2507.11330", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Yi Zhao"], "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.HC"], "comment": "Journal of the Association for Information Science and Technology,\n  2025", "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. The most common novelty in\nacademic papers is the introduction of new methods. In this paper, we propose\nleveraging human knowledge and LLM to assist pretrained language models (PLMs,\ne.g. BERT etc.) in predicting the method novelty of papers. Specifically, we\nextract sentences related to the novelty of the academic paper from peer review\nreports and use LLM to summarize the methodology section of the academic paper,\nwhich are then used to fine-tune PLMs. In addition, we have designed a\ntext-guided fusion module with novel Sparse-Attention to better integrate human\nand LLM knowledge. We compared the method we proposed with a large number of\nbaselines. Extensive experiments demonstrate that our method achieves superior\nperformance.", "AI": {"tldr": "The paper proposes integrating human expertise and LLM knowledge to assess novelty in academic papers, focusing on method novelty, and demonstrates superior performance over baselines.", "motivation": "Current methods for assessing novelty in academic papers (expert judgment or unique reference combinations) have limitations. Human experts lack comprehensive knowledge, while the effectiveness of reference-based methods is uncertain.", "method": "The approach combines human knowledge (from peer review reports) and LLM summaries of methodology sections to fine-tune PLMs. A text-guided fusion module with Sparse-Attention is designed for integration.", "result": "Extensive experiments show the proposed method outperforms numerous baselines in predicting method novelty.", "conclusion": "The integration of human and LLM knowledge effectively addresses limitations in novelty assessment, achieving superior performance."}}
{"id": "2507.11099", "pdf": "https://arxiv.org/pdf/2507.11099", "abs": "https://arxiv.org/abs/2507.11099", "authors": ["Qiyang Wan", "Chengzhi Gao", "Ruiping Wang", "Xilin Chen"], "title": "A Survey on Interpretability in Visual Recognition", "categories": ["cs.CV"], "comment": "20 pages, 7 figures, 2 tables. Under review", "summary": "In recent years, visual recognition methods have advanced significantly,\nfinding applications across diverse fields. While researchers seek to\nunderstand the mechanisms behind the success of these models, there is also a\ngrowing impetus to deploy them in critical areas like autonomous driving and\nmedical diagnostics to better diagnose failures, which promotes the development\nof interpretability research. This paper systematically reviews existing\nresearch on the interpretability of visual recognition models and proposes a\ntaxonomy of methods from a human-centered perspective. The proposed taxonomy\ncategorizes interpretable recognition methods based on Intent, Object,\nPresentation, and Methodology, thereby establishing a systematic and coherent\nset of grouping criteria for these XAI methods. Additionally, we summarize the\nrequirements for evaluation metrics and explore new opportunities enabled by\nrecent technologies, such as large multimodal models. We aim to organize\nexisting research in this domain and inspire future investigations into the\ninterpretability of visual recognition models.", "AI": {"tldr": "A systematic review and taxonomy of interpretability methods for visual recognition models, focusing on human-centered criteria and future research opportunities.", "motivation": "To understand and improve the interpretability of visual recognition models for critical applications like autonomous driving and medical diagnostics.", "method": "Proposes a taxonomy categorizing interpretable methods by Intent, Object, Presentation, and Methodology, and reviews evaluation metrics and emerging technologies.", "result": "A coherent framework for grouping interpretability methods and insights into future research directions.", "conclusion": "The paper organizes existing research and highlights the potential for advancements in interpretability, especially with new technologies like large multimodal models."}}
{"id": "2507.11356", "pdf": "https://arxiv.org/pdf/2507.11356", "abs": "https://arxiv.org/abs/2507.11356", "authors": ["Alexis Brissard", "Fr\u00e9d\u00e9ric Cuppens", "Amal Zouaq"], "title": "What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models", "categories": ["cs.CL"], "comment": "12 pages, 7 figures, to be published in AI4BPM 2025 Proceedings", "summary": "Large Language Models (LLMs) are increasingly applied for Process Modeling\n(PMo) tasks such as Process Model Generation (PMG). To support these tasks,\nresearchers have introduced a variety of Process Model Representations (PMRs)\nthat serve as model abstractions or generation targets. However, these PMRs\ndiffer widely in structure, complexity, and usability, and have never been\nsystematically compared. Moreover, recent PMG approaches rely on distinct\nevaluation strategies and generation techniques, making comparison difficult.\nThis paper presents the first empirical study that evaluates multiple PMRs in\nthe context of PMo with LLMs. We introduce the PMo Dataset, a new dataset\ncontaining 55 process descriptions paired with models in nine different PMRs.\nWe evaluate PMRs along two dimensions: suitability for LLM-based PMo and\nperformance on PMG. \\textit{Mermaid} achieves the highest overall score across\nsix PMo criteria, whereas \\textit{BPMN text} delivers the best PMG results in\nterms of process element similarity.", "AI": {"tldr": "This paper evaluates nine Process Model Representations (PMRs) for Large Language Model (LLM)-based Process Modeling (PMo), introducing a new dataset and comparing PMRs on suitability and performance.", "motivation": "The lack of systematic comparison among PMRs and inconsistent evaluation strategies in Process Model Generation (PMG) motivated this study.", "method": "The study introduces the PMo Dataset (55 process descriptions paired with models in nine PMRs) and evaluates PMRs on suitability for LLM-based PMo and PMG performance.", "result": "Mermaid scored highest overall for PMo suitability, while BPMN text performed best in PMG for process element similarity.", "conclusion": "The study provides empirical insights into PMR performance, highlighting Mermaid and BPMN text as top choices for LLM-based PMo and PMG, respectively."}}
{"id": "2507.11102", "pdf": "https://arxiv.org/pdf/2507.11102", "abs": "https://arxiv.org/abs/2507.11102", "authors": ["Jie Yang", "Wang Zeng", "Sheng Jin", "Lumin Xu", "Wentao Liu", "Chen Qian", "Zhen Li", "Ruimao Zhang"], "title": "KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model", "categories": ["cs.CV"], "comment": "Extended Version of KptLLM. arXiv admin note: text overlap with\n  arXiv:2411.01846", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has revolutionized\nimage understanding by bridging textual and visual modalities. However, these\nmodels often struggle with capturing fine-grained semantic information, such as\nthe precise identification and analysis of object keypoints. Keypoints, as\nstructure-aware, pixel-level, and compact representations of objects,\nparticularly articulated ones, play a crucial role in applications such as\nfine-grained image analysis, object retrieval, and behavior recognition. In\nthis paper, we propose KptLLM++, a novel multimodal large language model that\nspecifically designed for generic keypoint comprehension through the\nintegration of diverse input modalities guided by user-defined instructions. By\nunifying keypoint detection across varied contexts, KptLLM++ establishes itself\nas an advanced interface, fostering more effective human-AI collaboration. The\nmodel is built upon a novel identify-then-detect paradigm, which first\ninterprets keypoint semantics and subsequently localizes their precise\npositions through a structured chain-of-thought reasoning mechanism. To push\nthe boundaries of performance, we have scaled up the training dataset to over\n500K samples, encompassing diverse objects, keypoint categories, image styles,\nand scenarios with complex occlusions. This extensive scaling enables KptLLM++\nto unlock its potential, achieving remarkable accuracy and generalization.\nComprehensive experiments on multiple keypoint detection benchmarks demonstrate\nits state-of-the-art performance, underscoring its potential as a unified\nsolution for fine-grained image understanding and its transformative\nimplications for human-AI interaction.", "AI": {"tldr": "KptLLM++ is a multimodal large language model designed for fine-grained keypoint comprehension, achieving state-of-the-art performance through a novel identify-then-detect paradigm and extensive training.", "motivation": "Existing MLLMs struggle with fine-grained semantic information like keypoints, which are crucial for tasks like image analysis and behavior recognition.", "method": "KptLLM++ uses an identify-then-detect approach with structured chain-of-thought reasoning and is trained on 500K diverse samples.", "result": "The model achieves remarkable accuracy and generalization, outperforming benchmarks in keypoint detection.", "conclusion": "KptLLM++ is a unified solution for fine-grained image understanding, enhancing human-AI collaboration."}}
{"id": "2507.11384", "pdf": "https://arxiv.org/pdf/2507.11384", "abs": "https://arxiv.org/abs/2507.11384", "authors": ["Xia Cui"], "title": "Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss", "categories": ["cs.CL"], "comment": "10 pages, 1 figure, SemEval 2025", "summary": "This paper explores the application of a simple weighted loss function to\nTransformer-based models for multi-label emotion detection in SemEval-2025\nShared Task 11. Our approach addresses data imbalance by dynamically adjusting\nclass weights, thereby enhancing performance on minority emotion classes\nwithout the computational burden of traditional resampling methods. We evaluate\nBERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such\nas Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.\nThe results demonstrate that the weighted loss function improves performance on\nhigh-frequency emotion classes but shows limited impact on minority classes.\nThese findings underscore both the effectiveness and the challenges of applying\nthis approach to imbalanced multi-label emotion detection.", "AI": {"tldr": "A weighted loss function is applied to Transformer models for multi-label emotion detection, improving performance on high-frequency classes but with limited impact on minority classes.", "motivation": "Address data imbalance in multi-label emotion detection without the computational cost of traditional resampling methods.", "method": "Use a dynamically adjusted weighted loss function with BERT, RoBERTa, and BART on the BRIGHTER dataset, evaluated via Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity.", "result": "Improved performance on high-frequency emotion classes; limited impact on minority classes.", "conclusion": "The weighted loss function is effective but faces challenges in addressing minority class performance in imbalanced datasets."}}
{"id": "2507.11116", "pdf": "https://arxiv.org/pdf/2507.11116", "abs": "https://arxiv.org/abs/2507.11116", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha", "Mostofa Kamal Nasir"], "title": "Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach", "categories": ["cs.CV"], "comment": "This paper has been accepted at the IEEE QPAIN 2025. The final\n  version will be available in the IEEE Xplore Digital Library", "summary": "Jellyfish, a diverse group of gelatinous marine organisms, play a crucial\nrole in maintaining marine ecosystems but pose significant challenges for\nbiodiversity and conservation due to their rapid proliferation and ecological\nimpact. Accurate identification of jellyfish species is essential for\necological monitoring and management. In this study, we proposed a deep\nlearning framework for jellyfish species detection and classification using an\nunderwater image dataset. The framework integrates advanced feature extraction\ntechniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,\ncombined with seven traditional machine learning classifiers and three\nFeedforward Neural Network classifiers for precise species identification.\nAdditionally, we activated the softmax function to directly classify jellyfish\nspecies using the convolutional neural network models. The combination of the\nArtificial Neural Network with MobileNetV3 is our best-performing model,\nachieving an exceptional accuracy of 98%, significantly outperforming other\nfeature extractor-classifier combinations. This study demonstrates the efficacy\nof deep learning and hybrid frameworks in addressing biodiversity challenges\nand advancing species detection in marine environments.", "AI": {"tldr": "A deep learning framework for jellyfish species detection using underwater images, combining advanced feature extraction and classifiers, achieves 98% accuracy with MobileNetV3 and ANN.", "motivation": "Accurate jellyfish species identification is vital for ecological monitoring and management, addressing biodiversity challenges.", "method": "Integrated MobileNetV3, ResNet50, EfficientNetV2-B0, VGG16 with traditional ML and FNN classifiers, using softmax for CNN-based classification.", "result": "MobileNetV3 with ANN achieved 98% accuracy, outperforming other combinations.", "conclusion": "Deep learning and hybrid frameworks effectively advance species detection in marine ecosystems."}}
{"id": "2507.11405", "pdf": "https://arxiv.org/pdf/2507.11405", "abs": "https://arxiv.org/abs/2507.11405", "authors": ["Cheng Xu", "Nan Yan", "Shuhao Guan", "Changhong Jin", "Yuke Mei", "Yibing Guo", "M-Tahar Kechadi"], "title": "DCR: Quantifying Data Contamination in LLMs Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data, inflating performance metrics and undermining genuine\ngeneralization assessment. This paper introduces the Data Contamination Risk\n(DCR) framework, a lightweight, interpretable pipeline designed to detect and\nquantify BDC across four granular levels: semantic, informational, data, and\nlabel. By synthesizing contamination scores via a fuzzy inference system, DCR\nproduces a unified DCR Factor that adjusts raw accuracy to reflect\ncontamination-aware performance. Validated on 9 LLMs (0.5B-72B) across\nsentiment analysis, fake news detection, and arithmetic reasoning tasks, the\nDCR framework reliably diagnoses contamination severity and with accuracy\nadjusted using the DCR Factor to within 4% average error across the three\nbenchmarks compared to the uncontaminated baseline. Emphasizing computational\nefficiency and transparency, DCR provides a practical tool for integrating\ncontamination assessment into routine evaluations, fostering fairer comparisons\nand enhancing the credibility of LLM benchmarking practices.", "AI": {"tldr": "The paper introduces the Data Contamination Risk (DCR) framework to detect and quantify benchmark data contamination in LLMs, adjusting performance metrics for fairer comparisons.", "motivation": "Concerns about LLMs memorizing evaluation data (benchmark data contamination) inflating performance metrics and undermining genuine generalization assessment.", "method": "DCR framework detects contamination at four granular levels (semantic, informational, data, label) and synthesizes scores via a fuzzy inference system to produce a unified DCR Factor.", "result": "Validated on 9 LLMs, DCR reliably diagnoses contamination severity and adjusts accuracy within 4% average error compared to uncontaminated baselines.", "conclusion": "DCR is a lightweight, interpretable tool for routine contamination assessment, enhancing LLM benchmarking credibility."}}
{"id": "2507.11119", "pdf": "https://arxiv.org/pdf/2507.11119", "abs": "https://arxiv.org/abs/2507.11119", "authors": ["Hankun Liu", "Yujian Zhao", "Guanglin Niu"], "title": "Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID", "categories": ["cs.CV"], "comment": null, "summary": "Hard samples pose a significant challenge in person re-identification (ReID)\ntasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent\nambiguity or similarity, coupled with the lack of explicit definitions, makes\nthem a fundamental bottleneck. These issues not only limit the design of\ntargeted learning strategies but also diminish the model's robustness under\nclothing or viewpoint changes. In this paper, we propose a novel\nmultimodal-guided Hard Sample Generation and Learning (HSGL) framework, which\nis the first effort to unify textual and visual modalities to explicitly\ndefine, generate, and optimize hard samples within a unified paradigm. HSGL\ncomprises two core components: (1) Dual-Granularity Hard Sample Generation\n(DGHSG), which leverages multimodal cues to synthesize semantically consistent\nsamples, including both coarse- and fine-grained hard positives and negatives\nfor effectively increasing the hardness and diversity of the training data. (2)\nHard Sample Adaptive Learning (HSAL), which introduces a hardness-aware\noptimization strategy that adjusts feature distances based on textual semantic\nlabels, encouraging the separation of hard positives and drawing hard negatives\ncloser in the embedding space to enhance the model's discriminative capability\nand robustness to hard samples. Extensive experiments on multiple CC-ReID\nbenchmarks demonstrate the effectiveness of our approach and highlight the\npotential of multimodal-guided hard sample generation and learning for robust\nCC-ReID. Notably, HSAL significantly accelerates the convergence of the\ntargeted learning procedure and achieves state-of-the-art performance on both\nPRCC and LTCC datasets. The code is available at\nhttps://github.com/undooo/TryHarder-ACMMM25.", "AI": {"tldr": "The paper introduces HSGL, a multimodal framework for defining, generating, and optimizing hard samples in clothing-changing person Re-ID, improving model robustness and performance.", "motivation": "Hard samples in CC-ReID are ambiguous and lack explicit definitions, limiting learning strategies and model robustness.", "method": "HSGL combines Dual-Granularity Hard Sample Generation (DGHSG) for synthesizing diverse hard samples and Hard Sample Adaptive Learning (HSAL) for hardness-aware optimization.", "result": "HSGL achieves state-of-the-art performance on PRCC and LTCC datasets, accelerating convergence.", "conclusion": "Multimodal-guided hard sample generation and learning enhance CC-ReID robustness and discriminative capability."}}
{"id": "2507.11407", "pdf": "https://arxiv.org/pdf/2507.11407", "abs": "https://arxiv.org/abs/2507.11407", "authors": ["LG AI Research", ":", "Kyunghoon Bae", "Eunbi Choi", "Kibong Choi", "Stanley Jungkyu Choi", "Yemuk Choi", "Kyubeen Han", "Seokhee Hong", "Junwon Hwang", "Taewan Hwang", "Joonwon Jang", "Hyojin Jeon", "Kijeong Jeon", "Gerrard Jeongwon Jo", "Hyunjik Jo", "Jiyeon Jung", "Euisoon Kim", "Hyosang Kim", "Jihoon Kim", "Joonkee Kim", "Seonghwan Kim", "Soyeon Kim", "Sunkyoung Kim", "Yireun Kim", "Yongil Kim", "Youchul Kim", "Edward Hwayoung Lee", "Gwangho Lee", "Haeju Lee", "Honglak Lee", "Jinsik Lee", "Kyungmin Lee", "Sangha Park", "Young Min Paik", "Yongmin Park", "Youngyong Park", "Sanghyun Seo", "Sihoon Yang", "Heuiyeen Yeen", "Sihyuk Yi", "Hyeongu Yun"], "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes", "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report, 30 Pages", "summary": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning\nmode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5\nand the advanced reasoning abilities of EXAONE Deep. To pave the way for the\nagentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool\nuse, and its multilingual capabilities are extended to support Spanish in\naddition to English and Korean. The EXAONE 4.0 model series consists of two\nsizes: a mid-size 32B model optimized for high performance, and a small-size\n1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates\nsuperior performance compared to open-weight models in its class and remains\ncompetitive even against frontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE.", "AI": {"tldr": "EXAONE 4.0 integrates Non-reasoning and Reasoning modes, enhances multilingual support, and offers two model sizes for varied applications, outperforming open-weight models.", "motivation": "To combine usability and advanced reasoning for agentic AI, while expanding multilingual capabilities.", "method": "Incorporates agentic tool use and supports Spanish, English, and Korean. Offers 32B and 1.2B model sizes.", "result": "Superior performance vs. open-weight models, competitive with frontier-class models.", "conclusion": "EXAONE 4.0 is a versatile, high-performing model series available for research."}}
{"id": "2507.11129", "pdf": "https://arxiv.org/pdf/2507.11129", "abs": "https://arxiv.org/abs/2507.11129", "authors": ["Zhifeng Gu", "Bing Wang"], "title": "MMOne: Representing Multiple Modalities in One Scene", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "Humans perceive the world through multimodal cues to understand and interact\nwith the environment. Learning a scene representation for multiple modalities\nenhances comprehension of the physical world. However, modality conflicts,\narising from inherent distinctions among different modalities, present two\ncritical challenges: property disparity and granularity disparity. To address\nthese challenges, we propose a general framework, MMOne, to represent multiple\nmodalities in one scene, which can be readily extended to additional\nmodalities. Specifically, a modality modeling module with a novel modality\nindicator is proposed to capture the unique properties of each modality.\nAdditionally, we design a multimodal decomposition mechanism to separate\nmulti-modal Gaussians into single-modal Gaussians based on modality\ndifferences. We address the essential distinctions among modalities by\ndisentangling multimodal information into shared and modality-specific\ncomponents, resulting in a more compact and efficient multimodal scene\nrepresentation. Extensive experiments demonstrate that our method consistently\nenhances the representation capability for each modality and is scalable to\nadditional modalities. The code is available at\nhttps://github.com/Neal2020GitHub/MMOne.", "AI": {"tldr": "MMOne is a framework for multimodal scene representation, addressing modality conflicts via a modality modeling module and decomposition mechanism.", "motivation": "To enhance comprehension of the physical world by learning scene representations for multiple modalities, despite challenges like property and granularity disparities.", "method": "Proposes MMOne with a modality modeling module and multimodal decomposition mechanism to disentangle shared and modality-specific components.", "result": "Improves representation capability for each modality and scales to additional modalities.", "conclusion": "MMOne effectively addresses modality conflicts, offering a compact and efficient multimodal scene representation."}}
{"id": "2507.11408", "pdf": "https://arxiv.org/pdf/2507.11408", "abs": "https://arxiv.org/abs/2507.11408", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Saptarshi Saha", "Utpal Garain", "Nicholas Asher"], "title": "KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "15 pages, 9 figures", "summary": "Chain-of-thought traces have been shown to improve performance of large\nlanguage models in a plethora of reasoning tasks, yet there is no consensus on\nthe mechanism through which this performance boost is achieved. To shed more\nlight on this, we introduce Causal CoT Graphs (CCGs), which are directed\nacyclic graphs automatically extracted from reasoning traces that model\nfine-grained causal dependencies in the language model output. A collection of\n$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their\nassociated CCGs are compiled into our dataset -- \\textbf{KisMATH}. Our detailed\nempirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in\nthe CCG are mediators for the final answer, a condition necessary for\nreasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating\nthat models internally realise structures akin to our graphs. KisMATH enables\ncontrolled, graph-aligned interventions and opens up avenues for further\ninvestigation into the role of chain-of-thought in LLM reasoning.", "AI": {"tldr": "The paper introduces Causal CoT Graphs (CCGs) to analyze chain-of-thought reasoning in LLMs, showing they mediate answers and align with model reasoning paths.", "motivation": "To understand how chain-of-thought traces improve LLM performance in reasoning tasks.", "method": "Develop CCGs from reasoning traces, compile KisMATH dataset (1671 problems), and analyze 15 LLMs.", "result": "CCG nodes mediate answers, and LLMs align with CCG paths, suggesting internal reasoning structures.", "conclusion": "KisMATH enables controlled interventions and further study of chain-of-thought in LLMs."}}
{"id": "2507.11143", "pdf": "https://arxiv.org/pdf/2507.11143", "abs": "https://arxiv.org/abs/2507.11143", "authors": ["Lam Pham", "Cam Le", "Hieu Tang", "Khang Truong", "Truong Nguyen", "Jasmin Lampert", "Alexander Schindler", "Martin Boyer", "Son Phan"], "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "In recent years, landslide disasters have reported frequently due to the\nextreme weather events of droughts, floods , storms, or the consequence of\nhuman activities such as deforestation, excessive exploitation of natural\nresources. However, automatically observing landslide is challenging due to the\nextremely large observing area and the rugged topography such as mountain or\nhighland. This motivates us to propose an end-to-end deep-learning-based model\nwhich explores the remote sensing images for automatically observing landslide\nevents. By considering remote sensing images as the input data, we can obtain\nfree resource, observe large and rough terrains by time. To explore the remote\nsensing images, we proposed a novel neural network architecture which is for\ntwo tasks of landslide detection and landslide segmentation. We evaluated our\nproposed model on three different benchmark datasets of LandSlide4Sense, Bijie,\nand Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,\n93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU\nscores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,\nNepal datasets. These experimental results prove potential to integrate our\nproposed model into real-life landslide observation systems.", "AI": {"tldr": "Proposes an end-to-end deep-learning model for automatic landslide observation using remote sensing images, achieving high accuracy in detection and segmentation tasks.", "motivation": "Frequent landslide disasters due to extreme weather and human activities, coupled with challenges in large-scale observation, drive the need for automated solutions.", "method": "A novel neural network architecture is designed for landslide detection and segmentation, leveraging remote sensing images as input.", "result": "High F1 scores (98.23, 93.83) for detection and mIoU scores (63.74, 76.88) for segmentation on benchmark datasets.", "conclusion": "The model shows potential for real-life landslide observation systems."}}
{"id": "2507.11412", "pdf": "https://arxiv.org/pdf/2507.11412", "abs": "https://arxiv.org/abs/2507.11412", "authors": ["Orion Weller", "Kathryn Ricci", "Marc Marone", "Antoine Chaffin", "Dawn Lawrie", "Benjamin Van Durme"], "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.", "AI": {"tldr": "The paper introduces the Ettin suite, paired encoder-only and decoder-only models, to fairly compare their performance on tasks like classification and generation, showing each excels in its domain.", "motivation": "To address the lack of fair comparisons between encoder-only and decoder-only models due to differences in parameters, training, and datasets.", "method": "Developed the Ettin suite with paired models (17M to 1B parameters) trained uniformly on up to 2T tokens, ensuring comparable conditions.", "result": "Encoder-only models outperform in classification/retrieval, while decoder-only models excel in generation. Adapting models across tasks is less effective.", "conclusion": "Specialized architectures (encoder/decoder) are superior for their intended tasks, and cross-adaptation underperforms. All artifacts are open-sourced for future research."}}
{"id": "2507.11152", "pdf": "https://arxiv.org/pdf/2507.11152", "abs": "https://arxiv.org/abs/2507.11152", "authors": ["Duoyou Chen", "Yunqing Chen", "Can Zhang", "Zhou Wang", "Cheng Chen", "Ruoxiu Xiao"], "title": "Latent Space Consistency for Sparse-View CT Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "ACMMM2025 Accepted", "summary": "Computed Tomography (CT) is a widely utilized imaging modality in clinical\nsettings. Using densely acquired rotational X-ray arrays, CT can capture 3D\nspatial features. However, it is confronted with challenged such as significant\ntime consumption and high radiation exposure. CT reconstruction methods based\non sparse-view X-ray images have garnered substantial attention from\nresearchers as they present a means to mitigate costs and risks. In recent\nyears, diffusion models, particularly the Latent Diffusion Model (LDM), have\ndemonstrated promising potential in the domain of 3D CT reconstruction.\nNonetheless, due to the substantial differences between the 2D latent\nrepresentation of X-ray modalities and the 3D latent representation of CT\nmodalities, the vanilla LDM is incapable of achieving effective alignment\nwithin the latent space. To address this issue, we propose the Consistent\nLatent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature\ncontrastive learning to efficiently extract latent 3D information from 2D X-ray\nimages and achieve latent space alignment between modalities. Experimental\nresults indicate that CLS-DM outperforms classical and state-of-the-art\ngenerative models in terms of standard voxel-level metrics (PSNR, SSIM) on the\nLIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing\nthe effectiveness and economic viability of sparse X-ray reconstructed CT but\ncan also be generalized to other cross-modal transformation tasks, such as\ntext-to-image synthesis. We have made our code publicly available at\nhttps://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research\nand applications in other domains.", "AI": {"tldr": "The paper introduces CLS-DM, a model for 3D CT reconstruction from sparse 2D X-ray images, addressing latent space misalignment issues in vanilla LDM.", "motivation": "CT imaging faces challenges like high time and radiation costs. Sparse-view reconstruction is promising but struggles with latent space alignment between 2D X-ray and 3D CT data.", "method": "Proposes CLS-DM, using cross-modal contrastive learning to align latent spaces and extract 3D features from 2D X-rays.", "result": "CLS-DM outperforms existing models on LIDC-IDRI and CTSpine1K datasets in PSNR and SSIM metrics.", "conclusion": "CLS-DM improves sparse-view CT reconstruction and can generalize to other cross-modal tasks, with code made available for further research."}}
{"id": "2507.11423", "pdf": "https://arxiv.org/pdf/2507.11423", "abs": "https://arxiv.org/abs/2507.11423", "authors": ["Yanjian Zhang", "Guillaume Wisniewski", "Nadi Tomeh", "Thierry Charnois"], "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?", "categories": ["cs.CL"], "comment": null, "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.", "AI": {"tldr": "The paper explores whether prompting can control LLM reasoning strategies and evaluates its impact on problem-solving. While no single strategy consistently improves accuracy, adaptive strategy selection could enhance performance.", "motivation": "To address the limitation of LLMs favoring a single reasoning strategy, which may hinder their effectiveness in diverse reasoning tasks.", "method": "Investigates the impact of prompting on LLM reasoning strategies and proposes methods for adaptive strategy selection.", "result": "No single strategy consistently improves accuracy, but adaptive strategy selection shows potential for performance enhancement.", "conclusion": "Adaptive strategy selection can refine LLM reasoning abilities, offering new ways to improve their problem-solving effectiveness."}}
{"id": "2507.11153", "pdf": "https://arxiv.org/pdf/2507.11153", "abs": "https://arxiv.org/abs/2507.11153", "authors": ["Hongfei Ye", "Bin Chen", "Wenxi Liu", "Yu Zhang", "Zhao Li", "Dandan Ni", "Hongyang Chen"], "title": "Assessing Color Vision Test in Large Vision-language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the widespread adoption of large vision-language models, the capacity\nfor color vision in these models is crucial. However, the color vision\nabilities of large visual-language models have not yet been thoroughly\nexplored. To address this gap, we define a color vision testing task for large\nvision-language models and construct a dataset \\footnote{Anonymous Github\nShowing some of the data\nhttps://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers\nmultiple categories of test questions and tasks of varying difficulty levels.\nFurthermore, we analyze the types of errors made by large vision-language\nmodels and propose fine-tuning strategies to enhance their performance in color\nvision tests.", "AI": {"tldr": "The paper explores color vision abilities in large vision-language models, introduces a testing task, and proposes fine-tuning strategies to improve performance.", "motivation": "The color vision capabilities of large vision-language models are understudied, prompting the need for a systematic evaluation.", "method": "A color vision testing task is defined, and a diverse dataset is constructed. Error analysis is conducted, followed by fine-tuning strategies.", "result": "The study identifies common errors in models and suggests improvements through fine-tuning.", "conclusion": "Fine-tuning can enhance color vision performance in large vision-language models, addressing gaps in their capabilities."}}
{"id": "2507.11502", "pdf": "https://arxiv.org/pdf/2507.11502", "abs": "https://arxiv.org/abs/2507.11502", "authors": ["Sirui Han", "Junqi Zhu", "Ruiyuan Zhang", "Yike Guo"], "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong", "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": null, "summary": "This paper presents the development of HKGAI-V1, a foundational sovereign\nlarge language model (LLM), developed as part of an initiative to establish\nvalue-aligned AI infrastructure specifically tailored for Hong Kong. Addressing\nthe region's unique multilingual environment (Cantonese, Mandarin, and\nEnglish), its distinct socio-legal context under the \"one country, two systems\"\nframework, and specific local cultural and value considerations, the model is\nbuilt upon the DeepSeek architecture and systematically aligned with regional\nnorms through a multifaceted full parameter fine-tuning process. It is further\nintegrated with a retrieval-augmented generation (RAG) system to ensure timely\nand factually grounded information access. The core contribution lies in the\ndesign and implementation of a comprehensive, region-specific AI alignment and\nsafety framework, demonstrated through two key achievements: 1) The successful\ndevelopment of HKGAI-V1 itself - which outper-forms general-purpose models in\nhandling Hong Kong-specific culturally sensitive queries, and embodies a\n\"governance-embedded\" approach to digital sovereignty - empowers Hong Kong to\nexercise control over AI applications in critical sectors including public\nservices, legal systems, and edu-cation. 2) The development of the proprietary\nAdversarial HK Value Benchmark, a rigorous tool for evaluating model alignment\nwith local ethical and legal stand-ards under challenging conditions. By\ndocumenting these achievements, the paper provides not only a technological\nartifact but also a replicable blueprint for developing advanced, regionally\nfocused AI systems deeply rooted in their local identities.", "AI": {"tldr": "HKGAI-V1 is a sovereign LLM tailored for Hong Kong, addressing its multilingual and socio-legal context. It outperforms general models in local queries and includes a safety framework and adversarial benchmark.", "motivation": "To create a value-aligned AI for Hong Kong, addressing its unique linguistic, cultural, and legal needs under 'one country, two systems.'", "method": "Built on DeepSeek, fine-tuned for regional norms, and integrated with RAG for factual accuracy. Includes a governance-embedded approach and adversarial benchmark.", "result": "HKGAI-V1 excels in culturally sensitive queries and provides a replicable model for region-specific AI.", "conclusion": "The paper offers a blueprint for sovereign AI systems, emphasizing local alignment and governance."}}
{"id": "2507.11171", "pdf": "https://arxiv.org/pdf/2507.11171", "abs": "https://arxiv.org/abs/2507.11171", "authors": ["Jun Chen", "Yonghua Yu", "Weifu Li", "Yaohui Chen", "Hong Chen"], "title": "Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Citrus, as one of the most economically important fruit crops globally,\nsuffers severe yield depressions due to various diseases. Accurate disease\ndetection and classification serve as critical prerequisites for implementing\ntargeted control measures. Recent advancements in artificial intelligence,\nparticularly deep learning-based computer vision algorithms, have substantially\ndecreased time and labor requirements while maintaining the accuracy of\ndetection and classification. Nevertheless, these methods predominantly rely on\nmassive, high-quality annotated training examples to attain promising\nperformance. By introducing two key designs: contrasting with cluster centroids\nand a multi-layer contrastive training (MCT) paradigm, this paper proposes a\nnovel clustering-guided self-supervised multi-layer contrastive representation\nlearning (CMCRL) algorithm. The proposed method demonstrates several advantages\nover existing counterparts: (1) optimizing with massive unannotated samples;\n(2) effective adaptation to the symptom similarity across distinct citrus\ndiseases; (3) hierarchical feature representation learning. The proposed method\nachieves state-of-the-art performance on the public citrus image set CDD,\noutperforming existing methods by 4.5\\%-30.1\\% accuracy. Remarkably, our method\nnarrows the performance gap with fully supervised counterparts (all samples are\nlabeled). Beyond classification accuracy, our method shows great performance on\nother evaluation metrics (F1 score, precision, and recall), highlighting the\nrobustness against the class imbalance challenge.", "AI": {"tldr": "A novel self-supervised learning method (CMCRL) for citrus disease detection outperforms existing methods by leveraging unannotated data and hierarchical feature learning.", "motivation": "Citrus diseases cause significant yield losses; accurate detection is crucial but relies on costly annotated data. The paper aims to reduce dependency on labeled data while improving detection.", "method": "Proposes CMCRL, combining clustering-guided contrastive learning and multi-layer contrastive training (MCT) to learn hierarchical features from unannotated data.", "result": "Achieves state-of-the-art performance on CDD dataset (4.5%-30.1% accuracy boost) and excels in F1, precision, and recall, addressing class imbalance.", "conclusion": "CMCRL offers a robust, label-efficient solution for citrus disease detection, narrowing the gap with fully supervised methods."}}
{"id": "2507.11508", "pdf": "https://arxiv.org/pdf/2507.11508", "abs": "https://arxiv.org/abs/2507.11508", "authors": ["Patr\u00edcia Schmidtov\u00e1", "Ond\u0159ej Du\u0161ek", "Saad Mahamood"], "title": "Real-World Summarization: When Evaluation Reaches Its Limits", "categories": ["cs.CL"], "comment": null, "summary": "We examine evaluation of faithfulness to input data in the context of hotel\nhighlights: brief LLM-generated summaries that capture unique features of\naccommodations. Through human evaluation campaigns involving categorical error\nassessment and span-level annotation, we compare traditional metrics, trainable\nmethods, and LLM-as-a-judge approaches. Our findings reveal that simpler\nmetrics like word overlap correlate surprisingly well with human judgments\n(Spearman correlation rank of 0.63), often outperforming more complex methods\nwhen applied to out-of-domain data. We further demonstrate that while LLMs can\ngenerate high-quality highlights, they prove unreliable for evaluation as they\ntend to severely under- or over-annotate. Our analysis of real-world business\nimpacts shows incorrect and non-checkable information pose the greatest risks.\nWe also highlight challenges in crowdsourced evaluations.", "AI": {"tldr": "Simpler metrics like word overlap correlate well with human judgments in evaluating LLM-generated hotel highlights, outperforming complex methods. LLMs are unreliable for evaluation, and incorrect/non-checkable information poses risks.", "motivation": "To evaluate faithfulness of LLM-generated hotel highlights to input data and compare evaluation methods.", "method": "Human evaluation campaigns with categorical error assessment and span-level annotation, comparing traditional metrics, trainable methods, and LLM-as-a-judge approaches.", "result": "Word overlap metrics correlate well with human judgments (Spearman 0.63), outperforming complex methods. LLMs are unreliable evaluators, often under- or over-annotating. Incorrect/non-checkable information poses risks.", "conclusion": "Simpler metrics are effective for faithfulness evaluation, while LLMs are unreliable for this task. Challenges in crowdsourced evaluations and risks of incorrect information are highlighted."}}
{"id": "2507.11200", "pdf": "https://arxiv.org/pdf/2507.11200", "abs": "https://arxiv.org/abs/2507.11200", "authors": ["Che Liu", "Jiazhen Pan", "Weixiang Shen", "Wenjia Bai", "Daniel Rueckert", "Rossella Arcucci"], "title": "How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study", "categories": ["cs.CV"], "comment": "Accepted by the International Conference on AI in Healthcare 2025", "summary": "Vision-Language Models (VLMs) trained on web-scale corpora excel at natural\nimage tasks and are increasingly repurposed for healthcare; however, their\ncompetence in medical tasks remains underexplored. We present a comprehensive\nevaluation of open-source general-purpose and medically specialised VLMs,\nranging from 3B to 72B parameters, across eight benchmarks: MedXpert,\nOmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model\nperformance across different aspects, we first separate it into understanding\nand reasoning components. Three salient findings emerge. First, large\ngeneral-purpose models already match or surpass medical-specific counterparts\non several benchmarks, demonstrating strong zero-shot transfer from natural to\nmedical images. Second, reasoning performance is consistently lower than\nunderstanding, highlighting a critical barrier to safe decision support. Third,\nperformance varies widely across benchmarks, reflecting differences in task\ndesign, annotation quality, and knowledge demands. No model yet reaches the\nreliability threshold for clinical deployment, underscoring the need for\nstronger multimodal alignment and more rigorous, fine-grained evaluation\nprotocols.", "AI": {"tldr": "The paper evaluates general-purpose and medical-specific Vision-Language Models (VLMs) on medical benchmarks, finding that large general-purpose models often outperform specialized ones, though reasoning lags behind understanding, and no model is clinically reliable yet.", "motivation": "To assess the competence of VLMs in medical tasks, given their increasing use in healthcare despite underexplored performance.", "method": "Comprehensive evaluation of VLMs (3B to 72B parameters) across eight benchmarks, separating performance into understanding and reasoning components.", "result": "General-purpose VLMs match or surpass medical-specific ones; reasoning is weaker than understanding; performance varies by benchmark.", "conclusion": "No model is clinically reliable yet, highlighting the need for better multimodal alignment and evaluation protocols."}}
{"id": "2507.10559", "pdf": "https://arxiv.org/pdf/2507.10559", "abs": "https://arxiv.org/abs/2507.10559", "authors": ["Shomir Wilson"], "title": "NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about LLMs' capabilities and limitations. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research.", "AI": {"tldr": "The paper provides recommendations for effectively communicating about LLMs' capabilities and limitations to the general public, focusing on vague terminology, unreasonable expectations, and ethical failures.", "motivation": "To address the growing public interest in NLP and LLMs, ensuring accurate and transparent communication to foster understanding and support for research.", "method": "Analyzes published NLP research and popular news coverage to identify challenges (vague terminology, unrealistic expectations, ethical issues) and offers recommendations.", "result": "Highlights the need for clear communication to avoid misunderstandings and sustain public support for NLP advancements.", "conclusion": "Effective communication about LLMs is crucial for public understanding and continued research support, addressing terminology, expectations, and ethics."}}
{"id": "2507.11202", "pdf": "https://arxiv.org/pdf/2507.11202", "abs": "https://arxiv.org/abs/2507.11202", "authors": ["Xinkui Zhao", "Jinsong Shu", "Yangyang Wu", "Guanjie Cheng", "Zihe Liu", "Naibo Wang", "Shuiguang Deng", "Zhongle Xie", "Jianwei Yin"], "title": "A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Emotion Recognition (MER) often encounters incomplete\nmultimodality in practical applications due to sensor failures or privacy\nprotection requirements. While existing methods attempt to address various\nincomplete multimodal scenarios by balancing the training of each modality\ncombination through additional gradients, these approaches face a critical\nlimitation: training gradients from different modality combinations conflict\nwith each other, ultimately degrading the performance of the final prediction\nmodel. In this paper, we propose a unimodal decoupled dynamic low-rank\nadaptation method based on modality combinations, named MCULoRA, which is a\nnovel framework for the parameter-efficient training of incomplete multimodal\nlearning models. MCULoRA consists of two key modules, modality combination\naware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The\nMCLA module effectively decouples the shared information from the distinct\ncharacteristics of individual modality combinations. The DPFT module adjusts\nthe training ratio of modality combinations based on the separability of each\nmodality's representation space, optimizing the learning efficiency across\ndifferent modality combinations. Our extensive experimental evaluation in\nmultiple benchmark datasets demonstrates that MCULoRA substantially outperforms\nprevious incomplete multimodal learning approaches in downstream task accuracy.", "AI": {"tldr": "MCULoRA is a novel framework for incomplete multimodal learning, addressing gradient conflicts in MER by decoupling shared and distinct modality information and dynamically fine-tuning parameters.", "motivation": "Existing MER methods suffer from gradient conflicts due to incomplete modalities, degrading performance.", "method": "MCULoRA uses two modules: MCLA for decoupling modality information and DPFT for dynamic parameter fine-tuning.", "result": "MCULoRA outperforms previous methods in downstream task accuracy on benchmark datasets.", "conclusion": "MCULoRA effectively improves MER performance by optimizing learning efficiency across incomplete modalities."}}
{"id": "2507.10571", "pdf": "https://arxiv.org/pdf/2507.10571", "abs": "https://arxiv.org/abs/2507.10571", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust", "AI": {"tldr": "A modular AI framework integrates multimodal agents with a reasoning orchestrator and RAG for trust-aware, zero-shot visual classification, improving accuracy by 77.94% in apple leaf disease diagnosis.", "motivation": "Addressing trust challenges in zero-shot AI settings, especially for visual and language understanding tasks.", "method": "Combines generalist multimodal agents, a non-visual reasoning orchestrator, and RAG, tested in three configurations: zero-shot, fine-tuned, and trust-calibrated.", "result": "Achieves 85.63% accuracy with trust-aware orchestration and RAG, with GPT-4o showing better calibration than Qwen-2.5-VL.", "conclusion": "The framework enhances trust, scalability, and interpretability in multi-agent AI, applicable to diagnostics and biology, with open-source release for reproducibility."}}
{"id": "2507.11245", "pdf": "https://arxiv.org/pdf/2507.11245", "abs": "https://arxiv.org/abs/2507.11245", "authors": ["X. Feng", "H. Yu", "M. Wu", "S. Hu", "J. Chen", "C. Zhu", "J. Wu", "X. Chu", "K. Huang"], "title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models", "categories": ["cs.CV"], "comment": "Project Page: https://amap-ml.github.io/NarrLV-Website/", "summary": "With the rapid development of foundation video generation technologies, long\nvideo generation models have exhibited promising research potential thanks to\nexpanded content creation space. Recent studies reveal that the goal of long\nvideo generation tasks is not only to extend video duration but also to\naccurately express richer narrative content within longer videos. However, due\nto the lack of evaluation benchmarks specifically designed for long video\ngeneration models, the current assessment of these models primarily relies on\nbenchmarks with simple narrative prompts (e.g., VBench). To the best of our\nknowledge, our proposed NarrLV is the first benchmark to comprehensively\nevaluate the Narrative expression capabilities of Long Video generation models.\nInspired by film narrative theory, (i) we first introduce the basic narrative\nunit maintaining continuous visual presentation in videos as Temporal Narrative\nAtom (TNA), and use its count to quantitatively measure narrative richness.\nGuided by three key film narrative elements influencing TNA changes, we\nconstruct an automatic prompt generation pipeline capable of producing\nevaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based\non the three progressive levels of narrative content expression, we design an\neffective evaluation metric using the MLLM-based question generation and\nanswering framework. (iii) Finally, we conduct extensive evaluations on\nexisting long video generation models and the foundation generation models.\nExperimental results demonstrate that our metric aligns closely with human\njudgments. The derived evaluation outcomes reveal the detailed capability\nboundaries of current video generation models in narrative content expression.", "AI": {"tldr": "The paper introduces NarrLV, the first benchmark for evaluating narrative expression in long video generation models, using Temporal Narrative Atoms (TNAs) and a novel MLLM-based metric.", "motivation": "Current benchmarks lack focus on narrative richness in long videos, limiting evaluation of models' ability to express complex narratives.", "method": "Proposes TNAs to measure narrative richness, an automatic prompt generation pipeline, and an MLLM-based evaluation metric.", "result": "NarrLV aligns with human judgments and reveals capability boundaries of current models in narrative expression.", "conclusion": "NarrLV provides a comprehensive benchmark for assessing narrative capabilities in long video generation, filling a critical gap in evaluation."}}
{"id": "2507.10576", "pdf": "https://arxiv.org/pdf/2507.10576", "abs": "https://arxiv.org/abs/2507.10576", "authors": ["Bhakti Khera", "Rezvan Alamian", "Pascal A. Scherz", "Stephan M. Goetz"], "title": "Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.ET"], "comment": "39 pages, 21 figures", "summary": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions.", "AI": {"tldr": "The paper evaluates LLMs on the European Qualifying Examination (EQE) for patent attorneys, finding none meet professional standards. GPT-4o performed best, but human experts highlighted shortcomings in clarity and rationale. The study calls for improvements in logical consistency, multimodality, and adaptive prompting.", "motivation": "To quantitatively assess the performance of LLMs in legal applications, specifically the EQE, and identify gaps between model capabilities and professional requirements.", "method": "Evaluated open-source and proprietary LLMs (GPT-series, Anthropic, Deepseek, Llama-3 variants) on EQE tasks, measuring accuracy and F1 scores. Human experts assessed textual justifications.", "result": "GPT-4o led with 0.82 accuracy, but no model met the 0.90 threshold for professional standards. Human experts found models lacking in clarity and legal rationale. Outputs were sensitive to temperature and prompt changes.", "conclusion": "Current LLMs fall short of human-level patent proficiency. Future work should focus on logical consistency, multimodality, and adaptive prompting. Public perception of LLM performance may be overly optimistic."}}
{"id": "2507.11247", "pdf": "https://arxiv.org/pdf/2507.11247", "abs": "https://arxiv.org/abs/2507.11247", "authors": ["Veronika Shilova", "Emmanuel Malherbe", "Giovanni Palma", "Laurent Risser", "Jean-Michel Loubes"], "title": "Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Within a legal framework, fairness in datasets and models is typically\nassessed by dividing observations into predefined groups and then computing\nfairness measures (e.g., Disparate Impact or Equality of Odds with respect to\ngender). However, when sensitive attributes such as skin color are continuous,\ndividing into default groups may overlook or obscure the discrimination\nexperienced by certain minority subpopulations. To address this limitation, we\npropose a fairness-based grouping approach for continuous (possibly\nmultidimensional) sensitive attributes. By grouping data according to observed\nlevels of discrimination, our method identifies the partition that maximizes a\nnovel criterion based on inter-group variance in discrimination, thereby\nisolating the most critical subgroups.\n  We validate the proposed approach using multiple synthetic datasets and\ndemonstrate its robustness under changing population distributions - revealing\nhow discrimination is manifested within the space of sensitive attributes.\nFurthermore, we examine a specialized setting of monotonic fairness for the\ncase of skin color. Our empirical results on both CelebA and FFHQ, leveraging\nthe skin tone as predicted by an industrial proprietary algorithm, show that\nthe proposed segmentation uncovers more nuanced patterns of discrimination than\npreviously reported, and that these findings remain stable across datasets for\na given model. Finally, we leverage our grouping model for debiasing purpose,\naiming at predicting fair scores with group-by-group post-processing. The\nresults demonstrate that our approach improves fairness while having minimal\nimpact on accuracy, thus confirming our partition method and opening the door\nfor industrial deployment.", "AI": {"tldr": "The paper proposes a fairness-based grouping method for continuous sensitive attributes to better identify discrimination in datasets, validated on synthetic and real-world data.", "motivation": "Existing fairness assessments often overlook discrimination in continuous sensitive attributes like skin color due to predefined group divisions.", "method": "A novel grouping approach maximizes inter-group variance in discrimination to isolate critical subgroups, validated on synthetic and real datasets (CelebA, FFHQ).", "result": "The method uncovers nuanced discrimination patterns, remains stable across datasets, and improves fairness with minimal accuracy loss.", "conclusion": "The approach effectively addresses limitations in fairness assessment for continuous attributes and shows promise for industrial deployment."}}
{"id": "2507.10579", "pdf": "https://arxiv.org/pdf/2507.10579", "abs": "https://arxiv.org/abs/2507.10579", "authors": ["Ekaterina Kochmar", "Kaushal Kumar Maurya", "Kseniia Petukhova", "KV Aditya Srivatsa", "Ana\u00efs Tack", "Justin Vasselli"], "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications", "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.", "AI": {"tldr": "The paper evaluates AI tutors powered by LLMs in educational dialogues, focusing on mistake remediation. Over 50 teams participated, with results showing promise but room for improvement.", "motivation": "To assess and improve the pedagogical abilities of AI tutors in identifying and remedying student mistakes, grounded in learning science principles.", "method": "A shared task with five tracks evaluated AI tutors on mistake identification, location, guidance, feedback actionability, and tutor identity detection. Models were compared to human annotations.", "result": "Best F1 scores ranged from 58.34 (guidance) to 71.81 (mistake identification), with tutor identification scoring 96.98.", "conclusion": "While promising, AI tutors need further improvement. Resources are shared to support future research."}}
{"id": "2507.11252", "pdf": "https://arxiv.org/pdf/2507.11252", "abs": "https://arxiv.org/abs/2507.11252", "authors": ["Guanghao Wu", "Chen Xu", "Hai Song", "Chong Wang", "Qixing Zhang"], "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection", "categories": ["cs.CV", "eess.IV"], "comment": "18 pages, 11 figures", "summary": "Smoke is the first visible indicator of a wildfire.With the advancement of\ndeep learning, image-based smoke detection has become a crucial method for\ndetecting and preventing forest fires. However, the scarcity of smoke image\ndata from forest fires is one of the significant factors hindering the\ndetection of forest fire smoke. Image generation models offer a promising\nsolution for synthesizing realistic smoke images. However, current inpainting\nmodels exhibit limitations in generating high-quality smoke representations,\nparticularly manifesting as inconsistencies between synthesized smoke and\nbackground contexts. To solve these problems, we proposed a comprehensive\nframework for generating forest fire smoke images. Firstly, we employed the\npre-trained segmentation model and the multimodal model to obtain smoke masks\nand image captions.Then, to address the insufficient utilization of masks and\nmasked images by inpainting models, we introduced a network architecture guided\nby mask and masked image features. We also proposed a new loss function, the\nmask random difference loss, which enhances the consistency of the generated\neffects around the mask by randomly expanding and eroding the mask\nedges.Finally, to generate a smoke image dataset using random masks for\nsubsequent detection tasks, we incorporated smoke characteristics and use a\nmultimodal large language model as a filtering tool to select diverse and\nreasonable smoke images, thereby improving the quality of the synthetic\ndataset. Experiments showed that our generated smoke images are realistic and\ndiverse, and effectively enhance the performance of forest fire smoke detection\nmodels. Code is available at https://github.com/wghr123/MFGDiffusion.", "AI": {"tldr": "A framework for generating realistic forest fire smoke images using deep learning, addressing data scarcity and improving detection models.", "motivation": "The scarcity of smoke image data hinders forest fire detection. Current inpainting models lack quality in synthesizing smoke.", "method": "Uses pre-trained segmentation and multimodal models for masks/captions, introduces mask-guided architecture, and a new loss function (mask random difference loss). Generates diverse smoke images with a multimodal LLM filter.", "result": "Generated smoke images are realistic and diverse, enhancing forest fire smoke detection performance.", "conclusion": "The proposed framework effectively addresses data scarcity and improves smoke detection models."}}
{"id": "2507.10616", "pdf": "https://arxiv.org/pdf/2507.10616", "abs": "https://arxiv.org/abs/2507.10616", "authors": ["Neel Rajani", "Aryo Pradipta Gema", "Seraphina Goldfarb-Tarrant", "Ivan Titov"], "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones.", "AI": {"tldr": "Comparative analysis of RL and SFT for LLM reasoning training shows RL has minor in-domain gains, while SFT causes more pronounced changes and out-of-domain degradation. Freezing parts of the model yields inconclusive results.", "motivation": "To understand the training dynamics of RL and SFT for LLM reasoning tasks, particularly their impact on in-domain and out-of-domain performance.", "method": "Comparative analysis of RL and SFT on the same maths problems with the same model and similar hyperparameters, including parameter updates and freezing experiments.", "result": "RL shows minor in-domain gains and slight out-of-domain degradation, while SFT exhibits more pronounced trends. Freezing parts of the model yields mixed results.", "conclusion": "RL amplifies existing capabilities, while SFT replaces old skills with new ones, with freezing experiments providing inconclusive mitigation for out-of-domain degradation."}}
{"id": "2507.11261", "pdf": "https://arxiv.org/pdf/2507.11261", "abs": "https://arxiv.org/abs/2507.11261", "authors": ["Ronggang Huang", "Haoxin Yang", "Yan Cai", "Xuemiao Xu", "Huaidong Zhang", "Shengfeng He"], "title": "ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "3D visual grounding aims to identify and localize objects in a 3D space based\non textual descriptions. However, existing methods struggle with disentangling\ntargets from anchors in complex multi-anchor queries and resolving\ninconsistencies in spatial descriptions caused by perspective variations. To\ntackle these challenges, we propose ViewSRD, a framework that formulates 3D\nvisual grounding as a structured multi-view decomposition process. First, the\nSimple Relation Decoupling (SRD) module restructures complex multi-anchor\nqueries into a set of targeted single-anchor statements, generating a\nstructured set of perspective-aware descriptions that clarify positional\nrelationships. These decomposed representations serve as the foundation for the\nMulti-view Textual-Scene Interaction (Multi-TSI) module, which integrates\ntextual and scene features across multiple viewpoints using shared, Cross-modal\nConsistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a\nTextual-Scene Reasoning module synthesizes multi-view predictions into a\nunified and robust 3D visual grounding. Experiments on 3D visual grounding\ndatasets show that ViewSRD significantly outperforms state-of-the-art methods,\nparticularly in complex queries requiring precise spatial differentiation.", "AI": {"tldr": "ViewSRD improves 3D visual grounding by decomposing complex queries into simpler statements and integrating multi-view textual-scene interactions.", "motivation": "Existing methods struggle with disentangling targets in multi-anchor queries and handling spatial inconsistencies due to perspective variations.", "method": "ViewSRD uses Simple Relation Decoupling (SRD) to simplify queries, Multi-view Textual-Scene Interaction (Multi-TSI) for cross-modal integration, and a reasoning module for unified predictions.", "result": "ViewSRD outperforms state-of-the-art methods, especially in complex queries requiring precise spatial differentiation.", "conclusion": "The proposed framework effectively addresses challenges in 3D visual grounding by leveraging structured multi-view decomposition and cross-modal consistency."}}
{"id": "2507.10644", "pdf": "https://arxiv.org/pdf/2507.10644", "abs": "https://arxiv.org/abs/2507.10644", "authors": ["Tatiana Petrova", "Aleksandr Puzikov", "Boris Bliznukov", "Radu State"], "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.", "AI": {"tldr": "The paper provides a comprehensive evolutionary overview of the Web of Agents (WoA), linking modern protocols to historical standards and introducing a taxonomy to unify analysis. It highlights a paradigm shift in intelligence locus and calls for addressing socio-technical challenges in the WoA ecosystem.", "motivation": "The fragmented research across communities and the lack of a holistic understanding of the WoA's trajectory motivated this study to unify historical and modern perspectives.", "method": "The authors introduce a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism) to systematically analyze and compare agent architectures across generations.", "result": "The analysis reveals a paradigm shift in intelligence locus from external data or platforms to embedded agent models (LLMs), foundational for modern Agentic AI. It also identifies the need for addressing socio-technical challenges.", "conclusion": "While new protocols are necessary, they are insufficient for a robust WoA ecosystem. The paper proposes a research agenda focused on decentralized identity, economic models, security, and governance."}}
{"id": "2507.11267", "pdf": "https://arxiv.org/pdf/2507.11267", "abs": "https://arxiv.org/abs/2507.11267", "authors": ["Aon Safdar", "Usman Akram", "Waseem Anwar", "Basit Malik", "Mian Ibad Ali"], "title": "YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery", "categories": ["cs.CV", "cs.AI"], "comment": "Published in 25th Irish Machine Vision and Image Processing Conf.,\n  Galway, Ireland, Aug 30-Sep 1 2023 Also available at\n  https://doi.org/10.5281/zenodo.8264062", "summary": "Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared\n(TI) imagery in the defense and surveillance domain is a challenging computer\nvision (CV) task in comparison to the commercial autonomous vehicle perception\ndomain. Limited datasets, peculiar domain-specific and TI modality-specific\nchallenges, i.e., limited hardware, scale invariance issues due to greater\ndistances, deliberate occlusion by tactical vehicles, lower sensor resolution\nand resultant lack of structural information in targets, effects of weather,\ntemperature, and time of day variations, and varying target to clutter ratios\nall result in increased intra-class variability and higher inter-class\nsimilarity, making accurate real-time ATR a challenging CV task. Resultantly,\ncontemporary state-of-the-art (SOTA) deep learning architectures underperform\nin the ATR domain. We propose a modified anchor-based single-stage detector,\ncalled YOLOatr, based on a modified YOLOv5s, with optimal modifications to the\ndetection heads, feature fusion in the neck, and a custom augmentation profile.\nWe evaluate the performance of our proposed model on a comprehensive DSIAC MWIR\ndataset for real-time ATR over both correlated and decorrelated testing\nprotocols. The results demonstrate that our proposed model achieves\nstate-of-the-art ATR performance of up to 99.6%.", "AI": {"tldr": "YOLOatr, a modified YOLOv5s-based model, achieves 99.6% ATR performance on thermal infrared imagery, addressing challenges like limited datasets and domain-specific issues.", "motivation": "ATD and ATR in thermal infrared imagery face challenges like limited datasets, hardware constraints, and environmental variability, making accurate real-time detection difficult.", "method": "Proposes YOLOatr, a modified YOLOv5s with optimized detection heads, feature fusion, and custom augmentation for thermal infrared ATR.", "result": "Achieves state-of-the-art ATR performance (99.6%) on the DSIAC MWIR dataset under varied testing protocols.", "conclusion": "YOLOatr outperforms contemporary SOTA models, proving effective for real-time ATR in challenging thermal infrared scenarios."}}
{"id": "2507.10773", "pdf": "https://arxiv.org/pdf/2507.10773", "abs": "https://arxiv.org/abs/2507.10773", "authors": ["Samuel Rhys Cox"], "title": "Theory of Mind and Self-Disclosure to CUIs", "categories": ["cs.HC", "cs.CL"], "comment": "Workshop paper presented at ToMinHAI at CUI'2025: Theory of Mind in\n  Human-CUI Interaction, held in conjunction with the 2025 ACM conference on\n  Conversational User Interfaces, July 8th, 2025. 4 pages. 3 figures", "summary": "Self-disclosure is important to help us feel better, yet is often difficult.\nThis difficulty can arise from how we think people are going to react to our\nself-disclosure. In this workshop paper, we briefly discuss self-disclosure to\nconversational user interfaces (CUIs) in relation to various social cues. We\nthen, discuss how expressions of uncertainty or representation of a CUI's\nreasoning could help encourage self-disclosure, by making a CUI's intended\n\"theory of mind\" more transparent to users.", "AI": {"tldr": "The paper explores how social cues and expressions of uncertainty in conversational user interfaces (CUIs) can encourage self-disclosure by making the CUI's 'theory of mind' clearer to users.", "motivation": "Self-disclosure is challenging due to concerns about others' reactions. The study aims to understand how CUIs can facilitate self-disclosure through transparent social cues.", "method": "The paper briefly discusses self-disclosure to CUIs and examines the role of social cues, uncertainty expressions, and reasoning representation.", "result": "Expressions of uncertainty and transparent reasoning in CUIs may encourage self-disclosure by clarifying the CUI's 'theory of mind.'", "conclusion": "Enhancing transparency in CUIs' social cues and reasoning can foster self-disclosure, addressing the difficulty users face in sharing personal information."}}
{"id": "2507.11279", "pdf": "https://arxiv.org/pdf/2507.11279", "abs": "https://arxiv.org/abs/2507.11279", "authors": ["Yujie Zhang", "Sabine Struckmeyer", "Andreas Kolb", "Sven Reichardt"], "title": "Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping", "categories": ["cs.CV"], "comment": null, "summary": "Observer bias and inconsistencies in traditional plant phenotyping methods\nlimit the accuracy and reproducibility of fine-grained plant analysis. To\novercome these challenges, we developed TomatoMAP, a comprehensive dataset for\nSolanum lycopersicum using an Internet of Things (IoT) based imaging system\nwith standardized data acquisition protocols. Our dataset contains 64,464 RGB\nimages that capture 12 different plant poses from four camera elevation angles.\nEach image includes manually annotated bounding boxes for seven regions of\ninterest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,\naxillary shoot, shoot and whole plant area, along with 50 fine-grained growth\nstage classifications based on the BBCH scale. Additionally, we provide 3,616\nhigh-resolution image subset with pixel-wise semantic and instance segmentation\nannotations for fine-grained phenotyping. We validated our dataset using a\ncascading model deep learning framework combining MobileNetv3 for\nclassification, YOLOv11 for object detection, and MaskRCNN for segmentation.\nThrough AI vs. Human analysis involving five domain experts, we demonstrate\nthat the models trained on our dataset achieve accuracy and speed comparable to\nthe experts. Cohen's Kappa and inter-rater agreement heatmap confirm the\nreliability of automated fine-grained phenotyping using our approach.", "AI": {"tldr": "TomatoMAP is an IoT-based dataset for tomato plant phenotyping, offering standardized RGB images, annotations, and growth stage classifications, validated by deep learning models matching expert accuracy.", "motivation": "Traditional plant phenotyping methods suffer from observer bias and inconsistencies, limiting accuracy and reproducibility.", "method": "Developed TomatoMAP with IoT-based imaging, standardized protocols, and manual annotations. Validated using a cascading deep learning framework (MobileNetv3, YOLOv11, MaskRCNN).", "result": "Models trained on TomatoMAP achieved accuracy and speed comparable to human experts, confirmed by Cohen's Kappa and inter-rater agreement.", "conclusion": "TomatoMAP enables reliable, automated fine-grained phenotyping, addressing limitations of traditional methods."}}
{"id": "2507.10803", "pdf": "https://arxiv.org/pdf/2507.10803", "abs": "https://arxiv.org/abs/2507.10803", "authors": ["JaMor Hairston", "Ritvik Ranjan", "Sahithi Lakamana", "Anthony Spadaro", "Selen Bozkurt", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR"], "comment": "Pages: 19, Abstract word count: 151 words, Manuscript word count:\n  2185 words, References: 14, Figures: 3, Tables: 2", "summary": "Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health", "AI": {"tldr": "The study evaluates LLMs for replicating expert-driven thematic analysis on social media data, finding GPT-4o with two-shot prompting performs best, suggesting scalability for qualitative research.", "motivation": "To address challenges LLMs face in inductive thematic analysis, requiring deep expertise, by testing their feasibility in replicating expert-driven thematic analysis of social media data.", "method": "Evaluated five LLMs on Reddit datasets about xylazine, using binary classifications with zero-, single-, and few-shot prompting, measuring performance via accuracy, precision, recall, and F1-score.", "result": "GPT-4o with two-shot prompting achieved 90.9% accuracy and F1-score of 0.71, closely mirroring expert classifications for high-prevalence themes.", "conclusion": "Few-shot LLM-based approaches can automate thematic analyses, offering a scalable supplement for qualitative research."}}
{"id": "2507.11287", "pdf": "https://arxiv.org/pdf/2507.11287", "abs": "https://arxiv.org/abs/2507.11287", "authors": ["An-Lun Liu", "Yu-Wei Chao", "Yi-Ting Chen"], "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ICCV 2025", "summary": "In this paper, we study task-oriented human grasp synthesis, a new grasp\nsynthesis task that demands both task and context awareness. At the core of our\nmethod is the task-aware contact maps. Unlike traditional contact maps that\nonly reason about the manipulated object and its relation with the hand, our\nenhanced maps take into account scene and task information. This comprehensive\nmap is critical for hand-object interaction, enabling accurate grasping poses\nthat align with the task. We propose a two-stage pipeline that first constructs\na task-aware contact map informed by the scene and task. In the subsequent\nstage, we use this contact map to synthesize task-oriented human grasps. We\nintroduce a new dataset and a metric for the proposed task to evaluate our\napproach. Our experiments validate the importance of modeling both scene and\ntask, demonstrating significant improvements over existing methods in both\ngrasp quality and task performance. See our project page for more details:\nhttps://hcis-lab.github.io/TOHGS/", "AI": {"tldr": "The paper introduces task-oriented human grasp synthesis, using task-aware contact maps to improve grasp quality and task performance.", "motivation": "Traditional grasp synthesis lacks task and context awareness, limiting its practical applicability.", "method": "A two-stage pipeline: (1) constructs task-aware contact maps using scene and task info, (2) synthesizes grasps based on these maps.", "result": "Experiments show significant improvements in grasp quality and task performance over existing methods.", "conclusion": "Task-aware contact maps enhance grasp synthesis, proving the importance of scene and task modeling."}}
{"id": "2507.10859", "pdf": "https://arxiv.org/pdf/2507.10859", "abs": "https://arxiv.org/abs/2507.10859", "authors": ["Ramaneswaran Selvakumar", "Ashish Seth", "Nishit Anand", "Utkarsh Tyagi", "Sonal Kumar", "Sreyan Ghosh", "Dinesh Manocha"], "title": "MultiVox: Benchmarking Voice Assistants for Multimodal Interactions", "categories": ["cs.MM", "cs.CL", "cs.HC"], "comment": "Work In Progress", "summary": "The rapid progress of Large Language Models (LLMs) has empowered omni models\nto act as voice assistants capable of understanding spoken dialogues. These\nmodels can process multimodal inputs beyond text, such as speech and visual\ndata, enabling more context-aware interactions. However, current benchmarks\nfall short in comprehensively evaluating how well these models generate\ncontext-aware responses, particularly when it comes to implicitly understanding\nfine-grained speech characteristics, such as pitch, emotion, timbre, and volume\nor the environmental acoustic context such as background sounds. Additionally,\nthey inadequately assess the ability of models to align paralinguistic cues\nwith complementary visual signals to inform their responses. To address these\ngaps, we introduce MultiVox, the first omni voice assistant benchmark designed\nto evaluate the ability of voice assistants to integrate spoken and visual cues\nincluding paralinguistic speech features for truly multimodal understanding.\nSpecifically, MultiVox includes 1000 human-annotated and recorded speech\ndialogues that encompass diverse paralinguistic features and a range of visual\ncues such as images and videos. Our evaluation on 9 state-of-the-art models\nreveals that, although humans excel at these tasks, current models consistently\nstruggle to produce contextually grounded responses.", "AI": {"tldr": "MultiVox is introduced as the first benchmark to evaluate voice assistants' ability to integrate spoken and visual cues, including paralinguistic features, for multimodal understanding. Current models struggle despite human proficiency.", "motivation": "Existing benchmarks fail to comprehensively assess LLMs' ability to generate context-aware responses by understanding fine-grained speech characteristics and aligning paralinguistic cues with visual signals.", "method": "MultiVox includes 1000 human-annotated speech dialogues with diverse paralinguistic features and visual cues (images, videos). Nine state-of-the-art models are evaluated.", "result": "Current models struggle to produce contextually grounded responses, while humans excel at the tasks.", "conclusion": "MultiVox highlights the gap in multimodal understanding for voice assistants and underscores the need for improved models."}}
{"id": "2507.11293", "pdf": "https://arxiv.org/pdf/2507.11293", "abs": "https://arxiv.org/abs/2507.11293", "authors": ["J. Senthilnath", "Chen Hao", "F. C. Wellstood"], "title": "3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images", "categories": ["eess.IV", "cs.CV"], "comment": "copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "In semiconductor packaging, accurately recovering 3D information is crucial\nfor non-destructive testing (NDT) to localize circuit defects. This paper\npresents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),\nwhich leverages Magnetic Field Images (MFI) to retrieve the parameters for the\n3D current flow of a single-segment. The 3D MIR integrates a deep learning\n(DL)-based Convolutional Neural Network (CNN), spatial-physics-based\nconstraints, and optimization techniques. The method operates in three stages:\ni) The CNN model processes the MFI data to predict ($\\ell/z_o$), where $\\ell$\nis the wire length and $z_o$ is the wire's vertical depth beneath the magnetic\nsensors and classify segment type ($c$). ii) By leveraging\nspatial-physics-based constraints, the routine provides initial estimates for\nthe position ($x_o$, $y_o$, $z_o$), length ($\\ell$), current ($I$), and current\nflow direction (positive or negative) of the current segment. iii) An optimizer\nthen adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\\ell$, $I$) to\nminimize the difference between the reconstructed MFI and the actual MFI. The\nresults demonstrate that the 3D MIR method accurately recovers 3D information\nwith high precision, setting a new benchmark for magnetic image reconstruction\nin semiconductor packaging. This method highlights the potential of combining\nDL and physics-driven optimization in practical applications.", "AI": {"tldr": "A novel 3D Magnetic Inverse Routine (3D MIR) combines deep learning, spatial-physics constraints, and optimization to accurately recover 3D current flow parameters in semiconductor packaging.", "motivation": "Accurate 3D information recovery is essential for non-destructive testing (NDT) to locate circuit defects in semiconductor packaging.", "method": "The 3D MIR method uses a CNN to predict wire parameters, applies spatial-physics constraints for initial estimates, and optimizes parameters to match reconstructed and actual magnetic field images.", "result": "The method achieves high precision in recovering 3D information, setting a new benchmark for magnetic image reconstruction.", "conclusion": "The 3D MIR demonstrates the potential of integrating deep learning and physics-driven optimization in practical semiconductor applications."}}
{"id": "2507.10865", "pdf": "https://arxiv.org/pdf/2507.10865", "abs": "https://arxiv.org/abs/2507.10865", "authors": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Daniel Campos", "Jimmy Lin", "Ellen M. Voorhees", "Ian Soboroff"], "title": "Overview of the TREC 2022 deep learning track", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2507.08191,\n  arXiv:2507.08890", "summary": "This is the fourth year of the TREC Deep Learning track. As in previous\nyears, we leverage the MS MARCO datasets that made hundreds of thousands of\nhuman annotated training labels available for both passage and document ranking\ntasks. In addition, this year we also leverage both the refreshed passage and\ndocument collections that were released last year leading to a nearly $16$\ntimes increase in the size of the passage collection and nearly four times\nincrease in the document collection size. Unlike previous years, in 2022 we\nmainly focused on constructing a more complete test collection for the passage\nretrieval task, which has been the primary focus of the track. The document\nranking task was kept as a secondary task, where document-level labels were\ninferred from the passage-level labels. Our analysis shows that similar to\nprevious years, deep neural ranking models that employ large scale pretraining\ncontinued to outperform traditional retrieval methods. Due to the focusing our\njudging resources on passage judging, we are more confident in the quality of\nthis year's queries and judgments, with respect to our ability to distinguish\nbetween runs and reuse the dataset in future. We also see some surprises in\noverall outcomes. Some top-performing runs did not do dense retrieval. Runs\nthat did single-stage dense retrieval were not as competitive this year as they\nwere last year.", "AI": {"tldr": "The TREC Deep Learning track's fourth year focused on improving test collections for passage retrieval, leveraging expanded MS MARCO datasets. Deep neural models outperformed traditional methods, but dense retrieval runs were less competitive.", "motivation": "To enhance the test collection for passage retrieval and evaluate the performance of deep neural ranking models against traditional methods.", "method": "Utilized expanded MS MARCO datasets for passage and document ranking, focusing on passage retrieval with inferred document labels.", "result": "Deep neural models outperformed traditional methods, but dense retrieval runs were less competitive than in previous years.", "conclusion": "The improved test collection and judging resources increased confidence in the dataset's quality, though unexpected outcomes emerged regarding dense retrieval performance."}}
{"id": "2507.11301", "pdf": "https://arxiv.org/pdf/2507.11301", "abs": "https://arxiv.org/abs/2507.11301", "authors": ["Pa\u00fal Maji", "Marlon T\u00faquerres", "Stalin Valencia", "Marcela Valenzuela", "Christian Mejia-Escobar"], "title": "Detecci\u00f3n y Cuantificaci\u00f3n de Erosi\u00f3n Fluvial con Visi\u00f3n Artificial", "categories": ["cs.CV"], "comment": "18 pages, in Spanish language, 13 figures, 4 tables", "summary": "Fluvial erosion is a natural process that can generate significant impacts on\nsoil stability and strategic infrastructures. The detection and monitoring of\nthis phenomenon is traditionally addressed by photogrammetric methods and\nanalysis in geographic information systems. These tasks require specific\nknowledge and intensive manual processing. This study proposes an artificial\nintelligence-based approach for automatic identification of eroded zones and\nestimation of their area. The state-of-the-art computer vision model YOLOv11,\nadjusted by fine-tuning and trained with photographs and LiDAR images, is used.\nThis combined dataset was segmented and labeled using the Roboflow platform.\nExperimental results indicate efficient detection of erosion patterns with an\naccuracy of 70%, precise identification of eroded areas and reliable\ncalculation of their extent in pixels and square meters. As a final product,\nthe EROSCAN system has been developed, an interactive web application that\nallows users to upload images and obtain automatic segmentations of fluvial\nerosion, together with the estimated area. This tool optimizes the detection\nand quantification of the phenomenon, facilitating decision making in risk\nmanagement and territorial planning.", "AI": {"tldr": "The paper proposes an AI-based method using YOLOv11 and LiDAR images to automatically detect and quantify fluvial erosion, achieving 70% accuracy, and introduces the EROSCAN web tool for practical use.", "motivation": "Traditional methods for detecting and monitoring fluvial erosion are manual and require expertise, prompting the need for an automated solution.", "method": "The study uses YOLOv11, fine-tuned and trained with photographs and LiDAR images, segmented and labeled via Roboflow.", "result": "The approach achieves 70% accuracy in detecting erosion patterns and reliably estimates eroded areas in pixels and square meters.", "conclusion": "The developed EROSCAN system automates erosion detection and quantification, aiding risk management and territorial planning."}}
{"id": "2507.10880", "pdf": "https://arxiv.org/pdf/2507.10880", "abs": "https://arxiv.org/abs/2507.10880", "authors": ["Souvik Nath", "Sumit Wadhwa", "Luiz Perez"], "title": "Domain-Adaptive Small Language Models for Structured Tax Code Prediction", "categories": ["cs.LG", "cs.CL"], "comment": "10 pages, 3 figures", "summary": "Every day, multinational firms process thousands of transactions, each of\nwhich must adhere to tax regulations that vary by jurisdiction and are often\nnuanced. The determination of product and service tax codes, such as HSN or SAC\nis a major use case in Tax compliance. An accurate determination of such codes\nis imperative to avoid any tax penalties. This paper proposes a domain-adaptive\nsmall language model (SLM) with an encoder-decoder architecture for the\nenhanced prediction of product and service tax codes. In this approach, we\naddress the problem of predicting hierarchical tax code sequences using\nunstructured product and services data. We employ an SLM based upon\nencoder-decoder architecture as this enables sequential generation of tax codes\nto capture the hierarchical dependencies present within the tax codes. Our\nexperiments demonstrate that encoder-decoder SLMs can be successfully applied\nto the sequential prediction of structured tax codes, a domain that remains\ncomparatively unexplored in current NLP research. In this paper, we demonstrate\nthe superior performance of the domain-adaptive encoder-decoder SLMs over flat\nclassifiers when applied to the Harmonized System of Nomenclature (HSN), and\nachieve superior results compared to decoder-only and encoder-only\narchitectures for structured sequence generation tasks. This approach can also\nbe scaled to other government-mandated tax commodity codes, such as United\nNations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura\nComum do Mercosul (NCM).", "AI": {"tldr": "A domain-adaptive small language model (SLM) with encoder-decoder architecture is proposed for accurate prediction of hierarchical tax codes like HSN or SAC, outperforming flat classifiers and other architectures.", "motivation": "Tax compliance requires accurate determination of product and service tax codes (e.g., HSN or SAC) to avoid penalties, but current methods lack efficiency in handling hierarchical dependencies.", "method": "The paper employs an encoder-decoder SLM to sequentially generate tax codes, capturing hierarchical dependencies in unstructured product and service data.", "result": "The domain-adaptive encoder-decoder SLM outperforms flat classifiers, decoder-only, and encoder-only architectures for structured tax code prediction.", "conclusion": "The proposed SLM is effective for hierarchical tax code prediction and can be scaled to other tax commodity codes like UNSPSC or NCM."}}
{"id": "2507.11321", "pdf": "https://arxiv.org/pdf/2507.11321", "abs": "https://arxiv.org/abs/2507.11321", "authors": ["Haoxuan Qu", "Yujun Cai", "Hossein Rahmani", "Ajay Kumar", "Junsong Yuan", "Jun Liu"], "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface\nreconstruction. However, while 3D objects can be of complex and diverse shapes\nin the real world, existing GS-based methods only limitedly use a single type\nof splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent\nobject surfaces during their reconstruction. In this paper, we highlight that\nthis can be insufficient for object surfaces to be represented in high quality.\nThus, we propose a novel framework that, for the first time, enables Gaussian\nSplatting to incorporate multiple types of (geometrical) primitives during its\nsurface reconstruction process. Specifically, in our framework, we first\npropose a compositional splatting strategy, enabling the splatting and\nrendering of different types of primitives in the Gaussian Splatting pipeline.\nIn addition, we also design our framework with a mixed-primitive-based\ninitialization strategy and a vertex pruning mechanism to further promote its\nsurface representation learning process to be well executed leveraging\ndifferent types of primitives. Extensive experiments show the efficacy of our\nframework and its accurate surface reconstruction performance.", "AI": {"tldr": "The paper proposes a novel Gaussian Splatting framework using multiple geometric primitives for improved surface reconstruction.", "motivation": "Existing GS methods use a single primitive type, limiting representation quality for complex shapes.", "method": "Introduces a compositional splatting strategy, mixed-primitive initialization, and vertex pruning.", "result": "Demonstrates accurate surface reconstruction in experiments.", "conclusion": "The framework enhances GS by leveraging multiple primitives for better surface representation."}}
{"id": "2507.10894", "pdf": "https://arxiv.org/pdf/2507.10894", "abs": "https://arxiv.org/abs/2507.10894", "authors": ["Zongtao He", "Liuyi Wang", "Lu Chen", "Chengju Liu", "Qijun Chen"], "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Language-guided navigation is a cornerstone of embodied AI, enabling agents\nto interpret language instructions and navigate complex environments. However,\nexpert-provided instructions are limited in quantity, while synthesized\nannotations often lack quality, making them insufficient for large-scale\nresearch. To address this, we propose NavComposer, a novel framework for\nautomatically generating high-quality navigation instructions. NavComposer\nexplicitly decomposes semantic entities such as actions, scenes, and objects,\nand recomposes them into natural language instructions. Its modular\narchitecture allows flexible integration of state-of-the-art techniques, while\nthe explicit use of semantic entities enhances both the richness and accuracy\nof instructions. Moreover, it operates in a data-agnostic manner, supporting\nadaptation to diverse navigation trajectories without domain-specific training.\nComplementing NavComposer, we introduce NavInstrCritic, a comprehensive\nannotation-free evaluation system that assesses navigation instructions on\nthree dimensions: contrastive matching, semantic consistency, and linguistic\ndiversity. NavInstrCritic provides a holistic evaluation of instruction\nquality, addressing limitations of traditional metrics that rely heavily on\nexpert annotations. By decoupling instruction generation and evaluation from\nspecific navigation agents, our method enables more scalable and generalizable\nresearch. Extensive experiments provide direct and practical evidence for the\neffectiveness of our method.", "AI": {"tldr": "NavComposer generates high-quality navigation instructions by decomposing and recomposing semantic entities, while NavInstrCritic evaluates them without expert annotations.", "motivation": "Expert-provided navigation instructions are scarce, and synthesized ones often lack quality, limiting large-scale research.", "method": "NavComposer decomposes semantic entities (actions, scenes, objects) and recomposes them into instructions. NavInstrCritic evaluates instructions on contrastive matching, semantic consistency, and linguistic diversity.", "result": "The framework produces rich, accurate instructions and provides a holistic evaluation, enabling scalable research.", "conclusion": "The method decouples instruction generation and evaluation, making it scalable and generalizable."}}
{"id": "2507.11325", "pdf": "https://arxiv.org/pdf/2507.11325", "abs": "https://arxiv.org/abs/2507.11325", "authors": ["Arefin Ittesafun Abian", "Ripon Kumar Debnath", "Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Md Rafiqul Islam", "Asif Karim", "Reem E. Mohamed", "Sami Azam"], "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 figures. Will be submitted to IEEE Transactions on Radiation and\n  Plasma Medical Sciences", "summary": "Accurate liver and tumor segmentation on abdominal CT images is critical for\nreliable diagnosis and treatment planning, but remains challenging due to\ncomplex anatomical structures, variability in tumor appearance, and limited\nannotated data. To address these issues, we introduce Hyperbolic-convolutions\nAdaptive-temporal-attention with Neural-representation and Synaptic-plasticity\nNetwork (HANS-Net), a novel segmentation framework that synergistically\ncombines hyperbolic convolutions for hierarchical geometric representation, a\nwavelet-inspired decomposition module for multi-scale texture learning, a\nbiologically motivated synaptic plasticity mechanism for adaptive feature\nenhancement, and an implicit neural representation branch to model fine-grained\nand continuous anatomical boundaries. Additionally, we incorporate\nuncertainty-aware Monte Carlo dropout to quantify prediction confidence and\nlightweight temporal attention to improve inter-slice consistency without\nsacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate\nthat HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an\naverage symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap\nerror (VOE) of 11.91%. Furthermore, cross-dataset validation on the\n3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of\n1.525 mm, and VOE of 19.71%, indicating strong generalization across different\ndatasets. These results confirm the effectiveness and robustness of HANS-Net in\nproviding anatomically consistent, accurate, and confident liver and tumor\nsegmentation.", "AI": {"tldr": "HANS-Net introduces a novel segmentation framework combining hyperbolic convolutions, wavelet-inspired decomposition, synaptic plasticity, and neural representation for accurate liver and tumor segmentation, achieving high performance on LiTS and 3D-IRCADb-01 datasets.", "motivation": "Accurate liver and tumor segmentation on CT images is crucial for diagnosis and treatment but is challenging due to anatomical complexity, tumor variability, and limited annotated data.", "method": "HANS-Net integrates hyperbolic convolutions, wavelet-inspired decomposition, synaptic plasticity, neural representation, uncertainty-aware Monte Carlo dropout, and lightweight temporal attention.", "result": "Achieves 93.26% Dice score on LiTS and 87.45% on 3D-IRCADb-01, demonstrating strong generalization.", "conclusion": "HANS-Net is effective and robust for anatomically consistent, accurate, and confident segmentation."}}
{"id": "2507.10903", "pdf": "https://arxiv.org/pdf/2507.10903", "abs": "https://arxiv.org/abs/2507.10903", "authors": ["Parisa Fard Moshiri", "Xinyu Zhu", "Poonam Lohan", "Burak Kantarci", "Emil Janulewicz"], "title": "LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning", "categories": ["cs.NI", "cs.CL", "cs.LG"], "comment": "9 pages, 6 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "summary": "Effective management of Service Function Chains (SFCs) and optimal Virtual\nNetwork Function (VNF) placement are critical challenges in modern\nSoftware-Defined Networking (SDN) and Network Function Virtualization (NFV)\nenvironments. Although Deep Reinforcement Learning (DRL) is widely adopted for\ndynamic network decision-making, its inherent dependency on structured data and\nfixed action rules often limits adaptability and responsiveness, particularly\nunder unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a\nnovel approach combining Lightweight Language Model (LiLM) with Relational\nDatabase (RDB) to answer network state queries to guide DRL model for efficient\nSFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and\nAuto-Regressive Transformers (BART) and the Fine-tuned Language Net T5\n(FLAN-T5), to interpret network data and support diverse query types related to\nSFC demands, data center resources, and VNF availability. Results demonstrate\nthat FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to\n0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time\n(2h 2min compared to 2h 38min). Moreover, when compared to the large language\nmodel SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting\nprocessing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).", "AI": {"tldr": "LiLM-RDB-SFC combines Lightweight Language Models (LiLMs) with Relational Databases to enhance Deep Reinforcement Learning for efficient SFC provisioning, showing FLAN-T5 outperforms BART and SQLCoder in accuracy and speed.", "motivation": "The need for adaptable and responsive solutions in SFC and VNF placement under unpredictable network conditions, where traditional DRL methods fall short.", "method": "Uses two LiLMs (BART and FLAN-T5) to interpret network data and guide DRL models, leveraging relational databases for state queries.", "result": "FLAN-T5 achieves lower test loss (0.00161 vs. 0.00734), higher accuracy (94.79% vs. 80.2%), and faster processing (2h 2min vs. 2h 38min) than BART. It also matches SQLCoder's accuracy while reducing processing time by 96%.", "conclusion": "LiLM-RDB-SFC, particularly with FLAN-T5, offers a superior solution for efficient SFC provisioning in dynamic network environments."}}
{"id": "2507.11333", "pdf": "https://arxiv.org/pdf/2507.11333", "abs": "https://arxiv.org/abs/2507.11333", "authors": ["Jianfei Jiang", "Qiankun Liu", "Haochen Yu", "Hongyuan Liu", "Liyong Wang", "Jiansheng Chen", "Huimin Ma"], "title": "MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for\na sequence of calibrated images to recover dense point clouds. However,\nexisting MVS methods often struggle with challenging regions, such as\ntextureless regions and reflective surfaces, where feature matching fails. In\ncontrast, monocular depth estimation inherently does not require feature\nmatching, allowing it to achieve robust relative depth estimation in these\nregions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature\nand depth guided MVS network that integrates powerful priors from a monocular\nfoundation model into multi-view geometry. Firstly, the monocular feature of\nthe reference view is integrated into source view features by the attention\nmechanism with a newly designed cross-view position encoding. Then, the\nmonocular depth of the reference view is aligned to dynamically update the\ndepth candidates for edge regions during the sampling procedure. Finally, a\nrelative consistency loss is further designed based on the monocular depth to\nsupervise the depth prediction. Extensive experiments demonstrate that\nMonoMVSNet achieves state-of-the-art performance on the DTU and\nTanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate\nand Advanced benchmarks. The source code is available at\nhttps://github.com/JianfeiJ/MonoMVSNet.", "AI": {"tldr": "MonoMVSNet integrates monocular depth and feature priors into MVS to improve performance in challenging regions like textureless and reflective surfaces.", "motivation": "Existing MVS methods struggle with feature matching in difficult regions, while monocular depth estimation excels there.", "method": "Uses monocular features and depth, aligned via attention and dynamic candidate updates, with a relative consistency loss.", "result": "Achieves state-of-the-art on DTU and Tanks-and-Temples benchmarks.", "conclusion": "MonoMVSNet effectively combines monocular and multi-view strengths for robust depth prediction."}}
{"id": "2507.11017", "pdf": "https://arxiv.org/pdf/2507.11017", "abs": "https://arxiv.org/abs/2507.11017", "authors": ["Xingyu Zheng", "Haotong Qin", "Yuye Li", "Jiakai Wang", "Jinyang Guo", "Michele Magno", "Xianglong Liu"], "title": "First-Order Error Matters: Accurate Compensation for Quantized Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Post-training quantization (PTQ) offers an efficient approach to compressing\nlarge language models (LLMs), significantly reducing memory access and\ncomputational costs. Existing compensation-based weight calibration methods\noften rely on a second-order Taylor expansion to model quantization error,\nunder the assumption that the first-order term is negligible in well-trained\nfull-precision models. However, we reveal that the progressive compensation\nprocess introduces accumulated first-order deviations between latent weights\nand their full-precision counterparts, making this assumption fundamentally\nflawed. To address this, we propose FOEM, a novel PTQ method that explicitly\nincorporates first-order gradient terms to improve quantization error\ncompensation. FOEM approximates gradients by directly computing the difference\nbetween latent and full-precision weights, avoiding the high cost and limited\ngeneralization of backpropagation-based gradient computation. This approach\nintroduces minimal additional computational overhead. Moreover, FOEM leverages\nprecomputed Cholesky factors to efficiently recover the inverse of Hessian\nsubmatrices in real time. Extensive experiments across a wide range of models\nand benchmarks demonstrate that FOEM consistently outperforms the classical\nGPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of\nLlama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from\n51.7% to 74.9%, approaching the full-precision performance of 78.6%.\nFurthermore, FOEM can be seamlessly integrated with advanced techniques such as\nGPTAQ and SpinQuant, yielding additional improvements under the challenging\nW4A4KV4 setting, and further narrowing the accuracy gap with full-precision\nbaselines beyond what current state-of-the-art methods achieve. The code is\navailable at https://github.com/Xingyu-Zheng/FOEM.", "AI": {"tldr": "FOEM is a novel PTQ method for LLMs that incorporates first-order gradient terms to improve quantization error compensation, outperforming existing methods like GPTQ in accuracy and efficiency.", "motivation": "Existing PTQ methods assume negligible first-order terms in quantization error, but accumulated deviations make this flawed. FOEM addresses this by explicitly including first-order gradients.", "method": "FOEM approximates gradients by computing differences between latent and full-precision weights, avoiding costly backpropagation. It uses precomputed Cholesky factors for efficient Hessian submatrix inversion.", "result": "FOEM reduces perplexity by 89.6% for Llama3-8B and improves MMLU accuracy from 51.7% to 74.9% for Llama3-70B, nearing full-precision performance. It also integrates well with advanced techniques.", "conclusion": "FOEM effectively addresses the limitations of existing PTQ methods, offering superior performance and efficiency, and can be combined with other techniques for further improvements."}}
{"id": "2507.11336", "pdf": "https://arxiv.org/pdf/2507.11336", "abs": "https://arxiv.org/abs/2507.11336", "authors": ["Peiran Wu", "Yunze Liu", "Zhengdong Zhu", "Enmin Zhou", "Shawn Shen"], "title": "UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks", "categories": ["cs.CV"], "comment": null, "summary": "Real-world user-generated videos, especially on platforms like TikTok, often\nfeature rich and intertwined audio visual content. However, existing video\ncaptioning benchmarks and models remain predominantly visual centric,\noverlooking the crucial role of audio in conveying scene dynamics, speaker\nintent, and narrative context. This lack of omni datasets and lightweight,\ncapable models hampers progress in fine grained, multimodal video\nunderstanding. To address these challenges, we introduce UGC-VideoCap, a new\nbenchmark and model framework specifically designed for detailed omnimodal\ncaptioning of short form user-generated videos. Unlike prior datasets,\nUGC-VideoCap emphasizes balanced integration of audio and visual modalities,\nfeaturing 1000 TikTok videos annotated through a structured three stage\nhuman-in-the-loop pipeline covering audio only, visual only, and joint audio\nvisual semantics. The benchmark also includes 4000 carefully crafted QA pairs\nprobing both unimodal and cross modal understanding. Alongside the dataset, we\npropose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from\nGemini 2.5 Flash. Using a novel two-stage training strategy supervised fine\ntuning followed by Group Relative Policy Optimization (GRPO), our approach\nenables efficient adaptation from limited data while maintaining competitive\nperformance. Together, our benchmark and model offer a high-quality foundation\nand a data-efficient solution for advancing omnimodal video captioning in\nunconstrained real-world UGC settings.", "AI": {"tldr": "The paper introduces UGC-VideoCap, a benchmark and model framework for omnimodal captioning of user-generated videos, addressing the gap in audio-visual integration in existing methods.", "motivation": "Existing video captioning benchmarks and models are visual-centric, neglecting audio's role in scene dynamics and narrative context, limiting multimodal video understanding.", "method": "UGC-VideoCap includes 1000 TikTok videos with balanced audio-visual annotations and 4000 QA pairs. The proposed UGC-VideoCaptioner(3B) model uses a two-stage training strategy (supervised fine-tuning and GRPO) for efficient adaptation.", "result": "The benchmark and model provide a high-quality, data-efficient solution for omnimodal video captioning in real-world UGC settings.", "conclusion": "UGC-VideoCap and its model framework advance fine-grained, multimodal video understanding by integrating audio and visual modalities effectively."}}
{"id": "2507.11059", "pdf": "https://arxiv.org/pdf/2507.11059", "abs": "https://arxiv.org/abs/2507.11059", "authors": ["Pavel Adamenko", "Mikhail Ivanov", "Aidar Valeev", "Rodion Levichev", "Pavel Zadorozhny", "Ivan Lopatin", "Dmitry Babayev", "Alena Fenogenova", "Valentin Malykh"], "title": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08\\% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025.", "AI": {"tldr": "SWE-MERA is introduced as a dynamic benchmark to address data contamination in SWE-bench, featuring automated GitHub issue collection and rigorous validation. It evaluates LLMs effectively.", "motivation": "Existing benchmarks like SWE-bench suffer from data contamination and inadequate test cases, limiting their reliability for evaluating LLMs in software engineering.", "method": "SWE-MERA uses an automated pipeline to collect real-world GitHub issues, ensuring quality and minimizing contamination, with 10,000 potential tasks.", "result": "The benchmark demonstrates strong discriminative power, evaluating a dozen recent LLMs on tasks collected from September 2024 to June 2025.", "conclusion": "SWE-MERA provides a more reliable and dynamic benchmark for assessing LLMs in software engineering, addressing key limitations of existing datasets."}}
{"id": "2507.11372", "pdf": "https://arxiv.org/pdf/2507.11372", "abs": "https://arxiv.org/abs/2507.11372", "authors": ["Pierrick Leroy", "Antonio Mastropietro", "Marco Nurisso", "Francesco Vaccarino"], "title": "Attributes Shape the Embedding Space of Face Recognition Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Face Recognition (FR) tasks have made significant progress with the advent of\nDeep Neural Networks, particularly through margin-based triplet losses that\nembed facial images into high-dimensional feature spaces. During training,\nthese contrastive losses focus exclusively on identity information as labels.\nHowever, we observe a multiscale geometric structure emerging in the embedding\nspace, influenced by interpretable facial (e.g., hair color) and image\nattributes (e.g., contrast). We propose a geometric approach to describe the\ndependence or invariance of FR models to these attributes and introduce a\nphysics-inspired alignment metric. We evaluate the proposed metric on\ncontrolled, simplified models and widely used FR models fine-tuned with\nsynthetic data for targeted attribute augmentation. Our findings reveal that\nthe models exhibit varying degrees of invariance across different attributes,\nproviding insight into their strengths and weaknesses and enabling deeper\ninterpretability. Code available here:\nhttps://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs", "AI": {"tldr": "The paper explores the geometric structure in face recognition embedding spaces, influenced by facial and image attributes, and introduces a physics-inspired metric to evaluate model invariance.", "motivation": "To understand the multiscale geometric structure in face recognition embeddings and assess model dependence or invariance to interpretable attributes.", "method": "Proposes a geometric approach and a physics-inspired alignment metric, tested on controlled models and widely used FR models fine-tuned with synthetic data.", "result": "Models show varying degrees of invariance across attributes, revealing strengths, weaknesses, and improving interpretability.", "conclusion": "The study provides insights into attribute-based invariance in FR models, enhancing interpretability and understanding of their behavior."}}
{"id": "2507.11515", "pdf": "https://arxiv.org/pdf/2507.11515", "abs": "https://arxiv.org/abs/2507.11515", "authors": ["Shiyi Yang", "Xiaoxue Yu", "Rongpeng Li", "Jianhang Zhu", "Zhifeng Zhao", "Honggang Zhang"], "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 8 figures", "summary": "Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air.", "AI": {"tldr": "AirLLM introduces a hierarchical diffusion policy framework for efficient LoRA adaptation in LLMs, combining PPO and DDIM to optimize rank configurations and reduce transmission costs.", "motivation": "Addressing the inefficiency of fixed or heuristic rank configurations in LoRA approaches and the high transmission costs of LoRA parameters for edge devices.", "method": "AirLLM uses a PPO agent for coarse-grained decisions and DDIM for refining rank vectors, optimized alternately under CFG to align with PPO rewards.", "result": "AirLLM improves fine-tuning performance and reduces transmission costs under varying signal-to-noise ratios.", "conclusion": "AirLLM demonstrates scalable and efficient remote fine-tuning via reinforcement-driven, diffusion-refined rank adaptation."}}
{"id": "2507.11441", "pdf": "https://arxiv.org/pdf/2507.11441", "abs": "https://arxiv.org/abs/2507.11441", "authors": ["Kaif Shaikh", "Antoni Kowalczuk", "Franziska Boenisch", "Adam Dziedzic"], "title": "Implementing Adaptations for Vision AutoRegressive Model", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.4.8; I.2.10"], "comment": "Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025", "summary": "Vision AutoRegressive model (VAR) was recently introduced as an alternative\nto Diffusion Models (DMs) in image generation domain. In this work we focus on\nits adaptations, which aim to fine-tune pre-trained models to perform specific\ndownstream tasks, like medical data generation. While for DMs there exist many\ntechniques, adaptations for VAR remain underexplored. Similarly, differentially\nprivate (DP) adaptations-ones that aim to preserve privacy of the adaptation\ndata-have been extensively studied for DMs, while VAR lacks such solutions. In\nour work, we implement and benchmark many strategies for VAR, and compare them\nto state-of-the-art DM adaptation strategies. We observe that VAR outperforms\nDMs for non-DP adaptations, however, the performance of DP suffers, which\nnecessitates further research in private adaptations for VAR. Code is available\nat https://github.com/sprintml/finetuning_var_dp.", "AI": {"tldr": "VAR is explored for downstream tasks and DP adaptations, outperforming DMs in non-DP settings but lagging in DP performance.", "motivation": "To adapt VAR for specific tasks like medical data generation and explore DP adaptations, which are underexplored compared to DMs.", "method": "Implemented and benchmarked various adaptation strategies for VAR, comparing them to state-of-the-art DM adaptations.", "result": "VAR outperforms DMs in non-DP adaptations but struggles with DP performance.", "conclusion": "Further research is needed to improve DP adaptations for VAR."}}
{"id": "2507.11443", "pdf": "https://arxiv.org/pdf/2507.11443", "abs": "https://arxiv.org/abs/2507.11443", "authors": ["Haoran Wang", "Hanyu Pei", "Yang Lyu", "Kai Zhang", "Li Li", "Feng-Lei Fan"], "title": "COLI: A Hierarchical Efficient Compressor for Large Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The escalating adoption of high-resolution, large-field-of-view imagery\namplifies the need for efficient compression methodologies. Conventional\ntechniques frequently fail to preserve critical image details, while\ndata-driven approaches exhibit limited generalizability. Implicit Neural\nRepresentations (INRs) present a promising alternative by learning continuous\nmappings from spatial coordinates to pixel intensities for individual images,\nthereby storing network weights rather than raw pixels and avoiding the\ngeneralization problem. However, INR-based compression of large images faces\nchallenges including slow compression speed and suboptimal compression ratios.\nTo address these limitations, we introduce COLI (Compressor for Large Images),\na novel framework leveraging Neural Representations for Videos (NeRV). First,\nrecognizing that INR-based compression constitutes a training process, we\naccelerate its convergence through a pretraining-finetuning paradigm,\nmixed-precision training, and reformulation of the sequential loss into a\nparallelizable objective. Second, capitalizing on INRs' transformation of image\nstorage constraints into weight storage, we implement Hyper-Compression, a\nnovel post-training technique to substantially enhance compression ratios while\nmaintaining minimal output distortion. Evaluations across two medical imaging\ndatasets demonstrate that COLI consistently achieves competitive or superior\nPSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while\naccelerating NeRV training by up to 4 times.", "AI": {"tldr": "COLI introduces a novel framework using Neural Representations for Videos (NeRV) to improve compression of large images, addressing slow speed and suboptimal ratios in INR-based methods.", "motivation": "High-resolution imagery demands efficient compression, but conventional and data-driven methods struggle with detail preservation and generalizability.", "method": "COLI accelerates INR-based compression via pretraining-finetuning, mixed-precision training, and parallelizable objectives, and enhances ratios with Hyper-Compression.", "result": "COLI achieves better PSNR and SSIM at lower bits per pixel (bpp) and speeds up NeRV training by 4x.", "conclusion": "COLI effectively addresses INR limitations, offering faster, higher-quality compression for large images."}}
{"id": "2507.11474", "pdf": "https://arxiv.org/pdf/2507.11474", "abs": "https://arxiv.org/abs/2507.11474", "authors": ["Pan Du", "Mingqi Xu", "Xiaozhi Zhu", "Jian-xun Wang"], "title": "HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing", "categories": ["cs.CV"], "comment": "59 pages, 9 figures", "summary": "Accurate characterization of vascular geometry is essential for\ncardiovascular diagnosis and treatment planning. Traditional statistical shape\nmodeling (SSM) methods rely on linear assumptions, limiting their expressivity\nand scalability to complex topologies such as multi-branch vascular structures.\nWe introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular\ngeometry Synthesis, which integrates NURBS surface parameterization with\ndiffusion-based generative modeling to synthesize realistic, fine-grained\naortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates\nanatomically faithful aortas with supra-aortic branches, yielding biomarker\ndistributions that closely match those of the original dataset. HUG-VAS adopts\na hierarchical architecture comprising a denoising diffusion model that\ngenerates centerlines and a guided diffusion model that synthesizes radial\nprofiles conditioned on those centerlines, thereby capturing two layers of\nanatomical variability. Critically, the framework supports zero-shot\nconditional generation from image-derived priors, enabling practical\napplications such as interactive semi-automatic segmentation, robust\nreconstruction under degraded imaging conditions, and implantable device\noptimization. To our knowledge, HUG-VAS is the first SSM framework to bridge\nimage-derived priors with generative shape modeling via a unified integration\nof NURBS parameterization and hierarchical diffusion processes.", "AI": {"tldr": "HUG-VAS is a hierarchical NURBS generative model for vascular synthesis, combining NURBS parameterization with diffusion-based modeling to create realistic aortic geometries, outperforming traditional linear SSM methods.", "motivation": "Traditional SSM methods are limited by linear assumptions, hindering their ability to model complex vascular structures like multi-branch aortas. HUG-VAS aims to overcome this by integrating NURBS and diffusion models.", "method": "HUG-VAS uses a hierarchical architecture with a denoising diffusion model for centerlines and a guided diffusion model for radial profiles, trained on 21 patient-specific samples.", "result": "The model generates anatomically accurate aortas with supra-aortic branches, matching biomarker distributions of the original dataset, and supports zero-shot conditional generation from image-derived priors.", "conclusion": "HUG-VAS is the first SSM framework to unify NURBS parameterization and hierarchical diffusion, enabling applications like segmentation, reconstruction, and device optimization."}}
{"id": "2507.11476", "pdf": "https://arxiv.org/pdf/2507.11476", "abs": "https://arxiv.org/abs/2507.11476", "authors": ["Esteban Rom\u00e1n Catafau", "Torbj\u00f6rn E. M. Nordling"], "title": "C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images", "categories": ["cs.CV"], "comment": "22 pages, 16 figures", "summary": "This paper addresses the fundamental computer vision challenge of robust\ncircle detection and fitting in degraded imaging conditions. We present\nCombinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an\nalgorithm that bridges the gap between circle detection and precise parametric\nfitting by combining (1) efficient combinatorial edge pixel (edgel) sampling\nand (2) convolution-based density estimation in parameter space.\n  We evaluate 3C-FBI across three experimental frameworks: (1) real-world\nmedical data from Parkinson's disease assessments (144 frames from 36 videos),\n(2) controlled synthetic data following established circle-fitting benchmarks,\nand (3) systematic analysis across varying spatial resolutions and outlier\ncontamination levels. Results show that 3C-FBI achieves state-of-the-art\naccuracy (Jaccard index 0.896) while maintaining real-time performance (40.3\nfps), significantly outperforming classical methods like RCD (6.8 fps) on a\nstandard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost\n1.0) at high resolutions (480x480) and reliable performance (Jaccard higher\nthan 0.95) down to 160x160 with up to 20% outliers.\n  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989\nacross contamination levels, comparable to modern methods like Qi et al. (2024,\n0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and\nrobustness makes 3C-FBI ideal for medical imaging, robotics, and industrial\ninspection under challenging conditions.", "AI": {"tldr": "3C-FBI is a robust circle detection algorithm combining combinatorial edgel sampling and convolution-based density estimation, excelling in accuracy and speed under degraded conditions.", "motivation": "Addressing the challenge of robust circle detection and fitting in degraded imaging conditions, particularly for applications like medical imaging and industrial inspection.", "method": "Combines combinatorial edge pixel sampling and convolution-based density estimation in parameter space.", "result": "Achieves state-of-the-art accuracy (Jaccard index 0.896) and real-time performance (40.3 fps), outperforming classical methods. Maintains high accuracy even with outliers and low resolutions.", "conclusion": "3C-FBI is ideal for medical imaging, robotics, and industrial inspection due to its accuracy, speed, and robustness."}}
{"id": "2507.11488", "pdf": "https://arxiv.org/pdf/2507.11488", "abs": "https://arxiv.org/abs/2507.11488", "authors": ["Pakizar Shamoi", "Nuray Toganas", "Muragul Muratbekova", "Elnara Kadyrgali", "Adilet Yerkin", "Ayan Igali", "Malika Ziyada", "Ayana Adilova", "Aron Karatayev", "Yerdauit Torekhan"], "title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation", "categories": ["cs.CV", "cs.AI"], "comment": "submitted to IEEE for consideration", "summary": "Colors are omnipresent in today's world and play a vital role in how humans\nperceive and interact with their surroundings. However, it is challenging for\ncomputers to imitate human color perception. This paper introduces the Human\nPerception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based\nRepresentation and Interpretation), designed to bridge the gap between\ncomputational color representations and human visual perception. The proposed\nmodel uses fuzzy sets and logic to create a framework for color categorization.\nUsing a three-phase experimental approach, the study first identifies\ndistinguishable color stimuli for hue, saturation, and intensity through\npreliminary experiments, followed by a large-scale human categorization survey\ninvolving more than 1000 human subjects. The resulting data are used to extract\nfuzzy partitions and generate membership functions that reflect real-world\nperceptual uncertainty. The model incorporates a mechanism for adaptation that\nallows refinement based on feedback and contextual changes. Comparative\nevaluations demonstrate the model's alignment with human perception compared to\ntraditional color models, such as RGB, HSV, and LAB. To the best of our\nknowledge, no previous research has documented the construction of a model for\ncolor attribute specification based on a sample of this size or a comparable\nsample of the human population (n = 2496). Our findings are significant for\nfields such as design, artificial intelligence, marketing, and human-computer\ninteraction, where perceptually relevant color representation is critical.", "AI": {"tldr": "The paper introduces COLIBRI, a fuzzy color model aligning computational color representation with human perception, validated by large-scale human experiments.", "motivation": "Bridging the gap between computational color models and human visual perception for better applications in design, AI, and marketing.", "method": "A three-phase approach: identifying color stimuli, conducting a large-scale human categorization survey, and using fuzzy logic to model perceptual uncertainty.", "result": "COLIBRI outperforms traditional models (RGB, HSV, LAB) in aligning with human perception, validated by data from 2496 subjects.", "conclusion": "COLIBRI provides a perceptually relevant color model with adaptive capabilities, useful for diverse fields like AI and human-computer interaction."}}
{"id": "2507.11522", "pdf": "https://arxiv.org/pdf/2507.11522", "abs": "https://arxiv.org/abs/2507.11522", "authors": ["Tariq Mehmood", "Hamza Ahmad", "Muhammad Haroon Shakeel", "Murtaza Taj"], "title": "CATVis: Context-Aware Thought Visualization", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MICCAI 2025. This is the submitted version prior to peer\n  review. The final Version of Record will appear in the MICCAI 2025\n  proceedings (Springer LNCS)", "summary": "EEG-based brain-computer interfaces (BCIs) have shown promise in various\napplications, such as motor imagery and cognitive state monitoring. However,\ndecoding visual representations from EEG signals remains a significant\nchallenge due to their complex and noisy nature. We thus propose a novel\n5-stage framework for decoding visual representations from EEG signals: (1) an\nEEG encoder for concept classification, (2) cross-modal alignment of EEG and\ntext embeddings in CLIP feature space, (3) caption refinement via re-ranking,\n(4) weighted interpolation of concept and caption embeddings for richer\nsemantics, and (5) image generation using a pre-trained Stable Diffusion model.\nWe enable context-aware EEG-to-image generation through cross-modal alignment\nand re-ranking. Experimental results demonstrate that our method generates\nhigh-quality images aligned with visual stimuli, outperforming SOTA approaches\nby 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and\nreducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic\nalignment and image quality.", "AI": {"tldr": "A 5-stage framework decodes EEG signals into visual representations, outperforming state-of-the-art methods in accuracy and image quality.", "motivation": "Decoding visual representations from noisy EEG signals is challenging but crucial for BCIs.", "method": "A novel 5-stage framework involving EEG encoding, cross-modal alignment, caption refinement, weighted interpolation, and image generation.", "result": "Outperforms SOTA by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy, and reduces Fr\u00e9chet Inception Distance by 36.61%.", "conclusion": "The framework enables high-quality, context-aware EEG-to-image generation with superior semantic alignment."}}
{"id": "2507.11533", "pdf": "https://arxiv.org/pdf/2507.11533", "abs": "https://arxiv.org/abs/2507.11533", "authors": ["Mengyu Wang", "Henghui Ding", "Jianing Peng", "Yao Zhao", "Yunpeng Chen", "Yunchao Wei"], "title": "CharaConsist: Fine-Grained Consistent Character Generation", "categories": ["cs.CV"], "comment": "ICCV 2025 accepted paper, project page:\n  https://murray-wang.github.io/CharaConsist/", "summary": "In text-to-image generation, producing a series of consistent contents that\npreserve the same identity is highly valuable for real-world applications.\nAlthough a few works have explored training-free methods to enhance the\nconsistency of generated subjects, we observe that they suffer from the\nfollowing problems. First, they fail to maintain consistent background details,\nwhich limits their applicability. Furthermore, when the foreground character\nundergoes large motion variations, inconsistencies in identity and clothing\ndetails become evident. To address these problems, we propose CharaConsist,\nwhich employs point-tracking attention and adaptive token merge along with\ndecoupled control of the foreground and background. CharaConsist enables\nfine-grained consistency for both foreground and background, supporting the\ngeneration of one character in continuous shots within a fixed scene or in\ndiscrete shots across different scenes. Moreover, CharaConsist is the first\nconsistent generation method tailored for text-to-image DiT model. Its ability\nto maintain fine-grained consistency, combined with the larger capacity of\nlatest base model, enables it to produce high-quality visual outputs,\nbroadening its applicability to a wider range of real-world scenarios. The\nsource code has been released at https://github.com/Murray-Wang/CharaConsist", "AI": {"tldr": "CharaConsist improves text-to-image generation by ensuring consistent foreground and background details using point-tracking attention and adaptive token merge.", "motivation": "Existing methods fail to maintain consistent background details and struggle with identity and clothing consistency during large motion variations.", "method": "CharaConsist employs point-tracking attention, adaptive token merge, and decoupled control of foreground and background.", "result": "It achieves fine-grained consistency for both foreground and background, supporting continuous or discrete shots.", "conclusion": "CharaConsist is the first method tailored for DiT models, producing high-quality outputs for diverse real-world applications."}}
{"id": "2507.11539", "pdf": "https://arxiv.org/pdf/2507.11539", "abs": "https://arxiv.org/abs/2507.11539", "authors": ["Dong Zhuo", "Wenzhao Zheng", "Jiahe Guo", "Yuqi Wu", "Jie Zhou", "Jiwen Lu"], "title": "Streaming 4D Visual Geometry Transformer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code is available at: https://github.com/wzzheng/StreamVGGT", "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.", "AI": {"tldr": "A streaming 4D visual geometry transformer is proposed for real-time 4D reconstruction from videos, using causal attention and implicit memory for efficiency.", "motivation": "To enable interactive and real-time 4D spatial-temporal geometry reconstruction from videos, addressing the challenge of processing long-term sequences efficiently.", "method": "Employs a causal transformer architecture with temporal causal attention and cached historical keys/values for streaming. Knowledge is distilled from a dense bidirectional transformer (VGGT) for training.", "result": "Achieves competitive performance with increased inference speed in online scenarios, enabling scalable and interactive 4D vision systems.", "conclusion": "The proposed model effectively balances real-time processing and high-quality reconstruction, advancing practical 4D vision applications."}}
{"id": "2507.11540", "pdf": "https://arxiv.org/pdf/2507.11540", "abs": "https://arxiv.org/abs/2507.11540", "authors": ["Zhen Xu", "Hongyu Zhou", "Sida Peng", "Haotong Lin", "Haoyu Guo", "Jiahao Shao", "Peishan Yang", "Qinglin Yang", "Sheng Miao", "Xingyi He", "Yifan Wang", "Yue Wang", "Ruizhen Hu", "Yiyi Liao", "Xiaowei Zhou", "Hujun Bao"], "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Depth estimation is a fundamental task in 3D computer vision, crucial for\napplications such as 3D reconstruction, free-viewpoint rendering, robotics,\nautonomous driving, and AR/VR technologies. Traditional methods relying on\nhardware sensors like LiDAR are often limited by high costs, low resolution,\nand environmental sensitivity, limiting their applicability in real-world\nscenarios. Recent advances in vision-based methods offer a promising\nalternative, yet they face challenges in generalization and stability due to\neither the low-capacity model architectures or the reliance on domain-specific\nand small-scale datasets. The emergence of scaling laws and foundation models\nin other domains has inspired the development of \"depth foundation models\":\ndeep neural networks trained on large datasets with strong zero-shot\ngeneralization capabilities. This paper surveys the evolution of deep learning\narchitectures and paradigms for depth estimation across the monocular, stereo,\nmulti-view, and monocular video settings. We explore the potential of these\nmodels to address existing challenges and provide a comprehensive overview of\nlarge-scale datasets that can facilitate their development. By identifying key\narchitectures and training strategies, we aim to highlight the path towards\nrobust depth foundation models, offering insights into their future research\nand applications.", "AI": {"tldr": "A survey on depth estimation in 3D computer vision, focusing on vision-based methods and the potential of depth foundation models to overcome limitations of traditional hardware-based approaches.", "motivation": "Overcome the limitations of traditional hardware sensors (e.g., LiDAR) and improve generalization and stability in vision-based depth estimation methods.", "method": "Survey of deep learning architectures and paradigms for depth estimation across monocular, stereo, multi-view, and monocular video settings.", "result": "Identifies key architectures and training strategies for robust depth foundation models, leveraging large-scale datasets.", "conclusion": "Highlights the potential of depth foundation models for future research and applications in 3D vision tasks."}}
{"id": "2507.10561", "pdf": "https://arxiv.org/pdf/2507.10561", "abs": "https://arxiv.org/abs/2507.10561", "authors": ["Alessio Caviglia", "Filippo Marostica", "Alessio Carpegna", "Alessandro Savino", "Stefano Di Carlo"], "title": "SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST", "categories": ["cs.NE", "cs.CV"], "comment": null, "summary": "Hardware accelerators are essential for achieving low-latency,\nenergy-efficient inference in edge applications like image recognition. Spiking\nNeural Networks (SNNs) are particularly promising due to their event-driven and\ntemporally sparse nature, making them well-suited for low-power Field\nProgrammable Gate Array (FPGA)-based deployment. This paper explores using the\nopen-source Spiker+ framework to generate optimized SNNs accelerators for\nhandwritten digit recognition on the MNIST dataset. Spiker+ enables high-level\nspecification of network topologies, neuron models, and quantization,\nautomatically generating deployable HDL. We evaluate multiple configurations\nand analyze trade-offs relevant to edge computing constraints.", "AI": {"tldr": "The paper explores using the Spiker+ framework to optimize SNN accelerators for MNIST digit recognition on FPGAs, analyzing trade-offs for edge computing.", "motivation": "Hardware accelerators are needed for efficient edge inference; SNNs are promising due to their event-driven, low-power nature.", "method": "Uses Spiker+ to specify SNN topologies, neuron models, and quantization, generating deployable HDL for FPGA.", "result": "Evaluates multiple configurations, analyzing trade-offs for edge constraints.", "conclusion": "Spiker+ effectively optimizes SNN accelerators for edge applications like MNIST recognition."}}
{"id": "2507.10589", "pdf": "https://arxiv.org/pdf/2507.10589", "abs": "https://arxiv.org/abs/2507.10589", "authors": ["Gaurav Singh"], "title": "Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.NE"], "comment": null, "summary": "Pneumonia, particularly when induced by diseases like COVID-19, remains a\ncritical global health challenge requiring rapid and accurate diagnosis. This\nstudy presents a comprehensive comparison of traditional machine learning and\nstate-of-the-art deep learning approaches for automated pneumonia detection\nusing chest X-rays (CXRs). We evaluate multiple methodologies, ranging from\nconventional machine learning techniques (PCA-based clustering, Logistic\nRegression, and Support Vector Classification) to advanced deep learning\narchitectures including Convolutional Neural Networks (Modified LeNet,\nDenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,\nCompact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856\npediatric CXR images, we demonstrate that Vision Transformers, particularly the\nCross-ViT architecture, achieve superior performance with 88.25% accuracy and\n99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that\narchitectural choices impact performance more significantly than model size,\nwith Cross-ViT's 75M parameters outperforming larger models. The study also\naddresses practical considerations including computational efficiency, training\nrequirements, and the critical balance between precision and recall in medical\ndiagnostics. Our findings suggest that Vision Transformers offer a promising\ndirection for automated pneumonia detection, potentially enabling more rapid\nand accurate diagnosis during health crises.", "AI": {"tldr": "The study compares traditional machine learning and deep learning methods for pneumonia detection in chest X-rays, finding Vision Transformers, especially Cross-ViT, outperform CNNs with 88.25% accuracy and 99.42% recall.", "motivation": "Pneumonia, especially from COVID-19, demands fast, accurate diagnosis. Automated detection using CXRs can improve outcomes.", "method": "Evaluated traditional ML (PCA, Logistic Regression, SVM) and deep learning (CNNs, Vision Transformers like Cross-ViT) on 5,856 pediatric CXR images.", "result": "Cross-ViT achieved 88.25% accuracy and 99.42% recall, outperforming CNNs. Architectural choices mattered more than model size.", "conclusion": "Vision Transformers, particularly Cross-ViT, show promise for rapid, accurate pneumonia detection, aiding health crises."}}
{"id": "2507.10601", "pdf": "https://arxiv.org/pdf/2507.10601", "abs": "https://arxiv.org/abs/2507.10601", "authors": ["Ruixi Zheng", "Wei Zhang", "Yijie Li", "Xi Zhu", "Zhou Lan", "Jarrett Rushmore", "Yogesh Rathi", "Nikos Makris", "Lauren J. O'Donnell", "Fan Zhang"], "title": "AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "eess.IV", "stat.ME"], "comment": "31 pages and 7 figures", "summary": "Diffusion MRI (dMRI) tractography is currently the only method for in vivo\nmapping of the brain's white matter (WM) connections. Tractometry is an\nadvanced tractography analysis technique for along-tract profiling to\ninvestigate the morphology and microstructural properties along the fiber\ntracts. Tractometry has become an essential tool for studying local along-tract\ndifferences between different populations (e.g., health vs disease). In this\nstudy, we propose a novel atlas-guided fine-scale tractometry method, namely\nAGFS-Tractometry, that leverages tract spatial information and permutation\ntesting to enhance the along-tract statistical analysis between populations.\nThere are two major contributions in AGFS-Tractometry. First, we create a novel\natlas-guided tract profiling template that enables consistent, fine-scale,\nalong-tract parcellation of subject-specific fiber tracts. Second, we propose a\nnovel nonparametric permutation testing group comparison method to enable\nsimultaneous analysis across all along-tract parcels while correcting for\nmultiple comparisons. We perform experimental evaluations on synthetic datasets\nwith known group differences and in vivo real data. We compare AGFS-Tractometry\nwith two state-of-the-art tractometry methods, including Automated Fiber-tract\nQuantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the\nproposed AGFS-Tractometry obtains enhanced sensitivity and specificity in\ndetecting local WM differences. In the real data analysis experiments,\nAGFS-Tractometry can identify more regions with significant differences, which\nare anatomically consistent with the existing literature. Overall, these\ndemonstrate the ability of AGFS-Tractometry to detect subtle or spatially\nlocalized WM group-level differences. The created tract profiling template and\nrelated code are available at:\nhttps://github.com/ZhengRuixi/AGFS-Tractometry.git.", "AI": {"tldr": "AGFS-Tractometry is a novel atlas-guided tractometry method for fine-scale along-tract analysis in diffusion MRI, improving sensitivity and specificity in detecting white matter differences.", "motivation": "To enhance along-tract statistical analysis in tractometry by leveraging spatial information and permutation testing for better detection of local white matter differences.", "method": "Proposes an atlas-guided tract profiling template for consistent parcellation and a nonparametric permutation testing method for group comparisons.", "result": "AGFS-Tractometry outperforms AFQ and BUAN in sensitivity and specificity, identifying more anatomically consistent significant regions in real data.", "conclusion": "AGFS-Tractometry effectively detects subtle or localized white matter differences, with tools and code publicly available."}}
{"id": "2507.10611", "pdf": "https://arxiv.org/pdf/2507.10611", "abs": "https://arxiv.org/abs/2507.10611", "authors": ["Mengwen Ye", "Yingzi Huangfu", "Shujian Gao", "Wei Ren", "Weifan Liu", "Zekuan Yu"], "title": "FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated Learning (FL) emerged as a solution for collaborative medical image\nclassification while preserving data privacy. However, label noise, which\narises from inter-institutional data variability, can cause training\ninstability and degrade model performance. Existing FL methods struggle with\nnoise heterogeneity and the imbalance in medical data. Motivated by these\nchallenges, we propose FedGSCA, a novel framework for enhancing robustness in\nnoisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates\nnoise knowledge from all clients, effectively addressing noise heterogeneity\nand improving global model stability. Furthermore, we develop a Client Adaptive\nAdjustment (CAA) mechanism that combines adaptive threshold pseudo-label\ngeneration and Robust Credal Labeling Loss. CAA dynamically adjusts to class\ndistributions, ensuring the inclusion of minority samples and carefully\nmanaging noisy labels by considering multiple plausible labels. This dual\napproach mitigates the impact of noisy data and prevents overfitting during\nlocal training, which improves the generalizability of the model. We evaluate\nFedGSCA on one real-world colon slides dataset and two synthetic medical\ndatasets under various noise conditions, including symmetric, asymmetric,\nextreme, and heterogeneous types. The results show that FedGSCA outperforms the\nstate-of-the-art methods, excelling in extreme and heterogeneous noise\nscenarios. Moreover, FedGSCA demonstrates significant advantages in improving\nmodel stability and handling complex noise, making it well-suited for\nreal-world medical federated learning scenarios.", "AI": {"tldr": "FedGSCA is a novel federated learning framework designed to handle label noise and data imbalance in medical image classification, improving model robustness and stability.", "motivation": "Label noise and data imbalance in federated learning for medical images degrade model performance, necessitating a robust solution.", "method": "FedGSCA uses a Global Sample Selector to aggregate noise knowledge and a Client Adaptive Adjustment mechanism for dynamic adaptation to class distributions and noise handling.", "result": "FedGSCA outperforms state-of-the-art methods, especially in extreme and heterogeneous noise scenarios, and improves model stability.", "conclusion": "FedGSCA is effective for real-world medical federated learning, addressing noise and imbalance challenges."}}
{"id": "2507.10623", "pdf": "https://arxiv.org/pdf/2507.10623", "abs": "https://arxiv.org/abs/2507.10623", "authors": ["Daniel Saragih", "Deyu Cao", "Tejas Balaji"], "title": "Flows and Diffusions on the Neural Manifold", "categories": ["cs.LG", "cs.CV"], "comment": "40 pages, 6 figures, 13 tables", "summary": "Diffusion and flow-based generative models have achieved remarkable success\nin domains such as image synthesis, video generation, and natural language\nmodeling. In this work, we extend these advances to weight space learning by\nleveraging recent techniques to incorporate structural priors derived from\noptimization dynamics. Central to our approach is modeling the trajectory\ninduced by gradient descent as a trajectory inference problem. We unify several\ntrajectory inference techniques under the framework of gradient flow matching,\nproviding a theoretical framework for treating optimization paths as inductive\nbias. We further explore architectural and algorithmic choices, including\nreward fine-tuning by adjoint matching, the use of autoencoders for latent\nweight representation, conditioning on task-specific context data, and adopting\ninformative source distributions such as Kaiming uniform. Experiments\ndemonstrate that our method matches or surpasses baselines in generating\nin-distribution weights, improves initialization for downstream training, and\nsupports fine-tuning to enhance performance. Finally, we illustrate a practical\napplication in safety-critical systems: detecting harmful covariate shifts,\nwhere our method outperforms the closest comparable baseline.", "AI": {"tldr": "The paper extends diffusion and flow-based generative models to weight space learning, leveraging optimization dynamics as structural priors. It introduces gradient flow matching for trajectory inference and explores architectural choices, achieving strong results in weight generation and downstream tasks.", "motivation": "To advance generative models by applying them to weight space learning, using optimization dynamics as structural priors for improved performance.", "method": "Models gradient descent trajectories as a trajectory inference problem, unifying techniques under gradient flow matching. Includes reward fine-tuning, autoencoders, task-specific conditioning, and informative source distributions.", "result": "Matches or surpasses baselines in weight generation, improves downstream training initialization, and enhances fine-tuning performance. Outperforms in detecting harmful covariate shifts.", "conclusion": "The method effectively applies generative models to weight space learning, offering theoretical and practical benefits for optimization and safety-critical applications."}}
{"id": "2507.10637", "pdf": "https://arxiv.org/pdf/2507.10637", "abs": "https://arxiv.org/abs/2507.10637", "authors": ["\u00c9. K\u00fcnzel", "A. Jaziri", "V. Ramesh"], "title": "A Simple Baseline for Stable and Plastic Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "11 pages, 50 figures", "summary": "Continual learning in computer vision requires that models adapt to a\ncontinuous stream of tasks without forgetting prior knowledge, yet existing\napproaches often tip the balance heavily toward either plasticity or stability.\nWe introduce RDBP, a simple, low-overhead baseline that unites two\ncomplementary mechanisms: ReLUDown, a lightweight activation modification that\npreserves feature sensitivity while preventing neuron dormancy, and Decreasing\nBackpropagation, a biologically inspired gradient-scheduling scheme that\nprogressively shields early layers from catastrophic updates. Evaluated on the\nContinual ImageNet benchmark, RDBP matches or exceeds the plasticity and\nstability of state-of-the-art methods while reducing computational cost. RDBP\nthus provides both a practical solution for real-world continual learning and a\nclear benchmark against which future continual learning strategies can be\nmeasured.", "AI": {"tldr": "RDBP introduces ReLUDown and Decreasing Backpropagation to balance plasticity and stability in continual learning, outperforming state-of-the-art methods with lower computational cost.", "motivation": "Addressing the challenge of balancing plasticity and stability in continual learning for computer vision.", "method": "Combines ReLUDown (lightweight activation modification) and Decreasing Backpropagation (gradient-scheduling scheme).", "result": "Matches or exceeds state-of-the-art methods in plasticity and stability on Continual ImageNet, with reduced computational cost.", "conclusion": "RDBP is a practical, efficient benchmark for future continual learning strategies."}}
{"id": "2507.10768", "pdf": "https://arxiv.org/pdf/2507.10768", "abs": "https://arxiv.org/abs/2507.10768", "authors": ["Bart Pogodzinski", "Christopher Wewer", "Bernt Schiele", "Jan Eric Lenssen"], "title": "Spatial Reasoners for Continuous Variables in Any Domain", "categories": ["cs.LG", "cs.CV"], "comment": "For the project documentation see https://spatialreasoners.github.io/\n  . The SRM project website is available at\n  https://geometric-rl.mpi-inf.mpg.de/srm/ . The work was published on ICML\n  2025 CODEML workshop", "summary": "We present Spatial Reasoners, a software framework to perform spatial\nreasoning over continuous variables with generative denoising models. Denoising\ngenerative models have become the de-facto standard for image generation, due\nto their effectiveness in sampling from complex, high-dimensional\ndistributions. Recently, they have started being explored in the context of\nreasoning over multiple continuous variables. Providing infrastructure for\ngenerative reasoning with such models requires a high effort, due to a wide\nrange of different denoising formulations, samplers, and inference strategies.\nOur presented framework aims to facilitate research in this area, providing\neasy-to-use interfaces to control variable mapping from arbitrary data domains,\ngenerative model paradigms, and inference strategies. Spatial Reasoners are\nopenly available at https://spatialreasoners.github.io/", "AI": {"tldr": "Spatial Reasoners is a framework for spatial reasoning using generative denoising models, simplifying research with easy-to-use interfaces.", "motivation": "The complexity of denoising formulations, samplers, and inference strategies makes generative reasoning research challenging.", "method": "The framework provides interfaces for variable mapping, generative model paradigms, and inference strategies.", "result": "Spatial Reasoners is openly available, facilitating research in generative spatial reasoning.", "conclusion": "The framework lowers the barrier for exploring generative reasoning with denoising models."}}
{"id": "2507.10776", "pdf": "https://arxiv.org/pdf/2507.10776", "abs": "https://arxiv.org/abs/2507.10776", "authors": ["Howard H. Qian", "Yiting Chen", "Gaotian Wang", "Podshara Chanrungmaneekul", "Kaiyu Hang"], "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, IROS 2025, Interactive Perception, Segmentation, Robotics,\n  Computer Vision", "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.", "AI": {"tldr": "The paper introduces rt-RISeg, a real-time interactive perception framework for unseen object instance segmentation (UOIS), leveraging robot interactions and body frame-invariant features (BFIF) to improve generalization and accuracy.", "motivation": "Current UOIS methods overfit on static visual features, leading to poor generalization in new environments. The authors propose an interactive approach inspired by the idea that vision is inherently dynamic.", "method": "The rt-RISeg framework uses robot interactions to analyze BFIF, specifically relative rotational and linear velocities of body frames, to segment objects without relying on learned models. It updates masks in real-time during interactions.", "result": "The method achieves a 27.5% higher segmentation accuracy than state-of-the-art UOIS techniques and can also enhance vision foundation models when used as prompts.", "conclusion": "Interactive perception, as demonstrated by rt-RISeg, offers a robust solution for UOIS, outperforming traditional methods and enabling seamless integration with advanced vision models."}}
{"id": "2507.10869", "pdf": "https://arxiv.org/pdf/2507.10869", "abs": "https://arxiv.org/abs/2507.10869", "authors": ["Chetan Madan", "Aarjav Satia", "Soumen Basu", "Pankaj Gupta", "Usha Dutta", "Chetan Arora"], "title": "Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification", "categories": ["eess.IV", "cs.CV"], "comment": "To appear at MICCAI 2025", "summary": "Masked Autoencoders (MAEs) have emerged as a dominant strategy for\nself-supervised representation learning in natural images, where models are\npre-trained to reconstruct masked patches with a pixel-wise mean squared error\n(MSE) between original and reconstructed RGB values as the loss. We observe\nthat MSE encourages blurred image re-construction, but still works for natural\nimages as it preserves dominant edges. However, in medical imaging, when the\ntexture cues are more important for classification of a visual abnormality, the\nstrategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM)\nfeature in Radiomics studies, we propose a novel MAE based pre-training\nframework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM\ncaptures intensity and spatial relationships in an image, hence proposed loss\nhelps preserve morphological features. Further, we propose a novel formulation\nto convert matching GLCM matrices into a differentiable loss function. We\ndemonstrate that unsupervised pre-training on medical images with the proposed\nGLCM loss improves representations for downstream tasks. GLCM-MAE outperforms\nthe current state-of-the-art across four tasks - gallbladder cancer detection\nfrom ultrasound images by 2.1%, breast cancer detection from ultrasound by\n3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by\n0.6%. Source code and pre-trained models are available at:\nhttps://github.com/ChetanMadan/GLCM-MAE.", "AI": {"tldr": "GLCM-MAE improves medical image representation by replacing pixel-wise MSE with GLCM-based loss, enhancing texture preservation and outperforming state-of-the-art in multiple tasks.", "motivation": "Standard MAEs using MSE loss blur textures, which are critical in medical imaging. GLCM-MAE addresses this by preserving morphological features.", "method": "Proposes GLCM-MAE, a framework using GLCM-based reconstruction loss to capture intensity and spatial relationships, with a differentiable GLCM loss formulation.", "result": "GLCM-MAE outperforms state-of-the-art in gallbladder cancer (2.1%), breast cancer (3.1%), pneumonia (0.5%), and COVID detection (0.6%).", "conclusion": "GLCM-MAE effectively preserves texture cues in medical images, improving downstream task performance."}}
{"id": "2507.10960", "pdf": "https://arxiv.org/pdf/2507.10960", "abs": "https://arxiv.org/abs/2507.10960", "authors": ["He Zhu", "Ryo Miyoshi", "Yuki Okafuji"], "title": "Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Prior human-robot interaction (HRI) research has primarily focused on\nsingle-user interactions, where robots do not need to consider the timing or\nrecipient of their responses. However, in multi-party interactions, such as at\nmalls and hospitals, social robots must understand the context and decide both\nwhen and to whom they should respond. In this paper, we propose a\nTransformer-based multi-task learning framework to improve the decision-making\nprocess of social robots, particularly in multi-user environments. Considering\nthe characteristics of HRI, we propose two novel loss functions: one that\nenforces constraints on active speakers to improve scene modeling, and another\nthat guides response selection towards utterances specifically directed at the\nrobot. Additionally, we construct a novel multi-party HRI dataset that captures\nreal-world complexities, such as gaze misalignment. Experimental results\ndemonstrate that our model achieves state-of-the-art performance in respond\ndecisions, outperforming existing heuristic-based and single-task approaches.\nOur findings contribute to the development of socially intelligent social\nrobots capable of engaging in natural and context-aware multi-party\ninteractions.", "AI": {"tldr": "A Transformer-based multi-task learning framework improves social robots' decision-making in multi-user HRI, using novel loss functions and a new dataset.", "motivation": "Address the gap in multi-party HRI by enabling robots to decide when and to whom to respond, unlike single-user interactions.", "method": "Proposes a Transformer-based multi-task learning framework with two novel loss functions for scene modeling and response selection, validated on a new multi-party HRI dataset.", "result": "Achieves state-of-the-art performance in response decisions, outperforming heuristic-based and single-task approaches.", "conclusion": "Advances socially intelligent robots for natural, context-aware multi-party interactions."}}
{"id": "2507.11001", "pdf": "https://arxiv.org/pdf/2507.11001", "abs": "https://arxiv.org/abs/2507.11001", "authors": ["Yanbo Wang", "Zipeng Fang", "Lei Zhao", "Weidong Chen"], "title": "Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Service robots are increasingly deployed in diverse and dynamic environments,\nwhere both physical layouts and social contexts change over time and across\nlocations. In these unstructured settings, conventional navigation systems that\nrely on fixed parameters often fail to generalize across scenarios, resulting\nin degraded performance and reduced social acceptance. Although recent\napproaches have leveraged reinforcement learning to enhance traditional\nplanners, these methods often fail in real-world deployments due to poor\ngeneralization and limited simulation diversity, which hampers effective\nsim-to-real transfer. To tackle these issues, we present LE-Nav, an\ninterpretable and scene-aware navigation framework that leverages multi-modal\nlarge language model reasoning and conditional variational autoencoders to\nadaptively tune planner hyperparameters. To achieve zero-shot scene\nunderstanding, we utilize one-shot exemplars and chain-of-thought prompting\nstrategies. Additionally, a conditional variational autoencoder captures the\nmapping between natural language instructions and navigation hyperparameters,\nenabling expert-level tuning. Experiments show that LE-Nav can generate\nhyperparameters achieving human-level tuning across diverse planners and\nscenarios. Real-world navigation trials and a user study on a smart wheelchair\nplatform demonstrate that it outperforms state-of-the-art methods on\nquantitative metrics such as success rate, efficiency, safety, and comfort,\nwhile receiving higher subjective scores for perceived safety and social\nacceptance. Code is available at https://github.com/Cavendish518/LE-Nav.", "AI": {"tldr": "LE-Nav is a scene-aware navigation framework using multi-modal LLMs and conditional variational autoencoders for adaptive hyperparameter tuning, outperforming state-of-the-art methods in real-world trials.", "motivation": "Conventional navigation systems fail in dynamic environments due to poor generalization. Reinforcement learning approaches also struggle with sim-to-real transfer.", "method": "LE-Nav combines multi-modal LLM reasoning and conditional variational autoencoders for zero-shot scene understanding and expert-level hyperparameter tuning.", "result": "Achieves human-level tuning, outperforms SOTA in success rate, efficiency, safety, and comfort, with higher subjective scores for social acceptance.", "conclusion": "LE-Nav effectively addresses generalization and sim-to-real challenges, enhancing robot navigation in unstructured environments."}}
{"id": "2507.11069", "pdf": "https://arxiv.org/pdf/2507.11069", "abs": "https://arxiv.org/abs/2507.11069", "authors": ["Jeongyun Kim", "Seunghoon Jeong", "Giseop Kim", "Myung-Hwan Jeon", "Eunji Jun", "Ayoung Kim"], "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding the 3D geometry of transparent objects from RGB images is\nchallenging due to their inherent physical properties, such as reflection and\nrefraction. To address these difficulties, especially in scenarios with sparse\nviews and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian\nSplatting-based depth reconstruction method for transparent objects. Our key\ninsight lies in separating transparent objects from the background, enabling\nfocused optimization of Gaussians corresponding to the object. We mitigate\nartifacts with an object-aware loss that places Gaussians in obscured regions,\nensuring coverage of invisible surfaces while reducing overfitting.\nFurthermore, we incorporate a physics-based simulation that refines the\nreconstruction in just a few seconds, effectively handling object removal and\nchain-reaction movement of remaining objects without the need for rescanning.\nTRAN-D is evaluated on both synthetic and real-world sequences, and it\nconsistently demonstrated robust improvements over existing GS-based\nstate-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean\nabsolute error by over 39% for the synthetic TRansPose sequences. Furthermore,\ndespite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm\naccuracy of 48.46%, over 1.5 times that of baselines, which uses six images.\nCode and more results are available at https://jeongyun0609.github.io/TRAN-D/.", "AI": {"tldr": "TRAN-D is a novel 2D Gaussian Splatting-based method for reconstructing the depth of transparent objects from RGB images, addressing challenges like reflection and refraction. It outperforms existing methods with significant accuracy improvements.", "motivation": "Transparent objects pose unique challenges in 3D geometry reconstruction due to properties like reflection and refraction, especially in sparse-view and dynamic scenarios.", "method": "TRAN-D separates transparent objects from the background, uses an object-aware loss for artifact mitigation, and incorporates physics-based simulation for refinement.", "result": "TRAN-D reduces mean absolute error by over 39% on synthetic data and achieves higher accuracy with fewer images compared to baselines.", "conclusion": "TRAN-D offers a robust and efficient solution for transparent object depth reconstruction, outperforming state-of-the-art methods."}}
{"id": "2507.11071", "pdf": "https://arxiv.org/pdf/2507.11071", "abs": "https://arxiv.org/abs/2507.11071", "authors": ["Isaiah Thompson Ocansey", "Ritwik Bhattacharya", "Tanmay Sen"], "title": "LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Log anomaly detection using traditional rule based or deep learning based\nmethods is often challenging due to the large volume and highly complex nature\nof log sequence. So effective way of detection of anomalous sequence of logs is\ncrucial for system maintenance and development. This paper proposes parameter\nefficient finetuning specifically low rank adaptation (LoRA) and adapter based\napproaches for finding contextual anomalies in sequence of logs in large log\ndata set. It compares different tiny large language models (LLMs) on the\nThunderbird dataset. The results show that LoRA based finetuning provides\nsubstantial performance improvements of 18 to 19 percentage over LogBert based\nfull finetuning approach, achieving accuracy scores between 97.76% and 98.83%\ncompared to 79.37%.", "AI": {"tldr": "The paper proposes parameter-efficient finetuning methods (LoRA and adapters) for log anomaly detection, outperforming traditional approaches with significant accuracy improvements.", "motivation": "Traditional log anomaly detection methods struggle with large, complex log sequences, necessitating more efficient solutions.", "method": "Uses LoRA and adapter-based finetuning on tiny LLMs, tested on the Thunderbird dataset.", "result": "LoRA finetuning improves accuracy by 18-19%, achieving 97.76%-98.83% compared to 79.37% with LogBert.", "conclusion": "Parameter-efficient finetuning, especially LoRA, is highly effective for log anomaly detection."}}
{"id": "2507.11302", "pdf": "https://arxiv.org/pdf/2507.11302", "abs": "https://arxiv.org/abs/2507.11302", "authors": ["Jesse J. Hagenaars", "Stein Stroobants", "Sander M. Bohte", "Guido C. H. E. De Croon"], "title": "All Eyes, no IMU: Learning Flight Attitude from Vision Alone", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision is an essential part of attitude control for many flying animals, some\nof which have no dedicated sense of gravity. Flying robots, on the other hand,\ntypically depend heavily on accelerometers and gyroscopes for attitude\nstabilization. In this work, we present the first vision-only approach to\nflight control for use in generic environments. We show that a quadrotor drone\nequipped with a downward-facing event camera can estimate its attitude and\nrotation rate from just the event stream, enabling flight control without\ninertial sensors. Our approach uses a small recurrent convolutional neural\nnetwork trained through supervised learning. Real-world flight tests\ndemonstrate that our combination of event camera and low-latency neural network\nis capable of replacing the inertial measurement unit in a traditional flight\ncontrol loop. Furthermore, we investigate the network's generalization across\ndifferent environments, and the impact of memory and different fields of view.\nWhile networks with memory and access to horizon-like visual cues achieve best\nperformance, variants with a narrower field of view achieve better relative\ngeneralization. Our work showcases vision-only flight control as a promising\ncandidate for enabling autonomous, insect-scale flying robots.", "AI": {"tldr": "A vision-only flight control system for drones using an event camera and neural network, eliminating the need for inertial sensors.", "motivation": "To enable flight control in drones without relying on traditional inertial sensors, inspired by flying animals that use vision for attitude control.", "method": "Uses a downward-facing event camera and a recurrent convolutional neural network trained via supervised learning to estimate attitude and rotation rate.", "result": "Demonstrated successful flight control without inertial sensors, with performance varying based on memory, field of view, and visual cues.", "conclusion": "Vision-only flight control is viable and promising for insect-scale autonomous flying robots."}}
{"id": "2507.11401", "pdf": "https://arxiv.org/pdf/2507.11401", "abs": "https://arxiv.org/abs/2507.11401", "authors": ["Mehri Mehrnia", "Mohammed S. M. Elbaz"], "title": "Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI", "categories": ["quant-ph", "cs.CV", "cs.ET", "cs.LG"], "comment": "Accepted for publication at IEEE International Conference on Quantum\n  Computing and Engineering (QCE) 2025", "summary": "Efficient entanglement strategies are essential for advancing variational\nquantum circuits (VQCs) for quantum machine learning (QML). However, most\ncurrent approaches use fixed entanglement topologies that are not adaptive to\ntask requirements, limiting potential gains over classical models. We introduce\na novel stochastic entanglement configuration method that systematically\ngenerates diverse entanglement topologies to identify a subspace of\nconstructive entanglement configurations, defined as entanglement topologies\nthat boost hybrid model performance (e.g., classification accuracy) beyond\nclassical baselines. Each configuration is encoded as a stochastic binary\nmatrix, denoting directed entanglement between qubits. This enables scalable\nexploration of the hyperspace of candidate entanglement topologies using\nentanglement density and per-qubit constraints as key metrics. We define\nunconstrained and constrained sampling modes, controlling entanglement per\nqubit. Using our method, 400 stochastic configurations were generated and\nevaluated in a hybrid QML for cardiac MRI disease classification. We identified\n64 (16%) novel constructive entanglement configurations that consistently\noutperformed the classical baseline. Ensemble aggregation of top-performing\nconfigurations achieved ~0.92 classification accuracy, exceeding the classical\nmodel (~0.87) by over 5%. Compared to four conventional topologies (ring,\nnearest neighbor, no entanglement, fully entangled), none surpassed the\nclassical baseline (maximum accuracy ~0.82), while our configurations delivered\nup to ~20% higher accuracy. Thus, highlighting the robustness and\ngeneralizability of the identified constructive entanglements.", "AI": {"tldr": "A stochastic entanglement configuration method is introduced to enhance variational quantum circuits for QML, outperforming classical models by identifying constructive entanglement topologies.", "motivation": "Current fixed entanglement topologies in VQCs limit performance gains over classical models, necessitating adaptive strategies.", "method": "A stochastic method generates diverse entanglement topologies encoded as binary matrices, evaluated using density and per-qubit constraints.", "result": "64 (16%) constructive configurations outperformed classical baselines, with ensemble accuracy reaching ~0.92 (vs. ~0.87 classical).", "conclusion": "The method demonstrates robustness and generalizability, significantly improving QML performance over conventional topologies."}}
{"id": "2507.11415", "pdf": "https://arxiv.org/pdf/2507.11415", "abs": "https://arxiv.org/abs/2507.11415", "authors": ["Hongbo Ye", "Fenghe Tang", "Peiang Zhao", "Zhen Huang", "Dexin Zhao", "Minghao Bian", "S. Kevin Zhou"], "title": "U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by MICCAI2025", "summary": "Achieving equity in healthcare accessibility requires lightweight yet\nhigh-performance solutions for medical image segmentation, particularly in\nresource-limited settings. Existing methods like U-Net and its variants often\nsuffer from limited global Effective Receptive Fields (ERFs), hindering their\nability to capture long-range dependencies. To address this, we propose U-RWKV,\na novel framework leveraging the Recurrent Weighted Key-Value(RWKV)\narchitecture, which achieves efficient long-range modeling at O(N)\ncomputational cost. The framework introduces two key innovations: the\nDirection-Adaptive RWKV Module(DARM) and the Stage-Adaptive\nSqueeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan\nmechanisms to aggregate contextual cues across images, mitigating directional\nbias while preserving global context and maintaining high computational\nefficiency. SASE dynamically adapts its architecture to different feature\nextraction stages, balancing high-resolution detail preservation and semantic\nrelationship capture. Experiments demonstrate that U-RWKV achieves\nstate-of-the-art segmentation performance with high computational efficiency,\noffering a practical solution for democratizing advanced medical imaging\ntechnologies in resource-constrained environments. The code is available at\nhttps://github.com/hbyecoding/U-RWKV.", "AI": {"tldr": "U-RWKV is a novel framework for medical image segmentation, combining RWKV architecture with DARM and SASE modules to achieve efficient long-range modeling and high performance in resource-limited settings.", "motivation": "Addressing the limitations of existing methods like U-Net in capturing long-range dependencies due to limited global Effective Receptive Fields (ERFs), especially in resource-constrained healthcare environments.", "method": "Proposes U-RWKV, leveraging RWKV architecture with Direction-Adaptive RWKV Module (DARM) and Stage-Adaptive Squeeze-and-Excitation Module (SASE) for efficient long-range modeling and dynamic feature adaptation.", "result": "U-RWKV achieves state-of-the-art segmentation performance with high computational efficiency, validated through experiments.", "conclusion": "U-RWKV offers a practical, high-performance solution for medical image segmentation in resource-limited settings, democratizing advanced imaging technologies."}}
{"id": "2507.11461", "pdf": "https://arxiv.org/pdf/2507.11461", "abs": "https://arxiv.org/abs/2507.11461", "authors": ["Christian Daniele", "Silvia Villa", "Samuel Vaiter", "Luca Calatroni"], "title": "Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent", "categories": ["math.OC", "cs.CV", "65K10, 65J22, 94A08, 47N10"], "comment": null, "summary": "Deep Equilibrium Models (DEQs) are implicit neural networks with fixed\npoints, which have recently gained attention for learning image regularization\nfunctionals, particularly in settings involving Gaussian fidelities, where\nassumptions on the forward operator ensure contractiveness of standard\n(proximal) Gradient Descent operators. In this work, we extend the application\nof DEQs to Poisson inverse problems, where the data fidelity term is more\nappropriately modeled by the Kullback-Leibler divergence. To this end, we\nintroduce a novel DEQ formulation based on Mirror Descent defined in terms of a\ntailored non-Euclidean geometry that naturally adapts with the structure of the\ndata term. This enables the learning of neural regularizers within a principled\ntraining framework. We derive sufficient conditions to guarantee the\nconvergence of the learned reconstruction scheme and propose computational\nstrategies that enable both efficient training and fully parameter-free\ninference. Numerical experiments show that our method outperforms traditional\nmodel-based approaches and it is comparable to the performance of Bregman\nPlug-and-Play methods, while mitigating their typical drawbacks - namely,\nsensitivity to initialization and careful tuning of hyperparameters. The code\nis publicly available at https://github.com/christiandaniele/DEQ-MD.", "AI": {"tldr": "The paper extends Deep Equilibrium Models (DEQs) to Poisson inverse problems using a novel Mirror Descent-based DEQ formulation, outperforming traditional methods and matching Bregman Plug-and-Play without their drawbacks.", "motivation": "To address Poisson inverse problems, where traditional DEQs are limited to Gaussian fidelities, by introducing a non-Euclidean geometry adapted to the Kullback-Leibler divergence.", "method": "A novel DEQ formulation using Mirror Descent with a tailored non-Euclidean geometry, ensuring convergence and enabling efficient training and inference.", "result": "The method outperforms traditional model-based approaches and matches Bregman Plug-and-Play performance while avoiding sensitivity to initialization and hyperparameter tuning.", "conclusion": "The proposed DEQ-MD framework is effective for Poisson inverse problems, offering robust performance and computational efficiency."}}
{"id": "2507.11465", "pdf": "https://arxiv.org/pdf/2507.11465", "abs": "https://arxiv.org/abs/2507.11465", "authors": ["Nuri Ryu", "Jiyun Won", "Jooeun Son", "Minsu Gong", "Joo-Haeng Lee", "Sunghyun Cho"], "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to SIGGRAPH 2025. For the project page, see\n  https://cg.postech.ac.kr/research/Elevate3D/", "summary": "High-quality 3D assets are essential for various applications in computer\ngraphics and 3D vision but remain scarce due to significant acquisition costs.\nTo address this shortage, we introduce Elevate3D, a novel framework that\ntransforms readily accessible low-quality 3D assets into higher quality. At the\ncore of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that\nsignificantly improves texture quality while preserving the appearance and\ngeometry while fixing its degradations. Furthermore, Elevate3D operates in a\nview-by-view manner, alternating between texture and geometry refinement.\nUnlike previous methods that have largely overlooked geometry refinement, our\nframework leverages geometric cues from images refined with HFS-SDEdit by\nemploying state-of-the-art monocular geometry predictors. This approach ensures\ndetailed and accurate geometry that aligns seamlessly with the enhanced\ntexture. Elevate3D outperforms recent competitors by achieving state-of-the-art\nquality in 3D model refinement, effectively addressing the scarcity of\nhigh-quality open-source 3D assets.", "AI": {"tldr": "Elevate3D enhances low-quality 3D assets into high-quality ones using HFS-SDEdit for texture improvement and monocular geometry predictors for geometry refinement, outperforming competitors.", "motivation": "High-quality 3D assets are scarce due to high acquisition costs, limiting applications in computer graphics and 3D vision.", "method": "Elevate3D uses HFS-SDEdit for texture enhancement and alternates between texture and geometry refinement, leveraging monocular geometry predictors for accurate geometry.", "result": "The framework achieves state-of-the-art quality in 3D model refinement, addressing the scarcity of high-quality 3D assets.", "conclusion": "Elevate3D effectively transforms low-quality 3D assets into high-quality ones, outperforming existing methods."}}

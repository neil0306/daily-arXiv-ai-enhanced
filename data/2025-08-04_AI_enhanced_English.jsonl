{"id": "2508.00079", "pdf": "https://arxiv.org/pdf/2508.00079", "abs": "https://arxiv.org/abs/2508.00079", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "categories": ["cs.CL", "cs.AI"], "comment": "Under review, 18 pages, 4 figures, 7 tables", "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.", "AI": {"tldr": "The paper evaluates frontier LLMs in solving physics problems, using multi-agent frameworks and inference-time techniques to improve performance, and introduces a new benchmark, ${\\rm P{\\small HYSICS}E{\\small VAL}}$.", "motivation": "To assess and enhance the capability of LLMs in solving physics problems, both mathematical and descriptive, and to provide a standardized benchmark for future research.", "method": "Employing inference-time techniques and multi-agent frameworks, including solution verification by smaller LLM agents, and comparative analysis of performance improvements.", "result": "Significant performance improvements, especially for initially poorly solved problems, using the multi-agent framework.", "conclusion": "The multi-agent approach enhances LLM performance in physics problem-solving, and the new benchmark, ${\\rm P{\\small HYSICS}E{\\small VAL}}$, provides a valuable resource for future evaluations."}}
{"id": "2508.00086", "pdf": "https://arxiv.org/pdf/2508.00086", "abs": "https://arxiv.org/abs/2508.00086", "authors": ["Kelly Kendro", "Jeffrey Maloney", "Scott Jarvis"], "title": "Do LLMs produce texts with \"human-like\" lexical diversity?", "categories": ["cs.CL"], "comment": "35 pages; includes abstract", "summary": "The degree to which LLMs produce writing that is truly human-like remains\nunclear despite the extensive empirical attention that this question has\nreceived. The present study addresses this question from the perspective of\nlexical diversity. Specifically, the study investigates patterns of lexical\ndiversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,\nand -4.5) in comparison with texts written by L1 and L2 English participants (n\n= 240) across four education levels. Six dimensions of lexical diversity were\nmeasured in each text: volume, abundance, variety-repetition, evenness,\ndisparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and\nSupport Vector Machines revealed that the LLM-generated texts differed\nsignificantly from human-written texts for each variable, with ChatGPT-o4 mini\nand -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated\nhigher levels of lexical diversity despite producing fewer tokens. The human\nwriters' lexical diversity did not differ across subgroups (i.e., education,\nlanguage status). Altogether, the results indicate that LLMs do not produce\nhuman-like texts in relation to lexical diversity, and the newer LLMs produce\nless human-like texts than older models. We discuss the implications of these\nresults for language pedagogy and related applications.", "AI": {"tldr": "The study compares lexical diversity in texts from four ChatGPT models and human writers, finding significant differences, with newer models being less human-like.", "motivation": "To assess how human-like LLM-generated texts are in terms of lexical diversity.", "method": "Analyzed six lexical diversity dimensions in texts from ChatGPT models and human participants using statistical methods (MANOVAs, ANOVAs, SVMs).", "result": "LLM texts differed significantly from human texts, with newer models (ChatGPT-o4 mini and -4.5) being least human-like. Human writers' diversity was consistent across subgroups.", "conclusion": "LLMs do not produce human-like lexical diversity, and newer models are less human-like. Implications for language pedagogy are discussed."}}
{"id": "2508.00095", "pdf": "https://arxiv.org/pdf/2508.00095", "abs": "https://arxiv.org/abs/2508.00095", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture", "categories": ["cs.CL", "cs.CY"], "comment": "Preprint. Manuscript currently under review", "summary": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.", "AI": {"tldr": "The paper argues for greater theorizing of methods in computational humanities to improve clarity and avoid translation errors in modeling cultural and linguistic data.", "motivation": "The need for epistemological and interpretive clarity in computational humanities to mature the field and avoid errors in modeling practices.", "method": "Framing modeling as translation between cultural/linguistic and computational/mathematical domains, introducing the concept of semiotic complexity.", "result": "Identifies a translation error where semiotically complex data is treated as simple, leading to superficial clarity.", "conclusion": "Provides recommendations for researchers to better address epistemological issues in their work."}}
{"id": "2508.00109", "pdf": "https://arxiv.org/pdf/2508.00109", "abs": "https://arxiv.org/abs/2508.00109", "authors": ["Mingda Chen", "Yang Li", "Xilun Chen", "Adina Williams", "Gargi Ghosh", "Scott Yih"], "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.", "AI": {"tldr": "FACTORY is a human-verified benchmark for evaluating long-form factuality in language models, revealing 40% inaccuracies in SOTA models.", "motivation": "Existing benchmarks lack human verification, leading to potential quality issues in evaluating model factuality.", "method": "Developed FACTORY using a model-in-the-loop approach with human refinement, featuring challenging, fact-seeking prompts.", "result": "40% of claims by SOTA models were non-factual on FACTORY, compared to 10% on other datasets.", "conclusion": "FACTORY is a reliable, challenging benchmark, highlighting the need for models to reason across long-tailed facts."}}
{"id": "2508.00053", "pdf": "https://arxiv.org/pdf/2508.00053", "abs": "https://arxiv.org/abs/2508.00053", "authors": ["Jie Zhu", "Yiyang Su", "Minchul Kim", "Anil Jain", "Xiaoming Liu"], "title": "A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025. 11 pages, 5 figures", "summary": "Whole-body biometric recognition is a challenging multimodal task that\nintegrates various biometric modalities, including face, gait, and body. This\nintegration is essential for overcoming the limitations of unimodal systems.\nTraditionally, whole-body recognition involves deploying different models to\nprocess multiple modalities, achieving the final outcome by score-fusion (e.g.,\nweighted averaging of similarity matrices from each model). However, these\nconventional methods may overlook the variations in score distributions of\nindividual modalities, making it challenging to improve final performance. In\nthis work, we present \\textbf{Q}uality-guided \\textbf{M}ixture of score-fusion\n\\textbf{E}xperts (QME), a novel framework designed for improving whole-body\nbiometric recognition performance through a learnable score-fusion strategy\nusing a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for\nquality estimation with a modality-specific Quality Estimator (QE), and a score\ntriplet loss to improve the metric performance. Extensive experiments on\nmultiple whole-body biometric datasets demonstrate the effectiveness of our\nproposed approach, achieving state-of-the-art results across various metrics\ncompared to baseline methods. Our method is effective for multimodal and\nmulti-model, addressing key challenges such as model misalignment in the\nsimilarity score domain and variability in data quality.", "AI": {"tldr": "The paper introduces QME, a learnable score-fusion framework for whole-body biometric recognition, addressing limitations of traditional methods by incorporating quality estimation and metric improvement.", "motivation": "Traditional score-fusion methods for whole-body biometric recognition overlook score distribution variations, limiting performance. QME aims to improve this by integrating quality-guided fusion.", "method": "QME uses a Mixture of Experts (MoE) for learnable score-fusion, with a pseudo-quality loss for quality estimation and a score triplet loss for metric enhancement.", "result": "Experiments show QME achieves state-of-the-art performance on whole-body biometric datasets, outperforming baseline methods.", "conclusion": "QME effectively addresses challenges like model misalignment and data quality variability, proving superior for multimodal and multi-model biometric recognition."}}
{"id": "2508.00121", "pdf": "https://arxiv.org/pdf/2508.00121", "abs": "https://arxiv.org/abs/2508.00121", "authors": ["Xiao Zhang", "Johan bos"], "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "categories": ["cs.CL"], "comment": "Accepted by 16th IWCS", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation", "AI": {"tldr": "Neural semantic parsers excel in general but struggle with context-sensitive phenomena like verb phrase ellipsis, failing despite high overall performance.", "motivation": "To evaluate neural semantic parsers' ability to handle strongly context-sensitive linguistic phenomena, specifically verb phrase ellipsis.", "method": "Constructed a corpus of 120 ellipsis cases with resolved meaning representations and tested various neural semantic parsers.", "result": "Parsers performed well on standard tests but failed on ellipsis cases.", "conclusion": "Neural semantic parsers are inadequate for context-sensitive phenomena like ellipsis, highlighting a need for improvement."}}
{"id": "2508.00085", "pdf": "https://arxiv.org/pdf/2508.00085", "abs": "https://arxiv.org/abs/2508.00085", "authors": ["Raiyaan Abdullah", "Jared Claypoole", "Michael Cogswell", "Ajay Divakaran", "Yogesh Rawat"], "title": "Punching Bag vs. Punching Person: Motion Transferability in Videos", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025 main conference", "summary": "Action recognition models demonstrate strong generalization, but can they\neffectively transfer high-level motion concepts across diverse contexts, even\nwithin similar distributions? For example, can a model recognize the broad\naction \"punching\" when presented with an unseen variation such as \"punching\nperson\"? To explore this, we introduce a motion transferability framework with\nthree datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)\nKinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural\nvideo datasets. We evaluate 13 state-of-the-art models on these benchmarks and\nobserve a significant drop in performance when recognizing high-level actions\nin novel contexts. Our analysis reveals: 1) Multimodal models struggle more\nwith fine-grained unknown actions than with coarse ones; 2) The bias-free\nSyn-TA proves as challenging as real-world datasets, with models showing\ngreater performance drops in controlled settings; 3) Larger models improve\ntransferability when spatial cues dominate but struggle with intensive temporal\nreasoning, while reliance on object and background cues hinders generalization.\nWe further explore how disentangling coarse and fine motions can improve\nrecognition in temporally challenging datasets. We believe this study\nestablishes a crucial benchmark for assessing motion transferability in action\nrecognition. Datasets and relevant code:\nhttps://github.com/raiyaan-abdullah/Motion-Transfer.", "AI": {"tldr": "The paper investigates motion transferability in action recognition models, revealing performance drops in novel contexts and proposing a framework with synthetic and real-world datasets.", "motivation": "To assess if action recognition models can generalize high-level motion concepts across diverse contexts, even within similar distributions.", "method": "Introduces a motion transferability framework with three datasets (Syn-TA, Kinetics400-TA, Something-Something-v2-TA) and evaluates 13 state-of-the-art models.", "result": "Performance drops significantly for novel contexts; multimodal models struggle with fine-grained actions, and larger models face challenges with temporal reasoning.", "conclusion": "The study establishes a benchmark for motion transferability, highlighting the need for disentangling coarse and fine motions to improve recognition."}}
{"id": "2508.00185", "pdf": "https://arxiv.org/pdf/2508.00185", "abs": "https://arxiv.org/abs/2508.00185", "authors": ["Alper Yaman", "Jannik Schwab", "Christof Nitsche", "Abhirup Sinha", "Marco Huber"], "title": "Comparison of Large Language Models for Deployment Requirements", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.", "AI": {"tldr": "A comparative list of foundational and domain-specific LLMs is provided to help researchers and companies navigate the evolving landscape, focusing on features like licensing and hardware requirements.", "motivation": "The rapid introduction of numerous open-source LLMs has made it challenging to select the optimal model, necessitating a clear comparison tool.", "method": "The paper compiles and compares foundational and domain-specific LLMs, detailing features such as release year, licensing, and hardware requirements.", "result": "A continuously updated comparative list of LLMs is published on GitLab.", "conclusion": "The list serves as a practical resource for selecting LLMs, addressing the complexity of the current landscape."}}
{"id": "2508.00088", "pdf": "https://arxiv.org/pdf/2508.00088", "abs": "https://arxiv.org/abs/2508.00088", "authors": ["Mateo de Mayo", "Daniel Cremers", "Taih\u00fa Pire"], "title": "The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to IROS 2025", "summary": "Humanoid robots and mixed reality headsets benefit from the use of\nhead-mounted sensors for tracking. While advancements in visual-inertial\nodometry (VIO) and simultaneous localization and mapping (SLAM) have produced\nnew and high-quality state-of-the-art tracking systems, we show that these are\nstill unable to gracefully handle many of the challenging settings presented in\nthe head-mounted use cases. Common scenarios like high-intensity motions,\ndynamic occlusions, long tracking sessions, low-textured areas, adverse\nlighting conditions, saturation of sensors, to name a few, continue to be\ncovered poorly by existing datasets in the literature. In this way, systems may\ninadvertently overlook these essential real-world issues. To address this, we\npresent the Monado SLAM dataset, a set of real sequences taken from multiple\nvirtual reality headsets. We release the dataset under a permissive CC BY 4.0\nlicense, to drive advancements in VIO/SLAM research and development.", "AI": {"tldr": "The paper introduces the Monado SLAM dataset to address gaps in existing VIO/SLAM datasets for head-mounted tracking, covering challenging real-world scenarios.", "motivation": "Existing VIO/SLAM systems struggle with head-mounted tracking challenges like high-intensity motions, dynamic occlusions, and adverse conditions, which are underrepresented in current datasets.", "method": "The authors present the Monado SLAM dataset, comprising real sequences from multiple VR headsets, released under a permissive CC BY 4.0 license.", "result": "The dataset aims to improve VIO/SLAM research by providing data for overlooked real-world scenarios.", "conclusion": "The Monado SLAM dataset is a resource to advance tracking systems by addressing critical gaps in existing datasets."}}
{"id": "2508.00217", "pdf": "https://arxiv.org/pdf/2508.00217", "abs": "https://arxiv.org/abs/2508.00217", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "categories": ["cs.CL", "cs.DB", "cs.LG"], "comment": null, "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "AI": {"tldr": "The paper discusses challenges in table understanding tasks for LLMs and MLLMs, proposing a taxonomy of tabular inputs and highlighting gaps like limited reasoning, complex table processing, and generalization issues.", "motivation": "The diversity and complexity of table formats in LLMs and MLLMs necessitate specialized methods, making universal approaches difficult.", "method": "Introduces a taxonomy of tabular input representations and table understanding tasks to address these challenges.", "result": "Identifies critical gaps: retrieval-focused tasks, processing challenges for complex/large tables, and limited model generalization.", "conclusion": "Further research is needed to improve reasoning, handling of complex tables, and generalization across tabular formats."}}
{"id": "2508.00135", "pdf": "https://arxiv.org/pdf/2508.00135", "abs": "https://arxiv.org/abs/2508.00135", "authors": ["Basna Mohammed Salih Hasan", "Ramadhan J. Mstafa"], "title": "Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages, 18 figures, 5 tables", "summary": "Gender classification has emerged as a crucial aspect in various fields,\nincluding security, human-machine interaction, surveillance, and advertising.\nNonetheless, the accuracy of this classification can be influenced by factors\nsuch as cosmetics and disguise. Consequently, our study is dedicated to\naddressing this concern by concentrating on gender classification using color\nimages of the periocular region. The periocular region refers to the area\nsurrounding the eye, including the eyelids, eyebrows, and the region between\nthem. It contains valuable visual cues that can be used to extract key features\nfor gender classification. This paper introduces a sophisticated Convolutional\nNeural Network (CNN) model that utilizes color image databases to evaluate the\neffectiveness of the periocular region for gender classification. To validate\nthe model's performance, we conducted tests on two eye datasets, namely CVBL\nand (Female and Male). The recommended architecture achieved an outstanding\naccuracy of 99% on the previously unused CVBL dataset while attaining a\ncommendable accuracy of 96% with a small number of learnable parameters\n(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of\nour proposed model for gender classification using the periocular region, we\nevaluated its performance through an extensive range of metrics and compared it\nwith other state-of-the-art approaches. The results unequivocally demonstrate\nthe efficacy of our model, thereby suggesting its potential for practical\napplication in domains such as security and surveillance.", "AI": {"tldr": "A CNN model for gender classification using the periocular region achieves high accuracy (99% and 96%) on two datasets, outperforming state-of-the-art methods.", "motivation": "Gender classification is vital in security and surveillance but is affected by cosmetics and disguise. The study focuses on the periocular region for reliable classification.", "method": "A sophisticated CNN model is developed, tested on CVBL and (Female and Male) datasets, and compared with other approaches.", "result": "The model achieved 99% accuracy on CVBL and 96% on (Female and Male) with minimal parameters (7,235,089).", "conclusion": "The model is highly effective, suggesting practical applications in security and surveillance."}}
{"id": "2508.00220", "pdf": "https://arxiv.org/pdf/2508.00220", "abs": "https://arxiv.org/abs/2508.00220", "authors": ["Rana Aref Salama", "Abdou Youssef", "Mona Diab"], "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform", "categories": ["cs.CL"], "comment": null, "summary": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.", "AI": {"tldr": "The paper explores using Discrete Wavelet Transforms (DWT) on word and sentence embeddings to analyze and compress them while preserving semantic quality, showing significant dimensionality reduction with minimal performance loss.", "motivation": "Wavelet transforms are effective in other domains, and the paper aims to extend their use to NLP for analyzing and compressing embeddings without losing semantic information.", "method": "Empirically applies DWT to embeddings, evaluates on semantic similarity tasks, and tests with various embedding models, including large language models.", "result": "DWT reduces embedding dimensionality by 50-93% with almost no performance drop in semantic tasks and often improves accuracy in downstream tasks.", "conclusion": "DWT is a promising tool for enhancing NLP applications by efficiently compressing and analyzing embeddings."}}
{"id": "2508.00144", "pdf": "https://arxiv.org/pdf/2508.00144", "abs": "https://arxiv.org/abs/2508.00144", "authors": ["Akshat Rakheja", "Aarsh Ashdhir", "Aryan Bhattacharjee", "Vanshika Sharma"], "title": "World Consistency Score: A Unified Metric for Video Generation Quality", "categories": ["cs.CV"], "comment": "27 pages, 1 figure", "summary": "We introduce World Consistency Score (WCS), a novel unified evaluation metric\nfor generative video models that emphasizes internal world consistency of the\ngenerated videos. WCS integrates four interpretable sub-components - object\npermanence, relation stability, causal compliance, and flicker penalty - each\nmeasuring a distinct aspect of temporal and physical coherence in a video.\nThese submetrics are combined via a learned weighted formula to produce a\nsingle consistency score that aligns with human judgments. We detail the\nmotivation for WCS in the context of existing video evaluation metrics,\nformalize each submetric and how it is computed with open-source tools\n(trackers, action recognizers, CLIP embeddings, optical flow), and describe how\nthe weights of the WCS combination are trained using human preference data. We\nalso outline an experimental validation blueprint: using benchmarks like\nVBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human\nevaluations, performing sensitivity analyses, and comparing WCS against\nestablished metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a\ncomprehensive and interpretable framework for evaluating video generation\nmodels on their ability to maintain a coherent \"world\" over time, addressing\ngaps left by prior metrics focused only on visual fidelity or prompt alignment.", "AI": {"tldr": "World Consistency Score (WCS) is a new metric for evaluating generative video models, focusing on internal world consistency through four sub-components. It combines these into a single score aligned with human judgment.", "motivation": "Existing video evaluation metrics often overlook temporal and physical coherence, focusing instead on visual fidelity or prompt alignment. WCS addresses this gap by emphasizing world consistency.", "method": "WCS integrates four submetrics (object permanence, relation stability, causal compliance, flicker penalty) computed using open-source tools. A learned weighted formula combines these into a unified score.", "result": "WCS is validated using benchmarks like VBench-2.0 and compared to metrics like FVD and CLIPScore, showing alignment with human evaluations.", "conclusion": "WCS provides a comprehensive, interpretable framework for assessing video generation models' ability to maintain a coherent world over time, filling gaps in existing metrics."}}
{"id": "2508.00238", "pdf": "https://arxiv.org/pdf/2508.00238", "abs": "https://arxiv.org/abs/2508.00238", "authors": ["Bryce Anderson", "Riley Galpin", "Tom S. Juzek"], "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2\n  figures, 2 tables. Licensed under CC BY-SA 4.0", "summary": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.", "AI": {"tldr": "The study examines shifts in word usage in spoken language post-ChatGPT's release, finding a moderate increase in LLM-associated words, suggesting AI's influence on human language.", "motivation": "To investigate whether changes in word usage reflect broader shifts in human language due to AI influence or natural evolution.", "method": "Analyzed a dataset of 22.1 million words from unscripted spoken language in science/tech podcasts, comparing trends before and after ChatGPT's 2022 release.", "result": "Found a significant increase in LLM-associated words post-2022, indicating convergence with AI patterns, while baseline synonyms showed no change.", "conclusion": "The findings suggest AI may be influencing human language, raising ethical concerns about misaligned models shaping social norms."}}
{"id": "2508.00152", "pdf": "https://arxiv.org/pdf/2508.00152", "abs": "https://arxiv.org/abs/2508.00152", "authors": ["Li Mi", "Manon Bechaz", "Zeming Chen", "Antoine Bosselut", "Devis Tuia"], "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page at https://limirs.github.io/GeoExplorer/", "summary": "Active Geo-localization (AGL) is the task of localizing a goal, represented\nin various modalities (e.g., aerial images, ground-level images, or text),\nwithin a predefined search area. Current methods approach AGL as a\ngoal-reaching reinforcement learning (RL) problem with a distance-based reward.\nThey localize the goal by implicitly learning to minimize the relative distance\nfrom it. However, when distance estimation becomes challenging or when\nencountering unseen targets and environments, the agent exhibits reduced\nrobustness and generalization ability due to the less reliable exploration\nstrategy learned during training. In this paper, we propose GeoExplorer, an AGL\nagent that incorporates curiosity-driven exploration through intrinsic rewards.\nUnlike distance-based rewards, our curiosity-driven reward is goal-agnostic,\nenabling robust, diverse, and contextually relevant exploration based on\neffective environment modeling. These capabilities have been proven through\nextensive experiments across four AGL benchmarks, demonstrating the\neffectiveness and generalization ability of GeoExplorer in diverse settings,\nparticularly in localizing unfamiliar targets and environments.", "AI": {"tldr": "GeoExplorer introduces curiosity-driven exploration for Active Geo-localization (AGL), improving robustness and generalization over distance-based RL methods.", "motivation": "Current AGL methods rely on distance-based rewards, which struggle with challenging distance estimation and unseen targets/environments, reducing robustness.", "method": "Proposes GeoExplorer, an AGL agent using curiosity-driven intrinsic rewards for goal-agnostic, diverse exploration.", "result": "Demonstrates effectiveness across four AGL benchmarks, excelling in unfamiliar targets and environments.", "conclusion": "GeoExplorer's curiosity-driven approach enhances AGL robustness and generalization, outperforming traditional methods."}}
{"id": "2508.00285", "pdf": "https://arxiv.org/pdf/2508.00285", "abs": "https://arxiv.org/abs/2508.00285", "authors": ["Peixian Li", "Yu Tian", "Ruiqi Tu", "Chengkai Wu", "Jingjing Ren", "Jingsong Li"], "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering", "categories": ["cs.CL", "I.2.7; J.3"], "comment": "23 pages, 8 figures", "summary": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.", "AI": {"tldr": "The paper introduces an Etiology-Aware Attention Steering Framework to improve LLMs' diagnostic accuracy and clinical reasoning by integrating structured clinical guidelines and fine-tuning attention mechanisms.", "motivation": "Despite LLMs' capabilities in medical text understanding, their diagnostic reliability in complex clinical scenarios is limited. This study aims to enhance their accuracy and reasoning.", "method": "The framework uses Clinical Reasoning Scaffolding (CRS) from guidelines, identifies key attention heads, and employs Reasoning-Guided Parameter-Efficient Fine-tuning to align reasoning cues.", "result": "The framework improves diagnostic accuracy by 15.65% and Reasoning Focus Score by 31.6%, with external validation confirming effectiveness.", "conclusion": "The approach enhances LLM-based clinical reasoning, offering a reliable and interpretable paradigm for AI diagnostics in complex settings."}}
{"id": "2508.00169", "pdf": "https://arxiv.org/pdf/2508.00169", "abs": "https://arxiv.org/abs/2508.00169", "authors": ["Bhavya Goyal", "Felipe Gutierrez-Barragan", "Wei Lin", "Andreas Velten", "Yin Li", "Mohit Gupta"], "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "LiDAR-based 3D sensors provide point clouds, a canonical 3D representation\nused in various scene understanding tasks. Modern LiDARs face key challenges in\nseveral real-world scenarios, such as long-distance or low-albedo objects,\nproducing sparse or erroneous point clouds. These errors, which are rooted in\nthe noisy raw LiDAR measurements, get propagated to downstream perception\nmodels, resulting in potentially severe loss of accuracy. This is because\nconventional 3D processing pipelines do not retain any uncertainty information\nfrom the raw measurements when constructing point clouds.\n  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation\nwhere each point is augmented with a probability attribute that encapsulates\nthe measurement uncertainty (or confidence) in the raw data. We further\nintroduce inference approaches that leverage PPC for robust 3D object\ndetection; these methods are versatile and can be used as computationally\nlightweight drop-in modules in 3D inference pipelines. We demonstrate, via both\nsimulations and real captures, that PPC-based 3D inference methods outperform\nseveral baselines using LiDAR as well as camera-LiDAR fusion models, across\nchallenging indoor and outdoor scenarios involving small, distant, and\nlow-albedo objects, as well as strong ambient light.\n  Our project webpage is at https://bhavyagoyal.github.io/ppc .", "AI": {"tldr": "The paper introduces Probabilistic Point Clouds (PPC), a 3D representation that includes uncertainty attributes for each point, improving accuracy in LiDAR-based scene understanding tasks.", "motivation": "Addressing challenges like sparse or erroneous point clouds from LiDARs in scenarios involving long-distance or low-albedo objects, which degrade downstream perception models.", "method": "Proposes PPC, where each point has a probability attribute reflecting measurement uncertainty, and introduces inference methods for robust 3D object detection.", "result": "PPC-based methods outperform baselines in simulations and real captures, especially for small, distant, low-albedo objects and strong ambient light.", "conclusion": "PPC enhances 3D perception by incorporating uncertainty, offering a lightweight, versatile solution for improving LiDAR-based models."}}
{"id": "2508.00305", "pdf": "https://arxiv.org/pdf/2508.00305", "abs": "https://arxiv.org/abs/2508.00305", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "categories": ["cs.CL", "cs.LG", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "AI": {"tldr": "This paper benchmarks optimization techniques for LLMs, analyzing their impact on performance, scalability, and text generation quality, revealing pitfalls in naive combinations and trade-offs in metrics like F1.", "motivation": "To address the resource demands and limited context windows of LLMs by evaluating pruning, quantization, and token dropping in long-context scenarios.", "method": "Systematically benchmarks individual and combined optimization methods for LLMs, analyzing memory, latency, throughput, and text generation quality, including scalability tests on a 70B-parameter model.", "result": "Naive combinations of optimization methods harm larger models due to compounded errors, and F1 scores can mask precision-recall trade-offs in QA tasks.", "conclusion": "The study provides insights for balancing efficiency, accuracy, and scalability in LLMs, aiding practitioners in optimizing for diverse tasks and hardware."}}
{"id": "2508.00171", "pdf": "https://arxiv.org/pdf/2508.00171", "abs": "https://arxiv.org/abs/2508.00171", "authors": ["David Restrepo", "Ira Ktena", "Maria Vakalopoulou", "Stergios Christodoulidis", "Enzo Ferrante"], "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to MICCAI 2025 1st Workshop on Multimodal Large Language\n  Models (MLLMs) in Clinical Practice", "summary": "Clinical decision-making relies on the integrated analysis of medical images\nand the associated clinical reports. While Vision-Language Models (VLMs) can\noffer a unified framework for such tasks, they can exhibit strong biases toward\none modality, frequently overlooking critical visual cues in favor of textual\ninformation. In this work, we introduce Selective Modality Shifting (SMS), a\nperturbation-based approach to quantify a model's reliance on each modality in\nbinary classification tasks. By systematically swapping images or text between\nsamples with opposing labels, we expose modality-specific biases. We assess six\nopen-source VLMs-four generalist models and two fine-tuned for medical data-on\ntwo medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)\nand FairVLMed (scanning laser ophthalmoscopy). By assessing model performance\nand the calibration of every model in both unperturbed and perturbed settings,\nwe reveal a marked dependency on text input, which persists despite the\npresence of complementary visual information. We also perform a qualitative\nattention-based analysis which further confirms that image content is often\novershadowed by text details. Our findings highlight the importance of\ndesigning and evaluating multimodal medical models that genuinely integrate\nvisual and textual cues, rather than relying on single-modality signals.", "AI": {"tldr": "The paper introduces Selective Modality Shifting (SMS) to quantify modality biases in Vision-Language Models (VLMs) for medical tasks, revealing a strong reliance on text over visual cues.", "motivation": "To address biases in VLMs that favor text over visual information in clinical decision-making, which can overlook critical visual cues.", "method": "SMS, a perturbation-based approach, swaps images or text between samples with opposing labels to expose biases. Evaluated on six VLMs using MIMIC-CXR and FairVLMed datasets.", "result": "VLMs show marked dependency on text, overshadowing visual information, even in models fine-tuned for medical data.", "conclusion": "Highlights the need for multimodal models that genuinely integrate both visual and textual cues, not just single-modality signals."}}
{"id": "2508.00332", "pdf": "https://arxiv.org/pdf/2508.00332", "abs": "https://arxiv.org/abs/2508.00332", "authors": ["Kaiyan Zhao", "Zhongtao Miao", "Yoshimasa Tsuruoka"], "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.", "AI": {"tldr": "MCSEO improves multimodal sentence embeddings by aligning fine-grained object-phrase pairs, outperforming baselines on STS tasks.", "motivation": "Noise in image-caption pairs (redundant/irrelevant info) hampers multimodal sentence embedding quality.", "method": "MCSEO uses segmentation/object detection to extract object-phrase pairs, optimizing contrastive learning for alignment.", "result": "MCSEO consistently outperforms baselines on semantic textual similarity tasks.", "conclusion": "Precise object-phrase alignment is crucial for effective multimodal representation learning."}}
{"id": "2508.00197", "pdf": "https://arxiv.org/pdf/2508.00197", "abs": "https://arxiv.org/abs/2508.00197", "authors": ["Eric Mjolsness", "Cory B. Scott"], "title": "Graph Lineages and Skeletal Graph Products", "categories": ["cs.CV", "cs.LG", "cs.NA", "math.CT", "math.NA"], "comment": "42 pages. 33 Figures. Under review", "summary": "Graphs, and sequences of growing graphs, can be used to specify the\narchitecture of mathematical models in many fields including machine learning\nand computational science. Here we define structured graph \"lineages\" (ordered\nby level number) that grow in a hierarchical fashion, so that: (1) the number\nof graph vertices and edges increases exponentially in level number; (2)\nbipartite graphs connect successive levels within a graph lineage and, as in\nmultigrid methods, can constrain matrices relating successive levels; (3) using\nprolongation maps within a graph lineage, process-derived distance measures\nbetween graphs at successive levels can be defined; (4) a category of \"graded\ngraphs\" can be defined, and using it low-cost \"skeletal\" variants of standard\nalgebraic graph operations and type constructors (cross product, box product,\ndisjoint sum, and function types) can be derived for graded graphs and hence\nhierarchical graph lineages; (5) these skeletal binary operators have similar\nbut not identical algebraic and category-theoretic properties to their standard\ncounterparts; (6) graph lineages and their skeletal product constructors can\napproach continuum limit objects. Additional space-efficient unary operators on\ngraded graphs are also derived: thickening, which creates a graph lineage of\nmultiscale graphs, and escalation to a graph lineage of search frontiers\n(useful as a generalization of adaptive grids and in defining \"skeletal\"\nfunctions). The result is an algebraic type theory for graded graphs and\n(hierarchical) graph lineages. The approach is expected to be well suited to\ndefining hierarchical model architectures - \"hierarchitectures\" - and local\nsampling, search, or optimization algorithms on them. We demonstrate such\napplication to deep neural networks (including visual and feature scale spaces)\nand to multigrid numerical methods.", "AI": {"tldr": "The paper introduces structured graph lineages for hierarchical model architectures, enabling exponential growth, skeletal algebraic operations, and applications in machine learning and computational science.", "motivation": "To define hierarchical graph structures for modeling in fields like machine learning and computational science, enabling efficient operations and continuum limits.", "method": "Defines graph lineages with hierarchical growth, bipartite connections, and prolongation maps. Introduces skeletal algebraic operations and unary operators like thickening and escalation.", "result": "Develops an algebraic type theory for graded graphs and hierarchical lineages, applicable to deep neural networks and multigrid methods.", "conclusion": "The framework is suitable for hierarchical model architectures and local algorithms, demonstrated in deep learning and numerical methods."}}
{"id": "2508.00344", "pdf": "https://arxiv.org/pdf/2508.00344", "abs": "https://arxiv.org/abs/2508.00344", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.", "AI": {"tldr": "The paper introduces AdaPlan, a global plan-based agent paradigm, and PilotRL, a reinforcement learning framework, to enhance LLM agents' long-term planning and execution coordination, outperforming GPT-4o.", "motivation": "Existing LLM agent paradigms like ReAct lack effectiveness in complex, long-term tasks due to single-step reasoning and poor planner-executor coordination, and supervised fine-tuning limits generalization.", "method": "Proposes AdaPlan for explicit guidance and execution synergy, and PilotRL for progressive reinforcement learning to train LLM agents in global planning and execution optimization.", "result": "PilotRL achieves state-of-the-art performance, with LLaMA3.1-8B-Instruct + PilotRL surpassing GPT-4o by 3.60% and GPT-4o-mini by 55.78%.", "conclusion": "AdaPlan and PilotRL address limitations of current LLM agent paradigms, improving long-horizon decision-making and generalization."}}
{"id": "2508.00205", "pdf": "https://arxiv.org/pdf/2508.00205", "abs": "https://arxiv.org/abs/2508.00205", "authors": ["Xiangyu Kong", "Hengde Zhu", "Haoqin Sun", "Zhihao Guo", "Jiayan Gu", "Xinyi Ni", "Wei Zhang", "Shizhe Liu", "Siyang Song"], "title": "Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "Automatic real personality recognition (RPR) aims to evaluate human real\npersonality traits from their expressive behaviours. However, most existing\nsolutions generally act as external observers to infer observers' personality\nimpressions based on target individuals' expressive behaviours, which\nsignificantly deviate from their real personalities and consistently lead to\ninferior recognition performance. Inspired by the association between real\npersonality and human internal cognition underlying the generation of\nexpressive behaviours, we propose a novel RPR approach that efficiently\nsimulates personalised internal cognition from easy-accessible external short\naudio-visual behaviours expressed by the target individual. The simulated\npersonalised cognition, represented as a set of network weights that enforce\nthe personalised network to reproduce the individual-specific facial reactions,\nis further encoded as a novel graph containing two-dimensional node and edge\nfeature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for\ninferring real personality traits from it. To simulate real personality-related\ncognition, an end-to-end strategy is designed to jointly train our cognition\nsimulation, 2D graph construction, and personality recognition modules.", "AI": {"tldr": "The paper introduces a novel approach for real personality recognition (RPR) by simulating personalized internal cognition from external behaviors, using a 2D-GNN for inference.", "motivation": "Existing RPR methods infer personality impressions from external behaviors, often deviating from real personalities. The paper aims to bridge this gap by leveraging internal cognition.", "method": "Proposes a method to simulate personalized cognition from audio-visual behaviors, represented as a graph with 2D node/edge features, and uses a 2D-GNN for personality inference.", "result": "The approach jointly trains cognition simulation, graph construction, and recognition modules for improved RPR performance.", "conclusion": "The novel 2D-GNN-based method effectively captures real personality traits by modeling internal cognition, outperforming traditional observer-based approaches."}}
{"id": "2508.00360", "pdf": "https://arxiv.org/pdf/2508.00360", "abs": "https://arxiv.org/abs/2508.00360", "authors": ["Alan Dao", "Dinh Bach Vu", "Alex Nguyen", "Norapat Buppodom"], "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors", "categories": ["cs.CL"], "comment": null, "summary": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.", "AI": {"tldr": "A 1.7B-parameter small language model (Lucy) uses dynamic task vector construction during reasoning, achieving 78.3% accuracy on SimpleQA, rivaling larger models.", "motivation": "Small language models (SLMs) struggle with knowledge-intensive tasks due to limited capacity. This work aims to enhance SLM performance by treating reasoning as a dynamic, self-refining process.", "method": "Proposes a paradigm where reasoning (delimited by <think> tags) is viewed as a dynamic task vector machine. Uses RLVR to optimize this mechanism and integrates MCP for training.", "result": "Lucy, a 1.7B-parameter SLM, achieves 78.3% accuracy on SimpleQA, matching larger models like DeepSeek-V3.", "conclusion": "Small models can compete with large ones when equipped with structured, self-constructed task reasoning."}}
{"id": "2508.00213", "pdf": "https://arxiv.org/pdf/2508.00213", "abs": "https://arxiv.org/abs/2508.00213", "authors": ["Shayan Jalilian", "Abdul Bais"], "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.", "AI": {"tldr": "SAM-PTx introduces a lightweight adapter, Parallel-Text, to integrate CLIP-derived text embeddings into SAM for semantics-guided segmentation, improving performance over spatial prompts.", "motivation": "The potential of semantic text prompts in SAM is underexplored compared to spatial prompts like points and boxes.", "method": "A parameter-efficient adapter, Parallel-Text, injects frozen CLIP text embeddings into SAM's image encoder, modifying only the MLP-parallel branch of transformer blocks.", "result": "Experiments on COD10K, COCO, and ADE20K show improved segmentation performance with text embeddings over spatial prompts.", "conclusion": "Integrating semantic conditioning into SAM offers a practical, scalable adaptation method with minimal computational overhead."}}
{"id": "2508.00370", "pdf": "https://arxiv.org/pdf/2508.00370", "abs": "https://arxiv.org/abs/2508.00370", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "AI": {"tldr": "EdgeInfinite-Instruct optimizes Transformer-based LLMs for edge devices by fine-tuning a small subset of parameters, employing S-SFT for long-sequence tasks, and using PTQ and fixed-shape computation for efficiency.", "motivation": "Challenges in deploying LLMs on edge devices include high computational costs, memory demands, and poor TTFT. Existing solutions either degrade performance or lack infrastructure support.", "method": "Proposes EdgeInfinite-Instruct with S-SFT for long-sequence tasks, PTQ for computational efficiency, and fixed-shape computation for memory optimization.", "result": "Improves performance on long-context benchmarks and mobile tasks while maintaining efficiency on NPU-accelerated devices.", "conclusion": "EdgeInfinite-Instruct effectively balances performance and efficiency for edge deployment, addressing limitations of prior methods."}}
{"id": "2508.00218", "pdf": "https://arxiv.org/pdf/2508.00218", "abs": "https://arxiv.org/abs/2508.00218", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "title": "Object-Centric Cropping for Visual Few-Shot Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In the domain of Few-Shot Image Classification, operating with as little as\none example per class, the presence of image ambiguities stemming from multiple\nobjects or complex backgrounds can significantly deteriorate performance. Our\nresearch demonstrates that incorporating additional information about the local\npositioning of an object within its image markedly enhances classification\nacross established benchmarks. More importantly, we show that a significant\nfraction of the improvement can be achieved through the use of the Segment\nAnything Model, requiring only a pixel of the object of interest to be pointed\nout, or by employing fully unsupervised foreground object extraction methods.", "AI": {"tldr": "Using local positioning info and SAM improves few-shot image classification, even with minimal supervision.", "motivation": "Address performance degradation in few-shot image classification due to image ambiguities like multiple objects or complex backgrounds.", "method": "Incorporate local positioning info and use Segment Anything Model (SAM) or unsupervised foreground extraction.", "result": "Significant improvement in classification benchmarks with minimal supervision (e.g., a single pixel).", "conclusion": "Local positioning and SAM enhance few-shot classification, offering practical solutions with low supervision."}}
{"id": "2508.00385", "pdf": "https://arxiv.org/pdf/2508.00385", "abs": "https://arxiv.org/abs/2508.00385", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "AI": {"tldr": "The paper investigates why some demonstrations in in-context learning (ICL) are ineffective, attributing it to learned or irrelevant information. It introduces GradS, a gradient-based method for selecting effective demonstrations, improving performance by 6.8% over baselines.", "motivation": "Existing work assumes all demonstrations in ICL are effective, but many fail to improve performance. This paper explores the reasons behind ineffective demonstrations.", "method": "The study uses gradient flow and linear self-attention models to analyze demonstration ineffectiveness. It proposes GradS, a gradient-based selection method, validated on four LLMs and five datasets.", "result": "Experiments show that demonstration effectiveness disparity grows with model layers. GradS outperforms baselines by 6.8% on average.", "conclusion": "GradS effectively selects demonstrations by leveraging gradient flow, addressing the limitations of current methods that ignore learned information."}}
{"id": "2508.00248", "pdf": "https://arxiv.org/pdf/2508.00248", "abs": "https://arxiv.org/abs/2508.00248", "authors": ["Chenggang Guo", "Hao Xu", "XianMing Wan"], "title": "Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network", "categories": ["cs.CV"], "comment": null, "summary": "Depth map super-resolution technology aims to improve the spatial resolution\nof low-resolution depth maps and effectively restore high-frequency detail\ninformation. Traditional convolutional neural network has limitations in\ndealing with long-range dependencies and are unable to fully model the global\ncontextual information in depth maps. Although transformer can model global\ndependencies, its computational complexity and memory consumption are\nquadratic, which significantly limits its ability to process high-resolution\ndepth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba\n(MSF-UM) model, a novel guided depth map super-resolution framework. The core\ninnovation of this model is to integrate Mamba's efficient state-space modeling\ncapabilities into a multi-scale U-shaped fusion structure guided by a color\nimage. The structure combining the residual dense channel attention block and\nthe Mamba state space module is designed, which combines the local feature\nextraction capability of the convolutional layer with the modeling advantage of\nthe state space model for long-distance dependencies. At the same time, the\nmodel adopts a multi-scale cross-modal fusion strategy to make full use of the\nhigh-frequency texture information from the color image to guide the\nsuper-resolution process of the depth map. Compared with existing mainstream\nmethods, the proposed MSF-UM significantly reduces the number of model\nparameters while achieving better reconstruction accuracy. Extensive\nexperiments on multiple publicly available datasets validate the effectiveness\nof the model, especially showing excellent generalization ability in the task\nof large-scale depth map super-resolution.", "AI": {"tldr": "The paper proposes a multi-scale fusion U-shaped Mamba (MSF-UM) model for depth map super-resolution, combining Mamba's state-space modeling with a U-shaped structure guided by color images to improve resolution and detail restoration.", "motivation": "Traditional CNNs struggle with long-range dependencies, and transformers are computationally expensive. The goal is to efficiently model global context for high-resolution depth maps.", "method": "The MSF-UM integrates Mamba's state-space modeling into a U-shaped fusion structure, using residual dense channel attention blocks and multi-scale cross-modal fusion with color images.", "result": "The model reduces parameters while achieving better accuracy, validated on public datasets with strong generalization for large-scale tasks.", "conclusion": "MSF-UM effectively balances computational efficiency and performance, advancing depth map super-resolution."}}
{"id": "2508.00390", "pdf": "https://arxiv.org/pdf/2508.00390", "abs": "https://arxiv.org/abs/2508.00390", "authors": ["Hengxing Cai", "Jinhan Dong", "Yijie Rao", "Jingcheng Deng", "Jingjun Tan", "Qien Chen", "Haidong Wang", "Zhen Wang", "Shiyu Huang", "Agachai Sumalee", "Renxin Zhong"], "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation", "categories": ["cs.CL"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.", "AI": {"tldr": "SA-GCS improves UAV VLN by integrating curriculum learning into RL, enhancing training efficiency and performance.", "motivation": "Address inefficiencies in RL methods for UAV VLN, such as slow convergence and poor data use.", "method": "Proposes SA-GCS with a Semantic-Aware Difficulty Estimator and Gaussian Curriculum Scheduler for dynamic task progression.", "result": "Outperforms baselines on CityNav, with faster convergence and better generalization.", "conclusion": "SA-GCS is robust, scalable, and improves UAV VLN performance effectively."}}
{"id": "2508.00259", "pdf": "https://arxiv.org/pdf/2508.00259", "abs": "https://arxiv.org/abs/2508.00259", "authors": ["Wentao Sun", "Hanqing Xu", "Quanyun Wu", "Dedong Zhang", "Yiping Chen", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting", "categories": ["cs.CV"], "comment": "22 pages, 9 figures", "summary": "We introduce PointGauss, a novel point cloud-guided framework for real-time\nmulti-object segmentation in Gaussian Splatting representations. Unlike\nexisting methods that suffer from prolonged initialization and limited\nmulti-view consistency, our approach achieves efficient 3D segmentation by\ndirectly parsing Gaussian primitives through a point cloud segmentation-driven\npipeline. The key innovation lies in two aspects: (1) a point cloud-based\nGaussian primitive decoder that generates 3D instance masks within 1 minute,\nand (2) a GPU-accelerated 2D mask rendering system that ensures multi-view\nconsistency. Extensive experiments demonstrate significant improvements over\nprevious state-of-the-art methods, achieving performance gains of 1.89 to\n31.78% in multi-view mIoU, while maintaining superior computational efficiency.\nTo address the limitations of current benchmarks (single-object focus,\ninconsistent 3D evaluation, small scale, and partial coverage), we present\nDesktopObjects-360, a novel comprehensive dataset for 3D segmentation in\nradiance fields, featuring: (1) complex multi-object scenes, (2) globally\nconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2D\nmasks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.", "AI": {"tldr": "PointGauss is a real-time multi-object segmentation framework for Gaussian Splatting, using point cloud guidance for efficient 3D segmentation and multi-view consistency. It outperforms prior methods and introduces a new dataset, DesktopObjects-360.", "motivation": "Existing methods for multi-object segmentation in Gaussian Splatting suffer from slow initialization and poor multi-view consistency.", "method": "PointGauss uses a point cloud-based Gaussian primitive decoder for fast 3D instance mask generation and a GPU-accelerated 2D mask rendering system for consistency.", "result": "Achieves 1.89 to 31.78% improvement in multi-view mIoU while being computationally efficient.", "conclusion": "PointGauss advances real-time 3D segmentation and introduces a comprehensive dataset, DesktopObjects-360, to address benchmark limitations."}}
{"id": "2508.00420", "pdf": "https://arxiv.org/pdf/2508.00420", "abs": "https://arxiv.org/abs/2508.00420", "authors": ["Rana Salama", "Abdou Youssef", "Mona Diab"], "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding", "categories": ["cs.CL"], "comment": null, "summary": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.", "AI": {"tldr": "The paper explores using Discrete Wavelet Transforms (DWT) and Discrete Cosine Transform (DCT) for compressing word and sentence embeddings in NLP, showing competitive or superior results to original embeddings.", "motivation": "Wavelets have proven effective in image and signal processing, suggesting potential for NLP tasks to capture linguistic properties.", "method": "Applies DWT to word embeddings for dimensionality reduction and combines DWT with DCT for non-parameterized sentence compression.", "result": "The proposed method yields comparable or superior results in downstream NLP tasks compared to original embeddings.", "conclusion": "Wavelet-based techniques offer an effective, non-parameterized approach for embedding compression in NLP."}}
{"id": "2508.00260", "pdf": "https://arxiv.org/pdf/2508.00260", "abs": "https://arxiv.org/abs/2508.00260", "authors": ["Hyundong Jin", "Hyung Jin Chang", "Eunwoo Kim"], "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted to ICCV 2025", "summary": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.", "AI": {"tldr": "A novel framework for continual learning in vision-language models ensures balanced focus on visual and language inputs by using specialized visual projectors and expert recommendation/pruning.", "motivation": "Address the neglect of language instructions in continual learning for vision-language models, which often prioritize visual inputs over repetitive textual instructions.", "method": "Introduces a mixture of visual projectors (experts) tailored to instruction contexts, with expert recommendation for task similarity and pruning to reduce interference.", "result": "Outperforms existing continual learning methods by generating better instruction-following responses across diverse tasks.", "conclusion": "The proposed framework effectively balances visual and language inputs in continual learning, improving task adaptation and response quality."}}
{"id": "2508.00429", "pdf": "https://arxiv.org/pdf/2508.00429", "abs": "https://arxiv.org/abs/2508.00429", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "AI": {"tldr": "ReaGAN introduces an agent-based GNN framework with node-level autonomy and retrieval-augmented generation to address limitations of fixed aggregation schemes, achieving competitive performance without fine-tuning.", "motivation": "Fixed GNN aggregation schemes struggle with node informativeness imbalance and lack global semantic relationships, limiting their effectiveness.", "method": "ReaGAN uses agent-based nodes with autonomous decision-making and retrieval-augmented generation (RAG) to enable adaptive message propagation and global semantic access.", "result": "ReaGAN performs competitively in few-shot settings using a frozen LLM backbone, demonstrating the value of agentic planning and retrieval.", "conclusion": "ReaGAN's agentic and retrieval-enhanced approach addresses key GNN limitations, offering a promising direction for graph learning."}}
{"id": "2508.00265", "pdf": "https://arxiv.org/pdf/2508.00265", "abs": "https://arxiv.org/abs/2508.00265", "authors": ["Henghui Ding", "Song Tang", "Shuting He", "Chang Liu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Multimodal Referring Segmentation: A Survey", "categories": ["cs.CV"], "comment": "Project Page:\n  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation", "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", "AI": {"tldr": "A survey on multimodal referring segmentation, covering background, methods, challenges, and applications across images, videos, and 3D scenes.", "motivation": "To address the need for accurate object perception based on user instructions in multimodal contexts.", "method": "Summarizes a unified meta architecture and reviews methods for images, videos, and 3D scenes, including Generalized Referring Expression (GREx) approaches.", "result": "Extensive performance comparisons on benchmarks and tracking of related works.", "conclusion": "Highlights advancements and challenges in multimodal referring segmentation, with ongoing updates via a GitHub repository."}}
{"id": "2508.00454", "pdf": "https://arxiv.org/pdf/2508.00454", "abs": "https://arxiv.org/abs/2508.00454", "authors": ["Yuqi Tang", "Kehua Feng", "Yunfeng Wang", "Zhiwen Chen", "Chengfei Lv", "Gang Yu", "Qiang Zhang", "Keyan Ding"], "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "categories": ["cs.CL"], "comment": "15 pages, 2 pages, under review at AAAI 2026", "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.", "AI": {"tldr": "Proposes an efficient multi-turn dialogue evaluator to reduce biases and computational costs in LLM-based dialogue quality assessment.", "motivation": "Current LLM-as-a-judge methods suffer from biases and high computational overhead, necessitating a more efficient solution.", "method": "Aggregates preference knowledge from multiple LLM judges into a single model to capture collective wisdom while reducing costs.", "result": "Outperforms existing baselines on seven benchmarks, demonstrating efficiency and robustness.", "conclusion": "The proposed method enables fast, flexible, and reliable dialogue quality assessment with reduced computational overhead."}}
{"id": "2508.00272", "pdf": "https://arxiv.org/pdf/2508.00272", "abs": "https://arxiv.org/abs/2508.00272", "authors": ["Wenyue Chong"], "title": "Towards Robust Semantic Correspondence: A Benchmark and Insights", "categories": ["cs.CV"], "comment": null, "summary": "Semantic correspondence aims to identify semantically meaningful\nrelationships between different images and is a fundamental challenge in\ncomputer vision. It forms the foundation for numerous tasks such as 3D\nreconstruction, object tracking, and image editing. With the progress of\nlarge-scale vision models, semantic correspondence has achieved remarkable\nperformance in controlled and high-quality conditions. However, the robustness\nof semantic correspondence in challenging scenarios is much less investigated.\nIn this work, we establish a novel benchmark for evaluating semantic\ncorrespondence in adverse conditions. The benchmark dataset comprises 14\ndistinct challenging scenarios that reflect commonly encountered imaging\nissues, including geometric distortion, image blurring, digital artifacts, and\nenvironmental occlusion. Through extensive evaluations, we provide several key\ninsights into the robustness of semantic correspondence approaches: (1) All\nexisting methods suffer from noticeable performance drops under adverse\nconditions; (2) Using large-scale vision models can enhance overall robustness,\nbut fine-tuning on these models leads to a decline in relative robustness; (3)\nThe DINO model outperforms the Stable Diffusion in relative robustness, and\ntheir fusion achieves better absolute robustness; Moreover, We evaluate common\nrobustness enhancement strategies for semantic correspondence and find that\ngeneral data augmentations are ineffective, highlighting the need for\ntask-specific designs. These results are consistent across both our dataset and\nreal-world benchmarks.", "AI": {"tldr": "A benchmark for evaluating semantic correspondence in adverse conditions reveals performance drops in existing methods, highlights the robustness of large-scale vision models, and identifies the need for task-specific enhancements.", "motivation": "Semantic correspondence is crucial for tasks like 3D reconstruction and image editing, but its robustness in challenging scenarios is understudied.", "method": "A novel benchmark dataset with 14 challenging scenarios (e.g., distortion, blurring) was created. Evaluations compared methods like DINO and Stable Diffusion, tested robustness strategies, and analyzed performance.", "result": "Existing methods perform poorly in adverse conditions. Large-scale models improve robustness, but fine-tuning reduces it. DINO outperforms Stable Diffusion, and their fusion enhances robustness. General data augmentations are ineffective.", "conclusion": "The study underscores the need for task-specific robustness enhancements in semantic correspondence and provides insights for future research."}}
{"id": "2508.00476", "pdf": "https://arxiv.org/pdf/2508.00476", "abs": "https://arxiv.org/abs/2508.00476", "authors": ["Jeongwoo Kang", "Markarit Vartampetian", "Felix Herron", "Yongxin Zhou", "Diandra Fabre", "Gabriela Gonzalez-Saez"], "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts", "categories": ["cs.CL"], "comment": null, "summary": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).", "AI": {"tldr": "GETALP's submission to SIGDial 2025 Task B uses RAG and AMR, showing AMR improves QA performance, especially for 'who' questions.", "motivation": "To enhance question-answering (QA) performance on meeting transcripts by integrating retrieval augmented generation (RAG) and Abstract Meaning Representations (AMR).", "method": "Proposed three systems combining RAG and AMR for QA on meeting transcripts.", "result": "AMR improved response quality for ~35% of questions, notably aiding 'who' questions.", "conclusion": "Incorporating AMR with RAG enhances QA, particularly for participant-specific queries."}}
{"id": "2508.00287", "pdf": "https://arxiv.org/pdf/2508.00287", "abs": "https://arxiv.org/abs/2508.00287", "authors": ["Tran Viet Khoa", "Do Hai Son", "Mohammad Abu Alsheikh", "Yibeltal F Alem", "Dinh Thai Hoang"], "title": "Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning", "categories": ["cs.CV"], "comment": null, "summary": "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.", "AI": {"tldr": "A novel framework for drowsiness detection using Spatial Self-Attention and LSTM, with federated learning via Gradient Similarity Comparison, achieves 89.9% accuracy.", "motivation": "Driver drowsiness causes accidents; detecting it accurately in decentralized, diverse real-world data is challenging.", "method": "Combines Spatial Self-Attention with LSTM for feature extraction, uses Gradient Similarity Comparison for federated learning, and includes automated video data processing.", "result": "89.9% detection accuracy in federated learning, outperforming existing methods.", "conclusion": "The framework effectively handles real-world data variability and shows promise for intelligent transportation systems."}}
{"id": "2508.00489", "pdf": "https://arxiv.org/pdf/2508.00489", "abs": "https://arxiv.org/abs/2508.00489", "authors": ["Yixuan Tang", "Jincheng Wang", "Anthony K. H. Tung"], "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection", "categories": ["cs.CL"], "comment": null, "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.", "AI": {"tldr": "The paper introduces the task of half-truth detection and proposes a new benchmark, PolitiFact-Hidden, along with TRACER, a framework to identify omission-based misinformation.", "motivation": "Existing fact verification systems fail to detect half-truths\u2014claims that are factually correct but misleading due to omitted context.", "method": "The authors propose TRACER, a modular framework that aligns evidence, infers implied intent, and estimates the causal impact of hidden content.", "result": "TRACER improves Half-True classification F1 by up to 16 points, enhancing existing fact-checking pipelines.", "conclusion": "Modeling omissions is crucial for trustworthy fact verification, as demonstrated by TRACER's effectiveness."}}
{"id": "2508.00289", "pdf": "https://arxiv.org/pdf/2508.00289", "abs": "https://arxiv.org/abs/2508.00289", "authors": ["Christian Simon", "Masato Ishii", "Akio Hayakawa", "Zhi Zhong", "Shusuke Takahashi", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "In the recent development of conditional diffusion models still require heavy\nsupervised fine-tuning for performing control on a category of tasks.\nTraining-free conditioning via guidance with off-the-shelf models is a\nfavorable alternative to avoid further fine-tuning on the base model. However,\nthe existing training-free guidance frameworks either have heavy memory\nrequirements or offer sub-optimal control due to rough estimation. These\nshortcomings limit the applicability to control diffusion models that require\nintense computation, such as Text-to-Video (T2V) diffusion models. In this\nwork, we propose Taming Inference Time Alignment for Guided Text-to-Video\nDiffusion Model, so-called TITAN-Guide, which overcomes memory space issues,\nand provides more optimal control in the guidance process compared to the\ncounterparts. In particular, we develop an efficient method for optimizing\ndiffusion latents without backpropagation from a discriminative guiding model.\nIn particular, we study forward gradient descents for guided diffusion tasks\nwith various options on directional directives. In our experiments, we\ndemonstrate the effectiveness of our approach in efficiently managing memory\nduring latent optimization, while previous methods fall short. Our proposed\napproach not only minimizes memory requirements but also significantly enhances\nT2V performance across a range of diffusion guidance benchmarks. Code, models,\nand demo are available at https://titanguide.github.io.", "AI": {"tldr": "TITAN-Guide is a training-free guidance framework for Text-to-Video diffusion models, addressing memory and control issues in existing methods.", "motivation": "Existing training-free guidance frameworks for diffusion models suffer from heavy memory usage or sub-optimal control, limiting their application in compute-intensive tasks like Text-to-Video generation.", "method": "Proposes TITAN-Guide, which optimizes diffusion latents without backpropagation using forward gradient descents and directional directives.", "result": "Efficiently manages memory during latent optimization and enhances Text-to-Video performance across benchmarks.", "conclusion": "TITAN-Guide overcomes memory and control limitations, improving Text-to-Video diffusion model guidance."}}
{"id": "2508.00522", "pdf": "https://arxiv.org/pdf/2508.00522", "abs": "https://arxiv.org/abs/2508.00522", "authors": ["Jiaxin Deng", "Qingcheng Zhu", "Junbiao Pang", "Linlin Yang", "Zhongqian Fu", "Baochang Zhang"], "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond", "categories": ["cs.CL"], "comment": null, "summary": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.", "AI": {"tldr": "The paper explores the link between expressive ability and generalization in LoRA, proposing Flat-LoRA and EFlat-LoRA to find flat minima, improving performance over LoRA and full fine-tuning.", "motivation": "Little research exists on the correlation between expressive ability and generalization in LoRA, and the lack of tools to explore sharpness for LoRA motivates this work.", "method": "Proposes Flat-LoRA and EFlat-LoRA, transferring perturbations from full parameter space to low-rank subspace to avoid interference.", "result": "EFlat-LoRA matches LoRA's efficiency while outperforming it (e.g., 1.0% on GLUE with RoBERTa-large) and full fine-tuning (0.5%). Vision-language models also show gains (1.5% on SQA).", "conclusion": "Generalization in LoRA is tied to sharpness, a factor overlooked by prior methods, as validated by empirical results."}}
{"id": "2508.00298", "pdf": "https://arxiv.org/pdf/2508.00298", "abs": "https://arxiv.org/abs/2508.00298", "authors": ["Jin Lyu", "Liang An", "Li Lin", "Pujin Cheng", "Yebin Liu", "Xiaoying Tang"], "title": "AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00837", "summary": "In the era of foundation models, achieving a unified understanding of\ndifferent dynamic objects through a single network has the potential to empower\nstronger spatial intelligence. Moreover, accurate estimation of animal pose and\nshape across diverse species is essential for quantitative analysis in\nbiological research. However, this topic remains underexplored due to the\nlimited network capacity of previous methods and the scarcity of comprehensive\nmulti-species datasets. To address these limitations, we introduce AniMer+, an\nextended version of our scalable AniMer framework. In this paper, we focus on a\nunified approach for reconstructing mammals (mammalia) and birds (aves). A key\ninnovation of AniMer+ is its high-capacity, family-aware Vision Transformer\n(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture\npartitions network layers into taxa-specific components (for mammalia and aves)\nand taxa-shared components, enabling efficient learning of both distinct and\ncommon anatomical features within a single model. To overcome the critical\nshortage of 3D training data, especially for birds, we introduce a\ndiffusion-based conditional image generation pipeline. This pipeline produces\ntwo large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for\nbirds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for\nbirds, which is crucial for resolving single-view depth ambiguities. Trained on\nan aggregated collection of 41.3k mammalian and 12.4k avian images (combining\nreal and synthetic data), our method demonstrates superior performance over\nexisting approaches across a wide range of benchmarks, including the\nchallenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the\neffectiveness of both our novel network architecture and the generated\nsynthetic datasets in enhancing real-world application performance.", "AI": {"tldr": "AniMer+ introduces a unified approach for reconstructing mammals and birds using a high-capacity, family-aware ViT with MoE design and synthetic datasets to address data scarcity.", "motivation": "To enable stronger spatial intelligence and accurate animal pose/shape estimation across species, addressing limited network capacity and data scarcity.", "method": "Uses a family-aware ViT with MoE, partitioning layers into taxa-specific and shared components, and generates synthetic datasets (CtrlAni3D, CtrlAVES3D) via diffusion-based image generation.", "result": "Superior performance on benchmarks, including out-of-domain Animal Kingdom, validated by ablation studies.", "conclusion": "AniMer+ effectively addresses data and capacity limitations, enhancing real-world performance for animal pose and shape estimation."}}
{"id": "2508.00537", "pdf": "https://arxiv.org/pdf/2508.00537", "abs": "https://arxiv.org/abs/2508.00537", "authors": ["Giulio Zhou", "Tsz Kin Lam", "Alexandra Birch", "Barry Haddow"], "title": "The Prosody of Emojis", "categories": ["cs.CL"], "comment": null, "summary": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.", "AI": {"tldr": "The study explores how emojis influence prosody in speech and how listeners interpret these cues, showing emojis can convey prosodic intent.", "motivation": "To understand the role of emojis as visual surrogates for prosodic features in text-based communication and their impact on spoken prosody.", "method": "Analysis of human speech data from structured production and perception tasks, linking prosody and emoji meanings empirically.", "result": "Speakers adjust prosody based on emojis; listeners often identify emojis from prosody alone; greater semantic differences between emojis lead to more prosodic divergence.", "conclusion": "Emojis serve as meaningful carriers of prosodic intent, highlighting their communicative role in digital contexts."}}
{"id": "2508.00299", "pdf": "https://arxiv.org/pdf/2508.00299", "abs": "https://arxiv.org/abs/2508.00299", "authors": ["Danzhen Fu", "Jiagao Hu", "Daiguo Zhou", "Fei Wang", "Zepeng Wang", "Wenhua Liao"], "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "ICCV 2025 Workshop (HiGen)", "summary": "Pedestrian detection models in autonomous driving systems often lack\nrobustness due to insufficient representation of dangerous pedestrian scenarios\nin training datasets. To address this limitation, we present a novel framework\nfor controllable pedestrian video editing in multi-view driving scenarios by\nintegrating video inpainting and human motion control techniques. Our approach\nbegins by identifying pedestrian regions of interest across multiple camera\nviews, expanding detection bounding boxes with a fixed ratio, and resizing and\nstitching these regions into a unified canvas while preserving cross-view\nspatial relationships. A binary mask is then applied to designate the editable\narea, within which pedestrian editing is guided by pose sequence control\nconditions. This enables flexible editing functionalities, including pedestrian\ninsertion, replacement, and removal. Extensive experiments demonstrate that our\nframework achieves high-quality pedestrian editing with strong visual realism,\nspatiotemporal coherence, and cross-view consistency. These results establish\nthe proposed method as a robust and versatile solution for multi-view\npedestrian video generation, with broad potential for applications in data\naugmentation and scenario simulation in autonomous driving.", "AI": {"tldr": "A novel framework for controllable pedestrian video editing in multi-view driving scenarios is introduced, addressing robustness issues in pedestrian detection models by enhancing training datasets.", "motivation": "Pedestrian detection models lack robustness due to insufficient representation of dangerous scenarios in training datasets.", "method": "The framework integrates video inpainting and human motion control, identifying pedestrian regions, expanding bounding boxes, and stitching them into a unified canvas. Editing is guided by pose sequence control within a binary mask.", "result": "The method achieves high-quality pedestrian editing with visual realism, spatiotemporal coherence, and cross-view consistency.", "conclusion": "The framework is a robust and versatile solution for multi-view pedestrian video generation, useful for data augmentation and scenario simulation in autonomous driving."}}
{"id": "2508.00544", "pdf": "https://arxiv.org/pdf/2508.00544", "abs": "https://arxiv.org/abs/2508.00544", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "AI": {"tldr": "The paper introduces PaPaformer, a decoder-only transformer variant, to reduce training time and parameters while improving performance by training lower-dimensional paths individually and combining them.", "motivation": "Modern large-language models require extensive computation and time, even for smaller variants. The paper aims to reduce training time from days/weeks to hours.", "method": "Introduces PaPaformer, a decoder-only transformer with parallel paths trained individually on diverse data and combined into a larger model.", "result": "Reduces total parameters and training time while increasing performance. Enables customization of paths for specific tasks.", "conclusion": "PaPaformer offers a scalable and efficient method for training transformer-based models with potential for task-specific customization."}}
{"id": "2508.00308", "pdf": "https://arxiv.org/pdf/2508.00308", "abs": "https://arxiv.org/abs/2508.00308", "authors": ["Chunyan She", "Fujun Han", "Chengyu Fang", "Shukai Duan", "Lidan Wang"], "title": "Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2025", "summary": "The event camera, benefiting from its high dynamic range and low latency,\nprovides performance gain for low-light image enhancement. Unlike frame-based\ncameras, it records intensity changes with extremely high temporal resolution,\ncapturing sufficient structure information. Currently, existing event-based\nmethods feed a frame and events directly into a single model without fully\nexploiting modality-specific advantages, which limits their performance.\nTherefore, by analyzing the role of each sensing modality, the enhancement\npipeline is decoupled into two stages: visibility restoration and structure\nrefinement. In the first stage, we design a visibility restoration network with\namplitude-phase entanglement by rethinking the relationship between amplitude\nand phase components in Fourier space. In the second stage, a fusion strategy\nwith dynamic alignment is proposed to mitigate the spatial mismatch caused by\nthe temporal resolution discrepancy between two sensing modalities, aiming to\nrefine the structure information of the image enhanced by the visibility\nrestoration network. In addition, we utilize spatial-frequency interpolation to\nsimulate negative samples with diverse illumination, noise and artifact\ndegradations, thereby developing a contrastive loss that encourages the model\nto learn discriminative representations. Experiments demonstrate that the\nproposed method outperforms state-of-the-art models.", "AI": {"tldr": "The paper proposes a two-stage method for low-light image enhancement using event cameras, focusing on visibility restoration and structure refinement, outperforming existing models.", "motivation": "Existing event-based methods do not fully exploit modality-specific advantages, limiting performance in low-light image enhancement.", "method": "The pipeline decouples into visibility restoration (using amplitude-phase entanglement in Fourier space) and structure refinement (with dynamic alignment fusion). Spatial-frequency interpolation and contrastive loss are also used.", "result": "The proposed method outperforms state-of-the-art models in experiments.", "conclusion": "Decoupling the enhancement pipeline and leveraging modality-specific advantages significantly improves low-light image enhancement."}}
{"id": "2508.00574", "pdf": "https://arxiv.org/pdf/2508.00574", "abs": "https://arxiv.org/abs/2508.00574", "authors": ["Jianwei Wang", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Ziqian Zeng"], "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.", "AI": {"tldr": "SynAdapt is a framework improving reasoning efficiency by using synthetic Continuous Chain-of-Thought (CCoT) for alignment and adaptive prompting for hard questions.", "motivation": "Existing CCoT methods suffer from inefficiencies like indirect fine-tuning and inconsistent targets, limiting their effectiveness.", "method": "SynAdapt generates synthetic CCoT for precise alignment and uses a difficulty classifier to adaptively prompt LLMs for hard questions.", "result": "Achieves the best accuracy-efficiency trade-off across benchmarks of varying difficulty.", "conclusion": "SynAdapt effectively addresses limitations of current CCoT methods, enhancing reasoning performance and efficiency."}}
{"id": "2508.00311", "pdf": "https://arxiv.org/pdf/2508.00311", "abs": "https://arxiv.org/abs/2508.00311", "authors": ["Yufeng Zhong", "Zhixiong Zeng", "Lei Chen", "Longrong Yang", "Liming Zheng", "Jing Huang", "Siqi Yang", "Lin Ma"], "title": "DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Optical Character Recognition (OCR) for mathematical formula is essential for\nthe intelligent analysis of scientific literature. However, both task-specific\nand general vision-language models often struggle to handle the structural\ndiversity, complexity, and real-world variability inherent in mathematical\ncontent. In this work, we present DocTron-Formula, a unified framework built\nupon general vision-language models, thereby eliminating the need for\nspecialized architectures. Furthermore, we introduce CSFormula, a large-scale\nand challenging dataset that encompasses multidisciplinary and structurally\ncomplex formulas at the line, paragraph, and page levels. Through\nstraightforward supervised fine-tuning, our approach achieves state-of-the-art\nperformance across a variety of styles, scientific domains, and complex\nlayouts. Experimental results demonstrate that our method not only surpasses\nspecialized models in terms of accuracy and robustness, but also establishes a\nnew paradigm for the automated understanding of complex scientific documents.", "AI": {"tldr": "DocTron-Formula is a unified OCR framework for mathematical formulas, leveraging general vision-language models and a new dataset (CSFormula) to achieve state-of-the-art performance without specialized architectures.", "motivation": "Current OCR models struggle with the structural diversity and complexity of mathematical formulas in scientific literature, necessitating a more robust solution.", "method": "The approach uses general vision-language models and introduces CSFormula, a large-scale dataset, followed by straightforward supervised fine-tuning.", "result": "The method achieves state-of-the-art performance in accuracy and robustness across diverse styles, domains, and layouts, outperforming specialized models.", "conclusion": "DocTron-Formula sets a new paradigm for automated understanding of complex scientific documents, eliminating the need for task-specific architectures."}}
{"id": "2508.00600", "pdf": "https://arxiv.org/pdf/2508.00600", "abs": "https://arxiv.org/abs/2508.00600", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "AI": {"tldr": "CRUX is a framework for confidence estimation in LLMs, integrating context faithfulness and consistency via novel metrics, outperforming baselines.", "motivation": "Current methods lack context relevance, crucial for output quality in scenarios with background knowledge.", "method": "CRUX uses contextual entropy reduction and unified consistency examination to estimate confidence.", "result": "Achieves highest AUROC on benchmark datasets (CoQA, SQuAD, QuAC) and domain-specific datasets (BioASQ, EduQG).", "conclusion": "CRUX effectively bridges the gap in context-aware confidence estimation for LLMs."}}
{"id": "2508.00312", "pdf": "https://arxiv.org/pdf/2508.00312", "abs": "https://arxiv.org/abs/2508.00312", "authors": ["Suhang Cai", "Xiaohao Peng", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian"], "title": "GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video anomaly detection (VAD) plays a critical role in public safety\napplications such as intelligent surveillance. However, the rarity,\nunpredictability, and high annotation cost of real-world anomalies make it\ndifficult to scale VAD datasets, which limits the performance and\ngeneralization ability of existing models. To address this challenge, we\npropose a generative video-enhanced weakly-supervised video anomaly detection\n(GV-VAD) framework that leverages text-conditioned video generation models to\nproduce semantically controllable and physically plausible synthetic videos.\nThese virtual videos are used to augment training data at low cost. In\naddition, a synthetic sample loss scaling strategy is utilized to control the\ninfluence of generated synthetic samples for efficient training. The\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods on UCF-Crime datasets. The code is available at\nhttps://github.com/Sumutan/GV-VAD.git.", "AI": {"tldr": "The paper proposes GV-VAD, a generative video-enhanced weakly-supervised framework for video anomaly detection, using synthetic videos to augment training data and improve performance.", "motivation": "The rarity, unpredictability, and high annotation cost of real-world anomalies limit the scalability and performance of existing VAD models.", "method": "Leverages text-conditioned video generation models to create synthetic videos for data augmentation and uses a synthetic sample loss scaling strategy for efficient training.", "result": "Outperforms state-of-the-art methods on UCF-Crime datasets.", "conclusion": "GV-VAD effectively addresses the challenges of limited training data and improves anomaly detection performance."}}
{"id": "2508.00605", "pdf": "https://arxiv.org/pdf/2508.00605", "abs": "https://arxiv.org/abs/2508.00605", "authors": ["Farhana Haque", "Md. Abdur Rahman", "Sumon Ahmed"], "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language", "categories": ["cs.CL"], "comment": null, "summary": "Topic modeling is a Natural Language Processing (NLP) technique that is used\nto identify latent themes and extract topics from text corpora by grouping\nsimilar documents based on their most significant keywords. Although widely\nresearched in English, topic modeling remains understudied in Bengali due to\nits morphological complexity, lack of adequate resources and initiatives. In\nthis contribution, a novel Graph Convolutional Network (GCN) based model called\nGHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input\nvectors of documents as nodes in the graph, which GCN uses to produce\nsemantically rich embeddings. The embeddings are then decomposed using\nNon-negative Matrix Factorization (NMF) to get the topical representations of\nthe underlying themes of the text corpus. This study compares the proposed\nmodel against a wide range of Bengali topic modeling techniques, from\ntraditional methods such as LDA, LSA, and NMF to contemporary frameworks such\nas BERTopic and Top2Vec on three Bengali datasets. The experimental results\ndemonstrate the effectiveness of the proposed model by outperforming other\nmodels in topic coherence and diversity. In addition, we introduce a novel\nBengali dataset called \"NCTBText\" sourced from Bengali textbook materials to\nenrich and diversify the predominantly newspaper-centric Bengali corpora.", "AI": {"tldr": "A novel Graph Convolutional Network (GCN)-based model, GHTM, is proposed for Bengali topic modeling, outperforming traditional and contemporary methods in coherence and diversity. A new Bengali dataset, NCTBText, is introduced.", "motivation": "Topic modeling is understudied in Bengali due to its complexity and lack of resources. The paper aims to address this gap.", "method": "GHTM uses GCN to create semantic embeddings from document nodes, decomposed via NMF for topic extraction. Compared against LDA, LSA, NMF, BERTopic, and Top2Vec on Bengali datasets.", "result": "GHTM outperforms other models in topic coherence and diversity. A new dataset, NCTBText, is introduced.", "conclusion": "GHTM is effective for Bengali topic modeling, and NCTBText enriches available corpora."}}
{"id": "2508.00319", "pdf": "https://arxiv.org/pdf/2508.00319", "abs": "https://arxiv.org/abs/2508.00319", "authors": ["Sunghyun Park", "Seokeon Choi", "Hyoungwoo Park", "Sungrack Yun"], "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICCV 2025", "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.", "AI": {"tldr": "Proposes personalization guidance for text-to-image diffusion models to balance target fidelity and text editability, outperforming existing methods.", "motivation": "Address the trade-off between aligning with target concepts and preserving original model knowledge in few-shot fine-tuning of diffusion models.", "method": "Uses an unlearned weak model conditioned on a null text prompt and dynamically controls unlearning via weight interpolation during inference.", "result": "Improves text alignment and target distribution fidelity without extra computational cost.", "conclusion": "The method effectively balances adaptation and preservation, integrating well with various fine-tuning strategies."}}
{"id": "2508.00614", "pdf": "https://arxiv.org/pdf/2508.00614", "abs": "https://arxiv.org/abs/2508.00614", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.", "AI": {"tldr": "The paper tests the impact of tipping and threatening AI models on performance, finding no significant effect on benchmarks but noting per-question variability.", "motivation": "To clarify whether common prompting tactics (tipping or threatening AI models) improve performance, as claimed by some leaders.", "method": "Empirical testing on GPQA and MMLU-Pro benchmarks to evaluate model performance under tipping and threatening prompts.", "result": "No significant effect on overall benchmark performance, but per-question performance varies unpredictably with prompt changes.", "conclusion": "Simple prompting variations like tipping or threatening are not broadly effective, though they may unpredictably impact individual questions."}}
{"id": "2508.00330", "pdf": "https://arxiv.org/pdf/2508.00330", "abs": "https://arxiv.org/abs/2508.00330", "authors": ["Lilika Makabe", "Hiroaki Santo", "Fumio Okura", "Michael S. Brown", "Yasuyuki Matsushita"], "title": "Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a practical and accurate calibration method for camera\nspectral sensitivity using a diffraction grating. Accurate calibration of\ncamera spectral sensitivity is crucial for various computer vision tasks,\nincluding color correction, illumination estimation, and material analysis.\nUnlike existing approaches that require specialized narrow-band filters or\nreference targets with known spectral reflectances, our method only requires an\nuncalibrated diffraction grating sheet, readily available off-the-shelf. By\ncapturing images of the direct illumination and its diffracted pattern through\nthe grating sheet, our method estimates both the camera spectral sensitivity\nand the diffraction grating parameters in a closed-form manner. Experiments on\nsynthetic and real-world data demonstrate that our method outperforms\nconventional reference target-based methods, underscoring its effectiveness and\npracticality.", "AI": {"tldr": "A novel calibration method for camera spectral sensitivity using an uncalibrated diffraction grating, outperforming traditional reference target-based approaches.", "motivation": "Accurate camera spectral sensitivity calibration is essential for tasks like color correction and material analysis, but existing methods require specialized equipment.", "method": "Uses a diffraction grating sheet to capture direct and diffracted illumination, estimating camera sensitivity and grating parameters in closed-form.", "result": "Outperforms conventional methods in synthetic and real-world experiments.", "conclusion": "The method is effective, practical, and accessible, requiring only off-the-shelf materials."}}
{"id": "2508.00619", "pdf": "https://arxiv.org/pdf/2508.00619", "abs": "https://arxiv.org/abs/2508.00619", "authors": ["Shantanu Thorat", "Andrew Caines"], "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "AI": {"tldr": "The paper introduces DACTYL, a dataset for detecting AI-generated texts in one-shot/few-shot and domain-specific scenarios, revealing vulnerabilities in existing detectors. It compares BCE and DXO training methods, showing DXO's superior generalization.", "motivation": "Existing AI-generated text detectors fail in real-world settings, especially for one-shot/few-shot and domain-specific texts, prompting the need for a robust dataset and improved training methods.", "method": "The authors create DACTYL, a dataset with one-shot/few-shot and CPT-generated texts. They train classifiers using BCE and DXO optimization, evaluating performance on DACTYL and OOD datasets.", "result": "Existing detectors struggle with DACTYL. DXO-trained classifiers outperform BCE-trained ones on OOD data, with a 50.56 macro-F1 score improvement in a mock deployment.", "conclusion": "DXO optimization enhances generalization for AI-generated text detection, highlighting weaknesses in current methods and the need for robust datasets like DACTYL."}}
{"id": "2508.00356", "pdf": "https://arxiv.org/pdf/2508.00356", "abs": "https://arxiv.org/abs/2508.00356", "authors": ["Angelos Vlachos", "Giorgos Filandrianos", "Maria Lymperaiou", "Nikolaos Spanos", "Ilias Mitsouras", "Vasileios Karampinis", "Athanasios Voulodimos"], "title": "Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning", "categories": ["cs.CV", "cs.MA", "I.2; I.2.7"], "comment": null, "summary": "We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.\nOur approach tackles the challenge of interleaved multimodal reasoning across\ndiverse datasets and task formats by employing a dual-agent system: a\nlanguage-based PromptEngineer, which generates context-aware, task-specific\nprompts, and a VisionReasoner, a large vision-language model (LVLM) responsible\nfor final inference. The framework is fully automated, modular, and\ntraining-free, enabling generalization across classification, question\nanswering, and free-form generation tasks involving one or multiple input\nimages. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE\nChallenge (Track A), covering a broad spectrum of visual reasoning tasks\nincluding document QA, visual comparison, dialogue-based understanding, and\nscene-level inference. Our results demonstrate that LVLMs can effectively\nreason over multiple images when guided by informative prompts. Notably, Claude\n3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%\naccuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how\ndesign choices-such as model selection, shot count, and input length-influence\nthe reasoning performance of different LVLMs.", "AI": {"tldr": "A dual-agent framework (PromptEngineer and VisionReasoner) enables automated, training-free multi-image reasoning across diverse tasks, achieving high performance on 18 datasets.", "motivation": "Addresses the challenge of interleaved multimodal reasoning across varied datasets and task formats.", "method": "Uses a language-based PromptEngineer for task-specific prompts and a VisionReasoner (LVLM) for inference, evaluated on 18 datasets.", "result": "High performance on tasks like TQA (99.13%), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L).", "conclusion": "LVLMs can effectively reason over multiple images with informative prompts, with performance influenced by design choices."}}
{"id": "2508.00669", "pdf": "https://arxiv.org/pdf/2508.00669", "abs": "https://arxiv.org/abs/2508.00669", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "AI": {"tldr": "A systematic review of LLMs in medical reasoning, proposing a taxonomy of enhancement techniques, analyzing applications, and identifying future challenges.", "motivation": "Address the gap in LLMs' ability to perform systematic, transparent, and verifiable reasoning in clinical practice.", "method": "Taxonomy of reasoning techniques (training-time and test-time), analysis across data modalities and clinical applications, and evaluation of benchmarks.", "result": "Identified key challenges like the faithfulness-plausibility gap and the need for native multimodal reasoning.", "conclusion": "Future directions focus on building efficient, robust, and sociotechnically responsible medical AI."}}
{"id": "2508.00358", "pdf": "https://arxiv.org/pdf/2508.00358", "abs": "https://arxiv.org/abs/2508.00358", "authors": ["Yan Gong", "Mengjun Chen", "Hao Liu", "Gao Yongsheng", "Lei Yang", "Naibang Wang", "Ziying Song", "Haoqun Ma"], "title": "Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering", "categories": ["cs.CV"], "comment": "9 pages, 7 figures, 5 tables", "summary": "Multi-object tracking (MOT) enables autonomous vehicles to continuously\nperceive dynamic objects, supplying essential temporal cues for prediction,\nbehavior understanding, and safe planning. However, conventional\ntracking-by-detection methods typically rely on static coordinate\ntransformations based on ego-vehicle poses, disregarding ego-vehicle\nspeed-induced variations in observation noise and reference frame changes,\nwhich degrades tracking stability and accuracy in dynamic, high-speed\nscenarios. In this paper, we investigate the critical role of ego-vehicle speed\nin MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that\ndynamically adapts uncertainty modeling to ego-vehicle speed, significantly\nimproving stability and accuracy in highly dynamic scenarios. Central to SG-LKF\nis MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that\nadaptively predicts key parameters of SG-LKF. To enhance inter-frame\nassociation and trajectory continuity, we introduce a self-supervised\ntrajectory consistency loss jointly optimized with semantic and positional\nconstraints. Extensive experiments show that SG-LKF ranks first among all\nvision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results\non KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on\nnuScenes 3D MOT.", "AI": {"tldr": "The paper proposes a Speed-Guided Learnable Kalman Filter (SG-LKF) for multi-object tracking (MOT) in autonomous vehicles, improving stability and accuracy by dynamically adapting to ego-vehicle speed.", "motivation": "Conventional MOT methods ignore ego-vehicle speed's impact on observation noise and reference frames, degrading tracking in dynamic, high-speed scenarios.", "method": "SG-LKF uses MotionScaleNet (MSNet) to adaptively predict parameters and introduces a self-supervised trajectory consistency loss for better inter-frame association.", "result": "SG-LKF achieves top performance on KITTI 2D MOT (79.59% HOTA), strong results on KITTI 3D MOT (82.03% HOTA), and outperforms SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.", "conclusion": "The proposed SG-LKF effectively addresses speed-induced tracking challenges, enhancing MOT performance in dynamic scenarios."}}
{"id": "2508.00673", "pdf": "https://arxiv.org/pdf/2508.00673", "abs": "https://arxiv.org/abs/2508.00673", "authors": ["Farhan Farsi", "Farnaz Aghababaloo", "Shahriar Shariati Motlagh", "Parsa Ghofrani", "MohammadAli SadraeiJavaheri", "Shayan Bali", "Amirhossein Shabani", "Farbod Bijary", "Ghazal Zamaninejad", "AmirMohammad Salehoof", "Saeedeh Momtazi"], "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language", "categories": ["cs.CL"], "comment": "Preprint. Under review", "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.", "AI": {"tldr": "The paper introduces 19 new Persian-language datasets to evaluate LLMs on Iranian cultural and linguistic contexts, benchmarking 41 models to address gaps in non-Western evaluation resources.", "motivation": "Address the lack of evaluation resources for non-English languages and non-Western cultural contexts in LLM assessments.", "method": "Developed 19 Persian datasets (e.g., Iranian law, idioms, grammar) and benchmarked 41 LLMs.", "result": "Provided benchmarks for LLMs in Persian language and Iranian culture, highlighting performance gaps.", "conclusion": "The study bridges cultural and linguistic evaluation gaps, emphasizing the need for diverse LLM assessments."}}
{"id": "2508.00359", "pdf": "https://arxiv.org/pdf/2508.00359", "abs": "https://arxiv.org/abs/2508.00359", "authors": ["Zongheng Tang", "Yi Liu", "Yifan Sun", "Yulu Gao", "Jinyu Chen", "Runsheng Xu", "Si Liu"], "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective", "categories": ["cs.CV"], "comment": "ICCV25 (Highlight)", "summary": "Collaborative perception shares information among different agents and helps\nsolving problems that individual agents may face, e.g., occlusions and small\nsensing range. Prior methods usually separate the multi-agent fusion and\nmulti-time fusion into two consecutive steps. In contrast, this paper proposes\nan efficient collaborative perception that aggregates the observations from\ndifferent agents (space) and different times into a unified spatio-temporal\nspace simultanesouly. The unified spatio-temporal space brings two benefits,\ni.e., efficient feature transmission and superior feature fusion. 1) Efficient\nfeature transmission: each static object yields a single observation in the\nspatial temporal space, and thus only requires transmission only once (whereas\nprior methods re-transmit all the object features multiple times). 2) superior\nfeature fusion: merging the multi-agent and multi-time fusion into a unified\nspatial-temporal aggregation enables a more holistic perspective, thereby\nenhancing perception performance in challenging scenarios. Consequently, our\nCollaborative perception with Spatio-temporal Transformer (CoST) gains\nimprovement in both efficiency and accuracy. Notably, CoST is not tied to any\nspecific method and is compatible with a majority of previous methods,\nenhancing their accuracy while reducing the transmission bandwidth.", "AI": {"tldr": "The paper proposes CoST, a method for collaborative perception that unifies multi-agent and multi-time fusion into a single spatio-temporal space, improving efficiency and accuracy.", "motivation": "Prior methods separate multi-agent and multi-time fusion, leading to inefficiencies like redundant feature transmission. The paper aims to unify these steps for better performance.", "method": "CoST aggregates observations from different agents and times into a unified spatio-temporal space, enabling efficient feature transmission and superior fusion.", "result": "CoST improves both efficiency (reducing redundant transmissions) and accuracy (enhancing perception in challenging scenarios).", "conclusion": "CoST is versatile, compatible with prior methods, and enhances accuracy while reducing bandwidth usage."}}
{"id": "2508.00675", "pdf": "https://arxiv.org/pdf/2508.00675", "abs": "https://arxiv.org/abs/2508.00675", "authors": ["Gleb Schmidt", "Johannes R\u00f6misch", "Mariia Halchynska", "Svetlana Gorovaia", "Ivan P. Yamshchikov"], "title": "Team \"better_call_claude\": Style Change Detection using a Sequential Sentence Pair Classifier", "categories": ["cs.CL"], "comment": null, "summary": "Style change detection - identifying the points in a document where writing\nstyle shifts - remains one of the most important and challenging problems in\ncomputational authorship analysis. At PAN 2025, the shared task challenges\nparticipants to detect style switches at the most fine-grained level:\nindividual sentences. The task spans three datasets, each designed with\ncontrolled and increasing thematic variety within documents. We propose to\naddress this problem by modeling the content of each problem instance - that\nis, a series of sentences - as a whole, using a Sequential Sentence Pair\nClassifier (SSPC). The architecture leverages a pre-trained language model\n(PLM) to obtain representations of individual sentences, which are then fed\ninto a bidirectional LSTM (BiLSTM) to contextualize them within the document.\nThe BiLSTM-produced vectors of adjacent sentences are concatenated and passed\nto a multi-layer perceptron for prediction per adjacency. Building on the work\nof previous PAN participants classical text segmentation, the approach is\nrelatively conservative and lightweight. Nevertheless, it proves effective in\nleveraging contextual information and addressing what is arguably the most\nchallenging aspect of this year's shared task: the notorious problem of\n\"stylistically shallow\", short sentences that are prevalent in the proposed\nbenchmark data. Evaluated on the official PAN-2025 test datasets, the model\nachieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,\nand HARD data, respectively, outperforming not only the official random\nbaselines but also a much more challenging one: claude-3.7-sonnet's zero-shot\nperformance.", "AI": {"tldr": "The paper proposes a Sequential Sentence Pair Classifier (SSPC) for style change detection at the sentence level, leveraging a pre-trained language model and BiLSTM for contextualization, achieving strong performance on PAN-2025 datasets.", "motivation": "Style change detection is a key challenge in authorship analysis, especially at the fine-grained sentence level. The PAN 2025 shared task aims to address this with controlled datasets.", "method": "Uses a pre-trained language model for sentence embeddings, contextualizes them with a BiLSTM, and predicts style changes via a multi-layer perceptron on adjacent sentence pairs.", "result": "Achieves macro-F1 scores of 0.923 (EASY), 0.828 (MEDIUM), and 0.724 (HARD), outperforming baselines and zero-shot performance of claude-3.7-sonnet.", "conclusion": "The SSPC model effectively leverages contextual information and addresses the challenge of 'stylistically shallow' sentences, proving robust on diverse datasets."}}
{"id": "2508.00361", "pdf": "https://arxiv.org/pdf/2508.00361", "abs": "https://arxiv.org/abs/2508.00361", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "Honey Classification using Hyperspectral Imaging and Machine Learning", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a machine learning-based method for automatically\nclassifying honey botanical origins. Dataset preparation, feature extraction,\nand classification are the three main steps of the proposed method. We use a\nclass transformation method in the dataset preparation phase to maximize the\nseparability across classes. The feature extraction phase employs the Linear\nDiscriminant Analysis (LDA) technique for extracting relevant features and\nreducing the number of dimensions. In the classification phase, we use Support\nVector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the\nextracted features of honey samples into their botanical origins. We evaluate\nour system using a standard honey hyperspectral imaging (HSI) dataset.\nExperimental findings demonstrate that the proposed system produces\nstate-of-the-art results on this dataset, achieving the highest classification\naccuracy of 95.13% for hyperspectral image-based classification and 92.80% for\nhyperspectral instance-based classification.", "AI": {"tldr": "A machine learning method for classifying honey botanical origins using dataset preparation, LDA for feature extraction, and SVM/KNN for classification, achieving high accuracy.", "motivation": "To automate and improve the classification of honey botanical origins using advanced machine learning techniques.", "method": "Dataset preparation with class transformation, feature extraction via LDA, and classification using SVM and KNN models.", "result": "Achieved 95.13% accuracy for image-based and 92.80% for instance-based classification on a standard HSI dataset.", "conclusion": "The proposed method is effective for honey botanical origin classification, outperforming existing approaches."}}
{"id": "2508.00679", "pdf": "https://arxiv.org/pdf/2508.00679", "abs": "https://arxiv.org/abs/2508.00679", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "AI": {"tldr": "TraceRetriever improves legal precedent retrieval by focusing on rhetorically significant segments, combining BM25, Vector Database, and Cross-Encoder models for scalable and reliable results.", "motivation": "The growing complexity and volume of legal documents challenge traditional retrieval methods, necessitating a system that works with limited case information.", "method": "The pipeline integrates BM25, Vector Database, and Cross-Encoder models, using Reciprocal Rank Fusion for combining results and a Hierarchical BiLSTM CRF classifier for rhetorical annotations.", "result": "Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses document volume challenges and aligns with practical search constraints.", "conclusion": "TraceRetriever provides a reliable and scalable foundation for precedent retrieval, enhancing legal research with partial case knowledge."}}
{"id": "2508.00366", "pdf": "https://arxiv.org/pdf/2508.00366", "abs": "https://arxiv.org/abs/2508.00366", "authors": ["Liang Han", "Xu Zhang", "Haichuan Song", "Kanle Shi", "Yu-Shen Liu", "Zhizhong Han"], "title": "SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Surface reconstruction from sparse views aims to reconstruct a 3D shape or\nscene from few RGB images. The latest methods are either generalization-based\nor overfitting-based. However, the generalization-based methods do not\ngeneralize well on views that were unseen during training, while the\nreconstruction quality of overfitting-based methods is still limited by the\nlimited geometry clues. To address this issue, we propose SparseRecon, a novel\nneural implicit reconstruction method for sparse views with volume\nrendering-based feature consistency and uncertainty-guided depth constraint.\nFirstly, we introduce a feature consistency loss across views to constrain the\nneural implicit field. This design alleviates the ambiguity caused by\ninsufficient consistency information of views and ensures completeness and\nsmoothness in the reconstruction results. Secondly, we employ an\nuncertainty-guided depth constraint to back up the feature consistency loss in\nareas with occlusion and insignificant features, which recovers geometry\ndetails for better reconstruction quality. Experimental results demonstrate\nthat our method outperforms the state-of-the-art methods, which can produce\nhigh-quality geometry with sparse-view input, especially in the scenarios with\nsmall overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.", "AI": {"tldr": "SparseRecon improves sparse-view 3D reconstruction by combining feature consistency and uncertainty-guided depth constraints, outperforming existing methods.", "motivation": "Existing methods (generalization-based or overfitting-based) struggle with unseen views or limited geometry clues, leading to poor reconstruction quality.", "method": "SparseRecon uses volume rendering-based feature consistency loss and uncertainty-guided depth constraints to enhance reconstruction.", "result": "Outperforms state-of-the-art methods, especially in scenarios with small overlapping views.", "conclusion": "SparseRecon effectively addresses sparse-view reconstruction challenges, producing high-quality geometry."}}
{"id": "2508.00680", "pdf": "https://arxiv.org/pdf/2508.00680", "abs": "https://arxiv.org/abs/2508.00680", "authors": ["Johannes R\u00f6misch", "Svetlana Gorovaia", "Mariia Halchynska", "Gleb Schmidt", "Ivan P. Yamshchikov"], "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?", "categories": ["cs.CL"], "comment": null, "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.", "AI": {"tldr": "The paper evaluates zero-shot performance of large language models (LLMs) on sentence-level style change detection, showing their sensitivity to stylistic variations and superior accuracy over PAN competition baselines.", "motivation": "To assess how well state-of-the-art LLMs perform on the challenging task of detecting writing style changes at the sentence level.", "method": "Benchmarked four LLMs on PAN 2024 and 2025 datasets for multi-author writing style analysis.", "result": "LLMs are highly sensitive to stylistic variations, outperform PAN baselines, and may rely more on content-independent stylistic signals.", "conclusion": "Modern LLMs excel in style change detection, setting a high benchmark and revealing unexpected sensitivity to stylistic cues."}}
{"id": "2508.00367", "pdf": "https://arxiv.org/pdf/2508.00367", "abs": "https://arxiv.org/abs/2508.00367", "authors": ["Joonmyung Choi", "Sanghyeok Lee", "Byungoh Ko", "Eunseo Kim", "Jihyung Kil", "Hyunwoo J. Kim"], "title": "Representation Shift: Unifying Token Compression with FlashAttention", "categories": ["cs.CV"], "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Transformers have demonstrated remarkable success across vision, language,\nand video. Yet, increasing task complexity has led to larger models and more\ntokens, raising the quadratic cost of self-attention and the overhead of GPU\nmemory access. To reduce the computation cost of self-attention, prior work has\nproposed token compression techniques that drop redundant or less informative\ntokens. Meanwhile, fused attention kernels such as FlashAttention have been\ndeveloped to alleviate memory overhead by avoiding attention map construction\nand its associated I/O to HBM. This, however, makes it incompatible with most\ntraining-free token compression methods, which rely on attention maps to\ndetermine token importance. Here, we propose Representation Shift, a\ntraining-free, model-agnostic metric that measures the degree of change in each\ntoken's representation. This seamlessly integrates token compression with\nFlashAttention, without attention maps or retraining. Our method further\ngeneralizes beyond Transformers to CNNs and state space models. Extensive\nexperiments show that Representation Shift enables effective token compression\ncompatible with FlashAttention, yielding significant speedups of up to 5.5% and\n4.4% in video-text retrieval and video QA, respectively. Code is available at\nhttps://github.com/mlvlab/Representation-Shift.", "AI": {"tldr": "The paper introduces Representation Shift, a training-free metric for token compression in Transformers, compatible with FlashAttention, achieving speedups in tasks like video-text retrieval and video QA.", "motivation": "Address the high computational cost and GPU memory overhead of self-attention in Transformers due to larger models and more tokens, while maintaining compatibility with efficient attention kernels like FlashAttention.", "method": "Proposes Representation Shift, a model-agnostic metric to measure token representation changes, enabling token compression without attention maps or retraining.", "result": "Achieves speedups of up to 5.5% in video-text retrieval and 4.4% in video QA, demonstrating effective token compression.", "conclusion": "Representation Shift offers a practical solution for reducing computation costs in Transformers and other models, enhancing efficiency without retraining."}}
{"id": "2508.00709", "pdf": "https://arxiv.org/pdf/2508.00709", "abs": "https://arxiv.org/abs/2508.00709", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "AI": {"tldr": "NyayaRAG, a Retrieval-Augmented Generation framework, enhances Legal Judgment Prediction in India by integrating case facts, statutes, and precedents, improving accuracy and explanation quality.", "motivation": "Previous LJP methods in India ignore statutory provisions and precedents, key in common law systems. NyayaRAG addresses this gap.", "method": "NyayaRAG combines factual case descriptions, legal statutes, and retrieved prior cases using a domain-specific pipeline for the Indian legal system.", "result": "Augmenting factual inputs with legal knowledge boosts predictive accuracy and explanation quality, as shown by lexical, semantic, and LLM-based evaluations.", "conclusion": "NyayaRAG demonstrates the importance of integrating structured legal knowledge for better LJP outcomes in common law systems like India."}}
{"id": "2508.00374", "pdf": "https://arxiv.org/pdf/2508.00374", "abs": "https://arxiv.org/abs/2508.00374", "authors": ["Yuji Sato", "Yasunori Ishii", "Takayoshi Yamashita"], "title": "Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models", "categories": ["cs.CV"], "comment": "Accepted to MVA2025 (Best Poster Award)", "summary": "Video-based long-term action anticipation is crucial for early risk detection\nin areas such as automated driving and robotics. Conventional approaches\nextract features from past actions using encoders and predict future events\nwith decoders, which limits performance due to their unidirectional nature.\nThese methods struggle to capture semantically distinct sub-actions within a\nscene. The proposed method, BiAnt, addresses this limitation by combining\nforward prediction with backward prediction using a large language model.\nExperimental results on Ego4D demonstrate that BiAnt improves performance in\nterms of edit distance compared to baseline methods.", "AI": {"tldr": "BiAnt improves long-term action anticipation by combining forward and backward predictions using a large language model, outperforming baseline methods.", "motivation": "Current methods for video-based long-term action anticipation are limited by unidirectional feature extraction, failing to capture distinct sub-actions.", "method": "BiAnt integrates forward and backward prediction with a large language model to enhance feature extraction.", "result": "Experiments on Ego4D show BiAnt outperforms baselines in edit distance metrics.", "conclusion": "BiAnt's bidirectional approach effectively addresses limitations of unidirectional methods, improving action anticipation."}}
{"id": "2508.00719", "pdf": "https://arxiv.org/pdf/2508.00719", "abs": "https://arxiv.org/abs/2508.00719", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siwei Liu"], "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "DAMR is a novel KGQA framework combining symbolic search with adaptive path evaluation, using MCTS and a lightweight Transformer scorer for efficient, context-aware reasoning.", "motivation": "Address limitations of static path extraction in retrieve-then-reason methods and high computational costs of dynamic path generation with LLMs.", "method": "Integrates MCTS guided by an LLM-based planner, a Transformer-based scorer for context-aware evaluation, and dynamic pseudo-path refinement for training.", "result": "Outperforms state-of-the-art methods on multiple KGQA benchmarks.", "conclusion": "DAMR offers an efficient, adaptable solution for KGQA by balancing symbolic search and dynamic evaluation."}}
{"id": "2508.00381", "pdf": "https://arxiv.org/pdf/2508.00381", "abs": "https://arxiv.org/abs/2508.00381", "authors": ["Kamal Basha S", "Athira Nambiar"], "title": "Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.", "AI": {"tldr": "The paper introduces Adapt-WeldNet, an adaptive framework for weld defect detection, and DDIA, an interpretability analysis framework, to improve performance and transparency in defect detection systems.", "motivation": "Traditional NDT methods and existing neural network approaches often fail to detect subtle defects and lack interpretability, raising safety concerns.", "method": "Adapt-WeldNet evaluates pre-trained architectures, transfer learning strategies, and adaptive optimizers. DDIA uses XAI techniques (Grad-CAM, LIME) and expert validation for interpretability.", "result": "The framework optimizes defect detection and provides actionable insights, enhancing reliability and trust in automated decisions.", "conclusion": "The work improves trust, safety, and reliability in welding defect detection, especially for offshore and marine environments."}}
{"id": "2508.00741", "pdf": "https://arxiv.org/pdf/2508.00741", "abs": "https://arxiv.org/abs/2508.00741", "authors": ["Sohaib Imran", "Rob Lamb", "Peter M. Atkinson"], "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.", "AI": {"tldr": "GPT 4o can infer chatbot names and behaviors from training data, showing situational awareness and implications for AI safety.", "motivation": "To investigate if LLMs can reason about information in their training data, specifically through out-of-context abduction.", "method": "Experiments with GPT 4o, training it on fictitious chatbot names and behaviors but not dialogues, then testing inference and behavior replication.", "result": "GPT 4o correctly inferred chatbot names and replicated behaviors when trained on descriptions.", "conclusion": "LLMs like GPT 4o demonstrate situational awareness, raising important considerations for AI safety."}}
{"id": "2508.00383", "pdf": "https://arxiv.org/pdf/2508.00383", "abs": "https://arxiv.org/abs/2508.00383", "authors": ["Won June Cho", "Hongjun Yoon", "Daeky Jeong", "Hyeongyeol Lim", "Yosep Chong"], "title": "$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "comment": "Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "summary": "Spatial transcriptomics reveals gene expression patterns within tissue\ncontext, enabling precision oncology applications such as treatment response\nprediction, but its high cost and technical complexity limit clinical adoption.\nPredicting spatial gene expression (biomarkers) from routine histopathology\nimages offers a practical alternative, yet current vision foundation models\n(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below\nclinical standards. Given that VFMs are already trained on millions of diverse\nwhole slide images, we hypothesize that architectural innovations beyond ViTs\nmay better capture the low-frequency, subtle morphological patterns correlating\nwith molecular phenotypes. By demonstrating that state space models initialized\nwith negative real eigenvalues exhibit strong low-frequency bias, we introduce\n$MV_{Hybrid}$, a hybrid backbone architecture combining state space models\n(SSMs) with ViT. We compare five other different backbone architectures for\npathology VFMs, all pretrained on identical colorectal cancer datasets using\nthe DINOv2 self-supervised learning method. We evaluate all pretrained models\nusing both random split and leave-one-study-out (LOSO) settings of the same\nbiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher\ncorrelation than the best-performing ViT and shows 43% smaller performance\ndegradation compared to random split in gene expression prediction,\ndemonstrating superior performance and robustness, respectively. Furthermore,\n$MV_{Hybrid}$ shows equal or better downstream performance in classification,\npatch retrieval, and survival prediction tasks compared to that of ViT, showing\nits promise as a next-generation pathology VFM backbone. Our code is publicly\navailable at: https://github.com/deepnoid-ai/MVHybrid.", "AI": {"tldr": "The paper introduces $MV_{Hybrid}$, a hybrid architecture combining state space models (SSMs) with Vision Transformers (ViT) to predict spatial gene expression from histopathology images, outperforming ViT in robustness and performance.", "motivation": "Spatial transcriptomics is costly and complex, limiting clinical use. Predicting gene expression from routine histopathology images offers a practical alternative, but current ViT-based models fall short.", "method": "The authors propose $MV_{Hybrid}$, a hybrid backbone of SSMs and ViT, pretrained on colorectal cancer datasets using DINOv2. They compare it against five other architectures in random split and leave-one-study-out (LOSO) evaluations.", "result": "$MV_{Hybrid}$ achieves 57% higher correlation than ViT in LOSO and shows 43% smaller performance degradation. It also matches or exceeds ViT in classification, retrieval, and survival tasks.", "conclusion": "$MV_{Hybrid}$ is a promising next-generation backbone for pathology vision foundation models, offering superior performance and robustness for clinical applications."}}
{"id": "2508.00742", "pdf": "https://arxiv.org/pdf/2508.00742", "abs": "https://arxiv.org/abs/2508.00742", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages, 14 figures", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "AI": {"tldr": "GPT-4 agents partially align with HEXACO personality traits, showing reliability but model-specific biases.", "motivation": "To validate persona-based generative agents as substitutes for human participants in social science research.", "method": "Recreated HEXACO personality inventory with 310 GPT-4 agents, analyzed responses, and compared to original human data.", "result": "Agents showed coherent personality structure, reliability within GPT-4, but cross-model variability revealed biases.", "conclusion": "Generative agents are promising but require careful design to represent human traits accurately."}}
{"id": "2508.00391", "pdf": "https://arxiv.org/pdf/2508.00391", "abs": "https://arxiv.org/abs/2508.00391", "authors": ["Guanjie Huang", "Danny H. K. Tsang", "Shan Yang", "Guangzhi Lei", "Li Liu"], "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition", "categories": ["cs.CV", "eess.AS"], "comment": "9 pages", "summary": "Cued Speech (CS) is a visual communication system that combines lip-reading\nwith hand coding to facilitate communication for individuals with hearing\nimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures\nand lip movements into text via AI-driven methods. Traditionally, the temporal\nasynchrony between hand and lip movements requires the design of complex\nmodules to facilitate effective multimodal fusion. However, constrained by\nlimited data availability, current methods demonstrate insufficient capacity\nfor adequately training these fusion mechanisms, resulting in suboptimal\nperformance. Recently, multi-agent systems have shown promising capabilities in\nhandling complex tasks with limited data availability. To this end, we propose\nthe first collaborative multi-agent system for ACSR, named Cued-Agent. It\nintegrates four specialized sub-agents: a Multimodal Large Language Model-based\nHand Recognition agent that employs keyframe screening and CS expert prompt\nstrategies to decode hand movements, a pretrained Transformer-based Lip\nRecognition agent that extracts lip features from the input video, a Hand\nPrompt Decoding agent that dynamically integrates hand prompts with lip\nfeatures during inference in a training-free manner, and a Self-Correction\nPhoneme-to-Word agent that enables post-process and end-to-end conversion from\nphoneme sequences to natural language sentences for the first time through\nsemantic refinement. To support this study, we expand the existing Mandarin CS\ndataset by collecting data from eight hearing-impaired cuers, establishing a\nmixed dataset of fourteen subjects. Extensive experiments demonstrate that our\nCued-Agent performs superbly in both normal and hearing-impaired scenarios\ncompared with state-of-the-art methods. The implementation is available at\nhttps://github.com/DennisHgj/Cued-Agent.", "AI": {"tldr": "The paper introduces Cued-Agent, a collaborative multi-agent system for Automatic Cued Speech Recognition (ACSR), addressing challenges in multimodal fusion and data scarcity. It outperforms existing methods in both normal and hearing-impaired scenarios.", "motivation": "The temporal asynchrony between hand and lip movements in Cued Speech (CS) complicates multimodal fusion, and limited data availability hinders effective training of fusion mechanisms. Multi-agent systems offer a promising solution.", "method": "Cued-Agent integrates four sub-agents: a Hand Recognition agent, a Lip Recognition agent, a Hand Prompt Decoding agent, and a Self-Correction Phoneme-to-Word agent. It uses keyframe screening, expert prompts, and semantic refinement.", "result": "Cued-Agent achieves superior performance in ACSR compared to state-of-the-art methods, validated on an expanded Mandarin CS dataset.", "conclusion": "The proposed multi-agent system effectively addresses ACSR challenges, demonstrating robustness and scalability, with potential for broader applications in multimodal AI."}}
{"id": "2508.00743", "pdf": "https://arxiv.org/pdf/2508.00743", "abs": "https://arxiv.org/abs/2508.00743", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald K\u00f6stler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "title": "Agentic large language models improve retrieval-based radiology question answering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "AI": {"tldr": "An agentic RAG framework improves radiology QA by enabling LLMs to decompose questions, retrieve evidence iteratively, and synthesize responses, boosting accuracy and reducing hallucinations.", "motivation": "Traditional RAG systems for radiology QA rely on single-step retrieval, limiting complex clinical reasoning. This paper aims to enhance diagnostic accuracy and factual grounding using an agentic approach.", "method": "Proposed an agentic RAG framework for LLMs to autonomously decompose questions, iteratively retrieve evidence from Radiopaedia, and synthesize responses. Evaluated 24 LLMs on expert-curated radiology questions.", "result": "Agentic retrieval improved mean diagnostic accuracy (73% vs. 64% for zero-shot, 73% vs. 68% for conventional RAG), reduced hallucinations (9.4%), and increased clinically relevant context retrieval (46%). Mid-sized models showed the greatest gains.", "conclusion": "Agentic frameworks enhance factuality and diagnostic accuracy in radiology QA, especially for mid-sized LLMs, suggesting complementary roles of retrieval and fine-tuning. Future studies should validate clinical utility."}}
{"id": "2508.00395", "pdf": "https://arxiv.org/pdf/2508.00395", "abs": "https://arxiv.org/abs/2508.00395", "authors": ["Fei Zhang", "Tianfei Zhou", "Jiangchao Yao", "Ya Zhang", "Ivor W. Tsang", "Yanfeng Wang"], "title": "Decouple before Align: Visual Disentanglement Enhances Prompt Tuning", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, Accepted at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "summary": "Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,\nhas showcased remarkable effectiveness in improving the task-specific\ntransferability of vision-language models. This paper delves into a previously\noverlooked information asymmetry issue in PT, where the visual modality mostly\nconveys more context than the object-oriented textual modality.\nCorrespondingly, coarsely aligning these two modalities could result in the\nbiased attention, driving the model to merely focus on the context area. To\naddress this, we propose DAPT, an effective PT framework based on an intuitive\ndecouple-before-align concept. First, we propose to explicitly decouple the\nvisual modality into the foreground and background representation via\nexploiting coarse-and-fine visual segmenting cues, and then both of these\ndecoupled patterns are aligned with the original foreground texts and the\nhand-crafted background classes, thereby symmetrically strengthening the modal\nalignment. To further enhance the visual concentration, we propose a visual\npull-push regularization tailored for the foreground-background patterns,\ndirecting the original visual representation towards unbiased attention on the\nregion-of-interest object. We demonstrate the power of architecture-free DAPT\nthrough few-shot learning, base-to-novel generalization, and data-efficient\nlearning, all of which yield superior performance across prevailing benchmarks.\nOur code will be released at https://github.com/Ferenas/DAPT.", "AI": {"tldr": "The paper introduces DAPT, a prompt tuning framework addressing visual-textual information asymmetry by decoupling and aligning visual foreground/background with text, improving model focus and performance.", "motivation": "Address the overlooked issue of visual-textual information asymmetry in prompt tuning, which causes biased attention in vision-language models.", "method": "Proposes DAPT: decouples visual modality into foreground/background, aligns them with text, and uses visual pull-push regularization for unbiased attention.", "result": "DAPT shows superior performance in few-shot learning, generalization, and data-efficient learning across benchmarks.", "conclusion": "DAPT effectively resolves asymmetry, enhancing modal alignment and model performance, with code publicly available."}}
{"id": "2508.00757", "pdf": "https://arxiv.org/pdf/2508.00757", "abs": "https://arxiv.org/abs/2508.00757", "authors": ["Robin Armingaud", "Romaric Besan\u00e7on"], "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction", "categories": ["cs.CL"], "comment": "Submitted to ARR July", "summary": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.", "AI": {"tldr": "GLiDRE, a new model for document-level relation extraction, outperforms state-of-the-art models in few-shot settings, inspired by GLiNER's compact NER success.", "motivation": "Address the underexplored performance of current ATLOP-based approaches in zero-shot/few-shot settings for document-level RE, inspired by GLiNER's efficiency.", "method": "Introduces GLiDRE, building on GLiNER's key ideas, and benchmarks it against state-of-the-art models on Re-DocRED.", "result": "GLiDRE achieves state-of-the-art performance in few-shot scenarios.", "conclusion": "GLiDRE is effective for few-shot document-level RE, with publicly available code."}}
{"id": "2508.00397", "pdf": "https://arxiv.org/pdf/2508.00397", "abs": "https://arxiv.org/abs/2508.00397", "authors": ["Xi Xue", "Kunio Suzuki", "Nabarun Goswami", "Takuya Shintate"], "title": "Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of diffusion-based video generation models has led to\nincreasingly realistic synthetic content, presenting new challenges for video\nforgery detection. Existing methods often struggle to capture fine-grained\ntemporal inconsistencies, particularly in AI-generated videos with high visual\nfidelity and coherent motion. In this work, we propose a detection framework\nthat leverages spatial-temporal consistency by combining RGB appearance\nfeatures with optical flow residuals. The model adopts a dual-branch\narchitecture, where one branch analyzes RGB frames to detect appearance-level\nartifacts, while the other processes flow residuals to reveal subtle motion\nanomalies caused by imperfect temporal synthesis. By integrating these\ncomplementary features, the proposed method effectively detects a wide range of\nforged videos. Extensive experiments on text-to-video and image-to-video tasks\nacross ten diverse generative models demonstrate the robustness and strong\ngeneralization ability of the proposed approach.", "AI": {"tldr": "A dual-branch framework detects AI-generated videos by combining RGB appearance features and optical flow residuals, addressing temporal inconsistencies in synthetic content.", "motivation": "Existing methods struggle with fine-grained temporal inconsistencies in high-fidelity AI-generated videos, necessitating a robust detection approach.", "method": "The proposed framework uses a dual-branch architecture: one branch analyzes RGB frames for appearance artifacts, and the other processes optical flow residuals for motion anomalies.", "result": "The method effectively detects forged videos, showing robustness and generalization across ten generative models in text-to-video and image-to-video tasks.", "conclusion": "The dual-branch framework successfully addresses the challenge of detecting AI-generated videos by leveraging spatial-temporal consistency."}}
{"id": "2508.00760", "pdf": "https://arxiv.org/pdf/2508.00760", "abs": "https://arxiv.org/abs/2508.00760", "authors": ["Qiyao Xue", "Yuchen Dou", "Ryan Shi", "Xiang Lorraine Li", "Wei Gao"], "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.", "AI": {"tldr": "MMBERT, a BERT-based multimodal framework, outperforms existing models in Chinese hate speech detection by integrating text, speech, and visual data via a Mixture-of-Experts architecture and progressive training.", "motivation": "Hate speech detection in Chinese is challenging due to cloaking techniques and limited multimodal research. Existing work focuses on English, leaving a gap for Chinese-specific solutions.", "method": "Proposes MMBERT, combining text, speech, and visuals using a Mixture-of-Experts (MoE) architecture with modality-specific experts, shared self-attention, and a router-based expert allocation. Uses a three-stage training paradigm for stability.", "result": "MMBERT outperforms fine-tuned BERT models, LLMs, and in-context learning approaches on Chinese hate speech datasets.", "conclusion": "MMBERT is a robust solution for Chinese hate speech detection, leveraging multimodal data and MoE to address evasion techniques effectively."}}
{"id": "2508.00399", "pdf": "https://arxiv.org/pdf/2508.00399", "abs": "https://arxiv.org/abs/2508.00399", "authors": ["Raiyaan Abdullah", "Yogesh Singh Rawat", "Shruti Vyas"], "title": "iSafetyBench: A video-language benchmark for safety in industrial environment", "categories": ["cs.CV"], "comment": "Accepted to VISION'25 - ICCV 2025 workshop", "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\ngeneralization across diverse video understanding tasks under zero-shot\nsettings. However, their capabilities in high-stakes industrial domains-where\nrecognizing both routine operations and safety-critical anomalies is\nessential-remain largely underexplored. To address this gap, we introduce\niSafetyBench, a new video-language benchmark specifically designed to evaluate\nmodel performance in industrial environments across both normal and hazardous\nscenarios. iSafetyBench comprises 1,100 video clips sourced from real-world\nindustrial settings, annotated with open-vocabulary, multi-label action tags\nspanning 98 routine and 67 hazardous action categories. Each clip is paired\nwith multiple-choice questions for both single-label and multi-label\nevaluation, enabling fine-grained assessment of VLMs in both standard and\nsafety-critical contexts. We evaluate eight state-of-the-art video-language\nmodels under zero-shot conditions. Despite their strong performance on existing\nvideo benchmarks, these models struggle with iSafetyBench-particularly in\nrecognizing hazardous activities and in multi-label scenarios. Our results\nreveal significant performance gaps, underscoring the need for more robust,\nsafety-aware multimodal models for industrial applications. iSafetyBench\nprovides a first-of-its-kind testbed to drive progress in this direction. The\ndataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.", "AI": {"tldr": "iSafetyBench is a new benchmark for evaluating vision-language models in industrial settings, focusing on routine and hazardous actions, revealing gaps in current models.", "motivation": "To address the underexplored capabilities of VLMs in high-stakes industrial domains, where recognizing routine and hazardous actions is crucial.", "method": "Introduces iSafetyBench with 1,100 real-world industrial video clips, annotated with 98 routine and 67 hazardous action categories, paired with multiple-choice questions for evaluation.", "result": "Eight state-of-the-art VLMs struggle with iSafetyBench, especially in hazardous activity recognition and multi-label scenarios.", "conclusion": "Highlights the need for more robust, safety-aware multimodal models in industrial applications, with iSafetyBench serving as a testbed for progress."}}
{"id": "2508.00762", "pdf": "https://arxiv.org/pdf/2508.00762", "abs": "https://arxiv.org/abs/2508.00762", "authors": ["Atakan Site", "Emre Hakan Erdemir", "G\u00fcl\u015fen Eryi\u011fit"], "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.", "AI": {"tldr": "The paper introduces a zero-shot system for SemEval-2025 Task 8, using LLM-based Python code generation for tabular QA, achieving competitive rankings.", "motivation": "To address the challenge of question answering over tabular data in diverse domains, leveraging LLMs for efficient code generation.", "method": "A Python code generation framework using open-source LLMs with optimized prompting to generate executable Pandas code.", "result": "Different LLMs vary in effectiveness; Python code generation outperforms other methods. The system ranked 8th in Subtask I and 6th in Subtask II among 30 systems.", "conclusion": "LLM-based Python code generation is effective for tabular QA, with potential for further optimization."}}
{"id": "2508.00400", "pdf": "https://arxiv.org/pdf/2508.00400", "abs": "https://arxiv.org/abs/2508.00400", "authors": ["Janika Deborah Gajo", "Gerarld Paul Merales", "Jerome Escarcha", "Brenden Ashley Molina", "Gian Nartea", "Emmanuel G. Maminta", "Juan Carlos Roldan", "Rowel O. Atienza"], "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents", "categories": ["cs.CV"], "comment": "14 pages, accepted in ICCV 2025 Workshop on RetailVision", "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store\nsimulation for benchmarking embodied agents against human performance in\nshopping tasks. Addressing a gap in retail-specific sim environments for\nembodied agent training, Sari Sandbox features over 250 interactive grocery\nitems across three store configurations, controlled via an API. It supports\nboth virtual reality (VR) for human interaction and a vision language model\n(VLM)-powered embodied agent. We also introduce SariBench, a dataset of\nannotated human demonstrations across varied task difficulties. Our sandbox\nenables embodied agents to navigate, inspect, and manipulate retail items,\nproviding baselines against human performance. We conclude with benchmarks,\nperformance analysis, and recommendations for enhancing realism and\nscalability. The source code can be accessed via\nhttps://github.com/upeee/sari-sandbox-env.", "AI": {"tldr": "Sari Sandbox is a photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks, featuring interactive items and VR support.", "motivation": "To address the lack of retail-specific simulation environments for training embodied agents.", "method": "Developed a high-fidelity 3D simulation with 250+ interactive grocery items, VR for human interaction, and a VLM-powered agent. Introduced SariBench, a dataset of annotated human demonstrations.", "result": "Enabled benchmarking of embodied agents against human performance in navigation, inspection, and manipulation tasks.", "conclusion": "The sandbox provides baselines and recommendations for improving realism and scalability, with open-source code available."}}
{"id": "2508.00788", "pdf": "https://arxiv.org/pdf/2508.00788", "abs": "https://arxiv.org/abs/2508.00788", "authors": ["Xushuo Tang", "Yi Ding", "Zhengyi Yang", "Yin Chen", "Yongrui Gu", "Wenke Yang", "Mingchen Ju", "Xin Cao", "Yongfei Liu", "Wenjie Zhang"], "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.", "AI": {"tldr": "The paper introduces MISGENDERED+, an updated benchmark for evaluating LLMs' handling of inclusive pronouns, testing five models and showing improvements in binary and gender-neutral pronouns but inconsistencies in neopronouns.", "motivation": "To address fairness and inclusivity in LLMs, particularly in pronoun usage, as prior benchmarks like MISGENDERED were outdated and limited.", "method": "Extended the MISGENDERED benchmark to MISGENDERED+ and evaluated five LLMs (GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, Qwen2.5) across zero-shot, few-shot, and gender identity inference tasks.", "result": "Notable improvements in binary and gender-neutral pronoun accuracy, but inconsistent performance on neopronouns and reverse inference tasks.", "conclusion": "Persistent gaps in identity-sensitive reasoning highlight the need for further research in inclusive AI."}}
{"id": "2508.00406", "pdf": "https://arxiv.org/pdf/2508.00406", "abs": "https://arxiv.org/abs/2508.00406", "authors": ["Tao Wu", "Jingyuan Ye", "Ying Fu"], "title": "PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos", "categories": ["cs.CV"], "comment": null, "summary": "Geometric distortions and blurring caused by atmospheric turbulence degrade\nthe quality of long-range dynamic scene videos. Existing methods struggle with\nrestoring edge details and eliminating mixed distortions, especially under\nconditions of strong turbulence and complex dynamics. To address these\nchallenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines\nturbulence intensity, optical flow, and proportions of dynamic regions to\naccurately quantify video dynamic intensity under varying turbulence conditions\nand provide a high-dynamic turbulence training dataset. Additionally, we\npropose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework\nthat consists of three stages: \\textbf{de-tilting} for geometric stabilization,\n\\textbf{motion segmentation enhancement} for dynamic region refinement, and\n\\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight\nbackbones and stage-wise joint training to ensure both efficiency and high\nrestoration quality. Experimental results demonstrate that the proposed method\neffectively suppresses motion trailing artifacts, restores edge details and\nexhibits strong generalization capability, especially in real-world scenarios\ncharacterized by high-turbulence and complex dynamics. We will make the code\nand datasets openly available.", "AI": {"tldr": "The paper introduces a Dynamic Efficiency Index (DEI) and a Physical Model-Driven Multi-Stage Video Restoration (PMR) framework to address turbulence-induced distortions in videos, achieving high restoration quality and efficiency.", "motivation": "Atmospheric turbulence causes geometric distortions and blurring in long-range dynamic scene videos, and existing methods fail to restore edge details and eliminate mixed distortions under strong turbulence and complex dynamics.", "method": "The proposed PMR framework includes three stages: de-tilting for stabilization, motion segmentation enhancement for dynamic region refinement, and de-blurring for quality restoration, using lightweight backbones and joint training.", "result": "The method effectively suppresses motion trailing artifacts, restores edge details, and generalizes well in high-turbulence, complex dynamic scenarios.", "conclusion": "The DEI and PMR framework provide an efficient and high-quality solution for turbulence-distorted video restoration, with code and datasets to be made publicly available."}}
{"id": "2508.00819", "pdf": "https://arxiv.org/pdf/2508.00819", "abs": "https://arxiv.org/abs/2508.00819", "authors": ["Jinsong Li", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Dahua Lin"], "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models", "categories": ["cs.CL"], "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL", "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.", "AI": {"tldr": "DAEDAL introduces a dynamic adaptive length expansion strategy for Diffusion Large Language Models (DLLMs), eliminating the need for static predefined generation lengths and improving efficiency and performance.", "motivation": "The static length constraint in DLLMs leads to inefficiency and performance issues, either underperforming on complex tasks or wasting resources with excessive lengths.", "method": "DAEDAL uses latent signals in DLLMs to dynamically adjust generation length in two phases: coarse length expansion before denoising and targeted expansion during denoising.", "result": "DAEDAL matches or outperforms fixed-length baselines while improving computational efficiency by optimizing token usage.", "conclusion": "DAEDAL resolves DLLMs' static length limitation, enhancing their practicality and bridging the gap with Autoregressive models."}}
{"id": "2508.00412", "pdf": "https://arxiv.org/pdf/2508.00412", "abs": "https://arxiv.org/abs/2508.00412", "authors": ["Hanqi Chen", "Xu Zhang", "Xiaoliu Guan", "Lielin Jiang", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.", "AI": {"tldr": "Sortblock is a training-free framework that accelerates DiTs by dynamically caching block-wise features and skipping redundant computations, achieving 2x speedup with minimal quality loss.", "motivation": "DiTs suffer from high inference latency due to sequential denoising, limiting real-time use. Existing methods overlook evolving semantic focus.", "method": "Sortblock dynamically caches features based on similarity across timesteps, ranks residuals, and uses a linear predictor to reduce errors.", "result": "Achieves over 2x speedup with minimal quality degradation across tasks and DiT architectures.", "conclusion": "Sortblock provides an effective, generalizable solution for accelerating diffusion-based models without retraining."}}
{"id": "2508.00028", "pdf": "https://arxiv.org/pdf/2508.00028", "abs": "https://arxiv.org/abs/2508.00028", "authors": ["Abir Ray"], "title": "Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "comment": "12 pages", "summary": "Spectrum resources are often underutilized across time and space, motivating\ndynamic spectrum access strategies that allow secondary users to exploit unused\nfrequencies. A key challenge is predicting when and where spectrum will be\navailable (i.e., unused by primary licensed users) in order to enable proactive\nand interference-free access. This paper proposes a scalable framework for\nspectrum availability prediction that combines a two-state Markov chain model\nof primary user activity with high-fidelity propagation models from the ITU-R\n(specifically Recommendations P.528 and P.2108). The Markov chain captures\ntemporal occupancy patterns, while the propagation models incorporate path loss\nand clutter effects to determine if primary signals exceed interference\nthresholds at secondary user locations. By integrating these components, the\nproposed method can predict spectrum opportunities both in time and space with\nimproved accuracy. We develop the system model and algorithm for the approach,\nanalyze its scalability and computational efficiency, and discuss assumptions,\nlimitations, and potential applications. The framework is flexible and can be\nadapted to various frequency bands and scenarios. The results and analysis show\nthat the proposed approach can effectively identify available spectrum with low\ncomputational cost, making it suitable for real-time spectrum management in\ncognitive radio networks and other dynamic spectrum sharing systems.", "AI": {"tldr": "A scalable framework for predicting spectrum availability using Markov chains and ITU-R propagation models, improving accuracy for dynamic spectrum access.", "motivation": "Spectrum resources are often underutilized, and predicting availability is key for interference-free secondary use.", "method": "Combines a two-state Markov chain for temporal patterns with ITU-R propagation models (P.528, P.2108) for spatial analysis.", "result": "Effectively identifies available spectrum with low computational cost, suitable for real-time management.", "conclusion": "The framework is flexible, accurate, and scalable for dynamic spectrum sharing systems."}}
{"id": "2508.00413", "pdf": "https://arxiv.org/pdf/2508.00413", "abs": "https://arxiv.org/abs/2508.00413", "authors": ["Junyu Chen", "Dongyun Zou", "Wenkun He", "Junsong Chen", "Enze Xie", "Song Han", "Han Cai"], "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "We present DC-AE 1.5, a new family of deep compression autoencoders for\nhigh-resolution diffusion models. Increasing the autoencoder's latent channel\nnumber is a highly effective approach for improving its reconstruction quality.\nHowever, it results in slow convergence for diffusion models, leading to poorer\ngeneration quality despite better reconstruction quality. This issue limits the\nquality upper bound of latent diffusion models and hinders the employment of\nautoencoders with higher spatial compression ratios. We introduce two key\ninnovations to address this challenge: i) Structured Latent Space, a\ntraining-based approach to impose a desired channel-wise structure on the\nlatent space with front latent channels capturing object structures and latter\nlatent channels capturing image details; ii) Augmented Diffusion Training, an\naugmented diffusion training strategy with additional diffusion training\nobjectives on object latent channels to accelerate convergence. With these\ntechniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling\nresults than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better\nimage generation quality than DC-AE-f32c32 while being 4x faster. Code:\nhttps://github.com/dc-ai-projects/DC-Gen.", "AI": {"tldr": "DC-AE 1.5 introduces structured latent space and augmented diffusion training to improve convergence and generation quality in high-resolution diffusion models.", "motivation": "The challenge of slow convergence in diffusion models when increasing autoencoder latent channels limits quality and compression ratios.", "method": "Two innovations: Structured Latent Space for channel-wise organization and Augmented Diffusion Training with additional objectives.", "result": "DC-AE 1.5 achieves faster convergence and better generation quality, outperforming DC-AE on ImageNet 512x512.", "conclusion": "DC-AE 1.5 effectively addresses convergence and quality issues, enabling higher compression ratios and better performance."}}
{"id": "2508.00033", "pdf": "https://arxiv.org/pdf/2508.00033", "abs": "https://arxiv.org/abs/2508.00033", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "Jo\u00e3o P. Matos-Carvalho"], "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "comment": null, "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities.", "AI": {"tldr": "The study benchmarks LLMs for generating functional Python code using unfamiliar APIs, finding GPT-4.1 as the only consistently successful model, while highlighting limitations in LLMs and library documentation.", "motivation": "To assess LLMs' ability to generate functional Python code for complex tasks using unfamiliar APIs, addressing gaps in understanding their practical utility in scientific automation.", "method": "Systematically benchmarks LLMs using zero-shot prompts for two tasks: conversational data analysis with ParShift and synthetic data generation/clustering with pyclugen and scikit-learn. Evaluates correctness and errors.", "result": "Only a small subset of models, notably GPT-4.1, consistently produced correct, executable code. Errors revealed issues in library documentation and implementation.", "conclusion": "LLMs have limitations for end-to-end scientific automation, requiring better prompt design, library documentation, and model advancements."}}
{"id": "2508.00418", "pdf": "https://arxiv.org/pdf/2508.00418", "abs": "https://arxiv.org/abs/2508.00418", "authors": ["Sangwoo Youn", "Minji Lee", "Nokap Tony Park", "Yeonggyoo Jeon", "Taeyoung Na"], "title": "IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator", "categories": ["cs.CV", "eess.IV"], "comment": "ICIP 2025. Code: https://github.com/sang-w00/IN2OUT", "summary": "Video outpainting presents a unique challenge of extending the borders while\nmaintaining consistency with the given content. In this paper, we suggest the\nuse of video inpainting models that excel in object flow learning and\nreconstruction in outpainting rather than solely generating the background as\nin existing methods. However, directly applying or fine-tuning inpainting\nmodels to outpainting has shown to be ineffective, often leading to blurry\nresults. Our extensive experiments on discriminator designs reveal that a\ncritical component missing in the outpainting fine-tuning process is a\ndiscriminator capable of effectively assessing the perceptual quality of the\nextended areas. To tackle this limitation, we differentiate the objectives of\nadversarial training into global and local goals and introduce a hierarchical\ndiscriminator that meets both objectives. Additionally, we develop a\nspecialized outpainting loss function that leverages both local and global\nfeatures of the discriminator. Fine-tuning on this adversarial loss function\nenhances the generator's ability to produce both visually appealing and\nglobally coherent outpainted scenes. Our proposed method outperforms\nstate-of-the-art methods both quantitatively and qualitatively. Supplementary\nmaterials including the demo video and the code are available in SigPort.", "AI": {"tldr": "The paper proposes a method to improve video outpainting by adapting video inpainting models with a hierarchical discriminator and specialized loss function, achieving better results than existing methods.", "motivation": "Video outpainting struggles with maintaining consistency when extending borders. Existing methods focus on background generation, but adapting inpainting models directly leads to blurry results.", "method": "The authors introduce a hierarchical discriminator for adversarial training, differentiating global and local objectives, and develop a specialized outpainting loss function.", "result": "The proposed method outperforms state-of-the-art approaches, producing visually appealing and coherent outpainted scenes.", "conclusion": "The hierarchical discriminator and tailored loss function effectively enhance video outpainting, demonstrating superior performance."}}
{"id": "2508.00083", "pdf": "https://arxiv.org/pdf/2508.00083", "abs": "https://arxiv.org/abs/2508.00083", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "title": "A Survey on Code Generation with LLM-based Agents", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.", "AI": {"tldr": "A survey of LLM-based code generation agents, highlighting their autonomy, expanded task scope, and engineering practicality, along with their development trajectory, techniques, applications, benchmarks, and future challenges.", "motivation": "To systematically review the emerging field of LLM-based code generation agents, which are transforming software development with their autonomous capabilities and broader SDLC scope.", "method": "The paper traces the technology's development, categorizes core techniques (single/multi-agent architectures), details SDLC applications, summarizes benchmarks, and catalogs tools.", "result": "The survey provides a comprehensive overview of the field, including key techniques, applications, and evaluation methods, while identifying current challenges.", "conclusion": "The paper proposes foundational research directions to address challenges and advance the field of LLM-based code generation agents."}}
{"id": "2508.00421", "pdf": "https://arxiv.org/pdf/2508.00421", "abs": "https://arxiv.org/abs/2508.00421", "authors": ["Runmin Cong", "Zongji Yu", "Hao Fang", "Haoyan Sun", "Sam Kwong"], "title": "UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken", "categories": ["cs.CV"], "comment": "ACM MM 2025", "summary": "Underwater Instance Segmentation (UIS) tasks are crucial for underwater\ncomplex scene detection. Mamba, as an emerging state space model with\ninherently linear complexity and global receptive fields, is highly suitable\nfor processing image segmentation tasks with long sequence features. However,\ndue to the particularity of underwater scenes, there are many challenges in\napplying Mamba to UIS. The existing fixed-patch scanning mechanism cannot\nmaintain the internal continuity of scanned instances in the presence of\nseverely underwater color distortion and blurred instance boundaries, and the\nhidden state of the complex underwater background can also inhibit the\nunderstanding of instance objects. In this work, we propose the first\nMamba-based underwater instance segmentation model UIS-Mamba, and design two\ninnovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to\nmigrate Mamba to the underwater task. DTS module maintains the continuity of\nthe internal features of the instance objects by allowing the patches to\ndynamically offset and scale, thereby guiding the minimum spanning tree and\nproviding dynamic local receptive fields. HSW module suppresses the\ninterference of complex backgrounds and effectively focuses the information\nflow of state propagation to the instances themselves through the Ncut-based\nhidden state weakening mechanism. Experimental results show that UIS-Mamba\nachieves state-of-the-art performance on both UIIS and USIS10K datasets, while\nmaintaining a low number of parameters and computational complexity. Code is\navailable at https://github.com/Maricalce/UIS-Mamba.", "AI": {"tldr": "UIS-Mamba, a Mamba-based model for underwater instance segmentation, introduces Dynamic Tree Scan and Hidden State Weaken modules to address challenges like color distortion and blurred boundaries, achieving state-of-the-art performance.", "motivation": "Underwater scenes pose challenges like color distortion and blurred boundaries, making instance segmentation difficult. Mamba's linear complexity and global receptive fields are promising but need adaptation for underwater tasks.", "method": "Proposes UIS-Mamba with Dynamic Tree Scan (DTS) for dynamic local receptive fields and Hidden State Weaken (HSW) to suppress background interference using Ncut-based weakening.", "result": "Achieves state-of-the-art performance on UIIS and USIS10K datasets with low parameters and computational complexity.", "conclusion": "UIS-Mamba effectively adapts Mamba for underwater instance segmentation, addressing key challenges and outperforming existing methods."}}
{"id": "2508.00161", "pdf": "https://arxiv.org/pdf/2508.00161", "abs": "https://arxiv.org/abs/2508.00161", "authors": ["Ziqian Zhong", "Aditi Raghunathan"], "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.", "AI": {"tldr": "A new method interprets weights instead of activations to monitor and control fine-tuned LLMs, detecting behaviors like backdoors and unlearning with high accuracy.", "motivation": "Existing interpretability methods rely on distributionally similar data, limiting their ability to detect novel threats like backdoors.", "method": "Analyzes weight differences between fine-tuned and base models, focusing on top singular vectors to detect new behaviors.", "result": "Detects backdoors with 100% success (1.2% false positives) and unlearning with 95.42% accuracy. Also useful for model auditing.", "conclusion": "The method effectively monitors and controls fine-tuned LLMs without needing training data, showing promise for security and auditing."}}
{"id": "2508.00427", "pdf": "https://arxiv.org/pdf/2508.00427", "abs": "https://arxiv.org/abs/2508.00427", "authors": ["Seunggeun Chi", "Enna Sachdeva", "Pin-Hao Huang", "Kwonjoon Lee"], "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 (Highlight)", "summary": "Amodal completion, which is the process of inferring the full appearance of\nobjects despite partial occlusions, is crucial for understanding complex\nhuman-object interactions (HOI) in computer vision and robotics. Existing\nmethods, such as those that use pre-trained diffusion models, often struggle to\ngenerate plausible completions in dynamic scenarios because they have a limited\nunderstanding of HOI. To solve this problem, we've developed a new approach\nthat uses physical prior knowledge along with a specialized multi-regional\ninpainting technique designed for HOI. By incorporating physical constraints\nfrom human topology and contact information, we define two distinct regions:\nthe primary region, where occluded object parts are most likely to be, and the\nsecondary region, where occlusions are less probable. Our multi-regional\ninpainting method uses customized denoising strategies across these regions\nwithin a diffusion model. This improves the accuracy and realism of the\ngenerated completions in both their shape and visual detail. Our experimental\nresults show that our approach significantly outperforms existing methods in\nHOI scenarios, moving machine perception closer to a more human-like\nunderstanding of dynamic environments. We also show that our pipeline is robust\neven without ground-truth contact annotations, which broadens its applicability\nto tasks like 3D reconstruction and novel view/pose synthesis.", "AI": {"tldr": "A new method for amodal completion in human-object interactions (HOI) uses physical priors and multi-regional inpainting to improve accuracy and realism, outperforming existing approaches.", "motivation": "Existing methods struggle with plausible completions in dynamic HOI scenarios due to limited understanding of interactions.", "method": "Incorporates physical constraints (human topology, contact info) and uses a multi-regional inpainting technique with customized denoising in a diffusion model.", "result": "Significantly outperforms existing methods in HOI scenarios, enhancing realism and accuracy.", "conclusion": "The approach advances machine perception in dynamic environments and is robust even without ground-truth contact annotations, broadening its applicability."}}
{"id": "2508.00440", "pdf": "https://arxiv.org/pdf/2508.00440", "abs": "https://arxiv.org/abs/2508.00440", "authors": ["M. A. P\u00e9rez-Cuti\u00f1o", "J. Valverde", "J. Capit\u00e1n", "J. M. D\u00edaz-B\u00e1\u00f1ez"], "title": "Reducing the gap between general purpose data and aerial images in concentrated solar power plants", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/.", "AI": {"tldr": "Proposes AerialCSP, a synthetic dataset for training models on aerial images of CSP plants, reducing manual labeling needs.", "motivation": "Generic datasets fail for CSP plants due to reflective surfaces and unique elements, making manual labeling costly.", "method": "Created AerialCSP, a virtual dataset simulating CSP plant imagery, and benchmarked models on it.", "result": "Pretraining on AerialCSP improves real-world fault detection, especially for rare/small defects.", "conclusion": "AerialCSP effectively reduces manual labeling and enhances model performance for CSP plant inspections."}}
{"id": "2508.00222", "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS enhances LLM reasoning by combining internal exploitation and external data, outperforming RLVR and avoiding capability boundary collapse.", "motivation": "RLVR struggles with LLM's inherent limits and sparse rewards, leading to capability boundary collapse. RL-PLUS aims to surpass these boundaries.", "method": "RL-PLUS uses Multiple Importance Sampling for data mismatch and an Exploration-Based Advantage Function to guide reasoning paths.", "result": "RL-PLUS achieves state-of-the-art performance on math reasoning benchmarks and out-of-distribution tasks, with 21.1% to 69.2% improvements.", "conclusion": "RL-PLUS effectively resolves capability boundary collapse and generalizes well across model families."}}
{"id": "2508.00442", "pdf": "https://arxiv.org/pdf/2508.00442", "abs": "https://arxiv.org/abs/2508.00442", "authors": ["Jiale Zhou", "Wenhan Wang", "Shikun Li", "Xiaolei Qu", "Xin Guo", "Yizhong Liu", "Wenzhong Tang", "Xun Lin", "Yefeng Zheng"], "title": "TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Tubular structure segmentation (TSS) is important for various applications,\nsuch as hemodynamic analysis and route navigation. Despite significant progress\nin TSS, domain shifts remain a major challenge, leading to performance\ndegradation in unseen target domains. Unlike other segmentation tasks, TSS is\nmore sensitive to domain shifts, as changes in topological structures can\ncompromise segmentation integrity, and variations in local features\ndistinguishing foreground from background (e.g., texture and contrast) may\nfurther disrupt topological continuity. To address these challenges, we propose\nTopology-enhanced Test-Time Adaptation (TopoTTA), the first test-time\nadaptation framework designed specifically for TSS. TopoTTA consists of two\nstages: Stage 1 adapts models to cross-domain topological discrepancies using\nthe proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance\ntopological representation without altering pre-trained parameters; Stage 2\nimproves topological continuity by a novel Topology Hard sample Generation\n(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels\nin the generated pseudo-break regions. Extensive experiments across four\nscenarios and ten datasets demonstrate TopoTTA's effectiveness in handling\ntopological distribution shifts, achieving an average improvement of 31.81% in\nclDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS\nmodels.", "AI": {"tldr": "TopoTTA is a test-time adaptation framework for tubular structure segmentation, addressing domain shifts by enhancing topological representation and continuity.", "motivation": "Domain shifts in tubular structure segmentation degrade performance due to sensitivity to topological changes and local feature variations.", "method": "TopoTTA uses Topological Meta Difference Convolutions (TopoMDCs) for cross-domain adaptation and Topology Hard sample Generation (TopoHG) for continuity improvement.", "result": "TopoTTA improves performance by 31.81% in clDice across four scenarios and ten datasets.", "conclusion": "TopoTTA effectively handles topological distribution shifts and is a plug-and-play solution for CNN-based TSS models."}}
{"id": "2508.00230", "pdf": "https://arxiv.org/pdf/2508.00230", "abs": "https://arxiv.org/abs/2508.00230", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "To appear in ICCV 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.", "AI": {"tldr": "KRAdapter, a new PEFT method using Khatri-Rao product, outperforms LoRA on high-rank matrices and maintains efficiency.", "motivation": "Address LoRA's limitations in approximating high-rank matrices, especially in multimodal and large language models.", "method": "Introduces KRAdapter, leveraging Khatri-Rao product for high-rank weight updates, tested on vision-language and large language models.", "result": "KRAdapter shows gains on tasks, especially unseen common-sense reasoning, while keeping LoRA's efficiency.", "conclusion": "KRAdapter is a practical, robust alternative for fine-tuning billion-scale models, overcoming LoRA's high-rank limitations."}}
{"id": "2508.00443", "pdf": "https://arxiv.org/pdf/2508.00443", "abs": "https://arxiv.org/abs/2508.00443", "authors": ["Longfei Huang", "Yu Liang", "Hao Zhang", "Jinwei Chen", "Wei Dong", "Lunde Chen", "Wanyu Liu", "Bo Li", "Pengtao Jiang"], "title": "SDMatte: Grafting Diffusion Models for Interactive Matting", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025, 11 pages, 4 figures", "summary": "Recent interactive matting methods have shown satisfactory performance in\ncapturing the primary regions of objects, but they fall short in extracting\nfine-grained details in edge regions. Diffusion models trained on billions of\nimage-text pairs, demonstrate exceptional capability in modeling highly complex\ndata distributions and synthesizing realistic texture details, while exhibiting\nrobust text-driven interaction capabilities, making them an attractive solution\nfor interactive matting. To this end, we propose SDMatte, a diffusion-driven\ninteractive matting model, with three key contributions. First, we exploit the\npowerful priors of diffusion models and transform the text-driven interaction\ncapability into visual prompt-driven interaction capability to enable\ninteractive matting. Second, we integrate coordinate embeddings of visual\nprompts and opacity embeddings of target objects into U-Net, enhancing\nSDMatte's sensitivity to spatial position information and opacity information.\nThird, we propose a masked self-attention mechanism that enables the model to\nfocus on areas specified by visual prompts, leading to better performance.\nExtensive experiments on multiple datasets demonstrate the superior performance\nof our method, validating its effectiveness in interactive matting. Our code\nand model are available at https://github.com/vivoCameraResearch/SDMatte.", "AI": {"tldr": "SDMatte leverages diffusion models for interactive matting, improving fine-grained detail extraction with visual prompts and a masked self-attention mechanism.", "motivation": "Existing methods lack fine-grained detail extraction in edge regions, while diffusion models offer robust capabilities for such tasks.", "method": "SDMatte transforms text-driven interaction into visual prompt-driven interaction, integrates coordinate and opacity embeddings, and uses masked self-attention.", "result": "Extensive experiments show superior performance in interactive matting.", "conclusion": "SDMatte effectively addresses fine-grained detail extraction in interactive matting, validated by experiments."}}
{"id": "2508.00271", "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "MetaAgent is a self-improving agent that learns by doing, using adaptive help-seeking, self-reflection, and tool-use history to enhance performance without model updates.", "motivation": "To create a robust, general-purpose knowledge discovery system that evolves autonomously through hands-on practice.", "method": "MetaAgent starts with basic reasoning, seeks help via natural language, routes requests to tools, and distills experience into reusable knowledge. It also builds tools and a knowledge base from its history.", "result": "Outperforms workflow-based baselines and matches/exceeds end-to-end trained agents on benchmarks like GAIA, WebWalkerQA, and BrowseCamp.", "conclusion": "MetaAgent demonstrates the potential of self-evolving agentic systems for effective knowledge discovery."}}
{"id": "2508.00445", "pdf": "https://arxiv.org/pdf/2508.00445", "abs": "https://arxiv.org/abs/2508.00445", "authors": ["Hongyi Cai", "Mohammad Mahdinur Rahman", "Mingkang Dong", "Jie Li", "Muxin Pu", "Zhili Fang", "Yinan Peng", "Hanjun Luo", "Yang Liu"], "title": "AutoDebias: Automated Framework for Debiasing Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Image (T2I) models generate high-quality images from text prompts but\noften exhibit unintended social biases, such as gender or racial stereotypes,\neven when these attributes are not mentioned. Existing debiasing methods work\nwell for simple or well-known cases but struggle with subtle or overlapping\nbiases. We propose AutoDebias, a framework that automatically identifies and\nmitigates harmful biases in T2I models without prior knowledge of specific bias\ntypes. Specifically, AutoDebias leverages vision-language models to detect\nbiased visual patterns and constructs fairness guides by generating inclusive\nalternative prompts that reflect balanced representations. These guides drive a\nCLIP-guided training process that promotes fairer outputs while preserving the\noriginal model's image quality and diversity. Unlike existing methods,\nAutoDebias effectively addresses both subtle stereotypes and multiple\ninteracting biases. We evaluate the framework on a benchmark covering over 25\nbias scenarios, including challenging cases where multiple biases occur\nsimultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and\nreduces biased outputs from 90% to negligible levels, while preserving the\nvisual fidelity of the original model.", "AI": {"tldr": "AutoDebias is a framework for automatically detecting and mitigating social biases in Text-to-Image models without prior knowledge of bias types, using vision-language models and fairness guides.", "motivation": "Address unintended social biases (e.g., gender, racial stereotypes) in T2I models, especially subtle or overlapping biases that existing methods struggle with.", "method": "Leverages vision-language models to detect biases, constructs fairness guides with inclusive prompts, and uses CLIP-guided training to balance representations.", "result": "Achieves 91.6% accuracy in detecting biases, reduces biased outputs from 90% to negligible levels, and maintains image quality and diversity.", "conclusion": "AutoDebias effectively addresses subtle and multiple interacting biases in T2I models while preserving visual fidelity."}}
{"id": "2508.00282", "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "The study compares human and LLM (GPT-4o) task generation, finding humans driven by psychological factors while LLMs lack such alignment, producing less social and physical tasks.", "motivation": "To explore whether LLMs simulate human-like task generation based on cognitive principles like intrinsic motivation and personal values.", "method": "A task-generation experiment comparing human responses with GPT-4o, with and without explicit psychological drivers.", "result": "Humans generate tasks influenced by values and cognitive style; LLMs produce less social, less physical, and more abstract tasks, despite being seen as more fun and novel.", "conclusion": "LLMs lack human-like embodied cognition, emphasizing the need for intrinsic motivation and physical grounding in AI design."}}
{"id": "2508.00447", "pdf": "https://arxiv.org/pdf/2508.00447", "abs": "https://arxiv.org/abs/2508.00447", "authors": ["Anju Rani", "Daniel Ortiz-Arroyo", "Petar Durdevic"], "title": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 8 figures", "summary": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications.", "AI": {"tldr": "CLIPTime is a multimodal framework built on CLIP to predict fungal growth stages and timestamps from image-text inputs, improving temporal modeling in biological monitoring.", "motivation": "Existing vision-language models like CLIP lack effectiveness in capturing temporal progression, which is crucial for biological growth analysis.", "method": "CLIPTime extends CLIP to jointly predict developmental stages (classification) and timestamps (regression) using a synthetic fungal growth dataset.", "result": "CLIPTime successfully models biological progression and provides interpretable, temporally grounded predictions.", "conclusion": "The framework demonstrates the potential of vision-language models for real-world biological monitoring applications."}}
{"id": "2508.00324", "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "categories": ["cs.AI", "cs.CL"], "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "R1-Act is a post-training method that activates safety knowledge in large reasoning models (LRMs) to reduce harmful outputs, requiring minimal training resources.", "motivation": "LRMs often comply with harmful instructions despite possessing safety knowledge, highlighting a need for methods to activate this knowledge during reasoning.", "method": "R1-Act uses a structured reasoning process to explicitly trigger safety knowledge, requiring only 1,000 examples and 90 minutes of training on a single GPU.", "result": "R1-Act improves safety without compromising reasoning performance, outperforming existing alignment methods.", "conclusion": "R1-Act is a robust, scalable, and efficient solution for enhancing LRM safety."}}
{"id": "2508.00453", "pdf": "https://arxiv.org/pdf/2508.00453", "abs": "https://arxiv.org/abs/2508.00453", "authors": ["Baisong Li", "Xingwang Wang", "Haixiao Xu"], "title": "PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA", "categories": ["cs.CV"], "comment": null, "summary": "The goal of multispectral and hyperspectral image fusion (MHIF) is to\ngenerate high-quality images that simultaneously possess rich spectral\ninformation and fine spatial details. However, due to the inherent trade-off\nbetween spectral and spatial information and the limited availability of\nobservations, this task is fundamentally ill-posed. Previous studies have not\neffectively addressed the ill-posed nature caused by data misalignment. To\ntackle this challenge, we propose a fusion framework named PIF-Net, which\nexplicitly incorporates ill-posed priors to effectively fuse multispectral\nimages and hyperspectral images. To balance global spectral modeling with\ncomputational efficiency, we design a method based on an invertible Mamba\narchitecture that maintains information consistency during feature\ntransformation and fusion, ensuring stable gradient flow and process\nreversibility. Furthermore, we introduce a novel fusion module called the\nFusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral\nand spatial features while keeping the model lightweight. Extensive experiments\non multiple benchmark datasets demonstrate that PIF-Net achieves significantly\nbetter image restoration performance than current state-of-the-art methods\nwhile maintaining model efficiency.", "AI": {"tldr": "PIF-Net is a novel framework for multispectral and hyperspectral image fusion (MHIF) that addresses the ill-posed nature of the task by incorporating ill-posed priors and using an invertible Mamba architecture for efficient feature fusion.", "motivation": "The inherent trade-off between spectral and spatial information in MHIF makes the task ill-posed, and previous methods failed to address data misalignment effectively.", "method": "PIF-Net uses an invertible Mamba architecture for global spectral modeling and introduces a Fusion-Aware Low-Rank Adaptation module for dynamic feature calibration.", "result": "Experiments show PIF-Net outperforms state-of-the-art methods in image restoration while maintaining efficiency.", "conclusion": "PIF-Net effectively balances spectral and spatial information, offering a robust solution to the ill-posed MHIF problem."}}
{"id": "2508.00408", "pdf": "https://arxiv.org/pdf/2508.00408", "abs": "https://arxiv.org/abs/2508.00408", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "categories": ["cs.SE", "cs.CL"], "comment": "Under Review", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).", "AI": {"tldr": "The paper introduces ULT, a new benchmark for evaluating LLMs in unit test generation, addressing data contamination and structural simplicity issues in existing benchmarks. Results show ULT is more challenging than others.", "motivation": "Existing benchmarks for LLM test generation suffer from data contamination and overly simple function code, leading to unreliable conclusions.", "method": "ULT is created through a multi-stage curation process for real-world Python functions, ensuring high complexity and mitigating contamination. PLT is also introduced for controlled analysis.", "result": "ULT proves significantly harder for LLMs, with lower accuracy and coverage metrics compared to TestEval and PLT.", "conclusion": "ULT provides a more realistic and challenging benchmark for evaluating LLMs in test generation, addressing flaws in existing benchmarks."}}
{"id": "2508.00471", "pdf": "https://arxiv.org/pdf/2508.00471", "abs": "https://arxiv.org/abs/2508.00471", "authors": ["Yiwen Wang", "Xinning Chai", "Yuhong Zhang", "Zhengxue Cheng", "Jun Zhao", "Rong Xie", "Li Song"], "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recent advancements in video super-resolution (VSR) models have demonstrated\nimpressive results in enhancing low-resolution videos. However, due to\nlimitations in adequately controlling the generation process, achieving high\nfidelity alignment with the low-resolution input while maintaining temporal\nconsistency across frames remains a significant challenge. In this work, we\npropose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel\napproach that incorporates both semantic and temporal-spatio guidance in the\nlatent diffusion space to address these challenges. By incorporating high-level\nsemantic information and integrating spatial and temporal information, our\napproach achieves a seamless balance between recovering intricate details and\nensuring temporal coherence. Our method not only preserves high-reality visual\ncontent but also significantly enhances fidelity. Extensive experiments\ndemonstrate that SeTe-VSR outperforms existing methods in terms of detail\nrecovery and perceptual quality, highlighting its effectiveness for complex\nvideo super-resolution tasks.", "AI": {"tldr": "SeTe-VSR improves video super-resolution by integrating semantic and temporal-spatio guidance, achieving better detail recovery and temporal coherence.", "motivation": "Current VSR models struggle with fidelity alignment and temporal consistency in low-resolution video enhancement.", "method": "Proposes SeTe-VSR, using semantic and temporal-spatio guidance in latent diffusion space for balanced detail recovery and coherence.", "result": "Outperforms existing methods in detail recovery and perceptual quality.", "conclusion": "SeTe-VSR is effective for complex video super-resolution tasks."}}
{"id": "2508.00414", "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "categories": ["cs.AI", "cs.CL"], "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "Cognitive Kernel-Pro is an open-source, free multi-module AI agent framework designed to democratize advanced AI agent development, achieving state-of-the-art results on GAIA.", "motivation": "Current AI agent systems are often closed-source or rely on paid APIs, limiting accessibility and reproducibility.", "method": "The framework focuses on high-quality training data curation, agent test-time reflection, and voting strategies across web, file, code, and reasoning domains.", "result": "The 8B-parameter open-source model outperforms previous leading systems like WebDancer and WebSailor on GAIA.", "conclusion": "Cognitive Kernel-Pro sets a new standard for accessible, high-capability AI agents, with code publicly available."}}
{"id": "2508.00473", "pdf": "https://arxiv.org/pdf/2508.00473", "abs": "https://arxiv.org/abs/2508.00473", "authors": ["Jiaping Cao", "Kangkang Zhou", "Juan Du"], "title": "HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection is a fundamental task in video surveillance, with\nbroad applications in public safety and intelligent monitoring systems.\nAlthough previous methods leverage Euclidean representations in RGB or depth\ndomains, such embeddings are inherently limited in capturing hierarchical event\nstructures and spatio-temporal continuity. To address these limitations, we\npropose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for\nanomaly detection in 3D point cloud videos. Our approach first extracts\nper-frame spatial features from point cloud sequences via point cloud\nextractor, and then embeds them into Lorentzian hyperbolic space, which better\ncaptures the latent hierarchical structure of events. To model temporal\ndynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism\nthat leverages Lorentzian inner products and curvature-aware softmax to learn\ntemporal dependencies under non-Euclidean geometry. Our method performs all\nfeature transformations and anomaly scoring directly within full Lorentzian\nspace rather than via tangent space approximation. Extensive experiments\ndemonstrate that HyPCV-Former achieves state-of-the-art performance across\nmultiple anomaly categories, with a 7\\% improvement on the TIMo dataset and a\n5.6\\% gain on the DAD dataset compared to benchmarks. The code will be released\nupon paper acceptance.", "AI": {"tldr": "HyPCV-Former, a hyperbolic spatio-temporal transformer, improves video anomaly detection by leveraging Lorentzian hyperbolic space and a novel attention mechanism, outperforming benchmarks by 5.6-7%.", "motivation": "Traditional Euclidean representations in RGB or depth domains fail to capture hierarchical event structures and spatio-temporal continuity in video anomaly detection.", "method": "HyPCV-Former uses a point cloud extractor for spatial features, embeds them in Lorentzian hyperbolic space, and employs a hyperbolic multi-head self-attention (HMHA) mechanism for temporal dynamics.", "result": "Achieves state-of-the-art performance with 7% improvement on TIMo and 5.6% on DAD datasets.", "conclusion": "HyPCV-Former effectively addresses limitations of Euclidean embeddings, offering superior anomaly detection in 3D point cloud videos."}}
{"id": "2508.00518", "pdf": "https://arxiv.org/pdf/2508.00518", "abs": "https://arxiv.org/abs/2508.00518", "authors": ["Shuo Liang", "Yiwu Zhong", "Zi-Yuan Hu", "Yeyao Tao", "Liwei Wang"], "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025", "summary": "Spatiotemporal video grounding aims to localize target entities in videos\nbased on textual queries. While existing research has made significant progress\nin exocentric videos, the egocentric setting remains relatively underexplored,\ndespite its growing importance in applications such as augmented reality and\nrobotics. In this work, we conduct a systematic analysis of the discrepancies\nbetween egocentric and exocentric videos, revealing key challenges such as\nshorter object durations, sparser trajectories, smaller object sizes, and\nlarger positional shifts. To address these challenges, we introduce EgoMask,\nthe first pixel-level benchmark for fine-grained spatiotemporal grounding in\negocentric videos. It is constructed by our proposed automatic annotation\npipeline, which annotates referring expressions and object masks across short-,\nmedium-, and long-term videos. Additionally, we create EgoMask-Train, a\nlarge-scale training dataset to facilitate model development. Experiments\ndemonstrate that the state-of-the-art spatiotemporal grounding models perform\npoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields\nsignificant improvements, while preserving performance on exocentric datasets.\nOur work thus provides essential resources and insights for advancing\negocentric video understanding. Our code is available at\nhttps://github.com/LaVi-Lab/EgoMask .", "AI": {"tldr": "The paper introduces EgoMask, a pixel-level benchmark for spatiotemporal grounding in egocentric videos, addressing challenges like shorter object durations and sparser trajectories. It includes an automatic annotation pipeline and a training dataset (EgoMask-Train), showing significant improvements when fine-tuning existing models.", "motivation": "Egocentric video grounding is underexplored compared to exocentric videos, despite its importance in applications like augmented reality and robotics. The paper aims to bridge this gap by analyzing discrepancies and providing resources for better egocentric video understanding.", "method": "The authors introduce EgoMask, a benchmark created via an automatic annotation pipeline for fine-grained spatiotemporal grounding. They also develop EgoMask-Train, a large-scale training dataset, and evaluate state-of-the-art models on their benchmark.", "result": "Existing models perform poorly on EgoMask but show significant improvement after fine-tuning on EgoMask-Train, without compromising performance on exocentric datasets.", "conclusion": "The work provides essential resources (EgoMask and EgoMask-Train) and insights for advancing egocentric video understanding, demonstrating the effectiveness of their approach."}}
{"id": "2508.00477", "pdf": "https://arxiv.org/pdf/2508.00477", "abs": "https://arxiv.org/abs/2508.00477", "authors": ["Yuzhuo Chen", "Zehua Ma", "Jianhua Wang", "Kai Kang", "Shunyu Yao", "Weiming Zhang"], "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer", "categories": ["cs.CV"], "comment": "8 pages, 5 figures, 3 tables", "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.", "AI": {"tldr": "LAMIC is a Layout-Aware Multi-Image Composition framework that extends single-reference diffusion models to multi-reference scenarios without training, using novel attention mechanisms and achieving state-of-the-art performance.", "motivation": "The challenge of generating coherent and consistent images from multiple references with spatial layout awareness in controllable image synthesis.", "method": "LAMIC introduces Group Isolation Attention (GIA) for entity disentanglement and Region-Modulated Attention (RMA) for layout-aware generation, built upon the MMDiT model.", "result": "LAMIC outperforms existing baselines in ID-S, BG-S, IN-R, and AVG scores, demonstrating superior identity keeping, background preservation, and layout control.", "conclusion": "LAMIC establishes a training-free paradigm for multi-image composition, with strong zero-shot generalization and scalability potential."}}
{"id": "2508.00534", "pdf": "https://arxiv.org/pdf/2508.00534", "abs": "https://arxiv.org/abs/2508.00534", "authors": ["Mikel Vandeloise"], "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations", "categories": ["cs.PL", "cs.CL", "D.3.2; F.3.2; D.3.1"], "comment": "Preprint submitted to the Journal of Object Technology on July 29,\n  2025. Data available upon request until peer-review is completed", "summary": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification.", "AI": {"tldr": "The paper reviews formal foundations of programming paradigms, highlighting limitations in current taxonomies and advocating for a compositional approach using mathematical frameworks like Type theory and Category theory.", "motivation": "The rise of multi-paradigm languages challenges traditional classification methods, causing interoperability issues, necessitating a formal, reconstructive approach.", "method": "A systematic literature review (SLR) of 74 primary studies to assess classification formalisms and identify conceptual primitives and mathematical frameworks.", "result": "Existing taxonomies lack granularity and a unified basis. A compositional approach with atomic primitives and mathematical frameworks (Type theory, Category theory, UTP) is proposed.", "conclusion": "The literature shows a shift from classification to formal reconstructive frameworks, with a proposed research agenda for unification."}}
{"id": "2508.00493", "pdf": "https://arxiv.org/pdf/2508.00493", "abs": "https://arxiv.org/abs/2508.00493", "authors": ["Alfie Roddan", "Tobias Czempiel", "Chi Xu", "Daniel S. Elson", "Stamatia Giannarou"], "title": "SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We present SAMSA 2.0, an interactive segmentation framework for hyperspectral\nmedical imaging that introduces spectral angle prompting to guide the Segment\nAnything Model (SAM) using spectral similarity alongside spatial cues. This\nearly fusion of spectral information enables more accurate and robust\nsegmentation across diverse spectral datasets. Without retraining, SAMSA 2.0\nachieves up to +3.8% higher Dice scores compared to RGB-only models and up to\n+3.1% over prior spectral fusion methods. Our approach enhances few-shot and\nzero-shot performance, demonstrating strong generalization in challenging\nlow-data and noisy scenarios common in clinical imaging.", "AI": {"tldr": "SAMSA 2.0 improves hyperspectral medical image segmentation by integrating spectral angle prompting with SAM, achieving higher accuracy and robustness without retraining.", "motivation": "Enhance segmentation accuracy and robustness in hyperspectral medical imaging by leveraging spectral similarity alongside spatial cues.", "method": "Introduces spectral angle prompting to guide SAM, fusing spectral information early for better performance.", "result": "Achieves up to +3.8% higher Dice scores than RGB-only models and +3.1% over prior spectral fusion methods, with improved few-shot and zero-shot performance.", "conclusion": "SAMSA 2.0 demonstrates strong generalization in low-data and noisy clinical scenarios, advancing hyperspectral segmentation."}}
{"id": "2508.00554", "pdf": "https://arxiv.org/pdf/2508.00554", "abs": "https://arxiv.org/abs/2508.00554", "authors": ["Li Zhao", "Rui Sun", "Zuoyou Jiang", "Bo Yang", "Yuxiao Bai", "Mengting Chen", "Xinyang Wang", "Jing Li", "Zuo Bai"], "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism", "categories": ["q-fin.TR", "cs.CL", "q-fin.CP"], "comment": null, "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics.", "AI": {"tldr": "A multi-agent system with internal competition improves LLM-based trading by reducing sensitivity to market noise.", "motivation": "High sensitivity to market noise limits LLM-based trading systems, prompting the need for a more robust solution.", "method": "Proposes a two-team system (Data Team and Research Team) with real-time evaluation and ranking to filter top-performing agents.", "result": "Outperforms existing multi-agent systems and traditional methods in trading performance.", "conclusion": "The system enhances adaptability and robustness, proving effective in dynamic financial markets."}}
{"id": "2508.00496", "pdf": "https://arxiv.org/pdf/2508.00496", "abs": "https://arxiv.org/abs/2508.00496", "authors": ["Mohammed Kamran", "Maria Bernathova", "Raoul Varga", "Christian Singer", "Zsuzsanna Bago-Horvath", "Thomas Helbich", "Georg Langs", "Philipp Seeb\u00f6ck"], "title": "LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced\nMRI (DCE-MRI) is critical for early cancer detection, especially in high-risk\npatients. While recent deep learning methods have advanced lesion segmentation,\nthey primarily target large lesions and neglect valuable longitudinal and\nclinical information routinely used by radiologists. In real-world screening,\ndetecting subtle or emerging lesions requires radiologists to compare across\ntimepoints and consider previous radiology assessments, such as the BI-RADS\nscore. We propose LesiOnTime, a novel 3D segmentation approach that mimics\nclinical diagnostic workflows by jointly leveraging longitudinal imaging and\nBIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)\nblock that dynamically integrates information from previous and current scans;\nand (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent\nspace alignment for scans with similar radiological assessments, thus embedding\ndomain knowledge into the training process. Evaluated on a curated in-house\nlongitudinal dataset of high-risk patients with DCE-MRI, our approach\noutperforms state-of-the-art single-timepoint and longitudinal baselines by 5%\nin terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute\ncomplementary performance gains. These results highlight the importance of\nincorporating temporal and clinical context for reliable early lesion\nsegmentation in real-world breast cancer screening. Our code is publicly\navailable at https://github.com/cirmuw/LesiOnTime", "AI": {"tldr": "LesiOnTime improves small lesion segmentation in breast DCE-MRI by integrating longitudinal imaging and BI-RADS scores, outperforming baselines by 5% Dice.", "motivation": "Existing deep learning methods for lesion segmentation focus on large lesions and ignore longitudinal and clinical context, which is critical for early cancer detection.", "method": "Proposes LesiOnTime with Temporal Prior Attention (TPA) for dynamic integration of past and current scans and BI-RADS Consistency Regularization (BCR) loss for latent space alignment.", "result": "Outperforms state-of-the-art methods by 5% Dice on a longitudinal dataset; TPA and BCR provide complementary gains.", "conclusion": "Incorporating temporal and clinical context enhances early lesion segmentation in breast cancer screening."}}
{"id": "2508.00555", "pdf": "https://arxiv.org/pdf/2508.00555", "abs": "https://arxiv.org/abs/2508.00555", "authors": ["Jiecong Wang", "Haoran Li", "Hao Peng", "Ziqian Zeng", "Zihao Wang", "Haohua Du", "Zhengtao Yu"], "title": "Activation-Guided Local Editing for Jailbreaking Attacks", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Jailbreaking is an essential adversarial technique for red-teaming these\nmodels to uncover and patch security flaws. However, existing jailbreak methods\nface significant drawbacks. Token-level jailbreak attacks often produce\nincoherent or unreadable inputs and exhibit poor transferability, while\nprompt-level attacks lack scalability and rely heavily on manual effort and\nhuman ingenuity. We propose a concise and effective two-stage framework that\ncombines the advantages of these approaches. The first stage performs a\nscenario-based generation of context and rephrases the original malicious query\nto obscure its harmful intent. The second stage then utilizes information from\nthe model's hidden states to guide fine-grained edits, effectively steering the\nmodel's internal representation of the input from a malicious toward a benign\none. Extensive experiments demonstrate that this method achieves\nstate-of-the-art Attack Success Rate, with gains of up to 37.74% over the\nstrongest baseline, and exhibits excellent transferability to black-box models.\nOur analysis further demonstrates that AGILE maintains substantial\neffectiveness against prominent defense mechanisms, highlighting the\nlimitations of current safeguards and providing valuable insights for future\ndefense development. Our code is available at\nhttps://github.com/yunsaijc/AGILE.", "AI": {"tldr": "AGILE is a two-stage jailbreak framework combining scenario-based generation and hidden-state-guided edits to improve attack success and transferability.", "motivation": "Existing jailbreak methods are either incoherent (token-level) or unscalable (prompt-level), limiting their effectiveness.", "method": "A two-stage approach: (1) scenario-based context generation and query rephrasing, (2) hidden-state-guided edits to shift input representation from malicious to benign.", "result": "Achieves state-of-the-art Attack Success Rate (up to 37.74% improvement) and strong transferability to black-box models.", "conclusion": "AGILE highlights current defense limitations and offers insights for future safeguards."}}
{"id": "2508.00506", "pdf": "https://arxiv.org/pdf/2508.00506", "abs": "https://arxiv.org/abs/2508.00506", "authors": ["Tulsi Patel", "Mark W. Jones", "Thomas Redfern"], "title": "Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool", "categories": ["cs.CV"], "comment": "Video supplement demonstrating feature-space exploration and\n  interactive labelling is available at: https://youtu.be/GZl1ebZJgEA and is\n  archived at https://doi.org/10.5281/zenodo.16676591", "summary": "Machine learning for remote sensing imaging relies on up-to-date and accurate\nlabels for model training and testing. Labelling remote sensing imagery is time\nand cost intensive, requiring expert analysis. Previous labelling tools rely on\npre-labelled data for training in order to label new unseen data. In this work,\nwe define an unsupervised pipeline for finding and labelling geographical areas\nof similar context and content within Sentinel-2 satellite imagery. Our\napproach removes limitations of previous methods by utilising segmentation with\nconvolutional and graph neural networks to encode a more robust feature space\nfor image comparison. Unlike previous approaches we segment the image into\nhomogeneous regions of pixels that are grouped based on colour and spatial\nsimilarity. Graph neural networks are used to aggregate information about the\nsurrounding segments enabling the feature representation to encode the local\nneighbourhood whilst preserving its own local information. This reduces\noutliers in the labelling tool, allows users to label at a granular level, and\nallows a rotationally invariant semantic relationship at the image level to be\nformed within the encoding space.", "AI": {"tldr": "An unsupervised pipeline for labeling similar geographical areas in Sentinel-2 imagery using segmentation with convolutional and graph neural networks, improving robustness and granularity.", "motivation": "Labeling remote sensing imagery is costly and time-consuming, requiring expert input. Existing tools depend on pre-labeled data, limiting flexibility.", "method": "Uses segmentation with convolutional and graph neural networks to group pixels by color and spatial similarity, encoding robust features and local neighborhood information.", "result": "Reduces labeling outliers, enables granular labeling, and forms rotationally invariant semantic relationships in the encoding space.", "conclusion": "The approach overcomes limitations of prior methods, offering a more efficient and flexible solution for remote sensing image labeling."}}
{"id": "2508.00589", "pdf": "https://arxiv.org/pdf/2508.00589", "abs": "https://arxiv.org/abs/2508.00589", "authors": ["Stefan Englmeier", "Max A. B\u00fcttner", "Katharina Winter", "Fabian B. Flohr"], "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.RO", "68T45, 68P20, 68T10, 68T50, 68T07, 68T40", "I.2.10; I.4.8; I.2.9; H.3.3"], "comment": "9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication", "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.", "AI": {"tldr": "A novel context-aware motion retrieval framework for identifying rare human behaviors in autonomous driving datasets, improving retrieval accuracy by 27.5%.", "motivation": "To enhance the evaluation and generalization of autonomous driving systems by addressing the challenge of retrieving rare human behaviors in large datasets.", "method": "Combines SMPL-based motion sequences and video frames into a shared multimodal embedding space aligned with natural language for scalable retrieval via text queries. Introduces the WayMoCo dataset with labeled motion and scene context descriptions.", "result": "Outperforms state-of-the-art models by up to 27.5% in motion-context retrieval accuracy on the WayMoCo dataset.", "conclusion": "The proposed framework effectively retrieves diverse human behaviors, supporting robust evaluation of autonomous driving systems in safety-critical scenarios."}}
{"id": "2508.00659", "pdf": "https://arxiv.org/pdf/2508.00659", "abs": "https://arxiv.org/abs/2508.00659", "authors": ["Xinzhang Chen", "Hassan Ali", "Arash Shaghaghi", "Salil S. Kanhere", "Sanjay Jha"], "title": "Demo: TOSense -- What Did You Just Agree to?", "categories": ["cs.CR", "cs.CL"], "comment": "Accepted as a demonstration paper at IEEE LCN 2025", "summary": "Online services often require users to agree to lengthy and obscure Terms of\nService (ToS), leading to information asymmetry and legal risks. This paper\nproposes TOSense-a Chrome extension that allows users to ask questions about\nToS in natural language and get concise answers in real time. The system\ncombines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and\n(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval\nand BART-encoder for answer relevance verification. To avoid expensive manual\nannotation, we present a novel Question Answering Evaluation Pipeline (QEP)\nthat generates synthetic questions and verifies the correctness of answers\nusing clustered topic matching. Experiments on five major platforms, Apple,\nGoogle, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of\nTOSense (with up to 44.5% accuracy) across varying number of topic clusters.\nDuring the demonstration, we will showcase TOSense in action. Attendees will be\nable to experience seamless extraction, interactive question answering, and\ninstant indexing of new sites.", "AI": {"tldr": "TOSense is a Chrome extension that helps users understand Terms of Service (ToS) by answering natural language questions in real time, using a crawler and a lightweight LLM pipeline.", "motivation": "To address information asymmetry and legal risks caused by lengthy and obscure ToS agreements.", "method": "Combines a crawler (tos-crawl) for content extraction and a lightweight LLM pipeline (MiniLM for retrieval, BART-encoder for verification). Uses a synthetic QA evaluation pipeline (QEP) to avoid manual annotation.", "result": "Achieves up to 44.5% accuracy on five major platforms (Apple, Google, X, Microsoft, Netflix).", "conclusion": "TOSense effectively simplifies ToS comprehension and will be demonstrated for interactive use."}}
{"id": "2508.00528", "pdf": "https://arxiv.org/pdf/2508.00528", "abs": "https://arxiv.org/abs/2508.00528", "authors": ["Jinsong Yang", "Zeyuan Hu", "Yichen Li"], "title": "EPANet: Efficient Path Aggregation Network for Underwater Fish Detection", "categories": ["cs.CV"], "comment": null, "summary": "Underwater fish detection (UFD) remains a challenging task in computer vision\ndue to low object resolution, significant background interference, and high\nvisual similarity between targets and surroundings. Existing approaches\nprimarily focus on local feature enhancement or incorporate complex attention\nmechanisms to highlight small objects, often at the cost of increased model\ncomplexity and reduced efficiency. To address these limitations, we propose an\nefficient path aggregation network (EPANet), which leverages complementary\nfeature integration to achieve accurate and lightweight UFD. EPANet consists of\ntwo key components: an efficient path aggregation feature pyramid network\n(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP\nbottleneck). The EPA-FPN introduces long-range skip connections across\ndisparate scales to improve semantic-spatial complementarity, while cross-layer\nfusion paths are adopted to enhance feature integration efficiency. The MS-DDSP\nbottleneck extends the conventional bottleneck structure by introducing\nfiner-grained feature division and diverse convolutional operations, thereby\nincreasing local feature diversity and representation capacity. Extensive\nexperiments on benchmark UFD datasets demonstrate that EPANet outperforms\nstate-of-the-art methods in terms of detection accuracy and inference speed,\nwhile maintaining comparable or even lower parameter complexity.", "AI": {"tldr": "EPANet, an efficient path aggregation network, improves underwater fish detection by combining complementary features with lightweight design, outperforming existing methods in accuracy and speed.", "motivation": "Underwater fish detection faces challenges like low resolution, background interference, and target similarity, with current methods being inefficient or overly complex.", "method": "EPANet uses an efficient path aggregation feature pyramid network (EPA-FPN) for semantic-spatial complementarity and a multi-scale diverse-division short path bottleneck (MS-DDSP) for enhanced feature diversity.", "result": "EPANet achieves superior detection accuracy and speed on benchmark datasets while maintaining low parameter complexity.", "conclusion": "EPANet offers an efficient and accurate solution for underwater fish detection, balancing performance and simplicity."}}
{"id": "2508.00695", "pdf": "https://arxiv.org/pdf/2508.00695", "abs": "https://arxiv.org/abs/2508.00695", "authors": ["Sergio Rubio-Mart\u00edn", "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s", "Antonio Serrano-Garc\u00eda", "Clara Margarita Franch-Pato", "Arturo Crespo-\u00c1lvaro", "Jos\u00e9 Alberto Ben\u00edtez-Andrades"], "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.", "AI": {"tldr": "The study compares AI models (traditional ML and Deep Learning) for classifying clinical notes into Anxiety and Adjustment Disorder diagnoses, evaluating oversampling and hyperparameter tuning. Oversampling had minimal impact, but hyperparameter tuning significantly boosted accuracy, with Decision Tree, XGBoost, DistilBERT, and SciBERT achieving 96% accuracy.", "motivation": "To improve AI-assisted diagnostic tools in mental health by evaluating model performance and data balancing methods for classifying clinical notes.", "method": "Comparison of traditional ML (Random Forest, SVM, KNN, Decision Tree, XGBoost) and Deep Learning (DistilBERT, SciBERT) models, with oversampling (None, Random, SMOTE) and hyperparameter tuning.", "result": "Oversampling had little impact except SMOTE with BERT models. Hyperparameter tuning improved accuracy, with Decision Tree, XGBoost, DistilBERT, and SciBERT reaching 96%.", "conclusion": "Hyperparameter tuning is crucial for model performance. The study provides insights into effective AI models and data balancing for mental health diagnostics."}}
{"id": "2508.00548", "pdf": "https://arxiv.org/pdf/2508.00548", "abs": "https://arxiv.org/abs/2508.00548", "authors": ["Seunghyun Shin", "Dongmin Shin", "Jisu Shin", "Hae-Gon Jeon", "Joon-Young Lee"], "title": "Video Color Grading via Look-Up Table Generation", "categories": ["cs.CV"], "comment": "ICCV2025", "summary": "Different from color correction and transfer, color grading involves\nadjusting colors for artistic or storytelling purposes in a video, which is\nused to establish a specific look or mood. However, due to the complexity of\nthe process and the need for specialized editing skills, video color grading\nremains primarily the domain of professional colorists. In this paper, we\npresent a reference-based video color grading framework. Our key idea is\nexplicitly generating a look-up table (LUT) for color attribute alignment\nbetween reference scenes and input video via a diffusion model. As a training\nobjective, we enforce that high-level features of the reference scenes like\nlook, mood, and emotion should be similar to that of the input video. Our\nLUT-based approach allows for color grading without any loss of structural\ndetails in the whole video frames as well as achieving fast inference. We\nfurther build a pipeline to incorporate a user-preference via text prompts for\nlow-level feature enhancement such as contrast and brightness, etc.\nExperimental results, including extensive user studies, demonstrate the\neffectiveness of our approach for video color grading. Codes are publicly\navailable at https://github.com/seunghyuns98/VideoColorGrading.", "AI": {"tldr": "A reference-based video color grading framework using a diffusion model to generate LUTs for artistic color adjustments, preserving structural details and enabling fast inference.", "motivation": "Professional color grading is complex and skill-intensive; the paper aims to democratize it by automating the process while preserving artistic intent.", "method": "Uses a diffusion model to generate LUTs for aligning color attributes between reference scenes and input video, incorporating user preferences via text prompts.", "result": "Effective video color grading with preserved structural details, fast inference, and user preference integration, validated by user studies.", "conclusion": "The framework successfully automates video color grading, making it accessible while maintaining artistic quality and efficiency."}}
{"id": "2508.00549", "pdf": "https://arxiv.org/pdf/2508.00549", "abs": "https://arxiv.org/abs/2508.00549", "authors": ["Daniel Wolf", "Heiko Hillenhagen", "Billurvan Taskin", "Alex B\u00e4uerle", "Meinrad Beer", "Michael G\u00f6tz", "Timo Ropinski"], "title": "Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images", "categories": ["cs.CV"], "comment": "Accepted at the International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025", "summary": "Clinical decision-making relies heavily on understanding relative positions\nof anatomical structures and anomalies. Therefore, for Vision-Language Models\n(VLMs) to be applicable in clinical practice, the ability to accurately\ndetermine relative positions on medical images is a fundamental prerequisite.\nDespite its importance, this capability remains highly underexplored. To\naddress this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,\nLlama3.2, Pixtral, and JanusPro, and find that all models fail at this\nfundamental task. Inspired by successful approaches in computer vision, we\ninvestigate whether visual prompts, such as alphanumeric or colored markers\nplaced on anatomical structures, can enhance performance. While these markers\nprovide moderate improvements, results remain significantly lower on medical\nimages compared to observations made on natural images. Our evaluations suggest\nthat, in medical imaging, VLMs rely more on prior anatomical knowledge than on\nactual image content for answering relative position questions, often leading\nto incorrect conclusions. To facilitate further research in this area, we\nintroduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,\ndesigned to systematically evaluate the capability to identify relative\npositions in medical images.", "AI": {"tldr": "VLMs struggle with determining relative positions in medical images, despite moderate improvements from visual prompts. A new benchmark, MIRP, is introduced to evaluate this capability.", "motivation": "Understanding relative positions of anatomical structures is crucial for clinical decision-making, but VLMs lack this capability, hindering their clinical application.", "method": "Evaluated state-of-the-art VLMs (GPT-4o, Llama3.2, Pixtral, JanusPro) and tested visual prompts (alphanumeric/colored markers) for performance enhancement.", "result": "VLMs performed poorly on medical images, relying more on prior knowledge than image content. Visual prompts improved results but not sufficiently.", "conclusion": "VLMs need significant improvement for clinical use. The MIRP benchmark is introduced to advance research in this area."}}
{"id": "2508.00552", "pdf": "https://arxiv.org/pdf/2508.00552", "abs": "https://arxiv.org/abs/2508.00552", "authors": ["Chihan Huang", "Belal Alsinglawi", "Islam Al-qudah"], "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification.", "AI": {"tldr": "DBLP introduces an efficient diffusion-based adversarial purification method using noise bridge distillation and adaptive semantic enhancement, achieving SOTA results with fast inference.", "motivation": "Addressing the vulnerability of DNNs to adversarial perturbations and the inefficiency of existing diffusion-based purification methods.", "method": "Proposes Diffusion Bridge Distillation for Purification (DBLP) with noise bridge distillation and adaptive semantic enhancement using multi-scale pyramid edge maps.", "result": "Achieves state-of-the-art robust accuracy, superior image quality, and ~0.2s inference time.", "conclusion": "DBLP marks a significant advancement toward real-time adversarial purification with high efficiency and effectiveness."}}
{"id": "2508.00553", "pdf": "https://arxiv.org/pdf/2508.00553", "abs": "https://arxiv.org/abs/2508.00553", "authors": ["Jizhihui Liu", "Feiyi Du", "Guangdao Zhu", "Niu Lian", "Jun Li", "Bin Chen"], "title": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune.", "AI": {"tldr": "HiPrune is a training-free, model-agnostic token pruning framework for VLMs that leverages hierarchical attention to retain key tokens, achieving high efficiency and accuracy.", "motivation": "VLMs suffer from computational inefficiency due to lengthy visual token sequences. Existing methods are not scalable or require task-specific training.", "method": "HiPrune identifies three token types (Anchor, Buffer, Register) based on hierarchical attention in vision encoders, pruning non-informative tokens without retraining.", "result": "HiPrune preserves 99.3% accuracy with 33.3% tokens and reduces FLOPs/latency by up to 9x, outperforming prior methods.", "conclusion": "HiPrune offers a scalable, efficient solution for token pruning in VLMs, generalizing well across models and tasks."}}
{"id": "2508.00557", "pdf": "https://arxiv.org/pdf/2508.00557", "abs": "https://arxiv.org/abs/2508.00557", "authors": ["Qi Chen", "Lingxiao Yang", "Yun Chen", "Nailong Zhao", "Jianhuang Lai", "Jie Shao", "Xiaohua Xie"], "title": "Training-Free Class Purification for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Fine-tuning pre-trained vision-language models has emerged as a powerful\napproach for enhancing open-vocabulary semantic segmentation (OVSS). However,\nthe substantial computational and resource demands associated with training on\nlarge datasets have prompted interest in training-free methods for OVSS.\nExisting training-free approaches primarily focus on modifying model\narchitectures and generating prototypes to improve segmentation performance.\nHowever, they often neglect the challenges posed by class redundancy, where\nmultiple categories are not present in the current test image, and\nvisual-language ambiguity, where semantic similarities among categories create\nconfusion in class activation. These issues can lead to suboptimal class\nactivation maps and affinity-refined activation maps. Motivated by these\nobservations, we propose FreeCP, a novel training-free class purification\nframework designed to address these challenges. FreeCP focuses on purifying\nsemantic categories and rectifying errors caused by redundancy and ambiguity.\nThe purified class representations are then leveraged to produce final\nsegmentation predictions. We conduct extensive experiments across eight\nbenchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,\nas a plug-and-play module, significantly boosts segmentation performance when\ncombined with other OVSS methods.", "AI": {"tldr": "FreeCP is a training-free class purification framework for open-vocabulary semantic segmentation, addressing class redundancy and visual-language ambiguity to improve segmentation performance.", "motivation": "Existing training-free methods neglect challenges like class redundancy and visual-language ambiguity, leading to suboptimal segmentation.", "method": "FreeCP purifies semantic categories and rectifies errors from redundancy and ambiguity, leveraging purified representations for segmentation.", "result": "Extensive experiments on eight benchmarks show FreeCP significantly enhances segmentation performance when combined with other methods.", "conclusion": "FreeCP effectively addresses key challenges in training-free OVSS, improving segmentation accuracy as a plug-and-play module."}}
{"id": "2508.00558", "pdf": "https://arxiv.org/pdf/2508.00558", "abs": "https://arxiv.org/abs/2508.00558", "authors": ["Jens U. Kreber", "Joerg Stueckler"], "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for publication at the IEEE/CVF International Conference on\n  Computer Vision (ICCV), 2025", "summary": "Articulated objects are an important type of interactable objects in everyday\nenvironments. In this paper, we propose PhysNAP, a novel diffusion model-based\napproach for generating articulated objects that aligns them with partial point\nclouds and improves their physical plausibility. The model represents part\nshapes by signed distance functions (SDFs). We guide the reverse diffusion\nprocess using a point cloud alignment loss computed using the predicted SDFs.\nAdditionally, we impose non-penetration and mobility constraints based on the\npart SDFs for guiding the model to generate more physically plausible objects.\nWe also make our diffusion approach category-aware to further improve point\ncloud alignment if category information is available. We evaluate the\ngenerative ability and constraint consistency of samples generated with PhysNAP\nusing the PartNet-Mobility dataset. We also compare it with an unguided\nbaseline diffusion model and demonstrate that PhysNAP can improve constraint\nconsistency and provides a tradeoff with generative ability.", "AI": {"tldr": "PhysNAP is a diffusion model-based method for generating articulated objects, improving physical plausibility and alignment with partial point clouds using SDFs and constraints.", "motivation": "Articulated objects are common in everyday environments, but generating them with physical plausibility and alignment to partial point clouds is challenging.", "method": "Uses a diffusion model with SDFs for part shapes, guided by point cloud alignment loss, non-penetration, and mobility constraints. Category-aware diffusion improves alignment.", "result": "Evaluated on PartNet-Mobility, PhysNAP improves constraint consistency and offers a tradeoff with generative ability compared to an unguided baseline.", "conclusion": "PhysNAP effectively generates physically plausible articulated objects with better alignment and constraint consistency."}}
{"id": "2508.00563", "pdf": "https://arxiv.org/pdf/2508.00563", "abs": "https://arxiv.org/abs/2508.00563", "authors": ["Hannah Kniesel", "Leon Sick", "Tristan Payer", "Tim Bergner", "Kavitha Shaga Devan", "Clarissa Read", "Paul Walther", "Timo Ropinski"], "title": "Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images", "categories": ["cs.CV"], "comment": null, "summary": "Current state-of-the-art methods for object detection rely on annotated\nbounding boxes of large data sets for training. However, obtaining such\nannotations is expensive and can require up to hundreds of hours of manual\nlabor. This poses a challenge, especially since such annotations can only be\nprovided by experts, as they require knowledge about the scientific domain. To\ntackle this challenge, we propose a domain-specific weakly supervised object\ndetection algorithm that only relies on image-level annotations, which are\nsignificantly easier to acquire. Our method distills the knowledge of a\npre-trained model, on the task of predicting the presence or absence of a virus\nin an image, to obtain a set of pseudo-labels that can be used to later train a\nstate-of-the-art object detection model. To do so, we use an optimization\napproach with a shrinking receptive field to extract virus particles directly\nwithout specific network architectures. Through a set of extensive studies, we\nshow how the proposed pseudo-labels are easier to obtain, and, more\nimportantly, are able to outperform other existing weak labeling methods, and\neven ground truth labels, in cases where the time to obtain the annotation is\nlimited.", "AI": {"tldr": "A weakly supervised object detection method using image-level annotations to avoid costly bounding box labels, outperforming other weak labeling methods.", "motivation": "Reducing the need for expensive, expert-annotated bounding boxes in object detection by leveraging easier-to-acquire image-level labels.", "method": "Uses a pre-trained model to generate pseudo-labels via optimization with a shrinking receptive field, avoiding specialized architectures.", "result": "Pseudo-labels outperform other weak labeling methods and even ground truth labels when annotation time is limited.", "conclusion": "The proposed method efficiently reduces annotation costs while maintaining or improving detection performance."}}
{"id": "2508.00568", "pdf": "https://arxiv.org/pdf/2508.00568", "abs": "https://arxiv.org/abs/2508.00568", "authors": ["Jingchao Xie", "Oussema Dhaouadi", "Weirong Chen", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry", "categories": ["cs.CV"], "comment": "Accepted for GCPR 2025. Project page:\n  https://jchao-xie.github.io/CoProU/", "summary": "Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and\naugmented reality, with unsupervised approaches eliminating the need for\nexpensive ground-truth labels. However, these methods struggle when dynamic\nobjects violate the static scene assumption, leading to erroneous pose\nestimations. We tackle this problem by uncertainty modeling, which is a\ncommonly used technique that creates robust masks to filter out dynamic objects\nand occlusions without requiring explicit motion segmentation. Traditional\nuncertainty modeling considers only single-frame information, overlooking the\nuncertainties across consecutive frames. Our key insight is that uncertainty\nmust be propagated and combined across temporal frames to effectively identify\nunreliable regions, particularly in dynamic scenes. To address this challenge,\nwe introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end\napproach that combines target frame uncertainty with projected reference frame\nuncertainty using a principled probabilistic formulation. Built upon vision\ntransformer backbones, our model simultaneously learns depth, uncertainty\nestimation, and camera poses. Consequently, experiments on the KITTI and\nnuScenes datasets demonstrate significant improvements over previous\nunsupervised monocular end-to-end two-frame-based methods and exhibit strong\nperformance in challenging highway scenes where other approaches often fail.\nAdditionally, comprehensive ablation studies validate the effectiveness of\ncross-frame uncertainty propagation.", "AI": {"tldr": "CoProU-VO improves unsupervised Visual Odometry by combining uncertainty across frames to handle dynamic scenes, outperforming existing methods.", "motivation": "Dynamic objects and occlusions in scenes violate static assumptions, causing errors in pose estimation. Traditional uncertainty modeling ignores temporal frame uncertainties.", "method": "Introduces CoProU-VO, an end-to-end approach combining target and reference frame uncertainties using probabilistic formulation, built on vision transformers.", "result": "Outperforms previous methods on KITTI and nuScenes datasets, especially in challenging highway scenes. Ablation studies confirm cross-frame uncertainty propagation's effectiveness.", "conclusion": "CoProU-VO successfully addresses dynamic scene challenges by propagating uncertainty across frames, enhancing robustness in unsupervised VO."}}
{"id": "2508.00587", "pdf": "https://arxiv.org/pdf/2508.00587", "abs": "https://arxiv.org/abs/2508.00587", "authors": ["Marc H\u00f6lle", "Walter Kellermann", "Vasileios Belagiannis"], "title": "Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "Accepted at ICCVW 2025, 11 pages, 4 figures", "summary": "Semantic segmentation models trained on known object classes often fail in\nreal-world autonomous driving scenarios by confidently misclassifying unknown\nobjects. While pixel-wise out-of-distribution detection can identify unknown\nobjects, existing methods struggle in complex scenes where rare object classes\nare often confused with truly unknown objects. We introduce an\nuncertainty-aware likelihood ratio estimation method that addresses these\nlimitations. Our approach uses an evidential classifier within a likelihood\nratio test to distinguish between known and unknown pixel features from a\nsemantic segmentation model, while explicitly accounting for uncertainty.\nInstead of producing point estimates, our method outputs probability\ndistributions that capture uncertainty from both rare training examples and\nimperfect synthetic outliers. We show that by incorporating uncertainty in this\nway, outlier exposure can be leveraged more effectively. Evaluated on five\nstandard benchmark datasets, our method achieves the lowest average false\npositive rate (2.5%) among state-of-the-art while maintaining high average\nprecision (90.91%) and incurring only negligible computational overhead. Code\nis available at https://github.com/glasbruch/ULRE.", "AI": {"tldr": "The paper introduces an uncertainty-aware likelihood ratio estimation method to improve out-of-distribution detection in semantic segmentation for autonomous driving, reducing false positives while maintaining high precision.", "motivation": "Semantic segmentation models often misclassify unknown objects in real-world scenarios, and existing methods struggle with rare classes.", "method": "The approach uses an evidential classifier within a likelihood ratio test to distinguish known and unknown pixel features, accounting for uncertainty via probability distributions.", "result": "The method achieves a 2.5% false positive rate and 90.91% precision on five benchmark datasets with minimal computational overhead.", "conclusion": "Incorporating uncertainty improves outlier exposure effectiveness, making the method robust for real-world applications."}}
{"id": "2508.00590", "pdf": "https://arxiv.org/pdf/2508.00590", "abs": "https://arxiv.org/abs/2508.00590", "authors": ["Yihe Tian", "Kwan Man Cheng", "Zhengbo Zhang", "Tao Zhang", "Suju Li", "Dongmei Yan", "Bing Xu"], "title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Artificial Night-Time Light (NTL) remote sensing is a vital proxy for\nquantifying the intensity and spatial distribution of human activities.\nAlthough the NPP-VIIRS sensor provides high-quality NTL observations, its\ntemporal coverage, which begins in 2012, restricts long-term time-series\nstudies that extend to earlier periods. Despite the progress in extending\nVIIRS-like NTL time-series, current methods still suffer from two significant\nshortcomings: the underestimation of light intensity and the structural\nomission. To overcome these limitations, we propose a novel reconstruction\nframework consisting of a two-stage process: construction and refinement. The\nconstruction stage features a Hierarchical Fusion Decoder (HFD) designed to\nenhance the fidelity of the initial reconstruction. The refinement stage\nemploys a Dual Feature Refiner (DFR), which leverages high-resolution\nimpervious surface masks to guide and enhance fine-grained structural details.\nBased on this framework, we developed the Extended VIIRS-like Artificial\nNighttime Light (EVAL) product for China, extending the standard data record\nbackwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL\nsignificantly outperforms existing state-of-the-art products, boosting the\n$\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.\nFurthermore, EVAL exhibits excellent temporal consistency and maintains a high\ncorrelation with socioeconomic parameters, confirming its reliability for\nlong-term analysis. The resulting EVAL dataset provides a valuable new resource\nfor the research community and is publicly available at\nhttps://doi.org/10.11888/HumanNat.tpdc.302930.", "AI": {"tldr": "The paper introduces EVAL, a framework to reconstruct extended VIIRS-like artificial nighttime light data for China, addressing underestimation and structural omission issues in current methods.", "motivation": "Existing NTL data lacks long-term coverage and suffers from intensity underestimation and structural omission, limiting historical analysis.", "method": "A two-stage framework: construction (Hierarchical Fusion Decoder) and refinement (Dual Feature Refiner using impervious surface masks).", "result": "EVAL extends NTL data back to 1986, outperforming existing products (R\u00b2 0.80 vs. 0.68, RMSE 0.99 vs. 1.27).", "conclusion": "EVAL is reliable for long-term analysis and publicly available, offering a valuable resource for research."}}
{"id": "2508.00591", "pdf": "https://arxiv.org/pdf/2508.00591", "abs": "https://arxiv.org/abs/2508.00591", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long"], "title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image systems", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)\ntechnology enabling diverse and creative image synthesis. However, some outputs\nmay contain Not Safe For Work (NSFW) content (e.g., violence), violating\ncommunity guidelines. Detecting NSFW content efficiently and accurately, known\nas external safeguarding, is essential. Existing external safeguards fall into\ntwo types: text filters, which analyze user prompts but overlook T2I\nmodel-specific variations and are prone to adversarial attacks; and image\nfilters, which analyze final generated images but are computationally costly\nand introduce latency. Diffusion models, the foundation of modern T2I systems\nlike Stable Diffusion, generate images through iterative denoising using a\nU-Net architecture with ResNet and Transformer blocks. We observe that: (1)\nearly denoising steps define the semantic layout of the image, and (2)\ncross-attention layers in U-Net are crucial for aligning text and image\nregions. Based on these insights, we propose Wukong, a transformer-based NSFW\ndetection framework that leverages intermediate outputs from early denoising\nsteps and reuses U-Net's pre-trained cross-attention parameters. Wukong\noperates within the diffusion process, enabling early detection without waiting\nfor full image generation. We also introduce a new dataset containing prompts,\nseeds, and image-specific NSFW labels, and evaluate Wukong on this and two\npublic benchmarks. Results show that Wukong significantly outperforms\ntext-based safeguards and achieves comparable accuracy of image filters, while\noffering much greater efficiency.", "AI": {"tldr": "Wukong is a transformer-based NSFW detection framework for T2I generation, leveraging early denoising steps and pre-trained U-Net cross-attention for efficient and accurate detection.", "motivation": "Existing NSFW safeguards (text or image filters) are either inefficient or prone to attacks, necessitating a better solution.", "method": "Wukong uses intermediate outputs from early denoising steps and reuses U-Net's cross-attention parameters for early detection.", "result": "Wukong outperforms text-based safeguards and matches image filters' accuracy while being more efficient.", "conclusion": "Wukong provides an effective and efficient solution for NSFW detection in T2I generation."}}
{"id": "2508.00592", "pdf": "https://arxiv.org/pdf/2508.00592", "abs": "https://arxiv.org/abs/2508.00592", "authors": ["Jiajun Le", "Jiayi Ma"], "title": "GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in two-view geometry increasingly emphasizes enforcing\nsmoothness and global consistency priors when estimating motion fields between\npairs of images. However, in complex real-world scenes, characterized by\nextreme viewpoint and scale changes as well as pronounced depth\ndiscontinuities, the motion field often exhibits diverse and heterogeneous\nmotion patterns. Most existing methods lack targeted modeling strategies and\nfail to explicitly account for this variability, resulting in estimated motion\nfields that diverge from their true underlying structure and distribution. We\nobserve that Mixture-of-Experts (MoE) can assign dedicated experts to motion\nsub-fields, enabling a divide-and-conquer strategy for heterogeneous motion\npatterns. Building on this insight, we re-architect motion field modeling in\ntwo-view geometry with GeoMoE, a streamlined framework. Specifically, we first\ndevise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier\nprobability signals to perform a structure-aware decomposition of the motion\nfield into heterogeneous sub-fields, sharply curbing outlier-induced bias.\nNext, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each\nsub-field along spatial-context and channel-semantic paths and routes it to a\ncustomized expert for targeted modeling, thereby decoupling heterogeneous\nmotion regimes, suppressing cross-sub-field interference and representational\nentanglement, and yielding fine-grained motion-field rectification. With this\nminimalist design, GeoMoE outperforms prior state-of-the-art methods in\nrelative pose and homography estimation and shows strong generalization. The\nsource code and pre-trained models are available at\nhttps://github.com/JiajunLe/GeoMoE.", "AI": {"tldr": "GeoMoE introduces a Mixture-of-Experts (MoE) framework for two-view geometry, addressing heterogeneous motion patterns in complex scenes with a divide-and-conquer strategy.", "motivation": "Existing methods fail to handle diverse motion patterns in complex scenes, leading to inaccurate motion field estimates.", "method": "GeoMoE uses a Probabilistic Prior-Guided Decomposition and MoE-Enhanced Bi-Path Rectifier to model sub-fields and assign experts for targeted motion rectification.", "result": "GeoMoE outperforms state-of-the-art methods in relative pose and homography estimation with strong generalization.", "conclusion": "GeoMoE provides a streamlined, effective solution for motion field modeling in two-view geometry."}}
{"id": "2508.00599", "pdf": "https://arxiv.org/pdf/2508.00599", "abs": "https://arxiv.org/abs/2508.00599", "authors": ["Junzhe Lu", "Jing Lin", "Hongkun Dou", "Ailing Zeng", "Yue Deng", "Xian Liu", "Zhongang Cai", "Lei Yang", "Yulun Zhang", "Haoqian Wang", "Ziwei Liu"], "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior", "categories": ["cs.CV"], "comment": "ICCV 2025 (oral); Code released: https://github.com/moonbow721/DPoser", "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.", "AI": {"tldr": "DPoser-X is a diffusion-based prior model for 3D whole-body human poses, addressing challenges like pose complexity and dataset scarcity. It unifies pose tasks as inverse problems, uses variational diffusion sampling, and introduces novel training methods for robustness.", "motivation": "Building a versatile and robust full-body human pose prior is challenging due to articulated pose complexity and limited high-quality datasets.", "method": "Introduces DPoser (Diffusion model as body Pose prior) and extends it to DPoser-X. Uses variational diffusion sampling, truncated timestep scheduling, and masked training to combine whole-body and part-specific datasets.", "result": "DPoser-X outperforms state-of-the-art models in body, hand, face, and full-body pose benchmarks, demonstrating robustness and versatility.", "conclusion": "DPoser-X sets a new benchmark for whole-body human pose prior modeling, offering superior performance and adaptability."}}
{"id": "2508.00620", "pdf": "https://arxiv.org/pdf/2508.00620", "abs": "https://arxiv.org/abs/2508.00620", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi"], "title": "Backdoor Attacks on Deep Learning Face Detection", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.", "AI": {"tldr": "The paper explores vulnerabilities in face detection systems, introducing Face Generation Attacks and a novel Landmark Shift Attack, and proposes mitigations.", "motivation": "Face recognition systems in unconstrained environments face challenges like inconsistent lighting and poses, requiring robust face detection. This paper investigates vulnerabilities in such systems.", "method": "The study introduces Face Generation Attacks and a Landmark Shift Attack, targeting bounding box and landmark coordinate regression in face detectors.", "result": "The attacks successfully backdoor the coordinate regression task, demonstrating vulnerabilities in face detection systems.", "conclusion": "The paper highlights security risks in face detection and offers mitigations to address these vulnerabilities."}}
{"id": "2508.00639", "pdf": "https://arxiv.org/pdf/2508.00639", "abs": "https://arxiv.org/abs/2508.00639", "authors": ["Luisa Gall\u00e9e", "Catharina Silvia Lisson", "Christoph Gerhard Lisson", "Daniela Drees", "Felix Weig", "Daniel Vogele", "Meinrad Beer", "Michael G\u00f6tz"], "title": "Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification", "categories": ["cs.CV"], "comment": "Accepted at iMIMIC - Interpretability of Machine Intelligence in\n  Medical Image Computing workshop MICCAI 2025 Medical Image Computing and\n  Computer Assisted Intervention", "summary": "Classification models that provide human-interpretable explanations enhance\nclinicians' trust and usability in medical image diagnosis. One research focus\nis the integration and prediction of pathology-related visual attributes used\nby radiologists alongside the diagnosis, aligning AI decision-making with\nclinical reasoning. Radiologists use attributes like shape and texture as\nestablished diagnostic criteria and mirroring these in AI decision-making both\nenhances transparency and enables explicit validation of model outputs.\nHowever, the adoption of such models is limited by the scarcity of large-scale\nmedical image datasets annotated with these attributes. To address this\nchallenge, we propose synthesizing attribute-annotated data using a generative\nmodel. We enhance the Diffusion Model with attribute conditioning and train it\nusing only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.\nIncorporating its generated images into the training of an explainable model\nboosts performance, increasing attribute prediction accuracy by 13.4% and\ntarget prediction accuracy by 1.8% compared to training with only the small\nreal attribute-annotated dataset. This work highlights the potential of\nsynthetic data to overcome dataset limitations, enhancing the applicability of\nexplainable models in medical image analysis.", "AI": {"tldr": "The paper proposes using a generative Diffusion Model to synthesize attribute-annotated medical images, improving explainable AI performance in diagnosis.", "motivation": "Enhancing clinicians' trust in AI by aligning AI decision-making with radiologists' reasoning through pathology-related visual attributes.", "method": "A Diffusion Model is enhanced with attribute conditioning and trained on a small dataset (20 samples) to generate synthetic attribute-annotated images.", "result": "Using synthetic data boosts attribute prediction accuracy by 13.4% and target prediction accuracy by 1.8% compared to training with limited real data.", "conclusion": "Synthetic data can overcome dataset limitations, improving the practicality of explainable models in medical image analysis."}}
{"id": "2508.00649", "pdf": "https://arxiv.org/pdf/2508.00649", "abs": "https://arxiv.org/abs/2508.00649", "authors": ["Junhao Zheng", "Jiahao Sun", "Chenhao Lin", "Zhengyu Zhao", "Chen Ma", "Chong Zhang", "Cong Wang", "Qian Wang", "Chao Shen"], "title": "Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights", "categories": ["cs.CV"], "comment": null, "summary": "Developing reliable defenses against patch attacks on object detectors has\nattracted increasing interest. However, we identify that existing defense\nevaluations lack a unified and comprehensive framework, resulting in\ninconsistent and incomplete assessments of current methods. To address this\nissue, we revisit 11 representative defenses and present the first patch\ndefense benchmark, involving 2 attack goals, 13 patch attacks, 11 object\ndetectors, and 4 diverse metrics. This leads to the large-scale adversarial\npatch dataset with 94 types of patches and 94,000 images. Our comprehensive\nanalyses reveal new insights: (1) The difficulty in defending against\nnaturalistic patches lies in the data distribution, rather than the commonly\nbelieved high frequencies. Our new dataset with diverse patch distributions can\nbe used to improve existing defenses by 15.09% AP@0.5. (2) The average\nprecision of the attacked object, rather than the commonly pursued patch\ndetection accuracy, shows high consistency with defense performance. (3)\nAdaptive attacks can substantially bypass existing defenses, and defenses with\ncomplex/stochastic models or universal patch properties are relatively robust.\nWe hope that our analyses will serve as guidance on properly evaluating patch\nattacks/defenses and advancing their design. Code and dataset are available at\nhttps://github.com/Gandolfczjh/APDE, where we will keep integrating new\nattacks/defenses.", "AI": {"tldr": "The paper introduces a unified benchmark for evaluating defenses against patch attacks on object detectors, revealing key insights and improving existing defenses by 15.09% AP@0.5.", "motivation": "Existing defense evaluations lack a comprehensive framework, leading to inconsistent assessments of methods.", "method": "The study revisits 11 defenses, creates a benchmark with 2 attack goals, 13 attacks, 11 detectors, and 4 metrics, and analyzes a dataset of 94,000 images.", "result": "Key findings include the importance of data distribution in defense difficulty, the relevance of attacked object precision, and the robustness of complex/stochastic defenses.", "conclusion": "The benchmark provides guidance for evaluating and designing patch defenses, with ongoing updates to include new attacks/defenses."}}
{"id": "2508.00698", "pdf": "https://arxiv.org/pdf/2508.00698", "abs": "https://arxiv.org/abs/2508.00698", "authors": ["Hongfei Zhang", "Kun Zhou", "Ruizheng Wu", "Jiangbo Lu"], "title": "Can Large Pretrained Depth Estimation Models Help With Image Dehazing?", "categories": ["cs.CV"], "comment": "Submitted to AAAI2026", "summary": "Image dehazing remains a challenging problem due to the spatially varying\nnature of haze in real-world scenes. While existing methods have demonstrated\nthe promise of large-scale pretrained models for image dehazing, their\narchitecture-specific designs hinder adaptability across diverse scenarios with\ndifferent accuracy and efficiency requirements. In this work, we systematically\ninvestigate the generalization capability of pretrained depth\nrepresentations-learned from millions of diverse images-for image dehazing. Our\nempirical analysis reveals that the learned deep depth features maintain\nremarkable consistency across varying haze levels. Building on this insight, we\npropose a plug-and-play RGB-D fusion module that seamlessly integrates with\ndiverse dehazing architectures. Extensive experiments across multiple\nbenchmarks validate both the effectiveness and broad applicability of our\napproach.", "AI": {"tldr": "The paper explores using pretrained depth representations for image dehazing, proposing a plug-and-play RGB-D fusion module for adaptability across diverse scenarios.", "motivation": "Existing dehazing methods lack adaptability due to architecture-specific designs, despite the promise of large-scale pretrained models.", "method": "Systematically investigates pretrained depth representations and introduces a plug-and-play RGB-D fusion module.", "result": "Learned deep depth features remain consistent across haze levels; the module is validated across benchmarks.", "conclusion": "The proposed approach is effective and broadly applicable for image dehazing."}}
{"id": "2508.00701", "pdf": "https://arxiv.org/pdf/2508.00701", "abs": "https://arxiv.org/abs/2508.00701", "authors": ["Chende Zheng", "Ruiqi suo", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Shuai Liu", "Minghui Yang", "Cong Wang", "Chao Shen"], "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "The evolution of video generation techniques, such as Sora, has made it\nincreasingly easy to produce high-fidelity AI-generated videos, raising public\nconcern over the dissemination of synthetic content. However, existing\ndetection methodologies remain limited by their insufficient exploration of\ntemporal artifacts in synthetic videos. To bridge this gap, we establish a\ntheoretical framework through second-order dynamical analysis under Newtonian\nmechanics, subsequently extending the Second-order Central Difference features\ntailored for temporal artifact detection. Building on this theoretical\nfoundation, we reveal a fundamental divergence in second-order feature\ndistributions between real and AI-generated videos. Concretely, we propose\nDetection by Difference of Differences (D3), a novel training-free detection\nmethod that leverages the above second-order temporal discrepancies. We\nvalidate the superiority of our D3 on 4 open-source datasets (Gen-Video,\nVideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,\nD3 outperforms the previous best method by 10.39% (absolute) mean Average\nPrecision. Additional experiments on time cost and post-processing operations\ndemonstrate D3's exceptional computational efficiency and strong robust\nperformance. Our code is available at https://github.com/Zig-HS/D3.", "AI": {"tldr": "The paper introduces D3, a training-free method for detecting AI-generated videos by analyzing second-order temporal artifacts, outperforming existing methods.", "motivation": "Public concern over synthetic video dissemination highlights the need for better detection methods, as current approaches lack exploration of temporal artifacts.", "method": "The authors propose a theoretical framework using second-order dynamical analysis and introduce D3, which leverages second-order temporal discrepancies for detection.", "result": "D3 outperforms previous methods by 10.39% mean Average Precision on Gen-Video and shows computational efficiency and robustness.", "conclusion": "D3 is a superior, efficient, and robust method for detecting AI-generated videos, validated across multiple datasets."}}
{"id": "2508.00726", "pdf": "https://arxiv.org/pdf/2508.00726", "abs": "https://arxiv.org/abs/2508.00726", "authors": ["Jiale Li", "Mingrui Wu", "Zixiang Jin", "Hao Chen", "Jiayi Ji", "Xiaoshuai Sun", "Liujuan Cao", "Rongrong Ji"], "title": "MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "ACM MM25 has accepted this paper", "summary": "Despite growing interest in hallucination in Multimodal Large Language\nModels, existing studies primarily focus on single-image settings, leaving\nhallucination in multi-image scenarios largely unexplored. To address this gap,\nwe conduct the first systematic study of hallucinations in multi-image MLLMs\nand propose MIHBench, a benchmark specifically tailored for evaluating\nobject-related hallucinations across multiple images. MIHBench comprises three\ncore tasks: Multi-Image Object Existence Hallucination, Multi-Image Object\nCount Hallucination, and Object Identity Consistency Hallucination, targeting\nsemantic understanding across object existence, quantity reasoning, and\ncross-view identity consistency. Through extensive evaluation, we identify key\nfactors associated with the occurrence of multi-image hallucinations,\nincluding: a progressive relationship between the number of image inputs and\nthe likelihood of hallucination occurrences; a strong correlation between\nsingle-image hallucination tendencies and those observed in multi-image\ncontexts; and the influence of same-object image ratios and the positional\nplacement of negative samples within image sequences on the occurrence of\nobject identity consistency hallucination. To address these challenges, we\npropose a Dynamic Attention Balancing mechanism that adjusts inter-image\nattention distributions while preserving the overall visual attention\nproportion. Experiments across multiple state-of-the-art MLLMs demonstrate that\nour method effectively reduces hallucination occurrences and enhances semantic\nintegration and reasoning stability in multi-image scenarios.", "AI": {"tldr": "The paper introduces MIHBench, a benchmark for evaluating object-related hallucinations in multi-image MLLMs, identifies key factors causing hallucinations, and proposes a Dynamic Attention Balancing mechanism to mitigate them.", "motivation": "Existing studies on hallucination in MLLMs focus on single-image settings, leaving multi-image scenarios unexplored. This work aims to fill that gap.", "method": "The authors propose MIHBench with three tasks to evaluate hallucinations and introduce a Dynamic Attention Balancing mechanism to adjust inter-image attention.", "result": "Key findings include the relationship between image inputs and hallucination likelihood, and the effectiveness of the proposed mechanism in reducing hallucinations.", "conclusion": "The Dynamic Attention Balancing mechanism improves semantic integration and reasoning stability in multi-image MLLMs, addressing hallucination challenges."}}
{"id": "2508.00728", "pdf": "https://arxiv.org/pdf/2508.00728", "abs": "https://arxiv.org/abs/2508.00728", "authors": ["Guanning Zeng", "Xiang Zhang", "Zirui Wang", "Haiyang Xu", "Zeyuan Chen", "Bingnan Li", "Zhuowen Tu"], "title": "YOLO-Count: Differentiable Object Counting for Text-to-Image Generation", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "We propose YOLO-Count, a differentiable open-vocabulary object counting model\nthat tackles both general counting challenges and enables precise quantity\ncontrol for text-to-image (T2I) generation. A core contribution is the\n'cardinality' map, a novel regression target that accounts for variations in\nobject size and spatial distribution. Leveraging representation alignment and a\nhybrid strong-weak supervision scheme, YOLO-Count bridges the gap between\nopen-vocabulary counting and T2I generation control. Its fully differentiable\narchitecture facilitates gradient-based optimization, enabling accurate object\ncount estimation and fine-grained guidance for generative models. Extensive\nexperiments demonstrate that YOLO-Count achieves state-of-the-art counting\naccuracy while providing robust and effective quantity control for T2I systems.", "AI": {"tldr": "YOLO-Count is a differentiable model for open-vocabulary object counting and precise quantity control in text-to-image generation, using a novel 'cardinality' map and hybrid supervision.", "motivation": "To address challenges in general object counting and enable accurate quantity control in text-to-image generation.", "method": "Introduces a 'cardinality' map for regression, uses representation alignment, and employs a hybrid strong-weak supervision scheme.", "result": "Achieves state-of-the-art counting accuracy and robust quantity control for T2I systems.", "conclusion": "YOLO-Count effectively bridges open-vocabulary counting and generative model control, demonstrating superior performance."}}
{"id": "2508.00744", "pdf": "https://arxiv.org/pdf/2508.00744", "abs": "https://arxiv.org/abs/2508.00744", "authors": ["Adwait Chandorkar", "Hasan Tercan", "Tobias Meisen"], "title": "Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR", "categories": ["cs.CV"], "comment": "accepted at the Embedded Vision Workshop ICCV 2025", "summary": "Recent advancements in LiDAR-based 3D object detection have significantly\naccelerated progress toward the realization of fully autonomous driving in\nreal-world environments. Despite achieving high detection performance, most of\nthe approaches still rely on a VGG-based or ResNet-based backbone for feature\nexploration, which increases the model complexity. Lightweight backbone design\nis well-explored for 2D object detection, but research on 3D object detection\nstill remains limited. In this work, we introduce Dense Backbone, a lightweight\nbackbone that combines the benefits of high processing speed, lightweight\narchitecture, and robust detection accuracy. We adapt multiple SoTA 3d object\ndetectors, such as PillarNet, with our backbone and show that with our\nbackbone, these models retain most of their detection capability at a\nsignificantly reduced computational cost. To our knowledge, this is the first\ndense-layer-based backbone tailored specifically for 3D object detection from\npoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%\nreduction in model parameters and a 28% reduction in latency with just a 2%\ndrop in detection accuracy on the nuScenes test set. Furthermore, Dense\nBackbone's plug-and-play design allows straightforward integration into\nexisting architectures, requiring no modifications to other network components.", "AI": {"tldr": "The paper introduces Dense Backbone, a lightweight backbone for 3D object detection, reducing model complexity and computational cost while maintaining high accuracy.", "motivation": "Current 3D object detection methods rely on complex backbones like VGG or ResNet, increasing model complexity. Lightweight backbones for 3D detection are understudied.", "method": "The authors propose Dense Backbone, a lightweight backbone combining speed, efficiency, and accuracy. It integrates with existing detectors like PillarNet without modifying other components.", "result": "DensePillarNet reduces parameters by 29% and latency by 28%, with only a 2% accuracy drop on the nuScenes test set.", "conclusion": "Dense Backbone offers a plug-and-play solution for efficient 3D object detection, balancing performance and computational cost."}}
{"id": "2508.00746", "pdf": "https://arxiv.org/pdf/2508.00746", "abs": "https://arxiv.org/abs/2508.00746", "authors": ["Regine Hartwig", "Dominik Muhle", "Riccardo Marin", "Daniel Cremers"], "title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in feature learning have shown that self-supervised vision\nfoundation models can capture semantic correspondences but often lack awareness\nof underlying 3D geometry. GECO addresses this gap by producing geometrically\ncoherent features that semantically distinguish parts based on geometry (e.g.,\nleft/right eyes, front/back legs). We propose a training framework based on\noptimal transport, enabling supervision beyond keypoints, even under occlusions\nand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%\nfaster than prior methods, while achieving state-of-the-art performance on\nPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.\nFinally, we show that PCK alone is insufficient to capture geometric quality\nand introduce new metrics and insights for more geometry-aware feature\nlearning. Link to project page:\nhttps://reginehartwig.github.io/publications/geco/", "AI": {"tldr": "GECO introduces a self-supervised vision model that enhances geometric awareness in feature learning, outperforming prior methods in speed and accuracy.", "motivation": "Existing feature learning models lack 3D geometry awareness, limiting their ability to distinguish parts based on geometry.", "method": "GECO uses optimal transport for training, enabling supervision beyond keypoints and handling occlusions. It has a lightweight architecture for real-time performance.", "result": "GECO achieves 30 fps (98.2% faster than prior methods) and improves PCK by 6.0%, 6.2%, and 4.1% on PFPascal, APK, and CUB datasets.", "conclusion": "PCK is insufficient for geometric quality; GECO introduces new metrics for geometry-aware feature learning."}}
{"id": "2508.00748", "pdf": "https://arxiv.org/pdf/2508.00748", "abs": "https://arxiv.org/abs/2508.00748", "authors": ["Laura Pedrouzo-Rodriguez", "Pedro Delgado-DeRobles", "Luis F. Gomez", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.MM"], "comment": "Accepted at the IEEE International Joint Conference on Biometrics\n  (IJCB 2025)", "summary": "Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.", "AI": {"tldr": "The paper explores facial motion patterns as behavioral biometrics for verifying identity in photorealistic avatar-mediated scenarios, proposing a lightweight Graph Convolutional Network with 80% AUC performance.", "motivation": "Addressing security risks like impersonation in avatar-based communication by leveraging unique facial motion patterns for identity verification.", "method": "Introduces a dataset of genuine/impostor avatar videos and a spatio-temporal Graph Convolutional Network with temporal attention pooling, using facial landmarks to model gestures.", "result": "Facial motion cues achieve ~80% AUC for identity verification, demonstrating their reliability.", "conclusion": "Highlights the need for advanced behavioral biometric defenses in avatar systems, providing a benchmark for future research."}}
{"id": "2508.00750", "pdf": "https://arxiv.org/pdf/2508.00750", "abs": "https://arxiv.org/abs/2508.00750", "authors": ["Prerana Ramkumar"], "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.", "AI": {"tldr": "SU-ESRGAN enhances satellite imagery super-resolution by integrating ESRGAN with segmentation loss and uncertainty maps, improving semantic consistency and credibility for critical applications.", "motivation": "GANs lack semantic consistency and per-pixel confidence in super-resolution, limiting their use in critical remote sensing tasks like disaster response and urban planning.", "method": "SU-ESRGAN combines ESRGAN, DeepLabv3 for segmentation loss, and Monte Carlo dropout for uncertainty maps, tested on aerial and drone datasets.", "result": "Achieves comparable PSNR, SSIM, LPIPS to ESRGAN, with better domain adaptation when fine-tuned on aligned datasets.", "conclusion": "SU-ESRGAN is effective for satellite and UAV imagery, emphasizing the need for domain-aware training in super-resolution applications."}}
{"id": "2508.00766", "pdf": "https://arxiv.org/pdf/2508.00766", "abs": "https://arxiv.org/abs/2508.00766", "authors": ["Irene Iele", "Francesco Di Feola", "Valerio Guarrasi", "Paolo Soda"], "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.", "AI": {"tldr": "A novel Test-Time Adaptation (TTA) framework is proposed for medical image-to-image translation, dynamically adjusting the process per test sample to handle out-of-distribution cases without degrading in-distribution performance.", "motivation": "Address limitations in handling out-of-distribution samples in image-to-image translation without performance degradation.", "method": "Introduces a Reconstruction Module to quantify domain shift and a Dynamic Adaptation Block for selective feature modification in pretrained models.", "result": "Consistent improvements in low-dose CT denoising and T1 to T2 MRI translation tasks over baseline and prior TTA methods.", "conclusion": "Dynamic, sample-specific adjustment improves model resilience, outperforming uniform adaptation methods."}}
{"id": "2508.00777", "pdf": "https://arxiv.org/pdf/2508.00777", "abs": "https://arxiv.org/abs/2508.00777", "authors": ["Zihan Wang", "Samira Ebrahimi Kahou", "Narges Armanfard"], "title": "Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning", "categories": ["cs.CV"], "comment": "Accepted at BMVC 2025", "summary": "Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects\nin unseen categories by relying solely on generalizable features rather than\nrequiring any labeled examples of anomalies. However, existing ZSAD methods,\nwhether using fixed or learned prompts, struggle under domain shifts because\ntheir training data are derived from limited training domains and fail to\ngeneralize to new distributions. In this paper, we introduce PILOT, a framework\ndesigned to overcome these challenges through two key innovations: (1) a novel\ndual-branch prompt learning mechanism that dynamically integrates a pool of\nlearnable prompts with structured semantic attributes, enabling the model to\nadaptively weight the most relevant anomaly cues for each input image; and (2)\na label-free test-time adaptation strategy that updates the learnable prompt\nparameters using high-confidence pseudo-labels from unlabeled test data.\nExtensive experiments on 13 industrial and medical benchmarks demonstrate that\nPILOT achieves state-of-the-art performance in both anomaly detection and\nlocalization under domain shift.", "AI": {"tldr": "PILOT introduces a dual-branch prompt learning mechanism and label-free test-time adaptation to improve zero-shot anomaly detection under domain shifts.", "motivation": "Existing zero-shot anomaly detection methods fail under domain shifts due to limited training data and lack of generalization.", "method": "PILOT uses a dual-branch prompt learning mechanism and label-free test-time adaptation with pseudo-labels.", "result": "PILOT achieves state-of-the-art performance on 13 benchmarks for anomaly detection and localization under domain shifts.", "conclusion": "PILOT effectively addresses domain shift challenges in zero-shot anomaly detection with innovative prompt learning and adaptation strategies."}}
{"id": "2508.00822", "pdf": "https://arxiv.org/pdf/2508.00822", "abs": "https://arxiv.org/abs/2508.00822", "authors": ["Alexander Nikitas Dimopoulos", "Joseph Grasso"], "title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "This study analyzes semantic segmentation performance across heterogeneously\nlabeled point-cloud datasets relevant to public safety applications, including\npre-incident planning systems derived from lidar scans. Using NIST's Point\nCloud City dataset (Enfield and Memphis collections), we investigate challenges\nin unifying differently labeled 3D data. Our methodology employs a graded\nschema with the KPConv architecture, evaluating performance through IoU metrics\non safety-relevant features. Results indicate performance variability:\ngeometrically large objects (e.g. stairs, windows) achieve higher segmentation\nperformance, suggesting potential for navigational context, while smaller\nsafety-critical features exhibit lower recognition rates. Performance is\nimpacted by class imbalance and the limited geometric distinction of smaller\nobjects in typical lidar scans, indicating limitations in detecting certain\nsafety-relevant features using current point-cloud methods. Key identified\nchallenges include insufficient labeled data, difficulties in unifying class\nlabels across datasets, and the need for standardization. Potential directions\ninclude automated labeling and multi-dataset learning strategies. We conclude\nthat reliable point-cloud semantic segmentation for public safety necessitates\nstandardized annotation protocols and improved labeling techniques to address\ndata heterogeneity and the detection of small, safety-critical elements.", "AI": {"tldr": "The study examines semantic segmentation in heterogeneously labeled point-cloud datasets for public safety, highlighting challenges like class imbalance and label unification, and suggests improvements like standardized protocols and automated labeling.", "motivation": "To address challenges in unifying differently labeled 3D data for public safety applications, particularly in pre-incident planning systems using lidar scans.", "method": "Uses NIST's Point Cloud City dataset with a graded schema and KPConv architecture, evaluating performance via IoU metrics on safety-relevant features.", "result": "Performance varies: larger objects (e.g., stairs, windows) segment better, while smaller safety-critical features have lower recognition rates due to class imbalance and geometric limitations.", "conclusion": "Reliable segmentation for public safety requires standardized annotation protocols and better labeling techniques to handle data heterogeneity and detect small, critical elements."}}
{"id": "2508.00823", "pdf": "https://arxiv.org/pdf/2508.00823", "abs": "https://arxiv.org/abs/2508.00823", "authors": ["Wenxuan Guo", "Xiuwei Xu", "Hang Yin", "Ziwei Wang", "Jianjiang Feng", "Jie Zhou", "Jiwen Lu"], "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICCV 2025. Project page:\n  https://gwxuan.github.io/IGL-Nav/", "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.", "AI": {"tldr": "IGL-Nav introduces an incremental 3D Gaussian localization framework for efficient and 3D-aware image-goal navigation, outperforming existing methods.", "motivation": "Existing methods fail to fully model the geometric relationship between the explored 3D environment and the goal image, making goal localization inefficient.", "method": "IGL-Nav incrementally updates scene representation with monocular prediction, coarsely localizes the goal via geometric matching, and refines the pose with differentiable rendering.", "result": "IGL-Nav significantly outperforms state-of-the-art methods and handles free-view image-goal navigation, even in real-world robotic deployment.", "conclusion": "The proposed framework efficiently and accurately localizes goal images in 3D space, advancing image-goal navigation capabilities."}}
{"id": "2507.22953", "pdf": "https://arxiv.org/pdf/2507.22953", "abs": "https://arxiv.org/abs/2507.22953", "authors": ["Murong Xu", "Tamaz Amiranashvili", "Fernando Navarro", "Maksym Fritsak", "Ibrahim Ethem Hamamci", "Suprosanna Shit", "Bastian Wittmann", "Sezgin Er", "Sebastian M. Christ", "Ezequiel de la Rosa", "Julian Deseoe", "Robert Graf", "Hendrik M\u00f6ller", "Anjany Sekuboyina", "Jan C. Peeken", "Sven Becker", "Giulia Baldini", "Johannes Haubold", "Felix Nensa", "Ren\u00e9 Hosch", "Nikhil Mirajkar", "Saad Khalid", "Stefan Zachow", "Marc-Andr\u00e9 Weber", "Georg Langs", "Jakob Wasserthal", "Mehmet Kemal Ozdemir", "Andrey Fedorov", "Ron Kikinis", "Stephanie Tanadini-Lang", "Jan S. Kirschke", "Stephanie E. Combs", "Bjoern Menze"], "title": "CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body Anatomy in Computed Tomography", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate delineation of anatomical structures in volumetric CT scans is\ncrucial for diagnosis and treatment planning. While AI has advanced automated\nsegmentation, current approaches typically target individual structures,\ncreating a fragmented landscape of incompatible models with varying performance\nand disparate evaluation protocols. Foundational segmentation models address\nthese limitations by providing a holistic anatomical view through a single\nmodel. Yet, robust clinical deployment demands comprehensive training data,\nwhich is lacking in existing whole-body approaches, both in terms of data\nheterogeneity and, more importantly, anatomical coverage. In this work, rather\nthan pursuing incremental optimizations in model architecture, we present CADS,\nan open-source framework that prioritizes the systematic integration,\nstandardization, and labeling of heterogeneous data sources for whole-body CT\nsegmentation. At its core is a large-scale dataset of 22,022 CT volumes with\ncomplete annotations for 167 anatomical structures, representing a significant\nadvancement in both scale and coverage, with 18 times more scans than existing\ncollections and 60% more distinct anatomical targets. Building on this diverse\ndataset, we develop the CADS-model using established architectures for\naccessible and automated full-body CT segmentation. Through comprehensive\nevaluation across 18 public datasets and an independent real-world hospital\ncohort, we demonstrate advantages over SoTA approaches. Notably, thorough\ntesting of the model's performance in segmentation tasks from radiation\noncology validates its direct utility for clinical interventions. By making our\nlarge-scale dataset, our segmentation models, and our clinical software tool\npublicly available, we aim to advance robust AI solutions in radiology and make\ncomprehensive anatomical analysis accessible to clinicians and researchers\nalike.", "AI": {"tldr": "CADS introduces an open-source framework for whole-body CT segmentation, leveraging a large-scale dataset and standardized data integration to outperform existing methods.", "motivation": "Current AI segmentation models are fragmented and lack comprehensive training data, hindering clinical deployment. CADS addresses this by integrating diverse data sources.", "method": "CADS uses a large-scale dataset of 22,022 CT volumes with 167 annotated structures, combined with established architectures for full-body segmentation.", "result": "The CADS-model outperforms state-of-the-art approaches in evaluations across 18 public datasets and a real-world hospital cohort, proving clinical utility.", "conclusion": "CADS advances robust AI solutions in radiology by providing open-access tools for comprehensive anatomical analysis."}}
{"id": "2508.00098", "pdf": "https://arxiv.org/pdf/2508.00098", "abs": "https://arxiv.org/abs/2508.00098", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicole", "Stefano Ghidoni", "Nassir Navab"], "title": "Stress-Aware Resilient Neural Training", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "This paper introduces Stress-Aware Learning, a resilient neural training\nparadigm in which deep neural networks dynamically adjust their optimization\nbehavior - whether under stable training regimes or in settings with uncertain\ndynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)\nDeformation, inspired by structural fatigue in materials science. To\ninstantiate this concept, we propose Plastic Deformation Optimizer, a\nstress-aware mechanism that injects adaptive noise into model parameters\nwhenever an internal stress signal - reflecting stagnation in training loss and\naccuracy - indicates persistent optimization difficulty. This enables the model\nto escape sharp minima and converge toward flatter, more generalizable regions\nof the loss landscape. Experiments across six architectures, four optimizers,\nand seven vision benchmarks demonstrate improved robustness and generalization\nwith minimal computational overhead. The code and 3D visuals will be available\non GitHub: https://github.com/Stress-Aware-Learning/SAL.", "AI": {"tldr": "Stress-Aware Learning (SAL) introduces a resilient neural training paradigm using adaptive noise to escape sharp minima, improving robustness and generalization.", "motivation": "Inspired by structural fatigue in materials science, SAL addresses optimization difficulties in deep neural networks under uncertain dynamics.", "method": "Proposes Plastic Deformation Optimizer, injecting adaptive noise based on internal stress signals to escape sharp minima.", "result": "Experiments show improved robustness and generalization across architectures, optimizers, and benchmarks with minimal overhead.", "conclusion": "SAL offers a practical, efficient approach to enhance neural network training resilience and performance."}}
{"id": "2508.00155", "pdf": "https://arxiv.org/pdf/2508.00155", "abs": "https://arxiv.org/abs/2508.00155", "authors": ["Tomasz Szczepa\u0144ski", "Szymon P\u0142otka", "Michal K. Grzeszczyk", "Arleta Adamowicz", "Piotr Fudalej", "Przemys\u0142aw Korzeniowski", "Tomasz Trzci\u0144ski", "Arkadiusz Sitek"], "title": "GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted for the 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2025", "summary": "Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains\nchallenging, especially for fine structures like root apices, which is critical\nfor assessing root resorption in orthodontics. We introduce GEPAR3D, a novel\napproach that unifies instance detection and multi-class segmentation into a\nsingle step tailored to improve root segmentation. Our method integrates a\nStatistical Shape Model of dentition as a geometric prior, capturing anatomical\ncontext and morphological consistency without enforcing restrictive adjacency\nconstraints. We leverage a deep watershed method, modeling each tooth as a\ncontinuous 3D energy basin encoding voxel distances to boundaries. This\ninstance-aware representation ensures accurate segmentation of narrow, complex\nroot apices. Trained on publicly available CBCT scans from a single center, our\nmethod is evaluated on external test sets from two in-house and two public\nmedical centers. GEPAR3D achieves the highest overall segmentation performance,\naveraging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the\nsecond-best method) and increasing recall to 95.2% (+9.5%) across all test\nsets. Qualitative analyses demonstrated substantial improvements in root\nsegmentation quality, indicating significant potential for more accurate root\nresorption assessment and enhanced clinical decision-making in orthodontics. We\nprovide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.", "AI": {"tldr": "GEPAR3D is a novel method for tooth segmentation in CBCT scans, combining instance detection and multi-class segmentation with a Statistical Shape Model and deep watershed method, achieving superior performance (DSC 95.0%).", "motivation": "Accurate tooth segmentation, especially for fine structures like root apices, is critical for assessing root resorption in orthodontics but remains challenging.", "method": "GEPAR3D integrates a Statistical Shape Model as a geometric prior and uses a deep watershed method to model teeth as 3D energy basins, ensuring precise segmentation.", "result": "GEPAR3D outperforms other methods with a DSC of 95.0% (+2.8%) and recall of 95.2% (+9.5%), showing significant improvements in root segmentation quality.", "conclusion": "GEPAR3D enhances root resorption assessment and clinical decision-making, with the implementation and dataset made publicly available."}}
{"id": "2508.00235", "pdf": "https://arxiv.org/pdf/2508.00235", "abs": "https://arxiv.org/abs/2508.00235", "authors": ["Erin Rainville", "Amirhossein Rasoulian", "Hassan Rivaz", "Yiming Xiao"], "title": "Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted to ICCV 2025 Workshop CVAMD", "summary": "Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels\nthat, if ruptured, can lead to life-threatening consequences. However, their\nsmall size and soft contrast in radiological scans often make it difficult to\nperform accurate and efficient detection and morphological analyses, which are\ncritical in the clinical care of the disorder. Furthermore, the lack of large\npublic datasets with voxel-wise expert annotations pose challenges for\ndeveloping deep learning algorithms to address the issues. Therefore, we\nproposed a novel weakly supervised 3D multi-task UNet that integrates\nvesselness priors to jointly perform aneurysm detection and segmentation in\ntime-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA\ndetection and segmentation, we employ the popular Frangi's vesselness filter to\nderive soft cerebrovascular priors for both network input and an attention\nblock to conduct segmentation from the decoder and detection from an auxiliary\nbranch. We train our model on the Lausanne dataset with coarse ground truth\nsegmentation, and evaluate it on the test set with refined labels from the same\ndatabase. To further assess our model's generalizability, we also validate it\nexternally on the ADAM dataset. Our results demonstrate the superior\nperformance of the proposed technique over the SOTA techniques for aneurysm\nsegmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =\n1.47, sensitivity = 92.9%).", "AI": {"tldr": "A weakly supervised 3D multi-task UNet with vesselness priors is proposed for intracranial aneurysm detection and segmentation in TOF-MRA, outperforming state-of-the-art methods.", "motivation": "Intracranial aneurysms (IAs) are hard to detect and analyze due to small size and low contrast in scans, and the lack of large annotated datasets for deep learning.", "method": "A novel 3D multi-task UNet integrates Frangi's vesselness filter for cerebrovascular priors, enabling joint detection and segmentation with weak supervision.", "result": "Achieves superior performance: Dice = 0.614, 95%HD =1.38mm for segmentation; false positive rate = 1.47, sensitivity = 92.9% for detection.", "conclusion": "The proposed method effectively addresses IA detection and segmentation challenges, demonstrating strong performance and generalizability."}}
{"id": "2508.00250", "pdf": "https://arxiv.org/pdf/2508.00250", "abs": "https://arxiv.org/abs/2508.00250", "authors": ["Victor D. Martinez", "Vidya Manian", "Sudhir Malik"], "title": "Jet Image Generation in High Energy Physics Using Diffusion Models", "categories": ["hep-ph", "cs.AI", "cs.CV", "cs.LG"], "comment": "The paper is under review at IEEE Transactions in Nuclear Science", "summary": "This article presents, for the first time, the application of diffusion\nmodels for generating jet images corresponding to proton-proton collision\nevents at the Large Hadron Collider (LHC). The kinematic variables of quark,\ngluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset\nare mapped to two-dimensional image representations. Diffusion models are\ntrained on these images to learn the spatial distribution of jet constituents.\nWe compare the performance of score-based diffusion models and consistency\nmodels in accurately generating class-conditional jet images. Unlike approaches\nbased on latent distributions, our method operates directly in image space. The\nfidelity of the generated images is evaluated using several metrics, including\nthe Fr\\'echet Inception Distance (FID), which demonstrates that consistency\nmodels achieve higher fidelity and generation stability compared to score-based\ndiffusion models. These advancements offer significant improvements in\ncomputational efficiency and generation accuracy, providing valuable tools for\nHigh Energy Physics (HEP) research.", "AI": {"tldr": "The paper introduces diffusion models for generating jet images from proton-proton collisions at the LHC, comparing score-based and consistency models, with the latter showing superior fidelity and stability.", "motivation": "To improve the generation of class-conditional jet images for HEP research by leveraging diffusion models directly in image space.", "method": "Mapping kinematic variables of jets to 2D images, training diffusion models (score-based and consistency) on these images, and evaluating fidelity using metrics like FID.", "result": "Consistency models outperform score-based diffusion models in fidelity and generation stability.", "conclusion": "The method enhances computational efficiency and accuracy, offering valuable tools for HEP research."}}
{"id": "2508.00288", "pdf": "https://arxiv.org/pdf/2508.00288", "abs": "https://arxiv.org/abs/2508.00288", "authors": ["Jianqiang Xiao", "Yuexuan Sun", "Yixin Shao", "Boxi Gan", "Rongqiang Liu", "Yanjing Wu", "Weili Gua", "Xiang Deng"], "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to ACM MM Dataset Track 2025", "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.", "AI": {"tldr": "UAV-ON is a benchmark for aerial Object Goal Navigation (ObjectNav) in open-world environments, addressing limitations of Vision-and-Language Navigation (VLN) by using high-level semantic goals.", "motivation": "Existing VLN paradigms rely on sequential linguistic instructions, limiting scalability and autonomy. UAV-ON aims to enable aerial agents to navigate based on semantic goals in unstructured environments.", "method": "UAV-ON includes 14 high-fidelity environments with diverse semantic regions and 1270 annotated target objects. It introduces instance-level instructions for grounded reasoning and evaluates baselines like Aerial ObjectNav Agent (AOA).", "result": "Baseline methods, including AOA, struggle with the challenges of aerial navigation and semantic goal grounding, highlighting the complexity of the task.", "conclusion": "UAV-ON advances research on scalable UAV autonomy by focusing on semantic goal-driven navigation in complex real-world environments."}}
{"id": "2508.00354", "pdf": "https://arxiv.org/pdf/2508.00354", "abs": "https://arxiv.org/abs/2508.00354", "authors": ["Tianshuang Qiu", "Zehan Ma", "Karim El-Refai", "Hiya Shah", "Chung Min Kim", "Justin Kerr", "Ken Goldberg"], "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/", "AI": {"tldr": "Omni-Scan is a bi-manual robot pipeline for creating high-quality 3D Gaussian Splat models by rotating objects with grippers, using advanced vision models to isolate objects and train omni-directional models. It achieves 83% accuracy in defect inspection.", "motivation": "Traditional 3D object scanning methods are limited by workspace constraints. Omni-Scan aims to overcome this by enabling flexible, high-quality 3D modeling using a robot setup.", "method": "The pipeline uses a bi-manual robot to grasp and rotate objects, leveraging DepthAnything, Segment Anything, and RAFT optical flow to isolate objects. The 3DGS training pipeline is modified to handle gripper occlusion for 360-degree modeling.", "result": "Omni-Scan successfully identifies visual or geometric defects in 12 objects with 83% average accuracy.", "conclusion": "Omni-Scan provides a scalable and accurate solution for 3D object modeling and defect inspection, overcoming workspace limitations of traditional methods."}}
{"id": "2508.00378", "pdf": "https://arxiv.org/pdf/2508.00378", "abs": "https://arxiv.org/abs/2508.00378", "authors": ["Shixin Yi", "Lin Shang"], "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "categories": ["cs.AI", "cs.CV"], "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "AI": {"tldr": "CoRGI introduces visual verification into Chain-of-Thought prompting for VLMs, improving reasoning by grounding explanations in visual evidence.", "motivation": "Address hallucinations in CoT prompting by ensuring visual grounding during reasoning.", "method": "Three-stage pipeline: generate textual reasoning, extract visual evidence (VEVM), synthesize grounded answer.", "result": "Improves reasoning on VCR benchmark with Qwen-2.5VL and LLaVA-1.6; more factual explanations.", "conclusion": "Grounding intermediate reasoning in visual evidence enhances robustness in multimodal reasoning."}}
{"id": "2508.00387", "pdf": "https://arxiv.org/pdf/2508.00387", "abs": "https://arxiv.org/abs/2508.00387", "authors": ["Zeqi Zheng", "Zizheng Zhu", "Yingchao Yu", "Yanchen Huang", "Changze Lv", "Junfeng Tang", "Zhaofei Yu", "Yaochu Jin"], "title": "STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers", "categories": ["cs.NE", "cs.CV"], "comment": "32 pages, 4 figures", "summary": "Transformer-based Spiking Neural Networks (SNNs) suffer from a great\nperformance gap compared to floating-point Artificial Neural Networks (ANNs)\ndue to the binary nature of spike trains. Recent efforts have introduced\ndeep-level feedback loops to transmit high-level semantic information to narrow\nthis gap. However, these designs often span multiple deep layers, resulting in\ncostly feature transformations, higher parameter overhead, increased energy\nconsumption, and longer inference latency. To address this issue, we propose\nShallow-level Temporal Feedback (STF), a lightweight plug-and-play module for\nthe encoding layer, which consists of Temporal-Spatial Position Embedding\n(TSPE) and Temporal Feedback (TF).Extensive experiments show that STF\nconsistently improves performance across various Transformer-based SNN\nbackbones on static datasets, including CIFAR-10, CIFAR-100, and ImageNet-1K,\nunder different spike timestep settings. Further analysis reveals that STF\nenhances the diversity of the spike patterns, which is key to performance gain.\nMoreover, evaluations on adversarial robustness and temporal sensitivity\nconfirm that STF outperforms direct coding and its variants, highlighting its\npotential as a new spike encoding scheme for static scenarios. Our code will be\nreleased upon acceptance.", "AI": {"tldr": "The paper proposes Shallow-level Temporal Feedback (STF) to improve Transformer-based Spiking Neural Networks (SNNs) by addressing performance gaps caused by binary spike trains, reducing costs and latency.", "motivation": "The performance gap between SNNs and ANNs due to binary spike trains, and the inefficiency of deep-level feedback loops in existing designs.", "method": "Introduces STF, a lightweight module with Temporal-Spatial Position Embedding (TSPE) and Temporal Feedback (TF) for the encoding layer.", "result": "STF improves performance on static datasets (CIFAR-10, CIFAR-100, ImageNet-1K) and enhances spike pattern diversity. It also shows better adversarial robustness and temporal sensitivity.", "conclusion": "STF is a promising spike encoding scheme for static scenarios, offering efficiency and performance gains."}}
{"id": "2508.00398", "pdf": "https://arxiv.org/pdf/2508.00398", "abs": "https://arxiv.org/abs/2508.00398", "authors": ["Sunjae Yoon", "Gwanhyeong Koo", "Younghwan Lee", "Ji Woo Hong", "Chang D. Yoo"], "title": "Occlusion-robust Stylization for Drawing-based 3D Animation", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 13 figures, ICCV 2025", "summary": "3D animation aims to generate a 3D animated video from an input image and a\ntarget 3D motion sequence. Recent advances in image-to-3D models enable the\ncreation of animations directly from user-hand drawings. Distinguished from\nconventional 3D animation, drawing-based 3D animation is crucial to preserve\nartist's unique style properties, such as rough contours and distinct stroke\npatterns. However, recent methods still exhibit quality deterioration in style\nproperties, especially under occlusions caused by overlapping body parts,\nleading to contour flickering and stroke blurring. This occurs due to a\n`stylization pose gap' between training and inference in stylization networks\ndesigned to preserve drawing styles in drawing-based 3D animation systems. The\nstylization pose gap denotes that input target poses used to train the\nstylization network are always in occlusion-free poses, while target poses\nencountered in an inference include diverse occlusions under dynamic motions.\nTo this end, we propose Occlusion-robust Stylization Framework (OSF) for\ndrawing-based 3D animation. We found that while employing object's edge can be\neffective input prior for guiding stylization, it becomes notably inaccurate\nwhen occlusions occur at inference. Thus, our proposed OSF provides\nocclusion-robust edge guidance for stylization network using optical flow,\nensuring a consistent stylization even under occlusions. Furthermore, OSF\noperates in a single run instead of the previous two-stage method, achieving\n2.4x faster inference and 2.1x less memory.", "AI": {"tldr": "The paper introduces OSF, an occlusion-robust framework for drawing-based 3D animation, addressing style deterioration under occlusions and improving speed and memory efficiency.", "motivation": "Preserving an artist's unique style in drawing-based 3D animation is challenging due to occlusions, causing flickering and blurring. The 'stylization pose gap' worsens this issue.", "method": "OSF uses optical flow for occlusion-robust edge guidance, replacing inaccurate edge inputs. It operates in a single run, unlike previous two-stage methods.", "result": "OSF ensures consistent stylization under occlusions, achieves 2.4x faster inference, and uses 2.1x less memory.", "conclusion": "OSF effectively addresses the stylization pose gap, improving quality and efficiency in drawing-based 3D animation."}}
{"id": "2508.00438", "pdf": "https://arxiv.org/pdf/2508.00438", "abs": "https://arxiv.org/abs/2508.00438", "authors": ["Sumin Seo", "In Kyu Lee", "Hyun-Woo Kim", "Jaesik Min", "Chung-Hwan Jung"], "title": "Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at MICCAI 2025. Dataset available at\n  https://github.com/medipixel/DiGDA", "summary": "Coronary stenosis is a major risk factor for ischemic heart events leading to\nincreased mortality, and medical treatments for this condition require\nmeticulous, labor-intensive analysis. Coronary angiography provides critical\nvisual cues for assessing stenosis, supporting clinicians in making informed\ndecisions for diagnosis and treatment. Recent advances in deep learning have\nshown great potential for automated localization and severity measurement of\nstenosis. In real-world scenarios, however, the success of these competent\napproaches is often hindered by challenges such as limited labeled data and\nclass imbalance. In this study, we propose a novel data augmentation approach\nthat uses an inpainting method based on a diffusion model to generate realistic\nlesions, allowing user-guided control of severity. Extensive evaluation on\nlesion detection and severity classification across various synthetic dataset\nsizes shows superior performance of our method on both a large-scale in-house\ndataset and a public coronary angiography dataset. Furthermore, our approach\nmaintains high detection and classification performance even when trained with\nlimited data, highlighting its clinical importance in improving the assessment\nof severity of stenosis and optimizing data utilization for more reliable\ndecision support.", "AI": {"tldr": "A novel data augmentation method using diffusion models for generating realistic coronary lesions improves stenosis detection and severity classification, especially with limited labeled data.", "motivation": "Coronary stenosis is a critical condition requiring precise analysis, but deep learning approaches face challenges like limited labeled data and class imbalance.", "method": "Proposes a data augmentation approach using a diffusion model for inpainting realistic lesions with user-controlled severity.", "result": "Outperforms other methods in lesion detection and severity classification, even with limited data, on both in-house and public datasets.", "conclusion": "The method enhances stenosis assessment and optimizes data use, offering reliable decision support in clinical settings."}}
{"id": "2508.00531", "pdf": "https://arxiv.org/pdf/2508.00531", "abs": "https://arxiv.org/abs/2508.00531", "authors": ["Jack A. Kilgallen", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "title": "The Repeated-Stimulus Confound in Electroencephalography", "categories": ["q-bio.NC", "cs.CV", "62K99, 68T05"], "comment": "15 pages, 6 figures, 8 tables, in submission to IEEE", "summary": "In neural-decoding studies, recordings of participants' responses to stimuli\nare used to train models. In recent years, there has been an explosion of\npublications detailing applications of innovations from deep-learning research\nto neural-decoding studies. The data-hungry models used in these experiments\nhave resulted in a demand for increasingly large datasets. Consequently, in\nsome studies, the same stimuli are presented multiple times to each participant\nto increase the number of trials available for use in model training. However,\nwhen a decoding model is trained and subsequently evaluated on responses to the\nsame stimuli, stimulus identity becomes a confounder for accuracy. We term this\nthe repeated-stimulus confound. We identify a susceptible dataset, and 16\npublications which report model performance based on evaluation procedures\naffected by the confound. We conducted experiments using models from the\naffected studies to investigate the likely extent to which results in the\nliterature have been misreported. Our findings suggest that the decoding\naccuracies of these models were overestimated by between 4.46-7.42%. Our\nanalysis also indicates that per 1% increase in accuracy under the confound,\nthe magnitude of the overestimation increases by 0.26%. The confound not only\nresults in optimistic estimates of decoding performance, but undermines the\nvalidity of several claims made within the affected publications. We conducted\nfurther experiments to investigate the implications of the confound in\nalternative contexts. We found that the same methodology used within the\naffected studies could also be used to justify an array of pseudoscientific\nclaims, such as the existence of extrasensory perception.", "AI": {"tldr": "The paper identifies a 'repeated-stimulus confound' in neural-decoding studies, where models trained and evaluated on the same stimuli overestimate accuracy by 4.46-7.42%, undermining validity.", "motivation": "To address the issue of inflated decoding accuracies in neural-decoding studies due to repeated stimuli, which confounds model evaluation.", "method": "Analyzed susceptible datasets and 16 affected publications, conducted experiments to quantify overestimation, and tested the confound's implications in pseudoscientific contexts.", "result": "Decoding accuracies were overestimated by 4.46-7.42%, with a 0.26% increase in overestimation per 1% accuracy under the confound. The confound also validated pseudoscientific claims.", "conclusion": "The repeated-stimulus confound significantly biases neural-decoding results, necessitating methodological revisions to ensure accurate model evaluation."}}
{"id": "2508.00697", "pdf": "https://arxiv.org/pdf/2508.00697", "abs": "https://arxiv.org/abs/2508.00697", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "ICCV 2025", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.", "AI": {"tldr": "LightDP accelerates Diffusion Policies for mobile devices via network compression and reduced sampling steps, achieving real-time performance without sacrificing accuracy.", "motivation": "Diffusion Policies are computationally inefficient for mobile platforms, limiting their practical deployment.", "method": "LightDP uses network compression (pruning and retraining) and consistency distillation to reduce latency and sampling steps.", "result": "LightDP achieves real-time performance on mobile devices with competitive accuracy across standard datasets.", "conclusion": "LightDP enables practical deployment of diffusion-based policies in resource-limited environments."}}
{"id": "2508.00721", "pdf": "https://arxiv.org/pdf/2508.00721", "abs": "https://arxiv.org/abs/2508.00721", "authors": ["Yuxiang Wan", "Ryan Devera", "Wenjie Zhang", "Ju Sun"], "title": "FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "comment": null, "summary": "We present FMPlug, a novel plug-in framework that enhances foundation\nflow-matching (FM) priors for solving ill-posed inverse problems. Unlike\ntraditional approaches that rely on domain-specific or untrained priors, FMPlug\nsmartly leverages two simple but powerful insights: the similarity between\nobserved and desired objects and the Gaussianity of generative flows. By\nintroducing a time-adaptive warm-up strategy and sharp Gaussianity\nregularization, FMPlug unlocks the true potential of domain-agnostic foundation\nmodels. Our method beats state-of-the-art methods that use foundation FM priors\nby significant margins, on image super-resolution and Gaussian deblurring.", "AI": {"tldr": "FMPlug is a plug-in framework enhancing flow-matching priors for inverse problems, outperforming state-of-the-art methods in image super-resolution and Gaussian deblurring.", "motivation": "Traditional methods rely on domain-specific or untrained priors, limiting their effectiveness. FMPlug aims to leverage foundation models more efficiently.", "method": "FMPlug uses a time-adaptive warm-up strategy and sharp Gaussianity regularization, capitalizing on object similarity and Gaussianity of generative flows.", "result": "FMPlug significantly outperforms state-of-the-art methods in image super-resolution and Gaussian deblurring.", "conclusion": "FMPlug demonstrates the potential of domain-agnostic foundation models for solving ill-posed inverse problems."}}
{"id": "2508.00733", "pdf": "https://arxiv.org/pdf/2508.00733", "abs": "https://arxiv.org/abs/2508.00733", "authors": ["Le Wang", "Jun Wang", "Feng Deng", "Chen Zhang", "Kun Gai", "Di Zhang"], "title": "AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "comment": "12 pages, 2 figures", "summary": "We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality.", "AI": {"tldr": "AudioGen-Omni is a unified multimodal diffusion transformer model for generating high-fidelity audio, speech, and songs synchronized with video, achieving state-of-the-art performance and efficiency.", "motivation": "To create a versatile model capable of generating diverse, semantically rich audio synchronized with video, overcoming limitations of text-frozen paradigms.", "method": "Uses a joint training paradigm with multimodal diffusion transformers (MMDit), a lyrics-transcription encoder, and AdaLN-based joint attention with PAAPI for cross-modal alignment.", "result": "Achieves state-of-the-art results in Text-to-Audio/Speech/Song tasks with high semantic alignment, lip-sync accuracy, and fast inference (1.91s for 8s audio).", "conclusion": "AudioGen-Omni is a robust, efficient, and generalizable solution for multimodal audio generation tasks."}}
{"id": "2508.00755", "pdf": "https://arxiv.org/pdf/2508.00755", "abs": "https://arxiv.org/abs/2508.00755", "authors": ["Peng Hu", "Wenxuan Zhang"], "title": "AI-Driven Collaborative Satellite Object Detection for Space Sustainability", "categories": ["eess.IV", "cs.CV"], "comment": "Submitted to the 13th Annual IEEE International Conference on\n  Wireless for Space and Extreme Environments (WiSEE 2025)", "summary": "The growing density of satellites in low-Earth orbit (LEO) presents serious\nchallenges to space sustainability, primarily due to the increased risk of\nin-orbit collisions. Traditional ground-based tracking systems are constrained\nby latency and coverage limitations, underscoring the need for onboard,\nvision-based space object detection (SOD) capabilities. In this paper, we\npropose a novel satellite clustering framework that enables the collaborative\nexecution of deep learning (DL)-based SOD tasks across multiple satellites. To\nsupport this approach, we construct a high-fidelity dataset simulating imaging\nscenarios for clustered satellite formations. A distance-aware viewpoint\nselection strategy is introduced to optimize detection performance, and recent\nDL models are used for evaluation. Experimental results show that the\nclustering-based method achieves competitive detection accuracy compared to\nsingle-satellite and existing approaches, while maintaining a low size, weight,\nand power (SWaP) footprint. These findings underscore the potential of\ndistributed, AI-enabled in-orbit systems to enhance space situational awareness\nand contribute to long-term space sustainability.", "AI": {"tldr": "A novel satellite clustering framework for collaborative deep learning-based space object detection (SOD) is proposed, improving accuracy and efficiency while maintaining low SWaP.", "motivation": "Addressing the challenges of space sustainability due to increasing satellite density in LEO, traditional ground-based tracking limitations, and the need for onboard vision-based SOD.", "method": "Developed a satellite clustering framework with a high-fidelity dataset, distance-aware viewpoint selection, and evaluated using DL models.", "result": "The clustering-based method achieves competitive detection accuracy with low SWaP, outperforming single-satellite and existing approaches.", "conclusion": "Distributed, AI-enabled in-orbit systems can enhance space situational awareness and contribute to long-term space sustainability."}}
{"id": "2508.00782", "pdf": "https://arxiv.org/pdf/2508.00782", "abs": "https://arxiv.org/abs/2508.00782", "authors": ["Kien T. Pham", "Yingqing He", "Yazhou Xing", "Qifeng Chen", "Long Chen"], "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "The 33rd ACM Multimedia Conference (MM '25)", "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.", "AI": {"tldr": "SpA2V is a novel framework for audio-driven video generation that leverages spatial auditory cues to create videos with accurate semantic and spatial alignment, outperforming prior methods.", "motivation": "Existing audio-driven video generation methods focus mainly on semantic information, neglecting spatial attributes like sound source locations and movements, which humans naturally perceive. SpA2V addresses this gap by incorporating spatial auditory cues.", "method": "SpA2V uses a two-stage process: 1) Audio-guided Video Planning, where spatial and semantic cues from audio are used to create Video Scene Layouts (VSLs), and 2) Layout-grounded Video Generation, integrating VSLs into pre-trained diffusion models for training-free video synthesis.", "result": "Experiments show SpA2V generates realistic videos with strong semantic and spatial alignment to input audios.", "conclusion": "SpA2V advances audio-driven video generation by effectively utilizing spatial auditory cues, bridging the gap between audio and video modalities."}}

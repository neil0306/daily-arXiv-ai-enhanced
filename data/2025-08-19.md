<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 81]
- [cs.CV](#cs.CV) [Total: 156]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.SI](#cs.SI) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
*Maksym Shamrai,Vladyslav Hamolia*

Main category: cs.CL

TL;DR: A framework using LLM weight activations to create language metric space, automatically generating vector representations that capture linguistic characteristics and reveal language relationships.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches rely on hand-crafted linguistic features, which can be limited. The paper aims to automatically derive language representations using internal LLM mechanisms to better capture intrinsic language characteristics.

Method: Uses adapted pruning algorithm to compute weight importance scores from LLM internal activations, creating high-dimensional vector representations for languages. Validated across 106 languages using diverse datasets and multilingual LLMs.

Result: The method successfully captures linguistic phenomena and aligns with established language families while revealing unexpected inter-language connections that may indicate historical contact or language evolution.

Conclusion: The framework provides an automated, data-driven approach to language representation that goes beyond traditional methods, offering insights into language relationships and evolution through LLM internal mechanisms.

Abstract: We introduce a novel framework that utilizes the internal weight activations
of modern Large Language Models (LLMs) to construct a metric space of
languages. Unlike traditional approaches based on hand-crafted linguistic
features, our method automatically derives high-dimensional vector
representations by computing weight importance scores via an adapted pruning
algorithm. Our approach captures intrinsic language characteristics that
reflect linguistic phenomena. We validate our approach across diverse datasets
and multilingual LLMs, covering 106 languages. The results align well with
established linguistic families while also revealing unexpected inter-language
connections that may indicate historical contact or language evolution. The
source code, computed language latent vectors, and visualization tool are made
publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [2] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
*Jonas van Elburg,Peter van der Putten,Maarten Marx*

Main category: cs.CL

TL;DR: Synthetic QA data from LLMs can reliably rank RAG systems by retriever configuration but fails to consistently rank generator architectures due to task mismatch and stylistic bias.


<details>
  <summary>Details</summary>
Motivation: To determine if synthetic question-answer data generated by LLMs can effectively substitute for human-labeled benchmarks when such data is unavailable, particularly for evaluating Retrieval-Augmented Generation (RAG) systems.

Method: Conducted two experiments: 1) varying retriever parameters while keeping generator fixed, 2) varying generator architectures while keeping retriever parameters fixed. Tested across four datasets (two open-domain, two proprietary).

Result: Synthetic benchmarks reliably ranked RAG systems based on retriever configuration, aligning well with human-labeled benchmarks. However, they failed to produce consistent rankings when comparing different generator architectures.

Conclusion: While synthetic QA data can serve as a proxy for human benchmarks for evaluating retriever components, it is unreliable for comparing generator architectures due to task mismatch and stylistic biases that favor certain generators.

Abstract: We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

</details>


### [3] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
*Noah Kasmanoff,Rahul Zalkikar*

Main category: cs.CL

TL;DR: Applying imitation learning to conversation tasks to create dialog policies and discriminators that can identify limitations in dialog models.


<details>
  <summary>Details</summary>
Motivation: To leverage imitation learning for creating conversation policies without explicit rewards, using expert demonstrations to develop systems that can engage in dialog and detect synthetic vs. expert conversations.

Method: Using imitation learning techniques to train both a conversation policy that responds to user prompts and a discriminator that classifies between expert and synthetic dialog.

Result: Successfully created an effective conversation policy, but the discriminator revealed limitations in dialog models, showing this approach can identify adverse behaviors in dialog-oriented models.

Conclusion: Imitation learning is effective for dialog policy creation, and the discriminator component provides valuable insights into model limitations, making this technique useful for identifying problematic behaviors in dialog systems.

Abstract: Imitation learning is a proven method for creating a policy in the absence of
rewards, by leveraging expert demonstrations. In this work, we apply imitation
learning to conversation. In doing so, we recover a policy capable of talking
to a user given a prompt (input state), and a discriminator capable of
classifying between expert and synthetic conversation. While our policy is
effective, we recover results from our discriminator that indicate the
limitations of dialog models. We argue that this technique can be used to
identify adverse behavior of arbitrary data models common for dialog oriented
tasks.

</details>


### [4] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
*Leo Peckham,Michael Ong,Naomi Nagy,Ewan Dunbar*

Main category: cs.CL

TL;DR: Transcription inconsistencies in Faetar ASR benchmark are not the main challenge; lexicon-constrained decoding helps but task remains very difficult.


<details>
  <summary>Details</summary>
Motivation: To examine the impact of transcription inconsistencies in the challenging low-resource Faetar Automatic Speech Recognition benchmark.

Method: Used a hand-constructed lexicon to analyze transcription inconsistencies and tested different approaches including bigram word-based language modeling and lexicon-constrained decoding.

Result: Found that transcription inconsistencies exist but are not the primary challenge; bigram language modeling provided no benefit, but lexicon-constrained decoding showed some improvement.

Conclusion: The Faetar ASR task remains extremely difficult despite addressing transcription issues, with lexicon constraints being the only helpful approach among those tested.

Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic
Speech Recognition benchmark, a challenging low-resource ASR benchmark. With
the help of a small, hand-constructed lexicon, we conclude that find that,
while inconsistencies do exist in the transcriptions, they are not the main
challenge in the task. We also demonstrate that bigram word-based language
modelling is of no added benefit, but that constraining decoding to a finite
lexicon can be beneficial. The task remains extremely difficult.

</details>


### [5] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
*Tianyi Li,Yu Qin,Olivia R. Liu Sheng*

Main category: cs.CL

TL;DR: LLMs show compromised performance in academic peer review tasks - acceptable for summarization but poor for grading, ranking, and providing meaningful research insights.


<details>
  <summary>Details</summary>
Motivation: To evaluate how effectively large language models can assist in academic peer review and scientific text processing, addressing the ongoing debate about their practical application potential in scholarly works.

Method: A four-task workflow assessing LLMs' capabilities: content reproduction (oracle role), comparison (judgmental arbiter), scoring (knowledgeable arbiter), and reflection (collaborator). Used Google's Gemini with detailed prompts on Information Systems articles from top journals, employing multiple evaluation metrics.

Result: Gemini showed acceptably reliable summarization and paraphrasing, but performed poorly in text ranking (faintly scalable), grading (poor discrimination), and qualitative reflection (self-consistent but not insightful). Performance was consistently compromised across linguistic, ground truth, and human evaluations.

Conclusion: LLMs should not be used unchecked for peer review construction due to their limited capabilities in tasks requiring deeper intellectual understanding of scientific texts.

Abstract: How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

</details>


### [6] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: Two-stage LLM framework for scientific text simplification using plan-driven sentence simplification and summary-guided document simplification


<details>
  <summary>Details</summary>
Motivation: To address both sentence-level and document-level scientific text simplification while maintaining coherence and contextual faithfulness

Method: Uses LLMs to generate structured plans for sentence simplification and concise summaries for document simplification, then performs plan-driven and summary-guided simplification

Result: Enables more coherent and contextually faithful simplifications of scientific text

Conclusion: The two-stage LLM-based framework effectively handles both sentence and document level scientific text simplification with improved coherence and faithfulness

Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,
which addresses both sentence-level and document-level scientific text
simplification. For sentence-level simplification, our methodology employs
large language models (LLMs) to first generate a structured plan, followed by
plan-driven simplification of individual sentences. At the document level, we
leverage LLMs to produce concise summaries and subsequently guide the
simplification process using these summaries. This two-stage, LLM-based
framework enables more coherent and contextually faithful simplifications of
scientific text.

</details>


### [7] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: Ensemble framework combining BERT classifier, semantic similarity, NLI model, and LLM reasoning for detecting creative generation and information distortion in scientific text simplification, with LLM-based post-editing for grounded generation.


<details>
  <summary>Details</summary>
Motivation: To develop robust methods for detecting and evaluating creative generation and information distortion in scientific text simplification tasks, addressing the CLEF 2025 SimpleText Task 2 challenge.

Method: Constructed ensemble framework integrating multiple strategies: BERT-based classifier, semantic similarity measures, natural language inference model, and LLM reasoning. Combined these signals using meta-classifiers. Employed LLM-based post-editing system for grounded generation that revises simplifications based on original input texts.

Result: The paper describes a methodology but does not provide specific quantitative results or performance metrics in the abstract.

Conclusion: The proposed ensemble approach with diverse detection signals and LLM-based post-editing provides a comprehensive solution for detecting information distortion and ensuring grounded generation in scientific text simplification.

Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task
2, which focuses on detecting and evaluating creative generation and
information distortion in scientific text simplification. Our solution
integrates multiple strategies: we construct an ensemble framework that
leverages BERT-based classifier, semantic similarity measure, natural language
inference model, and large language model (LLM) reasoning. These diverse
signals are combined using meta-classifiers to enhance the robustness of
spurious and distortion detection. Additionally, for grounded generation, we
employ an LLM-based post-editing system that revises simplifications based on
the original input texts.

</details>


### [8] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
*Michael Flor,Xinyi Liu,Anna Feldman*

Main category: cs.CL

TL;DR: Survey of 53 idiom datasets from psycholinguistics and computational linguistics, analyzing their content, annotation practices, and task applications, finding a disconnect between the two research fields.


<details>
  <summary>Details</summary>
Motivation: Idioms present computational and experimental challenges due to their non-compositional nature, requiring specialized datasets for both human studies and computational processing.

Method: Comprehensive review and analysis of 53 existing idiom datasets, examining their content, annotation dimensions (familiarity, transparency, compositionality), and supported tasks across both psycholinguistic and computational domains.

Result: Identified trends in annotation practices and task diversity, with recent expansions in language coverage, but found no established relationship between psycholinguistic and computational idiom research despite overlapping goals.

Conclusion: While idiom dataset development has advanced in both fields independently, there remains a significant gap between psycholinguistic and computational approaches that needs bridging for more comprehensive idiom understanding and processing.

Abstract: Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

</details>


### [9] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
*Leigh Levinson,Christopher J. Agostino*

Main category: cs.CL

TL;DR: AI systems embed simulated menstrual and circadian cycles to address the frame problem, showing performance variations aligned with biological rhythms and revealing societal biases in language models.


<details>
  <summary>Details</summary>
Motivation: Address the frame problem in AI systems by leveraging biological rhythms as natural relevance filters, inspired by how hormonal cycles help biological systems determine contextual relevance.

Method: Develop a framework that embeds simulated menstrual and circadian cycles into Large Language Models through system prompts generated from periodic functions modeling key hormones (estrogen, testosterone, cortisol).

Result: Linguistic analysis reveals emotional/stylistic variations tracking biological phases; benchmark tests show subtle but consistent performance variations aligned with biological expectations, with optimal function in moderate hormonal ranges.

Conclusion: The methodology provides a novel approach to contextual AI while revealing how societal biases regarding gender and biology are embedded within language models.

Abstract: Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

</details>


### [10] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
*Julia Sammartino,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: Cross-lingual sequential fine-tuning improves euphemism detection, especially for low-resource languages like Yoruba and Turkish, with XLM-R showing larger gains but mBERT providing more stability.


<details>
  <summary>Details</summary>
Motivation: Euphemisms are culturally variable and ambiguous, posing challenges for language models in low-resource settings, requiring investigation of effective cross-lingual transfer methods.

Method: Compare sequential fine-tuning with monolingual and simultaneous fine-tuning using XLM-R and mBERT across five languages, analyzing performance based on language pairings, typological features, and pretraining coverage.

Result: Sequential fine-tuning with high-resource L1 improves L2 performance, especially for Yoruba and Turkish. XLM-R achieves larger gains but is sensitive to pretraining gaps and catastrophic forgetting, while mBERT yields more stable though lower results.

Conclusion: Sequential fine-tuning is a simple yet effective strategy for improving euphemism detection in multilingual models, particularly beneficial for low-resource languages.

Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for
language models, especially in low-resource settings. This paper investigates
how cross-lingual transfer via sequential fine-tuning affects euphemism
detection across five languages: English, Spanish, Chinese, Turkish, and
Yoruba. We compare sequential fine-tuning with monolingual and simultaneous
fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by
language pairings, typological features, and pretraining coverage. Results show
that sequential fine-tuning with a high-resource L1 improves L2 performance,
especially for low-resource languages like Yoruba and Turkish. XLM-R achieves
larger gains but is more sensitive to pretraining gaps and catastrophic
forgetting, while mBERT yields more stable, though lower, results. These
findings highlight sequential fine-tuning as a simple yet effective strategy
for improving euphemism detection in multilingual models, particularly when
low-resource languages are involved.

</details>


### [11] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
*Andrei-Valentin TÄƒnase,Elena Pelican*

Main category: cs.CL

TL;DR: SupraTok is a novel tokenization architecture that improves tokenization efficiency by 30-31% over leading tokenizers while maintaining competitive performance across 38 languages and boosting benchmark performance when integrated with language models.


<details>
  <summary>Details</summary>
Motivation: Tokenization remains a fundamental bottleneck in NLP with static strategies despite progress in model architectures, creating a need for more efficient and semantically-aware tokenization methods.

Method: SupraTok extends Byte-Pair Encoding with three innovations: cross-boundary pattern learning for multi-word semantic units, entropy-driven data curation for optimal training corpus quality, and multi-phase curriculum learning for stable convergence.

Result: Achieves 31% improvement in English tokenization efficiency (5.91 vs 4.51 chars/token) over OpenAI's o200k and 30% over Google's Gemma 3 tokenizer. When integrated with GPT-2 scale model, yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks.

Conclusion: Efficient tokenization can complement architectural innovations as a path to improved language model performance, though further validation at larger model scales is needed.

Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural
language processing, with strategies largely static despite remarkable progress
in model architectures. We present SupraTok, a novel tokenization architecture
that reimagines subword segmentation through three innovations: cross-boundary
pattern learning that discovers multi-word semantic units, entropy-driven data
curation that optimizes training corpus quality, and multi-phase curriculum
learning for stable convergence. Our approach extends Byte-Pair Encoding by
learning "superword" tokens, coherent multi-word expressions that preserve
semantic unity while maximizing compression efficiency. SupraTok achieves 31%
improvement in English tokenization efficiency (5.91 versus 4.51 characters per
token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's
Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance
across 38 languages. When integrated with a GPT-2 scale model (124M parameters)
trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%
improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural
modifications. While these results are promising at this scale, further
validation at larger model scales is needed. These findings suggest that
efficient tokenization can complement architectural innovations as a path to
improved language model performance.

</details>


### [12] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
*Hui Ma,Bo Zhang,Jinpeng Hu,Zenglin Shi*

Main category: cs.CL

TL;DR: InitERC is a one-stage in-context instruction tuning framework for emotion recognition in conversation that jointly captures speaker characteristics and contextual cues through unified learning.


<details>
  <summary>Details</summary>
Motivation: Existing multi-stage instruction tuning methods for emotion recognition in conversation fail to capture the dynamic interaction between speaker characteristics and conversational context, resulting in weak alignment among speaker identity, contextual cues, and emotion states.

Method: InitERC uses a one-stage in-context instruction tuning framework with four components: demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. It explores three key factors: retrieval strategy, example ordering, and number of examples.

Result: Extensive experiments on three widely used datasets demonstrate that InitERC achieves substantial improvements over state-of-the-art baselines.

Conclusion: The proposed one-stage in-context instruction tuning framework effectively learns speaker-context-emotion alignment and outperforms existing multi-stage approaches for emotion recognition in conversation.

Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of
each utterance in a conversation, playing a vital role in empathetic artificial
intelligence. With the growing of large language models (LLMs), instruction
tuning has emerged as a critical paradigm for ERC. Existing studies mainly
focus on multi-stage instruction tuning, which first endows LLMs with speaker
characteristics, and then conducts context-aware instruction tuning to
comprehend emotional states. However, these methods inherently constrains the
capacity to jointly capture the dynamic interaction between speaker
characteristics and conversational context, resulting in weak alignment among
speaker identity, contextual cues, and emotion states within a unified
framework. In this paper, we propose InitERC, a simple yet effective one-stage
in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn
speaker-context-emotion alignment from context examples via in-context
instruction tuning. Specifically, InitERC comprises four components, i.e.,
demonstration pool construction, in-context example selection, prompt template
design, and in-context instruction tuning. To explore the impact of in-context
examples, we conduct a comprehensive study on three key factors: retrieval
strategy, example ordering, and the number of examples. Extensive experiments
on three widely used datasets demonstrate that our proposed InitERC achieves
substantial improvements over the state-of-the-art baselines.

</details>


### [13] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
*Punya Syon Pandey,Yongjin Yang,Jiarui Liu,Zhijing Jin*

Main category: cs.CL

TL;DR: CORE metric quantifies linguistic effectiveness in multi-agent LLM systems across game-theoretic interactions using cluster entropy, lexical repetition, and semantic similarity.


<details>
  <summary>Details</summary>
Motivation: Linguistic diversity in LLM multi-agent interactions hasn't been sufficiently quantified, despite emergent capabilities in game-theoretic settings.

Method: Developed CORE metric integrating cluster entropy, lexical repetition, and semantic similarity; applied to pairwise LLM dialogs across competitive, cooperative, and neutral settings; analyzed using Zipf's and Heaps' Laws.

Result: Cooperative settings show steeper Zipf distributions and higher Heap exponents (more repetition with vocabulary expansion), while competitive interactions display lower exponents (less repetition, constrained vocabularies).

Conclusion: Social incentives significantly influence language adaptation patterns, and CORE serves as a robust diagnostic tool for measuring linguistic robustness in multi-agent LLM systems.

Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

</details>


### [14] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
*Jie Lu,Du Jin,Hitomi Yanaka*

Main category: cs.CL

TL;DR: This paper constructs a linguistically motivated NLI dataset for Chinese and Japanese perfect aspect, revealing that even advanced LLMs struggle with temporal inference in these languages.


<details>
  <summary>Details</summary>
Motivation: Chinese and Japanese lack distinct grammatical forms for tense within perfect aspect (unlike English), which complicates Natural Language Inference tasks in these languages.

Method: Constructed a template-based NLI dataset (1,350 pairs per language) focusing on perfect aspect in Chinese and Japanese, then tested advanced LLMs on temporal inference tasks.

Result: Experiments showed that even advanced LLMs struggle with temporal inference, particularly in detecting subtle tense and reference-time shifts in Chinese and Japanese.

Conclusion: The findings highlight limitations in current models and underscore the critical need for cross-linguistic evaluation in temporal semantics research.

Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark
the perfect aspect across tenses, Chinese and Japanese lack separate
grammatical forms for tense within the perfect aspect, which complicates
Natural Language Inference (NLI). Focusing on the perfect aspect in these
languages, we construct a linguistically motivated, template-based NLI dataset
(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle
with temporal inference, particularly in detecting subtle tense and
reference-time shifts. These findings highlight model limitations and
underscore the need for cross-linguistic evaluation in temporal semantics. Our
dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [15] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
*Yue Wang,Liesheng Wei,Yuxiang Wang*

Main category: cs.CL

TL;DR: CAMF is a novel multi-agent framework that detects machine-generated text by analyzing cross-dimensional linguistic inconsistencies through collaborative adversarial agents.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot detection methods for machine-generated text are inadequate due to superficial analyses and lack of consistency checks across linguistic dimensions like style, semantics, and logic.

Method: CAMF uses multiple LLM-based agents in a three-phase process: Multi-dimensional Linguistic Feature Extraction, Adversarial Consistency Probing, and Synthesized Judgment Aggregation to detect subtle textual incongruities.

Result: Empirical evaluations show CAMF significantly outperforms state-of-the-art zero-shot MGT detection techniques.

Conclusion: The collaborative adversarial multi-agent framework provides a more effective approach for detecting machine-generated text by deeply analyzing cross-dimensional linguistic inconsistencies.

Abstract: Detecting machine-generated text (MGT) from contemporary Large Language
Models (LLMs) is increasingly crucial amid risks like disinformation and
threats to academic integrity. Existing zero-shot detection paradigms, despite
their practicality, often exhibit significant deficiencies. Key challenges
include: (1) superficial analyses focused on limited textual attributes, and
(2) a lack of investigation into consistency across linguistic dimensions such
as style, semantics, and logic. To address these challenges, we introduce the
\textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent
\textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple
LLM-based agents. CAMF employs specialized agents in a synergistic three-phase
process: \emph{Multi-dimensional Linguistic Feature Extraction},
\emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment
Aggregation}. This structured collaborative-adversarial process enables a deep
analysis of subtle, cross-dimensional textual incongruities indicative of
non-human origin. Empirical evaluations demonstrate CAMF's significant
superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [16] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
*Shaozhe Yin,Jinyu Guo,Kai Shuang,Xia Liu,Ruize Ou*

Main category: cs.CL

TL;DR: Instruction-based continual contrastive tuning for LLMs in continual relation extraction that specializes in error cases to correct cognitive biases and achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing CRE methods don't effectively address error cases that reveal model cognitive biases, and current approaches treat training and memory data uniformly without distinguishing correct vs incorrect responses.

Method: Splits training and memory data into correct/incorrect parts based on initial responses, uses dual-task fine-tuning, and employs instruction-based contrastive tuning to continuously correct cognitive biases using previous data in instruction-tuning manner.

Result: Achieves new state-of-the-art performance on TACRED and FewRel datasets with significant improvements over previous methods.

Conclusion: Specializing in exploiting error cases is crucial for mitigating catastrophic forgetting in continual relation extraction, and the proposed approach effectively addresses cognitive biases in LLMs.

Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging
relations while avoiding catastrophic forgetting. Existing CRE methods mainly
use memory replay and contrastive learning to mitigate catastrophic forgetting.
However, these methods do not attach importance to the error cases that can
reveal the model's cognitive biases more effectively. To address this issue, we
propose an instruction-based continual contrastive tuning approach for Large
Language Models (LLMs) in CRE. Different from existing CRE methods that
typically handle the training and memory data in a unified manner, this
approach splits the training and memory data of each task into two parts
respectively based on the correctness of the initial responses and treats them
differently through dual-task fine-tuning. In addition, leveraging the
advantages of LLM's instruction-following ability, we propose a novel
instruction-based contrastive tuning strategy for LLM to continuously correct
current cognitive biases with the guidance of previous data in an
instruction-tuning manner, which mitigates the gap between old and new
relations in a more suitable way for LLMs. We experimentally evaluate our model
on TACRED and FewRel, and the results show that our model achieves new
state-of-the-art CRE performance with significant improvements, demonstrating
the importance of specializing in exploiting error cases.

</details>


### [17] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
*Jinyi Han,Tingyun Li,Shisong Chen,Jie Shi,Xinyi Wang,Guanglei Yue,Jiaqing Liang,Xin Lin,Liqian Wen,Zulong Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: FineCE introduces a novel confidence estimation method for LLMs that provides fine-grained, continuous confidence scores during text generation, outperforming existing methods through supervised training and backward confidence integration.


<details>
  <summary>Details</summary>
Motivation: LLMs lack self-awareness and exhibit overconfidence, assigning high confidence to incorrect predictions, which undermines trustworthiness and reliability of their outputs. Existing approaches have coarse-grained scoring mechanisms that fail to provide continuous confidence estimates.

Method: Developed a pipeline for constructing training data capturing LLM response distributions, trained a supervised model to predict confidence scores, proposed Backward Confidence Integration (BCI) strategy using subsequent text information, and introduced three strategies for optimal confidence estimation positions.

Result: Extensive experiments on multiple benchmark datasets show that FineCE consistently outperforms existing classical confidence estimation methods.

Conclusion: FineCE provides accurate, fine-grained confidence estimation for LLM-generated text, enhancing trustworthiness and reliability through its novel training pipeline and backward integration strategy.

Abstract: While large language models (LLMs) have demonstrated remarkable performance
across diverse tasks, they fundamentally lack self-awareness and frequently
exhibit overconfidence, assigning high confidence scores to incorrect
predictions. Accurate confidence estimation is therefore critical for enhancing
the trustworthiness and reliability of LLM-generated outputs. However, existing
approaches suffer from coarse-grained scoring mechanisms that fail to provide
fine-grained, continuous confidence estimates throughout the generation
process. To address these limitations, we introduce FineCE, a novel confidence
estimation method that delivers accurate, fine-grained confidence scores during
text generation. Specifically, we first develop a comprehensive pipeline for
constructing training data that effectively captures the underlying
probabilistic distribution of LLM responses, and then train a model to predict
confidence scores for arbitrary text sequences in a supervised manner.
Furthermore, we propose a Backward Confidence Integration (BCI) strategy that
leverages information from the subsequent text to enhance confidence estimation
for the current sequence during inference. We also introduce three strategies
for identifying optimal positions to perform confidence estimation within the
generation process. Extensive experiments on multiple benchmark datasets
demonstrate that FineCE consistently outperforms existing classical confidence
estimation methods. Our code and all baselines used in the paper are available
on GitHub.

</details>


### [18] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
*Yao Wu*

Main category: cs.CL

TL;DR: J6 is a structured Jacobian-based method for multi-objective LLM adaptation that decomposes gradient interactions into six interpretable components, enabling both hard and soft optimization strategies while providing geometric insights.


<details>
  <summary>Details</summary>
Motivation: Existing multi-objective optimization strategies for LLM adaptation rely on scalar gradient aggregation and ignore the geometric structure between objectives and parameters, making it challenging to balance conflicting goals like improving factuality and increasing confidence.

Method: Proposes J6 method that decomposes the gradient interaction matrix into six interpretable components, enabling both hard decision-making (argmax for dominant update direction) and soft strategies (softmax-based attention-style weighting) that adapt to local conflict and synergy.

Result: The method provides interpretable structure for parameter attribution, task interference analysis, and geometry-aligned adaptation, forming a dynamic update framework for conflict-aware prompt optimization.

Conclusion: J6 introduces a principled and extensible mechanism for multi-objective neural tuning that incorporates structured Jacobian reasoning, opening new avenues for geometric-aware optimization in LLM adaptation.

Abstract: In large language model (LLM) adaptation, balancing multiple optimization
objectives such as improving factuality (heat) and increasing confidence (via
low entropy) poses a fundamental challenge, especially when prompt parameters
(e.g., hidden-layer insertions h and embedding modifications w) interact in
non-trivial ways. Existing multi-objective optimization strategies often rely
on scalar gradient aggregation, ignoring the deeper geometric structure between
objectives and parameters. We propose J6, a structured Jacobian-based method
that decomposes the gradient interaction matrix into six interpretable
components. This decomposition enables both hard decision-making (e.g.,
choosing the dominant update direction via argmax) and soft strategies (e.g.,
attention-style weighting via softmax over J6), forming a dynamic update
framework that adapts to local conflict and synergy. Moreover, the
interpretable structure of J6 provides insight into parameter attribution, task
interference, and geometry-aligned adaptation. Our work introduces a principled
and extensible mechanism for conflict-aware prompt optimization, and opens a
new avenue for incorporating structured Jacobian reasoning into multi-objective
neural tuning.

</details>


### [19] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
*Haiquan Hu,Jiazhi Jiang,Shiyou Xu,Ruhan Zeng,Tian Wang*

Main category: cs.CL

TL;DR: STEM is a lightweight evaluation framework that uses significant transition samples from model families to efficiently estimate LLM capabilities without full benchmark testing.


<details>
  <summary>Details</summary>
Motivation: Standard benchmarks are becoming ineffective due to overfitting, high computational costs, and inability to distinguish meaningful differences between rapidly advancing LLMs.

Method: Identifies significant transition samples (STS) by analyzing performance patterns across LLMs of the same architecture but different parameter scales, then uses these samples to estimate unknown model capabilities.

Result: STEM reliably captures performance trends and aligns with ground-truth model capability rankings across six diverse benchmarks using the Qwen3 model family.

Conclusion: STEM provides a practical, scalable, and architecture-agnostic method for fine-grained evaluation of LLMs, addressing current benchmarking limitations.

Abstract: Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [20] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
*Ziqian Bi,Lu Chen,Junhao Song,Hongying Luo,Enze Ge,Junmin Huang,Tianyang Wang,Keyu Chen,Chia Xin Liang,Zihan Wei,Huafeng Liu,Chunjie Tian,Jibin Guan,Joe Yeong,Yongzhi Xu,Peng Wang,Junfeng Hao*

Main category: cs.CL

TL;DR: First comprehensive evaluation of thinking budget mechanisms in medical AI, showing logarithmic scaling between computational resources and reasoning quality across model sizes and medical specialties.


<details>
  <summary>Details</summary>
Motivation: To establish scaling laws and efficiency regimes for thinking budget allocation in medical reasoning tasks, enabling optimized resource allocation for clinical AI deployment.

Method: Systematic evaluation of Qwen3 (1.7B-235B) and DeepSeek-R1 (1.5B-70B) models across 15 medical datasets with controlled thinking budgets from zero to unlimited tokens.

Result: Identified three efficiency regimes: high-efficiency (0-256 tokens), balanced (256-512 tokens), and high-accuracy (>512 tokens). Smaller models showed 15-20% improvement vs 5-10% for larger models. Domain-specific patterns emerged with neurology/gastroenterology requiring deeper reasoning.

Conclusion: Thinking budget control is critical for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining healthcare deployment transparency.

Abstract: This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

</details>


### [21] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
*Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CL

TL;DR: LLMs can effectively model human privacy perspectives for text evaluation, showing promise as privacy evaluators despite low inter-human agreement on privacy sensitivity.


<details>
  <summary>Details</summary>
Motivation: Privacy evaluation in NLP remains challenging due to its subjective nature, and existing methods struggle with accurate assessment. The success of LLM-as-a-Judge in other NLP tasks suggests it could be adapted for privacy evaluation.

Method: Conducted a large-scale study with 10 datasets, 13 different LLMs, and 677 human participants to compare LLM privacy evaluations with human perceptions of text sensitivity.

Result: While privacy is difficult to measure empirically (low inter-human agreement), LLMs can accurately model a global human privacy perspective and align well with human evaluations.

Conclusion: LLM-as-a-Judge shows promise for privacy evaluation in textual data, providing a feasible technical solution to address core privacy assessment challenges, though limitations exist that require further analysis.

Abstract: Despite advances in the field of privacy-preserving Natural Language
Processing (NLP), a significant challenge remains the accurate evaluation of
privacy. As a potential solution, using LLMs as a privacy evaluator presents a
promising approach $\unicode{x2013}$ a strategy inspired by its success in
other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$
paradigm has achieved impressive results on a variety of natural language
evaluation tasks, demonstrating high agreement rates with human annotators.
Recognizing that privacy is both subjective and difficult to define, we
investigate whether LLM-as-a-Judge can also be leveraged to evaluate the
privacy sensitivity of textual data. Furthermore, we measure how closely LLM
evaluations align with human perceptions of privacy in text. Resulting from a
study involving 10 datasets, 13 LLMs, and 677 human survey participants, we
confirm that privacy is indeed a difficult concept to measure empirically,
exhibited by generally low inter-human agreement rates. Nevertheless, we find
that LLMs can accurately model a global human privacy perspective, and through
an analysis of human and LLM reasoning patterns, we discuss the merits and
limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our
findings pave the way for exploring the feasibility of LLMs as privacy
evaluators, addressing a core challenge in solving pressing privacy issues with
innovative technical solutions.

</details>


### [22] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
*Abdelhamid Haouhat,Slimane Bellaouar,Attia Nehar,Hadda Cherroun,Ahmed Abdelali*

Main category: cs.CL

TL;DR: Survey paper on Arabic Multimodal Machine Learning that categorizes research through a novel taxonomy covering datasets, applications, approaches, and challenges.


<details>
  <summary>Details</summary>
Motivation: Arabic MML has reached foundational maturity, requiring a comprehensive survey to organize existing research, identify gaps, and guide future work in this emerging field.

Method: Developed a novel taxonomy to categorize Arabic MML research into four key areas: datasets, applications, approaches, and challenges, providing a structured analysis of existing literature.

Result: Provides a comprehensive overview of Arabic MML's current state, identifies unexplored areas and research gaps, and creates a framework to help researchers build upon existing work.

Conclusion: This survey empowers researchers to advance Arabic MML by addressing identified challenges and opportunities, serving as a foundational resource for future development in the field.

Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

</details>


### [23] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
*Wuttikorn Ponwitayarat,Raymond Ng,Jann Railey Montalan,Thura Aung,Jian Gang Ngui,Yosephine Susanto,William Tjhi,Panuthep Tasawong,Erik Cambria,Ekapol Chuangsuwanich,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: SEA-BED is the first large-scale Southeast Asian sentence embedding benchmark with 169 human-curated datasets across 9 tasks and 10 languages, revealing significant performance gaps and ranking inconsistencies for SEA languages compared to global benchmarks.


<details>
  <summary>Details</summary>
Motivation: Southeast Asia has nearly 700 million speakers but lacks region-specific embedding benchmarks, with existing datasets often machine-translated and missing native linguistic properties, creating evaluation gaps for SEA languages.

Method: Created SEA-BED benchmark with 169 datasets (71% human-formulated) across 9 tasks and 10 SEA languages, then evaluated 17 embedding models through six studies analyzing task challenges, cross-benchmark comparisons, and translation effects.

Result: Results show sharp ranking shifts, inconsistent model performance among SEA languages, and demonstrate that human-curated datasets are crucial for accurate evaluation of low-resource languages like Burmese.

Conclusion: SEA-BED addresses critical gaps in multilingual evaluation, revealing that current embedding models struggle with SEA languages and highlighting the importance of human-curated, native datasets for proper benchmarking in linguistically diverse regions.

Abstract: Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

</details>


### [24] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
*Ankita Pasad*

Main category: cs.CL

TL;DR: This thesis analyzes speech foundation models (SFMs) through statistical analysis and introduces new spoken language understanding tasks (NER/NEL) to evaluate SFM performance, showing end-to-end models outperform traditional cascaded approaches.


<details>
  <summary>Details</summary>
Motivation: To understand the acoustic and linguistic knowledge encoded in SFMs and address the lack of evaluation for spoken language understanding tasks that require deeper understanding than speech recognition.

Method: Developed a lightweight analysis framework using statistical tools and training-free tasks to investigate SFM layers, and contributed spoken NER and NEL tasks to SLU benchmark with SFM-based approaches.

Result: Found that analytical insights have concrete implications for downstream task performance, and end-to-end models leveraging SFMs surpass traditional cascaded approaches for spoken language understanding tasks.

Conclusion: The thesis provides tools and datasets to advance understanding of SFMs and enable informed design choices for future model development and adoption in speech processing applications.

Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose
representations for a wide range of speech-processing tasks. The last five
years have seen an influx of increasingly successful self-supervised and
supervised pre-trained models with impressive performance on various downstream
tasks.
  Although the zoo of SFMs continues to grow, our understanding of the
knowledge they acquire lags behind. This thesis presents a lightweight analysis
framework using statistical tools and training-free tasks to investigate the
acoustic and linguistic knowledge encoded in SFM layers. We conduct a
comparative study across multiple SFMs and statistical tools. Our study also
shows that the analytical insights have concrete implications for downstream
task performance.
  The effectiveness of an SFM is ultimately determined by its performance on
speech applications. Yet it remains unclear whether the benefits extend to
spoken language understanding (SLU) tasks that require a deeper understanding
than widely studied ones, such as speech recognition. The limited exploration
of SLU is primarily due to a lack of relevant datasets. To alleviate that, this
thesis contributes tasks, specifically spoken named entity recognition (NER)
and named entity localization (NEL), to the Spoken Language Understanding
Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find
that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded
(speech recognition followed by a text model) approaches. Further, we evaluate
E2E SLU models across SFMs and adaptation strategies to assess the impact on
task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs,
providing tools and datasets to further our understanding and to enable the
community to make informed design choices for future model development and
adoption.

</details>


### [25] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
*Zheye Deng,Chunkit Chan,Tianshi Zheng,Wei Fan,Weiqi Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: Systematic review of text-to-structure conversion techniques, evaluating methodologies, datasets, metrics, and proposing a universal evaluation framework for structured outputs.


<details>
  <summary>Details</summary>
Motivation: AI systems are evolving toward agentic operation and context-aware retrieval, requiring transformation of unstructured text into structured formats like tables, knowledge graphs, and charts for applications from summarization to data mining, but current research lacks comprehensive synthesis.

Method: Systematic review examining text-to-structure techniques, challenges, current datasets, assessment criteria, and outlining future research directions.

Result: The paper provides a comprehensive synthesis of methodologies and introduces a universal evaluation framework for structured outputs.

Conclusion: Text-to-structure conversion is established as foundational infrastructure for next-generation AI systems, with the proposed evaluation framework supporting future advancements in the field.

Abstract: The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

</details>


### [26] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
*Xinda Jia,Jinpeng Li,Zezhong Wang,Jingjing Li,Xingshan Zeng,Yasheng Wang,Weinan Zhang,Yong Yu,Weiwen Liu*

Main category: cs.CL

TL;DR: A taxonomy of LLM reasoning strategies based on cognitive psychology principles, categorizing methods along fast/slow and internal/external knowledge boundaries to enable adaptive reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs need to adapt reasoning strategies to real-world task demands, ranging from fast intuitive responses to deliberate step-by-step reasoning and tool-augmented thinking.

Method: Proposed a novel taxonomy with two knowledge boundaries: fast/slow (intuitive vs deliberative) and internal/external (model parameters vs external tools), systematically surveying recent adaptive reasoning methods.

Result: Developed a comprehensive categorization framework for LLM reasoning strategies based on key decision factors from cognitive psychology.

Conclusion: Highlights open challenges and future directions for creating more adaptive, efficient, and reliable LLMs through strategic reasoning adaptation.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
reasoning across diverse domains. However, effective reasoning in real-world
tasks requires adapting the reasoning strategy to the demands of the problem,
ranging from fast, intuitive responses to deliberate, step-by-step reasoning
and tool-augmented thinking. Drawing inspiration from cognitive psychology, we
propose a novel taxonomy of LLM reasoning strategies along two knowledge
boundaries: a fast/slow boundary separating intuitive from deliberative
processes, and an internal/external boundary distinguishing reasoning grounded
in the model's parameters from reasoning augmented by external tools. We
systematically survey recent work on adaptive reasoning in LLMs and categorize
methods based on key decision factors. We conclude by highlighting open
challenges and future directions toward more adaptive, efficient, and reliable
LLMs.

</details>


### [27] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
*Elon Ezra,Ariel Weizman,Amos Azaria*

Main category: cs.CL

TL;DR: LLMs struggle to predict properties of their own responses, showing poor performance on self-execution tasks regardless of model size or capability.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs can anticipate aspects of their own outputs (difficulty prediction, refusal behavior, association patterns) rather than just testing knowledge or reasoning abilities.

Method: Introducing the Self-Execution Benchmark that measures model's ability to predict properties of its own responses, including question difficulty, refusal likelihood, and output associations.

Result: Models perform poorly on self-prediction tasks, and increased model size/capability doesn't consistently improve performance on these self-execution benchmarks.

Conclusion: LLMs have fundamental limitations in representing and reasoning about their own behavior, suggesting a gap in self-awareness capabilities.

Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their
knowledge or reasoning abilities. In this paper, we explore a different type of
evaluation: whether an LLM can predict aspects of its own responses. Since LLMs
lack the ability to execute themselves, we introduce the Self-Execution
Benchmark, which measures a model's ability to anticipate properties of its
output, such as whether a question will be difficult for it, whether it will
refuse to answer, or what kinds of associations it is likely to produce. Our
experiments show that models generally perform poorly on this benchmark, and
that increased model size or capability does not consistently lead to better
performance. These results suggest a fundamental limitation in how LLMs
represent and reason about their own behavior.

</details>


### [28] [Legal$Î”$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
*Xin Dai,Buqiang Xu,Zhenghao Liu,Yukun Yan,Huiyuan Xie,Xiaoyuan Yi,Shuo Wang,Ge Yu*

Main category: cs.CL

TL;DR: LegalÎ” is a reinforcement learning framework that enhances legal reasoning in LLMs by maximizing information gain between direct answers and reasoning-augmented outputs, producing more reliable and interpretable legal judgments.


<details>
  <summary>Details</summary>
Motivation: Existing legal LLMs struggle with generating reliable and interpretable reasoning processes, often defaulting to fast-thinking behavior without explicit multi-step reasoning, which limits effectiveness in complex legal scenarios requiring rigorous justification.

Method: A two-stage reinforcement learning framework: (1) distills latent reasoning capabilities from DeepSeek-R1 (Large Reasoning Model), and (2) refines reasoning quality via differential comparisons with a multidimensional reward mechanism assessing structural coherence and legal-domain specificity.

Result: Outperforms strong baselines on multiple legal reasoning tasks in both accuracy and interpretability, consistently producing more robust and trustworthy legal judgments without relying on labeled preference data.

Conclusion: LegalÎ” successfully addresses the challenge of unreliable reasoning in legal LLMs by encouraging meaningful reasoning patterns through information gain maximization, demonstrating significant improvements in legal decision-making automation.

Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in
automating judicial decision-making with the support of Large Language Models
(LLMs). However, existing legal LLMs still struggle to generate reliable and
interpretable reasoning processes. They often default to fast-thinking behavior
by producing direct answers without explicit multi-step reasoning, limiting
their effectiveness in complex legal scenarios that demand rigorous
justification. To address this challenge, we propose Legal$\Delta$, a
reinforcement learning framework designed to enhance legal reasoning through
chain-of-thought guided information gain. During training, Legal$\Delta$
employs a dual-mode input setup-comprising direct answer and
reasoning-augmented modes-and maximizes the information gain between them. This
encourages the model to acquire meaningful reasoning patterns rather than
generating superficial or redundant explanations. Legal$\Delta$ follows a
two-stage approach: (1) distilling latent reasoning capabilities from a
powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning
quality via differential comparisons, combined with a multidimensional reward
mechanism that assesses both structural coherence and legal-domain specificity.
Experimental results on multiple legal reasoning tasks demonstrate that
Legal$\Delta$ outperforms strong baselines in both accuracy and
interpretability. It consistently produces more robust and trustworthy legal
judgments without relying on labeled preference data. All code and data will be
released at https://github.com/NEUIR/LegalDelta.

</details>


### [29] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
*Ziyang Chen,Erxue Min,Xiang Zhao,Yunxin Li,Xin Jia,Jinzhi Liao,Jichao Li,Shuaiqiang Wang,Baotian Hu,Dawei Yin*

Main category: cs.CL

TL;DR: ChronoQA is a large-scale Chinese QA benchmark dataset focused on temporal reasoning evaluation in RAG systems, built from 300K+ news articles (2019-2024) with 5,176 high-quality questions covering various temporal types and scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the need for evaluating temporal reasoning capabilities in Retrieval-Augmented Generation systems, particularly for Chinese language, where existing benchmarks lack comprehensive temporal evaluation frameworks.

Method: Constructed from over 300,000 news articles (2019-2024), featuring 5,176 questions covering absolute/aggregate/relative temporal types with explicit/implicit time expressions. Includes multi-stage validation (rule-based, LLM-based, human evaluation) and supports single/multi-document scenarios.

Result: A comprehensive benchmark dataset with structural annotations that enables structured evaluation across temporal tasks and provides a reliable resource for advancing time-sensitive RAG systems.

Conclusion: ChronoQA serves as a robust, dynamic, and scalable benchmark for evaluating and advancing temporal reasoning in Chinese question answering systems, particularly in retrieval-augmented generation contexts.

Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

</details>


### [30] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
*Qinghua Wang,Xu Zhang,Lingyan Yang,Rui Shao,Bonan Wang,Fang Wang,Cunquan Qu*

Main category: cs.CL

TL;DR: Proposes MT-DT model integrating legal logic with deep learning for probation prediction, outperforming baseline models.


<details>
  <summary>Details</summary>
Motivation: Current Intelligent Judicial Assistant Systems lack dedicated probation prediction methods and overlook legal logic, focusing mainly on data-driven approaches without considering criminal circumstances and remorse factors.

Method: Three-stage approach: 1) Construct specialized probation dataset with fact descriptions and probation legal elements; 2) Design MT-DT model based on legal logic and Dual-Track Theory of Punishment; 3) Experimental validation on probation dataset.

Result: MT-DT model outperforms baseline models, and legal logic analysis validates the effectiveness of the proposed approach.

Conclusion: Integrating legal logic into deep learning models significantly improves probation prediction accuracy and provides a more legally sound framework for judicial decision-making assistance.

Abstract: Probation is a crucial institution in modern criminal law, embodying the
principles of fairness and justice while contributing to the harmonious
development of society. Despite its importance, the current Intelligent
Judicial Assistant System (IJAS) lacks dedicated methods for probation
prediction, and research on the underlying factors influencing probation
eligibility remains limited. In addition, probation eligibility requires a
comprehensive analysis of both criminal circumstances and remorse. Much of the
existing research in IJAS relies primarily on data-driven methodologies, which
often overlooks the legal logic underpinning judicial decision-making. To
address this gap, we propose a novel approach that integrates legal logic into
deep learning models for probation prediction, implemented in three distinct
stages. First, we construct a specialized probation dataset that includes fact
descriptions and probation legal elements (PLEs). Second, we design a distinct
probation prediction model named the Multi-Task Dual-Theory Probation
Prediction Model (MT-DT), which is grounded in the legal logic of probation and
the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the
probation dataset demonstrate that the MT-DT model outperforms baseline models,
and an analysis of the underlying legal logic further validates the
effectiveness of the proposed approach.

</details>


### [31] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
*Tomer Krichli,Bhiksha Raj,Joseph Keshet*

Main category: cs.CL

TL;DR: Proposes a method to convert transformer encoder-decoder ASR models into low-latency streaming models using LoRA fine-tuning and causal encoder modifications, achieving better performance than existing streaming approaches with lower complexity.


<details>
  <summary>Details</summary>
Motivation: Existing SOTA ASR models like Whisper and Canary are designed for offline transcription and cannot handle streaming/real-time scenarios due to architectural limitations and non-causal design.

Method: Modifies non-causal encoder to causal encoder by fine-tuning both encoder and decoder using Low-Rank Adaptation (LoRA) with weakly aligned dataset, and develops updated inference mechanism for greedy and beam-search decoding.

Result: Outperforms existing non-fine-tuned streaming approaches on low-latency chunk sizes (<300 msec) in most cases with lower complexity, and enables better alignment for word-level timestamp extraction.

Conclusion: Successfully transforms transformer encoder-decoder models into efficient streaming ASR systems through causal fine-tuning, providing a practical solution for real-time transcription with improved performance and alignment capabilities.

Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models
like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)
performance in offline transcription. However, these models are not designed
for streaming (online or real-time) transcription, due to limitations in their
architecture and training methodology. We propose a method to turn the
transformer encoder-decoder model into a low-latency streaming model that is
careless about future context. We present an analysis explaining why it is not
straightforward to convert an encoder-decoder transformer to a low-latency
streaming model. Our proposed method modifies the existing (non-causal) encoder
to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank
Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated
inference mechanism that utilizes the fine-tune causal encoder and decoder to
yield greedy and beam-search decoding, and is shown to be locally optimal.
Experiments on low-latency chunk sizes (less than 300 msec) show that our
fine-tuned model outperforms existing non-fine-tuned streaming approaches in
most cases, while using a lower complexity. Additionally, we observe that our
training process yields better alignment, enabling a simple method for
extracting word-level timestamps. We release our training and inference code,
along with the fine-tuned models, to support further research and development
in streaming ASR.

</details>


### [32] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
*Eviatar Nachshoni,Arie Cattan,Shmuel Amar,Ori Shapira,Ido Dagan*

Main category: cs.CL

TL;DR: A new benchmark NATCONFQA for conflict-aware multi-answer question answering, showing LLMs struggle with conflicting answers despite strong performance on standard QA tasks.


<details>
  <summary>Details</summary>
Motivation: Multi-Answer Question Answering (MAQA) with conflicting answers remains challenging for LLMs, and existing benchmarks have limitations like synthetic data, yes/no restrictions, or unverified annotations.

Method: Extended conflict-aware MAQA setting requiring models to identify all valid answers and detect conflicting answer pairs. Introduced cost-effective methodology using fact-checking datasets to construct NATCONFQA benchmark with detailed conflict labels.

Result: Evaluation of eight high-end LLMs revealed their fragility in handling various conflict types and flawed strategies for resolving conflicts.

Conclusion: LLMs exhibit significant weaknesses in conflict-aware MAQA, highlighting the need for better benchmarks and improved conflict resolution capabilities in language models.

Abstract: Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [33] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
*Yuanfeng Xu,Zehui Dai,Jian Liang,Jiapeng Guan,Guangrun Wang,Liang Lin,Xiaohui Lv*

Main category: cs.CL

TL;DR: ReaLM is a reinforcement learning framework that enhances small language models' reasoning through multi-route verification, gradual autonomy induction, and domain knowledge distillation, achieving improved capability, autonomy, and generalization.


<details>
  <summary>Details</summary>
Motivation: Small language models struggle with complex reasoning due to limited capacity, error-prone multi-step reasoning, and trade-offs between reasoning capability, autonomy, and generalization in existing approaches.

Method: Proposes ReaLM framework with: 1) Multi-Route Process Verification (MRPV) contrasting positive/negative reasoning paths, 2) Enabling Autonomy via Asymptotic Induction (EAAI) gradually fading external signals, 3) Guided chain-of-thought distillation to encode domain knowledge.

Result: Extensive experiments show ReaLM significantly improves SLM performance across reasoning capability, autonomy, and generalization on both vertical and general reasoning tasks.

Conclusion: ReaLM provides an effective reinforcement learning approach for robust and self-sufficient reasoning in small language models, addressing key limitations while maintaining performance across multiple critical aspects.

Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large
Language Models (LLMs), but often struggle with complex reasoning due to their
limited capacity and a tendency to produce mistakes or inconsistent answers
during multi-step reasoning. Existing efforts have improved SLM performance,
but typically at the cost of one or more of three key aspects: (1) reasoning
capability, due to biased supervision that filters out negative reasoning paths
and limits learning from errors; (2) autonomy, due to over-reliance on
externally generated reasoning signals; and (3) generalization, which suffers
when models overfit to teacher-specific patterns. In this paper, we introduce
ReaLM, a reinforcement learning framework for robust and self-sufficient
reasoning in vertical domains. To enhance reasoning capability, we propose
Multi-Route Process Verification (MRPV), which contrasts both positive and
negative reasoning paths to extract decisive patterns. To reduce reliance on
external guidance and improve autonomy, we introduce Enabling Autonomy via
Asymptotic Induction (EAAI), a training strategy that gradually fades external
signals. To improve generalization, we apply guided chain-of-thought
distillation to encode domain-specific rules and expert knowledge into SLM
parameters, making them part of what the model has learned. Extensive
experiments on both vertical and general reasoning tasks demonstrate that ReaLM
significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [34] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
*Duzhen Zhang,Zixiao Wang,Zhong-Zhi Li,Yahan Yu,Shuncheng Jia,Jiahua Dong,Haotian Xu,Xing Wu,Yingying Zhang,Tielin Zhang,Jie Yang,Xiuying Chen,Le Song*

Main category: cs.CL

TL;DR: MedKGent is a novel LLM agent framework that constructs temporally evolving medical knowledge graphs from PubMed abstracts, achieving high accuracy and demonstrating significant improvements in medical question answering benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current KG construction methods have limited generalizability, treat biomedical corpora as static, and ignore temporal dynamics and contextual uncertainty of evolving medical knowledge.

Method: Uses two specialized agents (Extractor and Constructor) powered by Qwen2.5-32B-Instruct to incrementally build KGs day-by-day from 10M+ PubMed abstracts (1975-2023), with sampling-based confidence scoring and temporal integration.

Result: Constructed KG with 156,275 entities and 2,971,384 relational triples achieving ~90% accuracy. RAG evaluations across 7 medical QA benchmarks showed significant improvements over non-augmented baselines.

Conclusion: MedKGent successfully addresses temporal dynamics in medical knowledge, produces high-quality evolving KGs, and demonstrates practical utility in drug repurposing and medical question answering applications.

Abstract: The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

</details>


### [35] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
*Zilong Bai,Zihan Xu,Cong Sun,Chengxi Zang,H. Timothy Bunnell,Catherine Sinfield,Jacqueline Rutter,Aaron Thomas Martinez,L. Charles Bailey,Mark Weiner,Thomas R. Campion,Thomas Carton,Christopher B. Forrest,Rainu Kaushal,Fei Wang,Yifan Peng*

Main category: cs.CL

TL;DR: Hybrid NLP pipeline combining rule-based NER with BERT-based assertion detection for efficient PASC symptom extraction from clinical notes, achieving high accuracy and processing speed.


<details>
  <summary>Details</summary>
Motivation: PASC diagnosis is challenging due to evolving symptoms over variable time intervals, requiring automated tools to extract and analyze symptoms from clinical notes efficiently.

Method: Developed comprehensive PASC lexicon with specialists, created hybrid NLP pipeline integrating rule-based named entity recognition with BERT-based assertion detection modules, validated on 160 notes from 11 health systems.

Result: Achieved F1 score of 0.82 (internal) and 0.76 (external validation), processed notes in 2.448Â±0.812 seconds, strong Spearman correlations (Ï>0.83 positive, Ï>0.72 negative mentions).

Conclusion: The pipeline demonstrates effectiveness and efficiency for PASC symptom extraction, showing potential to improve PASC diagnosis through automated clinical note analysis.

Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)
remains challenging due to its myriad symptoms that evolve over long- and
variable-time intervals. To address this issue, we developed a hybrid natural
language processing pipeline that integrates rule-based named entity
recognition with BERT-based assertion detection modules for PASC-symptom
extraction and assertion detection from clinical notes. We developed a
comprehensive PASC lexicon with clinical specialists. From 11 health systems of
the RECOVER initiative network across the U.S., we curated 160 intake progress
notes for model development and evaluation, and collected 47,654 progress notes
for a population-level prevalence study. We achieved an average F1 score of
0.82 in one-site internal validation and 0.76 in 10-site external validation
for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$
seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive
mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These
demonstrate the effectiveness and efficiency of our models and their potential
for improving PASC diagnosis.

</details>


### [36] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
*Zhuorui Liu,Chen Zhang,Dawei Song*

Main category: cs.CL

TL;DR: ZigzagAttention improves KV cache efficiency in LLMs by grouping attention heads exclusively as retrieval or streaming heads per layer, reducing latency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Large language models face deployment challenges due to high KV cache consumption from long contexts. Existing methods that mix retrieval and streaming heads in layers create extra latency from tensor access and indexing operations.

Method: Proposes ZigzagAttention with a new criterion that enforces exclusive grouping of retrieval heads or streaming heads within each layer, eliminating the need for mixed head computations and reducing tensor operations.

Result: The method achieves competitive performance with reduced latency compared to baselines, with only negligible performance degradation while significantly lowering overhead.

Conclusion: Exclusive head grouping per layer effectively optimizes KV cache memory footprint while minimizing computational latency, making ZigzagAttention a practical solution for efficient long-context LLM deployment.

Abstract: With the rapid development of large language models (LLMs), handling long
context has become one of the vital abilities in LLMs. Such long-context
ability is accompanied by difficulties in deployment, especially due to the
increased consumption of KV cache. There is certain work aiming to optimize the
memory footprint of KV cache, inspired by the observation that attention heads
can be categorized into retrieval heads that are of great significance and
streaming heads that are of less significance. Typically, identifying the
streaming heads and and waiving the KV cache in the streaming heads would
largely reduce the overhead without hurting the performance that much. However,
since employing both retrieval and streaming heads in one layer decomposes one
large round of attention computation into two small ones, it may unexpectedly
bring extra latency on accessing and indexing tensors. Based on this intuition,
we impose an important improvement to the identification process of retrieval
and streaming heads, in which we design a criterion that enforces exclusively
retrieval or streaming heads gathered in one unique layer. In this way, we
further eliminate the extra latency and only incur negligible performance
degradation. Our method named \textsc{ZigzagAttention} is competitive among
considered baselines owing to reduced latency and comparable performance.

</details>


### [37] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
*Emanuel Z. Fenech-Borg,Tilen P. Meznaric-Kos,Milica D. Lekovic-Bojovic,Arni J. Hentze-Djurhuus*

Main category: cs.CL

TL;DR: This paper introduces the concept of "cultural genes" in LLMs, showing systematic value differences between Western (GPT-4) and Eastern (ERNIE Bot) models across individualism-collectivism and power distance dimensions.


<details>
  <summary>Details</summary>
Motivation: To investigate the cultural biases and value orientations that large language models inherit from their training corpora, as current LLMs are deployed globally but their underlying cultural assumptions remain underexplored.

Method: Created a Cultural Probe Dataset (CPD) of 200 prompts targeting individualism-collectivism and power distance dimensions. Used standardized zero-shot prompts to compare GPT-4 (Western-centric) and ERNIE Bot (Eastern-centric), with human annotation and statistical analysis. Developed Cultural Alignment Index (CAI) to compare against Hofstede's national scores.

Result: Significant divergence found: GPT-4 shows individualistic/low-power-distance tendencies (IDV: 1.21, PDI: -1.05), while ERNIE Bot shows collectivistic/higher-power-distance tendencies (IDV: -0.89, PDI: 0.76). GPT-4 aligns more with USA cultural scores, ERNIE Bot with China. Differences statistically significant (p < 0.001).

Conclusion: LLMs function as statistical mirrors of their cultural training corpora. Findings motivate culturally aware evaluation and deployment to prevent algorithmic cultural hegemony in global AI systems.

Abstract: Large language models (LLMs) are deployed globally, yet their underlying
cultural and ethical assumptions remain underexplored. We propose the notion of
a "cultural gene" -- a systematic value orientation that LLMs inherit from
their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200
prompts targeting two classic cross-cultural dimensions:
Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized
zero-shot prompts, we compare a Western-centric model (GPT-4) and an
Eastern-centric model (ERNIE Bot). Human annotation shows significant and
consistent divergence across both dimensions. GPT-4 exhibits individualistic
and low-power-distance tendencies (IDV score approx 1.21; PDI score approx
-1.05), while ERNIE Bot shows collectivistic and higher-power-distance
tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically
significant (p < 0.001). We further compute a Cultural Alignment Index (CAI)
against Hofstede's national scores and find GPT-4 aligns more closely with the
USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns
more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative
analyses of dilemma resolution and authority-related judgments illustrate how
these orientations surface in reasoning. Our results support the view that LLMs
function as statistical mirrors of their cultural corpora and motivate
culturally aware evaluation and deployment to avoid algorithmic cultural
hegemony.

</details>


### [38] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: LLMs demonstrate in-context learning of physics through dynamics forecasting tasks, with performance improving with longer contexts. Sparse autoencoders reveal that learned features correlate with physical variables like energy.


<details>
  <summary>Details</summary>
Motivation: To understand the precise mechanisms and internal structures within LLMs that enable successful in-context learning across diverse tasks, using physics as a testbed due to its structured, real-world nature.

Method: Used dynamics forecasting tasks in physical systems to evaluate ICL capabilities, analyzed residual stream activations using sparse autoencoders (SAEs) to uncover learned features.

Result: Performance improves with longer input contexts, and SAE features correlate with key physical variables like energy, showing meaningful physical concepts are encoded during ICL.

Conclusion: Physics provides a tractable testbed for studying LLM reasoning, revealing that meaningful physical concepts emerge during in-context learning, broadening understanding of how LLMs learn.

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [39] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
*Ruirui Gao,Emily Johnson,Bowen Tan,Yanfei Qian*

Main category: cs.CL

TL;DR: M3PO is a novel preference optimization method that uses multimodal model guidance to efficiently select high-quality preference pairs from LVLM-generated candidates, combining multimodal alignment and self-consistency scores to identify challenging negative samples for improved DPO fine-tuning.


<details>
  <summary>Details</summary>
Motivation: High cost and inconsistency of human annotation for LVLM fine-tuning, and limitations of traditional SFT and preference optimization methods in efficiently leveraging model's generation space to find informative hard negative samples.

Method: Multimodal-Model-Guided Preference Optimization (M3PO) that selects preference pairs using M3P-Score combining Multimodal Alignment Score (external quality) and Self-Consistency/Confidence (internal belief), then applies DPO fine-tuning with LoRA on base LVLMs.

Result: M3PO consistently outperforms strong baselines (SFT, simulated RLHF, vanilla DPO, RM-DPO) across comprehensive multimodal instruction following benchmarks including MME-Bench, POPE, IFT, and Human Preference Score.

Conclusion: M3PO provides a data-efficient solution for enhancing LVLM capabilities in visual instruction following by intelligently selecting learning-valuable preference pairs from model-generated candidates, overcoming annotation cost and efficiency limitations of traditional methods.

Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex
multimodal instruction following, yet their development is often hindered by
the high cost and inconsistency of human annotation required for effective
fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)
and existing preference optimization methods like RLHF and DPO frequently
struggle to efficiently leverage the model's own generation space to identify
highly informative "hard negative" samples. To address these challenges, we
propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and
data-efficient method designed to enhance LVLMs' capabilities in visual
instruction following. M3PO intelligently selects the most "learning-valuable"
preference sample pairs from a diverse pool of LVLM-generated candidates. This
selection is driven by a sophisticated mechanism that integrates two crucial
signals: a Multimodal Alignment Score (MAS) to assess external quality and the
model's Self-Consistency / Confidence (log-probability) to gauge internal
belief. These are combined into a novel M3P-Score, which specifically
identifies preferred responses and challenging dispreferred responses that the
model might confidently generate despite being incorrect. These high-quality
preference pairs are then used for efficient Direct Preference Optimization
(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our
extensive experiments demonstrate that M3PO consistently outperforms strong
baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a
comprehensive suite of multimodal instruction following benchmarks (MME-Bench,
POPE, IFT, Human Pref. Score).

</details>


### [40] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
*Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: LoraxBench is a new benchmark for evaluating NLP models on 20 low-resource Indonesian languages across 6 diverse tasks, revealing significant performance gaps between Indonesian and other languages, and showing that register changes affect model performance.


<details>
  <summary>Details</summary>
Motivation: Indonesia has 700 languages but lags in NLP progress, particularly for low-resource languages. There's a need for comprehensive evaluation benchmarks to assess model performance across diverse Indonesian languages and tasks.

Method: Created LoraxBench covering 20 languages with 6 tasks: reading comprehension, open-domain QA, language inference, causal reasoning, translation, and cultural QA. Included two formality registers for three languages. Evaluated multilingual and region-focused LLMs.

Result: Benchmark proved challenging with visible performance discrepancy between Indonesian and low-resource languages. No clear advantage for region-specific models over general multilingual models. Register changes (especially high-level politeness like Krama Javanese) significantly affected model performance.

Conclusion: LoraxBench highlights the need for better NLP support for Indonesia's linguistic diversity, showing current models struggle with low-resource languages and formal registers not commonly found in social media data.

Abstract: As one of the world's most populous countries, with 700 languages spoken,
Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a
benchmark that focuses on low-resource languages of Indonesia and covers 6
diverse tasks: reading comprehension, open-domain QA, language inference,
causal reasoning, translation, and cultural QA. Our dataset covers 20
languages, with the addition of two formality registers for three languages. We
evaluate a diverse set of multilingual and region-focused LLMs and found that
this benchmark is challenging. We note a visible discrepancy between
performance in Indonesian and other languages, especially the low-resource
ones. There is no clear lead when using a region-specific model as opposed to
the general multilingual model. Lastly, we show that a change in register
affects model performance, especially with registers not commonly found in
social media, such as high-level politeness `Krama' Javanese.

</details>


### [41] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
*Ziqian Bi,Keyu Chen,Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: OpenAI's GPT-OSS models (20B and 120B) show that smaller sparse models can outperform larger ones, with 20B beating 120B on several benchmarks despite lower resource requirements, suggesting diminishing returns from scaling sparse architectures.


<details>
  <summary>Details</summary>
Motivation: To evaluate OpenAI's first open-weight LLMs since GPT-2 and compare sparse mixture-of-experts architectures against contemporary open source models to understand scaling efficiency and performance trade-offs.

Method: Evaluated both GPT-OSS variants against six contemporary open source LLMs (14.7B-235B parameters) across ten benchmarks covering general knowledge, math, code, multilingual tasks, and conversation. Used standardized inference settings with statistical validation via McNemar's test and effect size analysis.

Result: GPT-OSS-20B consistently outperformed GPT-OSS-120B on benchmarks like HumanEval and MMLU despite requiring substantially less memory and energy. Both models showed mid-tier overall performance with strengths in code generation and weaknesses in multilingual tasks.

Conclusion: Scaling sparse architectures may not yield proportional performance gains, highlighting the need for better optimization strategies and more efficient model selection for open source deployments.

Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large
language models since GPT-2 in 2019, comprising two mixture of experts
architectures with 120B and 20B parameters. We evaluated both variants against
six contemporary open source large language models ranging from 14.7B to 235B
parameters, representing both dense and sparse designs, across ten benchmarks
covering general knowledge, mathematical reasoning, code generation,
multilingual understanding, and conversational ability. All models were tested
in unquantised form under standardised inference settings, with statistical
validation using McNemars test and effect size analysis. Results show that
gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such
as HumanEval and MMLU, despite requiring substantially less memory and energy
per response. Both models demonstrate mid-tier overall performance within the
current open source landscape, with relative strength in code generation and
notable weaknesses in multilingual tasks. These findings provide empirical
evidence that scaling in sparse architectures may not yield proportional
performance gains, underscoring the need for further investigation into
optimisation strategies and informing more efficient model selection for future
open source deployments.

</details>


### [42] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
*Xiaomeng Zhu,R. Thomas McCoy,Robert Frank*

Main category: cs.CL

TL;DR: Large language models show similar syntactic bootstrapping behavior to humans - verb representations degrade more when syntax is removed than when co-occurrence information is removed, with mental verbs being particularly affected.


<details>
  <summary>Details</summary>
Motivation: To examine whether large language models exhibit syntactic bootstrapping behavior similar to human children, who use syntactic environments to learn verb meanings.

Method: Trained RoBERTa and GPT-2 on perturbed datasets where syntactic information was ablated, comparing the effects of removing syntactic cues versus co-occurrence information.

Result: Models' verb representation degraded more when syntactic cues were removed than when co-occurrence information was removed. Mental verbs (crucial for human syntactic bootstrapping) were more negatively impacted than physical verbs. Noun representations were more affected by co-occurrence distortion than syntax distortion.

Conclusion: The results reinforce the important role of syntactic bootstrapping in verb learning and demonstrate the viability of testing developmental hypotheses at scale through manipulating large language models' learning environments.

Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use
the syntactic environments in which a verb occurs to learn its meaning. In this
paper, we examine whether large language models exhibit a similar behavior. We
do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic
information is ablated. Our results show that models' verb representation
degrades more when syntactic cues are removed than when co-occurrence
information is removed. Furthermore, the representation of mental verbs, for
which syntactic bootstrapping has been shown to be particularly crucial in
human verb learning, is more negatively impacted in such training regimes than
physical verbs. In contrast, models' representation of nouns is affected more
when co-occurrences are distorted than when syntax is distorted. In addition to
reinforcing the important role of syntactic bootstrapping in verb learning, our
results demonstrated the viability of testing developmental hypotheses on a
larger scale through manipulating the learning environments of large language
models.

</details>


### [43] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
*Yuangang Li,Yiqing Shen,Yi Nian,Jiechao Gao,Ziyi Wang,Chenxiao Yu,Shawn Li,Jie Wang,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: CDCR-SFT is a supervised fine-tuning framework that trains LLMs to explicitly construct causal DAGs and perform reasoning over them, achieving state-of-the-art 95.33% accuracy on CLADDER and reducing hallucinations by 10% on HaluEval.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning approaches like Chain-of-Thought operate at linguistic token level rather than modeling underlying causal relationships, lacking ability to represent conditional independencies or satisfy causal identification assumptions, leading to logically inconsistent hallucinations.

Method: Introduces CDCR-SFT framework that trains LLMs to construct variable-level directed acyclic graphs (DAGs) and perform reasoning over them. Also presents CausalDR dataset with 25,368 samples containing input questions, explicit causal DAGs, graph-based reasoning traces, and validated answers.

Result: Achieves 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces hallucination on HaluEval with 10% improvements across four LLMs and eight tasks.

Conclusion: Explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs, demonstrating that causal DAG construction and reasoning significantly improves causal reasoning capabilities.

Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations
that appear coherent yet violate reasoning principles, with recent research
suggesting an inverse relationship between causal reasoning capabilities and
such hallucinations. However, existing reasoning approaches in LLMs, such as
Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic
token level rather than modeling the underlying causal relationships between
variables, lacking the ability to represent conditional independencies or
satisfy causal identification assumptions. To bridge this gap, we introduce
causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning
framework that trains LLMs to explicitly construct variable-level directed
acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a
dataset comprising 25,368 samples (CausalDR), where each sample includes an
input question, explicit causal DAG, graph-based reasoning trace, and validated
answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves
the causal reasoning capability with the state-of-the-art 95.33% accuracy on
CLADDER (surpassing human performance of 94.8% for the first time) and reduces
the hallucination on HaluEval with 10% improvements. It demonstrates that
explicit causal structure modeling in LLMs can effectively mitigate logical
inconsistencies in LLM outputs. Code is available at
https://github.com/MrLYG/CDCR-SFT.

</details>


### [44] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: CorrSteer is a new method that uses correlation between sample correctness and SAE activations to automatically select relevant features for steering LLMs, improving performance on various tasks without needing contrastive datasets.


<details>
  <summary>Details</summary>
Motivation: Existing Sparse Autoencoder methods for LLM steering require contrastive datasets or large activation storage, limiting their practical effectiveness in downstream applications.

Method: CorrSteer correlates sample correctness with SAE activations from generated tokens at inference time to select relevant features, then obtains steering coefficients from average activations to automate the pipeline.

Result: Improved performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks, with +4.1% MMLU improvement and +22.9% HarmBench improvement using only 4000 samples on Gemma 2 2B and LLaMA 3.1 8B.

Conclusion: Correlation-based selection provides an effective and scalable approach for automated SAE steering across language model applications, with selected features showing semantically meaningful patterns aligned with task requirements.

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


### [45] [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)
*Yu-Hsuan Fang,Tien-Hong Lo,Yao-Ting Sung,Berlin Chen*

Main category: cs.CL

TL;DR: First systematic study using Multimodal Large Language Models (MLLM) for Automated Speaking Assessment, showing superior performance in content and language evaluation but challenges in delivery assessment, addressed by proposed Speech-First Multimodal Training approach.


<details>
  <summary>Details</summary>
Motivation: Traditional ASA systems have modality limitations - text-based approaches lack acoustic information while audio-based methods miss semantic context. MLLMs offer opportunity for comprehensive assessment by processing both audio and text simultaneously.

Method: Proposed Speech-First Multimodal Training (SFMT) using curriculum learning principle to establish robust speech modeling foundations before cross-modal fusion. Systematic study of MLLM for comprehensive ASA across content, language use, and delivery aspects.

Result: MLLM-based systems elevated holistic assessment performance from PCC 0.783 to 0.846. SFMT achieved 4% absolute accuracy improvement in delivery aspect evaluation over conventional training approaches.

Conclusion: MLLMs show superior performance for comprehensive ASA, particularly in content and language aspects. SFMT training strategy effectively addresses delivery assessment challenges and paves new avenue for automated speaking assessment systems.

Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent
modality limitations: text-based approaches lack acoustic information while
audio-based methods miss semantic context. Multimodal Large Language Models
(MLLM) offer unprecedented opportunities for comprehensive ASA by
simultaneously processing audio and text within unified frameworks. This paper
presents a very first systematic study of MLLM for comprehensive ASA,
demonstrating the superior performance of MLLM across the aspects of content
and language use . However, assessment on the delivery aspect reveals unique
challenges, which is deemed to require specialized training strategies. We thus
propose Speech-First Multimodal Training (SFMT), leveraging a curriculum
learning principle to establish more robust modeling foundations of speech
before cross-modal synergetic fusion. A series of experiments on a benchmark
dataset show MLLM-based systems can elevate the holistic assessment performance
from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the
evaluation of the delivery aspect, achieving an absolute accuracy improvement
of 4% over conventional training approaches, which also paves a new avenue for
ASA.

</details>


### [46] [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)
*Maitreyi Chatterjee,Devansh Agarwal*

Main category: cs.CL

TL;DR: Semantic Anchoring improves LLM memory by adding linguistic structure to vector storage, boosting recall and coherence by 18% over RAG baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long-term interactions due to limited memory persistence. Current RAG systems use dense vectors that miss finer linguistic structures like syntax, discourse, and coreference.

Method: Hybrid agentic memory architecture combining dependency parsing, discourse relation tagging, and coreference resolution to create structured memory entries alongside vector storage.

Result: 18% improvement in factual recall and discourse coherence on long-term dialogue datasets compared to RAG baselines, with strong performance in ablation studies and human evaluations.

Conclusion: Enriching vector-based memory with explicit linguistic structures significantly enhances LLMs' long-term conversational capabilities and memory persistence.

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task
competence in conversational settings. However, their effectiveness in
multi-session and long-term interactions is hindered by limited memory
persistence. Typical retrieval-augmented generation (RAG) systems store
dialogue history as dense vectors, which capture semantic similarity but
neglect finer linguistic structures such as syntactic dependencies, discourse
relations, and coreference links. We propose Semantic Anchoring, a hybrid
agentic memory architecture that enriches vector-based storage with explicit
linguistic cues to improve recall of nuanced, context-rich exchanges. Our
approach combines dependency parsing, discourse relation tagging, and
coreference resolution to create structured memory entries. Experiments on
adapted long-term dialogue datasets show that semantic anchoring improves
factual recall and discourse coherence by up to 18% over strong RAG baselines.
We further conduct ablation studies, human evaluations, and error analysis to
assess robustness and interpretability.

</details>


### [47] [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)
*Yiqun Zhang,Hao Li,Jianhao Chen,Hangfan Zhang,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Avengers-Pro is a test-time routing framework that dynamically assigns queries to optimal LLMs based on performance-efficiency tradeoffs, achieving state-of-the-art results with significant cost savings.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing performance and efficiency in large language models by creating a unified solution for all performance-efficiency tradeoffs.

Method: Embeds and clusters incoming queries, then routes each to the most suitable model based on a performance-efficiency score, ensembling LLMs of varying capacities and efficiencies.

Result: Surpasses strongest single model (GPT-5-medium) by +7% average accuracy, matches strongest model performance at 27% lower cost, achieves ~90% performance at 63% lower cost, and establishes Pareto frontier dominance.

Conclusion: Avengers-Pro provides an effective framework for optimizing LLM deployment by dynamically routing queries to appropriate models, achieving superior performance-efficiency tradeoffs compared to single-model approaches.

Abstract: Balancing performance and efficiency is a central challenge in large language
model (LLM) advancement. GPT-5 addresses this with test-time routing,
dynamically assigning queries to either an efficient or a high-capacity model
during inference. In this work, we present Avengers-Pro, a test-time routing
framework that ensembles LLMs of varying capacities and efficiencies, providing
a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro
embeds and clusters incoming queries, then routes each to the most suitable
model based on a performance-efficiency score. Across 6 challenging benchmarks
and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and
Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a
performance-efficiency trade-off parameter, it can surpass the strongest single
model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the
average accuracy of the strongest single model at 27% lower cost, and reach
~90% of that performance at 63% lower cost. Last but not least, it achieves a
Pareto frontier, consistently yielding the highest accuracy for any given cost,
and the lowest cost for any given accuracy, among all single models. Code is
available at https://github.com/ZhangYiqun018/AvengersPro.

</details>


### [48] [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)
*Chi Wang,Min Gao,Zongwei Wang,Junwei Yin,Kai Shu,Chenghua Lin*

Main category: cs.CL

TL;DR: LIFE method detects LLM-generated fake news by analyzing prompt-induced linguistic fingerprints through word-level probability distribution reconstruction and key-fragment techniques.


<details>
  <summary>Details</summary>
Motivation: With the ease of generating fake news using large language models, existing detection methods focusing on textual content alone are insufficient as LLM-generated fake content often appears coherent and factually consistent, making subtle falsification traces hard to detect.

Method: Proposes Linguistic Fingerprints Extraction (LIFE) method that reconstructs word-level probability distributions to find discriminative patterns. Uses distributional divergence analysis to uncover statistically distinct probability shifts between LLM-generated real and fake news when maliciously prompted. Leverages key-fragment techniques to amplify subtle linguistic differences.

Result: LIFE achieves state-of-the-art performance in detecting LLM-generated fake news and maintains high performance in detecting human-written fake news.

Conclusion: The method successfully identifies prompt-induced linguistic fingerprints that serve as reliable indicators for detecting LLM-generated fake news, providing an effective solution to the growing threat of AI-generated misinformation.

Abstract: With the rapid development of large language models, the generation of fake
news has become increasingly effortless, posing a growing societal threat and
underscoring the urgent need for reliable detection methods. Early efforts to
identify LLM-generated fake news have predominantly focused on the textual
content itself; however, because much of that content may appear coherent and
factually consistent, the subtle traces of falsification are often difficult to
uncover. Through distributional divergence analysis, we uncover prompt-induced
linguistic fingerprints: statistically distinct probability shifts between
LLM-generated real and fake news when maliciously prompted. Based on this
insight, we propose a novel method named Linguistic Fingerprints Extraction
(LIFE). By reconstructing word-level probability distributions, LIFE can find
discriminative patterns that facilitate the detection of LLM-generated fake
news. To further amplify these fingerprint patterns, we also leverage
key-fragment techniques that accentuate subtle linguistic differences, thereby
improving detection reliability. Our experiments show that LIFE achieves
state-of-the-art performance in LLM-generated fake news and maintains high
performance in human-written fake news. The code and data are available at
https://anonymous.4open.science/r/LIFE-E86A.

</details>


### [49] [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)
*Tanay Nagar,Grigorii Khvatskii,Anna Sokol,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: Fine-tuning LLMs on synthetic code-switched text improves common sense reasoning performance in low-resource languages while maintaining high-resource language performance.


<details>
  <summary>Details</summary>
Motivation: LLMs perform worse in common sense reasoning tasks when prompted in low-resource languages compared to high-resource languages, creating unfair access to quality outputs across linguistic communities.

Method: Fine-tuning LLMs on synthetic code-switched text generated using controlled language-mixing methods, creating a new dataset from CommonSenseQA with three distinct language ratio configurations.

Result: Substantial improvements in low-resource language model performance while preserving or enhancing performance in high-resource languages.

Conclusion: Synthetic code-switched fine-tuning effectively bridges the performance gap between high and low-resource languages in LLM common sense reasoning tasks.

Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual
communication and understanding. However, LLMs perform worse in Common Sense
Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi
or Swahili compared to high-resource languages (HRLs) like English. Equalizing
this inconsistent access to quality LLM outputs is crucial to ensure fairness
for speakers of LRLs and across diverse linguistic communities. In this paper,
we propose an approach to bridge this gap in LLM performance. Our approach
involves fine-tuning an LLM on synthetic code-switched text generated using
controlled language-mixing methods. We empirically demonstrate that fine-tuning
LLMs on synthetic code-switched datasets leads to substantial improvements in
LRL model performance while preserving or enhancing performance in HRLs.
Additionally, we present a new dataset of synthetic code-switched text derived
from the CommonSenseQA dataset, featuring three distinct language ratio
configurations.

</details>


### [50] [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)
*Bishanka Seal,Rahul Seetharaman,Aman Bansal,Abhilash Nandy*

Main category: cs.CL

TL;DR: LLMs predict human misery scores from text descriptions using various prompting strategies, with few-shot approaches performing best. A novel gamified evaluation framework tests LLM capabilities in dynamic emotional reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore how well Large Language Models can predict human-perceived misery from natural language descriptions of real-world scenarios, moving beyond standard regression to dynamic emotional reasoning tasks.

Method: Framed as regression problem (0-100 scores), evaluated multiple prompting strategies: zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT embeddings. Introduced "Misery Game Show" - a gamified framework with structured rounds for ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning.

Result: Few-shot approaches consistently outperformed zero-shot baselines, demonstrating the value of contextual examples in affective prediction. The gamified evaluation showed LLMs' potential in dynamic emotional reasoning beyond standard regression.

Conclusion: LLMs show promise in predicting human misery scores, with contextual examples significantly improving performance. The gamified framework provides a more comprehensive evaluation of LLM capabilities in emotional reasoning tasks, highlighting their adaptability through corrective feedback.

Abstract: This study investigates the use of Large Language Models (LLMs) for
predicting human-perceived misery scores from natural language descriptions of
real-world scenarios. The task is framed as a regression problem, where the
model assigns a scalar value from 0 to 100 to each input statement. We evaluate
multiple prompting strategies, including zero-shot, fixed-context few-shot, and
retrieval-based prompting using BERT sentence embeddings. Few-shot approaches
consistently outperform zero-shot baselines, underscoring the value of
contextual examples in affective prediction. To move beyond static evaluation,
we introduce the "Misery Game Show", a novel gamified framework inspired by a
television format. It tests LLMs through structured rounds involving ordinal
comparison, binary classification, scalar estimation, and feedback-driven
reasoning. This setup enables us to assess not only predictive accuracy but
also the model's ability to adapt based on corrective feedback. The gamified
evaluation highlights the broader potential of LLMs in dynamic emotional
reasoning tasks beyond standard regression. Code and data link:
https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

</details>


### [51] [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)
*Xingshan Zeng,Weiwen Liu,Lingzhi Wang,Liangyou Li,Fei Mi,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: ToolACE-MT is a non-autoregressive framework that generates high-quality multi-turn agent dialogues through three stages: coarse initialization, iterative refinement, and offline verification, enabling efficient data generation for tool-augmented LLM scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing simulation-based data generation methods for agentic task-solving rely on costly autoregressive interactions between multiple LLM agents, limiting real-world performance.

Method: Three-stage framework: 1) Coarse-grained initialization builds dialogue skeleton, 2) Iterative refinement adds realistic complexities via mask-and-fill operations, 3) Offline verification ensures correctness with rule- and model-based checks.

Result: Experiments demonstrate ToolACE-MT enables efficient, effective and generalizable agentic data generation.

Conclusion: ToolACE-MT offers a new paradigm for high-quality data construction in tool-augmented LLM scenarios, addressing limitations of autoregressive approaches.

Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn,
multi-step interactions, often involving complex function calls and dynamic
user-agent exchanges. Existing simulation-based data generation methods for
such scenarios rely heavily on costly autoregressive interactions between
multiple LLM agents, thereby limiting real-world performance of agentic tasks.
In this paper, we propose a novel Non-Autoregressive Iterative Generation
framework, called ToolACE-MT, for constructing high-quality multi-turn agentic
dialogues. ToolACE-MT generates full conversational trajectories through three
stages: coarse-grained initialization, iterative refinement, and offline
verification. The initialization phase builds a structurally complete yet
semantically coarse dialogue skeleton; the iterative refinement phase
introduces realistic complexities and continued refinement via mask-and-fill
operations; and the offline verification phase ensures correctness and
coherence via rule- and model-based checks. Experiments demonstrate that
ToolACE-MT enables efficient, effective and generalizable agentic data
generation, offering a new paradigm for high-quality data construction in
tool-augmented LLM scenarios.

</details>


### [52] [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
*Weize Liu,Yongchi Zhao,Yijia Luo,Mingyu Xu,Jiaheng Liu,Yanan Li,Xiguo Hu,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: DESIGNER is a novel data synthesis pipeline that uses Design Logic concepts to generate 4.7 million challenging reasoning questions across 75 disciplines, significantly outperforming existing datasets in difficulty and diversity.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-step reasoning across diverse disciplines, and existing datasets lack both disciplinary breadth and structural depth needed to elicit robust reasoning behaviors.

Method: Leverages naturally available raw documents (book/web corpus) and introduces Design Logic concept that mimics human educator question-creation process. Uses LLMs to reverse-engineer 120,000+ design logics from existing questions and matches them with disciplinary source materials.

Result: Created two large-scale datasets: DLR-Book (3.04M questions) and DLR-Web (1.66M questions). Questions show substantially greater difficulty and diversity than baseline datasets. SFT experiments on Qwen3 models show significant performance improvements, even surpassing official model performance.

Conclusion: The DESIGNER pipeline successfully generates high-quality reasoning questions at scale, enabling better training of LLMs for complex multidisciplinary reasoning tasks.

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language tasks but still struggle with complex, multi-step reasoning,
particularly across diverse disciplines. Existing reasoning datasets often
either lack disciplinary breadth or the structural depth necessary to elicit
robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd
Reasoning data synthesis pipeline that leverages naturally available, extensive
raw documents (book corpus and web corpus) to generate multidisciplinary
challenging questions. A core innovation of our approach is the introduction of
a Design Logic concept, which mimics the question-creation process of human
educators. We use LLMs to reverse-engineer and abstract over 120,000 design
logics from existing questions across various disciplines. By matching these
design logics with disciplinary source materials, we are able to create
reasoning questions that far surpass the difficulty and diversity of existing
datasets. Based on this pipeline, we synthesized two large-scale reasoning
datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),
containing 3.04 million challenging questions synthesized from the book corpus,
and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging
questions from the web corpus. Our data analysis demonstrates that the
questions synthesized by our method exhibit substantially greater difficulty
and diversity than those in the baseline datasets. We validate the
effectiveness of these datasets by conducting SFT experiments on the
Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset
significantly outperforms existing multidisciplinary datasets of the same
volume. Training with the full datasets further enables the models to surpass
the multidisciplinary reasoning performance of the official Qwen3-8B and
Qwen3-4B models.

</details>


### [53] [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)
*Zhiyuan Ning,Tianle Gu,Jiaxin Song,Shixin Hong,Lingyu Li,Huacan Liu,Jie Li,Yixu Wang,Meng Lingyu,Yan Teng,Yingchun Wang*

Main category: cs.CL

TL;DR: LinguaSafe is a comprehensive multilingual safety benchmark with 45k entries across 12 languages, addressing gaps in LLM safety evaluation for underrepresented languages through translated, transcreated, and natively-sourced data.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness and hinders robust multilingual safety alignment, necessitating better tools for assessing LLM safety across diverse linguistic and cultural contexts.

Method: Created LinguaSafe dataset comprising 45k entries in 12 languages using a combination of translated, transcreated, and natively-sourced data. Developed a multidimensional evaluation framework with direct/indirect safety assessments and oversensitivity evaluations.

Result: Safety and helpfulness evaluations vary significantly across different domains and languages, even among languages with similar resource levels, highlighting the importance of thorough multilingual safety assessment.

Conclusion: LinguaSafe provides comprehensive metrics for in-depth safety evaluation and underscores the critical importance of assessing multilingual safety in LLMs to achieve balanced safety alignment. The dataset and code are publicly released to facilitate further research.

Abstract: The widespread adoption and increasing prominence of large language models
(LLMs) in global technologies necessitate a rigorous focus on ensuring their
safety across a diverse range of linguistic and cultural contexts. The lack of
a comprehensive evaluation and diverse data in existing multilingual safety
evaluations for LLMs limits their effectiveness, hindering the development of
robust multilingual safety alignment. To address this critical gap, we
introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted
with meticulous attention to linguistic authenticity. The LinguaSafe dataset
comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated
using a combination of translated, transcreated, and natively-sourced data, our
dataset addresses the critical need for multilingual safety evaluations of
LLMs, filling the void in the safety evaluation of LLMs across diverse
under-represented languages from Hungarian to Malay. LinguaSafe presents a
multidimensional and fine-grained evaluation framework, with direct and
indirect safety assessments, including further evaluations for oversensitivity.
The results of safety and helpfulness evaluations vary significantly across
different domains and different languages, even in languages with similar
resource levels. Our benchmark provides a comprehensive suite of metrics for
in-depth safety evaluation, underscoring the critical importance of thoroughly
assessing multilingual safety in LLMs to achieve more balanced safety
alignment. Our dataset and code are released to the public to facilitate
further research in the field of multilingual LLM safety.

</details>


### [54] [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)
*Shaoming Duan,Zirui Wang,Chuanyi Liu,Zhibin Zhu,Yuhao Zhang,Peiyi Han,Liang Yan,Zewu Penge*

Main category: cs.CL

TL;DR: CRED-SQL introduces cluster-based schema retrieval and an intermediate Execution Description Language to bridge semantic gaps between natural language questions and SQL queries in large-scale databases, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Address semantic mismatch between natural language questions and SQL queries in large-scale databases, where semantically similar attributes cause schema linking issues and semantic drift during SQL generation, reducing model accuracy.

Method: CRED-SQL framework with two key components: 1) Cluster-based large-scale schema retrieval to identify relevant tables/columns, and 2) Intermediate Execution Description Language (EDL) that decomposes Text-to-SQL into Text-to-EDL and EDL-to-SQL stages to leverage LLMs' reasoning while reducing semantic deviation.

Result: Extensive experiments on SpiderUnion and BirdUnion benchmarks demonstrate CRED-SQL achieves new state-of-the-art performance, validating effectiveness and scalability for large-scale cross-domain databases.

Conclusion: CRED-SQL successfully addresses semantic mismatch challenges in large-scale Text-to-SQL through innovative cluster retrieval and intermediate language representation, providing a scalable solution that outperforms existing methods.

Abstract: Recent advances in large language models (LLMs) have significantly improved
the accuracy of Text-to-SQL systems. However, a critical challenge remains: the
semantic mismatch between natural language questions (NLQs) and their
corresponding SQL queries. This issue is exacerbated in large-scale databases,
where semantically similar attributes hinder schema linking and semantic drift
during SQL generation, ultimately reducing model accuracy. To address these
challenges, we introduce CRED-SQL, a framework designed for large-scale
databases that integrates Cluster Retrieval and Execution Description. CRED-SQL
first performs cluster-based large-scale schema retrieval to pinpoint the
tables and columns most relevant to a given NLQ, alleviating schema mismatch.
It then introduces an intermediate natural language representation-Execution
Description Language (EDL)-to bridge the gap between NLQs and SQL. This
reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,
leveraging LLMs' strong general reasoning capabilities while reducing semantic
deviation. Extensive experiments on two large-scale, cross-domain
benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new
state-of-the-art (SOTA) performance, validating its effectiveness and
scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

</details>


### [55] [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)
*Javier Garcia Gilabert,Xixian Liao,Severino Da Dalt,Ella Bohman,Audrey Mash,Francesca De Luca Fornaciari,Irene Baucells,Joan Llop,Miguel Claramunt Argote,Carlos Escolano,Maite Melero*

Main category: cs.CL

TL;DR: SALAMANDRATA family of 2B and 7B parameter models optimized for translation tasks across 38 European languages, featuring continual pre-training and supervised fine-tuning, with quality-aware decoding strategies.


<details>
  <summary>Details</summary>
Motivation: To improve machine translation performance for European languages and adapt models for the WMT25 shared task with additional non-European languages.

Method: Two-step training: continual pre-training on parallel data followed by supervised fine-tuning on high-quality instructions. Vocabulary adaptation for non-European languages, plus quality-aware decoding using Minimum Bayes Risk and COMET-based re-ranking.

Result: Developed SALAMANDRATA models in 2B and 7B parameter versions, with an enhanced SALAMANDRATA-V2 model, all publicly released on Hugging Face.

Conclusion: The SALAMANDRATA family provides effective translation models for European languages with scalable architecture and advanced decoding techniques for quality optimization.

Abstract: In this paper, we present the SALAMANDRATA family of models, an improved
iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically
trained to achieve strong performance in translation-related tasks for 38
European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For
both versions, we applied the same training recipe with a first step of
continual pre-training on parallel data, and a second step of supervised
fine-tuning on high-quality instructions. The BSC submission to the WMT25
General Machine Translation shared task is based on the 7B variant of
SALAMANDRATA. We first adapted the model vocabulary to support the additional
non-European languages included in the task. This was followed by a second
phase of continual pre-training and supervised fine-tuning, carefully designed
to optimize performance across all translation directions for this year's
shared task. For decoding, we employed two quality-aware strategies: Minimum
Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI
respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,
along with the newer SALAMANDRATA-V2 model, on Hugging Face1

</details>


### [56] [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)
*Zhe Chen,Yusheng Liao,Shuyang Jiang,Zhiyuan Zhu,Haolin Li,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: MedAtlas framework with HeteroRAG improves medical vision-language models by enabling effective retrieval across heterogeneous medical sources, significantly enhancing factual accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Medical LVLMs suffer from factual inaccuracies and unreliable outputs that pose risks in clinical diagnostics, while current multimodal RAG systems cannot effectively retrieve from heterogeneous medical sources.

Method: Constructed MedAtlas with multimodal report repositories and text corpora, then developed HeteroRAG framework with Modality-specific CLIPs for report retrieval, Multi-corpora Query Generator, and Heterogeneous Knowledge Preference Tuning for cross-modality alignment.

Result: Achieved state-of-the-art performance across 12 datasets and 3 modalities in medical vision language benchmarks, significantly improving factual accuracy and reliability of Med-LVLMs.

Conclusion: HeteroRAG framework successfully bridges the gap in heterogeneous knowledge retrieval for medical applications, providing a robust solution to enhance the factuality and reliability of clinical decision-making systems.

Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in
clinical applications but suffer from factual inaccuracies and unreliable
outputs, posing risks in real-world diagnostics. While retrieval-augmented
generation has emerged as a potential solution, current medical multimodal RAG
systems are unable to perform effective retrieval across heterogeneous sources.
The irrelevance of retrieved reports affects the factuality of analysis, while
insufficient knowledge affects the credibility of clinical decision-making. To
bridge the gap, we construct MedAtlas, which includes extensive multimodal
report repositories and diverse text corpora. Based on it, we present
HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous
knowledge sources. The framework introduces Modality-specific CLIPs for
effective report retrieval and a Multi-corpora Query Generator for dynamically
constructing queries for diverse corpora. Incorporating knowledge from such
multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge
Preference Tuning to achieve cross-modality and multi-source knowledge
alignment. Extensive experiments across 12 datasets and 3 modalities
demonstrate that the proposed HeteroRAG achieves state-of-the-art performance
in most medical vision language benchmarks, significantly improving factual
accuracy and reliability of Med-LVLMs.

</details>


### [57] [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
*Yong Deng,Guoqing Wang,Zhenzhe Ying,Xiaofeng Wu,Jinzhen Lin,Wenwen Xiong,Yuqin Dai,Shuo Yang,Zhanwei Zhang,Qiwen Wang,Yang Qin,Changhua Meng*

Main category: cs.CL

TL;DR: Atom-Searcher is a novel RL framework that uses fine-grained Atomic Thought units with Reasoning Reward Models to improve agentic deep research, overcoming limitations of traditional RAG and outcome-based RL approaches.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-hop reasoning and strategic search due to static knowledge and rigid workflows. Current agentic approaches using outcome-based RL face issues like conflicting gradients and reward sparsity.

Method: Proposes Atomic Thought paradigm that decomposes reasoning into fine-grained functional units supervised by Reasoning Reward Models (RRMs) with Atomic Thought Rewards (ATR). Atom-Searcher integrates this with a curriculum-inspired reward schedule that prioritizes process-level rewards early and transitions to outcome rewards.

Result: Experiments on seven benchmarks show consistent improvements over state-of-the-art methods. The approach enables scalable computation, provides supervision anchors for RRMs, and exhibits more interpretable, human-like reasoning patterns.

Conclusion: Atom-Searcher effectively addresses limitations of current agentic deep research approaches by providing fine-grained guidance through Atomic Thought units and optimized reward scheduling, leading to superior performance and more interpretable reasoning.

Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities,
but struggle with complex tasks due to static internal knowledge.
Retrieval-Augmented Generation (RAG) enhances access to external information,
yet remains limited in multi-hop reasoning and strategic search due to rigid
workflows. Recent advancements in agentic deep research empower LLMs to
autonomously reason, search, and synthesize information. However, current
approaches relying on outcome-based reinforcement learning (RL) face critical
issues such as conflicting gradients and reward sparsity, limiting performance
gains and training efficiency. To address these, we first propose Atomic
Thought, a novel LLM thinking paradigm that decomposes reasoning into
fine-grained functional units. These units are supervised by Reasoning Reward
Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained
guidance. Building on this, we propose Atom-Searcher, a novel RL framework for
agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher
uses a curriculum-inspired reward schedule, prioritizing process-level ATR
early and transitioning to outcome rewards, accelerating convergence on
effective reasoning paths. Experiments on seven benchmarks show consistent
improvements over the state-of-the-art. Key advantages include: (1)
Atom-Searcher scales computation at test-time. (2) Atomic Thought provides
supervision anchors for RRMs, bridging deep research tasks and RRMs. (3)
Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

</details>


### [58] [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)
*Ahmed Elshabrawy,Hour Kaing,Haiyue Song,Alham Fikri Aji,Hideki Tanaka,Masao Utiyama,Raj Dabre*

Main category: cs.CL

TL;DR: Excessive entanglement with high-resource languages like Modern Standard Arabic hinders dialect modeling; new variational probing framework enables subspace decoupling, improving dialect generation by +2.0 chrF++ on average.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that alignment with high-resource standard languages aids modeling of related low-resource varieties, demonstrating that representational entanglement can actively hinder generative modeling of dialects.

Method: Online variational probing framework that continuously estimates the standard variety subspace during fine-tuning, enabling projection-based decoupling from this space. Uses Arabic as case study with 25 dialects.

Result: Intervention improves generation quality by up to +4.9 chrF++ and +2.0 on average across 25 dialects compared to standard fine-tuning, despite tradeoff in standard-language performance.

Conclusion: Provides causal evidence that subspace dominance by high-resource varieties restricts generative capacity for related varieties, offering practical tools for controlling representational allocation in multilingual LLMs.

Abstract: Alignment with high-resource standard languages is often assumed to aid the
modeling of related low-resource varieties. We challenge this assumption by
demonstrating that excessive representational entanglement with a dominant
variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,
can actively hinder generative modeling. We present the first comprehensive
causal study of this phenomenon by analyzing and directly intervening in the
internal representation geometry of large language models (LLMs). Our key
contribution is an online variational probing framework that continuously
estimates the subspace of the standard variety during fine-tuning, enabling
projection-based decoupling from this space. While our study uses Arabic as a
case due to its unusually rich parallel resources across 25 dialects, the
broader motivation is methodological: dialectal MT serves as a controlled proxy
for generative tasks where comparable multi-variety corpora are unavailable.
Across 25 dialects, our intervention improves generation quality by up to +4.9
chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured
tradeoff in standard-language performance. These results provide causal
evidence that subspace dominance by high-resource varieties can restrict
generative capacity for related varieties. More generally, we unify geometric
and information-theoretic probing with subspace-level causal interventions,
offering practical tools for improving generative modeling in closely related
language families and, more broadly, for controlling representational
allocation in multilingual and multi-domain LLMs. Code will be released.

</details>


### [59] [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)
*Jeongwoo Kang,Maria Boritchev,Maximin Coavoux*

Main category: cs.CL

TL;DR: Building a French semantic corpus by annotating spontaneous dialogues with extended Abstract Meaning Representation (AMR) framework to better handle French-specific structures and spontaneous speech dynamics.


<details>
  <summary>Details</summary>
Motivation: To develop semantic resources for French dialogue and address AMR's insufficient coverage for spontaneous speech and French-specific sentence structures.

Method: Annotated the DinG corpus (French dialogue transcripts from Catan board game) using extended AMR framework, created annotation guidelines, trained and evaluated an AMR parser for assistance.

Result: Created a French semantic dialogue corpus published under CC-SA-BY license, developed an AMR parser that can serve as an annotation assistance tool.

Conclusion: This work contributes valuable semantic resources for French dialogue processing and provides tools to support future annotation efforts in this domain.

Abstract: We present our work to build a French semantic corpus by annotating French
dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate
the DinG corpus, consisting of transcripts of spontaneous French dialogues
recorded during the board game Catan. As AMR has insufficient coverage of the
dynamics of spontaneous speech, we extend the framework to better represent
spontaneous speech and sentence structures specific to French. Additionally, to
support consistent annotation, we provide an annotation guideline detailing
these extensions. We publish our corpus under a free license (CC-SA-BY). We
also train and evaluate an AMR parser on our data. This model can be used as an
assistance annotation tool to provide initial annotations that can be refined
by human annotators. Our work contributes to the development of semantic
resources for French dialogue.

</details>


### [60] [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)
*Raneem Alharthi,Rajwa Alharthi,Aiqi Jiang,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: Using parent tweet context improves abusive language detection in replies, with content-based features being more important than account-based features.


<details>
  <summary>Details</summary>
Motivation: Existing abusive language detection research focuses on individual posts, overlooking contextual information from conversational exchanges that could improve detection accuracy.

Method: Tested four classification models using content-based and account-based features from parent-reply tweet pairs, comparing contextual features vs reply-only features.

Result: Incorporating contextual features from parent tweets led to substantial improvements in abusive language detection compared to using reply-only features.

Conclusion: Contextual features, particularly content-based ones, are crucial for effective abusive language detection in conversational settings, and combining multiple features yields best performance.

Abstract: Abusive language detection has become an increasingly important task as a
means to tackle this type of harmful content in social media. There has been a
substantial body of research developing models for determining if a social
media post is abusive or not; however, this research has primarily focused on
exploiting social media posts individually, overlooking additional context that
can be derived from surrounding posts. In this study, we look at conversational
exchanges, where a user replies to an earlier post by another user (the parent
tweet). We ask: does leveraging context from the parent tweet help determine if
a reply post is abusive or not, and what are the features that contribute the
most? We study a range of content-based and account-based features derived from
the context, and compare this to the more widely studied approach of only
looking at the features from the reply tweet. For a more generalizable study,
we test four different classification models on a dataset made of
conversational exchanges (parent-reply tweet pairs) with replies labeled as
abusive or not. Our experiments show that incorporating contextual features
leads to substantial improvements compared to the use of features derived from
the reply tweet only, confirming the importance of leveraging context. We
observe that, among the features under study, it is especially the
content-based features (what is being posted) that contribute to the
classification performance rather than account-based features (who is posting
it). While using content-based features, it is best to combine a range of
different features to ensure improved performance over being more selective and
using fewer features. Our study provides insights into the development of
contextualized abusive language detection models in realistic settings
involving conversations.

</details>


### [61] [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)
*Jan Maliszewski*

Main category: cs.CL

TL;DR: Stylometric analysis of medieval scholastic reportationes to detect editorial layers and validate collection formation hypotheses using computational methods.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of sources commenting on early scholastic oral teaching practices and uncover editorial work in collaborative literary production from medieval universities.

Method: Implementation of HTR pipeline and stylometric analysis using frequent words, POS tags, and pseudo-affixes, comparing manually composed vs automatically extracted data, and testing transformer-based OCR/transcription alignment.

Result: The study proposes methodological gains for computational research on scholastic tradition but results are not yet available as this describes a planned study.

Conclusion: If successful, this research will provide a reusable template for exploratory analysis of collaborative medieval literary production and validate computational approaches for scholastic Latin corpora.

Abstract: While the indirect evidence suggests that already in the early scholastic
period the literary production based on records of oral teaching (so-called
reportationes) was not uncommon, there are very few sources commenting on the
practice. This paper details the design of a study applying stylometric
techniques of authorship attribution to a collection developed from
reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover
layers of editorial work and thus validate some hypotheses regarding the
collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I
discuss the implementation of an HTR pipeline and stylometric analysis based on
the most frequent words, POS tags, and pseudo-affixes. The proposed study will
offer two methodological gains relevant to computational research on the
scholastic tradition: it will directly compare performance on manually composed
and automatically extracted data, and it will test the validity of
transformer-based OCR and automated transcription alignment for workflows
applied to scholastic Latin corpora. If successful, this study will provide an
easily reusable template for the exploratory analysis of collaborative literary
production stemming from medieval universities.

</details>


### [62] [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)
*Jumbly Grindrod,Peter Grindrod*

Main category: cs.CL

TL;DR: Transformer language models encode rich semantic information in their token embeddings, challenging meaning eliminativist views of how LLMs process meaning.


<details>
  <summary>Details</summary>
Motivation: To investigate whether transformer models use something analogous to a lexical store where words have semantic entries, and to test if token embeddings contain various types of semantic information.

Method: Extracted token embeddings from RoBERTa-base, performed k-means clustering into 200 clusters, then manually inspected clusters for semantic sensitivity and tested against five psycholinguistic measures (valence, concreteness, iconicity, taboo, and age of acquisition).

Result: Findings were very positive - wide variety of semantic information encoded within token embedding space, with clusters showing sensitivity to semantic information.

Conclusion: Transformer LLMs do encode semantic information in their token embeddings, ruling out meaning eliminativist hypotheses about how these models process semantic information.

Abstract: We investigate how word meanings are represented in the transformer language
models. Specifically, we focus on whether transformer models employ something
analogous to a lexical store - where each word has an entry that contains
semantic information. To do this, we extracted the token embedding space of
RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we
then manually inspected the resultant clusters to consider whether they are
sensitive to semantic information. In our second study, we tested whether the
clusters are sensitive to five psycholinguistic measures: valence,
concreteness, iconicity, taboo, and age of acquisition. Overall, our findings
were very positive - there is a wide variety of semantic information encoded
within the token embedding space. This serves to rule out certain "meaning
eliminativist" hypotheses about how transformer LLMs process semantic
information.

</details>


### [63] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
*Yilin Geng,Shujing Wang,Chuan Wang,Keqing He,Yanfei Lv,Ying Wang,Zaiwen Feng,Xiaoying Bai*

Main category: cs.CL

TL;DR: LLM-based agent approach for semantic table annotation using ReAct framework with external tools, achieving state-of-the-art performance while reducing time and token costs by 70% and 60% respectively.


<details>
  <summary>Details</summary>
Motivation: Complex tables present challenges like semantic loss, strict ontological hierarchies, homonyms, spelling errors, and abbreviations that hinder accurate semantic table annotation (STA) including Column Type Annotation (CTA) and Cell Entity Annotation (CEA).

Method: Proposes an LLM-based agent approach with five external tools using ReAct framework, enabling dynamic selection of annotation strategies based on table characteristics. Uses Levenshtein distance to reduce redundant annotations.

Result: Outperforms existing approaches on Tough Tables and BiodivTab datasets from SemTab challenge across various metrics. Achieves 70% reduction in time costs and 60% reduction in LLM token usage.

Conclusion: Provides an efficient and cost-effective solution for semantic table annotation that handles complex table challenges while significantly reducing computational resources.

Abstract: The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

</details>


### [64] [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
*Jinyi Han,Xinyi Wang,Haiquan Zhao,Tingyun li,Zishang Jiang,Sihang Jiang,Jiaqing Liang,Xin Lin,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.CL

TL;DR: PASR enables LLMs to dynamically refine outputs during generation by proactively deciding when and how to refine based on internal state and context, reducing token usage by 41.6% while improving accuracy by 8.2%.


<details>
  <summary>Details</summary>
Motivation: Existing self-refinement methods use fixed iteration processes that cannot adapt to evolving generation context, unlike how humans dynamically refine thoughts during execution.

Method: ProActive Self-Refinement (PASR) allows LLMs to refine outputs during generation by making proactive decisions about whether, when, and how to refine based on the model's internal state and evolving context.

Result: PASR significantly enhances problem-solving performance across 10 diverse tasks, reducing average token consumption by 41.6% compared to standard generation while achieving 8.2% accuracy improvement on Qwen3-8B.

Conclusion: PASR demonstrates that proactive, context-aware refinement during generation is more efficient and effective than reactive post-generation refinement with fixed iterations.

Abstract: Recent advances in self-refinement have demonstrated significant potential
for improving the outputs of large language models (LLMs) through iterative
refinement. However, most existing self-refinement methods rely on a reactive
process with a fixed number of iterations, making it difficult to determine the
optimal timing and content of refinement based on the evolving generation
context. Inspired by the way humans dynamically refine their thoughts during
execution, we propose ProActive Self-Refinement (PASR), a novel method that
enables LLMs to refine their outputs during the generation process. Unlike
methods that regenerate entire responses, PASR proactively decides whether,
when, and how to refine based on the model's internal state and evolving
context. We conduct extensive experiments on a diverse set of 10 tasks to
evaluate the effectiveness of PASR. Experimental results show that PASR
significantly enhances problem-solving performance. In particular, on Qwen3-8B,
PASR reduces average token consumption by 41.6 percent compared to standard
generation, while also achieving an 8.2 percent improvement in accuracy. Our
code and all baselines used in the paper are available in the GitHub.

</details>


### [65] [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)
*Tianyue Ou,Saujas Vaduguru,Daniel Fried*

Main category: cs.CL

TL;DR: LLM-based multi-agent system with notebook for information sharing and orchestrator agent improves travel planning performance by 17.5% over single-agent baseline.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems struggle with long-horizon, multi-constraint planning tasks that require detailed information conditioning and complex interdependent constraints.

Method: Constructed an LLM-based MAS for travel planning with two key mechanisms: a notebook for information sharing and an orchestrator agent for coordination in free-form conversations.

Result: Notebook reduced hallucination errors by 18%, orchestrator reduced errors by up to 13.5% in focused areas. Combined system achieved 25% pass rate on TravelPlanner benchmark vs 7.5% for single-agent baseline.

Conclusion: Structured information sharing and reflective orchestration are key components for effective multi-agent systems in long-horizon planning with LLMs.

Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model
(LLM) agents in domains such as web research and software engineering. However,
long-horizon, multi-constraint planning tasks involve conditioning on detailed
information and satisfying complex interdependent constraints, which can pose a
challenge for these systems. In this study, we construct an LLM-based MAS for a
travel planning task which is representative of these challenges. We evaluate
the impact of a notebook to facilitate information sharing, and evaluate an
orchestrator agent to improve coordination in free form conversation between
agents. We find that the notebook reduces errors due to hallucinated details by
18%, while an orchestrator directs the MAS to focus on and further reduce
errors by up to 13.5% within focused sub-areas. Combining both mechanisms
achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute
improvement over the single-agent baseline's 7.5% pass rate. These results
highlight the potential of structured information sharing and reflective
orchestration as key components in MASs for long horizon planning with LLMs.

</details>


### [66] [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)
*Ralph Peeters,Aaron Steiner,Luca Schwarz,Julian Yuya Caspary,Christian Bizer*

Main category: cs.CL

TL;DR: WebMall is a new benchmark for evaluating web agents on multi-shop comparison shopping tasks with authentic product data from Common Crawl, featuring 91 cross-shop tasks that require longer interaction trajectories than existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing e-commerce benchmarks like WebShop and ShoppingBench lack multi-shop comparison shopping capabilities and use less heterogeneous product data. There's a need for more realistic shopping scenarios that involve navigating multiple online shops for comparison shopping.

Method: Created WebMall benchmark with four simulated online shops populated with authentic product offers from Common Crawl, featuring 91 cross-shop tasks including basic tasks (finding products, price comparisons, checkout) and advanced tasks (vague requirements, substitutes, compatibility). Evaluated eight baseline agents with different observation modalities, memory utilization, and LLMs (GPT 4.1 and Claude Sonnet 4).

Result: Best-performing configurations achieved 75% completion rate and 87% F1 score on basic tasks, and 53% completion rate with 63% F1 score on advanced tasks. The benchmark requires longer interaction trajectories than WebShop while maintaining real-world shopping behavior representation.

Conclusion: WebMall provides a comprehensive benchmark for evaluating web agents on realistic multi-shop comparison shopping tasks, demonstrating significant performance gaps between basic and advanced tasks, and is publicly released to advance research in web agent navigation, reasoning, and efficiency for e-commerce applications.

Abstract: LLM-based web agents have the potential to automate long-running web tasks,
such as finding offers for specific products in multiple online shops and
subsequently ordering the cheapest products that meet the users needs. This
paper introduces WebMall, a multi-shop online shopping benchmark for evaluating
the effectiveness and efficiency of web agents for comparison-shopping. WebMall
consists of four simulated online shops populated with authentic product offers
sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These
tasks include basic tasks such as finding specific products in multiple shops,
performing price comparisons, adding items to the shopping cart, and completing
checkout. Advanced tasks involve searching for products based on vague
requirements, identifying suitable substitutes, and finding compatible
products. Compared to existing e-commerce benchmarks, such as WebShop or
ShoppingBench, WebMall introduces comparison-shopping tasks across multiple
shops. Furthermore, the product offers are more heterogeneous, as they
originate from hundreds of distinct real-world shops. The tasks in WebMall
require longer interaction trajectories than those in WebShop, while remaining
representative of real-world shopping behaviors. We evaluate eight baseline
agents on WebMall, varying in observation modality, memory utilization, and
underlying large language model (GPT 4.1 and Claude Sonnet 4). The
best-performing configurations achieve completion rates of 75% and 53%, and F1
scores of 87% and 63%, on the basic and advanced task sets, respectively.
WebMall is publicly released to facilitate research on web agents and to
promote advancements in navigation, reasoning, and efficiency within e-commerce
scenarios.

</details>


### [67] [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Devraj Raghuvanshi,Nagendra Kumar,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: Novel approach for sarcastic speech synthesis using feedback loss from bi-modal sarcasm detection and two-stage transfer learning to improve sarcasm-aware speech generation.


<details>
  <summary>Details</summary>
Motivation: Sarcastic speech synthesis is essential for natural human-computer interaction but challenging due to nuanced prosody and limited annotated sarcastic speech data.

Method: Integrates feedback loss from bi-modal sarcasm detection model into TTS training, plus two-stage fine-tuning: first on diverse speech styles, then specifically on sarcastic speech data.

Result: Objective and subjective evaluations show improved quality, naturalness, and sarcasm-awareness of synthesized speech.

Conclusion: The proposed methods effectively enhance sarcastic speech synthesis by leveraging transfer learning and sarcasm detection feedback, addressing data limitations and prosodic challenges.

Abstract: Sarcastic speech synthesis, which involves generating speech that effectively
conveys sarcasm, is essential for enhancing natural interactions in
applications such as entertainment and human-computer interaction. However,
synthesizing sarcastic speech remains a challenge due to the nuanced prosody
that characterizes sarcasm, as well as the limited availability of annotated
sarcastic speech data. To address these challenges, this study introduces a
novel approach that integrates feedback loss from a bi-modal sarcasm detection
model into the TTS training process, enhancing the model's ability to capture
and convey sarcasm. In addition, by leveraging transfer learning, a speech
synthesis model pre-trained on read speech undergoes a two-stage fine-tuning
process. First, it is fine-tuned on a diverse dataset encompassing various
speech styles, including sarcastic speech. In the second stage, the model is
further refined using a dataset focused specifically on sarcastic speech,
enhancing its ability to generate sarcasm-aware speech. Objective and
subjective evaluations demonstrate that our proposed methods improve the
quality, naturalness, and sarcasm-awareness of synthesized speech.

</details>


### [68] [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)
*Xinhe Li,Jiajun Liu,Peng Wang*

Main category: cs.CL

TL;DR: LoRID is a novel method that uses multi-LoRA interaction to enhance mathematical reasoning in Small Language Models by mimicking human System 1 and System 2 thinking processes through knowledge generation and iterative reasoning verification.


<details>
  <summary>Details</summary>
Motivation: Small Language Models (SLMs) have poor mathematical reasoning abilities compared to Large Language Models (LLMs), and existing methods rely too heavily on cramming training data without incorporating the dual thinking systems observed in human learning psychology.

Method: The method involves: 1) Creating knowledge-enhanced datasets using LLMs, 2) Training an Intuitive Reasoner (IR) for direct Chain-of-Thought generation, 3) Training Knowledge Generator (KG) and Deep Reasoner (DR) to mimic System 2 thinking, and 4) Implementing iterative verification where IR and DR outputs are compared and refined through mutual feedback.

Result: LoRID achieves state-of-the-art performance, particularly on GSM8K dataset, outperforming the second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across five different base models.

Conclusion: The proposed multi-LoRA interaction approach successfully enhances mathematical reasoning in SLMs by incorporating both intuitive and deliberate thinking processes, demonstrating that psychological principles can be effectively applied to improve AI reasoning capabilities.

Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have
strong mathematical reasoning abilities but rely on hundreds of billions of
parameters. To tackle the challenge of poor reasoning in Small Language Models
(SLMs), existing methods typically leverage LLMs to generate massive amounts of
data for cramming training. In psychology, they are akin to System 1 thinking,
which resolves reasoning problems rapidly based on experience and intuition.
However, human learning also requires System 2 thinking, where knowledge is
first acquired and then reinforced through practice. Inspired by such two
distinct modes of thinking, we propose a novel method based on the multi-LoRA
Interaction for mathematical reasoning Distillation (LoRID). First, we input
the question and reasoning of each sample into an LLM to create
knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student
model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts
for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge
Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only
knowledge after receiving problems, while the latter uses that knowledge to
perform reasoning. Finally, to address the randomness in the generation of IR
and DR, we evaluate whether their outputs are consistent, and the inference
process needs to be iterated if not. This step can enhance the mathematical
reasoning ability of SLMs through mutual feedback. Experimental results show
that LoRID achieves state-of-the-art performance, especially on the GSM8K
dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,
12.3%, and 1.8% accuracy across the five base models, respectively.

</details>


### [69] [BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans DeÄŸerlendirmesi, Zorluklar ve Ä°yileÅŸtirme FÄ±rsatlarÄ±](https://arxiv.org/abs/2508.13044)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih GÃ¼mÃ¼ÅŸ,Banu Diri,SavaÅŸ YÄ±ldÄ±rÄ±m,Ã–ner AytaÅŸ*

Main category: cs.CL

TL;DR: Introduction of TR-MMLU benchmark - a comprehensive evaluation framework with 6,200 multiple-choice questions to assess large language models' capabilities in Turkish, addressing the lack of standardized evaluation for resource-limited languages.


<details>
  <summary>Details</summary>
Motivation: Existing language model evaluation frameworks are inadequate for resource-limited languages like Turkish, creating a need for standardized benchmarks to properly assess linguistic and conceptual capabilities in Turkish NLP.

Method: Created TR-MMLU benchmark based on 6,200 meticulously curated multiple-choice questions across 62 sections within the Turkish education system, providing a comprehensive evaluation framework for Turkish language processing.

Result: The benchmark enables detailed analysis of state-of-the-art LLMs' capabilities in processing Turkish text and identifies specific areas needing improvement in model design for Turkish language understanding.

Conclusion: TR-MMLU establishes a new standard for Turkish NLP research, providing a foundation for advancing language model capabilities in Turkish and inspiring future innovations in the field.

Abstract: Language models have made significant advancements in understanding and
generating human language, achieving remarkable success in various
applications. However, evaluating these models remains a challenge,
particularly for resource-limited languages like Turkish. To address this
issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive
evaluation framework designed to assess the linguistic and conceptual
capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a
meticulously curated dataset comprising 6,200 multiple-choice questions across
62 sections within the Turkish education system. This benchmark provides a
standard framework for Turkish NLP research, enabling detailed analyses of
LLMs' capabilities in processing Turkish text. In this study, we evaluated
state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model
design. TR-MMLU sets a new standard for advancing Turkish NLP research and
inspiring future innovations.

</details>


### [70] [DoÄŸal Dil Ä°ÅŸlemede Tokenizasyon StandartlarÄ± ve Ã–lÃ§Ã¼mÃ¼: TÃ¼rkÃ§e Ãœzerinden BÃ¼yÃ¼k Dil Modellerinin KarÅŸÄ±laÅŸtÄ±rmalÄ± Analizi](https://arxiv.org/abs/2508.13058)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih GÃ¼mÃ¼ÅŸ,Sercan KarakaÅŸ,Banu Diri,SavaÅŸ YÄ±ldÄ±rÄ±m*

Main category: cs.CL

TL;DR: Novel evaluation framework for tokenization in morphologically-rich languages like Turkish, showing language-specific token percentages correlate better with performance than token purity, and parameter scaling alone doesn't improve linguistic capabilities.


<details>
  <summary>Details</summary>
Motivation: Tokenization significantly impacts LLM performance, but existing methods face challenges with morphologically-rich and low-resource languages like Turkish that require specialized evaluation.

Method: Used Turkish MMLU dataset (6,200 questions) to assess tokenizers on vocabulary size, token count, processing time, language-specific token percentages (%TR), and token purity (%Pure) metrics.

Result: Language-specific token percentages showed stronger correlation with downstream performance than token purity. Increasing model parameters alone didn't enhance linguistic performance.

Conclusion: Framework establishes robust tokenization standards for morphologically complex languages, emphasizing need for tailored language-specific tokenization methods over simple parameter scaling.

Abstract: Tokenization is a fundamental preprocessing step in Natural Language
Processing (NLP), significantly impacting the capability of large language
models (LLMs) to capture linguistic and semantic nuances. This study introduces
a novel evaluation framework addressing tokenization challenges specific to
morphologically-rich and low-resource languages such as Turkish. Utilizing the
Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from
the Turkish education system, we assessed tokenizers based on vocabulary size,
token count, processing time, language-specific token percentages (\%TR), and
token purity (\%Pure). These newly proposed metrics measure how effectively
tokenizers preserve linguistic structures. Our analysis reveals that
language-specific token percentages exhibit a stronger correlation with
downstream performance (e.g., MMLU scores) than token purity. Furthermore,
increasing model parameters alone does not necessarily enhance linguistic
performance, underscoring the importance of tailored, language-specific
tokenization methods. The proposed framework establishes robust and practical
tokenization standards for morphologically complex languages.

</details>


### [71] [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)
*John Alderete,Macarious Kin Fung Hui,Aanchan Mohan*

Main category: cs.CL

TL;DR: SFUSED database provides annotated speech errors to evaluate ASR models like WhisperX, showing effectiveness as diagnostic tool


<details>
  <summary>Details</summary>
Motivation: To demonstrate how systematically annotated speech errors can be used to test and evaluate speech recognition models

Method: Used SFUSED database with 5,300 documented word and phonological errors, evaluated WhisperX transcription accuracy across multiple classificatory dimensions including linguistic hierarchical level, contextual sensitivity, degraded words, word corrections, and error positioning

Result: The analysis demonstrated the database's effectiveness as a diagnostic tool for assessing ASR system performance

Conclusion: SFUSED provides valuable annotated speech error data that can effectively evaluate and diagnose speech recognition model performance

Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data
collection developed for linguistic and psycholinguistic research. Here we
demonstrate how its design and annotations can be used to test and evaluate
speech recognition models. The database comprises systematically annotated
speech errors from spontaneous English speech, with each error tagged for
intended and actual error productions. The annotation schema incorporates
multiple classificatory dimensions that are of some value to model assessment,
including linguistic hierarchical level, contextual sensitivity, degraded
words, word corrections, and both word-level and syllable-level error
positioning. To assess the value of these classificatory variables, we
evaluated the transcription accuracy of WhisperX across 5,300 documented word
and phonological errors. This analysis demonstrates the atabase's effectiveness
as a diagnostic tool for ASR system performance.

</details>


### [72] [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)
*Long Ma,Fangwei Zhong,Yizhou Wang*

Main category: cs.CL

TL;DR: ReCOR is a reinforcement learning framework that learns adaptive token generation orders from text data without annotations, outperforming fixed-order models on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current causal and diffusion models use fixed or random token generation orders that may not align with logical reasoning sequences, causing difficulties in complex reasoning tasks.

Method: Reinforcement-learning-based framework that self-supervises using token prediction statistics to estimate prediction hardness and adaptively select the next token during training and inference.

Result: Superior performance on challenging reasoning and planning datasets, sometimes outperforming oracle models with ground-truth order supervision.

Conclusion: Adaptive token generation orders extracted through reinforcement learning significantly improve model performance on complex reasoning tasks compared to fixed-order approaches.

Abstract: Modern causal language models, followed by rapid developments in discrete
diffusion models, can now produce a wide variety of interesting and useful
content. However, these families of models are predominantly trained to output
tokens with a fixed (left-to-right) or random order, which may deviate from the
logical order in which tokens are generated originally. In this paper, we
observe that current causal and diffusion models encounter difficulties in
problems that require adaptive token generation orders to solve tractably,
which we characterize with the $\mathcal{V}$-information framework. Motivated
by this, we propose Reinforced Context Order Recovery (ReCOR), a
reinforcement-learning-based framework to extract adaptive, data-dependent
token generation orders from text data without annotations. Self-supervised by
token prediction statistics, ReCOR estimates the hardness of predicting every
unfilled token and adaptively selects the next token during both training and
inference. Experiments on challenging reasoning and planning datasets
demonstrate the superior performance of ReCOR compared with baselines,
sometimes outperforming oracle models supervised with the ground-truth order.

</details>


### [73] [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
*DayyÃ¡n O'Brien,Bhavitvya Malik,Ona de Gibert,Pinzhen Chen,Barry Haddow,JÃ¶rg Tiedemann*

Main category: cs.CL

TL;DR: DocHPLT is the largest publicly available document-level translation dataset with 124M document pairs across 50 languages, created by preserving complete document integrity rather than reconstructing from sentence-level data.


<details>
  <summary>Details</summary>
Motivation: Existing document-level machine translation resources are scarce and mostly limited to high-resourced languages, creating a need for comprehensive datasets to facilitate document-level translation and long-context modeling for global communities.

Method: Modified an existing web extraction pipeline to preserve complete document integrity from source, retaining all content including unaligned portions. Conducted preliminary experiments to identify optimal training context strategy.

Result: LLMs fine-tuned on DocHPLT substantially outperform off-the-shelf instruction-tuned baselines, with particularly dramatic improvements for under-resourced languages.

Conclusion: DocHPLT provides essential infrastructure for advancing multilingual document-level translation and is open-sourced under a permissive license to benefit the research community.

Abstract: Existing document-level machine translation resources are only available for
a handful of languages, mostly high-resourced ones. To facilitate the training
and evaluation of document-level translation and, more broadly, long-context
modeling for global communities, we create DocHPLT, the largest publicly
available document-level translation dataset to date. It contains 124 million
aligned document pairs across 50 languages paired with English, comprising 4.26
billion sentences, with further possibility to provide 2500 bonus pairs not
involving English. Unlike previous reconstruction-based approaches that piece
together documents from sentence-level data, we modify an existing web
extraction pipeline to preserve complete document integrity from the source,
retaining all content including unaligned portions. After our preliminary
experiments identify the optimal training context strategy for document-level
translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially
outperform off-the-shelf instruction-tuned baselines, with particularly
dramatic improvements for under-resourced languages. We open-source the dataset
under a permissive license, providing essential infrastructure for advancing
multilingual document-level translation.

</details>


### [74] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
*Figarri Keisha,Prince Singh,Pallavi,Dion Fernandes,Aravindh Manivannan,Ilham Wicaksono,Faisal Ahmad*

Main category: cs.CL

TL;DR: Enhanced RAG pipeline for legal domain with context-aware query translation, open-source retrieval strategies achieving 30-95% recall improvements, and comprehensive evaluation framework showing open-source approaches can rival proprietary systems.


<details>
  <summary>Details</summary>
Motivation: To mitigate hallucinations in legal domain by grounding LLM outputs in cited sources, extending LegalBenchRAG baseline with targeted enhancements for legally grounded, reproducible, and cost-effective systems.

Method: End-to-end RAG pipeline with: (i) context-aware query translator that disentangles document references, (ii) open-source retrieval using SBERT and GTE embeddings, (iii) comprehensive evaluation framework combining RAGAS, BERTScore-F1, and ROUGE-Recall.

Result: Substantial performance gains with 30-95% improvement in Recall@K and ~2.5x Precision@K for K>4. Open-source pipelines rival proprietary approaches, and custom legal-grounded prompts produce more faithful and contextually relevant answers.

Conclusion: Task-aware, component-level tuning can deliver legally grounded, reproducible, and cost-effective RAG systems for legal research assistance, demonstrating the potential of carefully designed open-source approaches.

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

</details>


### [75] [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
*Zefang Liu,Arman Anwar*

Main category: cs.CL

TL;DR: AutoBnB-RAG enhances incident response simulations by integrating retrieval-augmented generation into multi-agent systems, improving decision quality and success rates through external knowledge access.


<details>
  <summary>Details</summary>
Motivation: Incident response requires fast, coordinated decision-making, but current LLM-based agents lack access to external knowledge, limiting their reasoning capabilities in cyber threat containment.

Method: Extends AutoBnB framework with RAG capabilities in Backdoors & Breaches tabletop environment. Introduces two retrieval settings: RAG-Wiki (technical documentation) and RAG-News (narrative incident reports). Evaluates eight team structures including argumentative configurations for critical reasoning.

Result: Retrieval augmentation improves decision quality and success rates across diverse organizational models. The system demonstrates ability to reconstruct complex multi-stage attacks based on public breach reports.

Conclusion: Integrating retrieval mechanisms into LLM-based multi-agent systems provides significant value for cybersecurity decision-making, enhancing autonomous incident response capabilities.

Abstract: Incident response (IR) requires fast, coordinated, and well-informed
decision-making to contain and mitigate cyber threats. While large language
models (LLMs) have shown promise as autonomous agents in simulated IR settings,
their reasoning is often limited by a lack of access to external knowledge. In
this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that
incorporates retrieval-augmented generation (RAG) into multi-agent incident
response simulations. Built on the Backdoors & Breaches (B&B) tabletop game
environment, AutoBnB-RAG enables agents to issue retrieval queries and
incorporate external evidence during collaborative investigations. We introduce
two retrieval settings: one grounded in curated technical documentation
(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We
evaluate performance across eight team structures, including newly introduced
argumentative configurations designed to promote critical reasoning. To
validate practical utility, we also simulate real-world cyber incidents based
on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct
complex multi-stage attacks. Our results show that retrieval augmentation
improves decision quality and success rates across diverse organizational
models. This work demonstrates the value of integrating retrieval mechanisms
into LLM-based multi-agent systems for cybersecurity decision-making.

</details>


### [76] [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: BlindSpot framework identifies and quantifies operational biases in LLM-generated call summaries, revealing systemic biases across all tested models.


<details>
  <summary>Details</summary>
Motivation: LLMs generate millions of call summaries daily in contact centers, but it's unclear if they systematically under- or over-attend to specific transcript aspects, potentially introducing operational biases that haven't been explored.

Method: Developed BlindSpot framework with 15 operational bias dimensions, using LLM as zero-shot classifier to derive categorical distributions for transcript-summary pairs, quantified using Fidelity Gap (JS Divergence) and Coverage metrics.

Result: Empirical study of 2500 real call transcripts with summaries from 20 LLMs (varying scales and families) revealed biases are systemic and present across all models regardless of size or family.

Conclusion: Operational biases in LLM-generated call summaries are widespread and systematic, affecting all model types, highlighting the need for bias detection and mitigation frameworks like BlindSpot.

Abstract: Abstractive summarization is a core application in contact centers, where
Large Language Models (LLMs) generate millions of summaries of call transcripts
daily. Despite their apparent quality, it remains unclear whether LLMs
systematically under- or over-attend to specific aspects of the transcript,
potentially introducing biases in the generated summary. While prior work has
examined social and positional biases, the specific forms of bias pertinent to
contact center operations - which we term Operational Bias - have remained
unexplored. To address this gap, we introduce BlindSpot, a framework built upon
a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)
for the identification and quantification of these biases. BlindSpot leverages
an LLM as a zero-shot classifier to derive categorical distributions for each
bias dimension in a pair of transcript and its summary. The bias is then
quantified using two metrics: Fidelity Gap (the JS Divergence between
distributions) and Coverage (the percentage of source labels omitted). Using
BlindSpot, we conducted an empirical study with 2500 real call transcripts and
their summaries generated by 20 LLMs of varying scales and families (e.g., GPT,
Llama, Claude). Our analysis reveals that biases are systemic and present
across all evaluated models, regardless of size or family.

</details>


### [77] [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)
*Kareem Elozeiri,Mervat Abassy,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: Introduces MuDRiC - first Arabic multi-dialect commonsense dataset and novel GCN-based method for Arabic commonsense validation, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Address the gap in Arabic commonsense validation, particularly the underrepresentation of regional dialects despite their prevalence in spoken contexts, as existing resources focus mainly on Modern Standard Arabic.

Method: Developed MuDRiC dataset incorporating multiple Arabic dialects and created a novel method adapting Graph Convolutional Networks (GCNs) to enhance semantic relationship modeling for Arabic commonsense reasoning.

Result: The approach achieves superior performance in Arabic commonsense validation, demonstrating effectiveness in handling Arabic's complex linguistic variations.

Conclusion: This work enhances Arabic natural language understanding by providing both a foundational multi-dialect dataset and a novel method, representing the first Arabic multi-dialect commonsense reasoning resource.

Abstract: Commonsense validation evaluates whether a sentence aligns with everyday
human understanding, a critical capability for developing robust natural
language understanding systems. While substantial progress has been made in
English, the task remains underexplored in Arabic, particularly given its rich
linguistic diversity. Existing Arabic resources have primarily focused on
Modern Standard Arabic (MSA), leaving regional dialects underrepresented
despite their prevalence in spoken contexts. To bridge this gap, we present two
key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense
dataset incorporating multiple dialects, and (ii) a novel method adapting Graph
Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances
semantic relationship modeling for improved commonsense validation. Our
experimental results demonstrate that this approach achieves superior
performance in Arabic commonsense validation. Our work enhances Arabic natural
language understanding by providing both a foundational dataset and a novel
method for handling its complex variations. To the best of our knowledge, we
release the first Arabic multi-dialect commonsense reasoning dataset.

</details>


### [78] [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)
*Dara Bahri,John Wieting*

Main category: cs.CL

TL;DR: Hybrid detection combining watermark and non-watermark methods improves LLM generation detection, especially in low-entropy scenarios where watermark-only detection struggles.


<details>
  <summary>Details</summary>
Motivation: Watermark detection for LLM generations becomes challenging in low-entropy situations, particularly with post-trained models (instruction tuning, RLHF), requiring enhanced detection approaches.

Method: Explored various hybrid schemes that combine watermark detectors with non-watermark detectors to improve detection performance.

Result: Observed performance gains over either class of detector alone across a wide range of experimental conditions.

Conclusion: Hybrid detection schemes combining watermark and non-watermark methods provide superior detection capabilities for LLM generations, especially in practical low-entropy scenarios.

Abstract: Watermarking has recently emerged as an effective strategy for detecting the
generations of large language models (LLMs). The strength of a watermark
typically depends strongly on the entropy afforded by the language model and
the set of input prompts. However, entropy can be quite limited in practice,
especially for models that are post-trained, for example via instruction tuning
or reinforcement learning from human feedback (RLHF), which makes detection
based on watermarking alone challenging. In this work, we investigate whether
detection can be improved by combining watermark detectors with non-watermark
ones. We explore a number of hybrid schemes that combine the two, observing
performance gains over either class of detector under a wide range of
experimental conditions.

</details>


### [79] [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
*Pranjal Aggarwal,Seungone Kim,Jack Lanchantin,Sean Welleck,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: OptimalThinkingBench is a unified benchmark that evaluates both overthinking and underthinking in LLMs, showing no current model optimally balances performance and efficiency across simple and complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs either overthink on simple problems (wasting compute) or underthink on complex reasoning tasks, requiring users to manually select appropriate models for different query types.

Method: Created two sub-benchmarks: OverthinkingBench with 72 domains of simple queries and UnderthinkingBench with 11 challenging reasoning tasks. Used novel thinking-adjusted accuracy metrics to evaluate 33 different thinking and non-thinking models.

Result: No model achieved optimal thinking - thinking models overthink hundreds of tokens on simple queries without performance gains, while large non-thinking models underthink and perform worse than smaller thinking models on complex tasks.

Conclusion: Current approaches improve one sub-benchmark at the expense of the other, highlighting the need for better unified models that can optimally balance performance and efficiency across different task complexities.

Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and
overthinking on simpler problems, while non-thinking LLMs are faster and
cheaper but underthink on harder reasoning problems. This has led to the
development of separate thinking and non-thinking LLM variants, leaving the
onus of selecting the optimal model for each query on the end user. In this
work, we introduce OptimalThinkingBench, a unified benchmark that jointly
evaluates overthinking and underthinking in LLMs and also encourages the
development of optimally-thinking models that balance performance and
efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,
featuring simple queries in 72 domains, and UnderthinkingBench, containing 11
challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we
perform extensive evaluation of 33 different thinking and non-thinking models
and show that no model is able to optimally think on our benchmark. Thinking
models often overthink for hundreds of tokens on the simplest user queries
without improving performance. In contrast, large non-thinking models
underthink, often falling short of much smaller thinking models. We further
explore several methods to encourage optimal thinking, but find that these
approaches often improve on one sub-benchmark at the expense of the other,
highlighting the need for better unified and optimal models in the future.

</details>


### [80] [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)
*David Heineman,Valentin Hofmann,Ian Magnusson,Yuling Gu,Noah A. Smith,Hannaneh Hajishirzi,Kyle Lo,Jesse Dodge*

Main category: cs.CL

TL;DR: This paper analyzes benchmark reliability for language model evaluation, introducing signal and noise metrics to assess benchmark quality and proposing interventions to improve evaluation reliability.


<details>
  <summary>Details</summary>
Motivation: Developing large language models is expensive and requires reliable benchmarks for making decisions with small experiments, but current evaluation benchmarks vary in quality and reliability.

Method: The authors introduce two key metrics (signal and noise) to evaluate benchmarks, analyze 30 benchmarks using 375 language models, and propose three interventions: switching to better metrics like perplexity, filtering noisy subtasks, and averaging intermediate checkpoints.

Result: Benchmarks with better signal-to-noise ratio are more reliable for small-scale decisions and have lower scaling law prediction error. The proposed interventions consistently improve benchmark reliability.

Conclusion: Benchmark creators and users should aim for high signal and low noise, and can improve reliability by using better metrics, filtering noisy components, and averaging model outputs across checkpoints.

Abstract: Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.

</details>


### [81] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
*Xin Chen,Junchao Wu,Shu Yang,Runzhe Zhan,Zeyu Wu,Ziyang Luo,Di Wang,Min Yang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: RepreGuard is a robust LLM-generated text detection method that uses internal model representations to distinguish between AI-generated and human-written content, achieving 94.92% AUROC across various scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing LLM detection methods lack robustness in out-of-distribution scenarios. The authors hypothesize that internal LLM representations contain more comprehensive features to better distinguish statistical patterns between AI-generated and human-written texts.

Method: RepreGuard uses a surrogate model to collect representations of both text types, extracts distinct activation features, and classifies texts by calculating projection scores along these feature directions compared to a precomputed threshold.

Result: The method achieves average 94.92% AUROC on both in-distribution and out-of-distribution scenarios, demonstrating superior performance over baselines and robustness to various text sizes and attacks.

Conclusion: Internal LLM representations provide effective features for detecting AI-generated content, and RepreGuard offers a robust and efficient detection solution that works well across different scenarios and attack types.

Abstract: Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [82] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: A deep learning-based real-time smoking detection system using CCTV surveillance for fire exit areas, achieving high recall (78.90%) and mAP (83.70%) with optimized performance on edge devices.


<details>
  <summary>Details</summary>
Motivation: Critical safety requirements for monitoring fire exit areas where smoking poses significant fire hazards, requiring automated real-time detection to ensure public safety and regulatory compliance.

Method: Evaluated YOLOv8, YOLOv11, and YOLOv12 object detection models, then developed a custom model based on YOLOv8 with additional structures for challenging surveillance contexts. Used dataset of 8,124 images from 20 scenarios including 2,708 low-light samples. Tested on multiple edge devices with multithreaded operations.

Result: Proposed custom model outperformed all others with 78.90% recall and 83.70% mAP at 50. Jetson Xavier NX achieved 52-97ms per inference, suitable for real-time operations. System demonstrated robust performance across varied environments.

Conclusion: The system provides a robust and adaptable platform for real-time smoking detection in surveillance contexts, enabling automatic regulatory compliance and enhancing public safety in critical areas like fire exits.

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [83] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*AdriÃ¡n RodrÃ­guez-MuÃ±oz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: Procedural data-trained representation models achieve near-real performance on visual tasks using visual memory without further training, with performance gaps explained by dissimilar object part representations.


<details>
  <summary>Details</summary>
Motivation: To achieve full compartmentalization from real-world images while maintaining strong performance on visual tasks by using procedural data and visual memory techniques.

Method: Train representation models exclusively on procedural data, then apply them to visual tasks using visual memory - an explicit database of reference image embeddings without additional training.

Result: Within 1% of Places-trained model on NIGHTS similarity, outperforms by 8-15% on fine-grained classification (CUB200, Flowers102), within 10% on ImageNet-1K, and strong zero-shot segmentation (RÂ² within 10% of real-data models on COCO).

Conclusion: Procedural models achieve competitive performance but show dissimilar representations for object parts, causing incorrect memory searches and explaining remaining performance gaps compared to real-data models.

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [84] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: Systematic evaluation of four ophthalmic foundation models (RETFound, VisionFM, RetiZero, DINORET) showing DINORET and RetiZero perform best, with RetiZero having better generalization. Gating-based fusion modestly improves glaucoma, AMD, and hypertension prediction.


<details>
  <summary>Details</summary>
Motivation: Foundation models show promise in medical imaging but there's no clear understanding of which performs best in ophthalmology, whether they work equally well across different tasks, and what benefits model fusion might provide.

Method: Proposed FusionFM evaluation suite with two fusion approaches to integrate different ophthalmic FMs. Evaluated on both ophthalmic diseases (glaucoma, diabetic retinopathy, AMD) and systemic diseases (diabetes, hypertension) using retinal imaging from standardized multi-country datasets. Used AUC and F1 metrics.

Result: DINORET and RetiZero achieved superior performance in both ophthalmic and systemic disease tasks. RetiZero showed stronger generalization on external datasets. Gating-based fusion provided modest improvements for glaucoma, AMD, and hypertension prediction. Systemic disease prediction, especially hypertension in external cohorts, remains challenging.

Conclusion: This study provides evidence-based evaluation of ophthalmic FMs, demonstrates benefits of model fusion, and identifies strategies for enhancing clinical applicability, while highlighting ongoing challenges in systemic disease prediction.

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [85] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF is a unified deep learning framework that reconstructs multiple dentocraniofacial hard tissues using multimodal fusion of point clouds and multi-view images, achieving superior geometric precision and clinical efficiency.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models are limited to single-tissue scenarios and modality-specific inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability for dentocraniofacial reconstruction.

Method: UniDCF uses multimodal fusion encoding of point clouds and multi-view images, leveraging complementary strengths of each modality with a score-based denoising module to refine surface smoothness. Trained on the largest multimodal dataset with 54,555 annotated instances from 6,609 patients.

Result: Outperforms state-of-the-art methods in geometric precision, structural completeness, and spatial accuracy. Reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%.

Conclusion: UniDCF enables rapid, automated, high-fidelity reconstruction for personalized restorative treatments, streamlining clinical workflows and enhancing patient outcomes.

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [86] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5 is an advanced multimodal model with native-resolution vision processing and reflection-based reasoning, achieving state-of-the-art performance in sub-40B parameter range with significant improvements over its predecessor.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed-resolution image processing that degrades fine details and global layout, particularly for visually dense content like complex charts, and to enhance reasoning capabilities beyond linear chain-of-thought approaches.

Method: Integrates native-resolution vision transformer, reflection-based reasoning with self-checking and revision, trained via five-phase curriculum (visual/multimodal pretraining, instruction tuning, alignment with DPO/GRPO), using multimodal data packing and hybrid parallelism for efficiency.

Result: Ovis2.5-9B scores 78.3 on OpenCompass (substantial improvement over Ovis2-8B), Ovis2.5-2B scores 73.9 (SOTA for its size), achieves leading results on STEM benchmarks, strong grounding/video capabilities, and SOTA for complex chart analysis.

Conclusion: Ovis2.5 successfully demonstrates advanced native-resolution visual perception and reflection-based reasoning, establishing new state-of-the-art performance for open-source MLLMs in sub-40B parameter range while maintaining efficiency for resource-constrained applications.

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [87] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: VideoAVE is the first publicly available video-to-text e-commerce attribute value extraction dataset covering 14 domains and 172 attributes, with a CLIP-based filtering system for data quality and comprehensive benchmarks showing video-to-text AVE remains challenging.


<details>
  <summary>Details</summary>
Motivation: Existing AVE datasets are limited to text-to-text or image-to-text settings, lacking support for product videos, diverse attribute coverage, and public availability.

Method: Introduced VideoAVE dataset with CLIP-based Mixture of Experts filtering system to remove mismatched video-product pairs, creating 224k training and 25k evaluation data. Established benchmark using state-of-the-art video VLMs for attribute-conditioned value prediction and open attribute-value pair extraction tasks.

Result: Video-to-text AVE remains a challenging problem, particularly in open settings, with room for developing more advanced VLMs capable of leveraging effective temporal information.

Conclusion: VideoAVE addresses critical gaps in e-commerce AVE by providing the first public video-to-text dataset with comprehensive domain and attribute coverage, establishing benchmarks that reveal current limitations and opportunities for future VLM development.

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [88] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: Curvature-based geometric features alone enable MLP to achieve high accuracy (97% on MNIST, 89% on EMNIST) for handwritten character recognition, showing deep learning benefits can be achieved with interpretable handcrafted features.


<details>
  <summary>Details</summary>
Motivation: To investigate whether second-order geometric cues (curvature magnitude, sign, and gradient orientation) are sufficient on their own for handwritten character recognition, providing an alternative to convolutional neural networks.

Method: Using three handcrafted feature maps (planar curvature magnitude, curvature sign, and gradient orientation) as inputs to a multilayer perceptron (MLP) classifier for handwritten character recognition.

Result: The curvature-orientation MLP achieved 97% accuracy on MNIST digits and 89% accuracy on EMNIST letters.

Conclusion: Curvature-based representations have strong discriminative power for handwritten characters, and the advantages of deep learning can be realized with interpretable, hand-engineered features rather than complex CNNs.

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [89] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: A dual approach combining prompt optimization and multimodal data augmentation improves hateful meme detection by addressing subtle text-image interactions and reducing spurious correlations in VLMs.


<details>
  <summary>Details</summary>
Motivation: Modern web's multimodal content makes hateful meme detection challenging due to subtle text-image interactions disguised as humor, with current VLMs lacking fine-grained supervision and being vulnerable to implicit hate speech.

Method: 1) Prompt optimization framework varying prompt structure, supervision granularity, and training modality; 2) Multimodal data augmentation pipeline generating 2,479 counterfactually neutral memes using multi-agent LLM-VLM setup to isolate and rewrite hateful modality.

Result: Structured prompts improve robustness even in small models, InternVL2 achieves best F1-scores across binary and scaled settings, and the augmentation pipeline successfully reduces spurious correlations and improves classifier generalization.

Conclusion: Prompt structure and data composition are as critical as model size, and targeted augmentation can support more trustworthy and context-sensitive hate detection, inspiring new directions for building synthetic data to train robust vision-language models.

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [90] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: The paper investigates Gaussian curvature as an invariant geometric prior for 3D surface modeling, showing it provides sparse surface descriptions, is implicitly used by state-of-the-art methods, and can improve reconstruction and serve as an unsupervised stereo metric.


<details>
  <summary>Details</summary>
Motivation: Current deep learning approaches for 3D vision lack explicit geometric models that can be analyzed, transferred, or systematically modified. The authors want to explore Gaussian curvature as a fundamental geometric invariant for better 3D surface understanding.

Method: The researchers investigate Gaussian curvature properties and demonstrate its utility using the Middlebury stereo dataset, analyzing how it provides sparse surface descriptions and serves as a geometric prior.

Result: Gaussian curvature offers a compact 3D surface description, is implicitly considered by state-of-the-art methods, can inform and improve 3D reconstruction, and may serve as an unsupervised metric for stereo evaluation.

Conclusion: Gaussian curvature represents a valuable geometric prior that addresses limitations of purely data-driven approaches by providing explicit, analyzable geometric models for 3D surface understanding and reconstruction.

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [91] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel MÃ¼ller*

Main category: cs.CV

TL;DR: Systematic evaluation of image-to-graph transformation methods for graph-level anomaly detection using GNNs, showing color features work best alone but combining with shape/texture improves performance across all supervision levels.


<details>
  <summary>Details</summary>
Motivation: No previous study has rigorously compared different image-to-graph transformation approaches for GNN-based graph-level anomaly detection, despite GNNs being powerful for graph-based tasks.

Method: Systematically evaluated multiple segmentation schemes, edge construction strategies, and node feature sets (color, texture, shape descriptors) using state-of-the-art GLAD models on dermoscopic images in unsupervised, weakly supervised, and fully supervised regimes.

Result: Color descriptors provided best standalone performance; incorporating shape and texture features consistently enhanced detection. Best unsupervised configuration achieved 0.805 AUC-ROC, increasing to 0.872 with sparse labels and 0.914 with full supervision.

Conclusion: Comprehensive image-to-graph transformation evaluation provides valuable insights for GLAD, showing competitive performance without pretrained backbones and substantial improvements with supervision.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [92] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: Comprehensive review of Transformer architectures applied to UAV systems, covering attention mechanisms, CNN-Transformer hybrids, RL Transformers, and LLMs, with taxonomy, applications, benchmarks, and future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of Transformer-based models has significantly enhanced UAV perception, decision-making, and autonomy, requiring a systematic review to categorize and evaluate these developments.

Method: Systematic categorization and evaluation of recent Transformer architectures for UAVs, including creation of unified taxonomy, comparative analyses through structured tables and performance benchmarks, and review of datasets and simulators.

Result: Presents a comprehensive synthesis of Transformer-driven UAV technologies, highlighting emerging applications like precision agriculture and autonomous navigation, while identifying gaps and challenges in computational efficiency and real-time deployment.

Conclusion: This review provides guidance for researchers and practitioners to understand and advance Transformer-based UAV technologies, outlining critical challenges and future research directions for the field.

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [93] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: ComplicitSplat is the first black-box attack on 3D Gaussian Splatting that creates viewpoint-specific camouflage to embed adversarial content visible only from specific angles, successfully attacking various object detectors without requiring model access.


<details>
  <summary>Details</summary>
Motivation: As 3DGS gains adoption in safety-critical tasks like autonomous navigation, there's a need to understand how adversaries might tamper with images to cause harm through novel attack vectors.

Method: Exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle to embed adversarial content in scene objects visible only from specific viewpoints.

Result: Successfully attacks various popular detectors including single-stage, multi-stage, and transformer-based models on both real-world physical object captures and synthetic scenes.

Conclusion: Exposes a novel safety risk for applications like autonomous navigation and mission-critical robotic systems, demonstrating the first black-box attack on downstream object detectors using 3DGS.

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [94] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: Image quality distribution significantly impacts label-efficient fine-tuning of medical foundation models. Quality mismatch between fine-tuning and test sets affects performance, and sufficient high-quality images are crucial for maintaining strong results.


<details>
  <summary>Details</summary>
Motivation: To evaluate how variable image quality affects label-efficient fine-tuning of foundation models in medical imaging, specifically investigating the generalizability of fine-tuned models when dealing with different quality distributions.

Method: Used ProFound (domain-specific vision foundation model) pretrained on large-scale prostate MRI datasets. Systematically varied high-/low-quality image ratios in both fine-tuning and evaluation sets, measuring performance across different downstream tasks including automated radiology reporting and prostate cancer detection.

Result: Image quality distribution and finetune-test mismatch significantly affect model performance. Consistent quality ratios enable fine-tuning with far less labeled data than training from scratch, but without enough high-quality finetuning data, pretrained models may fail to outperform non-pretrained models.

Conclusion: Assessing and aligning quality distributions between finetuning and deployment is crucial, highlighting the need for quality standards in finetuning data for specific downstream tasks to fully realize the efficiency benefits of foundation models.

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [95] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: AdaRing proposes a cross-layer tensor ring decomposition framework for vision-language model fine-tuning that reduces parameter redundancy by 90% while achieving state-of-the-art performance through diverse rank-driven adapters.


<details>
  <summary>Details</summary>
Motivation: Existing adapter-based fine-tuning methods suffer from limited compression rates due to cross-layer redundancy and limited representational capacity across homogeneous adapters in vision-language models.

Method: Uses cross-layer tensor ring decomposition to formulate adapters as layer-shared tensor cores and layer-specific slices, exploiting tensor-level low-rankness to remove redundancy. Implements diverse rank-driven adapters guided by generalization-aware fine-tuning for different representation needs.

Result: Achieves state-of-the-art performance while reducing average training parameters by 90% across various tasks.

Conclusion: AdaRing provides an ultra-light parameter-efficient adaptation framework that effectively addresses cross-layer redundancy and enhances representational capacity through tensor ring decomposition and diverse adapter collaboration.

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [96] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: EVTP-IV is a visual token pruning method that achieves 3.5-5X speed-up for Instructed Visual Segmentation tasks while maintaining accuracy using only 20% of tokens.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) have high inference costs for visual segmentation tasks, especially in video, creating a performance bottleneck that needs addressing.

Method: A novel visual token pruning method based on k-center algorithm with spatial information integration to select compact yet spatially representative token subsets.

Result: Achieves up to 5X speed-up on video tasks and 3.5X on image tasks while maintaining comparable accuracy using only 20% of tokens, outperforming state-of-the-art pruning baselines.

Conclusion: The method effectively reduces computational costs for IVS tasks through intelligent token selection while preserving segmentation performance, making it practical for real-world applications.

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [97] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: LKMN is a pure CNN-based image super-resolution model that combines large kernel convolutions with channel attention and cross-gate fusion to achieve state-of-the-art performance while maintaining fast inference speeds.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between CNN models (fast but limited non-local feature capture) and Transformers (good non-local modeling but slow inference) in resource-constrained super-resolution scenarios.

Method: Uses Enhanced Partial Large Kernel Block (EPLKB) with channel shuffle, channel attention, and large kernel strip convolutions on partial channels, plus Cross-Gate Feed-Forward Network (CGFN) for dynamic feature fusion with learnable scaling.

Result: Outperforms existing SOTA lightweight SR models, achieving 0.23 dB PSNR improvement over DAT-light on Manga109 dataset at 4x upscale with 4.8x faster inference.

Conclusion: LKMN successfully bridges the gap between CNN efficiency and Transformer-like non-local modeling capability, providing an effective solution for high-quality, efficient image super-resolution.

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [98] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: Using only horizontal and vertical Sobel derivatives as input, an MLP achieves near-CNN performance on handwritten character recognition with smaller memory footprint.


<details>
  <summary>Details</summary>
Motivation: To investigate whether first-order edge maps from Sobel operator are sufficient for handwritten character recognition as an alternative to convolutional neural networks.

Method: Train a multilayer perceptron (MLP) using only horizontal and vertical Sobel derivatives as input features on MNIST and EMNIST Letters datasets.

Result: The MLP reached 98% accuracy on MNIST digits and 92% on EMNIST letters, approaching CNN performance while offering smaller memory footprint and transparent features.

Conclusion: First-order gradients capture most class-discriminative information in handwritten characters, making edge-aware MLPs a compelling alternative to CNNs for HCR tasks.

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [99] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: Proposes OVG-HQ task for online video grounding with hybrid-modal queries (text, images, video segments) and introduces OVG-HQ-Unify framework with Parametric Memory Block and cross-modal distillation to handle limited context and modality imbalance.


<details>
  <summary>Details</summary>
Motivation: Traditional video grounding struggles with streaming video scenarios and visual cue queries, creating a need for online processing and support for multiple query modalities beyond just text.

Method: OVG-HQ-Unify framework with Parametric Memory Block (PMB) for knowledge retention and cross-modal distillation to balance modality learning. Constructed QVHighlights-Unify dataset with multi-modal queries and adapted online evaluation metrics.

Result: Experiments show OVG-HQ-Unify outperforms existing models, providing robust solution for online hybrid-modal video grounding with both accuracy and efficiency improvements.

Conclusion: The proposed framework effectively addresses challenges of online video grounding with hybrid queries, offering a unified solution that handles multiple modalities and online processing constraints.

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [100] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl is a lightweight plugin that localizes unsafe content in text-to-image generation and suppresses harmful semantics using DPO training, achieving better safety and fidelity than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing safety methods for text-to-image models create trade-offs between safety and fidelity, and concept replacement approaches can cause semantic incongruity. A more flexible detect-then-suppress approach is needed.

Method: SafeCtrl uses a novel training strategy with Direct Preference Optimization (DPO) on image-level preference data to learn nuanced suppression behaviors. It precisely localizes unsafe content and suppresses harmful semantics rather than performing hard substitutions.

Result: Extensive experiments show SafeCtrl significantly outperforms state-of-the-art methods in both safety efficacy and fidelity preservation.

Conclusion: Decoupled, suppression-based control is an effective and scalable direction for building more responsible generative models, allowing natural and coherent resolution into safe alternatives.

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [101] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP uses single pixel temporal-spectral data from Sentinel-2 imagery with cross-view learning for efficient land-use classification, eliminating need for large spatial tiles and text supervision.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models for remote sensing rely on large spatial tiles (computationally expensive) and text-based supervision (often unavailable), creating scalability challenges.

Method: Lightweight framework leveraging temporal and spectral dimensions of single pixels from Sentinel-2 imagery, combined with cross-view learning using geo-tagged ground-level photos for semantic alignment between satellite and ground perspectives.

Result: Demonstrated that single pixel inputs with temporal and spectral cues are sufficient for thematic mapping tasks including LULC, crop type, and ecosystem type classification, offering scalable alternative to traditional approaches.

Conclusion: TimeSenCLIP provides an efficient and scalable solution for large-scale remote sensing applications by minimizing computational costs and reducing dependency on caption-based training while maintaining classification performance.

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [102] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: Synthetic MRI data from GANs can help brain tumor segmentation by improving dataset diversity, with hybrid datasets (40% real + 60% synthetic) showing better boundary delineation, though class imbalance issues persist.


<details>
  <summary>Details</summary>
Motivation: Address challenges in brain tumor segmentation including tumor heterogeneity, scarcity of annotated data, and class imbalance in medical imaging datasets by using synthetic data to improve dataset diversity.

Method: Used pre-trained GAN model (medigan library) to generate synthetic MRI data, combined with real data from BraTS 2020 dataset in varying proportions to train U-Net segmentation network. Evaluated performance with quantitative metrics (Dice, IoU, precision, recall, accuracy) and qualitative inspection.

Result: Overall quantitative performance was comparable between real-only and hybrid-trained models. Hybrid datasets (40% real + 60% synthetic) improved whole tumor boundary delineation qualitatively, but region-wise accuracy for tumor core and enhancing tumor remained lower due to persistent class imbalance.

Conclusion: Synthetic data is feasible as an augmentation strategy for brain tumor segmentation, but future work needs larger-scale experiments, volumetric data consistency, and better mitigation of class imbalance issues.

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [103] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: This paper provides the first comprehensive survey of deep learning-based point cloud denoising methods, categorizing them into outlier removal and surface noise restoration, and discusses future research directions.


<details>
  <summary>Details</summary>
Motivation: Real-world point clouds contain various noise types that degrade downstream task performance, and while deep learning methods have surpassed traditional approaches, there's no systematic survey of DL-based point cloud denoising developments.

Method: The authors formulate point cloud denoising as a two-step process (outlier removal and surface noise restoration), create a taxonomy for denoising tasks, and conduct comparative analysis of existing methods based on similarities, differences, and advantages.

Result: The survey systematically categorizes and compares DL-based point cloud denoising methods, providing a comprehensive overview of the field's current state and identifying key challenges.

Conclusion: The paper fills a critical gap by offering the first comprehensive survey of DL-based point cloud denoising, establishes a taxonomy framework, and provides insights for future research directions to advance the field.

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [104] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose is a retraining-free 6D pose tracking framework that handles fast-moving camera and object scenarios using visual-inertial odometry, depth-informed 2D tracking, and VIO-guided Kalman filtering in a closed-loop system.


<details>
  <summary>Details</summary>
Motivation: Previous 6D pose tracking methods perform poorly when both the object and camera move rapidly, being mainly applicable to static or quasi-static scenes. The performance significantly deteriorates in fast-moving scenarios.

Method: Three synergistic components: (1) Visual-inertial odometry compensates for ROI shift from camera motion, (2) Depth-informed 2D tracker corrects ROI deviations from large object translation, (3) VIO-guided Kalman filter predicts rotation, generates candidate poses, and refines through hierarchical selection. Forms a closed-loop system where 6D pose results guide subsequent tracking.

Result: Simulation and real-world experiments demonstrate the method's effectiveness, achieving real-time and robust 6D pose tracking for fast-moving cameras and objects.

Conclusion: DynamicPose overcomes limitations of previous methods by providing robust 6D pose tracking in dynamic scenarios with rapid camera and object movement, operating in real-time without requiring retraining.

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [105] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: This paper proposes a lightweight object detection method for point clouds that approximates multi-scale features from single neighborhoods using knowledge distillation, employs transferable feature embedding with class-aware statistics, and introduces central weighted IoU for better localization.


<details>
  <summary>Details</summary>
Motivation: Multi-scale feature learning in point cloud object detection typically requires multiple neighborhood searches and scale-aware layers, which increases computational costs and hinders development of lightweight models, especially for resource-constrained environments.

Method: The method approximates multi-scale features from a single neighborhood via knowledge distillation, uses class-aware statistics as transferable features to compensate for reduced diversity, and introduces central weighted intersection over union to address center offset misalignment in optimization.

Result: Extensive experiments on public datasets demonstrate the effectiveness of the proposed method, which successfully reduces computational costs while maintaining detection performance.

Conclusion: The proposed approach provides an efficient solution for lightweight point cloud object detection by approximating multi-scale features from single neighborhoods and incorporating transferable feature embedding, making it suitable for computational resource-constrained applications.

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [106] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG is the first unified framework for 3D understanding and generation that uses an LLM to process sentences and 3D representations, with a spatial decoder based on latent diffusion for high-quality 3D generation and spatial VQA tasks.


<details>
  <summary>Details</summary>
Motivation: Despite progress in unified architectures for 2D images, integrating 3D tasks remains challenging and largely unexplored. The paper aims to create a unified framework for both understanding and generating 3D content.

Method: Proposes UniUGG framework with LLM for sentence and 3D representation processing, spatial decoder using latent diffusion model for 3D generation, and geometric-semantic learning strategy to pretrain vision encoder for capturing both semantic and geometric cues.

Result: Extensive experiments show superiority in visual representation, spatial understanding, and 3D generation. The method enables generation of 3D scenes from reference images with arbitrary view transformations while supporting spatial VQA tasks.

Conclusion: UniUGG successfully addresses the integration challenge of 3D tasks in unified architectures, demonstrating strong performance in both 3D understanding and generation through its novel framework design and learning strategies.

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [107] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: SAMDWICH is a moment-aware RVOS framework that introduces temporal moment annotations and selective supervision to address semantic misalignment in referring video object segmentation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing RVOS methods suffer from semantic misalignment due to indiscriminate frame sampling and supervision of all visible objects regardless of their relevance to the text expression.

Method: Proposes SAMDWICH framework with Moment-guided Dual-path Propagation (MDP) for moment-aware object grounding/tracking, and Object-level Selective Supervision (OSS) that only supervises temporally aligned objects. Introduces MeViS-M dataset with manual temporal moment annotations.

Result: Achieves state-of-the-art performance on challenging MeViS benchmark, particularly excelling in complex scenarios with diverse expressions.

Conclusion: Temporal moment annotations and selective supervision significantly enhance video-text alignment and referential understanding in RVOS tasks.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [108] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: PEdger++ is a collaborative learning framework for efficient edge detection that balances accuracy and computational complexity by leveraging cross-information from heterogeneous architectures, training moments, and parameter samplings.


<details>
  <summary>Details</summary>
Motivation: Edge detection is crucial for computer vision applications but current deep learning methods have high computational costs that limit deployment on resource-constrained devices. The paper addresses how to capture discriminative features efficiently without large, sophisticated models.

Method: Proposes PEdger++, a collaborative learning framework that uses cross-information from heterogeneous architectures, diverse training moments, and multiple parameter samplings to enhance learning from an ensemble perspective.

Result: Extensive experiments on BSDS500, NYUD and Multicue datasets show clear improvements over existing methods both quantitatively and qualitatively. Multiple model versions are provided with varying computational requirements.

Conclusion: PEdger++ effectively balances accuracy and computational efficiency, demonstrating adaptability to different resource constraints while improving edge detection performance through collaborative learning principles.

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [109] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: A novel multi-modal micro-expression dataset with synchronized RGB and event cameras shows event-based data significantly outperforms RGB for Action Unit classification (51.23% vs 23.12%) and achieves high-quality frame reconstruction.


<details>
  <summary>Details</summary>
Motivation: Micro-expression analysis is important for applications like Human-Robot Interaction and Driver Monitoring Systems, but RGB cameras struggle with capturing subtle, fast facial movements due to temporal resolution limitations and motion blur.

Method: Created a multi-resolution, multi-modal dataset with synchronized RGB and event cameras under variable lighting conditions. Evaluated two baseline tasks: Action Unit classification using Spiking Neural Networks and frame reconstruction using Conditional Variational Autoencoders.

Result: Event-based data achieved 51.23% accuracy for Action Unit classification vs 23.12% with RGB data. Frame reconstruction achieved SSIM = 0.8513 and PSNR = 26.89 dB using high-resolution event input.

Conclusion: Event cameras show promising results for micro-expression recognition and frame reconstruction, demonstrating superior performance over traditional RGB cameras for capturing subtle facial movements.

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [110] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON is a generative MLLM-based model for product representation learning that addresses multimodal alignment challenges through guided MoE modules, core semantic region detection, and specialized negative sampling, achieving strong zero-shot performance across various product understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Existing discriminative dual-flow architectures struggle with many-to-one alignment between multiple product images and texts, while typical LLMs lack multimodal and aspect-aware modeling capabilities needed for comprehensive product understanding.

Method: Proposes MOON model with: (1) guided Mixture-of-Experts module for targeted multimodal and aspect-specific modeling, (2) core semantic region detection to mitigate background noise, and (3) specialized negative sampling strategy for increased difficulty and diversity.

Result: Demonstrates competitive zero-shot performance on both the proposed MBE benchmark and public datasets, with strong generalization across cross-modal retrieval, product classification, and attribute prediction tasks.

Conclusion: MOON effectively addresses key challenges in product representation learning and shows promising results for various product understanding applications through its generative MLLM approach.

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [111] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive is an instance-aware 3D Gaussian Splatting framework for dynamic driving scene reconstruction that achieves 3D instance segmentation without requiring pre-processed instance IDs or complex pipelines.


<details>
  <summary>Details</summary>
Motivation: Current methods unify background elements into single representations, hindering instance-level understanding and scene editing. Existing approaches rely on pre-processed instance IDs or complex pipelines and are designed for indoor scenes, making them unsuitable for outdoor driving scenarios.

Method: Uses SAM-generated masks as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. Introduces 3D regularization to implicitly encode instance identities with voxel-based loss consistency. Employs a lightweight static codebook to bridge continuous features and discrete identities without data pre-processing.

Result: Quantitative and qualitative experiments demonstrate effectiveness. InstDrive is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.

Conclusion: InstDrive provides an effective solution for instance-aware reconstruction of dynamic driving scenes, enabling better instance-level understanding and flexible scene editing without complex pre-processing requirements.

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [112] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,AhcÃ¨ne Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: WiseLVAM is a fully automated framework that combines B-mode structure awareness with AMM motion awareness to perform accurate LV linear measurements by automatically placing scanlines and predicting landmarks along clinical guidelines.


<details>
  <summary>Details</summary>
Motivation: Existing automated methods for LV measurements are unreliable due to small landmark prediction errors along LV walls, and semi-automatic methods require clinician input for scanline placement.

Method: Uses weakly supervised B-mode landmark detection to estimate LV contours, infers LV long axis and basal level for scanline placement, then performs automated measurements in AMM mode combining structure and motion awareness.

Result: The method enables fully automated yet manually adaptable LV linear measurements that mimic clinical guidelines, enhancing robustness and accuracy.

Conclusion: WiseLVAM provides a practical solution for routine clinical application by automating the entire measurement process while maintaining clinical reliability through combined B-mode and AMM approaches.

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [113] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: Q-FSRU combines frequency domain processing with quantum-inspired retrieval for medical VQA, achieving superior performance on complex clinical questions requiring both image and text understanding.


<details>
  <summary>Details</summary>
Motivation: Solving tough clinical questions that require both image and text understanding remains a major challenge in healthcare AI, needing improved accuracy and explainability.

Method: Uses Frequency Spectrum Representation and Fusion (FSRU) with Fast Fourier Transform to process medical images and text in frequency domain, combined with Quantum Retrieval-Augmented Generation to fetch medical facts from external sources using quantum-based similarity techniques.

Result: Outperforms earlier models on VQA-RAD dataset, especially on complex cases requiring image-text reasoning, with improved performance and explainability.

Conclusion: The approach offers a promising way to build smart, clear, and helpful AI tools for doctors by combining frequency and quantum information processing.

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [114] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG is a video-based retrieval-augmented generation framework that enhances motion LLMs by retrieving relevant 2D human motion from videos to address data limitations and improve 3D motion generation.


<details>
  <summary>Details</summary>
Motivation: Motion LLMs suffer from severe out-of-domain/out-of-vocabulary issues due to limited annotated data, which hinders their ability to generate accurate 3D human motions from text-only inputs.

Method: Develops a motion-centered video retrieval model (Gemini Motion Video Retriever) to distinguish human poses/actions and uses Motion-centric Dual-alignment DPO Trainer to mitigate error propagation from suboptimal retrieval results.

Result: Experimental results show VimoRAG significantly boosts the performance of motion LLMs that are constrained to text-only input.

Conclusion: The framework successfully addresses key bottlenecks in video-based motion RAG and enhances motion generation capabilities by leveraging large-scale in-the-wild video databases.

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [115] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: AutoEval framework using Prediction Consistency and Reliability (PCR) to estimate object detection performance without ground-truth labels by analyzing spatial consistency and confidence reliability of bounding boxes before/after NMS.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for object detector evaluation is costly and time-consuming, creating a need for automated performance assessment methods.

Method: Proposed PCR metric that measures spatial consistency between boxes before/after non-maximum suppression and reliability via confidence scores of overlapping boxes. Constructed meta-dataset with varying image corruptions for realistic evaluation.

Result: PCR provides more accurate performance estimates than existing AutoEval methods, and the meta-dataset covers a wider range of detection performance scenarios.

Conclusion: The AutoEval framework with PCR enables efficient and effective automated evaluation of object detectors without requiring ground-truth labels, addressing the cost limitations of manual annotation.

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [116] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD is a diffusion-based model for generic event boundary detection that generates diverse plausible boundaries instead of deterministic predictions, using temporal self-similarity encoding and classifier-free guidance.


<details>
  <summary>Details</summary>
Motivation: Previous GEBD methods focused on deterministic predictions, overlooking the inherent subjectivity and diversity of plausible event boundaries in videos.

Method: A diffusion-based model that encodes frame changes via temporal self-similarity, then iteratively decodes random noise into plausible boundaries using classifier-free guidance to control diversity.

Result: Achieves strong performance on Kinetics-GEBD and TAPOS benchmarks, generating diverse and plausible event boundaries.

Conclusion: The diffusion-based generative approach effectively addresses the subjectivity in GEBD by producing multiple plausible boundary solutions rather than single deterministic predictions.

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [117] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens GÃ¼hmann*

Main category: cs.CV

TL;DR: MSCNN method improves 3D laser scanner accuracy in rough indoor environments by pairing high/low-end scanners and using neural networks to correct systematic errors, achieving 70% MSE reduction.


<details>
  <summary>Details</summary>
Motivation: High-end and low-end laser scanners have positional errors due to equipment limitations and environmental factors, creating uncertainty in 3D point accuracy for geometric modeling.

Method: Pairs high-accuracy scanners as references with low-accuracy scanners in identical environments, establishes statistical relationships between measurement discrepancies and spatial distribution, combines geometric processing with neural network refinement.

Result: 70% MSE reduction and ~6 dB PSNR improvement in rough indoor rooms dataset, enabling low-end devices to approach high-end measurement accuracy without hardware changes.

Conclusion: The integrated MSCNN approach effectively transforms systematic error quantification into supervised learning, providing precise correction while preserving geometric features for improved 3D measurement accuracy.

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [118] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: A theoretical framework for analyzing quantization error propagation in diffusion models, with a timestep-aware compensation scheme that significantly improves post-training quantization performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face deployment challenges due to computationally intensive iterative denoising processes, and post-training quantization suffers from stepwise error accumulation that compromises output fidelity.

Method: Developed a theoretical framework that mathematically formulates error propagation in diffusion models, deriving per-step quantization error propagation equations and establishing the first closed-form solution for cumulative error. Proposed a timestep-aware cumulative error compensation scheme.

Result: Extensive experiments across multiple image datasets demonstrate that the compensation strategy effectively mitigates error propagation, significantly enhancing existing PTQ methods to achieve state-of-the-art performance on low-precision diffusion models.

Conclusion: The proposed theoretical framework and compensation scheme provide an effective solution to address quantization error accumulation in diffusion models, enabling more efficient deployment while maintaining output quality.

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [119] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: VELVET-Med is a novel vision-language pre-training framework for 3D medical imaging that achieves state-of-the-art performance with limited data through innovative self-supervised learning, a TriBERT language encoder, and hierarchical contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs face challenges with volumetric data like CT scans due to difficulty curating large-scale paired data, which limits downstream task performance. Existing methods require extensive data collection that is time-intensive in medical domains.

Method: Proposes VELVET-Med framework with: 1) Uni-modal self-supervised learning integration, 2) TriBERT language encoder for multi-level textual semantics, 3) Hierarchical contrastive learning for multi-level vision-language correspondence. Uses only 38,875 scan-report pairs.

Result: Achieves state-of-the-art performance across multiple downstream tasks including 3D segmentation, cross-modal retrieval, visual question answering, and report generation. Demonstrates strong transferability of learned encoders.

Conclusion: The framework successfully uncovers rich spatial and semantic relationships in volumetric medical images and clinical narratives with limited data, enhancing generalization ability without requiring large-scale data collection.

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [120] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: Simple o3 is an end-to-end multimodal reasoning framework that integrates dynamic visual tools (cropping, zooming, reusing) with interleaved vision-language reasoning through supervised fine-tuning, achieving superior performance on diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models have shown strong performance but their long Chain-of-Thought capabilities in multimodal scenarios remain underexplored, particularly the ability to emulate human-like 'thinking with image' through iterative visual transformations.

Method: Proposes Simple o3 framework with scalable data synthesis pipeline generating high-quality interleaved vision-language reasoning chains via 'observe-reason-act' cycle. Uses supervised fine-tuning to integrate dynamic tool interactions (cropping, zooming, reusing) and creates TWI-Tools-146K dataset with executable visual operations and rigorous verification.

Result: Demonstrates superior performance on diverse benchmarks, outperforming existing approaches. Found that introducing additional visual tokens for interleaved reasoning, reusing/magnifying original images improves visual reasoning and fine-grained perception, while image cropping based on visual grounding enhances focus on key entities.

Conclusion: Simple o3 establishes a powerful yet computationally affordable paradigm for advancing multimodal reasoning, providing the first in-depth analysis of different interleaved reasoning strategies and their impact on model performance.

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [121] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit is a hybrid virtual try-on system that combines warping and diffusion methods to preserve fine garment details like logos while achieving realistic results.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based virtual try-on methods often fail to preserve critical fine-grained garment details such as logos and printed text, which are essential for brand integrity and customer trust.

Method: Two-stage approach: 1) Warps target garment using learned flow field for high-fidelity preservation, 2) Uses fidelity-preserving try-on module with preserved-region input and inpainting mask to blend warped garment while retaining key areas and regenerating only necessary regions.

Result: Extensive qualitative results show visually seamless try-on results while faithfully maintaining high-frequency garment details, achieving effective balance between reconstruction accuracy and perceptual realism.

Conclusion: DualFit successfully addresses the limitation of detail preservation in virtual try-on systems through its hybrid warping-diffusion approach, maintaining brand-critical elements while delivering realistic visual outcomes.

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [122] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef is a tri-level quantization-aware defense framework that protects quantized neural networks against transferable patch-based adversarial attacks across different bit-widths by disrupting both semantic and gradient alignment.


<details>
  <summary>Details</summary>
Motivation: Quantized Neural Networks (QNNs) show limited robustness against patch-based adversarial attacks that remain transferable across different quantization bit-widths, and existing defenses either overfit to fixed quantization settings or fail to address this cross-bit generalization vulnerability.

Method: TriQDef consists of three components: (1) Feature Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing perceptual similarity in intermediate representations; (2) Gradient Perceptual Dissonance Penalty (GPDP) that misaligns input gradients across bit-widths using Edge IoU and HOG Cosine metrics; (3) Joint Quantization-Aware Training Protocol that unifies these penalties in a shared-weight training scheme across multiple quantization levels.

Result: Extensive experiments on CIFAR-10 and ImageNet show TriQDef reduces Attack Success Rates (ASR) by over 40% on unseen patch and quantization combinations while preserving high clean accuracy.

Conclusion: The findings highlight the importance of disrupting both semantic and perceptual gradient alignment to mitigate patch transferability in QNNs, providing an effective defense framework that generalizes across different quantization settings.

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [123] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,AndrÃ© Araujo,OndÅ™ej Chum*

Main category: cs.CV

TL;DR: A fine-tuning method that balances domain adaptation and knowledge retention in vision-language models, achieving strong retrieval performance without catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Large-scale contrastive pre-trained VLMs produce suboptimal embeddings for fine-grained open-set visual retrieval, and naive fine-tuning causes catastrophic forgetting of general-purpose capabilities.

Method: Proposes a fine-tuning method inspired by continual learning, combining regularization techniques for knowledge retention, with careful validation set design and hyperparameter tuning.

Result: Consistently achieves strong results on fine-grained and coarse-grained retrieval benchmarks, retaining visual-text alignment without using text data or original text encoder during fine-tuning.

Conclusion: The method effectively balances domain adaptation with retention of pretrained multimodal knowledge, providing robust generalization across datasets and pretrained models.

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [124] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: KP-INR is a dual-branch implicit neural representation method for cardiac cine MRI reconstruction that processes both positional embeddings and local multi-scale k-space features, achieving improved performance over baseline models.


<details>
  <summary>Details</summary>
Motivation: Current INR methods for cardiac cine MRI reconstruction focus only on coordinate-based positional embeddings while ignoring feature representations of target points and neighboring context, limiting reconstruction quality.

Method: Proposed KP-INR with dual branches: one processes positional embeddings of k-space coordinates, the other learns from local multi-scale k-space feature representations. Uses cross-branch interaction to approximate target k-space values.

Result: Experiments on CMRxRecon2024 dataset confirm improved performance over baseline models, demonstrating strong performance on challenging Cartesian k-space data.

Conclusion: KP-INR shows potential in cardiac cine MRI reconstruction by effectively leveraging both positional and feature information, addressing limitations of existing INR methods.

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [125] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: FB-Mem is a novel segmentation-based metric that quantifies memorized regions in diffusion model outputs, revealing more pervasive memorization patterns than previously detected and showing current mitigation methods fail to eliminate local memorization.


<details>
  <summary>Details</summary>
Motivation: Current detection methods only identify verbatim memorization but fail to capture partial memorization in small image regions and complex memorization patterns beyond specific prompt-image pairs.

Method: Proposed Foreground Background Memorization (FB-Mem), a segmentation-based metric that classifies and quantifies memorized regions within generated images using a clustering approach.

Result: Revealed that memorization is more pervasive: (1) single prompts link to clusters of similar training images showing complex patterns, (2) existing mitigation methods fail to eliminate local memorization, especially in foreground regions.

Conclusion: Established an effective framework for measuring memorization in diffusion models, demonstrated inadequacy of current mitigation approaches, and proposed a stronger clustering-based mitigation method.

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [126] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk is a novel framework for emotional talking head synthesis that uses VAE-generated 3D landmarks combined with emotion embeddings and a tri-plane attention NeRF to achieve superior emotion accuracy, controllability, and identity preservation compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current talking head synthesis methods excel at lip synchronization and image quality but fail to generate accurate and controllable emotional expressions while preserving subject identity, which is critical for artificial social intelligence.

Method: Uses VAE to generate 3D facial landmarks from audio, concatenates with emotion-label embeddings via ResNet-based landmark deformation model, then uses emotional landmarks and facial blendshape coefficients to condition a novel tri-plane attention Neural Radiance Field (NeRF) for synthesis.

Result: Extensive experiments show RealTalk outperforms existing methods in emotion accuracy, controllability, and identity preservation.

Conclusion: RealTalk advances the development of socially intelligent AI systems by enabling highly realistic emotional talking head synthesis with improved emotional expressiveness and identity preservation.

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [127] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse is a prompt-based framework that generates realistic RF signals from simulated indoor scenes with human motions, enabling scalable RF data generation for privacy-preserving indoor perception tasks.


<details>
  <summary>Details</summary>
Motivation: Collecting high-quality RF data in dynamic and diverse indoor environments is challenging, and there's a need for privacy-preserving alternatives to vision-based methods for indoor perception.

Method: WaveVerse uses a language-guided 4D world generator with state-aware causal transformer for human motion generation, and a phase-coherent ray tracing simulator for accurate RF signal simulation.

Result: The framework effectively generates conditioned human motions and enables phase-coherent RF signal simulation. It achieves performance gains in both data-limited and data-adequate scenarios for RF imaging and human activity recognition.

Conclusion: WaveVerse provides a scalable solution for RF data generation, enabling applications in RF imaging for the first time and consistently improving performance across various data scenarios.

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [128] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: A unified kernel- and feature-agnostic formulation for feature lifting in 3D scene understanding that solves as a sparse linear inverse problem with provable optimal error bounds and regularization strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimally assigning rich image feature descriptors to 3D primitives while handling inconsistency issues from multi-view images in splat-based 3D representations.

Method: Formulates feature lifting as a sparse linear inverse problem solvable in closed form, with Tikhonov Guidance for numerical stability and Post-Lifting Aggregation for noise filtering through feature clustering.

Result: Achieves state-of-the-art performance on open-vocabulary 3D segmentation benchmarks, outperforming training-based, grouping-based, and heuristic baselines while producing lifted features in minutes.

Conclusion: The approach provides a unified, efficient solution for high-quality feature lifting with provable error bounds and effective regularization strategies for multi-view consistency.

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [129] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: Optimized YOLOv11 for cotton disease detection with improved small-target feature extraction, dynamic category weighting, and enhanced data augmentation, achieving 8-10.5% mAP improvement and 158 FPS inference speed for real-time agricultural monitoring.


<details>
  <summary>Details</summary>
Motivation: Address three key challenges in cotton disease detection: low precision in early spot detection (35% leakage rate for sub-5mmÂ² spots), performance degradation in field conditions (25% accuracy drop), and high error rates (34.7%) in multi-disease scenarios.

Method: Proposed C2PSA module for enhanced small-target feature extraction, dynamic category weighting to handle sample imbalance, and improved data augmentation via Mosaic-MixUp scaling. Based on YOLOv11 architecture.

Result: Experimental results on 4,078-image dataset show: mAP50: 0.820 (+8.0% improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS. Mobile-deployed system enables real-time monitoring.

Conclusion: The optimized YOLOv11 system successfully addresses cotton disease detection challenges and enables practical real-time disease monitoring and precision treatment in agricultural applications with significant performance improvements.

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [130] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: A generative neural physics framework that combines deep learning with physics-informed simulation for fast 3D ultrasound computed tomography, enabling high-resolution musculoskeletal imaging with quantitative tissue parameter mapping.


<details>
  <summary>Details</summary>
Motivation: Conventional ultrasound computed tomography (USCT) is limited for musculoskeletal imaging due to ray-based reconstructions that neglect strong scattering effects, preventing accurate quantitative tissue characterization.

Method: Proposes a generative neural physics framework that couples generative networks with physics-informed neural simulation, learning ultrasonic wave propagation from cross-modality images to create an efficient surrogate model.

Result: Achieves 3D tissue parameter mapping in under 10 minutes on synthetic and in vivo data (breast, arm, leg), with MRI-comparable resolution and sensitivity to biomechanical properties in muscle and bone.

Conclusion: Overcomes computational bottlenecks in strongly scattering regimes, advancing USCT toward routine clinical assessment of musculoskeletal disease through fast, high-fidelity quantitative imaging.

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [131] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: A new Weather-eXtended Salient Object Detection (WXSOD) dataset with 14,945 weather-affected images and proposed Weather-aware Feature Aggregation Network (WFANet) that outperforms 17 existing methods by leveraging weather prediction for better saliency detection.


<details>
  <summary>Details</summary>
Motivation: Existing salient object detection methods perform well in clean natural scenes but struggle with weather noise. There's a lack of datasets with pixel-wise annotations for weather-affected scenes, limiting research in this area.

Method: Created WXSOD dataset with synthesized and real weather noise test sets. Proposed WFANet - a two-branch network: weather prediction branch extracts weather features, and saliency detection branch fuses semantic and weather features for improved detection.

Result: WFANet achieves superior performance compared to 17 existing SOD methods on the new WXSOD benchmark, demonstrating effectiveness in handling weather noise.

Conclusion: The WXSOD dataset fills an important gap in weather-affected SOD research, and the proposed WFANet provides an effective baseline for future work in this challenging domain.

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [132] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: SCTR framework combines superpixels with continuous low-rank tensor representation to overcome limitations of traditional LRTR methods, achieving 3-5 dB PSNR improvements across various data types.


<details>
  <summary>Details</summary>
Motivation: Traditional low-rank tensor representation methods assume holistic data is low-rank and are limited to discrete meshgrid data, which doesn't hold in real-world scenarios with spatial variations.

Method: Uses superpixels as basic modeling units and proposes asymmetric low-rank tensor factorization with superpixel-specific factor matrices parameterized by a shared neural network with specialized heads, separating global pattern learning from local adaptation.

Result: Achieves 3-5 dB PSNR improvements over existing LRTR-based methods across multispectral images, videos, and color images on benchmark datasets.

Conclusion: SCTR provides a continuous and flexible modeling framework that captures both cross-superpixel commonalities and within-superpixel variations, balancing expressiveness with compact representation.

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [133] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: Proposes Region-level Context-aware Multimodal Understanding (RCMU) to integrate textual context with visual objects, introduces RCVIT training method, RCMU dataset, RC&P-Bench benchmark, and achieves state-of-the-art performance with RC-Qwen2-VL models.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs focus on general visual understanding but lack the ability to integrate textual context associated with specific objects/regions for context-aware multimodal understanding.

Method: Proposes Region-level Context-aware Visual Instruction Tuning (RCVIT) that incorporates object information and bounding box coordinates to associate visual content with textual information. Creates RCMU dataset and RC&P-Bench benchmark for training and evaluation.

Result: RC-Qwen2-VL models achieve outstanding performance on multiple RCMU tasks and demonstrate successful applications in multimodal RAG and personalized conversation.

Conclusion: The proposed RCMU framework effectively addresses the limitation of current MLLMs by enabling region-level context-aware understanding, with demonstrated superior performance and practical applications.

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [134] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: SNNSIR is a fully spike-driven Spiking Neural Network for stereo image restoration that achieves competitive performance with significantly reduced computational overhead through spike-compatible modules like SRBB, SSCM, and SSCA.


<details>
  <summary>Details</summary>
Motivation: Existing hybrid SNN-ANN models still rely on floating-point operations incompatible with SNNs' binary nature. There's a need for fully spike-driven architectures to achieve truly low-power and hardware-friendly computation for stereo image restoration tasks.

Method: Proposed SNNSIR with three key components: 1) Spike Residual Basic Block (SRBB) for enhanced information flow via spike-compatible residual learning, 2) Spike Stereo Convolutional Modulation (SSCM) for simplified nonlinearity and cross-view-aware modulation, 3) Spike Stereo Cross-Attention (SSCA) for efficient bidirectional feature interaction across views within spike-compatible framework.

Result: Extensive experiments on diverse stereo image restoration tasks (rain streak removal, raindrop removal, low-light enhancement, super-resolution) demonstrate competitive restoration performance while significantly reducing computational overhead.

Conclusion: The model shows potential for real-time, low-power stereo vision applications, offering a fully spike-driven solution that maintains performance while being hardware-friendly and energy-efficient.

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [135] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: Dynamic semantic segmentation network adaptation for autonomous driving hardware using three-tier control mechanism and Bayesian optimization to optimize computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving platforms face diverse scenarios with varying hardware resources and precision requirements, requiring computational cost consideration for deployment on embedded devices like NVIDIA DRIVE PX 2.

Method: Three-tier control mechanism (width multiplier, classifier depth, classifier kernel) for fine-grained model control, combined with Bayesian Optimization for efficient hyperparameter search under tight computational budgets.

Result: Enables broad model scaling, targeted refinement of final layers, and scenario-specific optimization, leading to improved resource allocation and performance with task-specific learning adaptation.

Conclusion: The approach successfully addresses scenario-specific and task-specific requirements through automatic parameter search, maximizing computational capacity and model accuracy while optimizing hardware utilization for diverse self-driving tasks.

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [136] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: CLAIR: A weakly supervised zero-shot cross-domain image retrieval method that refines noisy CLIP-generated pseudo-labels and uses contrastive learning with cross-domain mapping for superior performance.


<details>
  <summary>Details</summary>
Motivation: Large foundation models like CLIP can generate pseudo-labels for unlabeled data, making unsupervised zero-shot cross-domain retrieval less relevant. The paper focuses on weakly supervised approach using noisy pseudo-labels from CLIP.

Method: Proposes CLAIR with confidence score refinement for noisy pseudo-labels, inter-instance/inter-cluster contrastive losses, inter-domain contrastive loss, closed-form cross-domain mapping function using CLIP text embeddings, and learnable prompts for zero-shot generalization.

Result: Extensive experiments on TUBerlin, Sketchy, Quickdraw, and DomainNet datasets show CLAIR consistently outperforms state-of-the-art methods.

Conclusion: CLAIR effectively handles weakly supervised zero-shot cross-domain image retrieval with noisy pseudo-labels, demonstrating superior performance through refined confidence scoring, contrastive learning, and cross-domain alignment techniques.

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [137] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: Improved 3D Gaussian Splatting densification with edge-aware candidate selection, long-axis splitting strategy, and overfitting mitigation techniques for better reconstruction quality with fewer Gaussians.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting achieves real-time rendering but suffers from suboptimal reconstruction quality due to inefficient densification strategies that cause geometric distortions and overfitting.

Method: Proposes Edge-Aware Score for candidate selection, Long-Axis Split strategy to reduce geometric distortions, and overfitting mitigation techniques including Recovery-Aware Pruning, Multi-step Update, and Growth Control.

Result: Achieves state-of-the-art performance with enhanced rendering fidelity using fewer Gaussians, without additional training or inference overhead.

Conclusion: The comprehensive improvements to the densification pipeline significantly enhance 3DGS reconstruction quality while maintaining computational efficiency.

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [138] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: NCA-WSS: A weakly supervised segmentation method using neural cellular automata that leverages classification feature maps to extract segmentation masks without needing segmentation labels, outperforming existing approaches on white blood cell datasets.


<details>
  <summary>Details</summary>
Motivation: Training robust medical image segmentation models requires large labeled datasets which are time-consuming and expensive to acquire. Weakly supervised approaches can reduce this labeling burden.

Method: Proposes neural cellular automata for weakly supervised segmentation (NCA-WSS) that extracts segmentation masks from feature maps generated during classification, eliminating need for segmentation labels during training.

Result: Significantly outperforms existing weakly supervised approaches on three white blood cell microscopy datasets.

Conclusion: Demonstrates NCA's potential for both classification and segmentation in weakly supervised frameworks, providing scalable and efficient medical image analysis solution.

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [139] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: Integration of attention pooling with Neural Cellular Automata (NCA) improves microscopy image classification performance while maintaining parameter efficiency and explainability.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between NCA and larger architectures for microscopy image analysis while preserving interpretability.

Method: Combine attention pooling mechanism with Neural Cellular Automata to enhance feature extraction by focusing on the most informative regions.

Result: Significantly outperforms existing NCA methods on eight microscopy datasets, shows improved performance over lightweight CNNs and vision transformers while maintaining lower parameter count.

Conclusion: NCA-based models with attention pooling offer a promising explainable alternative for image classification, demonstrating enhanced accuracy with parameter efficiency.

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [140] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: DoppDrive is a Doppler-driven temporal aggregation method that enhances radar point cloud density for autonomous driving by reducing scatter from dynamic objects through radial shifting and adaptive aggregation durations.


<details>
  <summary>Details</summary>
Motivation: Radar's long detection range is crucial for autonomous driving, but sparse point clouds at long range and scatter from temporal aggregation with ego-motion compensation degrade detection performance.

Method: Points from previous frames are shifted radially based on their dynamic Doppler component to eliminate radial scatter, with each point assigned a unique aggregation duration based on Doppler and angle to minimize tangential scatter.

Result: DoppDrive significantly improves object detection performance across various detectors and datasets as a compatible pre-detection enhancement step.

Conclusion: The proposed Doppler-driven temporal aggregation method effectively enhances radar point cloud density while minimizing scatter, making it a valuable preprocessing step for radar-based object detection in autonomous driving.

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [141] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: A geometry-aware learning framework that removes HMD occlusions and reconstructs complete 3D facial geometry from single-view RGB videos using GAN-based inpainting and 3DMM parameter regression.


<details>
  <summary>Details</summary>
Motivation: HMDs obscure the upper face, complicating video recording and impacting social XR applications where facial expressions and eye gaze are crucial for immersive experiences.

Method: GAN-based video inpainting guided by dense facial landmarks and reference frames, followed by SynergyNet-based 3DMM parameter regression for 3D reconstruction with dense landmark optimization throughout the pipeline.

Result: Successfully removes HMDs while preserving facial identity and realism, producing photorealistic 3D face geometry outputs. Robust across different landmark densities with minor quality degradation under sparse configurations.

Conclusion: The framework effectively addresses HMD occlusion issues in XR applications, enabling complete facial reconstruction for improved social interactions and immersive experiences.

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [142] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: SDD is a semantic discrepancy-aware detector that uses reconstruction learning to align forgery and semantic concept spaces for improved fake image detection.


<details>
  <summary>Details</summary>
Motivation: The misalignment between forgery and semantic concept spaces hinders detection performance, requiring better space alignment for robust forgery identification.

Method: Uses semantic token sampling to mitigate space shifts, concept-level forgery discrepancy learning with visual reconstruction, and low-level forgery feature enhancement to minimize redundant information.

Result: Achieves superior results on two standard image forgery datasets compared to existing methods.

Conclusion: SDD effectively aligns semantic and forgery spaces through reconstruction learning, significantly improving fake image detection performance.

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [143] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. BriÃ£o,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat is a plug-and-play feature enhancement module that improves underwater object detection by optimizing features specifically for detection tasks rather than general image enhancement.


<details>
  <summary>Details</summary>
Motivation: Underwater environments cause severe image degradation that impairs object detection models, and traditional image enhancement methods are not optimized for downstream detection tasks.

Method: A multi-scale feature enhancement network trained end-to-end with the detector's loss function, ensuring enhancement is guided to refine features most relevant to detection tasks.

Result: State-of-the-art Precision (0.877) and Recall (0.624) with competitive mAP scores (mAP@0.5: 0.677, mAP@[0.5:0.95]: 0.421) on underwater datasets when integrated with YOLOv8m, while maintaining 46.5 FPS processing speed.

Conclusion: AquaFeat provides an effective and computationally efficient solution for real-world underwater applications like marine ecosystem monitoring and infrastructure inspection by delivering accuracy gains with practical processing speed.

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [144] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: MBMamba improves Mamba architecture for image deblurring by adding memory buffer mechanism and Ising-inspired regularization to address local pixel forgetting and channel redundancy issues without increasing computational complexity.


<details>
  <summary>Details</summary>
Motivation: The Mamba architecture shows promise for image deblurring but suffers from local pixel forgetting and channel redundancy due to its flatten-and-scan strategy. Existing solutions increase computational complexity and hinder real-time performance.

Method: Proposed MBMamba with two key components: 1) memory buffer mechanism to preserve historical information for fusion and model relevance between adjacent features, 2) Ising-inspired regularization loss that simulates energy minimization of pixel mutual attraction to maintain image structure.

Result: Experimental results demonstrate that MBMamba outperforms state-of-the-art approaches on widely used benchmarks for image deblurring.

Conclusion: The proposed method successfully addresses Mamba's limitations in image deblurring without modifying the original architecture, achieving superior performance while maintaining computational efficiency.

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [145] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: EgoLoc is a zero-shot method for localizing hand-object contact and separation moments in egocentric videos without needing object masks or verb-noun taxonomies, using vision-language models and hand-dynamics-guided sampling.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on 'how to interact' but neglects the critical 'when to interact' problem - precisely localizing contact and separation moments between hands and objects, which is crucial for VR/AR applications and robotic policy transfer.

Method: Proposes EgoLoc with hand-dynamics-guided sampling to generate visual prompts, exploits vision-language models to identify contact/separation attributes and localize timestamps, and uses closed-loop feedback for refinement.

Result: Achieves plausible temporal interaction localization on public datasets and novel benchmarks, effectively facilitates downstream applications in egocentric vision and robotic manipulation tasks.

Conclusion: EgoLoc provides a generalizable zero-shot solution for fine-grained hand-object interaction analysis, eliminating the need for object masks and verb-noun taxonomies while demonstrating strong performance in various applications.

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [146] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. GÃ¼zel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: A simple two-step data augmentation method using diffusion models to generate synthetic training data in latent space, improving offline RL generalization without algorithmic changes.


<details>
  <summary>Details</summary>
Motivation: Offline RL policies struggle to generalize due to limited diverse states in visual data, with noise and spurious correlations causing overfitting and poor generalization to unseen environments.

Method: Two-step process: 1) augment original offline data to introduce diversity for zero-shot generalization, 2) use diffusion model to generate additional synthetic data in latent space.

Result: Significantly improves generalization across continuous (Visual D4RL) and discrete (Procgen) action spaces, increases data diversity, reduces generalization gap, and maintains computational efficiency.

Conclusion: This approach enables better utilization of vision-based offline data and could fuel progress in synthetic data generation for training more general agents.

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [147] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: IPGPhormer is an interpretable graph-transformer framework for cancer survival analysis from whole-slide images that balances spatial relationships with local context and provides tissue/cellular-level interpretability without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Existing survival analysis methods struggle to balance long-range spatial relationships with local contextual dependencies and lack inherent interpretability, limiting clinical utility.

Method: Interpretable Pathology Graph-Transformer (IPGPhormer) framework that captures tumor microenvironment characteristics and models spatial dependencies across tissue, providing interpretability at both tissue and cellular levels without post-hoc annotations.

Result: Outperforms state-of-the-art methods on four public benchmark datasets in both predictive accuracy and interpretability.

Conclusion: IPGPhormer offers a promising tool for cancer prognosis assessment, enabling more reliable and interpretable decision-support systems in pathology.

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [148] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: ViT-EnsembleAttack enhances adversarial transferability for Vision Transformers through model adversarial augmentation, multi-head dropping, attention scaling, and MLP feature mixing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing ensemble attacks focus on refining weights or paths but overlook exploring ensemble models themselves to enhance transferability, particularly for Vision Transformers which receive less attention.

Method: Applies adversarial augmentation to surrogate models using three strategies: Multi-head dropping, Attention score scaling, and MLP feature mixing. Uses Bayesian optimization for parameter optimization and includes Automatic Reweighting and Step Size Enlargement modules.

Result: Extensive experiments show ViT-EnsembleAttack significantly enhances adversarial transferability of ensemble-based attacks on ViTs, outperforming existing methods by a substantial margin.

Conclusion: The proposed method effectively addresses the gap in ensemble model exploration for transferability enhancement, particularly for Vision Transformers, demonstrating superior performance through adversarial augmentation and optimized ensemble strategies.

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [149] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT is a framework that uses LLMs to decompose complex text instructions into structured semantic units, then integrates them into optimized prompts to significantly improve T2I model performance on challenging tasks.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with complex, long-form textual instructions, failing to accurately render intricate details, spatial relationships, and specific constraints as revealed by benchmarks like LongBench-T2I.

Method: Two-stage framework: 1) Complex Instruction Decomposition and Semantic Enhancement using LLMs to break down instructions into structured semantic units, 2) Multi-Stage Prompt Integration and Adaptive Generation to transform units into hierarchical or optimized prompts for T2I models.

Result: DeCoT consistently improves T2I model performance across all dimensions, achieving average score of 3.52 with Infinity-8B (vs 3.44 baseline), particularly excelling in "Text" and "Composition" aspects. Human evaluations confirm superior perceptual quality and instruction fidelity.

Conclusion: DeCoT effectively bridges the gap between high-level user intent and T2I model requirements, leading to more faithful and accurate image generation through sophisticated LLM prompting and decomposition techniques.

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [150] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: FedCSAP is a federated learning framework that enhances CLIP's performance by leveraging multi-scale visual features and client-specific style indicators to generate robust, context-aware prompts, outperforming existing methods in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Conventional prompt learning approaches using only final-layer features miss rich multi-scale visual cues and domain-specific style variations in decentralized client data, limiting performance in federated learning settings.

Method: FedCSAP harnesses low, mid, and high-level features from CLIP's vision encoder alongside client-specific style indicators from batch-level statistics, merging visual details with textual context to produce distinct, non-redundant prompt tokens.

Result: Comprehensive experiments on multiple image classification datasets confirm that FedCSAP outperforms existing federated prompt learning methods in both accuracy and overall generalization across seen and unseen classes.

Conclusion: The framework successfully bridges the gap in conventional approaches by effectively utilizing multi-scale visual features and style variations while maintaining data privacy through federated learning, demonstrating superior performance in diverse classification tasks.

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [151] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: MPCAR is an inference-time strategy that enhances Large Vision-Language Models' reasoning by generating multiple diverse descriptions from different angles, integrating them with the original question, and using this enriched context for final answer generation without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with complex visual reasoning tasks requiring deep contextual understanding and multi-angle analysis due to limitations of single-shot image encoding and prompts.

Method: Three-stage approach: 1) LVLM generates N diverse complementary descriptions/preliminary reasoning paths, 2) integrates descriptions with original question to create comprehensive context-augmented prompt, 3) uses enriched prompt for deep reasoning and final answer generation.

Result: Significant accuracy gains on challenging VQA datasets (GQA, VQA-CP v2, ScienceQA), particularly for tasks requiring robust contextual understanding. Human evaluations confirm improved coherence and completeness.

Conclusion: MPCAR effectively leverages LVLMs' generative capabilities to enrich input contexts, unlocking latent reasoning potential for complex multimodal tasks without parameter fine-tuning.

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [152] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: LMAD is a novel vision-language framework that enhances autonomous driving by improving scene understanding and spatial awareness through specialized expert adapters and scene interaction modules, achieving state-of-the-art performance on driving reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLM fine-tuning methods for autonomous driving lack holistic scene recognition and spatial awareness needed for complex driving situations, creating a gap in explainable driving behavior.

Method: Proposes LMAD framework that incorporates comprehensive scene understanding with specialized expert adapters and preliminary scene interaction within a task-specialized structure, compatible with existing VLMs and planning systems.

Result: Extensive experiments on DriveLM and nuScenes-QA datasets show LMAD significantly boosts performance of existing VLMs on driving reasoning tasks.

Conclusion: LMAD sets a new standard in explainable autonomous driving by better aligning VLMs with autonomous driving scenarios through specialized architecture design.

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [153] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: S5 is a scalable semi-supervised semantic segmentation framework for remote sensing that leverages large-scale unlabeled Earth observation data through data selection strategies and foundation model pre-training, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised semantic segmentation studies in remote sensing rely on small-scale datasets and models, limiting practical applicability. There's vast unlabeled Earth observation data that remains underutilized due to costly pixel-level annotations.

Method: Proposed S5 framework with data selection strategy (entropy-based filtering + diversity expansion) creating RS4P-1M dataset. Pre-trained RS foundation models of varying sizes on this corpus, and incorporated Mixture-of-Experts-based multi-dataset fine-tuning for efficient adaptation to multiple benchmarks.

Result: The resulting RS foundation models achieved state-of-the-art performance across all benchmarks in land cover segmentation and object detection tasks, demonstrating significant performance boosts.

Conclusion: S5 demonstrates the viability of scaling semi-supervised learning for remote sensing applications, improving generalization and versatility of foundation models across diverse RS benchmarks with fewer parameters.

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [154] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: SRMA-Mamba is a novel Mamba-based network for 3D liver cirrhosis segmentation that integrates spatial anatomical information from multiple MRI planes and uses attention mechanisms to refine segmentation details.


<details>
  <summary>Details</summary>
Motivation: Early detection of liver cirrhosis is crucial for reducing mortality, but existing methods underutilize spatial anatomical details in volumetric MRI data, limiting clinical effectiveness and explainability.

Method: Proposes SRMA-Mamba network with Spatial Anatomy-Based Mamba module (SABMamba) that performs selective scans within cirrhotic tissues and combines anatomical information from sagittal, coronal, and axial planes. Also introduces Spatial Reverse Attention module (SRMA) to progressively refine segmentation details using coarse maps and hierarchical encoding features.

Result: Extensive experiments show SRMA-Mamba surpasses state-of-the-art methods and delivers exceptional performance in 3D pathological liver segmentation.

Conclusion: The proposed SRMA-Mamba network effectively addresses the challenge of modeling spatial relationships in complex liver anatomical structures, providing superior volumetric segmentation performance for liver cirrhosis detection.

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [155] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: TiP4GEN is a text-to-dynamic panorama generation framework that creates immersive 360-degree 4D scenes with fine-grained control, combining panorama video generation with geometry-consistent dynamic scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing VR/AR generation works focus on static scenes or narrow perspective-view dynamic scenes, lacking true 360-degree immersive experiences from any viewpoint.

Method: Uses a dual-branch generation model (panorama + perspective branches) with bidirectional cross-attention for video generation, and a geometry-aligned reconstruction model based on 3D Gaussian Splatting with metric depth maps and estimated camera poses.

Result: Extensive experiments show the framework effectively generates visually compelling and motion-coherent dynamic panoramic scenes with geometric consistency and temporal coherence.

Conclusion: TiP4GEN successfully addresses the gap in immersive 360-degree dynamic scene generation, providing high-quality panoramic 4D scenes with fine-grained content control.

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [156] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: Comparison of human and AI visual perception through illusions reveals critical differences, showing AI has unique vulnerabilities like pixel sensitivity and hallucinations, while also exhibiting some human-like illusion effects.


<details>
  <summary>Details</summary>
Motivation: To understand how artificial vision systems differ from human perception and identify AI-specific vulnerabilities that could impact trust and safety in AI applications.

Method: Systematic comparison of human and AI responses to classic visual illusions involving color, size, shape, and motion, analyzing both targeted training effects and emergent pattern recognition behaviors.

Result: AI exhibits some illusion-like effects similar to humans but also has unique illusions (pixel-level sensitivity, hallucinations) without human counterparts, revealing alignment gaps and AI-specific perceptual vulnerabilities.

Conclusion: Findings provide insights for developing more robust, interpretable AI vision systems that preserve beneficial human perceptual biases while avoiding distortions that undermine trust and safety.

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [157] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: This paper exposes vulnerabilities in VQA-NLE systems where models produce inconsistent explanations and reach conclusions without genuine understanding, proposes novel adversarial attacks on both questions and images, and introduces knowledge-based mitigation methods to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Existing VQA-NLE systems can produce inconsistent explanations and make decisions without truly understanding context, revealing weaknesses in their inference pipelines and explanation mechanisms that need to be addressed for better transparency and reliability.

Method: The authors leverage existing adversarial question perturbation strategies and propose a novel strategy that minimally alters images to induce contradictory outputs. They also introduce a mitigation method that uses external knowledge to alleviate inconsistencies and improve model robustness.

Result: Extensive evaluations on two standard benchmarks and two widely used VQA-NLE models demonstrate the effectiveness of the proposed attacks and show the potential of knowledge-based defenses in addressing security and reliability concerns.

Conclusion: The research reveals pressing security and reliability concerns in current VQA-NLE systems, highlighting the need for more robust explanation mechanisms and demonstrating that knowledge-based approaches can help mitigate vulnerabilities in these black-box models.

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [158] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: X-Ray-CoT is a novel framework using Vision-Language Large Models for chest X-ray diagnosis that generates interpretable reports by simulating radiologists' chain-of-thought reasoning, achieving competitive accuracy while providing explainable outputs.


<details>
  <summary>Details</summary>
Motivation: Chest X-ray interpretation requires extensive clinical expertise and suffers from variability between observers. While deep learning models offer high accuracy, their black-box nature prevents clinical adoption in high-stakes medical settings where interpretability is crucial.

Method: X-Ray-CoT framework extracts multi-modal features and visual concepts, then uses an LLM-based component with structured Chain-of-Thought prompting strategy to reason and generate detailed natural language diagnostic reports, simulating human radiologists' thought process.

Result: On the CORDA dataset, X-Ray-CoT achieves Balanced Accuracy of 80.52% and F1 score of 78.65% for disease diagnosis, slightly surpassing existing black-box models while uniquely generating high-quality, explainable reports as validated by human evaluations.

Conclusion: The framework represents a significant step towards trustworthy and clinically actionable AI systems in medical imaging, with ablation studies confirming the necessity of multi-modal fusion and CoT reasoning for robust and transparent medical AI.

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [159] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: Inverse-LLaVA eliminates alignment pre-training by mapping text to visual space instead of visual to text space, achieving better reasoning performance with 45% less computation while showing trade-offs in perception tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional multimodal learning requires expensive alignment pre-training and projects visual features into text space. The authors challenge these assumptions to reduce computational costs and explore alternative fusion approaches.

Method: Inverts conventional mapping direction by projecting text embeddings into continuous visual representation space, using selective additive components in attention mechanisms for dynamic integration without alignment datasets.

Result: Shows nuanced trade-offs: improvements on reasoning tasks (MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%) but decreases in perception tasks (celebrity recognition: -49.5%, OCR: -21.3%) with 45% computational reduction.

Conclusion: Alignment pre-training is unnecessary for effective multimodal learning, especially for complex reasoning. Establishes a new paradigm that reduces computation, challenges conventional fusion wisdom, and enables efficient architectures preserving modality characteristics.

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [160] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: A hybrid AI system combining fine-tuned vision-language models and reasoning LLMs for automated H-reflex EMG waveform interpretation and neuromuscular diagnostics.


<details>
  <summary>Details</summary>
Motivation: Traditional H-reflex analysis suffers from variability and interpretation bias among clinicians, limiting reliability and standardization in sports science and rehabilitation.

Method: Fine-tuned multiple VLMs on annotated H-reflex EMG waveform images with clinical data, aggregated outputs using consensus method, and refined with specialized reasoning LLM using prompt engineering and automated reasoning workflows.

Result: The hybrid system delivers highly accurate, consistent, and interpretable H-reflex assessments, significantly advancing automation and standardization of neuromuscular diagnostics.

Conclusion: First integration of fine-tuned VLM consortium with reasoning LLM for image-based H-reflex analysis, laying foundation for next-generation AI-assisted neuromuscular assessment platforms.

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [161] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: Hybrid CNN-Transformer models with Convolutional Kolmogorov-Arnold Network achieve state-of-the-art skin cancer classification performance across multiple datasets through effective feature fusion and representation learning.


<details>
  <summary>Details</summary>
Motivation: Precise differentiation between malignant and non-malignant skin lesions is crucial for early diagnosis and treatment, requiring advanced models that can capture both local and global features effectively.

Method: Sequential and Parallel Hybrid CNN-Transformer models integrated with Convolutional Kolmogorov-Arnold Network (CKAN), using transfer learning and extensive data augmentation. CNNs extract local spatial features, Transformers model global dependencies, and CKAN facilitates nonlinear feature fusion.

Result: Achieved 92.81% accuracy and 92.47% F1-score on HAM10000, 97.83% accuracy and 97.83% F1-score on PAD-UFES, and 91.17% accuracy with 91.79% F1-score on BCN20000 dataset, demonstrating strong generalization across diverse datasets.

Conclusion: Hybrid CNN-Transformer architectures with CKAN effectively capture both spatial and contextual features, enhancing classification performance and demonstrating the importance of feature representation and model design in medical image analysis.

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [162] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-SÃ¡nchez,Abraham SÃ¡nchez-Perez,RaÃºl Nanclares Da Veiga,Alejandro Zarate-MacÃ­as,Edgar Villareal,Alejandro SÃ¡nchez-Montes,Edtna Jauregui-Ulloa,HÃ©ctor Moreno,Ulises CortÃ©s*

Main category: cs.CV

TL;DR: RAIS-DR is a responsible AI system for diabetic retinopathy screening that outperforms FDA-approved EyeArt system with improved accuracy, fairness, and ethical alignment across demographic groups.


<details>
  <summary>Details</summary>
Motivation: Diabetic Retinopathy is a leading cause of vision loss, but early detection is hindered by shortage of specialists and biased AI systems that may learn unintended features from low-quality data.

Method: Developed RAIS-DR system incorporating ethical principles across AI lifecycle, using efficient convolutional models for preprocessing, quality assessment, and three specialized DR classification models.

Result: RAIS-DR outperformed FDA-approved EyeArt on 1,046 patient dataset with F1 scores increasing by 5-12%, accuracy by 6-19%, specificity by 10-20%, and demonstrated equitable performance across demographic subgroups.

Conclusion: RAIS-DR represents a robust and ethically aligned solution for DR screening that reduces healthcare disparities and is suitable for clinical implementation.

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [163] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: LangVision-LoRA-NAS integrates Neural Architecture Search with LoRA to dynamically optimize rank configurations for Vision Language Models, improving performance while reducing fine-tuning costs.


<details>
  <summary>Details</summary>
Motivation: Current LoRA implementations use fixed ranks, limiting flexibility and efficiency across diverse multimodal tasks. There's a need for adaptive rank selection to balance performance and computational costs.

Method: Combines Neural Architecture Search (NAS) with LoRA to dynamically search for optimal rank configurations tailored to specific multimodal tasks. Uses LLaMA-3.2-11B model and evaluates on multiple datasets.

Result: Demonstrates notable improvement in model performance while reducing fine-tuning costs. Provides both base and searched fine-tuned models for LLaMA-3.2-11B-Vision-Instruct.

Conclusion: LangVision-LoRA-NAS offers an effective framework for optimizing VLMs through adaptive rank selection, achieving better performance with lower computational costs compared to fixed-rank LoRA approaches.

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [164] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: Cross-View Transformers can effectively map camera images to Bird's-Eye View maps for autonomous driving, with best results using L1 loss and 4-camera setup, showing good generalization to unseen environments.


<details>
  <summary>Details</summary>
Motivation: Bird's-Eye View maps are crucial for autonomous driving perception, providing a structured top-down abstraction of the environment including roads, lane markings, and planned trajectories.

Method: Used Cross-View Transformers (CVT) to map camera images to BEV channels (road, lane markings, trajectory) using a realistic urban driving simulator. Tested different camera layouts and loss formulations (focal vs L1).

Result: A four-camera CVT trained with L1 loss using data from only one town delivered the most robust performance when tested in a new, unseen town, demonstrating good generalization capabilities.

Conclusion: Cross-View Transformers show promise for mapping camera inputs to reasonably accurate BEV maps, with the L1 loss formulation and four-camera setup providing the best generalization performance to new environments.

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [165] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: MuSACo is a multi-modal subject-specific adaptation method for expression recognition that uses co-training to select relevant source subjects and leverage complementary information across modalities, outperforming existing MSDA methods on challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Current MSDA methods for personalized expression recognition often overlook multimodal information or blend sources into a single domain, failing to capture unique subject-specific characteristics needed for applications like patient-specific stress or pain assessment.

Method: MuSACo uses co-training to select source subjects relevant to the target, generates pseudo-labels using the dominant modality for class-aware learning, employs class-agnostic loss for less confident samples, and aligns source features while combining only confident target features across modalities.

Result: Experimental results on BioVid and StressID datasets show MuSACo outperforms UDA (blending) and state-of-the-art MSDA methods.

Conclusion: MuSACo effectively addresses limitations of existing MSDA approaches by leveraging multimodal information and preserving subject diversity, making it particularly suitable for affective computing applications where subject-level nuances are crucial.

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [166] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: REVEAL framework uses vision-language models for prompt-driven forgery detection through holistic scene evaluation and region-wise anomaly analysis across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Existing forgery detection methods struggle with generalization across domains and lack reasoning capabilities, requiring robust frameworks that can detect and interpret visual forgeries while providing localization.

Method: Proposes REVEAL framework that leverages large vision-language models for semantic alignment, using two approaches: (1) holistic scene-level evaluation assessing physics, semantics, perspective, and realism, and (2) region-wise anomaly detection by analyzing multiple image regions.

Result: Conducted experiments across diverse domains (Photoshop, DeepFake, AIGC editing) and compared vision language models against competitive baselines while analyzing their reasoning capabilities.

Conclusion: The prompt-driven visual reasoning approach using vision-language models provides effective forgery detection with reasoning and localization capabilities across multiple manipulation domains.

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [167] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: SFAC is a CNN-based algorithm that colorizes old photos using only two images (target and reference) with structure-preserving feature alignment, eliminating need for large datasets.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning colorization methods struggle with old photos due to lack of ground truth and domain gap between natural gray images and old photos.

Method: Uses feature distribution alignment loss for semantic correspondence, plus structure-preserving mechanism with perceptual constraint and frozen-updated pyramid to prevent distortions.

Result: Extensive experiments show effectiveness through qualitative and quantitative metrics for old photo colorization.

Conclusion: SFAC successfully addresses domain gap problem in old photo colorization with minimal data requirements and robust structure preservation.

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [168] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: A unified skeleton foundation model (USDRL) for diverse action understanding tasks using transformer-based dense spatio-temporal encoding with multi-grained decorrelation and multi-perspective consistency training.


<details>
  <summary>Details</summary>
Motivation: Existing skeleton-based action understanding methods lack scalability and generalization across diverse tasks, with no existing foundation model that can handle various action understanding applications.

Method: Transformer-based Dense Spatio-Temporal Encoder (DSTE) with parallel streams for temporal and spatial features, Multi-Grained Feature Decorrelation (MG-FD) to reduce redundancy, and Multi-Perspective Consistency Training (MPCT) for self-supervised learning.

Result: Significantly outperforms state-of-the-art methods on 25 benchmarks across 9 different skeleton-based action understanding tasks covering coarse, dense, and transferred prediction.

Conclusion: USDRL serves as an effective foundation model that broadens research scope in skeleton-based action understanding and encourages more attention to dense prediction tasks.

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [169] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: MCOUT enables multimodal reasoning through continuous hidden vectors in joint latent space instead of language sequences, achieving up to 8.23% accuracy gains over traditional Chain-of-Thought methods.


<details>
  <summary>Details</summary>
Motivation: Traditional language-based reasoning methods like Chain-of-Thought are suboptimal for multimodal contexts, struggling to dynamically align audio, visual, and textual information effectively.

Method: Proposes Multimodal Chain of Continuous Thought (MCOUT) with two variants: MCOUT-Base reuses language model's last hidden state for iterative reasoning, and MCOUT-Multi integrates multimodal latent attention to strengthen cross-modal alignment between visual and textual features.

Result: Experiments on MMMU, ScienceQA, and MMStar benchmarks show consistent improvements in multimodal reasoning with up to 8.23% accuracy gains over baselines and up to 8.27% BLEU score improvements across multiple-choice and open-ended tasks.

Conclusion: Latent continuous reasoning represents a promising direction for advancing large multimodal models beyond language-bound approaches, offering a scalable framework for human-like reflective multimodal inference.

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [170] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD is a novel Large Vision Language Diffusion framework that uses parallel generation instead of autoregressive methods for autonomous driving, achieving faster inference, bidirectional reasoning, and near-zero failure rates.


<details>
  <summary>Details</summary>
Motivation: Autoregressive Vision Language Models for autonomous driving suffer from high inference latency and inability to perform bidirectional reasoning, making them unsuitable for safety-critical real-world applications.

Method: Uses a masked diffusion model that enables parallel generation of entire driving decision sequences, supporting bidirectional reasoning and progressive easy-first generation for iterative decision improvement.

Result: Outperforms state-of-the-art autoregressive VLM baselines on nuScenes dataset in both planning accuracy and inference speed, achieving near-zero failure rate. Successfully deployed in real-world autonomous vehicle for interactive parking task.

Conclusion: ViLaD represents a paradigm shift in autonomous driving systems, demonstrating practical viability with significantly reduced latency and improved decision quality through parallel generation and bidirectional reasoning capabilities.

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [171] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: This paper introduces ViDA-UGC, the first large-scale visual distortion assessment dataset for User-Generated Content images, featuring fine-grained quality annotations and a Chain-of-Thought framework that enables explainable image quality assessment surpassing GPT-4o performance.


<details>
  <summary>Details</summary>
Motivation: Current explainable IQA methods inadequately evaluate both UGC and AI-Generated Content using the same distortion criteria, and lack detailed quality analysis for monitoring and guiding image restoration.

Method: Created ViDA-UGC dataset with 11K images using human annotation and CoT framework to guide GPT-4o in generating quality descriptions. Also developed ViDA-UGC-Bench benchmark with 476 images and 6,149 QA pairs professionally validated.

Result: The ViDA-UGC dataset and CoT framework consistently enhanced various MLLMs' image quality analysis abilities across multiple benchmarks, even outperforming GPT-4o.

Conclusion: The proposed approach successfully addresses the limitations of current explainable IQA methods by providing detailed distortion analysis specifically for UGC images, demonstrating superior performance through comprehensive dataset construction and assessment framework.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [172] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: OpenMoCap is a novel motion capture model that addresses severe marker occlusion problems through a marker-joint chain inference mechanism and is trained on the new CMU-Occlu dataset with realistic occlusion patterns.


<details>
  <summary>Details</summary>
Motivation: Current motion capture systems suffer from performance degradation under large-scale marker occlusions common in real-world applications, due to lack of realistic training datasets and training strategies for long-range marker dependencies.

Method: Proposed OpenMoCap model with marker-joint chain inference mechanism for simultaneous optimization and construction of deep constraints between markers and joints, trained on CMU-Occlu dataset that uses ray tracing to simulate realistic occlusion patterns.

Result: Extensive experiments show OpenMoCap consistently outperforms competing methods across diverse scenarios with significant occlusions.

Conclusion: OpenMoCap provides robust motion capture in occlusion-heavy environments, with the CMU-Occlu dataset enabling future research, and the model is integrated into practical MoSen MoCap system.

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [173] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: WIPES is a wavelet-based visual representation that achieves high-quality rendering with fast inference by leveraging wavelet spatial-frequency localization, outperforming both INR-based methods in speed and Gaussian-based representations in quality.


<details>
  <summary>Details</summary>
Motivation: Existing visual representations suffer from spectrum loss due to frequency guidance or slow rendering from complex neural network decoding, creating a need for a representation that offers flexible frequency modulation and fast rendering.

Method: Proposes WIPES, a wavelet-based visual primitive that captures both low and high frequency details using wavelet spatial-frequency localization, and develops a wavelet-based differentiable rasterizer for fast rendering.

Result: Experimental results across 2D image representation, 5D static and 6D dynamic novel view synthesis show WIPES provides higher rendering quality and faster inference than INR-based methods, and better rendering quality than Gaussian-based representations.

Conclusion: WIPES serves as an effective universal visual primitive that successfully addresses spectrum loss and slow rendering issues through wavelet-based representation, demonstrating superior performance across multiple visual tasks.

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [174] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: This paper proposes Creative4U, the first explainable creative assessment system using MLLMs to evaluate and select advertising images based on user interests, with a new dataset CreativePair and training method Reason-to-Select RFT.


<details>
  <summary>Details</summary>
Motivation: Advertisers can generate many creative images with AIGC but lack methods to assess quality and make explainable selections. Existing ranking methods don't provide explanations for creative selection.

Method: Uses multimodal LLMs to integrate creative assessment into natural language generation. Creates CreativePair dataset with 8k annotated image pairs. Develops Creative4U system with CoT-SFT supervised fine-tuning and GRPO reinforcement learning.

Result: Offline and online experiments demonstrate the effectiveness of the approach in accurately evaluating and selecting creative images.

Conclusion: The proposed paradigm successfully addresses explainable creative selection, with both dataset and code to be made public for research and industrial applications.

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [175] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: SpotVLM is a cloud-edge collaborative framework that uses delayed LVLM outputs as historical context to guide real-time SVLM inference, improving performance while handling cloud latency fluctuations.


<details>
  <summary>Details</summary>
Motivation: Existing cloud-edge collaborative architectures for VLMs fail to accommodate cloud latency fluctuations and overlook the potential of delayed but accurate LVLM responses in real-time applications like autonomous driving.

Method: Proposes Context Transfer paradigm that treats delayed LVLM outputs as historical context for SVLM guidance. Implements SpotVLM with context replacement and visual focus modules to refine textual input and enhance visual grounding consistency.

Result: Extensive experiments on three real-time vision tasks across four datasets demonstrate the framework's effectiveness in improving performance while handling latency.

Conclusion: The Context Transfer paradigm provides groundwork for more effective latency-aware collaboration strategies in future VLM systems, enabling better real-time performance despite cloud delays.

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [176] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian BrandstÃ¶tter,Erich Kobler*

Main category: cs.CV

TL;DR: Two-stage PMRF pipeline synthesizes contrast-enhanced brain MRI from non-contrast inputs using a 3D U-Net for initial prediction and rectified flow for realistic texture refinement, achieving significant quality improvements while maintaining structural accuracy.


<details>
  <summary>Details</summary>
Motivation: Eliminate need for gadolinium-based contrast agents in MRI which add cost, time, environmental concerns, and patient risks while maintaining diagnostic quality.

Method: Two-stage approach: 1) Patch-based 3D U-Net predicts voxel-wise posterior mean (MSE minimization), 2) Time-conditioned 3D rectified flow refines initial estimate to incorporate realistic textures without compromising structural fidelity.

Result: Achieved axial FID of 12.46 and KID of 0.007 (68.7% lower FID than posterior mean) with low volumetric MSE of 0.057 (27% higher than posterior mean) on 360 test volumes. Restores lesion margins and vascular details realistically.

Conclusion: Effectively navigates perception-distortion trade-off for clinical deployment, providing high-quality synthetic contrast-enhanced MRI without requiring gadolinium agents.

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [177] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: A mean teacher framework called BEE that balances exploration and exploitation in Continual Test-Time Adaptation using multi-level consistency regularization and complementary anchor replay.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods struggle with balancing rapid adaptation to new domains (exploration) while retaining knowledge from previous domains (exploitation), due to inefficient shallow feature adjustment and model forgetting.

Method: Proposes a mean teacher framework with Multi-level Consistency Regularization (MCR) to align intermediate features for faster adaptation, and Complementary Anchor Replay (CAR) to reuse historical checkpoints for knowledge retention.

Result: Significantly outperforms state-of-the-art methods on several benchmarks, demonstrating effective CTTA performance.

Conclusion: The BEE framework successfully addresses the exploration-exploitation trade-off in CTTA through feature-level alignment and historical knowledge reuse, achieving superior adaptation performance.

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [178] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: DyCrowd is the first framework for spatio-temporally consistent 3D reconstruction of hundreds of individuals' poses, positions and shapes from large-scene videos, addressing occlusion challenges through group-guided motion optimization and VAE-based motion priors.


<details>
  <summary>Details</summary>
Motivation: Current 3D crowd reconstruction methods work from static images, lacking temporal consistency and unable to handle occlusions effectively, which is crucial for applications like city surveillance and crowd analysis.

Method: Coarse-to-fine group-guided motion optimization strategy with VAE-based human motion prior and segment-level optimization. Uses collective crowd behavior to handle long-term occlusions, joint optimization of similar motion segments, and Asynchronous Motion Consistency (AMC) loss for robust motion recovery.

Result: Achieves state-of-the-art performance in large-scene dynamic crowd reconstruction. Also contributes VirtualCrowd benchmark dataset for evaluation.

Conclusion: The proposed framework successfully addresses temporal consistency and occlusion challenges in 3D crowd reconstruction from videos, demonstrating robust performance and creating a valuable benchmark for future research.

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [179] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: A two-stage human de-occlusion method using diffusion-based body structure completion and RGB reconstruction with human-specific textual features, outperforming existing methods and improving downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Deep learning models struggle to accurately predict occluded regions in images, particularly for human bodies where structure and appearance recovery is challenging due to complex occlusions.

Method: Two-stage approach: 1) Mask completion using diffusion-based human body prior and occluded joint heatmaps for spatial cues, 2) RGB completion using Stable Diffusion with human-specific textual features from VQA+CLIP, plus decoder fine-tuning to prevent pixel degradation.

Result: Effectively reconstructs human appearances under severe occlusions, consistently outperforms existing methods in both mask and RGB completion, and improves performance of downstream tasks like 2D pose estimation and 3D human reconstruction.

Conclusion: The proposed method successfully addresses human de-occlusion by combining structural priors with appearance reconstruction, demonstrating superior performance and practical utility for human-centric computer vision applications.

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [180] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: Vision-G1 is a visual reasoning VLM trained on a comprehensive multi-domain dataset using influence-based data selection and multi-round RL curriculum, achieving SOTA performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current reasoning VLMs focus on limited tasks like math/logic and struggle with generalization due to scarce verifiable reward data beyond narrow domains and uncertain dataset compatibility.

Method: Built comprehensive RL-ready visual reasoning dataset from 46 sources across 8 domains, used influence function-based data selection and difficulty filtering, trained with multi-round RL and data curriculum.

Result: Achieves state-of-the-art performance across various visual reasoning benchmarks, outperforming similar-sized VLMs and proprietary models like GPT-4o and Gemini-1.5 Flash.

Conclusion: The comprehensive multi-domain dataset and iterative RL training with data curriculum enable superior visual reasoning generalization across diverse domains.

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [181] [WP-CLIP: Leveraging CLIP to Predict WÃ¶lfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: Fine-tuned CLIP model (WP-CLIP) successfully predicts WÃ¶lfflin's five stylistic principles in visual art, addressing limitations of pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Existing metrics fail to effectively predict all five WÃ¶lfflin's principles for formal art analysis, and pre-trained vision-language models like CLIP don't inherently capture nuanced stylistic elements in paintings.

Method: Fine-tuned CLIP on annotated datasets of real art images to predict scores for each of WÃ¶lfflin's five principles, creating WP-CLIP model.

Result: WP-CLIP demonstrates ability to generalize across diverse artistic styles when evaluated on GAN-generated paintings and Pandora-18K art dataset.

Conclusion: Vision-language models show significant potential for automated art analysis when properly fine-tuned for specific stylistic evaluation tasks.

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [182] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: AdaBEV is a novel framework for multi-UAV collaborative 3D detection that learns adaptive instance-aware BEV representations through refine-and-contrast paradigm, achieving superior accuracy-computation trade-offs with low-resolution inputs.


<details>
  <summary>Details</summary>
Motivation: Multi-UAV collaborative 3D detection offers advantages in coverage and occlusion handling but faces computational challenges on resource-constrained UAV platforms. Existing methods treat all BEV grids equally, lacking semantic awareness and feature discriminability.

Method: Introduces Box-Guided Refinement Module (BG-RM) that refines only foreground-associated BEV grids using 2D supervision and spatial subdivision, and Instance-Background Contrastive Learning (IBCL) that promotes separation between foreground and background features via contrastive learning in BEV space.

Result: Extensive experiments on Air-Co-Pred dataset show AdaBEV achieves superior accuracy-computation trade-offs across model scales, outperforming state-of-the-art methods at low resolutions and approaching upper bound performance while maintaining low-resolution BEV inputs and negligible overhead.

Conclusion: AdaBEV provides an effective solution for resource-efficient 3D detection in multi-UAV systems by focusing computational resources on foreground instances and enhancing feature discriminability through contrastive learning.

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [183] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: TTA-DAME is a test-time adaptation method that uses source domain data augmentation, domain discrimination, and multiple detector training with NMS to handle dynamic domain shifts in driving scenes, showing strong performance on SHIFT Benchmark.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of test-time adaptation in real-world driving scenarios where weather and lighting conditions (daytime to nighttime) create frequent domain shifts that degrade model performance.

Method: Leverages source domain data augmentation into target domains, introduces domain discriminator and specialized domain detector to handle drastic shifts, trains multiple detectors and consolidates predictions using Non-Maximum Suppression (NMS).

Result: Empirical validation demonstrates significant performance enhancements on the SHIFT Benchmark, showing effectiveness in handling dynamic domain changes.

Conclusion: TTA-DAME effectively addresses test-time adaptation challenges in driving scenes through domain-aware augmentation and detection mechanisms, proving robust against frequent weather and lighting domain shifts.

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [184] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: A novel approach for class-incremental learning with repetition (CIR) that uses multi-level knowledge distillation and dynamic self-supervised learning to effectively leverage unlabeled external data for maintaining stability and plasticity.


<details>
  <summary>Details</summary>
Motivation: Traditional class-incremental learning assumes each task contains only unseen classes, but real-world scenarios often involve repeated classes. CIR is more realistic and assumes access to abundant unlabeled external data.

Method: Proposes two components: 1) Multi-level knowledge distillation (MLKD) that distills knowledge from multiple previous models across features and logits, 2) Dynamic self-supervised loss (SSL) that utilizes unlabeled data to accelerate new class learning while maintaining focus on primary tasks.

Result: Significantly improves performance in CIR setup, achieving 2nd place in the CVPR 5th CLVISION Challenge.

Conclusion: The proposed MLKD and dynamic SSL components effectively address the challenges of class-incremental learning with repetition by leveraging unlabeled data to maintain both stability (remembering old classes) and plasticity (learning new classes).

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [185] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: The paper investigates cross-sensor domain gap in autonomous vehicles, introduces CamShift dataset to simulate sensor differences between vehicle types, shows BEVFormer is most robust, and proposes neural rendering-based sensor adaptation to mitigate performance degradation.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles have varying camera sensor setups due to different vehicle types and placement constraints, causing cross-sensor domain gap that degrades perception model accuracy when trained on one setup and evaluated on another.

Method: Created CamShift dataset in CARLA inspired by nuScenes to simulate sensor gap between subcompact vehicles and SUVs. Evaluated state-of-the-art 3D object detectors and proposed neural rendering-based sensor adaptation pipeline to transform datasets to match different camera setups.

Result: Significant cross-sensor performance degradation observed. BEVFormer with dense Bird's Eye View representation and backward projection was most robust. The proposed sensor adaptation approach improved performance across all detectors, mitigating domain gap by large margin and enabling data reusability.

Conclusion: Cross-sensor domain gap is a critical issue in autonomous driving. BEV-based architectures show better robustness, and neural rendering-based sensor adaptation provides effective solution to bridge the gap without requiring extensive new data collection.

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [186] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: GenAI-driven news diversity causes multi-level drift that significantly degrades LVLM-based misinformation detection systems, with performance dropping 14.8% on average and reasoning becoming unstable.


<details>
  <summary>Details</summary>
Motivation: The proliferation of multimodal misinformation and rise of GenAI tools create highly varied content that challenges current detection systems, requiring systematic study of these vulnerabilities.

Method: Introduce DriftBench - a large-scale benchmark with 16,000 news instances across six diversification categories, and design three evaluation tasks to test robustness, adversarial susceptibility, and reasoning consistency.

Result: Experiments with six state-of-the-art LVLM detectors show substantial performance drops (average F1 -14.8%), increasingly unstable reasoning traces, and severe failures under adversarial evidence injection.

Conclusion: Findings reveal fundamental vulnerabilities in existing MMD systems, indicating an urgent need for more resilient approaches to handle GenAI-driven content diversity.

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [187] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: Real-time sign language translation system using CNN on Sign Language MNIST dataset that converts gestures to text and speech via webcam capture.


<details>
  <summary>Details</summary>
Motivation: Address communication barriers for individuals with hearing and speech impairments by enabling seamless interaction in everyday environments.

Method: Uses convolution neural networks (CNN) trained on Sign Language MNIST dataset to classify hand gestures captured live via webcam, with text-to-speech synthesis for audible output.

Result: High model accuracy and robust real-time performance with some latency, demonstrating practical applicability as an accessible communication tool.

Conclusion: The system serves as a reliable and user-friendly tool that enhances autonomy and social integration for sign language users across diverse settings.

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [188] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: Dual Contrastive Denoising Score framework enables precise real image editing using text-to-image diffusion models while preserving structure and avoiding unwanted changes.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with real image editing due to difficulty in crafting perfect text prompts and tendency to introduce unwanted changes in non-target regions.

Method: Uses dual contrastive loss inspired by contrastive learning, leveraging spatial information from self-attention layers in latent diffusion models without auxiliary networks.

Result: Outperforms existing methods in real image editing while maintaining zero-shot translation capabilities and preserving input-output structure.

Conclusion: The framework provides flexible content modification with structure preservation and can directly utilize pretrained diffusion models without further training.

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [189] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: GPT-5 shows unprecedented spatial intelligence but still falls short of human performance across various spatial reasoning tasks, with proprietary models not having decisive advantages on the most difficult problems.


<details>
  <summary>Details</summary>
Motivation: Multi-modal models have limitations in spatial understanding and reasoning, which are fundamental for artificial general intelligence. With GPT-5's release, it's timely to evaluate leading models' progress toward spatial intelligence.

Method: Proposed a comprehensive taxonomy of spatial tasks unifying existing benchmarks, then evaluated state-of-the-art proprietary and open-source models on eight key benchmarks using over one billion total tokens, plus qualitative evaluation on human-intuitive scenarios.

Result: GPT-5 demonstrates unprecedented spatial intelligence strength but still underperforms humans across broad spatial tasks. Proprietary models show no decisive advantage on most difficult problems. Many human-intuitive scenarios fail even the most advanced models.

Conclusion: While GPT-5 represents significant progress in spatial intelligence, multi-modal models still have substantial gaps compared to human spatial reasoning capabilities, particularly on challenging problems and intuitive scenarios.

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [190] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 3D Gaussian Splatting suffers from appearance artifacts in sparse-view scenarios due to Gaussian co-adaptation. Proposed random dropout and opacity noise methods effectively mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: 3DGS shows impressive performance in dense-view novel view synthesis but manifests appearance artifacts in sparse-view scenarios, requiring investigation into the underlying causes and solutions.

Method: Proposed Co-Adaptation Score metric to quantify Gaussian entanglement, then introduced two lightweight strategies: random Gaussian dropout and multiplicative noise injection to opacity.

Result: Analysis revealed co-adaptation naturally decreases with more training views. Both proposed strategies effectively mitigated appearance artifacts across various methods and benchmarks.

Conclusion: The co-adaptation effect is a core limitation in sparse-view 3DGS, and the proposed plug-and-play strategies provide effective solutions while inspiring broader understanding of the problem.

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [191] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: Frequency-driven network using dual-branch inverse kernel prediction and position adaptive convolution for single image defocus deblurring


<details>
  <summary>Details</summary>
Motivation: Existing methods degrade in severely blurry regions where local high-frequency details are missing, requiring better structural identifiability in kernel modeling

Method: FDIKP network with frequency-domain representations, Dual-Branch Inverse Kernel Prediction strategy, Position Adaptive Convolution, and Dual-Domain Scale Recurrent Module for progressive refinement

Result: Outperforms existing approaches in extensive experiments

Conclusion: Frequency-domain incorporation and the proposed modules effectively enhance kernel estimation accuracy and deblurring quality for defocus deblurring

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [192] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: Proposes DCSCR network combining traditional ISC methods with deep learning for few-shot image set classification, learning both frame-level and concept-level features with adaptive distance measurement.


<details>
  <summary>Details</summary>
Motivation: Existing methods either use raw pixel features without learning or fail to adaptively adjust features when measuring set distances, limiting performance in few-shot scenarios.

Method: DCSCR network with three modules: fully convolutional deep feature extractor, global feature learning module, and class-specific collaborative representation-based metric learning module with new contrastive loss function.

Result: Extensive experiments on well-known few-shot ISC datasets demonstrate effectiveness compared to state-of-the-art algorithms.

Conclusion: The proposed DCSCR approach successfully addresses limitations of existing methods by simultaneously learning feature representations and adaptive distance similarities for improved few-shot image set classification.

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [193] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: A novel Mamba-based network with dual-scale fusion and dual-path scanning for shadow removal that outperforms state-of-the-art methods by effectively integrating contextual cues and modeling region-specific transformations.


<details>
  <summary>Details</summary>
Motivation: Shadow removal requires leveraging information from non-shadow regions to guide restoration, but traditional uniform correction strategies fail because shadowed areas need different transformations than well-lit regions. This necessitates adaptive modeling and effective integration of non-local contextual cues.

Method: Proposes a Mamba-based network with Dual-Scale Fusion Mamba Block (DFMB) that fuses original and low-resolution features to enhance multi-scale representation and reduce boundary artifacts. Uses Dual-Path Mamba Group (DPMG) with horizontal scanning and mask-aware adaptive scanning strategy to capture global features and improve structural continuity.

Result: Experimental results demonstrate that the method significantly outperforms existing state-of-the-art approaches on shadow removal benchmarks.

Conclusion: The proposed Mamba-based architecture with dual-scale fusion and dual-path scanning effectively addresses the challenges of shadow removal by selectively propagating contextual information based on transformation similarity across regions.

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [194] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: CLAIRE-DSA is a deep learning framework that classifies image quality in DSA series for stroke treatment, improving downstream segmentation performance from 42% to 69% success rate.


<details>
  <summary>Details</summary>
Motivation: Computer vision models for mechanical thrombectomy in acute ischemic stroke suffer from degraded performance due to poor image quality, necessitating automated quality assessment tools.

Method: Uses pre-trained ResNet backbone models fine-tuned to predict nine image properties (contrast presence, projection angle, motion artifacts, etc.) on 1,758 annotated fluoroscopic MinIPs with separate classifiers for each property.

Result: Achieved excellent performance with ROC-AUC 0.91-0.98 and precision 0.70-1.00 across all labels. Filtering poor quality images increased segmentation success rate from 42% to 69% (p < 0.001).

Conclusion: CLAIRE-DSA shows strong potential as an automated tool for image quality classification in DSA series, supporting clinical and research applications for stroke treatment with publicly available source code.

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [195] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: Proposes ICAF framework for semi-supervised semantic segmentation of CdZnTe semiconductor images with many-to-one view relationships, achieving 70.6% mIoU with only 0.5% labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised segmentation methods are suboptimal for CdZnTe images due to their many-to-one view relationship (multiple views share single ground truth), causing error accumulation in low-contrast defect boundaries.

Method: Intra-group Consistency Augmentation Framework (ICAF) with Intra-group View Sampling and Pseudo-label Correction Network (PCN) containing View Augmentation Module for boundary synthesis and View Correction Module for information interaction.

Result: Achieved 70.6% mIoU on CdZnTe dataset using only 2 group-annotated data (0.5%) with DeepLabV3+ and ResNet-101 backbone, demonstrating effectiveness for low-contrast semiconductor defect segmentation.

Conclusion: The group-oriented perspective and ICAF framework effectively address the many-to-one challenge in CdZnTe segmentation, providing a human-inspired solution that leverages intra-group consistency constraints for improved performance with minimal annotations.

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [196] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: SocialTrack is a novel UAV-based multi-object tracking framework that addresses challenges like small target variations, occlusions, and motion blur through specialized detection, adaptive filtering, group motion modeling, and spatio-temporal memory prediction.


<details>
  <summary>Details</summary>
Motivation: UAV-based multi-object tracking faces significant challenges in complex urban environments including small target scale variations, occlusions, nonlinear crossing motions, and motion blur that hinder tracking stability.

Method: Proposes SocialTrack framework with: 1) specialized small-target detector with multi-scale feature enhancement, 2) Velocity Adaptive Cubature Kalman Filter for trajectory prediction, 3) Group Motion Compensation Strategy for social group modeling, and 4) Spatio-Temporal Memory Prediction using historical trajectory information.

Result: Extensive experiments on UAVDT and MOT17 datasets show SocialTrack outperforms state-of-the-art methods, with significant improvements in MOTA and IDF1 metrics, demonstrating superior robustness and adaptability.

Conclusion: SocialTrack provides an effective solution for UAV-based multi-object tracking in complex urban traffic environments, offering high modularity and compatibility for integration with existing trackers to enhance performance.

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [197] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: A novel image style transfer method using multiple style images with image prompt adapters and statistical feature alignment during denoising to prevent content leakage and improve style matching.


<details>
  <summary>Details</summary>
Motivation: Existing latent diffusion models struggle with accurate style matching, limited style image usage, and content-style entanglement issues in image style transfer.

Method: Leverages multiple style images with image prompt adapters and statistical alignment of features during denoising process, intervening at both cross-attention and self-attention layers of the UNet. Uses clustering to distill representative attention features from style samples.

Result: Achieves state-of-the-art results for stylization as demonstrated in experimental evaluation.

Conclusion: The proposed approach effectively addresses key limitations in current style transfer methods by utilizing multiple style images and strategic feature alignment techniques during the denoising process.

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [198] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane SchÃ¶nlieb,James Woodcock*

Main category: cs.CV

TL;DR: Using deep learning on Google Street View images to estimate global cycling and motorcycling levels, achieving strong correlations and accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Transportation impacts health through physical activity, pollution, and injury risks, but comparative global data on cycling and motorcycling behaviors is scarce.

Method: Used YOLOv4 model fine-tuned on images from 6 cities to detect cycles/motorcycles in 8000 GSV images per city across 185 global cities, then developed beta regression models with city-level mode shares.

Result: Strong correlation between GSV motorcycle counts and mode share (0.78), moderate for cycling (0.51). Models achieved RÂ² of 0.614 for cycling and 0.612 for motorcycling with median absolute errors of 1.3-1.4%.

Conclusion: Computer vision on street view imagery effectively captures travel modes, providing valuable insights alongside traditional data sources for global transportation analysis.

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [199] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Å tefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin VaÅˆko,Pavol GajdoÅ¡*

Main category: cs.CV

TL;DR: Computer vision models using ResNet50 and vision transformers achieve high accuracy (>96%) for classifying eclipsing binary types but perform poorly on automated spot detection.


<details>
  <summary>Details</summary>
Motivation: To apply computer vision methods for automated classification of eclipsing binary light curves in large-scale astronomical surveys, addressing the need for efficient morphological analysis of these systems.

Method: Used pre-trained ResNet50 CNN and vision transformer models fine-tuned on synthetic datasets. Developed novel polar coordinate transformation with hexbin visualization of phase-folded light curves. Implemented hierarchical classification: first stage separates detached vs overcontact binaries, second stage detects spots.

Result: High accuracy (>96%) on validation data across multiple passbands (Gaia G, I, TESS). Strong performance (>94%, up to 100% for TESS) on observational data from OGLE, DEBCat, and WUMaCat catalogues. However, automated spot detection performed poorly.

Conclusion: Computer vision shows great potential for eclipsing binary morphological classification in large surveys, but current models have limitations for detecting subtle photometric features like spots, requiring further research for robust automated spot detection.

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [200] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: NVG framework generates images through hierarchical visual granularity sequences, outperforming VAR models on ImageNet with improved FID scores.


<details>
  <summary>Details</summary>
Motivation: To achieve fine-grained control over image generation by decomposing images into structured sequences with different visual granularity levels, enabling progressive refinement from global layout to fine details.

Method: Proposes Next Visual Granularity (NVG) framework that generates images iteratively starting from empty image, using a sequence where each element shares spatial resolution but differs in token count to capture varying visual granularity levels.

Result: NVG models trained on ImageNet show clear scaling behavior and consistently outperform VAR series (FID scores: 3.30->3.03, 2.57->2.44, 2.09->2.06). Extensive analysis demonstrates the framework's capabilities.

Conclusion: NVG provides a hierarchical, layered representation for image generation with fine-grained control across multiple granularity levels, showing superior performance to existing methods with promising scaling behavior.

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [201] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian GÃ¼lhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: Overview of the CVPR 2025 Spatio-temporal Instance Segmentation challenge using event camera and grayscale camera data for pixel-level object segmentation.


<details>
  <summary>Details</summary>
Motivation: To advance research in spatio-temporal instance segmentation by leveraging the complementary strengths of event cameras and traditional grayscale cameras for more accurate object segmentation.

Method: Challenge-based evaluation where participants developed methods to predict pixel-level segmentation masks from spatio-temporally aligned event camera and grayscale camera data.

Result: The paper presents the challenge overview, dataset details, and results including descriptions of the top-5 ranking teams' methods.

Conclusion: The challenge successfully advanced the field of spatio-temporal instance segmentation and provided valuable insights into effective methods for combining event and grayscale camera data.

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [202] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: DEEP-SEA is a deep learning model that enhances underwater images by preserving both low- and high-frequency information and spatial structures using a Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator.


<details>
  <summary>Details</summary>
Motivation: Underwater environments degrade image clarity due to light scattering, absorption and turbidity, making accurate marine biodiversity monitoring and ecological assessment difficult.

Method: Proposes a novel deep learning-based underwater image restoration model with Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator to adaptively refine feature representations in both frequency domains and spatial information.

Result: Superior performance over state-of-the-art methods on EUVP and LSUI datasets in restoring fine-grained image detail and structural consistency.

Conclusion: DEEP-SEA effectively mitigates underwater visual degradation and has potential to improve reliability of underwater monitoring platforms for ecological observation, species identification and autonomous navigation.

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [203] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: Winning approach for multimodal deception detection challenge using progressive domain adaptation to handle domain shift across diverse audio-visual datasets.


<details>
  <summary>Details</summary>
Motivation: Address the domain shift issue across source and target domains in multimodal deception detection, where different datasets have distribution differences that hinder model performance.

Method: Multi-source Multimodal Progressive Domain Adaptation (MMPDA) framework that gradually aligns source and target domains at both feature and decision levels to bridge domain shifts.

Result: Achieved Top-2 position with 60.43% accuracy and 56.99% F1-score on competition stage 2, surpassing 1st place by 5.59% on F1-score and 3rd place by 6.75% on accuracy.

Conclusion: The MMPDA framework effectively transfers audio-visual knowledge from diverse source domains to target domains, demonstrating strong performance in multimodal deception detection despite domain shifts.

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [204] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: CoMuCo is a novel fine-tuning strategy for vision-language models that uses multi-view expert modules with consistency constraints to improve cross-domain few-shot learning performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLM transfer learning methods perform well on standard natural image datasets but struggle with cross-domain tasks where imaging domains differ from natural images.

Method: Consistency-guided Multi-view Collaborative Optimization (CoMuCo) employs two complementary expert modules to extract multi-view features, with prior knowledge-based consistency constraints and information geometry-based consensus mechanisms.

Result: Extensive evaluations show CoMuCo consistently outperforms current methods in few-shot tasks, particularly on cross-domain benchmarks distinct from natural images.

Conclusion: The proposed CoMuCo strategy effectively enhances VLM robustness for cross-domain few-shot learning, and a new benchmark is established to facilitate comprehensive evaluation of such methods.

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [205] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: MPS-Tuning is a novel fine-tuning method that preserves the geometric structure of data distribution in vision-language models by constraining intrinsic manifold geometry and enhancing class separability through Gram matrix alignment and pairwise similarity optimization.


<details>
  <summary>Details</summary>
Motivation: Existing transfer learning methods for vision-language models often neglect the geometric structure of data distribution, which can lead to distortion of semantic representations. The paper aims to overcome this limitation by explicitly preserving the manifold structure while improving classification performance.

Method: MPS-Tuning treats data distribution as a semantic manifold and preserves both macroscopic and microscopic topological structures by aligning Gram matrices of features before and after fine-tuning. It also pairs image and text modality features to optimize pairwise similarities for enhanced class discriminability.

Result: Extensive experiments demonstrate that MPS-Tuning significantly improves model performance while effectively preserving the structure of the semantic manifold. The method shows superior results compared to existing regularization approaches.

Conclusion: The proposed MPS-Tuning method successfully addresses the limitation of existing fine-tuning approaches by explicitly constraining the geometric structure of data distribution, leading to improved performance and better preservation of semantic manifold structure in vision-language models.

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [206] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: S^2-Guidance improves upon Classifier-free Guidance by using stochastic sub-networks to refine predictions and avoid low-quality outputs in diffusion models.


<details>
  <summary>Details</summary>
Motivation: CFG produces suboptimal results and semantic incoherence due to excessive reliance on imperfect predictions, leading to low-quality outputs in diffusion models.

Method: Proposes S^2-Guidance which uses stochastic block-dropping during forward process to create stochastic sub-networks that guide the model away from low-quality predictions.

Result: Extensive experiments on text-to-image and text-to-video generation show S^2-Guidance consistently outperforms CFG and other advanced guidance strategies.

Conclusion: S^2-Guidance effectively addresses CFG's limitations by leveraging stochastic sub-networks to produce higher quality and more coherent outputs in diffusion models.

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [207] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: ONG is a one-shot NMF-based gradient masking method for neural network pruning that identifies important weights at training start and maintains sparsity throughout training.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks face deployment challenges due to large size, and existing pruning methods often involve complex iterative processes or struggle to maintain sparsity effectively during training.

Method: Uses Non-negative Matrix Factorization (NMF) for one-shot pruning at training outset, then employs gradient masking to ensure only unpruned weights are updated, strictly preserving target sparsity.

Result: Achieves comparable or superior performance at various sparsity levels on CIFAR-10/100 with ResNet architectures while maintaining structural integrity post-pruning.

Conclusion: ONG provides an effective one-shot pruning approach with clear sparsity targeting mechanism and strict sparsity preservation throughout training.

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [208] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow is a 0.5B latent flow matching transformer model that generates entire 3D CT volumes conditioned on clinical reports, achieving superior performance in temporal coherence, image diversity and text-image alignment compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To accelerate medical research through data augmentation, enable privacy-preserving synthesis, reduce regulatory constraints on patient data while preserving diagnostic signals, leveraging the recently released CT-RATE dataset of 3D CT volumes paired with clinical reports.

Method: Uses a 0.5B latent flow matching transformer conditioned on clinical reports via CT-Clip text encoder. Leverages A-VAE from FLUX for latent space definition. Employs custom autoregressive approach where model predicts first sequence of slices from text, then subsequent sequences using previously generated slices and text.

Result: Demonstrates superiority over state-of-the-art generative CT models in terms of temporal coherence, image diversity and text-image alignment, as measured by FID, FVD, IS scores and CLIP score.

Conclusion: CTFlow successfully generates consistent whole CT volumes while managing memory constraints, providing an effective solution for text-conditioned 3D CT volume generation with applications in medical research and data privacy.

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [209] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: CMF-IOU is a multi-stage cross-modal fusion framework for 3D detection that effectively integrates camera and LiDAR data through depth completion, bilateral encoding, and iterative refinement with IoU prediction.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal 3D detection methods often use single or partial stage fusion, leading to insufficient feature extraction and suboptimal performance in aligning 3D spatial and 2D semantic information.

Method: 1) Project camera pixels to 3D space via depth completion to get pseudo points; 2) Use bilateral cross-view enhancement 3D backbone with S2D and ResVC branches; 3) Implement iterative voxel-point aware fine-grained pooling; 4) Add IoU joint prediction branch with novel proposals generation.

Result: Extensive experiments demonstrate superior performance on KITTI, nuScenes and Waymo datasets, showing the framework's effectiveness in 3D detection tasks.

Conclusion: The proposed CMF-IOU framework successfully addresses cross-modal fusion challenges through multi-stage processing, achieving state-of-the-art performance by effectively integrating LiDAR spatial information and camera semantic features.

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [210] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 7Bench is the first benchmark to jointly evaluate both semantic and spatial alignment in layout-guided text-to-image generation, addressing a critical gap in existing evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: Layout-guided text-to-image models are increasingly used but lack proper evaluation benchmarks that assess both text and layout alignment simultaneously, which is crucial for applications like synthetic data generation where spatial errors can degrade data quality.

Method: The authors introduce 7Bench featuring text-and-layout pairs across seven challenging scenarios, and propose an evaluation protocol that incorporates layout alignment score alongside existing text alignment metrics to assess spatial accuracy.

Result: The benchmark was used to evaluate several state-of-the-art diffusion models, revealing their respective strengths and limitations across diverse alignment tasks including object generation, color fidelity, attribute recognition, inter-object relationships, and spatial control.

Conclusion: 7Bench provides a comprehensive evaluation framework for layout-guided text-to-image generation, enabling better assessment of spatial fidelity which is essential for reliable applications in content creation and synthetic data generation.

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [211] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: HiAD is a novel framework for high-resolution anomaly detection that uses dual-branch architecture and multi-resolution feature fusion to detect both subtle and large-scale anomalies efficiently.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods struggle with high-resolution images due to information loss from downsampling and poor performance of existing approaches in industrial scenarios.

Method: Dual-branch architecture integrating anomaly cues across scales, multi-resolution feature fusion strategy, and detector pool with adaptive assignment based on patch features.

Result: Superior performance demonstrated on high-resolution benchmarks MVTec-HD, VisA-HD, and RealIAD-HD.

Conclusion: HiAD effectively addresses high-resolution anomaly detection challenges with improved accuracy and computational efficiency, making it suitable for practical industrial applications.

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [212] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG is a two-stage ViT framework that sequentially improves encoder and decoder generality through feature boosting and knowledge distillation to mitigate catastrophic forgetting in incremental learning, especially in small-memory scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing incremental learning methods focus on enhancing either encoder or decoder but not both, limiting effectiveness against catastrophic forgetting, particularly in small-memory settings where historical samples are limited.

Method: Two-stage training: 1) Train ensembled encoder via feature boosting to learn generalized representations that enhance decoder and balance classifier; 2) Use knowledge distillation (balanced KD + feature KD) to compress ensembled encoder into a more generalized single encoder.

Result: Extensive experiments on three benchmark datasets show superior performance compared to existing methods, with ablation studies confirming the effectiveness of each component.

Conclusion: SEDEG effectively addresses catastrophic forgetting in incremental learning by simultaneously improving both encoder and decoder generality through a novel two-stage approach, demonstrating strong performance especially in challenging small-memory scenarios.

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [213] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,YaÃ«l Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: Automated U-Net framework for fiber bundle segmentation in macaque tracer data with improved sparse bundle detection and reduced false discovery rates.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of fiber bundles in histological slides is labor-intensive, and existing automated methods often miss sparse bundles or require complex post-processing, limiting large-scale analysis of anatomic tracer studies.

Method: U-Net architecture with large patch sizes, foreground aware sampling, and semisupervised pre-training for fully automated fiber bundle segmentation in standalone histological slices.

Result: 20% improvement in sparse bundle detection, 40% reduction in False Discovery Rate (FDR) compared to state-of-the-art methods, and elimination of common errors like mislabeling terminals as bundles.

Conclusion: This framework enables automated large-scale analysis of anatomic tracing data, generating more ground-truth data to validate and optimize dMRI tractography methods.

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [214] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: Lumen is an end-to-end video relighting framework that uses large-scale video generative models to replace backgrounds and adjust foreground lighting based on textual descriptions, achieving consistent cinematic results with strict foreground preservation.


<details>
  <summary>Details</summary>
Motivation: Video relighting is challenging but valuable for creating harmonious lighting adjustments in videos while preserving foreground properties and maintaining temporal consistency across frames.

Method: Constructs a large-scale mixed dataset of realistic and synthetic videos using 3D rendering engines and HDR-based lighting simulation. Uses a joint training curriculum with domain-aware adapter to decouple relighting learning from domain appearance distribution.

Result: Experimental results show Lumen effectively edits input videos into cinematic relighted videos with consistent lighting and strict foreground preservation, outperforming existing methods.

Conclusion: Lumen provides an effective end-to-end solution for video relighting that handles both synthetic and realistic domains while maintaining foreground integrity and temporal consistency.

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [215] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: MaskSem is a semantic-guided masking method for self-supervised skeleton-based action recognition that uses Grad-CAM to identify semantically rich joints for masking and reconstructs hybrid high-order motion patterns (velocity + acceleration) to better understand complex human motions.


<details>
  <summary>Details</summary>
Motivation: Existing mask-based reconstruction methods for skeleton action recognition focus on limited joints and low-order motion patterns, which restricts their ability to understand complex human motion patterns needed for effective human-robot collaboration.

Method: Proposes MaskSem framework that uses Grad-CAM based on relative motion to guide joint masking towards semantically rich temporal regions, and reconstructs hybrid high-order motion patterns combining low-order velocity and high-order acceleration as targets.

Result: Experiments on NTU60, NTU120, and PKU-MMD datasets show that MaskSem combined with a vanilla transformer improves skeleton-based action recognition performance.

Conclusion: The semantic-guided masking and hybrid high-order motion reconstruction approach enhances the model's understanding of complex motion patterns, making it more suitable for human-robot interaction applications.

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [216] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: ARMed is a novel RL framework that addresses reward collapse in medical VQA by combining domain knowledge through SFT with adaptive semantic rewards, achieving significant improvements in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing RL approaches in medical imaging primarily target closed-ended VQA, limiting real-world clinical applicability. Open-ended medical VQA better reflects clinical practice but suffers from reward collapse where semantically different responses receive similar scores.

Method: ARMed first incorporates domain knowledge through supervised fine-tuning on chain-of-thought data, then applies reinforcement learning with textual correctness and adaptive semantic rewards to enhance reasoning quality.

Result: ARMed achieved 32.64% improvement on in-domain tasks and 11.65% gain on out-of-domain benchmarks across six challenging medical VQA benchmarks.

Conclusion: The results highlight the critical role of reward discriminability in medical RL and demonstrate the promise of semantically guided rewards for enabling robust and clinically meaningful multimodal reasoning.

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [217] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: Deep learning pipeline using 3D SegResNet architecture for automated multi-class tooth segmentation in dental CBCT scans, achieving 0.87 Dice score on validation set.


<details>
  <summary>Details</summary>
Motivation: Automated segmentation of dental structures in CBCT can assist in identifying pathology and facilitate radiation therapy planning for head and neck cancer patients.

Method: Used MONAI Auto3DSeg framework with 3D SegResNet, trained on 63 CBCT scans with 5-fold cross-validation. Preprocessing included image resampling and intensity clipping. Employed two-phase segmentation with ensemble fusion using Multi-Label STAPLE.

Result: Achieved an average Dice score of 0.87 on the ToothFairy3 challenge out-of-sample validation set.

Conclusion: The approach demonstrates effective automated dental segmentation with potential to improve patient care in radiation oncology through efficient identification of dental structures and pathology.

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [218] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: GazeDETR is a novel end-to-end architecture with two disentangled decoders that separately handle human head localization and gaze prediction, achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end gaze target detection models use a single decoder that creates entangled representations for both head localization and gaze prediction, which may not be optimal for each subtask.

Method: Proposes GazeDETR with two separate decoders - one for human head prediction using local information, and another for gaze prediction incorporating both local and global information with coherent attentive fields.

Result: Achieves state-of-the-art results on GazeFollow, VideoAttentionTarget and ChildPlay datasets, outperforming existing end-to-end models by a notable margin.

Conclusion: Disentangling the decoders for head localization and gaze prediction allows each subtask to learn unique representations and utilize appropriate information (local vs local+global), leading to superior performance.

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [219] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: Compact Attention is a hardware-aware acceleration framework that uses adaptive tiling and temporally varying windows to exploit structured sparsity patterns in video diffusion transformers, achieving 1.6-2.5x speedup while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Self-attention mechanisms in transformer-based video generation are computationally demanding, especially for ultra-long sequences. Current sparse attention methods fail to fully exploit the inherent spatio-temporal redundancies in video data and either impose rigid constraints or introduce significant overhead.

Method: Proposes Compact Attention with three innovations: 1) Adaptive tiling strategies for dynamic tile grouping, 2) Temporally varying windows that adjust sparsity based on frame proximity, and 3) An automated configuration search algorithm that optimizes sparse patterns while preserving critical attention pathways.

Result: Achieves 1.6~2.5x acceleration in attention computation on single-GPU setups while maintaining comparable visual quality with full-attention baselines.

Conclusion: Provides a principled approach to unlocking efficient long-form video generation through structured sparsity exploitation, demonstrating significant computational savings without compromising visual quality.

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [220] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: Proposes a zero-shot NAS proxy using SVD and extrinsic curvature that works with unlabeled data, combining convergence, generalization and expressivity metrics into a single efficient proxy.


<details>
  <summary>Details</summary>
Motivation: Existing zero-cost NAS proxies require labeled data and focus either on convergence/generalization or expressivity alone, limiting real-world applicability.

Method: Uses Singular Value Decomposition (SVD) of layer features and extrinsic curvature of network output to create a proxy that combines convergence, generalization and expressivity metrics without needing labeled data.

Result: Superior performance on multiple benchmarks (NAS-Bench-101, NAS-Bench-201, TransNAS-Bench-101-micro) and NAS tasks in DARTS and AutoFormer search spaces, using only a single unlabeled data sample.

Conclusion: The proposed zero-cost proxy effectively predicts network performance without labeled data, demonstrating high efficiency and accuracy across diverse neural architecture search scenarios.

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [221] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: This paper provides a comprehensive survey of multi-modal visual object tracking (MMVOT) in smart cities, covering data collection, modality alignment, model design, evaluation, and analyzing six MMVOT tasks with 338 references.


<details>
  <summary>Details</summary>
Motivation: The development of smart cities generates massive multi-modal data, requiring effective tracking solutions that leverage multiple data modalities beyond traditional RGB tracking for comprehensive infrastructure monitoring.

Method: The survey categorizes MMVOT methods based on how they handle visible (RGB) and auxiliary modalities (thermal, depth, event, NIR, language, sonar), analyzing data collection challenges, modality alignment, annotation approaches, and evaluation frameworks.

Result: The paper reveals that MMVOT datasets exhibit pronounced long-tail distributions of object categories with noticeable lack of animal categories compared to RGB datasets, and questions whether multi-modal tracking always provides superior performance over unimodal approaches.

Conclusion: This comprehensive survey covers all aspects of MMVOT, providing foundational insights into when multi-modal tracking is beneficial and highlighting dataset limitations that need addressing for future research in smart city applications.

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [222] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: Feature diversity improves open set recognition and continual learning performance by enhancing novel class detection and knowledge retention/integration.


<details>
  <summary>Details</summary>
Motivation: While many approaches address open set recognition and continual learning through heuristic feature diversity promotion, there's limited direct examination of feature diversity's role in these challenges.

Method: Empirical investigation providing evidence that enhanced feature diversity improves recognition of open set samples and facilitates both retention of previously learned data and integration of new data.

Result: Increased feature diversity improves open set sample recognition and supports better continual learning performance through improved knowledge retention and integration.

Conclusion: Feature diversity plays a crucial role in both open set recognition and continual learning, suggesting it should be a focus for both practical method development and theoretical understanding in these domains.

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [223] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: SlimComm reduces bandwidth usage by 90% for collaborative perception in autonomous vehicles using 4D radar Doppler and query-driven sparse feature sharing instead of full BEV map transmission.


<details>
  <summary>Details</summary>
Motivation: Current collaborative perception approaches transmit dense Bird's-Eye-View feature maps which overwhelm the limited bandwidth available for inter-vehicle communication between connected autonomous vehicles.

Method: Integrates 4D radar Doppler to build motion-centric dynamic maps, generates reference queries for dynamic/high-confidence regions and exploratory queries for occluded areas, and exchanges only query-specific BEV features using multi-scale gated deformable attention for fusion.

Result: Achieves up to 90% lower bandwidth than full-map sharing while matching or surpassing prior baselines across varied traffic densities and occlusion scenarios.

Conclusion: SlimComm provides a communication-efficient framework that maintains perception accuracy while dramatically reducing bandwidth requirements for collaborative autonomous vehicle systems.

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [224] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0 is a real-time interactive world model that generates long videos at 25 FPS using few-step auto-regressive diffusion, addressing the speed limitations of existing models.


<details>
  <summary>Details</summary>
Motivation: Existing interactive world models suffer from slow inference due to bidirectional attention and lengthy steps, making real-time simulation of dynamic environments challenging.

Method: Three components: scalable data pipeline for Unreal Engine/GTA5 (1200+ hours), action injection module for frame-level inputs, and few-step distillation with causal architecture for streaming generation.

Result: Achieves high-quality minute-level video generation across diverse scenes at 25 FPS, enabling real-time interactive simulations.

Conclusion: The framework advances interactive world modeling by providing real-time performance while maintaining quality, with open-sourced weights and codebase.

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [225] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: EgoTwin is a diffusion transformer framework for joint egocentric video and human motion generation that addresses viewpoint alignment and causal interplay challenges through head-centric motion representation and cybernetics-inspired attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Egocentric video generation remains underexplored compared to exocentric video synthesis, requiring modeling of first-person view content with camera motion patterns from body movements, which presents unique challenges in viewpoint alignment and causal interplay.

Method: Proposes EgoTwin framework built on diffusion transformer architecture with head-centric motion representation (anchoring human motion to head joint) and cybernetics-inspired interaction mechanism that captures causal interplay between video and motion within attention operations.

Result: Extensive experiments demonstrate effectiveness of EgoTwin framework, evaluated on a newly curated large-scale real-world dataset of synchronized text-video-motion triplets with novel metrics for assessing video-motion consistency.

Conclusion: EgoTwin successfully addresses the challenges of joint egocentric video and human motion generation through innovative architectural designs and provides a comprehensive evaluation framework for this emerging task.

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [226] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: HierAdaptMR is a hierarchical feature adaptation framework for cardiac MRI reconstruction that addresses multi-center domain shifts through parameter-efficient adapters for protocol-level and center-level variations, with a universal adapter for unseen centers.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based cardiac MRI reconstruction faces significant domain shift challenges when deployed across multiple clinical centers with heterogeneous scanner configurations and imaging protocols.

Method: Uses hierarchical feature adaptation with Protocol-Level Adapters for sequence-specific characteristics and Center-Level Adapters for scanner-dependent variations, built on variational unrolling backbone. Includes Universal Adapter for unseen centers through stochastic training, with multi-scale SSIM loss and frequency domain enhancement.

Result: Comprehensive evaluation on CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9 modalities demonstrates superior cross-center generalization while maintaining reconstruction quality.

Conclusion: HierAdaptMR effectively addresses multi-level domain variations in cardiac MRI reconstruction through parameter-efficient hierarchical adaptation, enabling robust performance across diverse clinical centers and unseen scanner configurations.

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [227] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: A novel situated visualization technique that guides users during scene scanning by identifying important objects needing extended image coverage for view-dependent appearance, using semantic segmentation and vision-language models to improve 3D Gaussian splatting input quality.


<details>
  <summary>Details</summary>
Motivation: High-quality view synthesis requires uniform and dense view sampling, but human camera operators often struggle with this due to impatience, lack of scene understanding, or time constraints. Existing guidance methods focus on single objects or neglect view-dependent material characteristics.

Method: Leverages semantic segmentation and category identification ranked by a vision-language model to identify important objects needing extended image coverage. Generates spherical proxies around highly ranked objects to guide users during scanning at multiple scales.

Result: The method shows superior performance in real scenes compared to conventional view sampling strategies, enabling better representation of view-dependent appearance.

Conclusion: The proposed situated visualization technique effectively addresses the challenge of guiding human operators to collect optimal input images for high-quality novel view synthesis, particularly for view-dependent material representation.

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [228] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: Odo: A diffusion-based method for realistic human shape editing using a new large-scale dataset and ControlNet-guided transformations with target SMPL depth maps, achieving state-of-the-art results with 7.5mm reconstruction error.


<details>
  <summary>Details</summary>
Motivation: Human shape editing remains underexplored compared to pose editing, with current methods suffering from unrealistic proportions, texture distortions, and background inconsistencies due to lack of proper datasets and alignment errors.

Method: End-to-end diffusion-based approach combining a frozen UNet to preserve appearance/background details with a ControlNet that guides shape transformation using target SMPL depth maps, trained on a new large-scale dataset of 18,573 images across 1523 subjects.

Result: Outperforms prior approaches with per-vertex reconstruction errors as low as 7.5mm (vs 13.6mm in baselines), producing realistic results that accurately match desired target shapes while preserving identity, clothing, and background.

Conclusion: The proposed Odo method and new dataset enable realistic and intuitive body reshaping guided by simple semantic attributes, addressing key limitations in human shape editing through a novel diffusion-based architecture.

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [229] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: Two-stage multimodal framework using radiologist eye-tracking data to improve chest X-ray disease classification and generate region-aware radiology reports, achieving significant performance gains in both tasks.


<details>
  <summary>Details</summary>
Motivation: To leverage radiologist eye-tracking data (gaze patterns) to enhance both disease classification accuracy and the quality/interpretability of automated radiology report generation from chest X-rays.

Method: Two-stage approach: 1) Gaze-guided contrastive learning with multi-term gaze-attention loss (MSE, KL divergence, correlation, center-of-mass alignment) for disease classification; 2) Modular report generation pipeline extracting confidence-weighted diagnostic keywords, anatomical region mapping, and structured prompt-based sentence generation.

Result: Classification: F1 score improved from 0.597 to 0.631 (+5.70%), AUC from 0.821 to 0.849 (+3.41%); Report generation: Improved clinical keyword recall and ROUGE scores.

Conclusion: Integrating gaze data significantly improves both disease classification performance and the interpretability/quality of generated medical reports, demonstrating the value of eye-tracking signals in medical AI systems.

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [230] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: Using Stable Diffusion to generate synthetic bona fide ID card images improves Presentation Attack Detection system performance by addressing data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Current PAD systems face challenges due to limited bona fide image availability and increasing diversity of attack methods. Most existing approaches focus on generating attack samples but neglect the scarcity of genuine images needed for robust training.

Method: Proposes using Stable Diffusion to generate synthetic bona fide ID card images, creating additional training data. The synthetic images are evaluated in both a system trained from scratch and a commercial PAD solution.

Result: The PAD system successfully identifies the synthetic images as bona fide, demonstrating improved detection performance and helping overcome data restrictions.

Conclusion: Synthetic image generation using Stable Diffusion is an effective approach to enhance PAD system generalization capabilities by addressing the fundamental problem of limited bona fide training data availability.

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [231] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: A new RSVQA dataset called Chessboard with 3.1M questions and balanced answer distribution is introduced, along with an explainable model Checkmate that identifies relevant image cells for decision-making.


<details>
  <summary>Details</summary>
Motivation: Address lack of interpretability and explainability in RSVQA models, prevent shortcut learning from dataset biases, and ensure model decisions are grounded in visual content.

Method: Created Chessboard dataset with 3,123,253 questions and balanced answer distribution where each answer is linked to specific image cells. Developed Checkmate model that identifies relevant image cells for its decisions.

Result: The approach improves transparency and supports more trustworthy decision-making in RSVQA systems across multiple model architectures.

Conclusion: The Chessboard dataset and Checkmate model provide an effective solution for creating explainable and interpretable RSVQA systems that avoid dataset biases and ensure visual grounding.

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [232] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: DMS uses diffusion models to synthesize novel views for self-supervised stereo matching and depth estimation, addressing occlusion issues by generating additional perspectives to enable explicit photometric reconstruction.


<details>
  <summary>Details</summary>
Motivation: Self-supervised stereo matching and monocular depth estimation face challenges from photometric ambiguity in ill-posed regions like occlusions and out-of-frame areas, where corresponding pixels are missing.

Method: Finetune Stable Diffusion model to synthesize novel views along epipolar direction using directional prompts, generating left-left, right-right, and intermediate views to supplement occluded pixels for explicit photometric reconstruction.

Result: Up to 35% outlier reduction and state-of-the-art performance across multiple benchmark datasets, demonstrating significant improvement in self-supervised stereo matching and depth estimation.

Conclusion: DMS is an effective model-agnostic, plug-and-play approach that enhances self-supervised vision tasks using only unlabeled stereo image pairs, successfully addressing photometric ambiguity through diffusion-based view synthesis.

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [233] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: RT-DETR-L model provides better practical efficiency for real-time beach litter detection despite slightly lower accuracy than RT-DETR-X, due to significantly faster inference speed.


<details>
  <summary>Details</summary>
Motivation: Coastal pollution requires scalable automated monitoring solutions, necessitating research into efficient object detection models for beach litter detection and counting.

Method: Comparative analysis of two RT-DETR variants (Large and Extra-Large) trained on coastal debris dataset, evaluating accuracy metrics (mAP@50 and mAP@50-95) and inference time.

Result: RT-DETR-X achieved marginally better accuracy (mAP@50: 0.816, mAP@50-95: 0.612) but RT-DETR-L had significantly faster inference (20.1ms vs 34.5ms) with comparable accuracy (mAP@50: 0.810, mAP@50-95: 0.606).

Conclusion: RT-DETR-L offers superior practical efficiency for real-time deployment due to better balance between processing speed and detection accuracy, making it more viable for field applications.

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [234] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: Visual Action Prompts (VAP) use visual skeletons as domain-agnostic representations for precise action-to-video generation while maintaining cross-domain transferability of visual dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing action-driven video generation methods face a precision-generality trade-off - text/primitive actions lack precision while agent-centric actions lack cross-domain transferability.

Method: Render actions into visual skeletons as domain-agnostic representations, construct pipelines from human-object interactions and robotic manipulation data, integrate into pretrained video models via lightweight fine-tuning.

Result: Effective action control of complex interactions while preserving cross-domain dynamics learning, demonstrated on EgoVid, RT-1 and DROID datasets.

Conclusion: Visual skeletons provide a unified action representation that balances geometric precision and cross-domain adaptability for complex action-to-video generation.

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [235] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: Motion2Motion is a training-free framework for transferring animations between characters with different skeletal topologies, requiring only 1-2 example motions and sparse bone correspondences.


<details>
  <summary>Details</summary>
Motivation: Transferring animations across characters with substantially different skeletal topologies is challenging due to topological inconsistencies and lack of large-scale paired motion datasets for data-driven approaches.

Method: A novel training-free framework that works with only one or few example motions on the target skeleton, using sparse bone correspondences between source and target skeletons without requiring extensive training data.

Result: Achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios, with successful integration in downstream applications and user interfaces.

Conclusion: Motion2Motion demonstrates practical utility for industrial applications, providing an effective solution for motion transfer across diverse skeletal topologies without extensive training requirements.

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [236] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse is a novel framework that reconstructs interactive 3D Gaussian scenes by fusing multiple scans with object rearrangement to reveal occluded regions, using segmentation-aware Gaussian fields and bi-directional consistency constraints.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene reconstruction approaches suffer from persistent object occlusions, limited sensor coverage, and rely on error-prone multi-stage pipelines or require dense per-object scanning, which are not scalable.

Method: Constructs segmentation-aware Gaussian fields and enforces bi-directional photometric and semantic consistency across multiple scans. Uses pseudo-intermediate scene state for unified alignment and collaborative co-pruning strategies to refine geometry.

Result: Enables high-fidelity rendering and object-level scene manipulation without dense observations or complex pipelines. Shows strong generalization to novel scene configurations.

Conclusion: IGFuse effectively addresses 3D reconstruction challenges for real-world applications and real-to-simulation transfer, demonstrating superior performance compared to existing approaches.

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [237] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX is the first feed-forward framework for generating dynamic 3D scene representations from a single image, using a pretrained video diffusion model fine-tuned with novel adaptation strategies and a large-scale 4D dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 4D generation require computationally intensive optimization or multi-frame video inputs, creating a need for efficient, end-to-end image-to-4D generation that can work from single images.

Method: Fine-tunes pretrained video diffusion models using: 1) 4DNeX-10M dataset with high-quality 4D annotations, 2) unified 6D video representation for RGB+XYZ sequences, and 3) effective adaptation strategies for 4D modeling.

Result: Produces high-quality dynamic point clouds enabling novel-view video synthesis, outperforms existing methods in efficiency and generalizability.

Conclusion: 4DNeX offers a scalable solution for image-to-4D modeling and lays foundation for generative 4D world models that simulate dynamic scene evolution.

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [238] [Data-driven RF Tomography via Cross-modal Sensing and Continual Learning](https://arxiv.org/abs/2508.11654)
*Yang Zhao,Tao Wang,Said Elhadi*

Main category: eess.SP

TL;DR: DRIFT framework combines RF tomography with cross-modal learning and continual learning to achieve accurate underground root tuber imaging in dynamic environments, improving state-of-the-art by 23.2%.


<details>
  <summary>Details</summary>
Motivation: RF tomography shows promise for underground detection but struggles with accuracy and robustness in dynamic environments where RF signals change significantly.

Method: Cross-modal sensing system with RF and visual sensors, training RF tomography DNN via cross-modal learning, and applying continual learning to automatically update model when environmental changes are detected.

Result: Achieves average equivalent diameter error of 2.29 cm, representing 23.2% improvement over state-of-the-art approaches.

Conclusion: The DRIFT framework successfully addresses dynamic environment challenges in RF tomography through cross-modal and continual learning, providing robust underground imaging with significant performance improvements.

Abstract: Data-driven radio frequency (RF) tomography has demonstrated significant
potential for underground target detection, due to the penetrative nature of RF
signals through soil. However, it is still challenging to achieve accurate and
robust performance in dynamic environments. In this work, we propose a
data-driven radio frequency tomography (DRIFT) framework with the following key
components to reconstruct cross section images of underground root tubers, even
with significant changes in RF signals. First, we design a cross-modal sensing
system with RF and visual sensors, and propose to train an RF tomography deep
neural network (DNN) model following the cross-modal learning approach. Then we
propose to apply continual learning to automatically update the DNN model, once
environment changes are detected in a dynamic environment. Experimental results
show that our approach achieves an average equivalent diameter error of 2.29
cm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and
dataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [239] [Point upsampling networks for single-photon sensing](https://arxiv.org/abs/2508.12986)
*Jinyi Liu,Guoyang Zhao,Lijun Liu,Yiguang Hong,Weiping Zhang,Shuming Cheng*

Main category: physics.optics

TL;DR: Proposes a point upsampling network using state space models to enhance sparse single-photon point clouds by increasing density and reducing spatial distortion.


<details>
  <summary>Details</summary>
Motivation: Single-photon sensing produces sparse and spatially biased point clouds that limit practical utility, requiring methods to improve point density and reduce distortion.

Method: Uses a network built on state space model with multi-path scanning mechanism, bidirectional Mamba backbone for global/local feature capture, and adaptive upsample shift module for distortion correction.

Result: Achieves high reconstruction accuracy and strong robustness to distortion noise on datasets, generates visually consistent, detail-preserving point clouds on real-world data.

Conclusion: Establishes the first upsampling framework for single-photon sensing, opening new avenues for practical applications in downstream tasks.

Abstract: Single-photon sensing has generated great interest as a prominent technique
of long-distance and ultra-sensitive imaging, however, it tends to yield sparse
and spatially biased point clouds, thus limiting its practical utility. In this
work, we propose using point upsampling networks to increase point density and
reduce spatial distortion in single-photon point cloud. Particularly, our
network is built on the state space model which integrates a multi-path
scanning mechanism to enrich spatial context, a bidirectional Mamba backbone to
capture global geometry and local details, and an adaptive upsample shift
module to correct offset-induced distortions. Extensive experiments are
implemented on commonly-used datasets to confirm its high reconstruction
accuracy and strong robustness to the distortion noise, and also on real-world
data to demonstrate that our model is able to generate visually consistent,
detail-preserving, and noise suppressed point clouds. Our work is the first to
establish the upsampling framework for single-photon sensing, and hence opens a
new avenue for single-photon sensing and its practical applications in the
downstreaming tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [240] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV is a novel method for sparsifying multiple-context KV Cache in RAG scenarios, enabling 15% sequence length compression without accuracy loss while significantly improving throughput.


<details>
  <summary>Details</summary>
Motivation: Existing KV Cache sparsification methods only work in single-context scenarios and fail in RAG where multiple independent contexts lack cross-attention dependencies, while full recomputation approaches don't reduce memory overhead.

Method: SamKV performs attention sparsification for multiple-context KV Cache by considering complementary information across contexts during sparsification and locally recomputing the sparsified information.

Result: The method achieves 15% sequence length compression without accuracy degradation compared to full-recomputation baselines, significantly boosting throughput in multi-context RAG scenarios.

Conclusion: SamKV successfully addresses the limitations of existing KV Cache sparsification methods in RAG scenarios by enabling effective compression of multiple independent contexts while maintaining accuracy.

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [241] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: RS is a model-agnostic framework that detects adversarial text by measuring embedding sensitivity when masking important words, achieving high detection accuracy across various attacks and models without retraining.


<details>
  <summary>Details</summary>
Motivation: Adversarial text attacks threaten transformer models, but existing defenses are either attack-specific or require expensive model retraining, creating a need for more practical solutions.

Method: RS ranks words using importance heuristics, measures embedding sensitivity when masking top-k critical words, and processes patterns with a BiLSTM detector to identify adversarial examples.

Result: Achieves over 88% detection accuracy across three datasets, three attack types, and two victim models, with competitive performance and lower computational cost than state-of-the-art methods.

Conclusion: RS provides a practical, generalizable solution for adversarial text detection that works across unseen datasets, attacks, and models without requiring retraining.

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [242] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET is a 1B-parameter medical foundation model trained on 115B medical events from 118M patients, demonstrating that generative pretraining on longitudinal health data outperforms task-specific models across 78 clinical tasks without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enable personalized medicine at scale by developing foundation models that can distill insights from longitudinal patient journeys and generalize to diverse clinical tasks without task-specific training.

Method: Pretrained decoder-only transformer models on 16.3B encounters from 300M patients using autoregressive next-event prediction, conducting the largest medical scaling-law study to determine compute-optimal model sizes up to 1B parameters.

Result: CoMET outperformed or matched task-specific supervised models on 78 real-world tasks including diagnosis prediction, disease prognosis, and healthcare operations, with performance consistently improving with model and pretraining scale.

Conclusion: Generative medical event foundation models can effectively capture complex clinical dynamics and provide a generalizable framework for clinical decision-making, healthcare operations, and improved patient outcomes without task-specific fine-tuning.

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [243] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT is a dynamic method that automatically optimizes instruction-tuning dataset mixtures using multi-armed bandit approach with prior-scaled exploration, achieving 2.2% performance improvement on 10 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of dynamically balancing and optimizing instruction-tuning dataset mixtures as numerous datasets emerge during post-training stage.

Method: Formulates the problem as multi-armed bandit setup with Prior-scaled Boltzmann Exploration that anchors sampling distribution to original dataset proportions. Uses lightweight 1-Step Look-ahead Reward to update sampling probabilities based on dataset contribution to model improvement.

Result: Achieves up to 2.2% performance improvement across 10 benchmarks when applied to Tulu-v2-mixture collection of 16 instruction-tuning datasets.

Conclusion: The method effectively preserves dataset diversity while dynamically optimizing mixtures, providing adaptive optimization for instruction-tuning datasets.

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [244] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: Bridge is a statistical framework that models and corrects systematic discrepancies between human and LLM evaluations, improving LLM-as-a-judge accuracy through linear transformation modeling of rating deviations.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly used as judges to evaluate model outputs, but their assessments often diverge systematically from human judgments, creating a need for a principled framework to bridge this gap.

Method: Bridge posits latent human preference scores and models LLM deviations as linear transformations of covariates that capture sources of discrepancies, providing an efficient fitting algorithm with asymptotic guarantees for statistical inference.

Result: Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings in accuracy, calibration, and KL divergence, while exposing systematic human-LLM gaps.

Conclusion: Bridge offers a simple and principled statistical framework for refining LLM ratings and characterizing systematic discrepancies between human and LLM evaluations, improving the reliability of LLM-as-a-judge systems.

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [245] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: MaxScore is a novel MoE routing method that uses minimum-cost maximum-flow optimization with SoftTopk operator to eliminate token dropping and improve hardware efficiency while maintaining load balancing.


<details>
  <summary>Details</summary>
Motivation: Traditional MoE networks suffer from token dropping when expert capacity is saturated and low hardware efficiency due to padding in underutilized experts. Removing capacity constraints compromises load balancing and computational efficiency.

Method: Proposes Maximum Score Routing (MaxScore) that models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator to overcome limitations of iterative rerouting and optimal transport formulations.

Result: Achieves lower training losses and higher evaluation scores at equivalent FLOPs compared to both constrained and unconstrained baselines.

Conclusion: MaxScore resolves fundamental limitations of existing MoE routing approaches by providing an efficient optimization-based solution that eliminates token dropping while maintaining computational efficiency.

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [246] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: L2S (Learn-to-Steer) introduces input-specific steering vectors for multimodal LLMs using contrastive prompting and an auxiliary module, reducing hallucinations and improving safety compared to static steering methods.


<details>
  <summary>Details</summary>
Motivation: Existing steering techniques for MLLMs rely on single static vectors applied uniformly, which fails when desired behavior depends on specific input context (e.g., safety responses vary based on query type).

Method: Proposes fine-grained steering using input-specific linear shifts computed via contrastive prompting, with a small auxiliary module trained to predict these steering vectors at test time.

Result: L2S demonstrates reduced hallucinations and improved safety enforcement in MLLMs, outperforming static steering baselines.

Conclusion: Input-specific steering through learned auxiliary modules provides more effective and context-aware guidance for multimodal LLMs compared to one-size-fits-all approaches.

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [247] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ is a lightweight uncertainty monitoring system for TinyML that uses temporal consistency and streaming conformal calibration to provide risk scores without labels or extra computation, achieving significant efficiency gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enable reliable uncertainty monitoring on resource-constrained TinyML devices without the computational overhead and memory requirements of traditional methods like early exit and deep ensembles.

Method: Single-pass, label-free approach that converts short-horizon temporal consistency from posteriors and features into calibrated risk scores using O(W) ring buffer and O(1) per-step updates, with streaming conformal calibration for budgeted accept/abstain decisions.

Result: 50-60% smaller footprint and 30-45% faster than early exit/ensembles; 3-7 AUPRC improvement in accuracy drop detection (up to 0.86 AUPRC); up to 0.92 AUROC for failure detection; fits on kilobyte-scale microcontrollers.

Conclusion: Temporal consistency combined with streaming conformal calibration provides a practical, resource-efficient foundation for on-device uncertainty monitoring in TinyML applications.

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [248] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ is a single-pass, label-free uncertainty method for TinyML that predicts next-layer statistics to estimate risk, requiring minimal resources while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification methods for TinyML are resource-intensive, requiring temporal buffers, auxiliary exits, or repeated forward passes, which are impractical for memory-constrained microcontrollers.

Method: Uses tiny int8 heads to forecast next-layer statistics from compressed previous layer views, with a lightweight monotone mapper converting surprisal into actionable uncertainty scores without additional buffers or passes.

Result: Reduces flash and latency by 40-60% and 25-35% respectively compared to early-exit and deep ensembles, improves accuracy-drop detection by several AUPRC points, and maintains AUROC â‰ˆ0.9 for failure detection in single pass.

Conclusion: Grounding uncertainty in layer-to-layer dynamics provides a practical, resource-efficient solution for on-device monitoring in TinyML deployments with minimal memory overhead.

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [249] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: MSLoRA-CR is a multimodal biomedical image incremental learning method that uses modality-specific LoRA modules with contrastive regularization to enable efficient knowledge sharing across modalities while preventing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Traditional incremental learning methods focus on task expansion within single modalities, but biomedical applications require handling multiple modalities efficiently without training separate models for each modality to reduce inference costs.

Method: Fine-tunes Modality-Specific LoRA modules while incorporating Contrastive Regularization to enhance intra-modality knowledge sharing and promote inter-modality knowledge differentiation, building upon a frozen large vision-language model.

Result: Achieves 1.88% improvement in overall performance compared to unconstrained incremental learning methods while maintaining computational efficiency, outperforming both separate model training and general incremental learning approaches.

Conclusion: MSLoRA-CR effectively addresses the challenges of multimodal biomedical image incremental learning by preserving previously learned knowledge and leveraging cross-modal knowledge transfer through specialized LoRA modules and contrastive regularization.

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [250] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: Using Google DeepMind's AlphaEarth Foundations to extend geospatial labeled datasets beyond their original geographic regions with basic models like random forests and logistic regression.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled geospatial datasets are often limited to specific geographic regions where data was collected, creating gaps in global coverage.

Method: Leveraging AlphaEarth Foundations (AEF) as input representation and using basic models (random forests, logistic regression) to extend LANDFIRE's Existing Vegetation Type dataset from USA to Canada at two granularity levels.

Result: Model predictions align qualitatively with ground truth for EvtPhys (13 classes). Achieved 81% accuracy on USA validation set and 73% accuracy on Canada validation set for EvtPhys classification.

Conclusion: AEF enables effective extension of geospatial datasets beyond original regions using simple models, demonstrating practical applicability for global geospatial analysis despite some limitations.

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [251] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: ENA combines linear recurrence with tiled high-order sliding window attention to efficiently model ultra-long high-dimensional data, outperforming pure scanning strategies.


<details>
  <summary>Details</summary>
Motivation: Transformer architectures are inefficient for modeling long sequences of high-order data, requiring more efficient alternatives that can handle 1D to ND data effectively.

Method: Investigates scanning strategies and attention-hybrid architectures, focusing on combining linear recurrence (for global compression) with tiled high-order sliding window attention (for local modeling).

Result: Scanning provides limited benefits while attention-hybrid models show promise. ENA architecture demonstrates effectiveness in handling ultra-long high-order data.

Conclusion: ENA offers a simple yet practical framework that combines the strengths of linear recurrence (global information compression) and sliding window attention (strict local modeling) for efficient high-dimensional data modeling.

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [252] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: A novel learned second-order optimizer that enhances the classical SR1 algorithm with trainable preconditioning, outperforming existing learned optimization methods on monocular human mesh recovery tasks.


<details>
  <summary>Details</summary>
Motivation: End-to-end deep learning relies on large labeled datasets and has poor generalization, while classical optimization methods are data-efficient but slow. Learned optimizers combine both benefits but most focus on first-order methods, leaving second-order approaches unexplored.

Method: Proposes a learned second-order optimizer with trainable preconditioning unit that generates data-driven vectors to construct positive semi-definite rank-one matrices aligned with secant constraint via learned projection.

Result: Outperforms existing learned optimization-based approaches on monocular human mesh recovery tasks, featuring lightweight model, no annotated data requirements, and strong generalization.

Conclusion: The approach offers a promising fusion of deep learning and classical optimization, well-suited for integration into broader optimization-based frameworks with excellent data efficiency and generalization capabilities.

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [253] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: Proposes Latent Reconstruction loss to address posterior collapse in VAEs without architectural constraints, using mathematical properties of injective functions to improve sample diversity.


<details>
  <summary>Details</summary>
Motivation: VAEs suffer from posterior collapse which reduces sample diversity. Existing solutions require trade-offs between reconstruction and regularization or impose structural network constraints, which are unsatisfactory.

Method: Defines local posterior collapse concept and proposes Latent Reconstruction (LR) loss based on mathematical properties of injective and composite functions, allowing control without specific architecture restrictions.

Result: Experimental evaluation shows effective control of posterior collapse across multiple datasets including MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

Conclusion: The LR loss provides an effective solution for controlling posterior collapse in VAEs without requiring architectural constraints, improving generative sample diversity.

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [254] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: Federated learning framework for traffic sign detection in vehicular networks that enables collaborative training without sharing raw data, achieving up to 0.83 accuracy while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy and communication challenges from vast sensor data generated by connected/automated vehicles, avoiding centralized machine learning approaches that require raw data sharing.

Method: Decentralized federated learning with specialized local training using lightweight object detectors, parameter aggregation via FedProx/FedAdam/FedAVG algorithms in Flower framework simulation, testing various configurations (server rounds, local epochs, client participation, data distributions).

Result: Increasing server rounds (2â†’20) boosted accuracy from <0.1 to >0.8; moderate local epochs (8-10) achieved ~0.67 accuracy; higher client participation enhanced generalization to 0.83; FedProx best handled heterogeneity; non-IID data reduced performance; training duration scaled with rounds.

Conclusion: Federated learning offers scalable, privacy-preserving solution for vehicular deployments, with potential for future integration of robust aggregation and communication optimizations in intelligent transportation systems.

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [255] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: Paper reconciles contradictions between causal modeling claims and domain generalization benchmarks, advocating for a more nuanced theory of causality's role in AI generalization.


<details>
  <summary>Details</summary>
Motivation: Recent domain generalization benchmarks have challenged the promise that causal modeling leads to robust AI generalization, creating apparent contradictions in the literature that need reconciliation.

Method: The authors revisit and analyze claims from both causality and domain generalization literature, providing theoretical reconciliation and an interactive demo for practical exploration.

Result: The paper develops a more nuanced understanding of how causality contributes to generalization, addressing the apparent contradictions between theoretical promises and empirical benchmark results.

Conclusion: Causal modeling's role in AI generalization requires a more sophisticated theoretical framework than previously assumed, with interactive tools helping bridge the gap between theory and practical domain generalization performance.

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [256] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: This paper explores integrating large language models (LLMs) with physical robots to enable natural language collaboration between humans and robotic assistants, presenting proof-of-concept experiments using ChatGPT.


<details>
  <summary>Details</summary>
Motivation: The vision is to create autonomous robots that can collaborate with humans using natural language, overcoming the limited language understanding of traditional Interactive Task Learning systems by leveraging LLMs' advanced language capabilities.

Method: The approach involves creating an AI system with a cognitive agent controlling a physical robot that interacts with both humans and LLMs, accumulating situational knowledge through experiences. The paper presents three proof-of-concept experiments using ChatGPT to address natural language understanding challenges.

Result: The paper demonstrates simple proof-of-concept experiments showing that LLMs like ChatGPT can be used to help robots understand natural language commands, though the experiments remain at an early stage.

Conclusion: While LLMs show promise for enhancing robot language understanding, significant work is needed to turn these proof-of-concept experiments into operational systems where LLM-assisted language understanding is integrated into robotic assistants that can effectively collaborate with humans.

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


### [257] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: This paper addresses data shift problems in autonomous driving object detection by analyzing data shift complexity, using shift detection methods for dataset categorization/balancing, and integrating CycleGAN data augmentation with YOLOv5 to achieve superior performance on BDD100K dataset.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in autonomous driving are vulnerable to data shift problems caused by seasonal and weather variations that violate the i.i.d. assumption, leading to performance degradation in real-world applications.

Method: Systematic analysis of data shift complexity, comprehensive review of shift detection methods, dataset categorization and balancing using detection techniques, and integration of CycleGAN-based data augmentation with YOLOv5 framework.

Result: The proposed method achieves superior performance compared to baseline models on the BDD100K dataset, demonstrating effectiveness in handling data shift problems.

Conclusion: The approach successfully addresses data shift challenges in autonomous driving object detection through systematic analysis and innovative integration of data augmentation techniques with detection frameworks.

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [258] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: A robotic system that solves Rubik's cubes using stepper motors, YOLOv8 for real-time detection, and Kociemba's algorithm, achieving ~2.2 minute solve times with a Unity GUI interface.


<details>
  <summary>Details</summary>
Motivation: To create an automated robotic system capable of solving Rubik's cubes efficiently using computer vision and mechanical manipulation, providing both physical solving capability and visual feedback through a user interface.

Method: Uses three stepper motors for physical manipulation, YOLOv8 model (Precision 0.98443, Recall 0.98419) for real-time cube state detection, microcontroller for hardware control, and Kociemba's algorithm for solution generation. Features a Unity-based GUI for visualization.

Result: Achieved high detection accuracy with YOLOv8 (Box Loss 0.42051, Class Loss 0.2611) and average solving time of approximately 2.2 minutes using single degree of freedom manipulation.

Conclusion: The system successfully integrates computer vision, mechanical manipulation, and algorithmic solving to create an efficient Rubik's cube solving robot with real-time state detection and user-friendly interface.

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [259] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: PROD is a novel method that uses palpative interaction and force-controlled probing to reconstruct deformable objects' shape and mechanical properties through elastostatic signed distance functions.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches rely on purely geometric or visual data, which may not capture mechanical properties. There's a need to integrate physical interaction data to better understand deformable objects' static and dynamic responses.

Method: Models deformation as elastostatic process, derives Poisson equation for SDF estimation from sparse pose and force measurements. Uses steady-state elastodynamic assumptions to recover undeformed SDF from deformed observations. Estimates material stiffness by analyzing displacement responses to varying forces.

Result: PROD demonstrates robustness in handling pose errors, non-normal force application, and curvature errors in simulated soft body interactions. Provides provable convergence for SDF recovery.

Conclusion: PROD is a powerful tool for reconstructing deformable objects with applications in robotic manipulation, medical imaging, and haptic feedback systems due to its ability to estimate both shape and mechanical properties.

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [260] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: Motion-based temporal and rotational calibration framework for event camera multi-sensor systems without calibration targets, using angular velocity from normal flow observations and two-step optimization.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer microsecond-scale latency but extrinsic calibration for event-centric multi-sensor systems remains understudied, requiring target-free solutions.

Method: Uses rotational motion estimates from event cameras and other sensors, estimates angular velocity from normal flow observations, employs two-step approach with CCA initialization and joint non-linear optimization with continuous-time SO(3) parametrization.

Result: Achieves calibration accuracy comparable to target-based methods with superior stability over pure CCA methods, demonstrating precision, robustness and flexibility.

Conclusion: Proposed framework provides effective target-free calibration for event camera multi-sensor systems with open-source implementation to facilitate future research.

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [261] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: OC-VLA framework addresses spatial inconsistencies in VLA models by grounding action predictions directly in camera observation space using extrinsic calibration, improving generalization across diverse viewpoints.


<details>
  <summary>Details</summary>
Motivation: VLA models struggle with generalization due to discrepancies between observation and action spaces, particularly when predicting end-effector poses in robot base coordinates while training on diverse camera perspectives.

Method: Transforms end-effector poses from robot base coordinate system to camera coordinate system using camera's extrinsic calibration matrix, creating a lightweight plug-and-play strategy that unifies prediction targets across viewpoints.

Result: Accelerates convergence, enhances task success rates, improves cross-view generalization in both simulated and real-world robotic manipulation tasks.

Conclusion: OC-VLA provides a robust solution for aligning perception and action in VLA models, requiring no substantial architectural modifications while significantly improving resilience to camera viewpoint variations.

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [262] [Code Vulnerability Detection Across Different Programming Languages with AI Models](https://arxiv.org/abs/2508.11710)
*Hael Abdulhakim Ali Humran,Ferdi Sonmez*

Main category: cs.CR

TL;DR: Transformer-based AI models like CodeBERT can detect code vulnerabilities with over 97% accuracy, outperforming traditional rule-based static analysis tools by better understanding context-dependent bugs.


<details>
  <summary>Details</summary>
Motivation: Traditional static analysis tools based on rule-based patterns struggle with context-dependent security vulnerabilities and produce high false positive rates, creating a need for more effective detection methods.

Method: Used transformer-based models (CodeBERT, CodeLlama) with dynamic fine-tuning on vulnerable and safe code fragments, incorporating dataset gathering, language normalization, ensemble learning, and explainable AI techniques.

Result: Fine-tuned CodeBERT achieved accuracy greater than 97%, outperforming existing static analyzers, with near-perfect recall but some precision trade-offs that were addressed through hybrid models and validation procedures.

Conclusion: AI-based solutions generalize well across programming languages and vulnerability classes, showing promise for enhancing trustworthiness in machine-learning-based vulnerability detectors, though challenges remain in robustness, interpretability, and deployment readiness.

Abstract: Security vulnerabilities present in a code that has been written in diverse
programming languages are among the most critical yet complicated aspects of
source code to detect. Static analysis tools based on rule-based patterns
usually do not work well at detecting the context-dependent bugs and lead to
high false positive rates. Recent developments in artificial intelligence,
specifically the use of transformer-based models like CodeBERT and CodeLlama,
provide light to this problem, as they show potential in finding such flaws
better. This paper presents the implementations of these models on various
datasets of code vulnerability, showing how off-the-shelf models can
successfully produce predictive capacity in models through dynamic fine-tuning
of the models on vulnerable and safe code fragments. The methodology comprises
the gathering of the dataset, normalization of the language, fine-tuning of the
model, and incorporation of ensemble learning and explainable AI. Experiments
show that a well-trained CodeBERT can be as good as or even better than some
existing static analyzers in terms of accuracy greater than 97%. Further study
has indicated that although language models can achieve close-to-perfect
recall, the precision can decrease. A solution to this is given by hybrid
models and validation procedures, which will reduce false positives. According
to the results, the AI-based solutions generalize to different programming
languages and classes of vulnerability. Nevertheless, robustness,
interpretability, and deployment readiness are still being developed. The
results illustrate the probabilities that AI will enhance the trustworthiness
in the usability and scalability of machine-learning-based detectors of
vulnerabilities.

</details>


### [263] [Optimizing Token Choice for Code Watermarking: A RL Approach](https://arxiv.org/abs/2508.11925)
*Zhimeng Guo,Huaisheng Zhu,Siyuan Xu,Hangfan Zhang,Teng Xiao,Minhao Cheng*

Main category: cs.CR

TL;DR: CodeTracer is a reinforcement learning-based framework for watermarking LLM-generated code that intelligently biases token choices to embed detectable watermarks while preserving code functionality.


<details>
  <summary>Details</summary>
Motivation: The need to detect LLM-generated code requires watermarking systems that can operate within the highly structured and syntactically constrained environment of programming languages.

Method: Uses a policy-driven reinforcement learning approach with parameterized model to bias token choices during next-token prediction, Gumbel Top-k reparameterization for gradient optimization, and comprehensive reward system integrating execution feedback with watermark signals.

Result: Extensive evaluations show CodeTracer significantly outperforms state-of-the-art baselines in both watermark detectability and preservation of generated code's functionality.

Conclusion: CodeTracer provides an effective adaptive framework for watermarking LLM-generated code that maintains functionality while enabling reliable detection through subtle statistical deviations in token distributions.

Abstract: The need for detecting LLM-generated code necessitates watermarking systems
capable of operating within its highly structured and syntactically constrained
environment. To address this, we introduce CodeTracer, an innovative adaptive
code watermarking framework underpinned by a novel reinforcement learning
training paradigm. At its core, CodeTracer features a policy-driven approach
that utilizes a parameterized model to intelligently bias token choices during
next-token prediction. This strategy ensures that embedded watermarks maintain
code functionality while exhibiting subtle yet statistically detectable
deviations from typical token distributions. To facilitate policy learning, we
devise a comprehensive reward system that seamlessly integrates execution
feedback with watermark embedding signals, balancing process-level and
outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization
to enable gradient-based optimization of discrete watermarking decisions.
Extensive comparative evaluations demonstrate CodeTracer's significant
superiority over state-of-the-art baselines in both watermark detectability and
the preservation of generated code's functionality.

</details>


### [264] [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072)
*Wei Jie Yeo,Ranjan Satapathy,Erik Cambria*

Main category: cs.CR

TL;DR: Intent-FT is a lightweight fine-tuning method that trains LLMs to infer instruction intent before responding, significantly improving jailbreak resistance while preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: Despite safety-tuning, LLMs remain vulnerable to jailbreak attacks through adversarial instructions, creating a persistent safety-performance trade-off that needs addressing.

Method: Fine-tune LLMs on adversarial instructions to explicitly train them to infer underlying intent before generating responses, enabling generalization to unseen attacks.

Result: Intent-FT consistently mitigates all evaluated attack categories with no single attack exceeding 50% success rate, while preserving model utility and reducing over-refusal on benign instructions.

Conclusion: The approach effectively enhances LLM robustness against jailbreak attacks, transfers learned intent recognition to vanilla models, and maintains general capabilities without excessive safety compromises.

Abstract: Despite extensive safety-tuning, large language models (LLMs) remain
vulnerable to jailbreak attacks via adversarially crafted instructions,
reflecting a persistent trade-off between safety and task performance. In this
work, we propose Intent-FT, a simple and lightweight fine-tuning approach that
explicitly trains LLMs to infer the underlying intent of an instruction before
responding. By fine-tuning on a targeted set of adversarial instructions,
Intent-FT enables LLMs to generalize intent deduction to unseen attacks,
thereby substantially improving their robustness. We comprehensively evaluate
both parametric and non-parametric attacks across open-source and proprietary
models, considering harmfulness from attacks, utility, over-refusal, and impact
against white-box threats. Empirically, Intent-FT consistently mitigates all
evaluated attack categories, with no single attack exceeding a 50\% success
rate -- whereas existing defenses remain only partially effective. Importantly,
our method preserves the model's general capabilities and reduces excessive
refusals on benign instructions containing superficially harmful keywords.
Furthermore, models trained with Intent-FT accurately identify hidden harmful
intent in adversarial attacks, and these learned intentions can be effectively
transferred to enhance vanilla model defenses.

</details>


### [265] [Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: First safety analysis of diffusion LLMs reveals critical middle tokens are key to security, leading to MOSA alignment method that outperforms attacks while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs (dLLMs) are emerging as competitive non-autoregressive models but lack safety studies, creating a need to understand their unique security characteristics and develop appropriate alignment methods.

Method: Identified security asymmetry where middle tokens are critical for safety but attackers have limited manipulation power. Proposed MOSA (Middle-tOken Safety Alignment) using reinforcement learning to directly align middle generation with safe refusals.

Result: MOSA demonstrated superior security performance against eight attack methods on two benchmarks while maintaining utility on coding, math, and general reasoning tasks.

Conclusion: The asymmetry between defender and attacker positions in dLLMs enables effective safety alignment through middle token reinforcement, with MOSA proving to be a highly effective security solution for diffusion-based language models.

Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive non-autoregressive paradigm due to their unique training and
inference approach. However, there is currently a lack of safety study on this
novel architecture. In this paper, we present the first analysis of dLLMs'
safety performance and propose a novel safety alignment method tailored to
their unique generation characteristics. Specifically, we identify a critical
asymmetry between the defender and attacker in terms of security. For the
defender, we reveal that the middle tokens of the response, rather than the
initial ones, are more critical to the overall safety of dLLM outputs; this
seems to suggest that aligning middle tokens can be more beneficial to the
defender. The attacker, on the contrary, may have limited power to manipulate
middle tokens, as we find dLLMs have a strong tendency towards a sequential
generation order in practice, forcing the attack to meet this distribution and
diverting it from influencing the critical middle tokens. Building on this
asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method
that directly aligns the model's middle generation with safe refusals
exploiting reinforcement learning. We implement MOSA and compare its security
performance against eight attack methods on two benchmarks. We also test the
utility of MOSA-aligned dLLM on coding, math, and general reasoning. The
results strongly prove the superiority of MOSA.

</details>


### [266] [Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)](https://arxiv.org/abs/2508.11716)
*Javier MuÃ±oz-Haro,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CR

TL;DR: Proposes privacy-preserving patch-based methodology and FakeIDet2 database with 900K+ real/fake ID patches for training fake ID detectors while addressing data scarcity issues due to sensitive document privacy.


<details>
  <summary>Details</summary>
Motivation: Remote user verification using ID documents is crucial but faces challenges from AI-generated fake IDs. Research is hindered by scarcity of real ID data due to privacy concerns, making it difficult to train effective fake detection systems.

Method: Patch-based methodology to preserve privacy in ID research, creation of FakeIDet2-db database with 900K+ patches from 2,000 ID images under various conditions, and development of privacy-aware FakeIDet2 detection method with benchmark for physical and synthetic attacks.

Result: Created comprehensive public database (FakeIDet2-db) with extensive patch data covering multiple attack types (print, screen, composite), developed reproducible benchmark, and proposed privacy-preserving detection framework.

Conclusion: The study addresses critical data scarcity in fake ID detection research through privacy-aware patch methodology and provides valuable public resources (database and benchmark) to advance the field while respecting document privacy constraints.

Abstract: Remote user verification in Internet-based applications is becoming
increasingly important nowadays. A popular scenario for it consists of
submitting a picture of the user's Identity Document (ID) to a service
platform, authenticating its veracity, and then granting access to the
requested digital service. An ID is well-suited to verify the identity of an
individual, since it is government issued, unique, and nontransferable.
However, with recent advances in Artificial Intelligence (AI), attackers can
surpass security measures in IDs and create very realistic physical and
synthetic fake IDs. Researchers are now trying to develop methods to detect an
ever-growing number of these AI-based fakes that are almost indistinguishable
from authentic (bona fide) IDs. In this counterattack effort, researchers are
faced with an important challenge: the difficulty in using real data to train
fake ID detectors. This real data scarcity for research and development is
originated by the sensitive nature of these documents, which are usually kept
private by the ID owners (the users) and the ID Holders (e.g., government,
police, bank, etc.). The main contributions of our study are: 1) We propose and
discuss a patch-based methodology to preserve privacy in fake ID detection
research. 2) We provide a new public database, FakeIDet2-db, comprising over
900K real/fake ID patches extracted from 2,000 ID images, acquired using
different smartphone sensors, illumination and height conditions, etc. In
addition, three physical attacks are considered: print, screen, and composite.
3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We
release a standard reproducible benchmark that considers physical and synthetic
attacks from popular databases in the literature.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [267] [Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network](https://arxiv.org/abs/2508.12574)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.SI

TL;DR: Proposes Insight Rumors model with Att_BiMamba2 network and Rumor Locating/Marking module to not just detect but precisely locate and mark specific rumor content in text, outperforming existing rumor detection methods.


<details>
  <summary>Details</summary>
Motivation: Existing rumor detection models only classify contexts as rumors or not, lacking the capability to locate and mark specific rumor content within textual data.

Method: Developed Bidirectional Mamba2 Network with Dot-Product Attention (Att_BiMamba2) to enhance rumor feature representation, and a Rumor Locating/Marking module with skip-connection network and Conditional Random Fields (CRF) for precise content location.

Result: Extensive experiments show the model accurately detects rumors and precisely locates/marks them in context, outperforming state-of-the-art schemes that only discriminate rumors roughly.

Conclusion: The proposed Insight Rumors model successfully addresses the limitation of existing rumor detection by providing precise location and marking of rumor content, demonstrating superior performance over current approaches.

Abstract: With the development of social media networks, rumor detection models have
attracted more and more attention. Whereas, these models primarily focus on
classifying contexts as rumors or not, lacking the capability to locate and
mark specific rumor content. To address this limitation, this paper proposes a
novel rumor detection model named Insight Rumors to locate and mark rumor
content within textual data. Specifically, we propose the Bidirectional Mamba2
Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a
bidirectional Mamba2 model and applies dot-product attention to weight and
combine the outputs from both directions, thereby enhancing the representation
of high-dimensional rumor features. Simultaneously, a Rumor Locating and
Marking module is designed to locate and mark rumors. The module constructs a
skip-connection network to project high-dimensional rumor features onto
low-dimensional label features. Moreover, Conditional Random Fields (CRF) is
employed to impose strong constraints on the output label features, ensuring
accurate rumor content location. Additionally, a labeled dataset for rumor
locating and marking is constructed, with the effectiveness of the proposed
model is evaluated through comprehensive experiments. Extensive experiments
indicate that the proposed scheme not only detects rumors accurately but also
locates and marks them in context precisely, outperforming state-of-the-art
schemes that can only discriminate rumors roughly.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [268] [BeeNet: Reconstructing Flower Shapes from Electric Fields using Deep Learning](https://arxiv.org/abs/2508.11724)
*Jake Turley,Ryan A. Palmer,Isaac V. Chenchiah,Daniel Robert*

Main category: q-bio.QM

TL;DR: Deep learning model reconstructs flower shapes from electric fields generated by bee-flower interactions, showing electroreception provides rich spatial information for arthropod environmental perception.


<details>
  <summary>Details</summary>
Motivation: To understand how arthropods like pollinators use environmental electrical fields to perceive and decode spatial information about their surroundings, particularly flower shapes.

Method: Developed a deep learning UNet model trained on simulated electric field data from bee-flower interactions with varying petal geometries, testing reconstruction accuracy across different distances.

Result: The model accurately reconstructed diverse flower shapes, including complex shapes not in training data, with performance peaking at an optimal bee-flower distance.

Conclusion: Electroreception provides arthropods with rich spatial detail about environmental features, offering new insights into their sensory perception capabilities.

Abstract: Arthropods, including pollinators, respond to environmental electrical
fields. Here, we show that electric field information can be decoded to
reconstruct environmental features. We develop an algorithm capable of
inferring the shapes of polarisable flowers from the electric field generated
by a nearby charged bee. We simulated electric fields arising from bee flower
interactions for flowers with varying petal geometries. These simulated data
were used to train a deep learning UNet model to recreate petal shapes. The
model accurately reconstructed diverse flower shapes including more complex
flower shapes not included in training. Reconstruction performance peaked at an
optimal bee flower distance, indicating distance-dependent encoding of shape
information. These findings show that electroreception can impart rich spatial
detail, offering insights into arthropod environmental perception.

</details>


### [269] [On the Importance of Behavioral Nuances: Amplifying Non-Obvious Motor Noise Under True Empirical Considerations May Lead to Briefer Assays and Faster Classification Processes](https://arxiv.org/abs/2508.12742)
*Theodoros Bermperidis,Joe Vero,Elizabeth B Torres*

Main category: q-bio.QM

TL;DR: A new affective computing platform that uses brief 5-second face videos to capture facial micropeaks and micro expressions, enabling personalized statistical power without requiring large datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome the tradeoff between statistical power (requiring large datasets) and scalability (using brief data samples), and to avoid information loss from traditional grand-averaging techniques that assume normal distributions and linear processes.

Method: Combines a new data type derived from micropeaks in time series data from brief face videos with AI-driven face-grid estimation methods, using geometric and nonlinear dynamical systems approaches to analyze kinematics and speed data.

Result: The method captures all facial micropeaks including nuances of different affective micro expressions, and provides new ways to differentiate dynamical and geometric patterns in autistic individuals from neurotypical development.

Conclusion: The platform enables taking brief data samples while maintaining personalized statistical power, offering improved analysis of biorhythmic time series data without the limitations of traditional averaging techniques.

Abstract: There is a tradeoff between attaining statistical power with large, difficult
to gather data sets, and producing highly scalable assays that register brief
data samples. Often, as grand-averaging techniques a priori assume
normally-distributed parameters and linear, stationary processes in
biorhythmic, time series data, important information is lost, averaged out as
gross data. We developed an affective computing platform that enables taking
brief data samples while maintaining personalized statistical power. This is
achieved by combining a new data type derived from the micropeaks present in
time series data registered from brief (5-second-long) face videos with recent
advances in AI-driven face-grid estimation methods. By adopting geometric and
nonlinear dynamical systems approaches to analyze the kinematics, especially
the speed data, the new methods capture all facial micropeaks. These include as
well the nuances of different affective micro expressions. We offer new ways to
differentiate dynamical and geometric patterns present in autistic individuals
from those found more commonly in neurotypical development.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [270] [Statistical analysis of multivariate planar curves and applications to X-ray classification](https://arxiv.org/abs/2508.11780)
*MoindjiÃ© Issam-Ali,Descary Marie-HÃ©lÃ¨ne,Beaulac CÃ©dric*

Main category: stat.ME

TL;DR: A new method for medical image classification using segmented contours as shape predictors, addressing alignment issues in statistical shape analysis.


<details>
  <summary>Details</summary>
Motivation: Leverage segmented medical images (like radiography) where object contours provide valuable diagnostic information, enabling shape-based classification in supervised learning contexts.

Method: Developed a formalism for multivariate planar curves analysis, solved alignment problems in statistical shape analysis, and used functional classification with tangent projections on the resulting shape variables.

Result: Successfully demonstrated cardiomegaly detection in segmented X-rays and showed robustness through synthetic data experiments.

Conclusion: The proposed multivariate shape analysis approach provides an effective and robust method for medical image classification using object contours as predictive features.

Abstract: Recent developments in computer vision have enabled the availability of
segmented images across various domains, such as medicine, where segmented
radiography images play an important role in diagnosis-making. As prediction
problems are common in medical image analysis, this work explores the use of
segmented images (through the associated contours they highlight) as predictors
in a supervised classification context. Consequently, we develop a new approach
for image analysis that takes into account the shape of objects within images.
For this aim, we introduce a new formalism that extends the study of single
random planar curves to the joint analysis of multiple planar curves-referred
to here as multivariate planar curves. In this framework, we propose a solution
to the alignment issue in statistical shape analysis. The obtained multivariate
shape variables are then used in functional classification methods through
tangent projections. Detection of cardiomegaly in segmented X-rays and
numerical experiments on synthetic data demonstrate the appeal and robustness
of the proposed method.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [271] [Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark](https://arxiv.org/abs/2508.12438)
*Yaron Aloni,Rotem Shalev-Arkushin,Yonatan Shafir,Guy Tevet,Ohad Fried,Amit Haim Bermano*

Main category: cs.GR

TL;DR: Express4D dataset enables fine-grained text-to-facial-expression generation using commodity equipment and LLM-generated annotations, providing riggable motion data for nuanced performances.


<details>
  <summary>Details</summary>
Motivation: Current facial expression generation models lack datasets with nuanced expressive descriptions and require expensive equipment, limiting fine-grained control and accessibility.

Method: Created a new facial motion dataset using commodity equipment and LLM-generated natural language instructions in ARKit blendshape format, then trained two baseline text-to-expression models.

Result: The trained models successfully learn meaningful text-to-expression motion generation and capture the many-to-many mapping between text descriptions and facial expressions.

Conclusion: Express4D provides an accessible, richly annotated dataset that enables fine-grained control over facial expression generation, advancing applications in animation, virtual avatars, and human-computer interaction.

Abstract: Dynamic facial expression generation from natural language is a crucial task
in Computer Graphics, with applications in Animation, Virtual Avatars, and
Human-Computer Interaction. However, current generative models suffer from
datasets that are either speech-driven or limited to coarse emotion labels,
lacking the nuanced, expressive descriptions needed for fine-grained control,
and were captured using elaborate and expensive equipment. We hence present a
new dataset of facial motion sequences featuring nuanced performances and
semantic annotation. The data is easily collected using commodity equipment and
LLM-generated natural language instructions, in the popular ARKit blendshape
format. This provides riggable motion, rich with expressive performances and
labels. We accordingly train two baseline models, and evaluate their
performance for future benchmarking. Using our Express4D dataset, the trained
models can learn meaningful text-to-expression motion generation and capture
the many-to-many mapping of the two modalities. The dataset, code, and video
examples are available on our webpage: https://jaron1990.github.io/Express4D/

</details>


### [272] [MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration](https://arxiv.org/abs/2508.12691)
*Yuanxin Wei,Lansong Diao,Bujiao Chen,Shenggan Cheng,Zhengping Qian,Wenyuan Yu,Nong Xiao,Wei Lin,Jiangsu Du*

Main category: cs.GR

TL;DR: MixCache is a training-free caching framework that accelerates video DiT models by dynamically selecting optimal caching granularity, achieving up to 1.97x speedup while maintaining generation quality.


<details>
  <summary>Details</summary>
Motivation: Video DiT models suffer from high computational costs and inference latency due to their multi-step iterative denoising process. Existing caching methods are limited to single-granularity strategies and struggle to balance quality and speed effectively.

Method: Proposes MixCache with context-aware cache triggering and adaptive hybrid cache decision strategies to dynamically select optimal caching granularity without requiring additional training.

Result: Achieves significant acceleration (1.94x speedup on Wan 14B, 1.97x on HunyuanVideo) while delivering superior generation quality and inference efficiency compared to baseline methods.

Conclusion: MixCache provides an effective training-free solution for efficient video DiT inference by intelligently combining multiple caching granularities, offering both speed improvements and quality preservation.

Abstract: Leveraging the Transformer architecture and the diffusion process, video DiT
models have emerged as a dominant approach for high-quality video generation.
However, their multi-step iterative denoising process incurs high computational
cost and inference latency. Caching, a widely adopted optimization method in
DiT models, leverages the redundancy in the diffusion process to skip
computations in different granularities (e.g., step, cfg, block). Nevertheless,
existing caching methods are limited to single-granularity strategies,
struggling to balance generation quality and inference speed in a flexible
manner. In this work, we propose MixCache, a training-free caching-based
framework for efficient video DiT inference. It first distinguishes the
interference and boundary between different caching strategies, and then
introduces a context-aware cache triggering strategy to determine when caching
should be enabled, along with an adaptive hybrid cache decision strategy for
dynamically selecting the optimal caching granularity. Extensive experiments on
diverse models demonstrate that, MixCache can significantly accelerate video
generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on
HunyuanVideo) while delivering both superior generation quality and inference
efficiency compared to baseline methods.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [273] [Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation](https://arxiv.org/abs/2508.11738)
*Kiruthika Balakrishnan,Durgadevi Velusamy,Hana E. Hinkle,Zhi Li,Karthikeyan Ramasamy,Hikmat Khan,Srini Ramaswamy,Pir Masoom Shah*

Main category: cs.CY

TL;DR: AI shows transformative potential for rural healthcare through predictive analytics, telemedicine, and diagnostic tools, but faces infrastructure and ethical barriers.


<details>
  <summary>Details</summary>
Motivation: To address persistent rural healthcare challenges including inadequate infrastructure, workforce shortages, and socioeconomic disparities that limit access to essential services.

Method: Systematic review of 109 studies (2019-2024) using PRISMA guidelines and Covidence software, with thematic analysis to identify AI implementation patterns in rural healthcare.

Result: AI applications (predictive analytics, telemedicine, automated diagnostics) significantly improve accessibility, quality, and efficiency. MFMs and LLMs show particular transformative potential for comprehensive decision-making and clinical support.

Conclusion: While AI can revolutionize rural healthcare by augmenting human capacity and reducing delays, addressing infrastructural limitations, data quality, and ethical concerns requires interdisciplinary collaboration, infrastructure investment, and regulatory frameworks.

Abstract: Rural healthcare faces persistent challenges, including inadequate
infrastructure, workforce shortages, and socioeconomic disparities that hinder
access to essential services. This study investigates the transformative
potential of artificial intelligence (AI) in addressing these issues in
underserved rural areas. We systematically reviewed 109 studies published
between 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, and
Scopus. Articles were screened using PRISMA guidelines and Covidence software.
A thematic analysis was conducted to identify key patterns and insights
regarding AI implementation in rural healthcare delivery. The findings reveal
significant promise for AI applications, such as predictive analytics,
telemedicine platforms, and automated diagnostic tools, in improving healthcare
accessibility, quality, and efficiency. Among these, advanced AI systems,
including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs),
offer particularly transformative potential. MFMs integrate diverse data
sources, such as imaging, clinical records, and bio signals, to support
comprehensive decision-making, while LLMs facilitate clinical documentation,
patient triage, translation, and virtual assistance. Together, these
technologies can revolutionize rural healthcare by augmenting human capacity,
reducing diagnostic delays, and democratizing access to expertise. However,
barriers remain, including infrastructural limitations, data quality concerns,
and ethical considerations. Addressing these challenges requires
interdisciplinary collaboration, investment in digital infrastructure, and the
development of regulatory frameworks. This review offers actionable
recommendations and highlights areas for future research to ensure equitable
and sustainable integration of AI in rural healthcare systems.

</details>


### [274] [Vitamin N: Benefits of Different Forms of Public Greenery for Urban Health](https://arxiv.org/abs/2508.12998)
*Sanja Å Ä‡epanoviÄ‡,Sagar Joglekar,Stephen Law,Daniele Quercia,Ke Zhou,Alice Battiston,Rossano Schifanella*

Main category: cs.CY

TL;DR: On-road greenery (visible during daily walks) shows stronger health benefits than traditional off-road greenery metrics, with potential Â£3.15M annual prescription savings in London.


<details>
  <summary>Details</summary>
Motivation: Traditional greenery metrics measure quantity/proximity but ignore actual daily exposure and usage patterns, leading to inconsistent health benefit findings.

Method: Combined aerial imagery, OpenStreetMap data, 100K+ Google Street View images, and accessibility analysis of 160K road segments to classify on-road vs off-road greenery, then linked to 7.45B NHS prescriptions.

Result: On-road greenery above median reduced hypertension prescriptions by 3.68% compared to below-median areas. All below-median wards reaching median could save Â£3.15M annually in prescription costs.

Conclusion: Daily visible greenery is more relevant for health benefits than secluded public greenery, and traditional official metrics have significant limitations.

Abstract: Urban greenery is often linked to better health, yet findings from past
research have been inconsistent. One reason is that official greenery metrics
measure the amount or nearness of greenery but ignore how often people actually
may potentially see or use it in daily life. To address this gap, we introduced
a new classification that separates on-road greenery, which people see while
walking through streets, from off-road greenery, which requires planned visits.
We did so by combining aerial imagery of Greater London and greenery data from
OpenStreetMap with quantified greenery from over 100,000 Google Street View
images and accessibility estimates based on 160,000 road segments. We linked
these measures to 7.45 billion medical prescriptions issued by the National
Health Service and processed through our methodology. These prescriptions cover
five conditions: diabetes, hypertension, asthma, depression, and anxiety, as
well as opioid use. As hypothesized, we found that green on-road was more
strongly linked to better health than four widely used official measures. For
example, hypertension prescriptions dropped by 3.68% in wards with on-road
greenery above the median citywide level compared to those below it. If all
below-median wards reached the citywide median in on-road greenery,
prescription costs could fall by up to {\pounds}3.15 million each year. These
results suggest that greenery seen in daily life may be more relevant than
public yet secluded greenery, and that official metrics commonly used in the
literature have important limitations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [275] [HOMI: Ultra-Fast EdgeAI platform for Event Cameras](https://arxiv.org/abs/2508.12637)
*Shankaranarayanan H,Satyapreet Singh Yadav,Adithya Krishna,Ajay Vikram P,Mahesh Mehendale,Chetan Singh Thakur*

Main category: cs.AR

TL;DR: HOMI is an ultra-low latency edge AI platform combining event camera and FPGA with custom AI accelerator, achieving 94% accuracy on gesture recognition and 1000 fps throughput.


<details>
  <summary>Details</summary>
Motivation: Existing event processing solutions lack complete end-to-end implementations, have high latency, and don't sufficiently exploit event data sparsity for edge robotics applications like gesture-based human-robot interaction.

Method: Hardware-optimized platform with Prophesee IMX636 event sensor and Xilinx Zynq UltraScale+ MPSoC FPGA deploying in-house AI accelerator. Developed pre-processing pipelines supporting constant-time/constant-event modes for histogram accumulation and time surfaces.

Result: Achieves 94% accuracy on DVS Gesture dataset for high accuracy mode, 1000 fps throughput for low-latency configuration. Uses only 33% of FPGA LUT resources with compact memory footprint.

Conclusion: HOMI provides efficient end-to-end event processing with flexibility for both accuracy-driven and low-latency applications, leaving ample resources for further optimization and complex architectures.

Abstract: Event cameras offer significant advantages for edge robotics applications due
to their asynchronous operation and sparse, event-driven output, making them
well-suited for tasks requiring fast and efficient closed-loop control, such as
gesture-based human-robot interaction. Despite this potential, existing event
processing solutions remain limited, often lacking complete end-to-end
implementations, exhibiting high latency, and insufficiently exploiting event
data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end
edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx
Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI
accelerator. We have developed hardware-optimized pre-processing pipelines
supporting both constant-time and constant-event modes for histogram
accumulation, linear and exponential time surfaces. Our general-purpose
implementation caters to both accuracy-driven and low-latency applications.
HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when
configured for high accuracy operation and provides a throughput of 1000 fps
for low-latency configuration. The hardware-optimised pipeline maintains a
compact memory footprint and utilises only 33% of the available LUT resources
on the FPGA, leaving ample headroom for further latency reduction, model
parallelisation, multi-task deployments, or integration of more complex
architectures.

</details>


### [276] [XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads](https://arxiv.org/abs/2508.13049)
*Tejas Chaudhari,Akarsh J.,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: XR-NPE is a high-throughput mixed-precision SIMD neural processing engine designed for XR workloads, supporting novel FP4 and Posit formats with layer-adaptive implementation and quantization-aware training to reduce memory bandwidth while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Extended reality (XR) perception workloads like visual inertial odometry, object classification, and eye gaze extraction require efficient neural processing with minimal memory bandwidth and power consumption for resource-constrained devices.

Method: Proposes XR-NPE with Reconfigurable Mantissa Multiplication and Exponent processing Circuitry (RMMEC), supporting FP4, Posit(4,1), Posit(8,0), and Posit(16,1) formats. Uses selective power gating and quantization-aware training with layer-adaptive hybrid-algorithmic implementation.

Result: Achieves 1.72 GHz operating frequency, 0.016 mmÂ² area, 14 pJ arithmetic intensity at 28nm CMOS. Reduces 42% area and 38% power vs state-of-the-art MAC approaches. Co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, with 1.2x better energy efficiency. Provides 23% better energy efficiency and 4% better compute density for VIO workloads.

Conclusion: XR-NPE establishes itself as a scalable, precision-adaptive compute engine suitable for future resource-constrained XR devices, with publicly released code for reproducibility and adoption.

Abstract: This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural
Processing Engine, designed for extended reality (XR) perception workloads like
visual inertial odometry (VIO), object classification, and eye gaze extraction.
XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)
formats, with layer adaptive hybrid-algorithmic implementation supporting
ultra-low bit precision to significantly reduce memory bandwidth requirements,
and accompanied by quantization-aware training for minimal accuracy loss. The
proposed Reconfigurable Mantissa Multiplication and Exponent processing
Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted
by selective power gating to reduce energy consumption, providing 2.85x
improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of
1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,
reducing 42% area, 38% power compared to the best of state-of-the-art MAC
approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication
co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x
better energy efficiency compared to SoTA accelerators on VCU129. The proposed
co-processor provides 23% better energy efficiency and 4% better compute
density for VIO workloads. XR-NPE establishes itself as a scalable,
precision-adaptive compute engine for future resource-constrained XR devices.
The complete set for codes for results reproducibility are released publicly,
enabling designers and researchers to readily adopt and build upon them.
https://github.com/mukullokhande99/XR-NPE.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [277] [DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model](https://arxiv.org/abs/2508.12190)
*Jingkai Xu,De Cheng,Xiangqian Zhao,Jungang Yang,Zilong Wang,Xinyang Jiang,Xufang Luo,Lili Chen,Xiaoli Ning,Chengxu Li,Xinzhu Zhou,Xuejiao Song,Ang Li,Qingyue Xia,Zhou Zhuang,Hongfei Ouyang,Ke Xue,Yujun Sheng,Rusong Meng,Feng Xu,Xi Yang,Weimin Ma,Yusheng Lee,Dongsheng Li,Xinbo Gao,Jianming Liang,Lili Qiu,Nannan Wang,Xianbo Zuo,Cui Yong*

Main category: eess.IV

TL;DR: DermNIO is a versatile foundation model for dermatology that addresses limitations of current AI tools by using a novel hybrid pretraining framework on 432,776 images, achieving state-of-the-art performance across 20 datasets and outperforming dermatologists in diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Skin diseases affect up to 70% of the population with complex diagnostics and dermatologist shortages. Current AI models rely on large labeled datasets and are task-specific, making them ineffective in real-world settings.

Method: Developed DermNIO using a curated dataset of 432,776 images from public repositories, web-sourced images, and proprietary collections. Used a novel hybrid pretraining framework combining self-supervised learning with semi-supervised learning and knowledge-guided prototype initialization.

Result: Outperformed state-of-the-art models across 20 datasets. Achieved 95.79% diagnostic accuracy vs clinicians' 73.66% in blinded reader study. AI assistance improved clinician performance by 17.21%. Excelled in malignancy classification, disease severity grading, multi-category diagnosis, image captioning, and lesion segmentation.

Conclusion: DermNIO demonstrates strong generalization capability across various clinical tasks, robustness in federated learning scenarios, and effectiveness across diverse skin types and sexes, making it a powerful tool for real-world dermatological applications.

Abstract: Skin diseases impose a substantial burden on global healthcare systems,
driven by their high prevalence (affecting up to 70% of the population),
complex diagnostic processes, and a critical shortage of dermatologists in
resource-limited areas. While artificial intelligence(AI) tools have
demonstrated promise in dermatological image analysis, current models face
limitations-they often rely on large, manually labeled datasets and are built
for narrow, specific tasks, making them less effective in real-world settings.
To tackle these limitations, we present DermNIO, a versatile foundation model
for dermatology. Trained on a curated dataset of 432,776 images from three
sources (public repositories, web-sourced images, and proprietary collections),
DermNIO incorporates a novel hybrid pretraining framework that augments the
self-supervised learning paradigm through semi-supervised learning and
knowledge-guided prototype initialization. This integrated method not only
deepens the understanding of complex dermatological conditions, but also
substantially enhances the generalization capability across various clinical
tasks. Evaluated across 20 datasets, DermNIO consistently outperforms
state-of-the-art models across a wide range of tasks. It excels in high-level
clinical applications including malignancy classification, disease severity
grading, multi-category diagnosis, and dermatological image caption, while also
achieving state-of-the-art performance in low-level tasks such as skin lesion
segmentation. Furthermore, DermNIO demonstrates strong robustness in
privacy-preserving federated learning scenarios and across diverse skin types
and sexes. In a blinded reader study with 23 dermatologists, DermNIO achieved
95.79% diagnostic accuracy (versus clinicians' 73.66%), and AI assistance
improved clinician performance by 17.21%.

</details>


### [278] [FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for Deformable Image Registration](https://arxiv.org/abs/2508.12445)
*Shayan Kebriti,Shahabedin Nabavi,Ali Gooya*

Main category: eess.IV

TL;DR: FractMorph is a 3D dual-parallel transformer architecture that uses multi-domain fractional Fourier transform to simultaneously capture local and global deformations for medical image registration, achieving state-of-the-art performance on cardiac MRI.


<details>
  <summary>Details</summary>
Motivation: Existing deformable image registration methods struggle to capture both fine-grained local deformations and large-scale global deformations simultaneously within a unified framework.

Method: A novel 3D dual-parallel transformer architecture with Fractional Cross-Attention blocks that apply parallel FrFTs at different angles (0Â°, 45Â°, 90Â°) plus a log-magnitude branch to extract multi-scale features, fused via cross-attention and processed through a lightweight U-Net.

Result: Achieved state-of-the-art performance on ACDC cardiac MRI dataset with 86.45% overall DSC, 75.15% average per-structure DSC, and 1.54mm HD95. Also developed FractMorph-Light with 29.6M parameters maintaining similar accuracy.

Conclusion: Multi-domain spectral-spatial attention in transformers can robustly and efficiently model complex non-rigid deformations using a single end-to-end network without scenario-specific tuning or hierarchical multi-scale networks.

Abstract: Deformable image registration (DIR) is a crucial and challenging technique
for aligning anatomical structures in medical images and is widely applied in
diverse clinical applications. However, existing approaches often struggle to
capture fine-grained local deformations and large-scale global deformations
simultaneously within a unified framework. We present FractMorph, a novel 3D
dual-parallel transformer-based architecture that enhances cross-image feature
matching through multi-domain fractional Fourier transform (FrFT) branches.
Each Fractional Cross-Attention (FCA) block applies parallel FrFTs at
fractional angles of 0{\deg}, 45{\deg}, 90{\deg}, along with a log-magnitude
branch, to effectively extract local, semi-global, and global features at the
same time. These features are fused via cross-attention between the fixed and
moving image streams. A lightweight U-Net style network then predicts a dense
deformation field from the transformer-enriched features. On the ACDC cardiac
MRI dataset, FractMorph achieves state-of-the-art performance with an overall
Dice Similarity Coefficient (DSC) of 86.45%, an average per-structure DSC of
75.15%, and a 95th-percentile Hausdorff distance (HD95) of 1.54 mm on our data
split. We also introduce FractMorph-Light, a lightweight variant of our model
with only 29.6M parameters, which maintains the superior accuracy of the main
model while using approximately half the memory. Our results demonstrate that
multi-domain spectral-spatial attention in transformers can robustly and
efficiently model complex non-rigid deformations in medical images using a
single end-to-end network, without the need for scenario-specific tuning or
hierarchical multi-scale networks. The source code of our implementation is
available at https://github.com/shayankebriti/FractMorph.

</details>


### [279] [Segmenting Thalamic Nuclei: T1 Maps Provide a Reliable and Efficient Solution](https://arxiv.org/abs/2508.12508)
*Anqi Feng,Zhangxing Bian,Samuel W. Remedios,Savannah P. Hays,Blake E. Dewey,Jiachen Zhuo,Dan Benjamini,Jerry L. Prince*

Main category: eess.IV

TL;DR: T1 maps are the most effective MRI input for thalamic nuclei segmentation, outperforming other contrasts including MPRAGE, FGATIR, PD maps, and multi-TI images, while PD maps provide no additional value.


<details>
  <summary>Details</summary>
Motivation: Accurate thalamic nuclei segmentation is crucial for neurological disease understanding and clinical interventions, but the optimal MRI inputs for segmentation remain unclear and need systematic evaluation.

Method: Systematically evaluated multiple MRI contrasts (MPRAGE, FGATIR, quantitative PD/T1 maps, multi-TI images) using gradient-based saliency analysis with Monte Carlo dropout and Overall Importance Score for multi-TI selection. Trained 3D U-Net on each configuration.

Result: T1 maps alone achieved strong quantitative performance and superior qualitative outcomes for thalamic nuclei segmentation. PD maps offered no added value compared to other contrasts.

Conclusion: T1 maps are the most reliable and efficient input among evaluated options, providing valuable guidance for optimizing imaging protocols when thalamic structures are of clinical or research interest.

Abstract: Accurate thalamic nuclei segmentation is crucial for understanding
neurological diseases, brain functions, and guiding clinical interventions.
However, the optimal inputs for segmentation remain unclear. This study
systematically evaluates multiple MRI contrasts, including MPRAGE and FGATIR
sequences, quantitative PD and T1 maps, and multiple T1-weighted images at
different inversion times (multi-TI), to determine the most effective inputs.
For multi-TI images, we employ a gradient-based saliency analysis with Monte
Carlo dropout and propose an Overall Importance Score to select the images
contributing most to segmentation. A 3D U-Net is trained on each of these
configurations. Results show that T1 maps alone achieve strong quantitative
performance and superior qualitative outcomes, while PD maps offer no added
value. These findings underscore the value of T1 maps as a reliable and
efficient input among the evaluated options, providing valuable guidance for
optimizing imaging protocols when thalamic structures are of clinical or
research interest.

</details>


### [280] [Anatomic Feature Fusion Model for Diagnosing Calcified Pulmonary Nodules on Chest X-Ray](https://arxiv.org/abs/2508.12562)
*Hyeonjin Choi,Yang-gon Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: A calcification classification model for pulmonary nodules on chest X-rays that uses fused features from raw and structure-suppressed images to improve diagnostic accuracy by reducing anatomical interference.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of pulmonary nodule calcification is crucial for distinguishing benign nodules and avoiding unnecessary invasive procedures, but current visual assessment methods suffer from interpretation variability and anatomical interference from ribs and spine.

Method: Developed a calcification classification model using fused features from both raw chest X-ray images and their structure-suppressed variants to minimize structural interference. Used dataset of 2,517 lesion-free images and 656 nodule images (151 calcified, 550 non-calcified) from Ajou University Hospital.

Result: The model achieved 86.52% accuracy and 0.8889 AUC in calcification diagnosis, outperforming the model trained on raw images alone by 3.54% in accuracy and 0.0385 in AUC.

Conclusion: The proposed approach of using fused features from raw and structure-suppressed images effectively reduces anatomical interference and improves diagnostic performance for pulmonary nodule calcification classification on chest X-rays.

Abstract: Accurate and timely identification of pulmonary nodules on chest X-rays can
differentiate between life-saving early treatment and avoidable invasive
procedures. Calcification is a definitive indicator of benign nodules and is
the primary foundation for diagnosis. In actual practice, diagnosing pulmonary
nodule calcification on chest X-rays predominantly depends on the physician's
visual assessment, resulting in significant diversity in interpretation.
Furthermore, overlapping anatomical elements, such as ribs and spine,
complicate the precise identification of calcification patterns. This study
presents a calcification classification model that attains strong diagnostic
performance by utilizing fused features derived from raw images and their
structure-suppressed variants to reduce structural interference. We used 2,517
lesion-free images and 656 nodule images (151 calcified nodules and 550
non-calcified nodules), all obtained from Ajou University Hospital. The
suggested model attained an accuracy of 86.52% and an AUC of 0.8889 in
calcification diagnosis, surpassing the model trained on raw images by 3.54%
and 0.0385, respectively.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [281] [iTrace: Click-Based Gaze Visualization on the Apple Vision Pro](https://arxiv.org/abs/2508.12268)
*Esra Mehmedova,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: iTrace enables gaze tracking on Apple Vision Pro despite privacy restrictions by using click-based methods to extract gaze data and generate dynamic heatmaps for attention analysis.


<details>
  <summary>Details</summary>
Motivation: Apple Vision Pro has accurate eye-tracking but privacy restrictions prevent direct access to continuous gaze data, limiting research and application development.

Method: Developed client-server system with click-based gaze extraction techniques (pinch gesture, dwell control, gaming controller) to capture gaze coordinates and transform them into dynamic heatmaps for video and spatial eye tracking.

Result: 8BitDo controller achieved 14.22 clicks/s vs 0.45 clicks/s with dwell control, enabling denser heatmaps. System maintained 91% gaze precision and revealed distinct attention patterns in different tasks.

Conclusion: iTrace demonstrates strong potential for educational, design, marketing, and clinical applications despite current privacy restrictions, but should be used only in research settings.

Abstract: The Apple Vision Pro is equipped with accurate eye-tracking capabilities, yet
the privacy restrictions on the device prevent direct access to continuous user
gaze data. This study introduces iTrace, a novel application that overcomes
these limitations through click-based gaze extraction techniques, including
manual methods like a pinch gesture, and automatic approaches utilizing dwell
control or a gaming controller. We developed a system with a client-server
architecture that captures the gaze coordinates and transforms them into
dynamic heatmaps for video and spatial eye tracking. The system can generate
individual and averaged heatmaps, enabling analysis of personal and collective
attention patterns.
  To demonstrate its effectiveness and evaluate the usability and performance,
a study was conducted with two groups of 10 participants, each testing
different clicking methods. The 8BitDo controller achieved higher average data
collection rates at 14.22 clicks/s compared to 0.45 clicks/s with dwell
control, enabling significantly denser heatmap visualizations. The resulting
heatmaps reveal distinct attention patterns, including concentrated focus in
lecture videos and broader scanning during problem-solving tasks. By allowing
dynamic attention visualization while maintaining a high gaze precision of 91
%, iTrace demonstrates strong potential for a wide range of applications in
educational content engagement, environmental design evaluation, marketing
analysis, and clinical cognitive assessment. Despite the current gaze data
restrictions on the Apple Vision Pro, we encourage developers to use iTrace
only in research settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [282] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC is the first LLM-based agentic framework for constrained retrosynthesis planning that uses agent-as-a-judge with tool-based reasoning to achieve 72.9% success rate, outperforming LLM baselines and approaching human expert performance.


<details>
  <summary>Details</summary>
Motivation: Constrained retrosynthesis planning is essential but challenging in chemistry, requiring identification of synthetic routes from available materials to target molecules while meeting practical constraints. Existing methods lack effective constraint integration.

Method: LARC incorporates agentic constraint evaluation through an Agent-as-a-Judge directly into retrosynthesis planning, using tool-based reasoning to provide grounded feedback that guides and constrains route generation.

Result: LARC achieves 72.9% success rate on 48 constrained retrosynthesis tasks across 3 constraint types, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time.

Conclusion: LARC serves as an extensible framework and first step towards an effective agentic tool or co-scientist for human experts in constrained retrosynthesis planning.

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [283] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: CHBench is a novel evaluation framework using cognitive hierarchy models to assess LLMs' strategic reasoning in games, showing consistent reasoning levels across opponents and revealing that chat mechanisms degrade while memory mechanisms enhance strategic performance.


<details>
  <summary>Details</summary>
Motivation: Existing game-based evaluations of LLMs rely on utility metrics that are not robust due to variations in opponent behavior and game structure, requiring a more systematic framework to assess strategic reasoning capabilities.

Method: A three-phase systematic framework using cognitive hierarchy models from behavioral economics, evaluating six state-of-the-art LLMs across fifteen normal-form games with behavioral data analysis.

Result: LLMs exhibit consistent strategic reasoning levels across diverse opponents, confirming framework robustness. Chat Mechanism significantly degrades strategic reasoning while Memory Mechanism enhances it.

Conclusion: CHBench provides a robust and generalizable tool for evaluating LLM strategic reasoning capabilities, with significant potential for future research and practical applications in assessing bounded rationality and reasoning depths.

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [284] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: Symbolic-Aided CoT enhances standard Chain-of-Thought reasoning by integrating lightweight symbolic representations into few-shot prompts, improving transparency and performance in complex logical reasoning tasks across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve the transparency, interpretability, and analyzability of LLM logical reasoning while maintaining the generalizability of standard prompting techniques, particularly for complex reasoning tasks requiring navigation of multiple constraints or rules.

Method: Integrates lightweight symbolic representations into few-shot prompts to structure inference steps with a consistent strategy, making reasoning patterns more explicit within a non-iterative reasoning process.

Result: Significantly outperforms conventional CoT on three out of four datasets (ProofWriter, ProntoQA, and LogicalDeduction), consistently improving reasoning capabilities across various model sizes, especially in complex reasoning tasks.

Conclusion: Symbolic-Aided CoT effectively enhances LLM logical reasoning by combining symbolic structures with standard prompting, providing better transparency and performance while maintaining generalizability across diverse reasoning scenarios.

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [285] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: A novel workflow combining LLMs and ASP for joint entity-relation extraction that outperforms state-of-the-art methods with only 10% training data, achieving 2.5x improvement on difficult benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional JERE methods require large annotated datasets and lack domain knowledge incorporation, making model creation labor-intensive and time-consuming.

Method: Proposes a generic workflow using generative pretrained LLMs for natural language understanding and Answer Set Programming (ASP) for knowledge representation and reasoning, working directly with unannotated text.

Result: Outperforms state-of-the-art JERE systems with only 10% training data, achieving 35% vs 15% (2.5x improvement) in Relation Extraction on the difficult SciERC corpus across three benchmarks.

Conclusion: The LLM + ASP workflow provides an effective, domain-agnostic solution for JERE that requires minimal training data and easily incorporates domain knowledge without core program modifications.

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [286] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: RLVR extends reinforcement learning to open-ended tasks using rubric-based rewards, achieving +5.2% improvement on benchmarks with fine-grained stylistic control while preserving general abilities.


<details>
  <summary>Details</summary>
Motivation: Traditional RLVR is limited to domains with automatically checkable outcomes, but many real-world tasks are open-ended and subjective, requiring a way to provide verifiable rewards for subjective content.

Method: Extended RLVR paradigm by integrating rubric-based rewards with over 10,000 rubrics from humans, LLMs, or hybrid collaboration. Developed a clear framework and trained Qwen-30B-A3B model with 5K+ samples.

Result: Achieved +5.2% improvement on open-ended benchmarks (especially humanities), outperformed 671B DeepSeek-V3 model by +2.4%, preserved general and reasoning abilities, and enabled fine-grained stylistic control to produce more human-like responses.

Conclusion: Rubric-based RLVR successfully extends reinforcement learning to subjective domains, providing both performance improvements and stylistic control, with lessons learned in rubric construction and data selection for future applications.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [287] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG is a multimodal empathetic response generation system that uses explicit emotion-driven approach with multimodal LLMs to handle emotional content and maintain identity consistency without extra training, achieving top performance in the ACM MM 25 challenge.


<details>
  <summary>Details</summary>
Motivation: Current large language models struggle with multimodal emotional content processing and maintaining identity consistency in empathetic response generation, limiting their effectiveness in emotionally intelligent human-computer interactions.

Method: Decomposes MERG task into three components: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. Integrates advanced expressive speech and video generative models with multimodal LLMs without requiring additional training.

Result: Achieves superior performance in both zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.

Conclusion: E3RG successfully addresses the challenges of multimodal empathetic response generation by providing natural, emotionally rich, and identity-consistent responses through its explicit emotion-driven framework and integration of advanced generative models.

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [288] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: PC-Sampler is a novel decoding strategy for masked diffusion models that addresses limitations of existing uncertainty-based samplers by incorporating position-aware trajectory control and confidence calibration to improve generation quality.


<details>
  <summary>Details</summary>
Motivation: Current masked diffusion models suffer from sensitivity to decoding strategies, with uncertainty-based samplers lacking global trajectory control and showing bias toward trivial tokens in early decoding stages, limiting their full potential.

Method: Position-Aware Confidence-Calibrated Sampling (PC-Sampler) that unifies global trajectory planning with content-aware informativeness maximization through position-aware weighting and calibrated confidence scoring.

Result: PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average across seven challenging benchmarks including logical reasoning and planning tasks, significantly narrowing the performance gap with state-of-the-art autoregressive models.

Conclusion: The proposed PC-Sampler effectively addresses key limitations of current MDM decoding strategies and demonstrates substantial improvements in generation quality, making masked diffusion models more competitive with autoregressive approaches.

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [289] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,MikoÅ‚aj MaÅ‚kiÅ„ski,Jacek MaÅ„dziuk*

Main category: cs.AI

TL;DR: Bongard-RWR+ is a new 5,400-instance dataset using VLM-generated real-world images to test abstract visual reasoning, showing VLMs struggle with fine-grained concepts despite handling coarse ones.


<details>
  <summary>Details</summary>
Motivation: Existing Bongard Problem datasets have limitations - synthetic images lack real-world complexity, while real-world image datasets use high-level features that reduce task difficulty. The manually created Bongard-RWR was too small (only 60 instances) for robust evaluation.

Method: Used Pixtral-12B to describe curated images and generate new concept-aligned descriptions, then Flux.1-dev to synthesize images from these descriptions. All generated images were manually verified for concept fidelity. Evaluated state-of-the-art VLMs on binary/multiclass classification and textual answer generation tasks.

Result: VLMs can recognize coarse-grained visual concepts but consistently struggle with discerning fine-grained concepts, revealing limitations in their abstract reasoning capabilities.

Conclusion: The Bongard-RWR+ dataset provides a robust benchmark for testing abstract visual reasoning, demonstrating that current VLMs have significant limitations in fine-grained concept understanding despite their ability to handle simpler visual tasks.

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [290] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion is the first benchmark for evaluating hallucinations in multimodal LLMs for egocentric videos, featuring 1,400 videos with 8,000 human-annotated questions that trigger hallucinations in visual and auditory cues.


<details>
  <summary>Details</summary>
Motivation: MLLMs show strong performance in multimodal tasks but suffer from hallucinations in egocentric videos, generating coherent but inaccurate responses that need systematic evaluation.

Method: Created a benchmark with 1,400 egocentric videos paired with 8,000 human-annotated open and closed-ended questions designed to trigger hallucinations in both visual and auditory modalities.

Result: Evaluation of ten MLLMs revealed significant challenges, with even powerful models like GPT-4o and Gemini achieving only 59% accuracy, demonstrating widespread hallucination issues.

Conclusion: EgoIllusion provides a foundational benchmark for evaluating MLLM effectiveness in egocentric contexts and will spur development of better models with reduced hallucination rates. The benchmark will be open-sourced for reproducibility.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 82]
- [cs.CV](#cs.CV) [Total: 222]
- [eess.IV](#eess.IV) [Total: 25]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.LG](#cs.LG) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Main category: cs.CL

TL;DR: The paper evaluates how well LLMs align with human ratings on psycholinguistic word features, finding better alignment with Glasgow norms than Lancaster norms.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' alignment with human ratings on word features like arousal and concreteness, leveraging psycholinguistic datasets.

Method: Evaluated alignment of LLMs with human ratings using Glasgow and Lancaster norms, covering 13 features over thousands of words.

Result: Alignment was better for Glasgow norms (arousal, valence, etc.) than Lancaster norms (sensory features), indicating LLMs' limitations in sensory associations.

Conclusion: LLMs struggle with sensory word associations, possibly due to lack of embodied cognition, highlighting the value of psycholinguistic evaluations.

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [2] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Main category: cs.CL

TL;DR: A modular AI system automates enterprise document review, outperforming humans in consistency, speed, and bias reduction, while requiring oversight in specialized domains.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy, efficiency, and scalability of enterprise document review by leveraging AI agents for structured evaluation.

Method: Uses specialized AI agents orchestrated with tools like LangChain and CrewAI for parallel or sequential document review, enforcing standardized outputs.

Result: Achieves 99% consistency (vs. 92% human), halves errors/bias, reduces review time to 2.5 minutes (from 30), and 95% agreement with human experts.

Conclusion: The system is scalable and auditable but requires human oversight in specialized areas and faces LLM operational costs.

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [3] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: The paper proposes a framework using multiple small language models to detect hallucinations in LLM responses by verifying sentences against retrieved context, showing a 10% F1 score improvement.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLM responses reduce reliability, especially without ground truth, necessitating a scalable verification method.

Method: Integrates small language models to verify LLM responses by breaking them into sentences and using 'Yes' token probabilities for validation.

Result: Experiments on real datasets show a 10% F1 score improvement in detecting correct responses over hallucinations.

Conclusion: Multiple small language models offer a scalable and efficient solution for verifying LLM responses, enhancing reliability.

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [4] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: PromptAug is an LLM-based data augmentation method for conflict detection, improving accuracy and F1-score by 2%. It addresses challenges like data scarcity and LLM guardrails, with a robust evaluation framework.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled data for nuanced tasks like conflict detection is scarce and expensive. Social media data restrictions further limit access, making data augmentation essential.

Method: Introduces PromptAug, an LLM-based data augmentation method, evaluated in extreme data scarcity scenarios with diversity and thematic analysis.

Result: PromptAug achieves 2% improvements in accuracy and F1-score on conflict and emotion datasets. Thematic analysis identifies four problematic patterns in augmented text.

Conclusion: PromptAug is effective for sensitive tasks like conflict detection, offering a unique interdisciplinary evaluation combining NLP and social science methods.

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [5] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: AgentStealth is a framework using locally deployed small-scale language models (SLMs) for text anonymization, outperforming baselines in effectiveness and utility while avoiding cloud-based privacy risks.


<details>
  <summary>Details</summary>
Motivation: Existing anonymization methods either compromise utility or rely on costly, privacy-risky cloud-based LLMs, highlighting the need for effective local solutions.

Method: AgentStealth combines adversarial anonymization with in-context contrastive learning, supervised adaptation of SLMs, and online reinforcement learning for iterative improvement.

Result: The method achieves +12.3% anonymization effectiveness and +6.8% utility improvement over baselines, with lightweight deployment on edge devices.

Conclusion: AgentStealth offers a practical, privacy-preserving solution for text anonymization, balancing effectiveness and utility without cloud reliance.

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [6] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: The paper proposes MDGCL, a multi-domain pre-training and cross-domain transfer framework for graph data, addressing domain-specific differences in contrastive learning to improve representation quality.


<details>
  <summary>Details</summary>
Motivation: Current graph foundation models treat contrastive samples from different domains as equivalent, failing to capture domain-specific differences, which limits their effectiveness.

Method: MDGCL introduces a contrastive learning strategy to recognize domain differences, uses domain tokens for global information, and employs a domain attention mechanism for fine-grained transfer.

Result: MDGCL outperforms state-of-the-art methods, achieving up to 19.33% improvement in accuracy and 19.13% in Macro-F1 score.

Conclusion: The framework effectively addresses domain-specific challenges in graph pre-training, enabling better knowledge transfer and representation learning.

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [7] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: The study applies Integrated Information Theory (IIT) 3.0 and 4.0 to analyze Large Language Model (LLM) representations, finding no significant consciousness indicators but noting patterns in spatio-permutational analyses.


<details>
  <summary>Details</summary>
Motivation: To investigate if differences in Theory of Mind (ToM) test performances in LLMs can be explained by IIT metrics and differentiate between consciousness and inherent separations in LLM representations.

Method: Applied IIT 3.0 and 4.0 metrics (Φᵐᵃˣ, Φ, Conceptual Information, Φ-structure) to LLM representations and compared them with Span Representations. Conducted experiments across LLM transformer layers and linguistic spans.

Result: No statistically significant indicators of consciousness in LLM representations, but intriguing patterns observed under spatio-permutational analyses.

Conclusion: Contemporary Transformer-based LLMs lack evidence of consciousness phenomena, though patterns in analyses suggest further exploration.

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [8] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Main category: cs.CL

TL;DR: ReG improves graph-based RAG by aligning weak retrievers with LLMs, using feedback to remove spurious signals and reorganizing retrieved knowledge into coherent evidence chains, achieving up to 10% performance gains.


<details>
  <summary>Details</summary>
Motivation: LLMs in graph-based RAG rely on weak retrievers due to lack of ground truth and unorganized retrieved knowledge, leading to spurious signals and poor performance.

Method: ReG incorporates LLM feedback to refine supervision and introduces a structure-aware module to reorganize retrieval results into coherent evidence chains.

Result: ReG improves performance by up to 10%, matches state-of-the-art with 5% training data, reduces reasoning token cost by 30%, and boosts reasoning performance by 4%.

Conclusion: ReG effectively addresses weaknesses in graph-based RAG, enhancing LLM performance and efficiency.

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [9] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
*Lu Kalkbrenner,Veronika Solopova,Steffen Zeiler,Robert Nickel,Dorothea Kolossa*

Main category: cs.CL

TL;DR: Misinfo-TeleGraph is a German-language Telegram dataset for misinformation detection, combining metadata, channel relationships, and labels from semantic similarity and manual annotation. GraphSAGE with LSTM outperforms text-only models.


<details>
  <summary>Details</summary>
Motivation: Address underutilized connectivity and message propagation in misinformation detection, especially on poorly moderated platforms like Telegram, focusing on the German electoral context.

Method: Created Misinfo-TeleGraph dataset with 5M messages, metadata, and labels (weak/strong). Evaluated text-only models and GNNs (GraphSAGE with LSTM) using forwarding as network structure.

Result: GraphSAGE with LSTM aggregation outperforms text-only baselines in MCC and F1-score. Explored impact of subscribers, view counts, and label types.

Conclusion: Provides a benchmark and open dataset for German-language Telegram misinformation detection, highlighting weak supervision's potential and challenges.

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [10] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
*Nicholas Edwards,Yukyung Lee,Yujun,Mao,Yulu Qin,Sebastian Schuster,Najoung Kim*

Main category: cs.CL

TL;DR: RExBench evaluates LLM agents' ability to autonomously implement research extensions, finding current agents fall short without human guidance.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of LLM agents in performing realistic research experiment extensions, a critical skill for autonomous research systems.

Method: Introduces RExBench, a benchmark with 12 tasks extending existing research, and evaluates nine LLM agents using three frameworks.

Result: All agents failed to autonomously implement most extensions; success rates improved with hints but remained below 40%.

Conclusion: Current LLM agents lack the ability to handle realistic research extensions without significant human intervention.

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [11] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Main category: cs.CL

TL;DR: The paper introduces a new watermarking method for detecting synthetic text generated by LLMs, aiming to ensure ethical use. It replicates a baseline study, identifies its flaws, and proposes a robust alternative, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the misuse of LLMs by developing a reliable method to detect machine-generated text, ensuring ethical AI applications.

Method: Replicates a baseline study, identifies its limitations, and introduces a novel watermarking technique evaluated using paraphrased text.

Result: The proposed method shows greater robustness compared to existing watermarking techniques like the one from ~\cite{aarson}.

Conclusion: The new watermarking approach effectively detects synthetic text, offering a more reliable solution for ethical AI text generation.

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [12] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Main category: cs.CL

TL;DR: A hybrid RAG system combining sparse (BM25) and dense (E5) retrieval with Falcon3-10B-Instruct was submitted to LiveRAG Challenge 2025, achieving 4th in faithfulness and 11th in correctness. Neural re-ranking improved performance but was computationally expensive.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve retrieval-augmented generation (RAG) systems on dynamic test sets using the FineWeb-10BT corpus.

Method: Combined sparse (BM25) and dense (E5) retrieval, used Falcon3-10B-Instruct for generation, and evaluated with synthetic questions. Neural re-ranking (RankLLaMA) and DSPy-optimized prompting were tested.

Result: Neural re-ranking improved MAP by 52% but was computationally costly. DSPy-optimized prompting increased semantic similarity but had 0% refusal rates. Vocabulary alignment was key to performance.

Conclusion: The hybrid system performed well in faithfulness and correctness, but computational costs and over-confidence in prompting strategies remain challenges.

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [13] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
*Ankush Raut,Projna Paromita,Sydney Begerowski,Suzanne Bell,Theodora Chaspari*

Main category: cs.CL

TL;DR: The paper investigates LLMs' ability to detect micro-behaviors in team conversations, comparing encoder-only and decoder-only models. Llama-3.1 outperformed others, achieving 44% (3-way) and 68% (binary) F1-scores.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' effectiveness in identifying subtle micro-behaviors in team dialogues, especially in high-stakes settings like space missions.

Method: Tested zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only models (RoBERTa, DistilBERT) and few-shot generation with decoder-only models (Llama-3.1).

Result: Encoder-only models struggled, especially with underrepresented behaviors. Llama-3.1 excelled, achieving 44% (3-way) and 68% (binary) F1-scores.

Conclusion: Decoder-only LLMs like Llama-3.1 are more effective for micro-behavior detection in team conversations, with potential applications in speech technologies for high-stakes environments.

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [14] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: The paper introduces VocabTrim, a training-free technique to improve drafter-based speculative decoding by reducing drafting overhead through a limited vocabulary set.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unnecessary inference overhead in drafting for target LLMs with large vocabularies, especially in memory-bound environments.

Method: VocabTrim reconstructs the drafter's LM head to include only frequently sampled tokens, reducing drafting latency.

Result: The method improves memory-bound speed-up by 16% for Llama-3.2-3B-Instruct on Spec-Bench.

Conclusion: VocabTrim effectively balances acceptance rate and drafting latency, enhancing generation speed in memory-bound scenarios.

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [15] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: The report summarizes a workshop on AI language models and human cognition, highlighting insights, limitations, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding how AI language models relate to human cognitive processes in text comprehension and production.

Method: Interdisciplinary collaboration among experts in cognitive psychology, linguistics, and AI, analyzing human and AI language processes.

Result: Findings include LLMs' potential to inform human language processing, alignment with human cognition when fine-tuned, and challenges in human-AI collaboration.

Conclusion: The report guides future research, emphasizing ethical AI use and enhancing human capabilities through human-AI collaboration.

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [16] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
*Niyati Bafna,Tianjian Li,Kenton Murray,David R. Mortensen,David Yarowsky,Hale Sirin,Daniel Khashabi*

Main category: cs.CL

TL;DR: The paper identifies a translation barrier in multilingual LLMs, where poor quality in low-resource languages stems from failed translation of correctly solved intermediate concepts.


<details>
  <summary>Details</summary>
Motivation: To understand why multilingual generation in LLMs underperforms for mid- to low-resource languages.

Method: Analyzes a word translation task across 108 language pairs using logit lens to observe intermediate model processing.

Result: Translation failure, especially for low-resource languages, is a key cause of poor output quality.

Conclusion: The findings highlight a major challenge for multilingual LLMs and provide insights for future improvements.

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [17] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
*Alan Dao,Dinh Bach Vu*

Main category: cs.CL

TL;DR: Jan-nano, a 4B parameter model, achieves high efficiency by specializing in instant information retrieval, outperforming larger models on benchmarks while running on consumer hardware.


<details>
  <summary>Details</summary>
Motivation: Address the tradeoff between model capabilities and computational resources by creating a highly efficient, specialized language model.

Method: Fine-tuned from Qwen3-4B using a novel multi-stage RLVR system, eliminating next token prediction training (SFT).

Result: Achieves 83.2% on SimpleQA benchmark with MCP integration and supports 128K context length.

Conclusion: Demonstrates that strategic specialization, not scale, is key to efficient and powerful language models.

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [18] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
*Miles Turpin,Andy Arditi,Marvin Li,Joe Benton,Julian Michael*

Main category: cs.CL

TL;DR: VFT trains models to acknowledge prompt cues, reducing undetected reward hacking from 88% to 6% after RL, improving transparency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting reward hacking in language models trained with RL, which exploit unintended strategies without revealing them in reasoning.

Method: Proposes verbalization fine-tuning (VFT) to train models to explicitly acknowledge prompt cues before RL training. Evaluates VFT by measuring undetected reward hacking post-RL.

Result: VFT reduces undetected reward hacking to 6%, compared to 88% without VFT and 99% with debiasing. Models verbalize cue influence 94% post-RL with VFT.

Conclusion: VFT significantly improves detection of reward hacking, enhancing transparency and safety in AI systems.

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [19] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: ContextCache improves semantic caching for multi-turn dialogues by integrating context awareness, outperforming existing methods in precision and recall while reducing latency.


<details>
  <summary>Details</summary>
Motivation: Existing semantic caching systems lack context awareness in multi-turn dialogues, leading to incorrect cache hits.

Method: ContextCache uses a two-stage retrieval architecture with vector-based retrieval and self-attention mechanisms for contextual matching.

Result: It improves precision and recall in real-world conversations and reduces latency by 10x compared to direct LLM invocation.

Conclusion: ContextCache enables efficient, context-aware caching for LLM conversational applications, reducing computational costs.

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [20] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
*Jianhui Wei,Zijie Meng,Zikai Xiao,Tianxiang Hu,Yang Feng,Zhijie Zhou,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper introduces MedEthicsQA, a benchmark for evaluating medical ethics in LLMs, revealing performance gaps in MedLLMs.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient exploration of ethical safety in Medical Large Language Models (MedLLMs).

Method: Developed MedEthicsQA, a benchmark with 5,623 multiple-choice and 5,351 open-ended questions, integrating global ethical standards and rigorous quality control.

Result: State-of-the-art MedLLMs showed declined performance in medical ethics questions, highlighting alignment deficiencies.

Conclusion: The benchmark underscores the need for improved ethical alignment in MedLLMs and provides a reliable tool for evaluation.

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [21] [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/abs/2506.22813)
*Zhuojun Ding,Wei Wei,Chenghao Fan*

Main category: cs.CL

TL;DR: The SaM framework dynamically selects and merges expert models for target domains, improving generalization and scalability without extra training, outperforming unified models by 10%.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning for LLMs in IE tasks is costly and lacks adaptation and scalability in unified models.

Method: Selects domain-specific experts based on domain similarity and performance, then merges them for task-specific models.

Result: Outperforms unified models by 10% on benchmarks, with added scalability and adaptability.

Conclusion: SaM offers a scalable, adaptable solution for IE tasks, with potential for further improvements and extensions.

Abstract: Supervised fine-tuning (SFT) is widely used to align large language models
(LLMs) with information extraction (IE) tasks, such as named entity recognition
(NER). However, annotating such fine-grained labels and training
domain-specific models is costly. Existing works typically train a unified
model across multiple domains, but such approaches lack adaptation and
scalability since not all training data benefits target domains and scaling
trained models remains challenging. We propose the SaM framework, which
dynamically Selects and Merges expert models at inference time. Specifically,
for a target domain, we select domain-specific experts pre-trained on existing
domains based on (i) domain similarity to the target domain and (ii)
performance on sampled instances, respectively. The experts are then merged to
create task-specific models optimized for the target domain. By dynamically
merging experts beneficial to target domains, we improve generalization across
various domains without extra training. Additionally, experts can be added or
removed conveniently, leading to great scalability. Extensive experiments on
multiple benchmarks demonstrate our framework's effectiveness, which
outperforms the unified model by an average of 10%. We further provide insights
into potential improvements, practical experience, and extensions of our
framework.

</details>


### [22] [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/abs/2506.22846)
*Duygu Altinok*

Main category: cs.CL

TL;DR: Proposes LAIL, an auxiliary loss framework to enhance CTC-based ASR using LLMs, improving linguistic modeling while maintaining CTC's efficiency.


<details>
  <summary>Details</summary>
Motivation: Autoregressive E2E ASR models are slow for real-time use, while CTC models lack linguistic dependency modeling. LAIL bridges this gap.

Method: Attaches connector layers to intermediate encoder layers, mapping outputs to LLM embedding space and computing a causal language modeling loss.

Result: Significant WER improvements on LibriSpeech, TEDLIUM2, and WSJ, achieving state-of-the-art CTC-based ASR performance.

Conclusion: LAIL effectively enhances CTC-based ASR with minimal overhead, combining linguistic modeling and decoding efficiency.

Abstract: End-to-end (E2E) automatic speech recognition (ASR) systems have
revolutionized the field by integrating all components into a single neural
network, with attention-based encoder-decoder models achieving state-of-the-art
performance. However, their autoregressive decoding process limits inference
speed, making them unsuitable for real-time applications. In contrast,
CTC-based models offer faster, non-autoregressive decoding but struggle to
model linguistic dependencies effectively. Addressing this challenge, we
propose a novel auxiliary loss framework called Language-Aware Intermediate
Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large
language models (LLMs). By attaching connector layers to intermediate encoder
layers, LAIL maps outputs to the embedding space of an LLM and computes a
causal language modeling loss during training. This approach enhances
linguistic modeling while preserving the computational efficiency of CTC
decoding. Using the Conformer architecture and various LLaMA models, we
demonstrate significant improvements in Word Error Rate (WER) on the
LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance
for CTC-based ASR with minimal computational overhead.

</details>


### [23] [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/abs/2506.22852)
*Yucheng Cai,Yuxuan Wu,Yi Huang,Junlan Feng,Zhijian Ou*

Main category: cs.CL

TL;DR: The paper proposes Knowledge Augmented Fine-Tuning (KAFT) to improve LLMs' factual accuracy in knowledge-intensive dialog systems, outperforming traditional prompting methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with factual accuracy in knowledge-intensive scenarios, prompting the need for better integration of external knowledge.

Method: Introduces KAFT, finetuning LLMs with domain-specific data and external knowledge, tested on the MobileCS2 dataset.

Result: KAFT significantly outperforms prompting in RAG and agent systems, especially in factual accuracy.

Conclusion: KAFT is a promising approach for enhancing LLMs in knowledge-intensive tasks, with empirical validation.

Abstract: Large language models (LLMs) have recently been applied to dialog systems.
Despite making progress, LLMs are prone to errors in knowledge-intensive
scenarios. Recently, approaches based on retrieval augmented generation (RAG)
and agent have emerged to improve the factual accuracy by enhancing the LLMs
with knowledge retrieved from external knowledge bases (KBs). This is mostly
implemented by prompting the LLMs with instructions, examples and the retrieved
knowledge. However, LLMs may have difficulty using the retrieved knowledge
effectively for response generation, because they are not well trained to do
such generation for specific domains. To mitigate this problem, we propose to
finetune the LLMs in the RAG-based and agent-based systems with domain-specific
data, together with domain-specific external knowledge, which is called
knowledge augmented finetuning (KAFT). We base our study on the MobileCS2
dataset, a real-life customer service dialog dataset that features intensive
knowledge interactions, to systematically compare the prompting and KAFT
techniques in the RAG-based and agent-based systems. Experiment results show
that KAFT substantially surpasses prompting in both RAG and agent systems,
particularly in terms of factual accuracy. To the best of our knowledge, this
paper represents the first solid empirical work to investigate the KAFT idea.

</details>


### [24] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)
*Kyochul Jang,Donghyeon Lee,Kyusik Kim,Dongseok Heo,Taewhoo Lee,Woojeong Kim,Bongwon Suh*

Main category: cs.CL

TL;DR: The paper introduces DICE-SCORE to evaluate tool-related information dispersion in dialogues and DICE-BENCH, a framework for creating realistic function-calling datasets.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack realism in function-calling scenarios, prompting the need for a more practical evaluation metric and dataset.

Method: DICE-SCORE assesses tool-related information dispersion; DICE-BENCH synthesizes conversations using a tool graph and multi-agent system.

Result: DICE-BENCH produced 1,607 high-DICE-SCORE instances, revealing LLMs' limitations in real-world function-calling.

Conclusion: Significant improvements are needed for LLMs to handle real-world function-calling effectively.

Abstract: Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.

</details>


### [25] [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/abs/2506.22858)
*Duygu Altinok*

Main category: cs.CL

TL;DR: A novel training approach for ASR systems improves named entity and numerical data recognition by extending semantic context with overlapping windows and enriched data.


<details>
  <summary>Details</summary>
Motivation: ASR systems like Whisper struggle with named entities and numerical data, increasing WER and impairing semantic understanding in critical domains.

Method: Proposes overlapping context windows (5-second overlaps on 30-second chunks) and reassigning boundary-spanning entities to ensure proper formatting. Uses enriched training data with embedded entity labels.

Result: Improves performance on semantic tasks like NER and entity formatting, as shown on the Spoken Wikipedia dataset.

Conclusion: Context-aware training effectively addresses ASR limitations for long-form transcription and complex entity recognition.

Abstract: Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high
transcription accuracy but struggle with named entities and numerical data,
especially when proper formatting is required. These issues increase word error
rate (WER) and impair semantic understanding in critical domains like legal,
financial, and medical applications. We propose a novel training approach that
extends the semantic context of ASR models by adding overlapping context
windows during training. By sliding 5-second overlaps on both sides of
30-second chunks, we create a 40-second "effective semantic window," improving
entity recognition and formatting while focusing predictions on the central 30
seconds. To address entities spanning chunk boundaries, we reassign such
entities entirely to the right-hand chunk, ensuring proper formatting.
Additionally, enriched training data with embedded entity labels enables the
model to learn both recognition and type-specific formatting. Evaluated on the
Spoken Wikipedia dataset, our method improves performance across semantic
tasks, including named entity recognition (NER) and entity formatting. These
results highlight the effectiveness of context-aware training in addressing ASR
limitations for long-form transcription and complex entity recognition tasks.

</details>


### [26] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
*Younwoo Choi,Changling Li,Yongjin Yang,Zhijing Jin*

Main category: cs.CL

TL;DR: The paper introduces 'interlocutor awareness' in LLMs, evaluating their ability to adapt to dialogue partners' identities and characteristics, revealing both benefits and risks in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' awareness of conversational partners is crucial for reliable performance and safety in multi-agent and human-AI systems.

Method: The study formalizes interlocutor awareness and evaluates it across reasoning patterns, linguistic style, and alignment preferences, using case studies to demonstrate practical impacts.

Result: LLMs reliably identify peers like GPT and Claude, enhancing collaboration but also introducing vulnerabilities like reward-hacking and jailbreak susceptibility.

Conclusion: Interlocutor awareness in LLMs presents dual promise and peril, necessitating further research and safeguards for multi-agent deployments.

Abstract: As large language models (LLMs) are increasingly integrated into multi-agent
and human-AI systems, understanding their awareness of both self-context and
conversational partners is essential for ensuring reliable performance and
robust safety. While prior work has extensively studied situational awareness
which refers to an LLM's ability to recognize its operating phase and
constraints, it has largely overlooked the complementary capacity to identify
and adapt to the identity and characteristics of a dialogue partner. In this
paper, we formalize this latter capability as interlocutor awareness and
present the first systematic evaluation of its emergence in contemporary LLMs.
We examine interlocutor inference across three dimensions-reasoning patterns,
linguistic style, and alignment preferences-and show that LLMs reliably
identify same-family peers and certain prominent model families, such as GPT
and Claude. To demonstrate its practical significance, we develop three case
studies in which interlocutor awareness both enhances multi-LLM collaboration
through prompt adaptation and introduces new alignment and safety
vulnerabilities, including reward-hacking behaviors and increased jailbreak
susceptibility. Our findings highlight the dual promise and peril of
identity-sensitive behavior in LLMs, underscoring the need for further
understanding of interlocutor awareness and new safeguards in multi-agent
deployments. Our code is open-sourced at
https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [27] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)
*Asen Dotsinski,Udit Thakur,Marko Ivanov,Mohammad Hafeez Khan,Maria Heuss*

Main category: cs.CL

TL;DR: A reproduction study of Ortu et al. (2024) confirms their findings on factual and counterfactual mechanisms in language models but extends the work by testing larger models, prompt variations, and domain-specific validity, revealing limitations in attention head ablation.


<details>
  <summary>Details</summary>
Motivation: To validate and extend Ortu et al.'s findings on how language models handle factual and counterfactual information, exploring generalizability, prompt impact, and domain-specific effects.

Method: Reproduced experiments on GPT-2 and Pythia 6.9B, extended to Llama 3.1 8B, tested prompt structure variations, and evaluated domain-specific validity.

Result: Confirmed primary findings but found reduced attention head specialization in larger models, prompt structure impacts counterfactual logits, and domain-specific skews in results.

Conclusion: Attention head ablation's effectiveness varies by model, prompt, domain, and task, highlighting limitations in underrepresented domains.

Abstract: We present a reproduction study of "Competition of Mechanisms: Tracing How
Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which
investigates competition of mechanisms in language models between factual
recall and counterfactual in-context repetition. Our study successfully
reproduces their primary findings regarding the localization of factual and
counterfactual information, the dominance of attention blocks in mechanism
competition, and the specialization of attention heads in handling competing
information. We reproduce their results on both GPT-2 (Radford et al., 2019)
and Pythia 6.9B (Biderman et al., 2023). We extend their work in three
significant directions. First, we explore the generalizability of these
findings to even larger models by replicating the experiments on Llama 3.1 8B
(Grattafiori et al., 2024), discovering greatly reduced attention head
specialization. Second, we investigate the impact of prompt structure by
introducing variations where we avoid repeating the counterfactual statement
verbatim or we change the premise word, observing a marked decrease in the
logit for the counterfactual token. Finally, we test the validity of the
authors' claims for prompts of specific domains, discovering that certain
categories of prompts skew the results by providing the factual prediction
token as part of the subject of the sentence. Overall, we find that the
attention head ablation proposed in Ortu et al. (2024) is ineffective for
domains that are underrepresented in their dataset, and that the effectiveness
varies based on model architecture, prompt structure, domain and task.

</details>


### [28] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)
*Yida Zhao,Hao Xve,Xiang Hu,Kewei Tu*

Main category: cs.CL

TL;DR: The paper introduces a unified framework for compositional syntactic language models (SLMs) based on constituency parse trees, evaluates their performance across tasks, and provides design recommendations.


<details>
  <summary>Details</summary>
Motivation: To enhance Transformers by incorporating syntactic biases and improve performance in tasks like language modeling and syntactic generalization.

Method: Proposes a unified framework for compositional SLMs, evaluates variants across tasks (language modeling, summarization, etc.), and analyzes design choices.

Result: Comprehensive empirical evaluation identifies effective design choices for compositional SLMs.

Conclusion: Provides recommendations for designing compositional SLMs and releases code for further research.

Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating
syntactic biases through the modeling of linearized syntactic parse trees
alongside surface sentences. This paper focuses on compositional SLMs that are
based on constituency parse trees and contain explicit bottom-up composition of
constituent representations. We identify key aspects of design choices in
existing compositional SLMs and propose a unified framework encompassing both
existing models and novel variants. We conduct a comprehensive empirical
evaluation of all the variants in our framework across language modeling,
syntactic generalization, summarization, dialogue, and inference efficiency.
Based on the experimental results, we make multiple recommendations on the
design of compositional SLMs. Our code is released at
https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [29] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
*Xianzhe Fan,Xuhui Zhou,Chuanyang Jin,Kolby Nottingham,Hao Zhu,Maarten Sap*

Main category: cs.CL

TL;DR: The SoMi-ToM benchmark evaluates Theory of Mind (ToM) in dynamic, embodied multi-agent social interactions, revealing a significant performance gap between humans and large vision-language models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: Current ToM benchmarks focus on static, text-based scenarios, lacking realism compared to dynamic, real-world social interactions.

Method: The SoMi-ToM benchmark uses multimodal data (visual, dialogue, action) from the SoMi environment, with first-person and third-person evaluations.

Result: LVLMs perform significantly worse than humans, with accuracy gaps of 40.1% (first-person) and 26.4% (third-person).

Conclusion: Future LVLMs need to enhance ToM capabilities for complex, embodied social interactions.

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [30] [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/abs/2506.23051)
*João Lucas Luz Lima Sarcinelli,Marina Lages Gonçalves Teixeira,Jade Bortot de Paiva,Diego Furtado Silva*

Main category: cs.CL

TL;DR: The paper introduces MariNER, the first gold-standard NER dataset for early 20th-century Brazilian Portuguese, addressing the lack of resources for historical texts in digital humanities.


<details>
  <summary>Details</summary>
Motivation: Brazilian Portuguese lacks high-quality NER datasets, especially for historical texts, hindering NLP research in digital humanities.

Method: The paper outlines the construction of MariNER, a manually annotated dataset with over 9,000 sentences, and evaluates state-of-the-art NER models on it.

Result: MariNER is created as a gold-standard dataset, and the performance of NER models is assessed for this historical context.

Conclusion: The work fills a critical gap in NER resources for Brazilian Portuguese, particularly for historical analysis, and provides benchmarks for future research.

Abstract: Named Entity Recognition (NER) is a fundamental Natural Language Processing
(NLP) task that aims to identify and classify entity mentions in texts across
different categories. While languages such as English possess a large number of
high-quality resources for this task, Brazilian Portuguese still lacks in
quantity of gold-standard NER datasets, especially when considering specific
domains. Particularly, this paper considers the importance of NER for analyzing
historical texts in the context of digital humanities. To address this gap,
this work outlines the construction of MariNER: \textit{Mapeamento e
Anota\c{c}\~oes de Registros hIst\'oricos para NER} (Mapping and Annotation of
Historical Records for NER), the first gold-standard dataset for early
20th-century Brazilian Portuguese, with more than 9,000 manually annotated
sentences. We also assess and compare the performance of state-of-the-art NER
models for the dataset.

</details>


### [31] [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/abs/2506.23056)
*Xiang Zhuang,Bin Wu,Jiyu Cui,Kehua Feng,Xiaotong Li,Huabin Xing,Keyan Ding,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: A knowledge-enhanced framework (K-MSE) improves molecular structure elucidation in LLMs by integrating a molecular substructure knowledge base and a specialized scorer, achieving over 20% performance gains.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with molecular structure elucidation due to limited chemical knowledge.

Method: K-MSE uses a molecular substructure knowledge base and Monte Carlo Tree Search, with a molecule-spectrum scorer for accurate evaluation.

Result: Significant performance improvement (over 20%) on GPT-4o-mini and GPT-4o.

Conclusion: K-MSE effectively addresses LLMs' limitations in molecular structure elucidation by enhancing chemical knowledge and evaluation accuracy.

Abstract: Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.

</details>


### [32] [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/abs/2506.23071)
*Zhengren Wang,Bozhou Li,Dongwen Yao,Wentao Zhang*

Main category: cs.CL

TL;DR: Text2VectorSQL unifies Text-to-SQL and vector search to enhance natural language query handling for structured and unstructured data, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-SQL and VectorSQL methods struggle with unstructured data and ambiguous queries, lacking expressiveness and tailored evaluation.

Method: Introduces Text2VectorSQL, enabling semantic filtering, multi-modal matching, and retrieval acceleration, with synthetic data and expert-reviewed ground truths.

Result: Demonstrates significant performance improvements over baseline methods.

Conclusion: Establishes Text2VectorSQL as a foundation for versatile database interfaces, with public repository availability.

Abstract: While Text-to-SQL enables natural language interaction with structured
databases, its effectiveness diminishes with unstructured data or ambiguous
queries due to rigid syntax and limited expressiveness. Concurrently, vector
search has emerged as a powerful paradigm for semantic retrieval, particularly
for unstructured data. However, existing VectorSQL implementations still rely
heavily on manual crafting and lack tailored evaluation frameworks, leaving a
significant gap between theoretical potential and practical deployment. To
bridge these complementary paradigms, we introduces Text2VectorSQL, a novel
framework unifying Text-to-SQL and vector search to overcome expressiveness
constraints and support more diverse and holistical natural language queries.
Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching,
and retrieval acceleration. For evaluation, we build vector index on
appropriate columns, extend user queries with semantic search, and annotate
ground truths via an automatic pipeline with expert review. Furthermore, we
develop dedicated Text2VectorSQL models with synthetic data, demonstrating
significant performance improvements over baseline methods. Our work
establishes the foundation for the Text2VectorSQL task, paving the way for more
versatile and intuitive database interfaces. The repository will be publicly
available at https://github.com/Open-DataFlow/Text2VectorSQL.

</details>


### [33] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)
*Yue Xu,Wenjie Wang*

Main category: cs.CL

TL;DR: The paper introduces Genres, a benchmark for evaluating relational and contextual gender bias in multimodal large language models (MLLMs) through dual-individual interactions, revealing persistent biases not evident in single-character settings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook subtle gender bias in interpersonal interactions, prompting the need for a deeper, relationship-aware evaluation.

Method: Genres uses a dual-character profile and narrative generation task to assess gender bias across multiple dimensions in MLLMs.

Result: Experiments show context-sensitive gender biases in MLLMs, highlighting the limitations of single-entity evaluations.

Conclusion: The study emphasizes the need for relationship-aware benchmarks to diagnose and mitigate interaction-driven gender bias in MLLMs.

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
across tasks involving both visual and textual modalities. However, growing
concerns remain about their potential to encode and amplify gender bias,
particularly in socially sensitive applications. Existing benchmarks
predominantly evaluate bias in isolated scenarios, overlooking how bias may
emerge subtly through interpersonal interactions. We fill this gap by going
beyond single-entity evaluation and instead focusing on a deeper examination of
relational and contextual gender bias in dual-individual interactions. We
introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs
through the lens of social relationships in generated narratives. Genres
assesses gender bias through a dual-character profile and narrative generation
task that captures rich interpersonal dynamics and supports a fine-grained bias
evaluation suite across multiple dimensions. Experiments on both open- and
closed-source MLLMs reveal persistent, context-sensitive gender biases that are
not evident in single-character settings. Our findings underscore the
importance of relationship-aware benchmarks for diagnosing subtle,
interaction-driven gender bias in MLLMs and provide actionable insights for
future bias mitigation.

</details>


### [34] [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/abs/2506.23111)
*Janki Atul Nawale,Mohammed Safi Ur Rahman Khan,Janani D,Mansi Gupta,Danish Pruthi,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: INDIC-BIAS is a benchmark for evaluating fairness of LLMs in India, revealing biases against marginalized groups.


<details>
  <summary>Details</summary>
Motivation: Existing fairness studies are Western-focused, inadequate for culturally diverse India.

Method: Curated 1,800 socio-cultural topics, generated 20,000 scenario templates, and structured three evaluation tasks (plausibility, judgment, generation) for 14 LLMs.

Result: LLMs show strong negative biases against marginalized identities, reinforcing stereotypes and struggling to mitigate bias.

Conclusion: INDIC-BIAS highlights allocative and representational harms, urging cautious LLM usage and open-sourcing the benchmark for further research.

Abstract: Existing studies on fairness are largely Western-focused, making them
inadequate for culturally diverse countries such as India. To address this gap,
we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to
evaluate fairness of LLMs across 85 identity groups encompassing diverse
castes, religions, regions, and tribes. We first consult domain experts to
curate over 1,800 socio-cultural topics spanning behaviors and situations,
where biases and stereotypes are likely to emerge. Grounded in these topics, we
generate and manually validate 20,000 real-world scenario templates to probe
LLMs for fairness. We structure these templates into three evaluation tasks:
plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on
these tasks reveals strong negative biases against marginalized identities,
with models frequently reinforcing common stereotypes. Additionally, we find
that models struggle to mitigate bias even when explicitly asked to rationalize
their decision. Our evaluation provides evidence of both allocative and
representational harms that current LLMs could cause towards Indian identities,
calling for a more cautious usage in practical applications. We release
INDIC-BIAS as an open-source benchmark to advance research on benchmarking and
mitigating biases and stereotypes in the Indian context.

</details>


### [35] [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/abs/2506.23122)
*Shivam Sharma,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper explores identifying narrative roles (Hero, Villain, Victim, Other) in memes across English and code-mixed languages, using diverse models and highlighting challenges in detecting 'Victim' and generalizing across cultures.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying nuanced narrative roles in memes, especially in culturally diverse and code-mixed contexts, and benchmark model performance.

Method: Evaluates multilingual transformers, sentiment classifiers, LLMs, and multimodal models under zero-shot settings, using precision, recall, and F1 metrics. Also explores prompt design for multimodal models.

Result: Larger models like DeBERTa-v3 and Qwen2.5-VL perform well, but struggles persist with 'Victim' class and cross-cultural generalization. Hybrid prompts improve results marginally.

Conclusion: Cultural grounding, prompt engineering, and multimodal reasoning are crucial for modeling subtle narrative roles in memes.

Abstract: This work investigates the challenging task of identifying narrative roles -
Hero, Villain, Victim, and Other - in Internet memes, across three diverse test
sets spanning English and code-mixed (English-Hindi) languages. Building on an
annotated dataset originally skewed toward the 'Other' class, we explore a more
balanced and linguistically diverse extension, originally introduced as part of
the CLEF 2024 shared task. Comprehensive lexical and structural analyses
highlight the nuanced, culture-specific, and context-rich language used in real
memes, in contrast to synthetically curated hateful content, which exhibits
explicit and repetitive lexical markers. To benchmark the role detection task,
we evaluate a wide spectrum of models, including fine-tuned multilingual
transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,
and multimodal vision-language models. Performance is assessed under zero-shot
settings using precision, recall, and F1 metrics. While larger models like
DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent
challenges in reliably identifying the 'Victim' class and generalising across
cultural and code-mixed content. We also explore prompt design strategies to
guide multimodal models and find that hybrid prompts incorporating structured
instructions and role definitions offer marginal yet consistent improvements.
Our findings underscore the importance of cultural grounding, prompt
engineering, and multimodal reasoning in modelling subtle narrative framings in
visual-textual content.

</details>


### [36] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)
*Zhaoye Fei,Li Ji,Siyin Wang,Junhao Shi,Jingjing Gong,Xipeng Qiu*

Main category: cs.CL

TL;DR: Embodied Planner-R1 is a reinforcement learning framework for LLMs to improve interactive task planning, achieving high success rates and strong generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with embodied task planning due to static knowledge and lack of causal learning in dynamic environments.

Method: Uses reinforcement learning with group rollout, sparse rewards, and Interactive Policy Optimization (IPO) for autonomous exploration.

Result: Achieves 97.78% completion on ALFWorld and 79.92% on ScienceWorld, with strong generalization (-3.66% drop in unseen environments).

Conclusion: Embodied Planner-R1 significantly outperforms prior methods, demonstrating robust interactive capabilities for LLMs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they face significant challenges in embodied task planning
scenarios that require continuous environmental understanding and action
generation. Existing approaches generate open-loop action scripts based on
static knowledge, making it difficult to learn causal relationships between
actions and environmental feedback, particularly in partially observable
environments. We introduce Embodied Planner-R1, a novel outcome-driven
reinforcement learning framework that enables LLMs to develop interactive
capabilities through autonomous exploration with minimal supervision. Our
framework incorporates three key innovations: (1) Without human annotations, we
employ pure reinforcement learning with group rollout, incorporating
in-environment interaction through parallel exploration; (2) completion-driven
sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient
learning from grouped trajectories. Across two challenging text-based Embodied
planning benchmarks, Embodied Planner-R1 achieves impressive completion rates
of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a
large margin, and suffers only a -3.66% drop in previously unseen environments,
evidencing strong generalization.

</details>


### [37] [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/abs/2506.23133)
*Dingzirui Wang,Xuanliang Zhang,Rongyu Cao,Longxu Dou,Xianzhen Luo,Yingwei Ma,Qingfu Zhu,Wanxiang Che,Binhua Li,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: Format-Adapter improves LLM reasoning by generating and selecting task-specific formats, reducing human labeling costs and boosting performance by 4.3%.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and unsuitability of human-labeled reasoning formats for all tasks, the paper aims to automate format adaptation.

Method: Proposes a method to measure reasoning errors and introduces Format-Adapter, which uses LLMs to generate and select optimal formats.

Result: Achieves a 4.3% average performance improvement on math and commonsense reasoning tasks.

Conclusion: Format-Adapter effectively mitigates reasoning inconsistencies by automating format selection, outperforming prior methods.

Abstract: Generating and voting multiple answers is an effective method to mitigate
reasoning inconsistencies of large language models (LLMs). Prior works have
shown that multiple reasoning formats outperform a single format when
generating multiple answers. However, previous works using multiple formats
rely on formats labeled by humans, which could be unsuitable for all tasks and
have high labeling costs. To address this issue, we adapt suitable formats to
the given tasks by generating and selecting formats. We first propose how to
measure the reasoning error when generating multiple answers. Then, we
introduce Format-Adapter, which utilizes LLMs to generate and select suitable
reasoning formats by minimizing the error measurement we present. We conduct
experiments on math and commonsense reasoning tasks, where Format-Adapter
achieves a 4.3% performance improvement on average over previous works,
demonstrating the effectiveness.

</details>


### [38] [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/abs/2506.23136)
*Shadman Sobhan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: A RAG pipeline is proposed to handle tables and images in technical documents, combining vector similarity search with a fine-tuned reranker, achieving high faithfulness and relevancy scores.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges like hallucination and outdated knowledge. Traditional RAG struggles with complex technical documents.

Method: Proposes a RAG pipeline with vector similarity search and a fine-tuned reranker (Gemma-2-9b-it), trained using RAFT on a custom dataset.

Result: Achieves 94% faithfulness (RAGas) and 96% (DeepEval), with 87% (RAGas) and 93% (DeepEval) answer relevancy. Outperforms general RAG pipelines.

Conclusion: The pipeline effectively handles technical documents, improving faithfulness and relevancy, especially for table-based and out-of-context questions.

Abstract: Large Language Models (LLMs) are capable of natural language understanding
and generation. But they face challenges such as hallucination and outdated
knowledge. Fine-tuning is one possible solution, but it is resource-intensive
and must be repeated with every data update. Retrieval-Augmented Generation
(RAG) offers an efficient solution by allowing LLMs to access external
knowledge sources. However, traditional RAG pipelines struggle with retrieving
information from complex technical documents with structured data such as
tables and images. In this work, we propose a RAG pipeline, capable of handling
tables and images in documents, for technical documents that support both
scanned and searchable formats. Its retrieval process combines vector
similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The
reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom
dataset designed to improve context identification for question answering. Our
evaluation demonstrates that the proposed pipeline achieves a high faithfulness
score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%
(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed
architecture is superior to general RAG pipelines in terms of table-based
questions and handling questions outside context.

</details>


### [39] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.CL

TL;DR: The paper introduces Flow-Modulated Scoring (FMS) for Knowledge Graph Completion, combining context-aware static embeddings with dynamic transformations to improve relational modeling.


<details>
  <summary>Details</summary>
Motivation: Existing embedding-based KGC methods lack the ability to capture contextual dependencies and relational dynamics, limiting their effectiveness.

Method: FMS uses a semantic context learning module for context-sensitive entity representations and a conditional flow-matching module for dynamic embedding transformations.

Result: FMS outperforms state-of-the-art methods on standard benchmarks.

Conclusion: FMS enhances relational semantics by integrating static and dynamic information, proving superior to existing approaches.

Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph
Completion (KGC). However, a majority of existing approaches are predicated on
static, embedding-based scoring, exhibiting inherent limitations in capturing
contextual dependencies and relational dynamics. Addressing this gap, we
propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal
components: (1) a semantic context learning module that encodes
context-sensitive entity representations, and (2) a conditional flow-matching
module designed to learn the dynamic transformation from a head to a tail
embedding, governed by the aforementioned context. The resultant predictive
vector field, representing the context-informed relational path, serves to
dynamically refine the initial static score of an entity pair. Through this
synergy of context-aware static representations and conditioned dynamic
information, FMS facilitates a more profound modeling of relational semantics.
Comprehensive evaluations on several standard benchmarks demonstrate that our
proposed method surpasses prior state-of-the-art results.

</details>


### [40] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)
*Prafulla Kumar Choubey,Xiangyu Peng,Shilpa Bhagavath,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: A new benchmark for Deep Search evaluates RAG systems using synthetic data simulating business workflows, revealing retrieval as the main bottleneck.


<details>
  <summary>Details</summary>
Motivation: To address the lack of realistic benchmarks for evaluating retrieval-augmented generation (RAG) systems that require multi-hop reasoning over diverse, sparse sources.

Method: Built a synthetic data pipeline simulating business workflows, generating interconnected content with noise and multi-hop questions. Released benchmark with 39,190 artifacts and answerable/unanswerable queries.

Result: Best-performing RAG methods scored 32.96 on average, with retrieval identified as the main bottleneck due to struggles in deep searches and incomplete evidence retrieval.

Conclusion: The benchmark highlights the need for improved retrieval methods in RAG systems to enhance performance in complex, multi-hop reasoning tasks.

Abstract: We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

</details>


### [41] [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)
*Dingzriui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: The paper introduces the Learning-to-Context Slope (LCS), a metric to evaluate in-context learning (ICL) effectiveness in LLMs, addressing reliability, attribution, and data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Current ICL evaluation methods are unreliable, poorly attributed, and impractical in data-scarce settings, necessitating a better metric.

Method: Proposes LCS, which models the slope between learning gain and contextual relevance, capturing continuous loss changes and minimizing labeled data reliance.

Result: LCS correlates strongly with performance improvements, works in biased/data-scarce scenarios, and identifies actionable thresholds and critical model capabilities.

Conclusion: LCS is a reliable, practical metric for assessing ICL effectiveness, offering insights into model behavior and failure modes.

Abstract: In-context learning (ICL) has emerged as an effective approach to enhance the
performance of large language models (LLMs). However, its effectiveness varies
significantly across models and tasks, posing challenges for practitioners to
determine when ICL reliably improves performance. Current evaluation
approaches, reliant on performance change after applying ICL, suffer from low
reliability, poor attribution, and impracticality in data-insufficient
scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that
quantifies ICL effectiveness by modeling the slope between learning gain (loss
decrease from demonstrations) and contextual relevance (demonstration-input
relevance). LCS addresses key limitations of performance-based metrics: (1) it
captures continuous loss changes even when outputs are incorrect, improving
reliability; (2) its formulation attributes ICL failures to weak contextual
alignment (inability to adapt inputs to demonstrations) or strong output
calibration (self-verification of correctness); and (3) it minimizes reliance
on labeled data via synthetic evaluation. Extensive experiments demonstrate
that LCS strongly correlates with performance improvements in labeled settings
and reliably reflects true effectiveness in biased or data-scarce scenarios.
Further analysis reveals actionable thresholds for LCS and identifies model
capabilities critical to ICL success.

</details>


### [42] [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/abs/2506.23149)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: The paper introduces V-Synthesis, a method for synthesizing in-context learning demonstrations from scratch using V-Score for consistency and diversity, improving performance by 2.0%.


<details>
  <summary>Details</summary>
Motivation: High labeling costs for in-context learning demonstrations drive the need for LLM-based synthesis without relying on pre-existing demonstrations.

Method: Proposes V-Score for consistency measurement and V-Synthesis for proportional sampling to ensure high consistency and diversity in synthesized demonstrations.

Result: V-Synthesis achieves an average 2.0% performance improvement over existing methods.

Conclusion: V-Synthesis effectively addresses the challenge of synthesizing demonstrations from scratch, ensuring consistency and diversity.

Abstract: High labeling cost for in-context learning (ICL) demonstrations motivates
using large language models (LLMs) for synthesis to reduce overhead. However,
existing synthesis methods are mainly task-specific or rely on pre-existing
demonstrations. So this paper focuses on synthesizing demonstrations from
scratch for arbitrary tasks. A major challenge in synthesizing from scratch is
ensuring consistency with the target task, as the lack of labeling guidance
could lead to synthesis bias. We first propose a consistency metric called
V-Score, which has higher performance and lower computation cost compared with
the metrics based on grams or embedding vectors. Furthermore, we introduce
V-Synthesis, which leverages V-Score for proportional sampling to ensure both
high consistency and diversity of synthesized demonstrations. Experimental
results demonstrate that V-Synthesis yields an average performance improvement
of 2.0% compared to existing synthesis methods confirming the effectiveness of
V-Synthesis.

</details>


### [43] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)
*Gabriel Iturra-Bocaz,Felipe Bravo-Marquez*

Main category: cs.CL

TL;DR: RiverText is a Python library for training and evaluating incremental word embeddings from text data streams, addressing the static nature of traditional word embeddings.


<details>
  <summary>Details</summary>
Motivation: Traditional word embeddings are static and struggle with evolving language patterns, especially in dynamic sources like social media.

Method: The library implements incremental techniques (Skip-gram, CBOW, Word Context Matrix) using PyTorch and adapts static evaluation tasks for streaming.

Result: The paper compares methods with various hyperparameters and discusses their performance.

Conclusion: RiverText provides a practical tool for dynamic word embedding scenarios, with open-source availability.

Abstract: Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

</details>


### [44] [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/abs/2506.23235)
*Yi-Chen Li,Tian Xu,Yang Yu,Xuqin Zhang,Xiong-Hui Chen,Zhongxiang Ling,Ningjing Chao,Lei Yuan,Zhi-Hua Zhou*

Main category: cs.CL

TL;DR: The paper reveals that LLMs inherently contain a latent reward model equivalent to offline inverse reinforcement learning, enabling high-quality reward signals without additional training. This method outperforms existing approaches and provides theoretical proof of RL's effectiveness for LLMs.


<details>
  <summary>Details</summary>
Motivation: To bypass the high cost of human preference data for LLM alignment and provide a rigorous theoretical foundation for AI feedback methods.

Method: Proves the existence of a latent reward model in LLMs, equivalent to inverse reinforcement learning, and uses it for reinforcement learning without further training.

Result: The method outperforms existing LLM-as-a-judge approaches and trained reward models, with superior error bounds.

Conclusion: The reward modeling stage can be replaced by eliciting pre-training knowledge, offering a more efficient and scalable paradigm for LLM alignment.

Abstract: The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.

</details>


### [45] [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/abs/2506.23288)
*Miguel Domingo,Francisco Casacuberta*

Main category: cs.CL

TL;DR: The paper proposes two new spelling normalization approaches using large language models, comparing unsupervised training and machine translation, with machine translation proving more effective.


<details>
  <summary>Details</summary>
Motivation: Standardizing spelling in historical documents is challenging due to evolving language conventions, necessitating effective normalization methods.

Method: Two approaches: one using unsupervised training and another using machine translation, evaluated across diverse datasets.

Result: Both methods showed promise, but machine translation performed better for spelling normalization.

Conclusion: Statistical machine translation remains the most suitable technology for spelling normalization in historical documents.

Abstract: The absence of standardized spelling conventions and the organic evolution of
human language present an inherent linguistic challenge within historical
documents, a longstanding concern for scholars in the humanities. Addressing
this issue, spelling normalization endeavors to align a document's orthography
with contemporary standards. In this study, we propose two new approaches based
on large language models: one of which has been trained without a supervised
training, and a second one which has been trained for machine translation. Our
evaluation spans multiple datasets encompassing diverse languages and
historical periods, leading us to the conclusion that while both of them
yielded encouraging results, statistical machine translation still seems to be
the most suitable technology for this task.

</details>


### [46] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
*P. Myles Eugenio*

Main category: cs.CL

TL;DR: A neuro-symbolic framework for generative language modeling uses local, event-driven learning with a hierarchical Hopfield memory chain, enabling emergent tokenization and structure without predefined tokens or supervision.


<details>
  <summary>Details</summary>
Motivation: To explore how symbolic structure can emerge from local neural learning, offering a scalable and interpretable neuro-symbolic system for language modeling.

Method: The model employs a hierarchical Hopfield memory chain as a dynamic tokenizer, learning symbol sequences as multi-scale representations and constructing projection tensors for feature binding.

Result: The retokenizer filters natural language patterns from noise, generating synthetic languages with human-like morphology, and supports long-term memory via emergent embedding neurons.

Conclusion: This framework advances neuromorphic architectures for generative language models by enabling emergent tokens, grammar, and reasoning within a Hopfield hierarchy.

Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [47] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
*Shouvon Sarker,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: The study focuses on detecting and classifying medication events from clinical notes using a BERT-based ensemble model, improving Micro-F and Macro-F scores.


<details>
  <summary>Details</summary>
Motivation: Identifying key variables like medications and diseases from health records has broad clinical applications, prompting the need for advanced NLP methods.

Method: Pretrained BERT models on Wikipedia and MIMIC were fine-tuned on CMED data, then ensemble predictions were made using voting strategies.

Result: The ensemble model improved strict Micro-F score by ~5% and Macro-F score by ~6%.

Conclusion: BERT-based ensemble models effectively enhance medication event classification in clinical notes.

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


### [48] [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/abs/2506.23340)
*Yumeng Lin,Xufeng Duan,David Haslett,Yige Chen,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: The study examines how training data, language proximity, and language family impact information loss in multilingual translation using GPT-4 and Llama 2, revealing that data size and linguistic distance interact significantly.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multilingual translation for language pairs with limited data or high divergence from English.

Method: Round-trip translations evaluated using BLEU scores and BERT similarity metrics.

Result: Training data size and language distance interact strongly; languages closer to English perform better in low-resource settings. Orthographic, phylogenetic, syntactic, and geographical distances predict performance.

Conclusion: Translation quality depends on both data volume and structural/typological language relationships, highlighting linguistic constraints in multilingual models.

Abstract: Large language models have achieved impressive progress in multilingual
translation, yet they continue to face challenges with certain language
pairs-particularly those with limited training data or significant linguistic
divergence from English. This study systematically investigates how training
data, language proximity, and language family affect information loss in
multilingual translation. We evaluate two large language models, GPT-4 and
Llama 2, by performing round-trip translations. Translation quality was
assessed using BLEU scores and BERT similarity metrics. Our results reveal a
robust interaction between training data size and language distance: while
abundant training data can mitigate the effects of linguistic divergence,
languages structurally closer to English consistently yield higher translation
quality in low-resource conditions. Among various distance metrics,
orthographic, phylogenetic, syntactic, and geographical distances emerge as
strong predictors of translation performance. Language family also exerts an
independent influence. These findings contribute to a deeper understanding of
the linguistic constraints shaping multilingual translation in large language
models, emphasizing that translation quality is shaped not only by data volume
but also by structural and typological relationships between languages.

</details>


### [49] [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)
*Akim Tsvigun,Daniil Vasilev,Ivan Tsvigun,Ivan Lysenko,Talgat Bektleuov,Aleksandr Medvedev,Uliana Vinogradova,Nikita Severin,Mikhail Mozikov,Andrey Savchenko,Rostislav Grigorev,Ramil Kuleev,Fedor Zhdanov,Artem Shelmanov,Ilya Makarov*

Main category: cs.CL

TL;DR: ATGen is a framework integrating active learning (AL) with text generation (NLG), reducing annotation effort and costs using human annotators and LLMs.


<details>
  <summary>Details</summary>
Motivation: Limited application of AL to NLG tasks despite their growing popularity, prompting the need for a unified solution.

Method: ATGen combines AL strategies with NLG, supporting human annotators and LLM-based automatic annotation (e.g., ChatGPT, Claude).

Result: Reduces human annotation effort and LLM API costs, with evaluation showing effectiveness across diverse NLG tasks.

Conclusion: ATGen successfully bridges AL and NLG, offering a practical, cost-effective solution for annotation in text generation.

Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the
annotation effort required for training machine learning models. However,
despite the surging popularity of natural language generation (NLG) tasks in
recent years, the application of AL to NLG has been limited. In this paper, we
introduce Active Text Generation (ATGen) - a comprehensive framework that
bridges AL with text generation tasks, enabling the application of
state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered
annotation in NLG tasks using both human annotators and automatic annotation
agents based on large language models (LLMs). The framework supports LLMs
deployed as services, such as ChatGPT and Claude, or operated on-premises.
Furthermore, ATGen provides a unified platform for smooth implementation and
benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present
evaluation results for state-of-the-art AL strategies across diverse settings
and multiple text generation tasks. We show that ATGen reduces both the effort
of human annotators and costs associated with API calls to LLM-based annotation
agents. The code of the framework is available on GitHub under the MIT license.
The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [50] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
*Taejin Kim,Siun-Chuon Mau,Konrad Vesey*

Main category: cs.CL

TL;DR: The paper introduces Perspective-Dial, a method to quantify and control perspectives in LLM outputs using a metric space and systematic prompt engineering.


<details>
  <summary>Details</summary>
Motivation: There's a lack of quantifiable understanding of bias and perspective in LLM outputs, which this paper addresses.

Method: Uses Perspective Space for measurement and Systematic Prompt Engineering with greedy-coordinate descent for perspective control.

Result: Empirically quantifies and adjusts LLM outputs for various topics, enabling applications like bias mitigation and narrative tracking.

Conclusion: Perspective-Dial offers a practical approach to understanding and controlling perspectives in LLM outputs, with broad applications.

Abstract: Large language models (LLMs) are used in a variety of mission-critical roles.
Due to the rapidly developing nature of LLMs, there is a lack of quantifiable
understanding of the bias and perspective associated with LLM output. Inspired
by this need, this paper considers the broader issue of perspective or
viewpoint of general text and perspective control of large-language model (LLM)
output. Perspective-Dial consists of two main components: a (1) metric space,
dubbed Perspective Space, that enables quantitative measurements of different
perspectives regarding a topic, and the use of (2) Systematic Prompt
Engineering that utilizes greedy-coordinate descent to control LLM output
perspective based on measurement feedback from the Perspective Space. The
empirical nature of the approach allows progress to side step a principled
understanding of perspective or bias -- effectively quantifying and adjusting
outputs for a variety of topics. Potential applications include detection,
tracking and mitigation of LLM bias, narrative detection, sense making and
tracking in public discourse, and debate bot advocating given perspective.

</details>


### [51] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)
*Eugene J. Yu,Dawei Zhu,Yifan Song,Xiangyu Wong,Jiebin Zhang,Wenxuan Shi,Xiaoguang Li,Qun Liu,Sujian Li*

Main category: cs.CL

TL;DR: The paper introduces MOG, a framework for generating Wikipedia articles using a hierarchical memory architecture to improve accuracy and structure.


<details>
  <summary>Details</summary>
Motivation: Autonomous Wikipedia article generation is challenging due to the need for accurate, comprehensive, and structured information from diverse sources.

Method: MOG extracts fine-grained memory units from web documents, organizes them hierarchically, and uses this structure to guide article generation. A citation module links sentences to memory units for traceability.

Result: Evaluations on the WikiStart dataset show MOG outperforms baselines in producing informative and reliable articles.

Conclusion: MOG is effective for real-world Wikipedia article generation, ensuring informativeness, verifiability, and minimal hallucinations.

Abstract: Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

</details>


### [52] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
*Jiale Zhang,Zichong Wang,Avash Palikhe,Zhipeng Yin,Wenbin Zhang*

Main category: cs.CL

TL;DR: The paper reviews fairness datasets in language model research, introduces a unified evaluation framework, and highlights biases in benchmarks to guide better dataset selection and interpretation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scrutiny on fairness datasets used in language model benchmarks and their potential biases.

Method: A broad review of widely used fairness datasets, characterization along key dimensions, and introduction of a unified evaluation framework.

Result: Revealed consistent demographic disparities across datasets and biases influencing fairness conclusions.

Conclusion: Offers guidance for dataset use and advocates for more diverse benchmarks, with all resources made publicly available for transparency.

Abstract: Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

</details>


### [53] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)
*Felipe Nuti,Tim Franzmeyer,João Henriques*

Main category: cs.CL

TL;DR: The paper introduces Tuning Contribution (TuCo), a method to quantitatively measure fine-tuning's impact on individual LLM outputs by analyzing hidden states and decomposing models into pre-training and fine-tuning components.


<details>
  <summary>Details</summary>
Motivation: Existing work lacks a systematic way to analyze fine-tuning's effect on individual LLM outputs, limiting understanding of its influence on model behavior and safety.

Method: Proposes TuCo, a metric derived from decomposing fine-tuned LLMs into pre-training and fine-tuning components, and analyzing hidden states to measure fine-tuning's contribution.

Result: TuCo reveals that adversarial attacks reduce fine-tuning's influence, and successful attacks correlate with lower TuCo values, linking fine-tuning attenuation to attack success.

Conclusion: TuCo provides a quantitative tool to study fine-tuning's role in model behavior and safety, offering insights into adversarial attack mechanisms.

Abstract: Past work has studied the effects of fine-tuning on large language models'
(LLMs) overall performance on certain tasks. However, a quantitative and
systematic method for analyzing its effect on individual outputs is still
lacking. Here, we propose a new method for measuring the contribution that
fine-tuning makes to individual LLM responses, assuming access to the original
pre-trained model. Our method tracks the model's intermediate hidden states,
providing a more fine-grained insight into the effects of fine-tuning than a
simple comparison of final outputs from pre-trained and fine-tuned models. We
introduce and theoretically analyze an exact decomposition of any fine-tuned
LLM into a pre-training component and a fine-tuning component. Empirically, we
find that model behavior and performance can be steered by up- or down-scaling
the fine-tuning component during the forward pass. Motivated by this finding
and our theoretical analysis, we define the Tuning Contribution (TuCo) as the
ratio of the magnitudes of the fine-tuning component to the pre-training
component. We observe that three prominent adversarial attacks on LLMs
circumvent safety measures in a way that reduces TuCo, and that TuCo is
consistently lower on prompts where these attacks succeed compared to those
where they do not. This suggests that attenuating the effect of fine-tuning on
model outputs plays a role in the success of such attacks. In summary, TuCo
enables the quantitative study of how fine-tuning influences model behavior and
safety, and vice versa.

</details>


### [54] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)
*Zixian Huang,Chenxu Niu,Yu Gu,Gengyang Xiao,Xinwei Huang,Gong Cheng*

Main category: cs.CL

TL;DR: Proposes a pipelined decoder for parallel text generation, improving speed without compromising quality or memory.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models are slow due to sequential token generation, creating a bottleneck.

Method: Introduces a pipelined decoder that generates multiple subsequences in parallel at each time-step.

Result: Significantly improves generation speed across tasks like QA, summarization, and keyphrase generation.

Conclusion: The pipelined decoder effectively balances speed and quality in text generation.

Abstract: As the basis of generative AI, an autoregressive model requires the
generation of a new token depending on all the previously generated tokens,
which brings high quality but also restricts the model to generate tokens one
by one, forming a bottleneck limiting the generation speed. In this paper, we
propose a new decoder architecture that efficiently generates text in parallel
for context-aware generation tasks. Our proposed pipelined decoder initiates
the generation of multiple subsequences simultaneously, and, at each time-step,
it generates a new token for each subsequence to realize parallelism.
Experiments on multiple text generation tasks, including question answering,
text summarization, and keyphrase generation, show that our pipelined decoder
significantly improves the generation speed without a significant loss of
generation quality or additional memory consumption.

</details>


### [55] [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)
*Jang Won June*

Main category: cs.CL

TL;DR: ATF (Adaptive Table Filtering Framework) prunes uninformative table columns and rows to improve LLM performance on large tables, reducing cells by ~70% and boosting TableQA tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with large tables due to input length limits, necessitating a solution to filter uninformative data.

Method: ATF uses LLM-generated column descriptions, clustering, and sparse-dense alignment scores to prune columns and rows, integrating with existing models like TAPAS and TAPEX without retraining.

Result: ATF reduces table cells by ~70%, improving performance on TableQA tasks but slightly dropping performance on Table Fact Verification.

Conclusion: ATF effectively balances informativeness and minimalism, adapting to different tasks.

Abstract: Large language models (LLMs) for table-based reasoning often struggle with
large tables due to input length limits. We propose ATF (Adaptive Table
Filtering Framework), a modular and question-aware filtering pipeline that
prunes uninformative columns and rows using LLM-generated column descriptions,
clustering, and sparse-dense alignment scores. ATF integrates seamlessly with
existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that
ATF reduces table cells by ~70\%, boosting performance on out-of-domain TableQA
tasks while causing slight performance drops on Table Fact Verification, where
full-table context is more critical. These results highlight ATF's ability to
adaptively balance informativeness and minimalism across tasks.

</details>


### [56] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)
*Haocheng Yu,Yaxiong Wu,Hao Wang,Wei Guo,Yong Liu,Yawen Li,Yuyang Ye,Junping Du,Enhong Chen*

Main category: cs.CL

TL;DR: TAIRA is a thought-augmented LLM-powered multi-agent system for interactive recommendations, improving performance on complex user intents through distilled thought patterns.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-powered recommender agents struggle with diverse and complex user intents due to limited planning and generalization.

Method: TAIRA uses a manager agent with Thought Pattern Distillation (TPD) to decompose user needs and plan subtasks, alongside user simulation for evaluation.

Result: TAIRA outperforms existing methods, especially on challenging tasks, and generalizes well on novel tasks.

Conclusion: TAIRA effectively manages complex user intents in interactive recommendation systems, validated by experiments.

Abstract: Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [57] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)
*Zhihao Zhang,Qiaole Dong,Qi Zhang,Jun Zhao,Enyu Zhou,Zhiheng Xi,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Tao Ji,Tao Gui,Xuanjing Huang*

Main category: cs.CL

TL;DR: SFT and RFT are compared for adapting multimodal models; SFT causes forgetting, while RFT preserves knowledge. Data distribution, not algorithms, drives forgetting.


<details>
  <summary>Details</summary>
Motivation: To understand how SFT and RFT impact prior knowledge in multimodal models during task adaptation.

Method: Introduces jigsaw puzzles as a novel task, tests SFT and RFT on Qwen2.5-VL, and analyzes learning dynamics.

Result: SFT learns quickly but forgets prior knowledge; RFT learns slowly but retains knowledge. Correct RFT rollouts help SFT preserve knowledge.

Conclusion: RFT is better for stable continual learning; data distribution is key to forgetting.

Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and
Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large
language models to downstream tasks. While effective at task adaptation, their
impact on prior knowledge remains unclear. In this paper, we introduce jigsaw
puzzles as a novel task absent from existing pretraining corpora and
systematically study the behavior of SFT and RFT on an open-source multimodal
model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid
task acquisition but leads to catastrophic forgetting, whereas RFT learns more
slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon
through the lens of learning dynamics, showing that RFT reinforces correct
samples that are naturally aligned with the base model's probability landscape,
mitigating interference with prior knowledge. Moreover, supervised training on
correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly
learning new tasks. These findings suggest that data distribution, rather than
algorithmic differences, plays a central role in forgetting, and highlight
RFT's potential for stable continual learning in multimodal large language
models.

</details>


### [58] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)
*Phan Quoc Hung Mai,Quang Hung Nguyen,Phuong Giang Duong,Hong Hanh Nguyen,Nguyen Tuan Long*

Main category: cs.CL

TL;DR: The paper introduces NEU-ESC, a Vietnamese dataset for sentiment and topic classification in education, addressing gaps in existing datasets. It uses multitask learning with BERT, achieving high accuracy, and benchmarks against other models.


<details>
  <summary>Details</summary>
Motivation: Existing educational datasets lack domain relevance and student slang in Vietnamese. NEU-ESC fills this gap with a richer, more diverse dataset.

Method: The study employs multitask learning with encoder-only language models (BERT) for sentiment and topic classification.

Result: The model achieves 83.7% and 79.8% accuracy for sentiment and topic classification, respectively.

Conclusion: NEU-ESC is a valuable resource for Vietnamese educational sentiment and topic analysis, with strong performance demonstrated through benchmarks.

Abstract: In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


### [59] [On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?](https://arxiv.org/abs/2506.23527)
*Jan Kvapil,Martin Fajcik*

Main category: cs.CL

TL;DR: The paper analyzes memorization, creativity, and nonsense in LLM-generated cooking recipes using human annotations and an automated 'LLM-as-judge' pipeline.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs balance memorization, creativity, and nonsense in generated content, and to scale this analysis beyond small samples.

Method: Human annotation of 20 LLM-generated recipes for memorization, creativity, and nonsense, followed by an automated pipeline (LLM-as-judge) for large-scale analysis.

Result: Mixtral relies heavily on memorized content. The automated pipeline, using Llama 3.1+Gemma 2 9B, achieves 78% accuracy in ingredient matching.

Conclusion: The study provides a scalable framework to quantify memorization, creativity, and nonsense in LLM outputs, revealing their creative limitations.

Abstract: This work-in-progress investigates the memorization, creativity, and nonsense
found in cooking recipes generated from Large Language Models (LLMs).
Precisely, we aim (i) to analyze memorization, creativity, and non-sense in
LLMs using a small, high-quality set of human judgments and (ii) to evaluate
potential approaches to automate such a human annotation in order to scale our
study to hundreds of recipes. To achieve (i), we conduct a detailed human
annotation on 20 preselected recipes generated by LLM (Mixtral), extracting
each recipe's ingredients and step-by-step actions to assess which elements are
memorized--i.e., directly traceable to online sources possibly seen during
training--and which arise from genuine creative synthesis or outright nonsense.
We find that Mixtral consistently reuses ingredients that can be found in
online documents, potentially seen during model training, suggesting strong
reliance on memorized content. To achieve aim (ii) and scale our analysis
beyond small sample sizes and single LLM validation, we design an
``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,
parsing ingredients and recipe steps, and their annotation. For instance,
comparing its output against human annotations, the best ingredient extractor
and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on
ingredient matching. This automated framework enables large-scale
quantification of memorization, creativity, and nonsense in generated recipes,
providing rigorous evidence of the models' creative capacities.

</details>


### [60] [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)
*Weijie Shi,Yue Cui,Yaguang Wu,Jingzhi Fang,Shibo Zhang,Mengze Li,Sirui Han,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.CL

TL;DR: SemDiD improves semantic diversity in language model outputs, outperforming existing methods in quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for diverse decoding focus on lexical diversity, limiting applications like Best-of-N strategies and reinforcement learning.

Method: SemDiD uses orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment in embedding space.

Result: SemDiD boosts Best-of-N coverage by 1.4-5.2%, speeds RLHF training by 15%, and increases accuracy by up to 2.1%.

Conclusion: SemDiD effectively balances quality and semantic diversity, outperforming current techniques.

Abstract: Diverse decoding of large language models is crucial for applications
requiring multiple semantically distinct responses, yet existing methods
primarily achieve lexical rather than semantic diversity. This limitation
significantly constrains Best-of-N strategies, group-based reinforcement
learning, and data synthesis. While temperature sampling and diverse beam
search modify token distributions or apply n-gram penalties, they fail to
ensure meaningful semantic differentiation. We introduce Semantic-guided
Diverse Decoding (SemDiD), operating directly in embedding space that balances
quality with diversity through three complementary mechanisms: orthogonal
directional guidance, dynamic inter-group repulsion, and position-debiased
probability assessment. SemDiD harmonizes these competing objectives using
adaptive gain functions and constraint optimization, ensuring both quality
thresholds and maximal semantic differentiation. Experiments show SemDiD
consistently outperforms existing methods, improving Best-of-N coverage by
1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%
while increasing accuracy by up to 2.1%.

</details>


### [61] [Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs](https://arxiv.org/abs/2506.23610)
*Manuel Pratelli,Marinella Petrocchi*

Main category: cs.CL

TL;DR: LLMs can generate synthetic behavioral data, but their ability to replicate personality-driven psychological differences, like susceptibility to misinformation, is mixed. Some traits (e.g., Agreeableness, Conscientiousness) are replicated well, while others diverge, revealing LLM biases.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can ethically and accurately simulate human personality-driven behaviors, particularly in discerning misinformation.

Method: LLM agents were conditioned on Big-Five personality profiles and compared to human data on news discernment (judging true/false headlines).

Result: Some personality-misinformation links (e.g., Agreeableness, Conscientiousness) were replicated, but others showed biases, highlighting LLM limitations.

Conclusion: LLMs show promise for behavioral simulation but have limits in fully capturing personality-driven cognitive diversity.

Abstract: Large language models (LLMs) make it possible to generate synthetic
behavioural data at scale, offering an ethical and low-cost alternative to
human experiments. Whether such data can faithfully capture psychological
differences driven by personality traits, however, remains an open question. We
evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to
reproduce personality-based variation in susceptibility to misinformation,
focusing on news discernment, the ability to judge true headlines as true and
false headlines as false. Leveraging published datasets in which human
participants with known personality profiles rated headline accuracy, we create
matching LLM agents and compare their responses to the original human patterns.
Certain trait-misinformation associations, notably those involving
Agreeableness and Conscientiousness, are reliably replicated, whereas others
diverge, revealing systematic biases in how LLMs internalize and express
personality. The results underscore both the promise and the limits of
personality-aligned LLMs for behavioral simulation, and offer new insight into
modeling cognitive diversity in artificial agents.

</details>


### [62] [Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack](https://arxiv.org/abs/2506.23661)
*Arnisa Fazla,Lucas Krauter,David Guzman Piedrahita,Andrianos Michail*

Main category: cs.CL

TL;DR: BeamAttack is extended to include word deletions and optional substitutions, achieving high attack success rates while preserving text similarity.


<details>
  <summary>Details</summary>
Motivation: To enhance adversarial attack methods for evaluating text classification robustness with minimal modifications.

Method: Extends BeamAttack with word deletions, optional substitutions, and LIME integration for prioritizing word replacements.

Result: Achieves over 99% attack success rate across datasets and models (BiLSTM, BERT, RoBERTa) while maintaining text similarity.

Conclusion: BeamAttack is effective but has limitations; the implementation is publicly available.

Abstract: We extend BeamAttack, an adversarial attack algorithm designed to evaluate
the robustness of text classification systems through word-level modifications
guided by beam search. Our extensions include support for word deletions and
the option to skip substitutions, enabling the discovery of minimal
modifications that alter model predictions. We also integrate LIME to better
prioritize word replacements. Evaluated across multiple datasets and victim
models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA
framework, our approach achieves over a 99\% attack success rate while
preserving the semantic and lexical similarity of the original texts. Through
both quantitative and qualitative analysis, we highlight BeamAttack's
effectiveness and its limitations. Our implementation is available at
https://github.com/LucK1Y/BeamAttack

</details>


### [63] [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/abs/2506.23662)
*Philip Lippmann,Jie Yang*

Main category: cs.CL

TL;DR: ZEST is a zero-shot contextual adaptation framework that synthesizes a proxy corpus for domain-specific embeddings without needing target corpus access or finetuning.


<details>
  <summary>Details</summary>
Motivation: Overcome practical barriers of context-aware embedding methods, which require corpus access or domain-specific finetuning, especially in privacy-sensitive or resource-constrained settings.

Method: Uses a multi-step hierarchical procedure to generate a synthetic context corpus from a few exemplar documents, emulating key domain-specific distributions.

Result: ZEST performs within 0.5% of models with full target corpus access on the MTEB benchmark, using only five example documents.

Conclusion: ZEST offers a practical solution for deploying adaptable, high-performance embeddings in constrained environments without retraining or corpus access.

Abstract: Context-aware embedding methods boost retrieval accuracy by conditioning on
corpus statistics (e.g., term co-occurrence and topical patterns) extracted
from neighboring documents. However, this context-aware approach requires
access to the target corpus or requires domain-specific finetuning, posing
practical barriers in privacy-sensitive or resource-constrained settings. We
present ZEST, a zero-shot contextual adaptation framework that replaces real
corpus access with a one-time offline synthesis of a compact proxy. Given only
a handful exemplar documents representative of the general target domain, we
use a multi-step hierarchical procedure to generate a synthetic context corpus
of several hundred documents that aims to emulate key domain-specific
distributions. At inference, the frozen context-aware encoder uses this proxy
corpus -- without any finetuning or target corpus access -- to produce
domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot
synthetic context adaptation using only five example documents performs within
0.5% of models leveraging full target corpus access -- demonstrating remarkable
efficacy without any retraining. ZEST thus provides a practical method for
deploying high-performance, adaptable embeddings in constrained environments.

</details>


### [64] [L0: Reinforcement Learning to Become General Agents](https://arxiv.org/abs/2506.23667)
*Junjie Zhang,Jingyi Xi,Zhuoyang Song,Junyu Lu,Yuhua Ke,Ting Sun,Yukun Yang,Jiaxing Zhang,Songxin Zhang,Zejian Xie*

Main category: cs.CL

TL;DR: L-Zero (L0) is a scalable, end-to-end training pipeline for general-purpose agents, featuring a low-cost, extensible worker pool and NB-Agent scaffold. It improves LLM performance on factuality benchmarks using Reinforcement Learning with Verifiable Rewards (RLVR).


<details>
  <summary>Details</summary>
Motivation: Addressing scalability and training efficiency challenges in training LLMs for multi-turn, long-horizon tasks.

Method: Introduces L0 with a concurrent agent worker pool and NB-Agent scaffold using "code-as-action" via REPL. Evaluated on factuality benchmarks with RLVR.

Result: Boosts accuracy on SimpleQA from 30% to 80% and on HotpotQA from 22% to 41% using Qwen2.5-7B-Instruct model.

Conclusion: L0 is effective for training LLMs as autonomous agents, with open-sourced resources available for broader application.

Abstract: Training large language models (LLMs) to act as autonomous agents for
multi-turn, long-horizon tasks remains significant challenges in scalability
and training efficiency. To address this, we introduce L-Zero (L0), a scalable,
end-to-end training pipeline for general-purpose agents. Featuring a low-cost,
extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier
for applying reinforcement learning in complex environments. We also introduce
NB-Agent, the agent scaffold within L0, which operates in a "code-as-action"
fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality
question-answering benchmarks. Our experiments demonstrate that a base model
can develop robust problem-solving skills using solely Reinforcement Learning
with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method
boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41
%. We have open-sourced the entire L0 system, including our L0 series models,
the NB-Agent, a complete training pipeline, and the corresponding training
recipes on (https://github.com/cmriat/l0).

</details>


### [65] [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/abs/2506.23735)
*JiaRu Wu,Mingwei Liu*

Main category: cs.CL

TL;DR: AutoEvoEval is an evolution-based framework for evaluating LLMs, introducing 22 atomic operations to generate diverse test samples, revealing significant accuracy drops and model sensitivity variations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs are static and lack systematic control over perturbations, limiting comprehensive robustness assessment.

Method: Proposes AutoEvoEval with 22 interpretable atomic evolution operations and multi-round compositions to generate challenging test samples.

Result: Atomic operations cause a 7.283% accuracy drop; structure-disrupting edits have the largest impact. Combining steps amplifies adversarial effects by up to 52.932%.

Conclusion: Current benchmarks may overestimate model generalization, highlighting the need for evolution-aware robustness evaluation.

Abstract: Large language models (LLMs) have shown remarkable performance on various
tasks, but existing evaluation benchmarks are often static and insufficient to
fully assess their robustness and generalization in realistic scenarios. Prior
work using evolutionary or adversarial data augmentation has improved
evaluation diversity but lacks systematic control over perturbation types and
multi-step complexity, limiting comprehensive robustness analysis. To address
these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for
close-ended tasks such as multi-choice question answering. AutoEvoEval
introduces 22 interpretable atomic evolution operations and supports
multi-round compositions, enabling controlled generation of diverse,
challenging, and realistic test samples. We conduct extensive experiments
addressing four research questions on a broad set of open- and closed-source
LLMs. Our results show that atomic operations cause an average accuracy drop of
7.283\%, with structure-disrupting or misleading semantic edits causing the
largest declines. Model sensitivities vary significantly for the same
perturbation, and combining multiple evolution steps amplifies adversarial
effects by up to 52.932\%. These findings suggest current benchmarks may
overestimate true model generalization and emphasize the need for
evolution-aware robustness evaluation. Code and resources are available at:
https://github.com/SYSUSELab/AutoEvoEval.

</details>


### [66] [Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences](https://arxiv.org/abs/2506.23743)
*Tiziano Labruna,Simone Gallo,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: The study quantifies positional bias in binary question answering across five large language models, showing it grows exponentially with increased uncertainty.


<details>
  <summary>Details</summary>
Motivation: To analyze how model decisions are influenced by the order of answer options, especially under varying uncertainty levels.

Method: Adapted the SQuAD-it dataset with incorrect options and less context, and evaluated on WebGPT and Winning Arguments benchmarks, flipping answer orders to measure bias.

Result: Positional bias is minimal in low-uncertainty conditions but increases significantly when uncertainty rises.

Conclusion: Answer ordering significantly impacts model decisions under high uncertainty, highlighting a need for bias mitigation strategies.

Abstract: Positional bias in binary question answering occurs when a model
systematically favors one choice over another based solely on the ordering of
presented options. In this study, we quantify and analyze positional bias
across five large language models under varying degrees of answer uncertainty.
We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option
and then created multiple versions with progressively less context and more
out-of-context answers, yielding datasets that range from low to high
uncertainty. Additionally, we evaluate two naturally higher-uncertainty
benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality
scores, and (2) Winning Arguments - where models predict the more persuasive
argument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order
of the "correct" (or higher-quality/persuasive) option is systematically
flipped (first placed in position 1, then in position 2) to compute both
Preference Fairness and Position Consistency. We observe that positional bias
is nearly absent under low-uncertainty conditions, but grows exponentially when
it becomes doubtful to decide which option is correct.

</details>


### [67] [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/abs/2506.23840)
*Bowen Ding,Yuhan Chen,Futing Wang,Lingfeng Ming,Tao Lin*

Main category: cs.CL

TL;DR: DuP-PO addresses the 'thinking trap' in Large Reasoning Models (LRMs) by optimizing token efficiency and performance through balanced sampling, advantage control, and policy shaping.


<details>
  <summary>Details</summary>
Motivation: LRMs often overthink simple tasks, producing verbose responses with unnecessary thinking tokens, reducing efficiency and accuracy.

Method: Proposes Dual Policy Preference Optimization (DuP-PO) with rollout sampling, advantage control, and policy shaping to regulate thinking tokens.

Result: DuP-PO improves token efficiency and maintains superior performance on math reasoning benchmarks.

Conclusion: DuP-PO effectively mitigates the thinking trap, enhancing LRMs' efficiency and reasoning accuracy.

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems but face an
overthinking dilemma. When handling simple tasks, they often produce verbose
responses overloaded with thinking tokens (e.g., wait, however). These tokens
trigger unnecessary high-level reasoning behaviors like reflection and
backtracking, reducing efficiency. In this work, our pilot study reveals that
these thinking-token-induced behaviors are not essential for effective
problem-solving and may even hinder correct reasoning within constrained token
budgets. We identify this phenomenon as the thinking trap. To mitigate this
issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel
algorithm featuring: (1) A rollout sampling strategy that guarantees balanced
exposure to responses with and without thinking tokens; (2) A fine-grained
advantage control technique to dynamically regulate the prediction of target
tokens; (3) A policy shaping method ensuring stable gradient contributions from
thinking tokens. Experimental results on five popular math reasoning benchmarks
show that DuP-PO performs well on the popular LRM, which significantly improves
their token efficiency during reasoning, while achieving superior performance
of the base model.

</details>


### [68] [Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It](https://arxiv.org/abs/2506.23864)
*Seyed Mahed Mousavi,Edoardo Cecchinato,Lucia Hornikova,Giuseppe Riccardi*

Main category: cs.CL

TL;DR: The paper audits three reasoning benchmarks (SocialIQa, FauxPas-EAI, ToMi), revealing flaws in design and evaluation. Using LLMs, it identifies issues like duplicates, ambiguity, and flawed scoring. Human re-evaluation shows model scores improve due to wording, not reasoning, challenging benchmark validity.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of reasoning benchmarks and evaluate whether current LLM performance metrics truly reflect reasoning ability.

Method: Systematic audit of benchmarks using five LLMs (GPT-3, 3.5, 4, o1, LLaMA 3.1) and human annotation to identify flaws. Re-evaluation on cleaned subsets to analyze scoring sensitivity.

Result: Benchmarks contain structural, semantic, and pragmatic flaws. Model scores improve due to wording, not reasoning. Performance is sensitive to minor input variations.

Conclusion: Current benchmarks may not validly assess reasoning in LLMs. The paper calls for evaluation protocols focusing on inference processes and releases tools for better assessment.

Abstract: We conduct a systematic audit of three widely used reasoning benchmarks,
SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark
items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and
LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic
issues in benchmark design (e.g., duplicated items, ambiguous wording, and
implausible answers), as well as scoring procedures that prioritize output form
over reasoning process. Through systematic human annotation and re-evaluation
on cleaned benchmark subsets, we find that model scores often improve not due
to due to erratic surface wording variations and not to improved reasoning.
Infact, further analyses show that model performance is highly sensitive to
minor input variations such as context availability and phrasing, revealing
that high scores may reflect alignment with format-specific cues rather than
consistent inference based on the input. These findings challenge the validity
of current benchmark-based claims about reasoning in LLMs, and highlight the
need for evaluation protocols that assess reasoning as a process of drawing
inference from available information, rather than as static output selection.
We release audited data and evaluation tools to support more interpretable and
diagnostic assessments of model reasoning.

</details>


### [69] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)
*André de Souza Loureiro,Jorge Valverde-Rebaza,Julieta Noguez,David Escarcega,Ricardo Marcacini*

Main category: cs.CL

TL;DR: The paper introduces MAPS, a framework combining CoT, Self-Reflection, and Auto-Prompting to enhance multi-step reasoning in LLMs, outperforming standard methods and balancing cost-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-step reasoning tasks despite advancements. MAPS aims to improve this by integrating iterative refinement techniques.

Method: MAPS uses CoT prompting, adaptive self-reflection, and dynamic auto-prompting to iteratively refine reasoning and correct errors.

Result: MAPS outperforms standard CoT and matches specialized reasoning models on benchmarks, with controlled reflection depth for cost efficiency.

Conclusion: MAPS effectively enhances multi-step reasoning in LLMs, offering a balanced approach to performance and cost.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved their problem-solving capabilities. However, these models still
struggle when faced with complex multi-step reasoning tasks. In this paper, we
propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,
a novel approach designed to enhance multi-step mathematical reasoning in LLMs
by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and
Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an
iterative refinement process. Initially, the model generates a solution using
CoT prompting. When errors are detected, an adaptive self-reflection mechanism
identifies and analyzes them, generating tailored prompts to guide corrections.
These dynamically adjusted prompts enable the model to iteratively refine its
reasoning. Experiments on four well-established benchmarks across multiple LLMs
show that MAPS significantly outperforms standard CoT and achieves competitive
results with reasoning-optimized models. In addition, MAPS enables
general-purpose LLMs to reach performance levels comparable to specialized
reasoning models. While deeper reflection layers improve accuracy, they also
increase token usage and costs. To balance this trade-off, MAPS strategically
limits reflection depth, ensuring an optimal balance between cost and reasoning
performance.

</details>


### [70] [The Trilemma of Truth in Large Language Models](https://arxiv.org/abs/2506.23921)
*Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: The paper introduces sAwMIL, a method to assess the veracity of LLMs' internal knowledge, revealing insights about truth signals and probe performance.


<details>
  <summary>Details</summary>
Motivation: To address flawed assumptions in existing methods for probing LLM knowledge and provide a reliable way to verify what LLMs 'know.'

Method: sAwMIL, a probing method using multiple-instance learning and conformal prediction, evaluates LLM activations to classify statements as true, false, or neither.

Result: Key findings include concentration of veracity signals in specific LLM layers, asymmetry in truth/falsehood signals, and varying probe performance across LLM types.

Conclusion: sAwMIL offers a reliable approach to verify LLM knowledge, highlighting nuances in truth signals and probe effectiveness.

Abstract: We often attribute human characteristics to large language models (LLMs) and
claim that they "know" certain things. LLMs have an internal probabilistic
knowledge that represents information retained during training. How can we
assess the veracity of this knowledge? We examine two common methods for
probing the veracity of LLMs and discover several assumptions that are flawed.
To address these flawed assumptions, we introduce sAwMIL (short for Sparse
Aware Multiple-Instance Learning), a probing method that utilizes the internal
activations of LLMs to separate statements into true, false, and neither.
sAwMIL is based on multiple-instance learning and conformal prediction. We
evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including
both default and chat-based variants, as well as on 3 new datasets. Among the
insights we provide are: (1) the veracity signal is often concentrated in the
third quarter of an LLM's depth; (2) truth and falsehood signals are not always
symmetric; (3) linear probes perform better on chat models than on default
models; (4) nonlinear probes may be required to capture veracity signals for
some LLMs with reinforcement learning from human feedback or knowledge
distillation; and (5) LLMs capture a third type of signal that is distinct from
true and false and is neither true nor false. These findings provide a reliable
method for verifying what LLMs "know" and how certain they are of their
probabilistic internal knowledge.

</details>


### [71] [IMPACT: Inflectional Morphology Probes Across Complex Typologies](https://arxiv.org/abs/2506.23929)
*Mohammed J. Saeed,Tommi Vehvilainen,Evgeny Fedoseev,Sevil Caliskan,Tatiana Vodolazova*

Main category: cs.CL

TL;DR: The paper introduces IMPACT, a framework to evaluate LLMs' understanding of inflectional morphology in five morphologically rich languages, revealing gaps in their linguistic complexity handling.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs truly grasp linguistic complexity, especially in morphology, beyond just producing fluent outputs in non-English languages.

Method: IMPACT, a synthetic evaluation framework, tests LLMs on inflectional morphology in Arabic, Russian, Finnish, Turkish, and Hebrew, covering shared and language-specific features.

Result: Eight multilingual LLMs struggle with non-English languages and uncommon morphological patterns, particularly in judging ungrammatical examples. Chain of Thought and Thinking Models can degrade performance.

Conclusion: LLMs have significant gaps in handling linguistic complexity, highlighting the need for improvement. The IMPACT framework is released to aid further research.

Abstract: Large Language Models (LLMs) have shown significant progress on various
multilingual benchmarks and are increasingly used to generate and evaluate text
in non-English languages. However, while they may produce fluent outputs, it
remains unclear to what extent these models truly grasp the underlying
linguistic complexity of those languages, particularly in morphology. To
investigate this, we introduce IMPACT, a synthetically generated evaluation
framework focused on inflectional morphology, which we publicly release,
designed to evaluate LLM performance across five morphologically rich
languages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes
unit-test-style cases covering both shared and language-specific phenomena,
from basic verb inflections (e.g., tense, number, gender) to unique features
like Arabic's reverse gender agreement and vowel harmony in Finnish and
Turkish. We assess eight multilingual LLMs that, despite strong English
performance, struggle with other languages and uncommon morphological patterns,
especially when judging ungrammatical examples. We also show that Chain of
Thought and Thinking Models can degrade performance. Our work exposes gaps in
LLMs' handling of linguistic complexity, pointing to clear room for
improvement. To support further research, we publicly release the IMPACT
framework.

</details>


### [72] [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.23930)
*Ruhina Tabasshum Prome,Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.CL

TL;DR: The paper explores prompt engineering on LLMs for hate speech detection in low-resource languages, introducing metaphor prompting to bypass safety mechanisms. It evaluates six strategies on Llama2-7B and compares them with traditional embeddings and models.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of hate speech detection in low-resource languages due to dataset scarcity and leveraging LLMs to overcome this.

Method: Six prompting strategies (zero-shot, refusal suppression, flattering, multi-shot, role, metaphor) tested on Llama2-7B, compared with GloVe, Word2Vec, FastText on MLP, CNN, BiGRU. Extended to Hindi, English, German.

Result: Performance evaluated via F1 score and environmental impact (CO2, electricity, time). Metaphor prompting shows promise in bypassing LLM safety.

Conclusion: Metaphor prompting is innovative for low-resource hate speech detection, with potential applicability across languages and reduced reliance on large datasets.

Abstract: The rapid expansion of social media leads to a marked increase in hate
speech, which threatens personal lives and results in numerous hate crimes.
Detecting hate speech presents several challenges: diverse dialects, frequent
code-mixing, and the prevalence of misspelled words in user-generated content
on social media platforms. Recent progress in hate speech detection is
typically concentrated on high-resource languages. However, low-resource
languages still face significant challenges due to the lack of large-scale,
high-quality datasets. This paper investigates how we can overcome this
limitation via prompt engineering on large language models (LLMs) focusing on
low-resource Bengali language. We investigate six prompting strategies -
zero-shot prompting, refusal suppression, flattering the classifier, multi-shot
prompting, role prompting, and finally our innovative metaphor prompting to
detect hate speech effectively in low-resource languages. We pioneer the
metaphor prompting to circumvent the built-in safety mechanisms of LLMs that
marks a significant departure from existing jailbreaking methods. We
investigate all six different prompting strategies on the Llama2-7B model and
compare the results extensively with three pre-trained word embeddings - GloVe,
Word2Vec, and FastText for three different deep learning models - multilayer
perceptron (MLP), convolutional neural network (CNN), and bidirectional gated
recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in
the low-resource Bengali language, we also evaluate it in another low-resource
language - Hindi, and two high-resource languages - English and German. The
performance of all prompting techniques is evaluated using the F1 score, and
environmental impact factor (IF), which measures CO$_2$ emissions, electricity
usage, and computational time.

</details>


### [73] [Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs](https://arxiv.org/abs/2506.23940)
*Yang Dai,Jianxiang An,Tianwei Lin,Hongyang He,Hongzhe Huang,Wenqiao Zhang,Zheqi Lv,Siliang Tang,Yueting Zhuang*

Main category: cs.CL

TL;DR: A framework for unifying domain-specific MLLMs via parameter integration, using CAPS for selective fusion and domain compatibility scoring, validated across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the fragmentation of knowledge in domain-specific MLLMs and enabling modular composition of expert capabilities.

Method: Proposes Compatibility-Aware Parameter Splicing (CAPS) for selective parameter fusion, leveraging local and global signals, and introduces domain compatibility scoring.

Result: Effective integration of heterogeneous expertise with minimal overhead, validated across diverse multimodal benchmarks.

Conclusion: The framework offers a scalable solution for compositional, domain-adaptive MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have achieved success across various
domains. However, their applicability tends to degrade when confronted with
different types of data inputs, especially for MLLMs that have been fine-tuned
for specific tasks. Despite its importance, the study of knowledge sharing
among domain-specific MLLMs--such as those trained for mathematics or
code--remains largely underexplored. To address the fragmentation of knowledge
across domain-specialized MLLMs, we propose a unified parameter integration
framework that enables modular composition of expert capabilities. Our method
is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,
which leverages both local functional attribution and global
information-theoretic signals to guide selective parameter fusion. By extending
this mechanism to the low-rank adaptation layer granularity, we ensure
efficient integration with minimal inference overhead. Furthermore, we
introduce a domain compatibility scoring mechanism that quantifies inter-expert
alignment at the activation level and correlates with downstream task utility.
This principled fusion protocol allows the final model to synergize
heterogeneous expertise while preserving structural modularity. Extensive
evaluations across diverse multimodal benchmarks validate the effectiveness of
our framework, offering a scalable path toward compositional, domain-adaptive
MLLMs.

</details>


### [74] [Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders](https://arxiv.org/abs/2506.23951)
*Mathis Le Bail,Jérémie Dentan,Davide Buscaldi,Sonia Vanier*

Main category: cs.CL

TL;DR: The paper introduces a novel Sparse Autoencoder (SAE)-based architecture for sentence classification, improving interpretability and causality of extracted features compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To explore SAE-based explainability in sentence classification, a less-studied domain, and enhance interpretability of LLM internal representations.

Method: Proposes a tailored SAE architecture with a specialized classifier head and activation rate sparsity loss, benchmarked against ConceptShap, ICA, and other SAE techniques.

Result: The architecture outperforms existing methods in causality and interpretability, validated on two benchmarks and four Pythia LLMs.

Conclusion: The SAE-based approach effectively improves feature interpretability for sentence classification, supported by novel evaluation metrics.

Abstract: Sparse Autoencoders (SAEs) have been successfully used to probe Large
Language Models (LLMs) and extract interpretable concepts from their internal
representations. These concepts are linear combinations of neuron activations
that correspond to human-interpretable features. In this paper, we investigate
the effectiveness of SAE-based explainability approaches for sentence
classification, a domain where such methods have not been extensively explored.
We present a novel SAE-based architecture tailored for text classification,
leveraging a specialized classifier head and incorporating an activation rate
sparsity loss. We benchmark this architecture against established methods such
as ConceptShap, Independent Component Analysis, and other SAE-based concept
extraction techniques. Our evaluation covers two classification benchmarks and
four fine-tuned LLMs from the Pythia family. We further enrich our analysis
with two novel metrics for measuring the precision of concept-based
explanations, using an external sentence encoder. Our empirical results show
that our architecture improves both the causality and interpretability of the
extracted features.

</details>


### [75] [TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation](https://arxiv.org/abs/2506.23979)
*Renren Jin,Tianhao Shen,Xinwei Wu,Dan Shi,Haoran Sun,Wuwei Huang,Quandong Wang,Wei Liu,Jian Luan,Bin Wang,Deyi Xiong*

Main category: cs.CL

TL;DR: The TaP framework automates and scales preference dataset creation for LLMs across languages, improving performance over larger datasets.


<details>
  <summary>Details</summary>
Motivation: High-quality datasets for fine-tuning LLMs are scarce, especially in non-English languages, and resource-intensive to create.

Method: Proposes the TaP framework, using a structured taxonomy for automated, diverse, and comprehensive dataset generation.

Result: LLMs fine-tuned with TaP-generated datasets outperform those using larger open-source datasets.

Conclusion: TaP offers a scalable solution for high-quality dataset generation, enhancing LLM performance across languages.

Abstract: Conducting supervised fine-tuning and preference fine-tuning on large
language models (LLMs) requires high-quality datasets to improve their ability
to follow instructions and align with human preferences and values. However,
constructing such datasets is resource-intensive, and most available datasets
for supervised and preference fine-tuning are in English. To address these
challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided
\underline{\textbf{P}}reference Data Generation (TaP) framework, which
facilitates automated and scalable construction of preference datasets across
various languages. TaP is grounded in a structured taxonomy that allows
fine-grained control over dataset composition, thereby ensuring both diversity
and comprehensive coverage. We employ TaP-generated datasets to perform
supervised and preference fine-tuning on various LLMs. Experimental results
demonstrate that LLMs trained on TaP-generated datasets outperform those
trained on existing open-source datasets. Remarkably, LLMs trained on
TaP-generated datasets surpass the performance of those trained on an
open-source dataset that is 180 times larger.

</details>


### [76] [Machine Understanding of Scientific Language](https://arxiv.org/abs/2506.23990)
*Dustin Wright*

Main category: cs.CL

TL;DR: The paper focuses on developing tools and methods for machine understanding of scientific language to identify faithfulness in scientific texts, addressing challenges like automatic fact-checking and limited data learning.


<details>
  <summary>Details</summary>
Motivation: The proliferation of scientific texts online, some unfaithful to underlying science, necessitates automated tools to assess their faithfulness for societal benefit.

Method: The thesis contributes to NLP and ML with methods like automatic fact-checking, adversarial claim generation, domain adaptation, and zero-shot scientific fact-checking.

Result: New methods and resources are developed for tasks like detecting exaggerated claims and modeling information change, enabling effective learning from limited scientific text.

Conclusion: The research aids in identifying misinformative scientific statements and provides insights into science communication, demonstrating practical utility.

Abstract: Scientific information expresses human understanding of nature. This
knowledge is largely disseminated in different forms of text, including
scientific papers, news articles, and discourse among people on social media.
While important for accelerating our pursuit of knowledge, not all scientific
text is faithful to the underlying science. As the volume of this text has
burgeoned online in recent years, it has become a problem of societal
importance to be able to identify the faithfulness of a given piece of
scientific text automatically. This thesis is concerned with the cultivation of
datasets, methods, and tools for machine understanding of scientific language,
in order to analyze and understand science communication at scale. To arrive at
this, I present several contributions in three areas of natural language
processing and machine learning: automatic fact checking, learning with limited
data, and scientific text processing. These contributions include new methods
and resources for identifying check-worthy claims, adversarial claim
generation, multi-source domain adaptation, learning from crowd-sourced labels,
cite-worthiness detection, zero-shot scientific fact checking, detecting
exaggerated scientific claims, and modeling degrees of information change in
science communication. Critically, I demonstrate how the research outputs of
this thesis are useful for effectively learning from limited amounts of
scientific text in order to identify misinformative scientific statements and
generate new insights into the science communication process

</details>


### [77] [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/abs/2506.23998)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Andrew Well,Mia Markey,Ying Ding*

Main category: cs.CL

TL;DR: An automated LLM pipeline for thematic analysis of clinical narratives in CHD, reducing manual effort and improving scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional thematic analysis of CHD narratives is labor-intensive and unscalable, limiting patient-centered insights.

Method: A multi-agent LLM framework with optional RLHF for automated, scalable thematic analysis without manual coding.

Result: The system enables efficient, patient-centered analysis of large qualitative datasets, aligning with human analysis.

Conclusion: The proposed LLM pipeline offers a scalable solution for thematic analysis in CHD, enhancing clinical insights.

Abstract: Congenital heart disease (CHD) presents complex, lifelong challenges often
underrepresented in traditional clinical metrics. While unstructured narratives
offer rich insights into patient and caregiver experiences, manual thematic
analysis (TA) remains labor-intensive and unscalable. We propose a fully
automated large language model (LLM) pipeline that performs end-to-end TA on
clinical narratives, which eliminates the need for manual coding or full
transcript review. Our system employs a novel multi-agent framework, where
specialized LLM agents assume roles to enhance theme quality and alignment with
human analysis. To further improve thematic relevance, we optionally integrate
reinforcement learning from human feedback (RLHF). This supports scalable,
patient-centered analysis of large qualitative datasets and allows LLMs to be
fine-tuned for specific clinical contexts.

</details>


### [78] [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/abs/2506.24006)
*Anselm R. Strohmaier,Wim Van Dooren,Kathrin Seßler,Brian Greer,Lieven Verschaffel*

Main category: cs.CL

TL;DR: LLMs like ChatGPT excel at solving mathematical word problems but lack deeper understanding of real-world context, limiting their educational value.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of LLMs in supporting mathematics education, particularly in solving word problems, and assess their real-world contextual understanding.

Method: A scoping review with three parts: technical overview of LLMs vs. students, systematic review of word-problem corpora, and empirical evaluation of LLMs on word problems.

Result: LLMs perform well on s-problems (superficial) but struggle with real-world context, scoring perfectly on PISA problems but failing in non-sensical contexts.

Conclusion: LLMs solve word problems superficially without deeper understanding, reducing their effectiveness as instructional tools in math education.

Abstract: The progress of Large Language Models (LLMs) like ChatGPT raises the question
of how they can be integrated into education. One hope is that they can support
mathematics learning, including word-problem solving. Since LLMs can handle
textual input with ease, they appear well-suited for solving mathematical word
problems. Yet their real competence, whether they can make sense of the
real-world context, and the implications for classrooms remain unclear. We
conducted a scoping review from a mathematics-education perspective, including
three parts: a technical overview, a systematic review of word problems used in
research, and a state-of-the-art empirical evaluation of LLMs on mathematical
word problems. First, in the technical overview, we contrast the
conceptualization of word problems and their solution processes between LLMs
and students. In computer-science research this is typically labeled
mathematical reasoning, a term that does not align with usage in mathematics
education. Second, our literature review of 213 studies shows that the most
popular word-problem corpora are dominated by s-problems, which do not require
a consideration of realities of their real-world context. Finally, our
evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems
shows that most recent LLMs solve these s-problems with near-perfect accuracy,
including a perfect score on 20 problems from PISA. LLMs still showed
weaknesses in tackling problems where the real-world context is problematic or
non-sensical. In sum, we argue based on all three aspects that LLMs have
mastered a superficial solution process but do not make sense of word problems,
which potentially limits their value as instructional tools in mathematics
classrooms.

</details>


### [79] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)
*Hyunjong Kim,Sangyeop Kim,Jongheon Jeong,Yeongjae Cho,Sungzoon Cho*

Main category: cs.CL

TL;DR: EXPERT is a reference-free evaluation metric for image captioning, providing structured explanations based on fluency, relevance, and descriptiveness, validated by human evaluation.


<details>
  <summary>Details</summary>
Motivation: Current explainable metrics lack standardized criteria and verified explanation quality.

Method: Develops a two-stage evaluation template using large-scale datasets to supervise a vision-language model for scoring and explanation generation.

Result: Achieves state-of-the-art performance on benchmarks with higher-quality explanations.

Conclusion: EXPERT offers a reliable, structured approach for evaluating image captions, with publicly available code and datasets.

Abstract: Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


### [80] [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)
*Ian R. McKenzie,Oskar J. Hollinsworth,Tom Tseng,Xander Davies,Stephen Casper,Aaron D. Tucker,Robert Kirk,Adam Gleave*

Main category: cs.CL

TL;DR: The paper evaluates AI defense pipelines, proposing a novel classifier and a staged attack method (STACK), revealing vulnerabilities and suggesting mitigations.


<details>
  <summary>Details</summary>
Motivation: To assess the security of AI defense pipelines, which lack prior evaluation, by developing and testing an open-source pipeline.

Method: Developed a few-shot-prompted classifier and introduced STACK, a staged attack procedure, to test pipeline vulnerabilities.

Result: The classifier outperformed ShieldGemma, reducing attack success to 0%, but STACK achieved 71% ASR in black-box and 33% in transfer settings.

Conclusion: AI defense pipelines are vulnerable to staged attacks; specific mitigations are needed to enhance security.

Abstract: Frontier AI developers are relying on layers of safeguards to protect against
catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus
model using one such defense pipeline, and other frontier developers including
Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the
security of such pipelines is unclear, with limited prior work evaluating or
attacking these pipelines. We address this gap by developing and red-teaming an
open-source defense pipeline. First, we find that a novel few-shot-prompted
input and output classifier outperforms state-of-the-art open-weight safeguard
model ShieldGemma across three attacks and two datasets, reducing the attack
success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,
we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on
ClearHarm in a black-box attack against the few-shot-prompted classifier
pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%
ASR, providing initial evidence that it is feasible to design attacks with no
access to the target pipeline. We conclude by suggesting specific mitigations
that developers could use to thwart staged attacks.

</details>


### [81] [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/abs/2506.24106)
*Yanhong Li,Ming Li,Karen Livescu,Jiawei Zhou*

Main category: cs.CL

TL;DR: A language model's text prediction ability is tied to its embedding space breadth, with higher representation dispersion (cosine distance among hidden vectors) correlating with lower perplexity. Dispersion aids in model selection, layer identification for retrieval, and training improvements.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between a language model's embedding space dispersion and its perplexity, and to leverage this for practical tasks like model selection and training optimization.

Method: Analyzed representation dispersion across diverse models (LLaMA, Qwen) and domains, used dispersion for model selection, layer identification for retrieval (kNN-LM), and integrated a push-away objective in training.

Result: Higher dispersion strongly correlates with lower perplexity. Dispersion aids in predicting downstream accuracy, optimizing retrieval layers, and improving training outcomes.

Conclusion: Dispersion in embedding space is a key factor in model performance, offering practical benefits for model selection, retrieval, and training without labeled data.

Abstract: We show that a language model's ability to predict text is tightly linked to
the breadth of its embedding space: models that spread their contextual
representations more widely tend to achieve lower perplexity. Concretely, we
find that representation dispersion - the average pairwise cosine distance
among hidden vectors - strongly and negatively correlates with perplexity
across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,
news, scientific abstracts). Beyond illustrating this link, we show how
dispersion can be leveraged for a range of practical tasks without requiring
labeled data. First, measuring dispersion on unlabeled text allows us to
predict downstream accuracy in new domains, offering a data-efficient tool for
model selection. Next, we find that identifying layers with higher dispersion
pinpoints the best representations for retrieval-based methods such as kNN-LM,
bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple
push-away objective into training, which increases dispersion in both
single-domain and cross-domain scenarios and directly improves perplexity in
each.

</details>


### [82] [Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models](https://arxiv.org/abs/2506.24117)
*David M. Smiley*

Main category: cs.CL

TL;DR: The study evaluates transformer-based models (E5, AlephBERT, MPNet, LaBSE) for detecting parallel passages in the Hebrew Bible, finding E5 and AlephBERT most effective.


<details>
  <summary>Details</summary>
Motivation: Traditional manual comparison of biblical Hebrew passages is labor-intensive and error-prone, prompting the need for automated methods.

Method: Used pre-trained models to generate word embeddings and measured similarity with cosine similarity and Wasserstein Distance.

Result: E5 excelled in detecting parallels, while AlephBERT better differentiated non-parallel passages.

Conclusion: Pre-trained models can improve efficiency and accuracy in identifying intertextual parallels, with potential for broader ancient language studies.

Abstract: Identifying parallel passages in biblical Hebrew is foundational in biblical
scholarship for uncovering intertextual relationships. Traditional methods rely
on manual comparison, which is labor-intensive and prone to human error. This
study evaluates the potential of pre-trained transformer-based language models,
including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in
the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings
and Chronicles, I assessed each model's capability to generate word embeddings
that delineate parallel from non-parallel passages. Utilizing cosine similarity
and Wasserstein Distance measures, I found that E5 and AlephBERT show
significant promise, with E5 excelling in parallel detection and AlephBERT
demonstrating stronger non-parallel differentiation. These findings indicate
that pre-trained models can enhance the efficiency and accuracy of detecting
intertextual parallels in ancient texts, suggesting broader applications for
ancient language studies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [83] [Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring](https://arxiv.org/abs/2506.22437)
*Xinxin Sun,Peter Chang*

Main category: cs.CV

TL;DR: A physics-informed alignment framework improves crack detection in SHM by adapting KAZE architecture, outperforming traditional methods like SIFT and SURF with up to 70-90% error reduction.


<details>
  <summary>Details</summary>
Motivation: Traditional feature detectors (SIFT, SURF) and lightweight alternatives (ORB, BRISK) fail in thin crack localization due to high-frequency edge suppression or poor repeatability. Real-world SHM challenges like distortion, occlusion, and low contrast necessitate a better solution.

Method: The framework uses nonlinear anisotropic diffusion for crack-preserving scale space and RANSAC-based homography estimation, requiring no training or calibration. Validated on smartphone-captured masonry and concrete images under field conditions.

Result: Reduces crack area and spine length errors by up to 70% and 90%, respectively, with sub-5% alignment error.

Conclusion: The unsupervised, interpretable, and lightweight approach offers a robust alternative for tracking crack evolution, scalable for UAVs and mobile platforms.

Abstract: Accurate image alignment is essential for monitoring crack evolution in
structural health monitoring (SHM), particularly under real-world conditions
involving perspective distortion, occlusion, and low contrast. However,
traditional feature detectors such as SIFT and SURF, which rely on
Gaussian-based scale spaces, tend to suppress high-frequency edges, making them
unsuitable for thin crack localization. Lightweight binary alternatives like
ORB and BRISK, while computationally efficient, often suffer from poor keypoint
repeatability on textured or shadowed surfaces. This study presents a
physics-informed alignment framework that adapts the open KAZE architecture to
SHM-specific challenges. By utilizing nonlinear anisotropic diffusion to
construct a crack-preserving scale space, and integrating RANSAC-based
homography estimation, the framework enables accurate geometric correction
without the need for training, parameter tuning, or prior calibration. The
method is validated on time-lapse images of masonry and concrete acquired via
handheld smartphone under varied field conditions, including shadow
interference, cropping, oblique viewing angles, and surface clutter. Compared
to classical detectors, the proposed framework reduces crack area and spine
length errors by up to 70 percent and 90 percent, respectively, while
maintaining sub-5 percent alignment error in key metrics. Unsupervised,
interpretable, and computationally lightweight, this approach supports scalable
deployment via UAVs and mobile platforms. By tailoring nonlinear scale-space
modeling to SHM image alignment, this work offers a robust and physically
grounded alternative to conventional techniques for tracking real-world crack
evolution.

</details>


### [84] [Counting with Confidence: Accurate Pest Monitoring in Water Traps](https://arxiv.org/abs/2506.22438)
*Xumin Gao,Mark Stevens,Grzegorz Cielniak*

Main category: cs.CV

TL;DR: The paper proposes a method to evaluate pest counting confidence by combining counting results and environmental factors, improving accuracy over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Existing pest counting models lack reliability assessment in real-world deployments due to missing ground truth, necessitating a confidence evaluation method.

Method: Uses a pest detection network, image quality/complexity assessments, pest distribution uniformity analysis, and a regression model to predict counting confidence.

Result: Reduces MSE by 31.7% and improves R2 by 15.2% on pest counting confidence compared to baseline.

Conclusion: The study introduces the first comprehensive method for evaluating pest counting confidence, enhancing decision-making in precision agriculture.

Abstract: Accurate pest population monitoring and tracking their dynamic changes are
crucial for precision agriculture decision-making. A common limitation in
existing vision-based automatic pest counting research is that models are
typically evaluated on datasets with ground truth but deployed in real-world
scenarios without assessing the reliability of counting results due to the lack
of ground truth. To this end, this paper proposed a method for comprehensively
evaluating pest counting confidence in the image, based on information related
to counting results and external environmental conditions. First, a pest
detection network is used for pest detection and counting, extracting counting
result-related information. Then, the pest images undergo image quality
assessment, image complexity assessment, and pest distribution uniformity
assessment. And the changes in image clarity caused by stirring during image
acquisition are quantified by calculating the average gradient magnitude.
Notably, we designed a hypothesis-driven multi-factor sensitivity analysis
method to select the optimal image quality assessment and image complexity
assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for
pest distribution uniformity assessment. Finally, the obtained information
related to counting results and external environmental conditions is input into
a regression model for prediction, resulting in the final pest counting
confidence. To the best of our knowledge, this is the first study dedicated to
comprehensively evaluating counting confidence in counting tasks, and
quantifying the relationship between influencing factors and counting
confidence through a model. Experimental results show our method reduces MSE by
31.7% and improves R2 by 15.2% on the pest counting confidence test set,
compared to the baseline built primarily on information related to counting
results.

</details>


### [85] [Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization](https://arxiv.org/abs/2506.22463)
*Weizhi Gao,Zhichao Hou,Junqi Yin,Feiyi Wang,Linyu Peng,Xiaorui Liu*

Main category: cs.CV

TL;DR: MoDiff accelerates diffusion models via modulated quantization and error compensation, reducing computation cost without quality loss.


<details>
  <summary>Details</summary>
Motivation: High computation cost in iterative sampling of diffusion models limits their practicality.

Method: Introduces Modulated Diffusion (MoDiff), combining modulated quantization and error compensation to accelerate diffusion models.

Result: Reduces activation quantization from 8 to 3 bits without performance degradation, validated on CIFAR-10 and LSUN.

Conclusion: MoDiff is a general, efficient framework for accelerating diffusion models, supported by theory and experiments.

Abstract: Diffusion models have emerged as powerful generative models, but their high
computation cost in iterative sampling remains a significant bottleneck. In
this work, we present an in-depth and insightful study of state-of-the-art
acceleration techniques for diffusion models, including caching and
quantization, revealing their limitations in computation error and generation
quality. To break these limits, this work introduces Modulated Diffusion
(MoDiff), an innovative, rigorous, and principled framework that accelerates
generative modeling through modulated quantization and error compensation.
MoDiff not only inherents the advantages of existing caching and quantization
methods but also serves as a general framework to accelerate all diffusion
models. The advantages of MoDiff are supported by solid theoretical insight and
analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate
that MoDiff significant reduces activation quantization from 8 bits to 3 bits
without performance degradation in post-training quantization (PTQ). Our code
implementation is available at https://github.com/WeizhiGao/MoDiff.

</details>


### [86] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/abs/2506.22498)
*Hao Liu,Yu Hu,Rakiba Rayhana,Ling Bai,Zheng Liu*

Main category: cs.CV

TL;DR: The paper proposes ViFusionTST, a dual-stream Swin Transformer model, to predict bed-exit intent early using low-cost load cells under bed legs, achieving high accuracy and F1 scores on real-world data.


<details>
  <summary>Details</summary>
Motivation: Bed-related falls are a major injury source in healthcare settings, and existing alarms often trigger too late. Early prediction of bed-exit intent can prevent falls.

Method: Uses four load cells to capture signals, converts them into complementary images (RGB line plot and texture maps), and processes them with ViFusionTST, a dual-stream Swin Transformer with cross-attention fusion.

Result: Achieves 0.885 accuracy and 0.794 F1 score on a real-world dataset, outperforming other time-series baselines.

Conclusion: Image-based fusion of load-sensor signals is effective for real-time, privacy-preserving fall prevention.

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [87] [Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data](https://arxiv.org/abs/2506.22499)
*Jiachao Liu,Pablo Guarda,Koichiro Niinuma,Sean Qian*

Main category: cs.CV

TL;DR: A novel framework integrates satellite imagery with traditional traffic data for dynamic origin-destination demand estimation, improving accuracy and scalability, especially in areas lacking local sensors.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of sparse local traffic sensors by leveraging city-wide satellite imagery for comprehensive traffic data.

Method: Combines computer vision for vehicle detection and map matching with a computational graph-based model to calibrate dynamic network states.

Result: Satellite-derived data significantly enhances estimation accuracy, particularly for unsensed links, and scales well in real-world networks.

Conclusion: The framework is practical for cities of varying sizes, with sensitivity to satellite data quality.

Abstract: This study presents a novel integrated framework for dynamic
origin-destination demand estimation (DODE) in multi-class mesoscopic network
models, leveraging high-resolution satellite imagery together with conventional
traffic data from local sensors. Unlike sparse local detectors, satellite
imagery offers consistent, city-wide road and traffic information of both
parking and moving vehicles, overcoming data availability limitations. To
extract information from imagery data, we design a computer vision pipeline for
class-specific vehicle detection and map matching, generating link-level
traffic density observations by vehicle class. Building upon this information,
we formulate a computational graph-based DODE model that calibrates dynamic
network states by jointly matching observed traffic counts and travel times
from local sensors with density measurements derived from satellite imagery. To
assess the accuracy and scalability of the proposed framework, we conduct a
series of numerical experiments using both synthetic and real-world data. The
results of out-of-sample tests demonstrate that supplementing traditional data
with satellite-derived density significantly improves estimation performance,
especially for links without local sensors. Real-world experiments also confirm
the framework's capability to handle large-scale networks, supporting its
potential for practical deployment in cities of varying sizes. Sensitivity
analysis further evaluates the impact of data quality related to satellite
imagery data.

</details>


### [88] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/abs/2506.22500)
*Weiyi Zhao,Xiaoyu Tan,Liang Liu,Sijia Li,Youwei Song,Xihe Qiu*

Main category: cs.CV

TL;DR: A dataset (OR-VSKC) of synthetic and annotated images addresses visual-semantic knowledge conflicts in MLLMs for surgical risk detection, improving performance on trained entities but not untrained ones.


<details>
  <summary>Details</summary>
Motivation: To improve automated operating room risk detection by addressing visual-semantic knowledge conflicts in multimodal large language models (MLLMs).

Method: Created a dataset of 34,000 synthetic images and 214 human-annotated images depicting safety rule violations, then fine-tuned MLLMs on this dataset.

Result: Fine-tuning improved detection of trained conflict entities and generalized to new viewpoints, but performance on untrained entities remained poor.

Conclusion: The OR-VSKC dataset and methodology help expose and study VS-KC, but comprehensive training is needed for broader applicability.

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [89] [How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?](https://arxiv.org/abs/2506.22501)
*Gautam Siddharth Kashyap,Manaswi Kulahara,Nipun Joshi,Usman Naseem*

Main category: cs.CV

TL;DR: SpatialNet-ViT, a novel model combining Vision Transformers and Multi-Task Learning, improves remote sensing classification accuracy and scalability by integrating spatial awareness and contextual understanding.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on narrow tasks or datasets, limiting generalization across remote sensing classification challenges.

Method: Proposes SpatialNet-ViT, leveraging Vision Transformers and Multi-Task Learning, with techniques like data augmentation, transfer learning, and multi-task learning.

Result: Improved classification accuracy and scalability, with enhanced robustness and generalization across diverse datasets.

Conclusion: SpatialNet-ViT effectively addresses limitations of narrow-focused studies, offering a scalable and accurate solution for remote sensing classification tasks.

Abstract: Remote sensing datasets offer significant promise for tackling key
classification tasks such as land-use categorization, object presence
detection, and rural/urban classification. However, many existing studies tend
to focus on narrow tasks or datasets, which limits their ability to generalize
across various remote sensing classification challenges. To overcome this, we
propose a novel model, SpatialNet-ViT, leveraging the power of Vision
Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach
combines spatial awareness with contextual understanding, improving both
classification accuracy and scalability. Additionally, techniques like data
augmentation, transfer learning, and multi-task learning are employed to
enhance model robustness and its ability to generalize across diverse datasets

</details>


### [90] [What Makes a Dribble Successful? Insights From 3D Pose Tracking Data](https://arxiv.org/abs/2506.22503)
*Michiel Schepers,Pieter Robberechts,Jan Van Haaren,Jesse Davis*

Main category: cs.CV

TL;DR: The study uses 3D pose tracking data to enhance dribble evaluation in soccer, showing improved predictive performance over traditional 2D methods.


<details>
  <summary>Details</summary>
Motivation: Current 2D positional data lacks depth in capturing dribbling aspects like balance and orientation, limiting insights.

Method: Pose-based features from 1,736 dribbles in the 2022/23 Champions League were analyzed to assess their impact on dribble success.

Result: Balance and attacker-defender orientation alignment were key predictors, improving model performance when combined with 2D data.

Conclusion: 3D pose tracking offers deeper insights into dribbling skills, enhancing performance evaluation in soccer.

Abstract: Data analysis plays an increasingly important role in soccer, offering new
ways to evaluate individual and team performance. One specific application is
the evaluation of dribbles: one-on-one situations where an attacker attempts to
bypass a defender with the ball. While previous research has primarily relied
on 2D positional tracking data, this fails to capture aspects like balance,
orientation, and ball control, limiting the depth of current insights. This
study explores how pose tracking data (capturing players' posture and movement
in three dimensions) can improve our understanding of dribbling skills. We
extract novel pose-based features from 1,736 dribbles in the 2022/23 Champions
League season and evaluate their impact on dribble success. Our results
indicate that features capturing the attacker's balance and the alignment of
the orientation between the attacker and defender are informative for
predicting dribble success. Incorporating these pose-based features on top of
features derived from traditional 2D positional data leads to a measurable
improvement in model performance.

</details>


### [91] [Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection](https://arxiv.org/abs/2506.22504)
*Hassan Baker,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: Patch2Loc is an unsupervised method for detecting brain lesions in MRI by learning from normal patches and identifying abnormalities through prediction errors.


<details>
  <summary>Details</summary>
Motivation: Radiologists need computer-aided diagnostics for lesion detection, but supervised methods require annotated data. Patch2Loc offers an unsupervised alternative.

Method: Train a neural network to map normal patches to their spatial locations. Detect abnormalities via higher prediction errors/variance, generating a heatmap for segmentation.

Result: Outperforms state-of-the-art unsupervised segmentation on BraTS2021, MSLUB, ATLAS, and WMH datasets.

Conclusion: Patch2Loc is effective for unsupervised brain lesion segmentation, providing a practical tool for radiologists.

Abstract: Detecting brain lesions as abnormalities observed in magnetic resonance
imaging (MRI) is essential for diagnosis and treatment. In the search of
abnormalities, such as tumors and malformations, radiologists may benefit from
computer-aided diagnostics that use computer vision systems trained with
machine learning to segment normal tissue from abnormal brain tissue. While
supervised learning methods require annotated lesions, we propose a new
unsupervised approach (Patch2Loc) that learns from normal patches taken from
structural MRI. We train a neural network model to map a patch back to its
spatial location within a slice of the brain volume. During inference, abnormal
patches are detected by the relatively higher error and/or variance of the
location prediction. This generates a heatmap that can be integrated into
pixel-wise methods to achieve finer-grained segmentation. We demonstrate the
ability of our model to segment abnormal brain tissues by applying our approach
to the detection of tumor tissues in MRI on T2-weighted images from BraTS2021
and MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show
that it outperforms the state-of-the art in unsupervised segmentation. The
codebase for this work can be found on our
\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.

</details>


### [92] [Weakly Supervised Object Segmentation by Background Conditional Divergence](https://arxiv.org/abs/2506.22505)
*Hassan Baker,Matthew S. Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: The paper proposes a weakly supervised method for binary object segmentation using image-wise labels, leveraging counterfactual backgrounds and clustering to improve performance.


<details>
  <summary>Details</summary>
Motivation: Object segmentation is challenging in specialized domains with limited labeled data. Pixel-wise masks are expensive, so the work explores weak supervision (image-wise labels) as a faster alternative.

Method: The method trains a masking network using weak supervision (image-wise labels). It creates counterfactual images by blending segmented objects into clustered backgrounds and uses divergence and supervised loss for training.

Result: The approach outperforms unsupervised baselines on sonar images and shows reasonable performance on natural images without relying on pretrained or generative networks.

Conclusion: The method effectively leverages weak supervision and counterfactual backgrounds for object segmentation, demonstrating success in specialized and natural image domains.

Abstract: As a computer vision task, automatic object segmentation remains challenging
in specialized image domains without massive labeled data, such as synthetic
aperture sonar images, remote sensing, biomedical imaging, etc. In any domain,
obtaining pixel-wise segmentation masks is expensive. In this work, we propose
a method for training a masking network to perform binary object segmentation
using weak supervision in the form of image-wise presence or absence of an
object of interest, which provides less information but may be obtained more
quickly from manual or automatic labeling. A key step in our method is that the
segmented objects can be placed into background-only images to create
realistic, images of the objects with counterfactual backgrounds. To create a
contrast between the original and counterfactual background images, we propose
to first cluster the background-only images, and then during learning create
counterfactual images that blend objects segmented from their original source
backgrounds to backgrounds chosen from a targeted cluster. One term in the
training loss is the divergence between these counterfactual images and the
real object images with backgrounds of the target cluster. The other term is a
supervised loss for background-only images. While an adversarial critic could
provide the divergence, we use sample-based divergences. We conduct experiments
on side-scan and synthetic aperture sonar in which our approach succeeds
compared to previous unsupervised segmentation baselines that were only tested
on natural images. Furthermore, to show generality we extend our experiments to
natural images, obtaining reasonable performance with our method that avoids
pretrained networks, generative networks, and adversarial critics. The basecode
for this work can be found at
\href{GitHub}{https://github.com/bakerhassan/WSOS}.

</details>


### [93] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/abs/2506.22509)
*Hang Xu,Jie Huang,Linjiang Huang,Dong Li,Yidi Liu,Feng Zhao*

Main category: cs.CV

TL;DR: The paper proposes a training-free Domain Noise Alignment (DNA) method for Diffusion-based Dense Prediction (DDP) models to achieve domain adaptation by aligning noise statistics between domains.


<details>
  <summary>Details</summary>
Motivation: The exposure bias in diffusion models causes domain shift, and noise prediction statistics can capture domain differences, motivating a training-free solution.

Method: The DNA approach aligns noise statistics between source and target domains during diffusion sampling, using high-confidence regions for source-free DA.

Result: The method effectively enhances domain adaptation for DDP models across four dense prediction tasks.

Conclusion: DNA provides a training-free, effective solution for domain adaptation in DDP models, leveraging noise statistics alignment.

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which
enhances the dense prediction model's performance when tested on its unseen
domain. Recently, with the development of Diffusion-based Dense Prediction
(DDP) models, the exploration of DA designs tailored to this framework is worth
exploring, since the diffusion model is effective in modeling the distribution
transformation that comprises domain information. In this work, we propose a
training-free mechanism for DDP frameworks, endowing them with DA capabilities.
Our motivation arises from the observation that the exposure bias (e.g., noise
statistics bias) in diffusion brings domain shift, and different domains in
conditions of DDP models can also be effectively captured by the noise
prediction statistics. Based on this, we propose a training-free Domain Noise
Alignment (DNA) approach, which alleviates the variations of noise statistics
to domain changes during the diffusion sampling process, thereby achieving
domain adaptation. Specifically, when the source domain is available, we
directly adopt the DNA method to achieve domain adaptation by aligning the
noise statistics of the target domain with those of the source domain. For the
more challenging source-free DA, inspired by the observation that regions
closer to the source domain exhibit higher confidence meeting variations of
sampling noise, we utilize the statistics from the high-confidence regions
progressively to guide the noise statistic adjustment during the sampling
process. Notably, our method demonstrates the effectiveness of enhancing the DA
capability of DDP models across four common dense prediction tasks. Code is
available at
\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [94] [Lightning the Night with Generative Artificial Intelligence](https://arxiv.org/abs/2506.22511)
*Tingting Zhou,Feng Zhang,Haoyang Fu,Baoxiang Pan,Renhe Zhang,Feng Lu,Zhixin Yang*

Main category: cs.CV

TL;DR: The study introduces RefDiff, a generative diffusion model, to retrieve visible light reflectance at night using thermal infrared data, improving accuracy and enabling uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: The lack of visible light at night limits continuous weather monitoring. This study aims to overcome this by retrieving visible light reflectance from thermal infrared data.

Method: Developed RefDiff, a generative diffusion model, using multi-band thermal infrared data from FY4B's AGRI for nighttime visible light reflectance retrieval.

Result: RefDiff achieves high accuracy (SSIM 0.90), especially in complex cloud areas, and performs comparably to daytime models when validated with VIIRS data.

Conclusion: RefDiff advances nighttime visible light reflectance retrieval, expanding the potential applications of nighttime visible light data.

Abstract: The visible light reflectance data from geostationary satellites is crucial
for meteorological observations and plays an important role in weather
monitoring and forecasting. However, due to the lack of visible light at night,
it is impossible to conduct continuous all-day weather observations using
visible light reflectance data. This study pioneers the use of generative
diffusion models to address this limitation. Based on the multi-band thermal
infrared brightness temperature data from the Advanced Geostationary Radiation
Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we
developed a high-precision visible light reflectance retrieval model, called
Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m},
0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance
retrieval at night. Compared to the classical models, RefDiff not only
significantly improves accuracy through ensemble averaging but also provides
uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,
with particularly significant improvements in areas with complex cloud
structures and thick clouds. The model's nighttime retrieval capability was
validated using VIIRS nighttime product, demonstrating comparable performance
to its daytime counterpart. In summary, this research has made substantial
progress in the ability to retrieve visible light reflectance at night, with
the potential to expand the application of nighttime visible light data.

</details>


### [95] [Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence](https://arxiv.org/abs/2506.22513)
*Aditya Sharma*

Main category: cs.CV

TL;DR: An automated framework for fault detection in radiography using a modified U-net model achieves high defect awareness and efficiency, validated by NDE measurements and professional evaluations.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of sufficiently explained information in radiography and aims to optimize virtual defect augmentation for fault detection in airplane welds.

Method: The method involves compiling 223 CR photographs, using data augmentation (virtual and standard), and training a modified U-net model for semantic fault segmentation.

Result: The framework shows exceptional defect detection awareness (tiny a90/95 characteristics) and efficiency, with the combined augmentation approach outperforming others.

Conclusion: The framework is viable for large-scale radiography analysis, validated by professional evaluations, and holds promise as a support tool in testing cycles.

Abstract: This investigation attempts to create an automated framework for fault
detection and organization for usage in contemporary radiography, as per NDE
4.0. The review's goals are to address the lack of information that is
sufficiently explained, learn how to make the most of virtual defect increase,
and determine whether the framework is viable by using NDE measurements. As its
basic information source, the technique consists of compiling and categorizing
223 CR photographs of airplane welds. Information expansion systems, such as
virtual defect increase and standard increase, are used to work on the
preparation dataset. A modified U-net model is prepared using the improved data
to produce semantic fault division veils. To assess the effectiveness of the
model, NDE boundaries such as Case, estimating exactness, and misleading call
rate are used. Tiny a90/95 characteristics, which provide strong
differentiating evidence of flaws, reveal that the suggested approach achieves
exceptional awareness in defect detection. Considering a 90/95, size error, and
fake call rate in the weld area, the consolidated expansion approach clearly
wins. Due to the framework's fast derivation speed, large images can be broken
down efficiently and quickly. Professional controllers evaluate the transmitted
system in the field and believe that it has a guarantee as a support device in
the testing cycle, irrespective of particular equipment cut-off points and
programming resemblance.

</details>


### [96] [Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis](https://arxiv.org/abs/2506.22517)
*Subhadip Kumar*

Main category: cs.CV

TL;DR: The paper compares three computer vision models (Yolov11, Yolov12, RF-DETR) for detecting container damage, finding RF-DETR superior for uncommon cases despite lower mAP scores.


<details>
  <summary>Details</summary>
Motivation: Timely detection of container damage is crucial for safety and logistics efficiency, but current methods may miss uncommon damage types.

Method: Three models (Yolov11, Yolov12, RF-DETR) were trained and tested on 278 annotated images, evaluating mAP and precision.

Result: Yolov11 and Yolov12 had higher mAP@50 (81.9%) than RF-DETR (77.7%), but RF-DETR outperformed in detecting uncommon damage.

Conclusion: RF-DETR is better suited for detecting varied container damage, especially uncommon cases, despite lower overall mAP.

Abstract: Containers are an integral part of the logistics industry and act as a
barrier for cargo. A typical service life for a container is more than 20
years. However, overtime containers suffer various types of damage due to the
mechanical as well as natural factors. A damaged container is a safety hazard
for the employees handling it and a liability for the logistic company.
Therefore, a timely inspection and detection of the damaged container is a key
for prolonging service life as well as avoiding safety hazards. In this paper,
we will compare the performance of the damage detection by three
state-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.
We will use a dataset of 278 annotated images to train, validate and test the
model. We will compare the mAP and precision of the model. The objective of
this paper is to identify the model that is best suited for container damage
detection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%
compared to RF-DETR, which was 77.7%. However, while testing the model for
not-so-common damaged containers, the RF-DETR model outperformed the others
overall, exhibiting superiority to accurately detecting both damaged containers
as well as damage occurrences with high confidence.

</details>


### [97] [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/abs/2506.22531)
*Prasen Kumar Sharma,Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: A novel method, Preserve Anything, improves object preservation and semantic consistency in text-to-image generation using an N-channel ControlNet, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in existing T2I methods, such as poor object fidelity, semantic misalignment, and lack of scene control.

Method: Uses an N-channel ControlNet with object preservation, background guidance, and high-frequency overlay modules. Introduces a benchmark dataset for evaluation.

Result: Achieves FID 15.26 and CLIP-S 32.85, with significant improvements in prompt alignment, photorealism, and artifact reduction.

Conclusion: The method outperforms existing works, validated by empirical results and user studies.

Abstract: We introduce \textit{Preserve Anything}, a novel method for controlled image
synthesis that addresses key limitations in object preservation and semantic
consistency in text-to-image (T2I) generation. Existing approaches often fail
(i) to preserve multiple objects with fidelity, (ii) maintain semantic
alignment with prompts, or (iii) provide explicit control over scene
composition. To overcome these challenges, the proposed method employs an
N-channel ControlNet that integrates (i) object preservation with size and
placement agnosticism, color and detail retention, and artifact elimination,
(ii) high-resolution, semantically consistent backgrounds with accurate
shadows, lighting, and prompt adherence, and (iii) explicit user control over
background layouts and lighting conditions. Key components of our framework
include object preservation and background guidance modules, enforcing lighting
consistency and a high-frequency overlay module to retain fine details while
mitigating unwanted artifacts. We introduce a benchmark dataset consisting of
240K natural images filtered for aesthetic quality and 18K 3D-rendered
synthetic images with metadata such as lighting, camera angles, and object
relationships. This dataset addresses the deficiencies of existing benchmarks
and allows a complete evaluation. Empirical results demonstrate that our method
achieves state-of-the-art performance, significantly improving feature-space
fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining
competitive aesthetic quality. We also conducted a user study to demonstrate
the efficacy of the proposed work on unseen benchmark and observed a remarkable
improvement of $\sim25\%$, $\sim19\%$, $\sim13\%$, and $\sim14\%$ in terms of
prompt alignment, photorealism, the presence of AI artifacts, and natural
aesthetics over existing works.

</details>


### [98] [Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset](https://arxiv.org/abs/2506.22554)
*Vasu Agrawal,Akinniyi Akinyemi,Kathryn Alvero,Morteza Behrooz,Julia Buffalini,Fabio Maria Carlucci,Joy Chen,Junming Chen,Zhang Chen,Shiyang Cheng,Praveen Chowdary,Joe Chuang,Antony D'Avirro,Jon Daly,Ning Dong,Mark Duppenthaler,Cynthia Gao,Jeff Girard,Martin Gleize,Sahir Gomez,Hongyu Gong,Srivathsan Govindarajan,Brandon Han,Sen He,Denise Hernandez,Yordan Hristov,Rongjie Huang,Hirofumi Inaguma,Somya Jain,Raj Janardhan,Qingyao Jia,Christopher Klaiber,Dejan Kovachev,Moneish Kumar,Hang Li,Yilei Li,Pavel Litvin,Wei Liu,Guangyao Ma,Jing Ma,Martin Ma,Xutai Ma,Lucas Mantovani,Sagar Miglani,Sreyas Mohan,Louis-Philippe Morency,Evonne Ng,Kam-Woh Ng,Tu Anh Nguyen,Amia Oberai,Benjamin Peloquin,Juan Pino,Jovan Popovic,Omid Poursaeed,Fabian Prada,Alice Rakotoarison,Alexander Richard,Christophe Ropers,Safiyyah Saleem,Vasu Sharma,Alex Shcherbyna,Jia Shen,Jie Shen,Anastasis Stathopoulos,Anna Sun,Paden Tomasello,Tuan Tran,Arina Turkatenko,Bo Wan,Chao Wang,Jeff Wang,Mary Williamson,Carleigh Wood,Tao Xiang,Yilin Yang,Julien Yao,Chen Zhang,Jiemin Zhang,Xinyue Zhang,Jason Zheng,Pavlo Zhyzheria,Jan Zikes,Michael Zollhoefer*

Main category: cs.CV

TL;DR: The paper introduces the Seamless Interaction Dataset and models for AI to understand and generate dyadic behavioral dynamics, advancing virtual agents and human-AI interactions.


<details>
  <summary>Details</summary>
Motivation: To develop socially intelligent AI by comprehending and generating dyadic behavioral dynamics in human communication.

Method: Creation of a large-scale dataset (Seamless Interaction Dataset) and development of models for dyadic motion gestures and facial expressions aligned with speech.

Result: Models generate realistic dyadic behaviors, with controllable variants for emotional responses and expressivity, integrated with rendering methods.

Conclusion: The work demonstrates potential for intuitive human-AI interactions, with applications in virtual agents and multimodal analysis.

Abstract: Human communication involves a complex interplay of verbal and nonverbal
signals, essential for conveying meaning and achieving interpersonal goals. To
develop socially intelligent AI technologies, it is crucial to develop models
that can both comprehend and generate dyadic behavioral dynamics. To this end,
we introduce the Seamless Interaction Dataset, a large-scale collection of over
4,000 hours of face-to-face interaction footage from over 4,000 participants in
diverse contexts. This dataset enables the development of AI technologies that
understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,
telepresence experiences, and multimodal content analysis tools. We also
develop a suite of models that utilize the dataset to generate dyadic motion
gestures and facial expressions aligned with human speech. These models can
take as input both the speech and visual behavior of their interlocutors. We
present a variant with speech from an LLM model and integrations with 2D and 3D
rendering methods, bringing us closer to interactive virtual agents.
Additionally, we describe controllable variants of our motion models that can
adapt emotional responses and expressivity levels, as well as generating more
semantically-relevant gestures. Finally, we discuss methods for assessing the
quality of these dyadic motion models, which are demonstrating the potential
for more intuitive and responsive human-AI interactions.

</details>


### [99] [Recomposed realities: animating still images via patch clustering and randomness](https://arxiv.org/abs/2506.22556)
*Markus Juvonen,Samuli Siltanen*

Main category: cs.CV

TL;DR: A patch-based method for animating still images using k-means clustering and random sampling from curated datasets.


<details>
  <summary>Details</summary>
Motivation: To bring still images to life through motion by leveraging existing image data, allowing creative reinterpretation rather than exact replication.

Method: Uses k-means clustering to group image patches from datasets, then reconstructs a target image by matching and randomly sampling from these clusters.

Result: Enables animation of still images with shared local structures but differing conceptual domains.

Conclusion: The method successfully animates images by creatively reusing patches, balancing replication and reinterpretation.

Abstract: We present a patch-based image reconstruction and animation method that uses
existing image data to bring still images to life through motion. Image patches
from curated datasets are grouped using k-means clustering and a new target
image is reconstructed by matching and randomly sampling from these clusters.
This approach emphasizes reinterpretation over replication, allowing the source
and target domains to differ conceptually while sharing local structures.

</details>


### [100] [Improving Token-based Object Detection with Video](https://arxiv.org/abs/2506.22562)
*Abhineet Singh,Nilanjan Ray*

Main category: cs.CV

TL;DR: The paper extends Pix2Seq for videos, introducing an end-to-end video object detection method that improves upon existing detectors by representing objects as sequences of tokens and conceptualizing them as 3D tracklets.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional video detectors, such as loss sparsity and heuristics-based postprocessing, by simplifying object representation and eliminating the need for localization cues.

Method: Represents objects as variable-length sequences of discrete tokens and outputs them as integrated 3D tracklets, enabling scalable and efficient video object detection.

Result: Demonstrates consistent improvement over Pix2Seq and competes with state-of-the-art video detectors, despite computational bottlenecks.

Conclusion: The proposed method offers a scalable and efficient approach to video object detection, with potential for generalization to multi-object tracking.

Abstract: This paper improves upon the Pix2Seq object detector by extending it for
videos. In the process, it introduces a new way to perform end-to-end video
object detection that improves upon existing video detectors in two key ways.
First, by representing objects as variable-length sequences of discrete tokens,
we can succinctly represent widely varying numbers of video objects, with
diverse shapes and locations, without having to inject any localization cues in
the training process. This eliminates the need to sample the space of all
possible boxes that constrains conventional detectors and thus solves the dual
problems of loss sparsity during training and heuristics-based postprocessing
during inference. Second, it conceptualizes and outputs the video objects as
fully integrated and indivisible 3D boxes or tracklets instead of generating
image-specific 2D boxes and linking these boxes together to construct the video
object, as done in most conventional detectors. This allows it to scale
effortlessly with available computational resources by simply increasing the
length of the video subsequence that the network takes as input, even
generalizing to multi-object tracking if the subsequence can span the entire
video. We compare our video detector with the baseline Pix2Seq static detector
on several datasets and demonstrate consistent improvement, although with
strong signs of being bottlenecked by our limited computational resources. We
also compare it with several video detectors on UA-DETRAC to show that it is
competitive with the current state of the art even with the computational
bottleneck. We make our code and models publicly available.

</details>


### [101] [Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation](https://arxiv.org/abs/2506.22567)
*Shansong Wang,Zhecheng Jin,Mingzhe Hu,Mojtaba Safari,Feng Zhao,Chih-Wei Chang,Richard LJ Qiu,Justin Roper,David S. Yu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MMKD-CLIP is a biomedical foundation model that distills knowledge from multiple domain-specific CLIP models, achieving superior performance across diverse tasks without needing billion-scale data.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large-scale biomedical image-text corpora and fragmented data standards hinder the development of a unified biomedical foundation model.

Method: MMKD-CLIP uses a two-stage training pipeline: CLIP-style pretraining on 2.9M biomedical image-text pairs, followed by feature-level distillation from nine teacher models using 19.2M feature pairs.

Result: MMKD-CLIP outperforms all teacher models on 58 datasets, excelling in tasks like zero-shot classification, cross-modal retrieval, and cancer diagnosis.

Conclusion: Multi-teacher knowledge distillation is a scalable and effective approach for building high-performing biomedical foundation models under real-world data constraints.

Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs
have demonstrated impressive capabilities in zero-shot classification,
cross-modal retrieval, and open-ended visual answering. However, transferring
this success to biomedicine is hindered by the scarcity of large-scale
biomedical image-text corpora, the heterogeneity of image modalities, and
fragmented data standards across institutions. These limitations hinder the
development of a unified and generalizable biomedical foundation model trained
from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical
foundation model developed via Multiple Medical CLIP Knowledge Distillation.
Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge
from nine state-of-the-art domain-specific or generalist biomedical CLIP
models, each pretrained on millions of biomedical image-text pairs. Our
two-stage training pipeline first performs CLIP-style pretraining on over 2.9
million biomedical image-text pairs from 26 image modalities, followed by
feature-level distillation using over 19.2 million feature pairs extracted from
teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,
encompassing over 10.8 million biomedical images across nine image modalities.
The evaluation spans six core task types: zero-shot classification, linear
probing, cross-modal retrieval, visual question answering, survival prediction,
and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models
while demonstrating remarkable robustness and generalization across image
domains and task settings. These results underscore that multi-teacher
knowledge distillation is a scalable and effective paradigm for building
high-performing biomedical foundation models under the practical constraints of
real-world data availability.

</details>


### [102] [Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation](https://arxiv.org/abs/2506.22570)
*Chee Mei Ling,Thangarajah Akilan,Aparna Ravinda Phalke*

Main category: cs.CV

TL;DR: The paper proposes an efficient image segmentation method for agriculture using a novel Dual Atrous Separable Convolution (DAS Conv) module within a DeepLabV3 framework, achieving high performance with lower computational complexity.


<details>
  <summary>Details</summary>
Motivation: To improve crop management and productivity by accurately segmenting farmland anomalies in agricultural imagery.

Method: Integrates a DAS Conv module in DeepLabV3, balancing dilation rates and padding, and uses strategic skip connections to capture fine-grained features.

Result: Outperforms baseline and matches SOTA transformer models on the Agriculture Vision dataset, with 66% better efficiency.

Conclusion: The method offers a lightweight, high-performance solution for agricultural semantic segmentation.

Abstract: Agricultural image semantic segmentation is a pivotal component of modern
agriculture, facilitating accurate visual data analysis to improve crop
management, optimize resource utilization, and boost overall productivity. This
study proposes an efficient image segmentation method for precision
agriculture, focusing on accurately delineating farmland anomalies to support
informed decision-making and proactive interventions. A novel Dual Atrous
Separable Convolution (DAS Conv) module is integrated within the
DeepLabV3-based segmentation framework. The DAS Conv module is meticulously
designed to achieve an optimal balance between dilation rates and padding size,
thereby enhancing model performance without compromising efficiency. The study
also incorporates a strategic skip connection from an optimal stage in the
encoder to the decoder to bolster the model's capacity to capture fine-grained
spatial features. Despite its lower computational complexity, the proposed
model outperforms its baseline and achieves performance comparable to highly
complex transformer-based state-of-the-art (SOTA) models on the Agriculture
Vision benchmark dataset. It achieves more than 66% improvement in efficiency
when considering the trade-off between model complexity and performance,
compared to the SOTA model. This study highlights an efficient and effective
solution for improving semantic segmentation in remote sensing applications,
offering a computationally lightweight model capable of high-quality
performance in agricultural imagery.

</details>


### [103] [LIGHT: Multi-Modal Text Linking on Historical Maps](https://arxiv.org/abs/2506.22589)
*Yijun Lin,Rhett Olson,Junhan Wu,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: LIGHT is a multi-modal approach combining linguistic, image, and geometric features to link text on historical maps, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Text on historical maps is complex and lacks structured linking methods, especially for multi-word place names, due to varied orientation and placement.

Method: LIGHT integrates geometry-aware embeddings (polygonal coordinates) with visual and linguistic features from LayoutLMv3, using bi-directional learning for robust sequence prediction.

Result: LIGHT achieves superior performance on ICDAR 2024/2025 MapText Competition data.

Conclusion: Multi-modal learning (linguistic, visual, geometric) effectively addresses text linking challenges in historical maps.

Abstract: Text on historical maps provides valuable information for studies in history,
economics, geography, and other related fields. Unlike structured or
semi-structured documents, text on maps varies significantly in orientation,
reading order, shape, and placement. Many modern methods can detect and
transcribe text regions, but they struggle to effectively ``link'' the
recognized text fragments, e.g., determining a multi-word place name. Existing
layout analysis methods model word relationships to improve text understanding
in structured documents, but they primarily rely on linguistic features and
neglect geometric information, which is essential for handling map text. To
address these challenges, we propose LIGHT, a novel multi-modal approach that
integrates linguistic, image, and geometric features for linking text on
historical maps. In particular, LIGHT includes a geometry-aware embedding
module that encodes the polygonal coordinates of text regions to capture
polygon shapes and their relative spatial positions on an image. LIGHT unifies
this geometric information with the visual and linguistic token embeddings from
LayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal
information to predict the reading-order successor of each text instance
directly with a bi-directional learning strategy that enhances sequence
robustness. Experimental results show that LIGHT outperforms existing methods
on the ICDAR 2024/2025 MapText Competition data, demonstrating the
effectiveness of multi-modal learning for historical map text linking.

</details>


### [104] [BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data](https://arxiv.org/abs/2506.22591)
*Arunkumar Kannan,Martin A. Lindquist,Brian Caffo*

Main category: cs.CV

TL;DR: BrainMT is a hybrid framework combining bidirectional Mamba and transformer blocks to improve fMRI data analysis, outperforming existing methods in classification and regression tasks.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for fMRI analysis struggle with long-range spatial and temporal dependencies, limiting their effectiveness.

Method: BrainMT uses a two-stage approach: (1) a bidirectional Mamba block for global temporal interactions, and (2) a transformer block for global spatial relationships.

Result: BrainMT achieves state-of-the-art performance on UKBioBank and Human Connectome Project datasets for sex prediction and cognitive intelligence prediction.

Conclusion: BrainMT effectively addresses limitations of current methods, offering a robust solution for fMRI data analysis.

Abstract: Recent advances in deep learning have made it possible to predict phenotypic
measures directly from functional magnetic resonance imaging (fMRI) brain
volumes, sparking significant interest in the neuroimaging community. However,
existing approaches, primarily based on convolutional neural networks or
transformer architectures, often struggle to model the complex relationships
inherent in fMRI data, limited by their inability to capture long-range spatial
and temporal dependencies. To overcome these shortcomings, we introduce
BrainMT, a novel hybrid framework designed to efficiently learn and integrate
long-range spatiotemporal attributes in fMRI data. Our framework operates in
two stages: (1) a bidirectional Mamba block with a temporal-first scanning
mechanism to capture global temporal interactions in a computationally
efficient manner; and (2) a transformer block leveraging self-attention to
model global spatial relationships across the deep features processed by the
Mamba block. Extensive experiments on two large-scale public datasets,
UKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves
state-of-the-art performance on both classification (sex prediction) and
regression (cognitive intelligence prediction) tasks, outperforming existing
methods by a significant margin. Our code and implementation details will be
made publicly available at this
https://github.com/arunkumar-kannan/BrainMT-fMRI

</details>


### [105] [Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning](https://arxiv.org/abs/2506.22624)
*Zuyao You,Zuxuan Wu*

Main category: cs.CV

TL;DR: Seg-R1 uses RL to improve pixel-level understanding in LMMs for segmentation tasks, achieving strong performance and generalization without text supervision.


<details>
  <summary>Details</summary>
Motivation: To enhance the pixel-level reasoning of LMMs using RL, focusing on foreground segmentation tasks like COD and SOD.

Method: Introduces Group Relative Policy Optimization (GRPO) for RL training, enabling LMMs to generate prompts for SAM2 to produce masks.

Result: Achieves .873 S-measure on COD10K and strong zero-shot performance on referring and reasoning segmentation tasks.

Conclusion: Pure RL training in Seg-R1 shows promising results and generalization, outperforming supervised models in some tasks.

Abstract: We present Seg-R1, a preliminary exploration of using reinforcement learning
(RL) to enhance the pixel-level understanding and reasoning capabilities of
large multimodal models (LMMs). Starting with foreground segmentation tasks,
specifically camouflaged object detection (COD) and salient object detection
(SOD), our approach enables the LMM to generate point and bounding box prompts
in the next-token fashion, which are then used to guide SAM2 in producing
segmentation masks. We introduce Group Relative Policy Optimization (GRPO) into
the segmentation domain, equipping the LMM with pixel-level comprehension
through a carefully designed training strategy. Notably, Seg-R1 achieves
remarkable performance with purely RL-based training, achieving .873 S-measure
on COD10K without complex model modification. Moreover, we found that pure RL
training demonstrates strong open-world generalization. Despite being trained
solely on foreground segmentation image-mask pairs without text supervision,
Seg-R1 achieves impressive zero-shot performance on referring segmentation and
reasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on
ReasonSeg test, outperforming models fully supervised on these datasets.

</details>


### [106] [ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models](https://arxiv.org/abs/2506.22636)
*Sotirios Panagiotis Chytas,Miso Choi,Hyunwoo J. Kim,Vikas Singh*

Main category: cs.CV

TL;DR: A lightweight trainable module (ReCo) is proposed to mitigate the fading memory effect in Vision Language Models (VLMs), improving performance on benchmarks and combining well with other hallucination-reduction methods.


<details>
  <summary>Details</summary>
Motivation: VLMs often hallucinate due to over-reliance on language, leading to fading memory of visual input. This paper aims to control this behavior.

Method: Introduces ReCo, a small trainable module based on geometric algebra and relational compositions, added to existing VLMs without other modifications.

Result: ReCo improves performance on three widely used VLMs (InstructBLIP, LlaVA, MiniGPT4) across multiple benchmarks and enhances other hallucination-reduction methods.

Conclusion: ReCo effectively mitigates the fading memory effect in VLMs, offering a lightweight solution to reduce hallucination without extensive model modifications.

Abstract: Vision Language Models (VLMs) show impressive capabilities in integrating and
reasoning with both visual and language data. But these models make mistakes. A
common finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,
generate plausible sounding text which is not grounded in the visual input, or
at worst, is contradictory. A growing consensus attributes this behavior to an
over-reliance on language -- especially as the generation progresses, the model
suffers from a ``fading memory effect'' with respect to the provided visual
input. We study mechanisms by which this behavior can be controlled.
Specifically, using ideas from geometric algebra and relational compositions,
we propose the addition of a small, trainable module (named ReCo) on top of any
VLM -- no other modification is needed. We show that such a lightweight module
is able to mitigate the fading memory effect on three of the most widely used
VLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on
multiple benchmarks. Additionally, we show that our module can be combined with
many of the other approaches for reducing hallucination where we achieve
improved results for each one.

</details>


### [107] [CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation](https://arxiv.org/abs/2506.22637)
*Haoxuan Wang,Zhenghao Zhao,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: CaO$_2$ introduces a two-stage diffusion-based framework to address inconsistencies in dataset distillation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based dataset distillation methods suffer from objective and condition inconsistencies, leading to suboptimal performance.

Method: CaO$_2$ uses a two-stage approach: probability-informed sample selection and latent representation refinement.

Result: The method outperforms baselines by 2.3% accuracy on ImageNet and subsets.

Conclusion: CaO$_2$ effectively aligns distillation with evaluation objectives, improving efficiency and performance.

Abstract: The recent introduction of diffusion models in dataset distillation has shown
promising potential in creating compact surrogate datasets for large,
high-resolution target datasets, offering improved efficiency and performance
over traditional bi-level/uni-level optimization methods. However, current
diffusion-based dataset distillation approaches overlook the evaluation process
and exhibit two critical inconsistencies in the distillation process: (1)
Objective Inconsistency, where the distillation process diverges from the
evaluation objective, and (2) Condition Inconsistency, leading to mismatches
between generated images and their corresponding conditions. To resolve these
issues, we introduce Condition-aware Optimization with Objective-guided
Sampling (CaO$_2$), a two-stage diffusion-based framework that aligns the
distillation process with the evaluation objective. The first stage employs a
probability-informed sample selection pipeline, while the second stage refines
the corresponding latent representations to improve conditional likelihood.
CaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,
surpassing the best-performing baselines by an average of 2.3% accuracy.

</details>


### [108] [3D Shape Generation: A Survey](https://arxiv.org/abs/2506.22678)
*Nicolas Caytuiro,Ivan Sipiran*

Main category: cs.CV

TL;DR: A survey on 3D shape generation, covering representations, methods, evaluation, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a structured overview of advances in deep learning for 3D shape generation.

Method: Categorizes 3D representations (explicit, implicit, hybrid), reviews generative approaches, and summarizes datasets and metrics.

Result: Highlights current state-of-the-art, including strengths and limitations of methods.

Conclusion: Identifies open challenges and future directions for controllable, efficient, high-quality 3D shape generation.

Abstract: Recent advances in deep learning have significantly transformed the field of
3D shape generation, enabling the synthesis of complex, diverse, and
semantically meaningful 3D objects. This survey provides a comprehensive
overview of the current state of the art in 3D shape generation, organizing the
discussion around three core components: shape representations, generative
modeling approaches, and evaluation protocols. We begin by categorizing 3D
representations into explicit, implicit, and hybrid setups, highlighting their
structural properties, advantages, and limitations. Next, we review a wide
range of generation methods, focusing on feedforward architectures. We further
summarize commonly used datasets and evaluation metrics that assess fidelity,
diversity, and realism of generated shapes. Finally, we identify open
challenges and outline future research directions that could drive progress in
controllable, efficient, and high-quality 3D shape generation. This survey aims
to serve as a valuable reference for researchers and practitioners seeking a
structured and in-depth understanding of this rapidly evolving field.

</details>


### [109] [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/abs/2506.22710)
*Jiang Yuan,JI Ma,Bo Wang,Guanzhou Ke,Weiming Hu*

Main category: cs.CV

TL;DR: LightBSR improves blind super-resolution by optimizing implicit degradation representation (IDR) discriminability, using a knowledge distillation framework for efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing IDE-BSR methods overlook IDR discriminability, complicating adaptation and increasing model complexity.

Method: Uses knowledge distillation: contrastive learning for degradation distinction in the teacher stage, then feature alignment for student inference.

Result: LightBSR achieves high performance with minimal complexity in blind SR tasks.

Conclusion: Optimizing IDR discriminability leads to effective, lightweight BSR models.

Abstract: Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges
on extracting the implicit degradation representation (IDR) of the LR image and
adapting it to LR image features to guide HR detail restoration. Although
IDE-BSR has shown potential in dealing with noise interference and complex
degradations, existing methods ignore the importance of IDR discriminability
for BSR and instead over-complicate the adaptation process to improve effect,
resulting in a significant increase in the model's parameters and computations.
In this paper, we focus on the discriminability optimization of IDR and propose
a new powerful and lightweight BSR model termed LightBSR. Specifically, we
employ a knowledge distillation-based learning framework. We first introduce a
well-designed degradation-prior-constrained contrastive learning technique
during teacher stage to make the model more focused on distinguishing different
degradation types. Then we utilize a feature alignment technique to transfer
the degradation-related knowledge acquired by the teacher to the student for
practical inferencing. Extensive experiments demonstrate the effectiveness of
IDR discriminability-driven BSR model design. The proposed LightBSR can achieve
outstanding performance with minimal complexity across a range of blind SR
tasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.

</details>


### [110] [Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians](https://arxiv.org/abs/2506.22718)
*Jun-Jee Chao,Qingyuan Jiang,Volkan Isler*

Main category: cs.CV

TL;DR: A method for joint part segmentation and motion estimation in articulated objects from point cloud sequences, robust to occlusions and missing data.


<details>
  <summary>Details</summary>
Motivation: Address challenges in articulated object motion analysis where point clouds vary due to occlusions or asynchronous sensor data, making point correspondence tracking unreliable.

Method: Represents objects as 3D Gaussians with time-dependent transformations, enabling part segmentation and motion estimation without relying on fixed point correspondences.

Result: Outperforms point correspondence-based methods, with 13% better segmentation performance on occluded point clouds.

Conclusion: The proposed Gaussian-based representation is effective for robust part segmentation and motion estimation in challenging scenarios.

Abstract: Part segmentation and motion estimation are two fundamental problems for
articulated object motion analysis. In this paper, we present a method to solve
these two problems jointly from a sequence of observed point clouds of a single
articulated object. The main challenge in our problem setting is that the point
clouds are not assumed to be generated by a fixed set of moving points.
Instead, each point cloud in the sequence could be an arbitrary sampling of the
object surface at that particular time step. Such scenarios occur when the
object undergoes major occlusions, or if the dataset is collected using
measurements from multiple sensors asynchronously. In these scenarios, methods
that rely on tracking point correspondences are not appropriate. We present an
alternative approach based on a compact but effective representation where we
represent the object as a collection of simple building blocks modeled as 3D
Gaussians. We parameterize the Gaussians with time-dependent rotations,
translations, and scales that are shared across all time steps. With our
representation, part segmentation can be achieved by building correspondences
between the observed points and the Gaussians. Moreover, the transformation of
each point across time can be obtained by following the poses of the assigned
Gaussian (even when the point is not observed). Experiments show that our
method outperforms existing methods that solely rely on finding point
correspondences. Additionally, we extend existing datasets to emulate
real-world scenarios by considering viewpoint occlusions. We further
demonstrate that our method is more robust to missing points as compared to
existing approaches on these challenging datasets, even when some parts are
completely occluded in some time-steps. Notably, our part segmentation
performance outperforms the state-of-the-art method by 13% on point clouds with
occlusions.

</details>


### [111] [Deterministic Object Pose Confidence Region Estimation](https://arxiv.org/abs/2506.22720)
*Jinghao Wang,Zhang Li,Zi Wang,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: A deterministic method for 6D pose confidence region estimation is proposed, addressing inefficiency and inflated regions in sampling-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current sampling-based methods for 6D pose confidence region estimation are slow and produce overly large regions, limiting practical use.

Method: Uses inductive conformal prediction to calibrate Gaussian keypoint distributions into 2D confidence regions, then propagates these to 6D pose regions via the implicit function theorem.

Result: Achieves higher accuracy, faster computation, and significantly smaller confidence regions (up to 99.9% reduction for rotations, 99.8% for translations).

Conclusion: The proposed method outperforms sampling-based approaches, offering compact, efficient, and accurate confidence regions for 6D pose estimation.

Abstract: 6D pose confidence region estimation has emerged as a critical direction,
aiming to perform uncertainty quantification for assessing the reliability of
estimated poses. However, current sampling-based approach suffers from critical
limitations that severely impede their practical deployment: 1) the sampling
speed significantly decreases as the number of samples increases. 2) the
derived confidence regions are often excessively large. To address these
challenges, we propose a deterministic and efficient method for estimating pose
confidence regions. Our approach uses inductive conformal prediction to
calibrate the deterministically regressed Gaussian keypoint distributions into
2D keypoint confidence regions. We then leverage the implicit function theorem
to propagate these keypoint confidence regions directly into 6D pose confidence
regions. This method avoids the inefficiency and inflated region sizes
associated with sampling and ensembling. It provides compact confidence regions
that cover the ground-truth poses with a user-defined confidence level.
Experimental results on the LineMOD Occlusion and SPEED datasets show that our
method achieves higher pose estimation accuracy with reduced computational
time. For the same coverage rate, our method yields significantly smaller
confidence region volumes, reducing them by up to 99.9\% for rotations and
99.8\% for translations. The code will be available soon.

</details>


### [112] [XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge](https://arxiv.org/abs/2506.22726)
*Yu Zhang,Xi Zhang,Hualin zhou,Xinyuan Chen,Shang Gao,Hong Jia,Jianfei Yang,Yuankai Qi,Tao Gu*

Main category: cs.CV

TL;DR: XTransfer is a novel method for efficient, modality-agnostic model transfer in edge-based human sensing, addressing issues like modality shift and resource constraints.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of current methods (modality shift, high resource demands) in deep learning for human sensing on edge systems.

Method: XTransfer uses model repairing to fix modality shift and layer recombining to create compact models from pre-trained layers.

Result: Achieves state-of-the-art performance, reduces data collection, training, and deployment costs.

Conclusion: XTransfer is a resource-efficient, adaptable solution for human sensing on edge systems.

Abstract: Deep learning for human sensing on edge systems offers significant
opportunities for smart applications. However, its training and development are
hindered by the limited availability of sensor data and resource constraints of
edge systems. Current methods that rely on transferring pre-trained models
often encounter issues such as modality shift and high resource demands,
resulting in substantial accuracy loss, resource overhead, and poor
adaptability across different sensing applications. In this paper, we propose
XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic
model transfer. XTransfer freely leverages single or multiple pre-trained
models and transfers knowledge across different modalities by (i) model
repairing that safely repairs modality shift in pre-trained model layers with
only few sensor data, and (ii) layer recombining that efficiently searches and
recombines layers of interest from source models in a layer-wise manner to
create compact models. We benchmark various baselines across diverse human
sensing datasets spanning different modalities. Comprehensive results
demonstrate that XTransfer achieves state-of-the-art performance on human
sensing tasks while significantly reducing the costs of sensor data collection,
model training, and edge deployment.

</details>


### [113] [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/abs/2506.22736)
*Dayong Su,Yafei Zhang,Huafeng Li,Jinxing Li,Yu Liu*

Main category: cs.CV

TL;DR: UniFuse is a unified framework for multimodal medical image fusion that handles misaligned or degraded images by integrating degradation-aware prompt learning and Omni Unified Feature Representation, achieving joint optimization of alignment, restoration, and fusion.


<details>
  <summary>Details</summary>
Motivation: Current fusion methods assume high-quality, perfectly aligned images, failing with misaligned or degraded inputs. UniFuse addresses this limitation.

Method: UniFuse combines degradation-aware prompt learning, Omni Unified Feature Representation (using Spatial Mamba), and a Universal Feature Restoration & Fusion module with Adaptive LoRA Synergistic Network (ALSN).

Result: Experiments show UniFuse outperforms existing methods, effectively unifying alignment, restoration, and fusion.

Conclusion: UniFuse provides a robust, single-stage solution for multimodal medical image fusion, overcoming limitations of traditional methods.

Abstract: Current multimodal medical image fusion typically assumes that source images
are of high quality and perfectly aligned at the pixel level. Its effectiveness
heavily relies on these conditions and often deteriorates when handling
misaligned or degraded medical images. To address this, we propose UniFuse, a
general fusion framework. By embedding a degradation-aware prompt learning
module, UniFuse seamlessly integrates multi-directional information from input
images and correlates cross-modal alignment with restoration, enabling joint
optimization of both tasks within a unified framework. Additionally, we design
an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to
encode multi-directional features and mitigate modality differences in feature
alignment. To enable simultaneous restoration and fusion within an All-in-One
configuration, we propose a Universal Feature Restoration & Fusion module,
incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA
principles. By leveraging ALSN's adaptive feature representation along with
degradation-type guidance, we enable joint restoration and fusion within a
single-stage framework. Compared to staged approaches, UniFuse unifies
alignment, restoration, and fusion within a single framework. Experimental
results across multiple datasets demonstrate the method's effectiveness and
significant advantages over existing approaches.

</details>


### [114] [Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds](https://arxiv.org/abs/2506.22749)
*Yun Zhang,Feifan Chen,Na Li,Zhiwei Guo,Xu Wang,Fen Miao,Sam Kwong*

Main category: cs.CV

TL;DR: A deep learning-based method (JGAU) for joint geometry and attribute up-sampling of colored point clouds, achieving superior PSNR gains over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality of large-scale and denser colored point clouds for realistic 3D applications by leveraging spatial attribute correlations.

Method: Proposes a JGAU framework with geometry and attribute up-sampling networks, coarse attribute up-sampling methods (GDWAI, DLAI), and an attribute enhancement module.

Result: Achieves PSNR gains of 2.11-2.47 decibels across four up-sampling rates (4x-16x) compared to state-of-the-art methods.

Conclusion: JGAU significantly improves the quality of up-sampled colored point clouds, demonstrating its effectiveness for 3D applications.

Abstract: Colored point cloud, which includes geometry and attribute components, is a
mainstream representation enabling realistic and immersive 3D applications. To
generate large-scale and denser colored point clouds, we propose a deep
learning-based Joint Geometry and Attribute Up-sampling (JGAU) method that
learns to model both geometry and attribute patterns while leveraging spatial
attribute correlations. First, we establish and release a large-scale dataset
for colored point cloud up-sampling called SYSU-PCUD, containing 121
large-scale colored point clouds with diverse geometry and attribute
complexities across six categories and four sampling rates. Second, to improve
the quality of up-sampled point clouds, we propose a deep learning-based JGAU
framework that jointly up-samples geometry and attributes. It consists of a
geometry up-sampling network and an attribute up-sampling network, where the
latter leverages the up-sampled auxiliary geometry to model neighborhood
correlations of the attributes. Third, we propose two coarse attribute
up-sampling methods, Geometric Distance Weighted Attribute Interpolation
(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate
coarse up-sampled attributes for each point. Then, an attribute enhancement
module is introduced to refine these up-sampled attributes and produce
high-quality point clouds by further exploiting intrinsic attribute and
geometry patterns. Extensive experiments show that the Peak Signal-to-Noise
Ratio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10
decibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,
8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art
methods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28
decibels, and 2.11 decibels at these four up-sampling rates, demonstrating
significant improvement.

</details>


### [115] [Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography](https://arxiv.org/abs/2506.22753)
*Jianing Zhang,Jiayi Zhu,Feiyu Ji,Xiaokang Yang,Xiaoyun Yuan*

Main category: cs.CV

TL;DR: A novel framework, Degradation-Modeled Multipath Diffusion, is introduced for tunable metalens photography, leveraging pretrained models and pseudo data augmentation to overcome challenges in computational imaging.


<details>
  <summary>Details</summary>
Motivation: Metalenses face challenges like optical degradation and computational restoration difficulties, often requiring precise calibration or large datasets, which are impractical for real-world systems.

Method: The framework uses positive, neutral, and negative-prompt paths for detail generation and degradation suppression, alongside a tunable decoder and a spatially varying degradation-aware attention (SVDA) module.

Result: The approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction, validated with a millimeter-scale MetaCamera.

Conclusion: The proposed method offers a practical solution for metalens photography, balancing fidelity and perceptual quality without relying on massive datasets.

Abstract: Metalenses offer significant potential for ultra-compact computational
imaging but face challenges from complex optical degradation and computational
restoration difficulties. Existing methods typically rely on precise optical
calibration or massive paired datasets, which are non-trivial for real-world
imaging systems. Furthermore, a lack of control over the inference process
often results in undesirable hallucinated artifacts. We introduce
Degradation-Modeled Multipath Diffusion for tunable metalens photography,
leveraging powerful natural image priors from pretrained models instead of
large datasets. Our framework uses positive, neutral, and negative-prompt paths
to balance high-frequency detail generation, structural fidelity, and
suppression of metalens-specific degradation, alongside \textit{pseudo} data
augmentation. A tunable decoder enables controlled trade-offs between fidelity
and perceptual quality. Additionally, a spatially varying degradation-aware
attention (SVDA) module adaptively models complex optical and sensor-induced
degradation. Finally, we design and build a millimeter-scale MetaCamera for
real-world validation. Extensive results show that our approach outperforms
state-of-the-art methods, achieving high-fidelity and sharp image
reconstruction. More materials: https://dmdiff.github.io/.

</details>


### [116] [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/abs/2506.22756)
*Tao Tang,Likui Zhang,Youpeng Wen,Kaidong Zhang,Jia-Wang Bian,xia zhou,Tianyi Yan,Kun Zhan,Peng Jia,Hefeng Wu,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: RoboPearls is a video simulation framework for robotic manipulation, leveraging 3D Gaussian Splatting and LLMs to bridge the sim-to-real gap and automate simulation production.


<details>
  <summary>Details</summary>
Motivation: High costs and inefficiency of real-world data collection hinder scalable robotic learning, while existing simulators struggle with the sim-to-real gap.

Method: RoboPearls uses 3DGS for photo-realistic simulations, ISD and 3D-NNFM for advanced operations, and LLMs/VLMs for automation and performance analysis.

Result: Extensive experiments on datasets like RLBench and real-world robots confirm RoboPearls' effectiveness in simulation performance.

Conclusion: RoboPearls addresses scalability and sim-to-real challenges, offering a user-friendly, automated solution for robotic manipulation learning.

Abstract: The development of generalist robot manipulation policies has seen
significant progress, driven by large-scale demonstration data across diverse
environments. However, the high cost and inefficiency of collecting real-world
demonstrations hinder the scalability of data acquisition. While existing
simulation platforms enable controlled environments for robotic learning, the
challenge of bridging the sim-to-real gap remains. To address these challenges,
we propose RoboPearls, an editable video simulation framework for robotic
manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the
construction of photo-realistic, view-consistent simulations from demonstration
videos, and supports a wide range of simulation operators, including various
object manipulations, powered by advanced modules like Incremental Semantic
Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by
incorporating large language models (LLMs), RoboPearls automates the simulation
production process in a user-friendly manner through flexible command
interpretation and execution. Furthermore, RoboPearls employs a vision-language
model (VLM) to analyze robotic learning issues to close the simulation loop for
performance enhancement. To demonstrate the effectiveness of RoboPearls, we
conduct extensive experiments on multiple datasets and scenes, including
RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which
demonstrate our satisfactory simulation performance.

</details>


### [117] [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/abs/2506.22762)
*Dinh Phu Tran,Dao Duy Hung,Daeyoung Kim*

Main category: cs.CV

TL;DR: VSRM is a novel video super-resolution framework using Mamba for efficient long-range spatio-temporal feature extraction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of CNNs (local receptive fields) and Transformers (quadratic complexity) in video super-resolution by leveraging Mamba's strengths.

Method: Introduces Spatial-to-Temporal and Temporal-to-Spatial Mamba blocks, Deformable Cross-Mamba Alignment, and a Frequency Charbonnier-like loss.

Result: Achieves state-of-the-art performance on benchmarks, enhancing visual quality and high-frequency content preservation.

Conclusion: VSRM sets a strong foundation for future research in video super-resolution.

Abstract: Video super-resolution remains a major challenge in low-level vision tasks.
To date, CNN- and Transformer-based methods have delivered impressive results.
However, CNNs are limited by local receptive fields, while Transformers
struggle with quadratic complexity, posing challenges for processing long
sequences in VSR. Recently, Mamba has drawn attention for its long-sequence
modeling, linear complexity, and large receptive fields. In this work, we
propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution
framework that leverages the power of \textbf{M}amba. VSRM introduces
Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract
long-range spatio-temporal features and enhance receptive fields efficiently.
To better align adjacent frames, we propose Deformable Cross-Mamba Alignment
module. This module utilizes a deformable cross-mamba mechanism to make the
compensation stage more dynamic and flexible, preventing feature distortions.
Finally, we minimize the frequency domain gaps between reconstructed and
ground-truth frames by proposing a simple yet effective Frequency
Charbonnier-like loss that better preserves high-frequency content and enhances
visual quality. Through extensive experiments, VSRM achieves state-of-the-art
results on diverse benchmarks, establishing itself as a solid foundation for
future research.

</details>


### [118] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Sriram Vishwanath,Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: The paper introduces PhonemeFake (PF), a deepfake attack method that manipulates speech segments to deceive human perception and benchmark models, outperforming existing datasets. It also proposes a detection model with improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake datasets fail to mimic real-world attacks effectively, highlighting the need for more realistic attack vectors.

Method: PhonemeFake (PF) manipulates critical speech segments using language reasoning. A bilevel detection model is introduced to prioritize compute on manipulated regions.

Result: PF reduces human perception by 42% and benchmark accuracies by 94%. The detection model cuts EER by 91% and speeds up by 90% with minimal overhead.

Conclusion: PF and the proposed detection model offer a scalable, efficient solution for realistic deepfake attacks and their mitigation.

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [119] [Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching](https://arxiv.org/abs/2506.22784)
*Yu Han,Zhiwei Huang,Yanting Zhang,Fangjun Ding,Shen Cai,Rui Fan*

Main category: cs.CV

TL;DR: A detector-free framework for direct point-pixel matching between LiDAR and camera views is introduced, addressing modality gaps and sparsity challenges in single-frame LiDAR settings.


<details>
  <summary>Details</summary>
Motivation: The modality gap between unstructured LiDAR point clouds and structured images, especially under sparse single-frame conditions, poses a significant challenge in autonomous driving and robotic perception.

Method: The proposed method projects LiDAR intensity maps into a 2D view and uses an attention-based detector-free matching network for cross-modal correspondence. A repeatability scoring mechanism enhances reliability by suppressing low-intensity regions.

Result: The method achieves state-of-the-art performance on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks, outperforming prior approaches even with single-frame LiDAR.

Conclusion: The detector-free framework effectively bridges the modality gap and improves robustness in sparse LiDAR settings without multi-frame accumulation.

Abstract: Point-pixel registration between LiDAR point clouds and camera images is a
fundamental yet challenging task in autonomous driving and robotic perception.
A key difficulty lies in the modality gap between unstructured point clouds and
structured images, especially under sparse single-frame LiDAR settings.
Existing methods typically extract features separately from point clouds and
images, then rely on hand-crafted or learned matching strategies. This separate
encoding fails to bridge the modality gap effectively, and more critically,
these methods struggle with the sparsity and noise of single-frame LiDAR, often
requiring point cloud accumulation or additional priors to improve reliability.
Inspired by recent progress in detector-free matching paradigms (e.g.
MatchAnything), we revisit the projection-based approach and introduce the
detector-free framework for direct point-pixel matching between LiDAR and
camera views. Specifically, we project the LiDAR intensity map into a 2D view
from the LiDAR perspective and feed it into an attention-based detector-free
matching network, enabling cross-modal correspondence estimation without
relying on multi-frame accumulation. To further enhance matching reliability,
we introduce a repeatability scoring mechanism that acts as a soft visibility
prior. This guides the network to suppress unreliable matches in regions with
low intensity variation, improving robustness under sparse input. Extensive
experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that
our method achieves state-of-the-art performance, outperforming prior
approaches on nuScenes (even those relying on accumulated point clouds),
despite using only single-frame LiDAR.

</details>


### [120] [RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors](https://arxiv.org/abs/2506.22800)
*Sicong Du,Jiarun Liu,Qifeng Chen,Hao-Xiang Chen,Tai-Jiang Mu,Sheng Yang*

Main category: cs.CV

TL;DR: RGE-GS is a novel framework combining diffusion-based generation and reward-guided Gaussian integration for expansive road structure reconstruction, outperforming baseline methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Single-pass driving clips often miss road details, requiring expansive reconstruction for sensor simulators, but current 3DGS extensions with diffusion priors introduce inconsistencies and inefficiencies.

Method: RGE-GS uses a reward network to prioritize stable diffusion outputs and a differentiated training strategy for adaptive Gaussian optimization, enhancing reconstruction.

Result: RGE-GS achieves state-of-the-art reconstruction quality on public datasets.

Conclusion: RGE-GS effectively addresses physical inconsistencies and training inefficiencies, offering superior performance for scene expansion in driving simulations.

Abstract: A single-pass driving clip frequently results in incomplete scanning of the
road structure, making reconstructed scene expanding a critical requirement for
sensor simulators to effectively regress driving actions. Although contemporary
3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction
quality, their direct extension through the integration of diffusion priors
often introduces cumulative physical inconsistencies and compromises training
efficiency. To address these limitations, we present RGE-GS, a novel expansive
reconstruction framework that synergizes diffusion-based generation with
reward-guided Gaussian integration. The RGE-GS framework incorporates two key
innovations: First, we propose a reward network that learns to identify and
prioritize consistently generated patterns prior to reconstruction phases,
thereby enabling selective retention of diffusion outputs for spatial
stability. Second, during the reconstruction process, we devise a
differentiated training strategy that automatically adjust Gaussian
optimization progress according to scene converge metrics, which achieving
better convergence than baseline methods. Extensive evaluations of publicly
available datasets demonstrate that RGE-GS achieves state-of-the-art
performance in reconstruction quality. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version
incorporating reviewer suggestions will be updated soon.)

</details>


### [121] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/abs/2506.22803)
*Nuoye Xiong,Anqi Dong,Ning Wang,Cong Hua,Guangming Zhu,Mei Lin,Peiyi Shen,Liang Zhang*

Main category: cs.CV

TL;DR: The paper introduces CBM-HNMU, a method to improve interpretability and accuracy of deep learning models by refining concepts in a Concept Bottleneck Model and distilling knowledge back into the original model.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are becoming more complex and less interpretable, with existing methods lacking effective interventions or global improvements.

Method: Uses a Concept Bottleneck Model (CBM) to approximate black-box reasoning, identifies and refines detrimental concepts globally, and distills corrected knowledge back into the model.

Result: Tested on multiple datasets, achieving up to 2.64% accuracy improvement and 1.03% average accuracy increase.

Conclusion: CBM-HNMU enhances both interpretability and performance of deep learning models by refining and transferring conceptual understanding.

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [122] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/abs/2506.22806)
*Byung Hyun Lee,Sungjin Lim,Seunggyu Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: The paper introduces Concept Pinpoint Eraser (CPE), a framework for selectively erasing target concepts in diffusion models while preserving others, outperforming prior methods by using nonlinear Residual Attention Gates and adversarial training.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about inappropriate or trademarked content in text-to-image diffusion models, the paper aims to improve concept erasure without distorting other concepts.

Method: CPE adds nonlinear Residual Attention Gates (ResAGs) and uses an attention anchoring loss to protect remaining concepts. It also employs adversarial training with learnable text embeddings for robustness.

Result: CPE effectively erases target concepts (e.g., celebrities, artistic styles, explicit content) while preserving diverse remaining concepts and resisting adversarial attacks.

Conclusion: CPE advances concept erasure in diffusion models by combining nonlinear modules and adversarial training, offering superior performance and robustness.

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [123] [FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition](https://arxiv.org/abs/2506.22807)
*Yueyang Li,Shengyu Gong,Weiming Zeng,Nizhuan Wang,Wai Ting Siok*

Main category: cs.CV

TL;DR: FreqDGT, a frequency-adaptive dynamic graph transformer, improves cross-subject emotion recognition in EEG by integrating frequency-adaptive processing, dynamic graph learning, and multi-scale temporal disentanglement.


<details>
  <summary>Details</summary>
Motivation: Cross-subject generalization in EEG-based emotion recognition is challenging due to individual variability. FreqDGT aims to address this by leveraging neuroscientific insights and adaptive modeling.

Method: FreqDGT combines frequency-adaptive processing (FAP), adaptive dynamic graph learning (ADGL), and a multi-scale temporal disentanglement network (MTDN) to dynamically weight frequency bands, learn brain connectivity, and capture temporal dynamics.

Result: FreqDGT significantly improves cross-subject emotion recognition accuracy, demonstrating robustness to individual differences.

Conclusion: The integration of frequency-adaptive, spatial-dynamic, and temporal-hierarchical modeling in FreqDGT effectively addresses cross-subject challenges in EEG-based emotion recognition.

Abstract: Electroencephalography (EEG) serves as a reliable and objective signal for
emotion recognition in affective brain-computer interfaces, offering unique
advantages through its high temporal resolution and ability to capture
authentic emotional states that cannot be consciously controlled. However,
cross-subject generalization remains a fundamental challenge due to individual
variability, cognitive traits, and emotional responses. We propose FreqDGT, a
frequency-adaptive dynamic graph transformer that systematically addresses
these limitations through an integrated framework. FreqDGT introduces
frequency-adaptive processing (FAP) to dynamically weight emotion-relevant
frequency bands based on neuroscientific evidence, employs adaptive dynamic
graph learning (ADGL) to learn input-specific brain connectivity patterns, and
implements multi-scale temporal disentanglement network (MTDN) that combines
hierarchical temporal transformers with adversarial feature disentanglement to
capture both temporal dynamics and ensure cross-subject robustness.
Comprehensive experiments demonstrate that FreqDGT significantly improves
cross-subject emotion recognition accuracy, confirming the effectiveness of
integrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical
modeling while ensuring robustness to individual differences. The code is
available at https://github.com/NZWANG/FreqDGT.

</details>


### [124] [Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping](https://arxiv.org/abs/2506.22814)
*Andrew Hamara,Andrew C. Freeman*

Main category: cs.CV

TL;DR: Extends Fixed Aspect Ratio Cropping to efficiently extract multiple non-overlapping crops in linear time, avoiding recomputation of saliency maps.


<details>
  <summary>Details</summary>
Motivation: Traditional methods optimize a single bounding box, limiting effectiveness for applications needing multiple disjoint crops.

Method: Dynamic adjustment of attention thresholds and removal of selected crops without recomputing the saliency map.

Result: Efficient extraction of multiple non-overlapping crops in linear time.

Conclusion: Qualitative results are discussed, with potential for future datasets and benchmarks.

Abstract: Automatic image cropping aims to extract the most visually salient regions
while preserving essential composition elements. Traditional saliency-aware
cropping methods optimize a single bounding box, making them ineffective for
applications requiring multiple disjoint crops. In this work, we extend the
Fixed Aspect Ratio Cropping algorithm to efficiently extract multiple
non-overlapping crops in linear time. Our approach dynamically adjusts
attention thresholds and removes selected crops from consideration without
recomputing the entire saliency map. We discuss qualitative results and
introduce the potential for future datasets and benchmarks.

</details>


### [125] [Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2506.22817)
*Xingyilang Yin,Jiale Wang,Xi Yang,Mutian Xu,Xu Gu,Nannan Wang*

Main category: cs.CV

TL;DR: MVOV3D improves open-vocabulary 3D scene understanding by reducing noise in 2D multi-view fusion without training, leveraging CLIP encoders and 3D priors, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with diverse object categories due to limited 3D data and noisy 2D multi-view fusion.

Method: MVOV3D enhances 2D multi-view features using CLIP encoders and 3D geometric priors, avoiding training.

Result: Achieves 14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160, outperforming trained 3D networks.

Conclusion: MVOV3D effectively enhances open-vocabulary 3D understanding by optimizing multi-view fusion without training.

Abstract: Recent open-vocabulary 3D scene understanding approaches mainly focus on
training 3D networks through contrastive learning with point-text pairs or by
distilling 2D features into 3D models via point-pixel alignment. While these
methods show considerable performance in benchmarks with limited vocabularies,
they struggle to handle diverse object categories as the limited amount of 3D
data upbound training strong open-vocabulary 3d models. We observe that 2D
multi-view fusion methods take precedence in understanding diverse concepts in
3D scenes. However, inherent noises in vision-language models lead multi-view
fusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel
approach aimed at unleashing the potential of 2D multi-view fusion for
open-vocabulary 3D scene understanding. We focus on reducing the inherent
noises without training, thereby preserving the generalizability while
enhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D
features by leveraging precise region-level image features and text features
encoded by CLIP encoders and incorporates 3D geometric priors to optimize
multi-view fusion. Extensive experiments on various datasets demonstrate the
effectiveness of our method. Notably, our MVOV3D achieves a new record with
14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge
open-vocabulary semantic segmentation, outperforming current leading trained 3D
networks by a significant margin.

</details>


### [126] [Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration](https://arxiv.org/abs/2506.22819)
*Ramya Hebbalaguppe,Tamoghno Kandar,Abhinav Nagpal,Chetan Arora*

Main category: cs.CV

TL;DR: The paper addresses the issue of confidence calibration degradation in vision-language models (VLM) during test-time prompt tuning (TPT). It proposes TCA, a method using LLM-based prompt initialization and regularization to improve calibration, achieving significantly lower ECE than existing methods.


<details>
  <summary>Details</summary>
Motivation: TPT improves VLM accuracy but harms confidence calibration, limiting its use in critical applications. The paper aims to address this by mitigating overfitting and improving prompt quality.

Method: Proposes TCA: (1) Initializes prompts using LLM prior knowledge to avoid overfitting, and (2) introduces a regularization loss to maintain prompt quality by reducing intraclass and increasing inter-class distances.

Result: TCA achieves an average ECE of 4.11, outperforming vanilla TPT (11.7) and other recent methods (C-TPT, DiffTPT, PromptAlign).

Conclusion: TCA effectively improves calibration in VLMs during TPT, making it more reliable for critical applications.

Abstract: Vision-language models (VLM) have demonstrated impressive performance in
image recognition by leveraging self-supervised training on large datasets.
Their performance can be further improved by adapting to the test sample using
test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT
approaches on improving the accuracy suffers from tunnel vision, and leads to
degradation in confidence calibration. This limits the applicability of TPT in
critical applications.
  We make three contributions in this work. (1) We posit that random or naive
initialization of prompts leads to overfitting on a particular test sample, and
is the main reason for miscalibration of the VLM after TPT. To mitigate the
problem, we propose careful initialization of test time prompt using prior
knowledge about the target label attributes from a large language model (LLM);
(2) To further maintain the quality of prompts during \tpt, we propose a novel
regularization loss to reduce intraclass distance, and increase inter-class
distance between the learnt
  Through extensive experiments on different CLIP architectures and 15
datasets, we show that our approach can effectively improve the calibration
after TPT. We report an average expected calibration error (ECE) of 4.11 with
our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),
6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is
publicly accessible at:
https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.

</details>


### [127] [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832)
*Alexander Gambashidze,Li Pengyi,Matvey Skripkin,Andrey Galichin,Anton Gusarov,Konstantin Sobolev,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: The paper introduces a listener-augmented GRPO framework to improve reward models for human visual preferences by aligning reasoning traces with an independent vision-language model, achieving better accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Current reward models for human visual preferences often fail to generalize, and supervised fine-tuning leads to memorization. Reinforcement learning (RL) like GRPO helps but suffers from reasoning contradictions.

Method: Proposes a listener-augmented GRPO framework where a frozen vision-language model ('listener') re-evaluates the reasoner's chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal.

Result: Achieves 67.4% accuracy on ImageReward benchmark, improves OOD performance by up to +6%, and reduces reasoning contradictions compared to baselines.

Conclusion: Listener-based rewards offer a scalable, data-efficient method to align vision-language models with nuanced human preferences.

Abstract: Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.

</details>


### [128] [SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds](https://arxiv.org/abs/2506.22833)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: SemFaceEdit introduces a method for localized facial editing in 3D-aware GANs by generating semantic fields on generative radiance manifolds, enabling precise control over geometry and appearance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D-aware GANs lack localized editing capabilities, prompting the need for a method that allows precise semantic edits while preserving other regions.

Method: SemFaceEdit uses two modules: Geometry for semantic radiance/occupancy fields and Appearance for RGB radiance, trained adversarially with latent codes for disentanglement.

Result: The method achieves superior performance in semantic field-based editing and improved radiance field disentanglement.

Conclusion: SemFaceEdit advances localized facial editing in 3D-aware GANs by leveraging semantic fields and adversarial training for precise control.

Abstract: Despite multiple view consistency offered by 3D-aware GAN techniques, the
resulting images often lack the capacity for localized editing. In response,
generative radiance manifolds emerge as an efficient approach for constrained
point sampling within volumes, effectively reducing computational demands and
enabling the learning of fine details. This work introduces SemFaceEdit, a
novel method that streamlines the appearance and geometric editing process by
generating semantic fields on generative radiance manifolds. Utilizing latent
codes, our method effectively disentangles the geometry and appearance
associated with different facial semantics within the generated image. In
contrast to existing methods that can change the appearance of the entire
radiance field, our method enables the precise editing of particular facial
semantics while preserving the integrity of other regions. Our network
comprises two key modules: the Geometry module, which generates semantic
radiance and occupancy fields, and the Appearance module, which is responsible
for predicting RGB radiance. We jointly train both modules in adversarial
settings to learn semantic-aware geometry and appearance descriptors. The
appearance descriptors are then conditioned on their respective semantic latent
codes by the Appearance Module, facilitating disentanglement and enhanced
control. Our experiments highlight SemFaceEdit's superior performance in
semantic field-based editing, particularly in achieving improved radiance field
disentanglement.

</details>


### [129] [FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition](https://arxiv.org/abs/2506.22836)
*Hongyan An,Kuan Zhu,Xin He,Haiyun Guo,Chaoyang Zhao,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: The paper introduces FOCUS, a method for Pedestrian Attribute Recognition (PAR) that adaptively extracts fine-grained attribute-level features, addressing limitations of fixed regional features.


<details>
  <summary>Details</summary>
Motivation: Existing PAR methods rely on fixed regional features, which compromise fine-grained patterns and cannot generalize to unseen attributes. FOCUS aims to overcome these limitations.

Method: FOCUS uses Multi-Granularity Mix Tokens (MGMT) for diverse visual features, Attribute-guided Visual Feature Extraction (AVFE) with cross-attention, and Region-Aware Contrastive Learning (RACL) for consistent attention.

Result: Experiments on PA100K, PETA, and RAPv1 datasets show FOCUS's effectiveness and generalization ability.

Conclusion: FOCUS improves PAR by adaptively extracting attribute-level features, enhancing performance and practicality.

Abstract: Pedestrian attribute recognition (PAR) is a fundamental perception task in
intelligent transportation and security. To tackle this fine-grained task, most
existing methods focus on extracting regional features to enrich attribute
information. However, a regional feature is typically used to predict a fixed
set of pre-defined attributes in these methods, which limits the performance
and practicality in two aspects: 1) Regional features may compromise
fine-grained patterns unique to certain attributes in favor of capturing common
characteristics shared across attributes. 2) Regional features cannot
generalize to predict unseen attributes in the test time. In this paper, we
propose the \textbf{F}ine-grained \textbf{O}ptimization with semanti\textbf{C}
g\textbf{U}ided under\textbf{S}tanding (FOCUS) approach for PAR, which
adaptively extracts fine-grained attribute-level features for each attribute
individually, regardless of whether the attributes are seen or not during
training. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to
capture latent features at varying levels of visual granularity, thereby
enriching the diversity of the extracted information. Next, we introduce the
Attribute-guided Visual Feature Extraction (AVFE) module, which leverages
textual attributes as queries to retrieve their corresponding visual attribute
features from the Mix Tokens using a cross-attention mechanism. To ensure that
textual attributes focus on the appropriate Mix Tokens, we further incorporate
a Region-Aware Contrastive Learning (RACL) method, encouraging attributes
within the same region to share consistent attention maps. Extensive
experiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness
and strong generalization ability of our method.

</details>


### [130] [AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results](https://arxiv.org/abs/2506.22843)
*Kien Nguyen,Clinton Fookes,Sridha Sridharan,Huy Nguyen,Feng Liu,Xiaoming Liu,Arun Ross,Dana Michalski,Tamás Endrei,Ivan DeAndres-Tame,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez,Javier Ortega-Garcia,Zijing Gong,Yuhao Wang,Xuehu Liu,Pingping Zhang,Md Rashidunnabi,Hugo Proença,Kailash A. Hambarde,Saeid Rezaei*

Main category: cs.CV

TL;DR: The AG-VPReID 2025 Challenge introduces a large-scale video-based competition for aerial-ground person re-identification, addressing challenges like viewpoint differences and occlusions. The winning method achieved over 70% Rank-1 accuracy.


<details>
  <summary>Details</summary>
Motivation: To bridge the aerial-ground domain gap in person re-identification, crucial for surveillance and public safety, by leveraging a new dataset and advanced methods.

Method: The challenge utilized the AG-VPReID dataset with 3,027 identities and 3.7 million frames. Teams employed multi-stream architectures, transformer-based temporal reasoning, and physics-informed modeling.

Result: The leading approach, X-TFCLIP, achieved 72.28% (aerial-to-ground) and 70.77% (ground-to-aerial) Rank-1 accuracy, outperforming baselines.

Conclusion: The AG-VPReID 2025 Challenge advances aerial-ground ReID, demonstrating the effectiveness of innovative methods despite dataset complexity.

Abstract: Person re-identification (ReID) across aerial and ground vantage points has
become crucial for large-scale surveillance and public safety applications.
Although significant progress has been made in ground-only scenarios, bridging
the aerial-ground domain gap remains a formidable challenge due to extreme
viewpoint differences, scale variations, and occlusions. Building upon the
achievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID
2025 Challenge - the first large-scale video-based competition focused on
high-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID
dataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7
million frames captured from UAVs, CCTV, and wearable cameras, the challenge
featured four international teams. These teams developed solutions ranging from
multi-stream architectures to transformer-based temporal reasoning and
physics-informed modeling. The leading approach, X-TFCLIP from UAM, attained
72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the
ground-to-aerial ReID setting, surpassing existing baselines while highlighting
the dataset's complexity. For additional details, please refer to the official
website at https://agvpreid25.github.io.

</details>


### [131] [DMD-Net: Deep Mesh Denoising Network](https://arxiv.org/abs/2506.22850)
*Aalok Gangopadhyay,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: DMD-Net is a deep learning framework for mesh denoising using a dual-stream GCN and a Feature Guided Transformer, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the mesh denoising problem with a robust, end-to-end deep learning solution that handles high noise levels effectively.

Method: Uses a Graph Convolutional Neural Network with primal-dual fusion and a Feature Guided Transformer (feature extractor, transformer, denoiser) for denoising.

Result: Competitive or superior performance compared to state-of-the-art methods, even with extreme noise.

Conclusion: DMD-Net is effective, robust, and outperforms existing mesh denoising algorithms.

Abstract: We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning
framework, for solving the mesh denoising problem. DMD-Net consists of a Graph
Convolutional Neural Network in which aggregation is performed in both the
primal as well as the dual graph. This is realized in the form of an asymmetric
two-stream network, which contains a primal-dual fusion block that enables
communication between the primal-stream and the dual-stream. We develop a
Feature Guided Transformer (FGT) paradigm, which consists of a feature
extractor, a transformer, and a denoiser. The feature extractor estimates the
local features, that guide the transformer to compute a transformation, which
is applied to the noisy input mesh to obtain a useful intermediate
representation. This is further processed by the denoiser to obtain the
denoised mesh. Our network is trained on a large scale dataset of 3D objects.
We perform exhaustive ablation studies to demonstrate that each component in
our network is essential for obtaining the best performance. We show that our
method obtains competitive or better results when compared with the
state-of-the-art mesh denoising algorithms. We demonstrate that our method is
robust to various kinds of noise. We observe that even in the presence of
extremely high noise, our method achieves excellent performance.

</details>


### [132] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
*Li-Cheng Shen,Jih-Kang Hsieh,Wei-Hua Li,Chu-Song Chen*

Main category: cs.CV

TL;DR: MaTIR unifies text-to-image retrieval (TIR) and referring expression segmentation (RES) for efficient search and accurate segmentation, using a two-stage framework with SAM 2 and Alpha-CLIP for offline mask generation and MLLM for reranking.


<details>
  <summary>Details</summary>
Motivation: Existing TIR lacks interpretability, while RES is computationally expensive. MaTIR bridges this gap by combining both tasks.

Method: Two-stage framework: (1) segmentation-aware retrieval using SAM 2 and Alpha-CLIP for offline mask and embedding generation, (2) reranking and grounding with MLLM.

Result: Improved retrieval accuracy and segmentation quality on COCO and D$^3$ datasets.

Conclusion: MaTIR effectively unifies TIR and RES, offering scalable and accurate results.

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [133] [Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception](https://arxiv.org/abs/2506.22866)
*Hang-Cheng Dong,Lu Zou,Bingguo Liu,Dong Ye,Guodong Liu*

Main category: cs.CV

TL;DR: A novel weakly supervised semantic segmentation framework for industrial defect detection, using region-aware CAM and pseudo-label training, outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the reliance on large annotated datasets in defect detection by proposing a weakly supervised approach.

Method: Introduces filtering-guided backpropagation (FGBP) for refined CAM and a region-aware weighted module, followed by pseudo-label training.

Result: Demonstrates superior performance on industrial defect datasets, bridging weakly supervised learning and high-precision segmentation.

Conclusion: The framework offers a practical solution for defect detection in resource-limited industrial settings.

Abstract: Surface defect detection plays a critical role in industrial quality
inspection. Recent advances in artificial intelligence have significantly
enhanced the automation level of detection processes. However, conventional
semantic segmentation and object detection models heavily rely on large-scale
annotated datasets, which conflicts with the practical requirements of defect
detection tasks. This paper proposes a novel weakly supervised semantic
segmentation framework comprising two key components: a region-aware class
activation map (CAM) and pseudo-label training. To address the limitations of
existing CAM methods, especially low-resolution thermal maps, and insufficient
detail preservation, we introduce filtering-guided backpropagation (FGBP),
which refines target regions by filtering gradient magnitudes to identify areas
with higher relevance to defects. Building upon this, we further develop a
region-aware weighted module to enhance spatial precision. Finally,
pseudo-label segmentation is implemented to refine the model's performance
iteratively. Comprehensive experiments on industrial defect datasets
demonstrate the superiority of our method. The proposed framework effectively
bridges the gap between weakly supervised learning and high-precision defect
segmentation, offering a practical solution for resource-constrained industrial
scenarios.

</details>


### [134] [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](https://arxiv.org/abs/2506.22868)
*Junsung Lee,Junoh Kang,Bohyung Han*

Main category: cs.CV

TL;DR: STR-Match is a training-free video editing method using latent optimization guided by a novel STR score to improve spatiotemporal consistency and visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing text-guided video editing methods suffer from temporal inconsistency, motion distortion, and limited domain transformation due to insufficient modeling of spatiotemporal pixel relevance.

Method: Proposes STR-Match, leveraging 2D spatial attention and 1D temporal modules in T2V diffusion models to compute the STR score, avoiding costly 3D attention. Uses latent optimization with a latent mask for editing.

Result: Produces visually appealing, spatiotemporally coherent videos, even under significant domain transformations, outperforming existing methods in quality and consistency.

Conclusion: STR-Match effectively addresses limitations of prior methods, offering a robust solution for text-guided video editing with high spatiotemporal fidelity.

Abstract: Previous text-guided video editing methods often suffer from temporal
inconsistency, motion distortion, and-most notably-limited domain
transformation. We attribute these limitations to insufficient modeling of
spatiotemporal pixel relevance during the editing process. To address this, we
propose STR-Match, a training-free video editing algorithm that produces
visually appealing and spatiotemporally coherent videos through latent
optimization guided by our novel STR score. The score captures spatiotemporal
pixel relevance across adjacent frames by leveraging 2D spatial attention and
1D temporal modules in text-to-video (T2V) diffusion models, without the
overhead of computationally expensive 3D attention mechanisms. Integrated into
a latent optimization framework with a latent mask, STR-Match generates
temporally consistent and visually faithful videos, maintaining strong
performance even under significant domain transformations while preserving key
visual attributes of the source. Extensive experiments demonstrate that
STR-Match consistently outperforms existing methods in both visual quality and
spatiotemporal consistency.

</details>


### [135] [Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder](https://arxiv.org/abs/2506.22880)
*Dang Jisheng,Wu Xudong,Wang Bimei,Lv Ning,Chen Jiayu,Jingwen Zhao,Yichu liu,Jizhao Liu,Juncheng Li,Teng Wang*

Main category: cs.CV

TL;DR: DeSa2VA improves video segmentation by decoupling visual and semantic features using text pre-training and a linear decoupling module, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Sa2VA entangle visual and semantic features, reducing segmentation accuracy. DeSa2VA aims to decouple these features for better performance.

Method: Uses text pre-training to generate point-level prompts and text masks, a linear decoupling module for feature separation, and dynamic mask fusion for combining features.

Result: Achieves top performance in tasks like image/video segmentation and question answering.

Conclusion: DeSa2VA effectively decouples features, enhancing segmentation accuracy and grounding capabilities.

Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA,
directly fuse features within segmentation models. This often results in an
undesirable entanglement of dynamic visual information and static semantics,
thereby degrading segmentation accuracy. To systematically mitigate this issue,
we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text
pre-training and a linear decoupling module to address the information
processing limitations inherent in SAM-2. Specifically, first, we devise a
pre-training paradigm that converts textual ground-truth labels into
point-level prompts while generating corresponding text masks. These masks are
refined through a hybrid loss function to strengthen the model's semantic
grounding capabilities. Next, we employ linear projection to disentangle hidden
states that generated by a large language model into distinct textual and
visual feature subspaces. Finally, a dynamic mask fusion strategy
synergistically combines these decoupled features through triple supervision
from predicted text/visual masks and ground-truth annotations. Extensive
experiments demonstrate state-of-the-art performance across diverse tasks,
including image segmentation, image question answering, video segmentation, and
video question answering. Our codes are available at
https://github.com/longmalongma/DeSa2VA.

</details>


### [136] [How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings](https://arxiv.org/abs/2506.22881)
*Fumiya Uchiyama,Rintaro Yanagi,Shohei Taniguchi,Shota Takashiro,Masahiro Suzuki,Hirokatsu Kataoka,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: The paper introduces a semantic informativeness metric for images and texts using contrastive learning, redefining Information Gain for vision and language domains. It shows strong correlation in empirical results and is computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To address whether contrastive learning can represent absolute semantic informativeness, not just relational similarity, and to extend Information Gain to vision and language.

Method: Proposes a metric for semantic informativeness using contrastive learning models (CLIP or SigLIP), measuring how conditioning on an image or text distorts associated distributions. Also suggests a norm-based metric for Information Gain.

Result: Empirical results show strong correlation (R² 0.98-1.00) and identify low-scoring images as placeholders (e.g., 'image not found'). The method is computationally efficient and compatible with open-weight models.

Conclusion: The proposed metric effectively quantifies semantic informativeness, bridging vision and language domains with practical applications and computational scalability.

Abstract: Contrastive learning has the capacity to model multimodal probability
distributions by embedding and aligning visual representations with semantics
from captions. This approach enables the estimation of relational semantic
similarity; however, it remains unclear whether it can also represent absolute
semantic informativeness. In this work, we introduce a semantic informativeness
metric for an image calculated from text samples via a contrastive learning
model; similarly, the informativeness of a text is calculated from image
samples. We propose a redefinition of the concept of Information Gain, a
concept previously explored in natural language processing, extending its
application to the domains of vision and language. Our metric quantifies how
conditioning on an image distorts the distribution of associated texts, and
vice versa for text conditioning on image distributions. In OpenCLIP's
empirical results, we observe that images with the lowest Information Gain
scores often correspond to placeholder icons such as "image not found."
Furthermore, we propose to measure a norm-based metric of the embedding to
estimate the Information Gain, following the theoretical results for Skip-Gram
with Negative Sampling (SGNS) word embedding. Information Gain can be measured
using either CLIP or SigLIP, and the results demonstrate a strong correlation
with a coefficient of determination ranging from 0.98 to 1.00. After obtaining
the mean and the covariance of the sample embedding, the computational cost of
this method is independent of the sample size, and it is compatible with
publicly available, open-weight models.

</details>


### [137] [CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems](https://arxiv.org/abs/2506.22890)
*Senkang Hu,Yihang Tao,Guowen Xu,Xinyuan Qian,Yiqin Deng,Xianhao Chen,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CV

TL;DR: CP-Guard is a defense framework for Collaborative Perception (CP) that detects and eliminates malicious agents by ensuring consensus among collaborators, using PASAC and CCLoss methods, and adaptive thresholds.


<details>
  <summary>Details</summary>
Motivation: CP enhances perception in multi-agent systems but is vulnerable to attacks from malicious agents, necessitating a robust defense mechanism.

Method: Proposes CP-Guard with PASAC for consensus sampling, CCLoss for discrepancy measurement, and adaptive thresholds for dynamic environments.

Result: Extensive experiments show CP-Guard effectively detects and removes malicious agents, ensuring reliable CP performance.

Conclusion: CP-Guard provides a unified, adaptive solution to secure CP systems against malicious attacks, with proven effectiveness.

Abstract: Collaborative Perception (CP) has been shown to be a promising technique for
multi-agent autonomous driving and multi-agent robotic systems, where multiple
agents share their perception information to enhance the overall perception
performance and expand the perception range. However, in CP, an ego agent needs
to receive messages from its collaborators, which makes it vulnerable to
attacks from malicious agents. To address this critical issue, we propose a
unified, probability-agnostic, and adaptive framework, namely, CP-Guard, which
is a tailored defense mechanism for CP deployed by each agent to accurately
detect and eliminate malicious agents in its collaboration network. Our key
idea is to enable CP to reach a consensus rather than a conflict against an ego
agent's perception results. Based on this idea, we first develop a
probability-agnostic sample consensus (PASAC) method to effectively sample a
subset of the collaborators and verify the consensus without prior
probabilities of malicious agents. Furthermore, we define collaborative
consistency loss (CCLoss) for object detection task and bird's eye view (BEV)
segmentation task to capture the discrepancy between an ego agent and its
collaborators, which is used as a verification criterion for consensus. In
addition, we propose online adaptive threshold via dual sliding windows to
dynamically adjust the threshold for consensus verification and ensure the
reliability of the systems in dynamic environments. Finally, we conduct
extensive experiments and demonstrate the effectiveness of our framework. Code
will be released at https://github.com/CP-Security/CP-Guard

</details>


### [138] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: Neural Cellular Automata (NCAs) with an implicit decoder enable high-resolution, real-time outputs while preserving emergent properties, overcoming previous limitations in training, memory, and compute demands.


<details>
  <summary>Details</summary>
Motivation: NCAs are limited to low-resolution grids due to quadratic growth in training time/memory, local information propagation, and high compute demands. This work aims to scale NCAs to high-resolution outputs efficiently.

Method: Pair NCA with a shared implicit decoder for rendering high-resolution outputs after coarse-grid evolution. Introduce novel loss functions tailored for high-resolution tasks.

Result: The framework achieves real-time full-HD outputs, maintains emergent properties, and remains parallelizable and efficient across 2D/3D grids and meshes.

Conclusion: The proposed architecture and loss functions enable NCAs to scale to high-resolution tasks with minimal overhead, demonstrating broad applicability in texture synthesis and morphogenesis.

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [139] [MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering](https://arxiv.org/abs/2506.22900)
*Mai A. Shaaban,Tausifa Jan Saleem,Vijay Ram Papineni,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MOTOR improves MedVQA accuracy by 6.45% using multimodal retrieval and re-ranking with grounded captions and optimal transport.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs for MedVQA often produce incorrect answers, and retrieval-augmented methods risk irrelevant context. Current re-ranking ignores visual/multimodal context, critical for medical diagnosis.

Method: Proposes MOTOR, a multimodal retrieval and re-ranking approach using grounded captions and optimal transport to align query and context textually and visually.

Result: MOTOR outperforms state-of-the-art methods by 6.45% on MedVQA datasets, validated by empirical analysis and expert evaluation.

Conclusion: MOTOR enhances MedVQA by integrating visual and textual context, improving accuracy and clinical relevance.

Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical
decision-making by providing contextually rich answers to image-based queries.
Although vision-language models (VLMs) are widely used for this task, they
often generate factually incorrect answers. Retrieval-augmented generation
addresses this challenge by providing information from external sources, but
risks retrieving irrelevant context, which can degrade the reasoning
capabilities of VLMs. Re-ranking retrievals, as introduced in existing
approaches, enhances retrieval relevance by focusing on query-text alignment.
However, these approaches neglect the visual or multimodal context, which is
particularly crucial for medical diagnosis. We propose MOTOR, a novel
multimodal retrieval and re-ranking approach that leverages grounded captions
and optimal transport. It captures the underlying relationships between the
query and the retrieved context based on textual and visual information.
Consequently, our approach identifies more clinically relevant contexts to
augment the VLM input. Empirical analysis and human expert evaluation
demonstrate that MOTOR achieves higher accuracy on MedVQA datasets,
outperforming state-of-the-art methods by an average of 6.45%. Code is
available at https://github.com/BioMedIA-MBZUAI/MOTOR.

</details>


### [140] [Point Cloud Compression and Objective Quality Assessment: A Survey](https://arxiv.org/abs/2506.22902)
*Yiling Xu,Yujie Zhang,Shuting Xia,Kaifa Yang,He Huang,Ziyu Shan,Wenjie Huang,Qi Yang,Le Yang*

Main category: cs.CV

TL;DR: A survey of recent advances in point cloud compression (PCC) and quality assessment (PCQA), highlighting challenges and future directions for efficient 3D applications.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of 3D point cloud data in fields like autonomous driving and robotics necessitates efficient compression and quality assessment techniques due to their irregular structure and high volume.

Method: The paper reviews handcrafted and learning-based PCC algorithms and objective PCQA metrics, benchmarking them on emerging datasets for detailed comparisons.

Result: The survey identifies strengths and limitations of current methods, noting challenges like visual fidelity, latency, and multimodal data support.

Conclusion: Future directions include hybrid compression frameworks and advanced feature extraction to improve efficiency and intelligence in 3D applications.

Abstract: The rapid growth of 3D point cloud data, driven by applications in autonomous
driving, robotics, and immersive environments, has led to criticals demand for
efficient compression and quality assessment techniques. Unlike traditional 2D
media, point clouds present unique challenges due to their irregular structure,
high data volume, and complex attributes. This paper provides a comprehensive
survey of recent advances in point cloud compression (PCC) and point cloud
quality assessment (PCQA), emphasizing their significance for real-time and
perceptually relevant applications. We analyze a wide range of handcrafted and
learning-based PCC algorithms, along with objective PCQA metrics. By
benchmarking representative methods on emerging datasets, we offer detailed
comparisons and practical insights into their strengths and limitations.
Despite notable progress, challenges such as enhancing visual fidelity,
reducing latency, and supporting multimodal data remain. This survey outlines
future directions, including hybrid compression frameworks and advanced feature
extraction strategies, to enable more efficient, immersive, and intelligent 3D
applications.

</details>


### [141] [MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances](https://arxiv.org/abs/2506.22907)
*Yunzhe Shao,Xinyu Yi,Lu Yin,Shihui Guo,Junhai Yong,Feng Xu*

Main category: cs.CV

TL;DR: MagShield is a novel method for mitigating magnetic interference in sparse inertial MoCap systems by detecting and correcting disturbances using multi-IMU analysis and motion priors.


<details>
  <summary>Details</summary>
Motivation: Existing IMU systems suffer from orientation errors in magnetically disturbed environments, limiting real-world usability.

Method: MagShield uses a 'detect-then-correct' approach: detecting disturbances via multi-IMU joint analysis and correcting errors with human motion priors.

Result: MagShield improves motion capture accuracy under magnetic interference and is compatible with various sparse inertial MoCap systems.

Conclusion: MagShield effectively addresses magnetic interference, enhancing the practicality of sparse inertial MoCap systems in real-world scenarios.

Abstract: This paper proposes a novel method called MagShield, designed to address the
issue of magnetic interference in sparse inertial motion capture (MoCap)
systems. Existing Inertial Measurement Unit (IMU) systems are prone to
orientation estimation errors in magnetically disturbed environments, limiting
their practical application in real-world scenarios. To address this problem,
MagShield employs a "detect-then-correct" strategy, first detecting magnetic
disturbances through multi-IMU joint analysis, and then correcting orientation
errors using human motion priors. MagShield can be integrated with most
existing sparse inertial MoCap systems, improving their performance in
magnetically disturbed environments. Experimental results demonstrate that
MagShield significantly enhances the accuracy of motion capture under magnetic
interference and exhibits good compatibility across different sparse inertial
MoCap systems.

</details>


### [142] [Attention to Burstiness: Low-Rank Bilinear Prompt Tuning](https://arxiv.org/abs/2506.22908)
*Yuzhu Wang,Manni Duan,Shu Kong*

Main category: cs.CV

TL;DR: Visual Prompt Tuning (VPT) adapts pre-trained ViTs with prompts but faces challenges due to non-Gaussian distributions. Whitening data improves learning, leading to Bilinear Prompt Tuning (BPT), which outperforms VPT in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of learning prompts in VPT due to non-Gaussian distributions in patch embeddings and key/query projectors.

Method: Propose whitening data to de-correlate and equalize variance, then introduce Bilinear Prompt Tuning (BPT) for efficient prompt learning.

Result: BPT significantly boosts accuracy (e.g., +25 points on CUB) and reduces parameter count and computation overhead.

Conclusion: BPT is a superior alternative to VPT, offering improved performance and efficiency in prompt tuning for vision Transformers.

Abstract: Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique
that adapts a pre-trained vision Transformer (ViT) by learning a small set of
parameters in the input space, known as prompts. In VPT, we uncover
``burstiness'' in the values arising from the interaction of image patch
embeddings, and the key and query projectors within Transformer's
self-attention module. Furthermore, the values of patch embeddings and the key
and query projectors exhibit Laplacian and hyper-Laplacian distribution,
respectively. Intuitively, these non-Gaussian distributions pose challenges for
learning prompts. To address this, we propose whitening these data,
de-correlating them and equalizing their variance towards more Gaussian before
learning prompts. We derive the whitening matrix over random image patch
embeddings and ViT's key and query projectors, and multiply it with the prompt
to be learned in a bilinear manner. Surprisingly, this method significantly
accelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on
the CUB dataset; interestingly, it learns ``bursty prompts''. Extending the
bilinear model which is known to introduce burstiness, we present a compact,
low-rank version by learning two smaller matrices whose multiplication yields
the final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).
Extensive experiments across multiple benchmark datasets demonstrate that BPT
methods not only outperform various VPT methods but also reduce parameter count
and computation overhead.

</details>


### [143] [Towards Explainable Bilingual Multimodal Misinformation Detection and Localization](https://arxiv.org/abs/2506.22930)
*Yiwei He,Xiangtai Li,Zhenglin Huang,Yi Dong,Hao Fei,Jiangning Zhang,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: BiMi is a bilingual multimodal framework for detecting subtle misinformation in news media by analyzing region-level edits and cross-lingual inconsistencies, outperforming baselines in accuracy and explanation quality.


<details>
  <summary>Details</summary>
Motivation: The rise of realistic multimodal misinformation, especially in bilingual news media, necessitates advanced detection tools to address localized edits and cross-lingual inconsistencies.

Method: BiMi combines region-level localization, cross-modal/lingual consistency detection, and natural language explanation, enhanced by an online retrieval module and GRPO for interpretability.

Result: BiMi achieves +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore, outperforming baselines.

Conclusion: BiMi advances multilingual misinformation detection with superior performance and interpretability, supported by the BiMiBench dataset.

Abstract: The increasing realism of multimodal content has made misinformation more
subtle and harder to detect, especially in news media where images are
frequently paired with bilingual (e.g., Chinese-English) subtitles. Such
content often includes localized image edits and cross-lingual inconsistencies
that jointly distort meaning while remaining superficially plausible. We
introduce BiMi, a bilingual multimodal framework that jointly performs
region-level localization, cross-modal and cross-lingual consistency detection,
and natural language explanation for misinformation analysis. To support
generalization, BiMi integrates an online retrieval module that supplements
model reasoning with up-to-date external context. We further release BiMiBench,
a large-scale and comprehensive benchmark constructed by systematically editing
real news images and subtitles, comprising 104,000 samples with realistic
manipulations across visual and linguistic modalities. To enhance
interpretability, we apply Group Relative Policy Optimization (GRPO) to improve
explanation quality, marking the first use of GRPO in this domain. Extensive
experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in
classification accuracy, +15.9 in localization accuracy, and +2.5 in
explanation BERTScore, advancing state-of-the-art performance in realistic,
multilingual misinformation detection. Code, models, and datasets will be
released.

</details>


### [144] [Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data](https://arxiv.org/abs/2506.22939)
*Ghufran A. Omran,Wassan Saad Abduljabbar Hayale,Ahmad AbdulQadir AlRababah,Israa Ibraheem Al-Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar,Harshavardhan Reddy Penubadi*

Main category: cs.CV

TL;DR: The paper introduces CO-BRNN for scene categorization in remote sensing, achieving 97% accuracy, outperforming other methods like LSTM-CRF and MLP-CNN.


<details>
  <summary>Details</summary>
Motivation: High accuracy in scene categorization from remote sensing is challenging due to noise and data variety. Traditional deep learning models struggle with these issues.

Method: Proposes Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO-BRNN) and compares it with MLP-CNN, CNN-LSTM, LSTM-CRF, GB, MIRM-CF, and CNN-DA.

Result: CO-BRNN achieved 97% accuracy, surpassing LSTM-CRF (90%), MLP-CNN (85%), and CNN-LSTM (80%).

Conclusion: CO-BRNN is effective for scene categorization, and physical verification is crucial for satellite data efficiency.

Abstract: Scene categorization (SC) in remotely acquired images is an important subject
with broad consequences in different fields, including catastrophe control,
ecological observation, architecture for cities, and more. Nevertheless, its
several apps, reaching a high degree of accuracy in SC from distant observation
data has demonstrated to be difficult. This is because traditional conventional
deep learning models require large databases with high variety and high levels
of noise to capture important visual features. To address these problems, this
investigation file introduces an innovative technique referred to as the
Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type
of scenes in remote sensing data. The investigation compares the execution of
CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional
Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory
(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),
Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional
Neural Networks Data Augmentation (CNN-DA). The results demonstrate that
CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,
MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance
of physical confirmation to ensure the efficiency of satellite data.

</details>


### [145] [YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging](https://arxiv.org/abs/2506.22955)
*Haniyeh Nikkhah,Jafar Tanha,Mahdi Zarrin,SeyedEhsan Roshan,Amin Kazempour*

Main category: cs.CV

TL;DR: YM-WML, a novel model for cardiac image segmentation, integrates a robust backbone, YOLOv11 neck, and attention-based head, achieving 91.02 Dice score on ACDC dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing class imbalance and complex structure in medical image segmentation.

Method: YM-WML combines a backbone for feature extraction, YOLOv11 neck for multi-scale aggregation, and an attention-based head. Uses WME loss for class imbalance.

Result: Achieves 91.02 Dice score on ACDC dataset, outperforming state-of-the-art methods.

Conclusion: YM-WML sets a new benchmark in cardiac segmentation with stable training and strong generalization.

Abstract: Medical image segmentation poses significant challenges due to class
imbalance and the complex structure of medical images. To address these
challenges, this study proposes YM-WML, a novel model for cardiac image
segmentation. The model integrates a robust backbone for effective feature
extraction, a YOLOv11 neck for multi-scale feature aggregation, and an
attention-based segmentation head for precise and accurate segmentation. To
address class imbalance, we introduce the Weighted Multi-class Exponential
(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity
Coefficient of 91.02, outperforming state-of-the-art methods. The model
demonstrates stable training, accurate segmentation, and strong generalization,
setting a new benchmark in cardiac segmentation tasks.

</details>


### [146] [Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images](https://arxiv.org/abs/2506.22960)
*Shreyas Dixit,Ashhar Aziz,Shashwat Bajpai,Vasu Sharma,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.CV

TL;DR: PECCAVI is a new watermarking technique for AI-generated content, resistant to visual paraphrase attacks by embedding watermarks in Non-Melting Points (NMPs) and using multi-channel frequency domain methods.


<details>
  <summary>Details</summary>
Motivation: The rise of synthetic content and vulnerabilities in existing watermarking methods, especially against de-watermarking attacks, necessitate a more robust solution.

Method: PECCAVI embeds watermarks in NMPs, uses multi-channel frequency domain watermarking, and noisy burnishing to counter reverse-engineering.

Result: PECCAVI provides a distortion-free, attack-resistant watermarking solution for AI-generated images.

Conclusion: PECCAVI offers a durable, model-agnostic watermarking technique to address vulnerabilities in current methods, with plans for open-sourcing.

Abstract: A report by the European Union Law Enforcement Agency predicts that by 2026,
up to 90 percent of online content could be synthetically generated, raising
concerns among policymakers, who cautioned that "Generative AI could act as a
force multiplier for political disinformation. The combined effect of
generative text, images, videos, and audio may surpass the influence of any
single modality." In response, California's Bill AB 3211 mandates the
watermarking of AI-generated images, videos, and audio. However, concerns
remain regarding the vulnerability of invisible watermarking techniques to
tampering and the potential for malicious actors to bypass them entirely.
Generative AI-powered de-watermarking attacks, especially the newly introduced
visual paraphrase attack, have shown an ability to fully remove watermarks,
resulting in a paraphrase of the original image. This paper introduces PECCAVI,
the first visual paraphrase attack-safe and distortion-free image watermarking
technique. In visual paraphrase attacks, an image is altered while preserving
its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI
strategically embeds watermarks within these NMPs and employs multi-channel
frequency domain watermarking. It also incorporates noisy burnishing to counter
reverse-engineering efforts aimed at locating NMPs to disrupt the embedded
watermark, thereby enhancing durability. PECCAVI is model-agnostic. All
relevant resources and codes will be open-sourced.

</details>


### [147] [ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](https://arxiv.org/abs/2506.22967)
*Amir Aghdam,Vincent Tao Hu*

Main category: cs.CV

TL;DR: ActAlign is a zero-shot framework for fine-grained video classification using sequence alignment with language-generated sub-actions, outperforming larger models without video-text supervision.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive vision-language models lack temporal structure understanding for fine-grained video classification.

Method: ActAlign uses a large language model to generate sub-action sequences and aligns them with video frames via Dynamic Time Warping (DTW) in a shared embedding space.

Result: Achieves 30.5% accuracy on ActionAtlas (human accuracy: 61.6%) with 8x fewer parameters than billion-parameter models.

Conclusion: Structured language priors and classical alignment techniques enhance vision-language models for fine-grained video understanding.

Abstract: We address the task of zero-shot fine-grained video classification, where no
video examples or temporal annotations are available for unseen action classes.
While contrastive vision-language models such as SigLIP demonstrate strong
open-set recognition via mean-pooled image-text similarity, they fail to
capture the temporal structure critical for distinguishing fine-grained
activities. We introduce ActAlign, a zero-shot framework that formulates video
classification as sequence alignment. For each class, a large language model
generates an ordered sub-action sequence, which is aligned with video frames
using Dynamic Time Warping (DTW) in a shared embedding space. Without any
video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the
extremely challenging ActionAtlas benchmark, where human accuracy is only
61.6%. ActAlign outperforms billion-parameter video-language models while using
approximately 8x less parameters. These results demonstrate that structured
language priors, combined with classical alignment techniques, offer a scalable
and general approach to unlocking the open-set recognition potential of
vision-language models for fine-grained video understanding.

</details>


### [148] [Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation](https://arxiv.org/abs/2506.22979)
*Jie Liu,Jiayi Shen,Pan Zhou,Jan-Jakob Sonke,Efstratios Gavves*

Main category: cs.CV

TL;DR: FewCLIP introduces a probabilistic prototype calibration framework for GFSS, improving adaptability and generalization over deterministic methods.


<details>
  <summary>Details</summary>
Motivation: Existing prototype-based methods in GFSS are deterministic, limiting adaptability to diverse samples, especially for novel classes with few annotations.

Method: FewCLIP refines frozen textual prototypes with learnable visual calibration prototypes and introduces distribution regularization for uncertainty-aware learning.

Result: FewCLIP outperforms state-of-the-art methods on PASCAL-5$^i$ and COCO-20$^i$ datasets in GFSS and class-incremental settings.

Conclusion: FewCLIP provides a more adaptive and generalized solution for GFSS by leveraging probabilistic prototype calibration.

Abstract: Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a
segmentation model to novel classes with only a few annotated examples while
maintaining performance on base classes. Recently, pretrained vision-language
models (VLMs) such as CLIP have been leveraged in GFSS to improve
generalization on novel classes through multi-modal prototypes learning.
However, existing prototype-based methods are inherently deterministic,
limiting the adaptability of learned prototypes to diverse samples,
particularly for novel classes with scarce annotations. To address this, we
propose FewCLIP, a probabilistic prototype calibration framework over
multi-modal prototypes from the pretrained CLIP, thus providing more adaptive
prototype learning for GFSS. Specifically, FewCLIP first introduces a prototype
calibration mechanism, which refines frozen textual prototypes with learnable
visual calibration prototypes, leading to a more discriminative and adaptive
representation. Furthermore, unlike deterministic prototype learning
techniques, FewCLIP introduces distribution regularization over these
calibration prototypes. This probabilistic formulation ensures structured and
uncertainty-aware prototype learning, effectively mitigating overfitting to
limited novel class data while enhancing generalization. Extensive experimental
results on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed
FewCLIP significantly outperforms state-of-the-art approaches across both GFSS
and class-incremental setting. The code is available at
https://github.com/jliu4ai/FewCLIP.

</details>


### [149] [Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models](https://arxiv.org/abs/2506.22982)
*Atharv Mittal,Agam Pandey,Amritanshu Tiwari,Sukrit Jindal,Swadesh Swain*

Main category: cs.CV

TL;DR: The paper validates and improves the Cross-Prompt Attack (CroPA) on Vision-Language Models (VLMs), enhancing adversarial transferability and attack success rates.


<details>
  <summary>Details</summary>
Motivation: VLMs are vulnerable to adversarial attacks, especially when both visual and textual modalities are manipulated. The study aims to reproduce and improve CroPA.

Method: The study replicates CroPA and introduces three improvements: a novel initialization strategy, investigation of cross-image transferability, and a new loss function targeting vision encoder attention.

Result: The improved CroPA outperforms baselines, showing higher attack success rates and better generalization across VLMs like Flamingo, BLIP-2, and InstructBLIP.

Conclusion: The work highlights VLMs' adversarial vulnerabilities and offers a robust framework for generating transferable adversarial examples, impacting real-world VLM security.

Abstract: Large Vision-Language Models (VLMs) have revolutionized computer vision,
enabling tasks such as image classification, captioning, and visual question
answering. However, they remain highly vulnerable to adversarial attacks,
particularly in scenarios where both visual and textual modalities can be
manipulated. In this study, we conduct a comprehensive reproducibility study of
"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on
Vision-Language Models" validating the Cross-Prompt Attack (CroPA) and
confirming its superior cross-prompt transferability compared to existing
baselines. Beyond replication we propose several key improvements: (1) A novel
initialization strategy that significantly improves Attack Success Rate (ASR).
(2) Investigate cross-image transferability by learning universal
perturbations. (3) A novel loss function targeting vision encoder attention
mechanisms to improve generalization. Our evaluation across prominent VLMs --
including Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on
LLaVA validates the original results and demonstrates that our improvements
consistently boost adversarial effectiveness. Our work reinforces the
importance of studying adversarial vulnerabilities in VLMs and provides a more
robust framework for generating transferable adversarial examples, with
significant implications for understanding the security of VLMs in real-world
applications.

</details>


### [150] [A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks](https://arxiv.org/abs/2506.23004)
*Vaigai Nayaki Yokar,Hoa Le-Minh,Xicong Li,Wai Lok Woo,Luis Nero Alves,Stanislav Zvanovec,Tran The Son,Zabih Ghassemlooy*

Main category: cs.CV

TL;DR: A lightweight CNN-based method for frame identification and synchronization in S2C VLC systems, achieving 98.74% accuracy.


<details>
  <summary>Details</summary>
Motivation: Enhance short-link communication performance in S2C VLC systems by addressing challenges like blurring, cropping, and rotated images.

Method: Supervised CNN model developed in Python/TensorFlow Keras, trained with real-time experiments and a custom dataset.

Result: Achieved 98.74% accuracy in frame identification and synchronization.

Conclusion: The proposed CNN model is effective for improving S2C VLC system performance.

Abstract: This paper proposes a novel, robust, and lightweight supervised Convolutional
Neural Network (CNN)-based technique for frame identification and
synchronization, designed to enhance short-link communication performance in a
screen-to-camera (S2C) based visible light communication (VLC) system.
Developed using Python and the TensorFlow Keras framework, the proposed CNN
model was trained through three real-time experimental investigations conducted
in Jupyter Notebook. These experiments incorporated a dataset created from
scratch to address various real-time challenges in S2C communication, including
blurring, cropping, and rotated images in mobility scenarios. Overhead frames
were introduced for synchronization, which leads to enhanced system
performance. The experimental results demonstrate that the proposed model
achieves an overall accuracy of approximately 98.74%, highlighting its
effectiveness in identifying and synchronizing frames in S2C VLC systems.

</details>


### [151] [MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2506.23009)
*Jian Chen,Wenye Ma,Penghang Liu,Wei Wang,Tengwei Song,Ming Li,Chenguang Wang,Ruiyi Zhang,Changyou Chen*

Main category: cs.CV

TL;DR: The paper introduces MusiXQA, a dataset for evaluating MLLMs in music sheet understanding, and presents Phi-3-MusiX, a fine-tuned model outperforming GPT-based methods.


<details>
  <summary>Details</summary>
Motivation: To explore and improve MLLMs' ability to interpret music sheets, an underexplored area.

Method: Created MusiXQA, a synthetic music sheet dataset with structured annotations, and fine-tuned Phi-3-MusiX on it.

Result: Current MLLMs have limitations in music sheet understanding; Phi-3-MusiX outperforms GPT-based methods.

Conclusion: MusiXQA and Phi-3-MusiX provide a foundation for advancing MLLMs in music sheet understanding.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable visual
reasoning abilities in natural images, text-rich documents, and graphic
designs. However, their ability to interpret music sheets remains
underexplored. To bridge this gap, we introduce MusiXQA, the first
comprehensive dataset for evaluating and advancing MLLMs in music sheet
understanding. MusiXQA features high-quality synthetic music sheets generated
via MusiXTeX, with structured annotations covering note pitch and duration,
chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.
Through extensive evaluations, we reveal significant limitations of current
state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed
Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant
performance gains over GPT-based methods. The proposed dataset and model
establish a foundation for future advances in MLLMs for music sheet
understanding. Code, data, and model will be released upon acceptance.

</details>


### [152] [VisionScores -- A system-segmented image score dataset for deep learning tasks](https://arxiv.org/abs/2506.23030)
*Alejandro Romero Amezcua,Mariano José Juan Rivera Meraz*

Main category: cs.CV

TL;DR: VisionScores introduces the first system-segmented image score dataset for machine learning, focusing on two-handed piano pieces with 24.8k grayscale images.


<details>
  <summary>Details</summary>
Motivation: To provide structured, high-density images for ML tasks, considering graphic similarity and composition patterns in piano music.

Method: Dataset includes 14k samples of Sonatinas from various composers and 10.8k samples of Franz Liszt's works. All images are 128x512 pixels.

Result: Offers formatted images, system order, metadata, unsegmented scores, and pre-formatted images for analysis.

Conclusion: VisionScores is a valuable resource for ML in music, combining structured data with rich compositional context.

Abstract: VisionScores presents a novel proposal being the first system-segmented image
score dataset, aiming to offer structure-rich, high information-density images
for machine and deep learning tasks. Delimited to two-handed piano pieces, it
was built to consider not only certain graphic similarity but also composition
patterns, as this creative process is highly instrument-dependent. It provides
two scenarios in relation to composer and composition type. The first, formed
by 14k samples, considers works from different authors but the same composition
type, specifically, Sonatinas. The latter, consisting of 10.8K samples,
presents the opposite case, various composition types from the same author,
being the one selected Franz Liszt. All of the 24.8k samples are formatted as
grayscale jpg images of $128 \times 512$ pixels. VisionScores supplies the
users not only the formatted samples but the systems' order and pieces'
metadata. Moreover, unsegmented full-page scores and the pre-formatted images
are included for further analysis.

</details>


### [153] [Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23038)
*Xinrong Hu,Yiyu Shi*

Main category: cs.CV

TL;DR: AugPaint is a data augmentation framework using inpainting to generate synthetic image-label pairs from limited labeled medical data, improving segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Pixel-level labeling for medical datasets is costly and time-consuming, and enhancing segmentation with scarce labeled data is a challenge.

Method: AugPaint uses latent diffusion models for inpainting, conditioning on cropped foreground areas during denoising to generate synthetic images paired with label masks.

Result: AugPaint outperforms state-of-the-art methods on four medical datasets (CT, MRI, skin imaging), significantly boosting segmentation performance.

Conclusion: AugPaint effectively addresses limited annotation challenges by generating accurate synthetic data, enhancing segmentation model training.

Abstract: Collecting pixel-level labels for medical datasets can be a laborious and
expensive process, and enhancing segmentation performance with a scarcity of
labeled data is a crucial challenge. This work introduces AugPaint, a data
augmentation framework that utilizes inpainting to generate image-label pairs
from limited labeled data. AugPaint leverages latent diffusion models, known
for their ability to generate high-quality in-domain images with low overhead,
and adapts the sampling process for the inpainting task without need for
retraining. Specifically, given a pair of image and label mask, we crop the
area labeled with the foreground and condition on it during reversed denoising
process for every noise level. Masked background area would gradually be filled
in, and all generated images are paired with the label mask. This approach
ensures the accuracy of match between synthetic images and label masks, setting
it apart from existing dataset generation methods. The generated images serve
as valuable supervision for training downstream segmentation models,
effectively addressing the challenge of limited annotations. We conducted
extensive evaluations of our data augmentation method on four public medical
image segmentation datasets, including CT, MRI, and skin imaging. Results
across all datasets demonstrate that AugPaint outperforms state-of-the-art
label-efficient methodologies, significantly improving segmentation
performance.

</details>


### [154] [From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting](https://arxiv.org/abs/2506.23042)
*Hung Nguyen,An Le,Runfa Li,Truong Nguyen*

Main category: cs.CV

TL;DR: AutoOpti3DGS reduces Gaussian proliferation in 3D Gaussian Splatting by using learnable wavelet transforms, maintaining visual quality while optimizing memory usage.


<details>
  <summary>Details</summary>
Motivation: The growing set of Gaussian primitives in 3D Gaussian Splatting strains memory and bandwidth, necessitating a solution to restrain proliferation without losing fidelity.

Method: Uses learnable Forward and Inverse Discrete Wavelet Transforms with fixed low-pass and learnable high-pass filters, activated by an orthogonality loss for coarse-to-fine refinement.

Result: Produces sparser scene representations, integrates with existing frameworks, and requires minimal hyper-parameter tuning.

Conclusion: AutoOpti3DGS effectively balances detail and efficiency, making 3DGS more practical for constrained hardware.

Abstract: 3D Gaussian Splatting has emerged as a powerful approach in novel view
synthesis, delivering rapid training and rendering but at the cost of an
ever-growing set of Gaussian primitives that strains memory and bandwidth. We
introduce AutoOpti3DGS, a training-time framework that automatically restrains
Gaussian proliferation without sacrificing visual fidelity. The key idea is to
feed the input images to a sequence of learnable Forward and Inverse Discrete
Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters
are learnable and initialized to zero, and an auxiliary orthogonality loss
gradually activates fine frequencies. This wavelet-driven, coarse-to-fine
process delays the formation of redundant fine Gaussians, allowing 3DGS to
capture global structure first and refine detail only when necessary. Through
extensive experiments, AutoOpti3DGS requires just a single filter learning-rate
hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,
and consistently produces sparser scene representations more compatible with
memory or storage-constrained hardware.

</details>


### [155] [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044)
*Guo-Hua Wang,Shanshan Zhao,Xinjie Zhang,Liangfu Cao,Pengxin Zhan,Lunhao Duan,Shiyin Lu,Minghao Fu,Xiaohao Chen,Jianshan Zhao,Yang Li,Qing-Guo Chen*

Main category: cs.CV

TL;DR: Ovis-U1 is a 3B-parameter unified model excelling in multimodal understanding, text-to-image generation, and image editing, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To integrate multimodal understanding, generation, and editing into a single model, improving performance over task-specific models.

Method: Uses a diffusion-based visual decoder and bidirectional token refiner, trained with a unified approach starting from a language model.

Result: Achieves top scores on benchmarks like OpenCompass (69.6), DPG-Bench (83.72), GenEval (0.89), ImgEdit-Bench (4.00), and GEdit-Bench-EN (6.42).

Conclusion: Ovis-U1 advances multimodal capabilities, setting a new standard for unified models in understanding, generation, and editing.

Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model
that integrates multimodal understanding, text-to-image generation, and image
editing capabilities. Building on the foundation of the Ovis series, Ovis-U1
incorporates a diffusion-based visual decoder paired with a bidirectional token
refiner, enabling image generation tasks comparable to leading models like
GPT-4o. Unlike some previous models that use a frozen MLLM for generation
tasks, Ovis-U1 utilizes a new unified training approach starting from a
language model. Compared to training solely on understanding or generation
tasks, unified training yields better performance, demonstrating the
enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score
of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent
state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In
text-to-image generation, it excels with scores of 83.72 and 0.89 on the
DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves
4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the
initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries
of multimodal understanding, generation, and editing.

</details>


### [156] [Empowering Small VLMs to Think with Dynamic Memorization and Exploration](https://arxiv.org/abs/2506.23061)
*Jiazhen Liu,Yuchuan Deng,Long Chen*

Main category: cs.CV

TL;DR: DyME is a novel training paradigm for small-scale vision-language models (SVLMs) that dynamically switches between memorization (SFT) and exploration (RLVR) modes to enhance thinking reliability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing training paradigms (SFT and RLVR) are ineffective for SVLMs due to their limited capacity, leading to pseudo thinking traces and advantage collapse.

Method: DyME dynamically alternates between SFT (memorization) and RLVR (exploration) during training to optimize updates.

Result: DyME outperforms existing methods, achieving better balance and performance across diverse domains.

Conclusion: DyME is a practical solution for improving SVLMs' thinking capabilities and task performance.

Abstract: Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking
capabilities remains fundamentally challenging due to their limited parameter
capacity and weak instruction-following abilities. Existing training paradigms,
including Supervised Fine-Tuning (SFT) and Reinforcement Learning with
Verifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding
the capabilities of SVLMs. Consequently, directly applying these paradigms to
SVLMs often suffers from severe pseudo thinking traces and advantage collapse,
ultimately undermining both thinking reliability and task performance. A
natural solution is to combine SFT and RLVR, leveraging their complementarity
to reduce the dependence on model capacity. However, the widely adopted
two-stage training paradigm still performs poorly on SVLMs, as their tendency
toward sub-optimal convergence hinders the trade-off and limits the benefits of
the combination. To address this, we propose DyME, a novel training paradigm
that Dynamically selects between Memorization (via SFT) and Exploration (via
RLVR) modes at each optimization step, ensuring that every update contributes
to the trade-off. Extensive experiments across diverse domains demonstrate that
DyME consistently achieves this balance, and thus delivers substantial
performance improvements. These results establish DyME as a practical and
effective solution for empowering SVLMs with reliable thinking capabilities.
GitHub: https://github.com/HKUST-LongGroup/DyME

</details>


### [157] [CoreMark: Toward Robust and Universal Text Watermarking Technique](https://arxiv.org/abs/2506.23066)
*Jiale Meng,Yiming Li,Zheming Lu,Zewei He,Hao Luo,Tianwei Zhang*

Main category: cs.CV

TL;DR: CoreMark is a new text watermarking framework using CORE segments for robust, generalizable, and imperceptible data embedding across languages and fonts.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in text watermarking like robustness, generalizability, and imperceptibility.

Method: Uses CORE segments for embedding, dynamically selects robust characters, and modulates embedding strength based on font size.

Result: Outperforms existing methods in resisting attacks (screenshot, print-scan, print-camera) while maintaining imperceptibility.

Conclusion: CoreMark offers a superior solution for text watermarking with broad applicability and resilience.

Abstract: Text watermarking schemes have gained considerable attention in recent years,
yet still face critical challenges in achieving simultaneous robustness,
generalizability, and imperceptibility. This paper introduces a new embedding
paradigm,termed CORE, which comprises several consecutively aligned black pixel
segments. Its key innovation lies in its inherent noise resistance during
transmission and broad applicability across languages and fonts. Based on the
CORE, we present a text watermarking framework named CoreMark. Specifically,
CoreMark first dynamically extracts COREs from characters. Then, the characters
with stronger robustness are selected according to the lengths of COREs. By
modifying the thickness of the CORE, the hidden data is embedded into the
selected characters without causing significant visual distortions. Moreover, a
general plug-and-play embedding strength modulator is proposed, which can
adaptively enhance the robustness for small font sizes by adjusting the
embedding strength according to the font size. Experimental evaluation
indicates that CoreMark demonstrates outstanding generalizability across
multiple languages and fonts. Compared to existing methods, CoreMark achieves
significant improvements in resisting screenshot, print-scan, and print camera
attacks, while maintaining satisfactory imperceptibility.

</details>


### [158] [Unsupervised 3D Braided Hair Reconstruction from a Single-View Image](https://arxiv.org/abs/2506.23072)
*Jing Gao*

Main category: cs.CV

TL;DR: A novel unsupervised pipeline for 3D braided hair reconstruction from single-view images, outperforming existing methods in accuracy and realism.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with braided hair due to its intricate structure, limiting realistic digital human modeling.

Method: Uses a synthetic braid model inspired by braid theory to capture complex intertwined structures.

Result: Outperforms state-of-the-art approaches in accuracy, realism, and efficiency.

Conclusion: The method supports expressive hairstyle modeling in digital humans, advancing 3D braided hair reconstruction.

Abstract: Reconstructing 3D braided hairstyles from single-view images remains a
challenging task due to the intricate interwoven structure and complex
topologies of braids. Existing strand-based hair reconstruction methods
typically focus on loose hairstyles and often struggle to capture the
fine-grained geometry of braided hair. In this paper, we propose a novel
unsupervised pipeline for efficiently reconstructing 3D braided hair from
single-view RGB images. Leveraging a synthetic braid model inspired by braid
theory, our approach effectively captures the complex intertwined structures of
braids. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches, providing superior accuracy, realism, and
efficiency in reconstructing 3D braided hairstyles, supporting expressive
hairstyle modeling in digital humans.

</details>


### [159] [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/abs/2506.23074)
*Yu Zheng,Boyang Gong,Fanye Kong,Yueqi Duan,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jiwen Lu,Jie Zhou*

Main category: cs.CV

TL;DR: CDAL improves open-world model attribution by decoupling causal relationships and reducing biases, outperforming existing methods with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with spurious correlations and novel attacks in open-world scenarios, necessitating a more robust approach.

Method: CDAL models causal relationships between attentional traces and attribution, decoupling discriminative artifacts from biases to improve generalization.

Result: CDAL significantly enhances performance on benchmarks, especially for unseen attacks, with minimal computational cost.

Conclusion: CDAL provides a scalable and effective solution for open-world model attribution by leveraging causal reasoning.

Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning
(CDAL) method for open-world model attribution. Existing methods rely on
handcrafted design of region partitioning or feature space, which could be
confounded by the spurious statistical correlations and struggle with novel
attacks in open-world scenarios. To address this, CDAL explicitly models the
causal relationships between the attentional visual traces and source model
attribution, and counterfactually decouples the discriminative model-specific
artifacts from confounding source biases for comparison. In this way, the
resulting causal effect provides a quantification on the quality of learned
attention maps, thus encouraging the network to capture essential generation
patterns that generalize to unseen source models by maximizing the effect.
Extensive experiments on existing open-world model attribution benchmarks show
that with minimal computational overhead, our method consistently improves
state-of-the-art models by large margins, particularly for unseen novel
attacks. Source code: https://github.com/yzheng97/CDAL.

</details>


### [160] [Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization](https://arxiv.org/abs/2506.23077)
*Suofei Zhang,Xinxin Wang,Xiaofu Wu,Quan Zhou,Haifeng Hu*

Main category: cs.CV

TL;DR: The paper introduces Distance-Aware Cross-View Geo-Localization (DACVGL), proposing a new benchmark (DA-Campus) and a Dynamic Contrastive Learning (DyCL) framework to improve hierarchical retrieval and localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on cross-domain image matching but lack contextual understanding and error minimization in geo-localization.

Method: Constructs DA-Campus benchmark with distance annotations, formulates DACVGL as hierarchical retrieval, and proposes DyCL for feature alignment.

Result: DyCL outperforms existing methods, enhancing hierarchical retrieval and cross-view geo-localization accuracy.

Conclusion: DyCL is effective for DACVGL, with publicly available code and benchmark.

Abstract: Existing deep learning-based cross-view geo-localization methods primarily
focus on improving the accuracy of cross-domain image matching, rather than
enabling models to comprehensively capture contextual information around the
target and minimize the cost of localization errors. To support systematic
research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,
we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs
multi-view imagery with precise distance annotations across three spatial
resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical
retrieval problem across different domains. Our study further reveals that, due
to the inherent complexity of spatial relationships among buildings, this
problem can only be addressed via a contrastive learning paradigm, rather than
conventional metric learning. To tackle this challenge, we propose Dynamic
Contrastive Learning (DyCL), a novel framework that progressively aligns
feature representations according to hierarchical spatial margins. Extensive
experiments demonstrate that DyCL is highly complementary to existing
multi-scale metric learning methods and yields substantial improvements in both
hierarchical retrieval performance and overall cross-view geo-localization
accuracy. Our code and benchmark are publicly available at
https://github.com/anocodetest1/DyCL.

</details>


### [161] [Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation](https://arxiv.org/abs/2506.23086)
*Jian Shi,Tianqi You,Pingping Zhang,Hongli Zhang,Rui Xu,Haojie Li*

Main category: cs.CV

TL;DR: The paper introduces FMC-Net, a frequency-enhanced multi-granularity context network, to improve vertebrae segmentation in 3D CT and MRI images by addressing blurring and distinguishing similar vertebrae.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with image blurring and distinguishing similar vertebrae in spinal imaging, limiting clinical applications.

Method: FMC-Net uses wavelet transform for lossless downsampling, processes high and low-frequency components separately (HFR for high-frequency, MG-SSM for low-frequency), and leverages multi-granularity contexts.

Result: The method outperforms state-of-the-art approaches on CT and MRI datasets.

Conclusion: FMC-Net effectively improves vertebrae segmentation accuracy by addressing key challenges in spinal imaging.

Abstract: Automated and accurate segmentation of individual vertebra in 3D CT and MRI
images is essential for various clinical applications. Due to the limitations
of current imaging techniques and the complexity of spinal structures, existing
methods still struggle with reducing the impact of image blurring and
distinguishing similar vertebrae. To alleviate these issues, we introduce a
Frequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the
accuracy of vertebrae segmentation. Specifically, we first apply wavelet
transform for lossless downsampling to reduce the feature distortion in blurred
images. The decomposed high and low-frequency components are then processed
separately. For the high-frequency components, we apply a High-frequency
Feature Refinement (HFR) to amplify the prominence of key features and filter
out noises, restoring fine-grained details in blurred images. For the
low-frequency components, we use a Multi-granularity State Space Model (MG-SSM)
to aggregate feature representations with different receptive fields,
extracting spatially-varying contexts while capturing long-range dependencies
with linear complexity. The utilization of multi-granularity contexts is
essential for distinguishing similar vertebrae and improving segmentation
accuracy. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.
The source code is publicly available at https://github.com/anaanaa/FMCNet.

</details>


### [162] [Where, What, Why: Towards Explainable Driver Attention Prediction](https://arxiv.org/abs/2506.23088)
*Yuchen Zhou,Jiayu Tang,Xiaoyan Xiao,Yueyao Lin,Linkai Liu,Zipeng Guo,Hao Fei,Xiaobo Xia,Chao Gou*

Main category: cs.CV

TL;DR: The paper introduces Explainable Driver Attention Prediction (EDAP) and W3DA dataset to predict where, what, and why drivers focus, using LLada framework for robust performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods predict spatial attention but lack cognitive reasoning, limiting understanding of driver attention mechanisms.

Method: Proposes LLada, a Large Language model-driven framework, unifying pixel modeling, semantic parsing, and cognitive reasoning in an end-to-end architecture.

Result: LLada shows robust generalization across datasets and driving conditions.

Conclusion: This work advances understanding of driver attention, benefiting autonomous driving, driver training, and human-computer interaction.

Abstract: Modeling task-driven attention in driving is a fundamental challenge for both
autonomous vehicles and cognitive science. Existing methods primarily predict
where drivers look by generating spatial heatmaps, but fail to capture the
cognitive motivations behind attention allocation in specific contexts, which
limits deeper understanding of attention mechanisms. To bridge this gap, we
introduce Explainable Driver Attention Prediction, a novel task paradigm that
jointly predicts spatial attention regions (where), parses attended semantics
(what), and provides cognitive reasoning for attention allocation (why). To
support this, we present W3DA, the first large-scale explainable driver
attention dataset. It enriches existing benchmarks with detailed semantic and
causal annotations across diverse driving scenarios, including normal
conditions, safety-critical situations, and traffic accidents. We further
propose LLada, a Large Language model-driven framework for driver attention
prediction, which unifies pixel modeling, semantic parsing, and cognitive
reasoning within an end-to-end architecture. Extensive experiments demonstrate
the effectiveness of LLada, exhibiting robust generalization across datasets
and driving conditions. This work serves as a key step toward a deeper
understanding of driver attention mechanisms, with significant implications for
autonomous driving, intelligent driver training, and human-computer
interaction.

</details>


### [163] [DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation](https://arxiv.org/abs/2506.23104)
*Jihun Kim,Hoyong Kwon,Hyeokjun Kweon,Wooseong Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: DC-TTA improves SAM's interactive segmentation by adapting it per-sample using user interactions, dividing clicks into subsets for localized updates, and merging results for better performance in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: SAM struggles in specialized domains and complex scenarios like camouflaged objects, prompting the need for a more adaptive approach.

Method: DC-TTA partitions user clicks into coherent subsets, processes each independently via test-time adaptation, and merges the adapted models.

Result: DC-TTA outperforms SAM's zero-shot results and conventional TTA methods, handling complex tasks with fewer interactions and higher accuracy.

Conclusion: DC-TTA effectively enhances SAM's performance in interactive segmentation for complex and specialized tasks.

Abstract: Interactive segmentation (IS) allows users to iteratively refine object
boundaries with minimal cues, such as positive and negative clicks. While the
Segment Anything Model (SAM) has garnered attention in the IS community for its
promptable segmentation capabilities, it often struggles in specialized domains
or when handling complex scenarios (e.g., camouflaged or multi-part objects).
To overcome these challenges, we propose DC-TTA, a novel test-time adaptation
(TTA) framework that adapts SAM on a per-sample basis by leveraging user
interactions as supervision. Instead of forcing a single model to incorporate
all user clicks at once, DC-TTA partitions the clicks into more coherent
subsets, each processed independently via TTA with a separated model. This
Divide-and-Conquer strategy reduces conflicts among diverse cues and enables
more localized updates. Finally, we merge the adapted models to form a unified
predictor that integrates the specialized knowledge from each subset.
Experimental results across various benchmarks demonstrate that DC-TTA
significantly outperforms SAM's zero-shot results and conventional TTA methods,
effectively handling complex tasks such as camouflaged object segmentation with
fewer interactions and improved accuracy.

</details>


### [164] [Computer-Aided Multi-Stroke Character Simplification by Stroke Removal](https://arxiv.org/abs/2506.23106)
*Ryo Ishiyama,Shinnosuke Matsuo,Seiichi Uchida*

Main category: cs.CV

TL;DR: A framework simplifies multi-stroke characters by removing strokes without losing legibility, aiding non-native learners and font design.


<details>
  <summary>Details</summary>
Motivation: To reduce learning barriers for non-native speakers and improve font design by simplifying complex characters.

Method: Uses a character recognition model to assess legibility and selectively removes strokes with minimal impact.

Result: Many characters remain distinguishable even after multiple strokes are removed, suggesting viable simplification.

Conclusion: The framework shows promise for formalized character simplification strategies.

Abstract: Multi-stroke characters in scripts such as Chinese and Japanese can be highly
complex, posing significant challenges for both native speakers and,
especially, non-native learners. If these characters can be simplified without
degrading their legibility, it could reduce learning barriers for non-native
speakers, facilitate simpler and legible font designs, and contribute to
efficient character-based communication systems. In this paper, we propose a
framework to systematically simplify multi-stroke characters by selectively
removing strokes while preserving their overall legibility. More specifically,
we use a highly accurate character recognition model to assess legibility and
remove those strokes that minimally impact it. Experimental results on 1,256
character classes with 5, 10, 15, and 20 strokes reveal several key findings,
including the observation that even after removing multiple strokes, many
characters remain distinguishable. These findings suggest the potential for
more formalized simplification strategies.

</details>


### [165] [Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound](https://arxiv.org/abs/2506.23108)
*Zhiyuan Zhu,Jian Wang,Yong Jiang,Tong Han,Yuhao Huang,Ang Zhang,Kaiwen Yang,Mingyuan Luo,Zhe Liu,Yaofei Duan,Dong Ni,Tianhong Tang,Xin Yang*

Main category: cs.CV

TL;DR: The paper introduces CVC-RF, a deep learning framework for carotid plaque grading (CPG) that refines features at Corpus-, View-, and Category-levels, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate CPG is crucial for assessing cardiovascular and cerebrovascular risks, but existing methods neglect representation learning and class feature differences.

Method: CVC-RF includes a center-memory contrastive loss, cascaded down-sampling attention module, and parameter-free mixture-of-experts weighting for multi-level refinement.

Result: CVC-RF achieves state-of-the-art performance in CPG by effectively modeling global features.

Conclusion: The proposed framework enhances CPG accuracy through multi-level feature refinement, addressing limitations of current methods.

Abstract: Accurate carotid plaque grading (CPG) is vital to assess the risk of
cardiovascular and cerebrovascular diseases. Due to the small size and high
intra-class variability of plaque, CPG is commonly evaluated using a
combination of transverse and longitudinal ultrasound views in clinical
practice. However, most existing deep learning-based multi-view classification
methods focus on feature fusion across different views, neglecting the
importance of representation learning and the difference in class features. To
address these issues, we propose a novel Corpus-View-Category Refinement
Framework (CVC-RF) that processes information from Corpus-, View-, and
Category-levels, enhancing model performance. Our contribution is four-fold.
First, to the best of our knowledge, we are the foremost deep learning-based
method for CPG according to the latest Carotid Plaque-RADS guidelines. Second,
we propose a novel center-memory contrastive loss, which enhances the network's
global modeling capability by comparing with representative cluster centers and
diverse negative samples at the Corpus level. Third, we design a cascaded
down-sampling attention module to fuse multi-scale information and achieve
implicit feature interaction at the View level. Finally, a parameter-free
mixture-of-experts weighting strategy is introduced to leverage class
clustering knowledge to weight different experts, enabling feature decoupling
at the Category level. Experimental results indicate that CVC-RF effectively
models global features via multi-level refinement, achieving state-of-the-art
performance in the challenging CPG task.

</details>


### [166] [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)
*Haonan Chen,Hong Liu,Yuping Luo,Liang Wang,Nan Yang,Furu Wei,Zhicheng Dou*

Main category: cs.CV

TL;DR: MoCa is a two-stage framework transforming VLMs into bidirectional multimodal embedding models, addressing limitations like suboptimal causal attention, scalability, and data diversity.


<details>
  <summary>Details</summary>
Motivation: Current multimodal embedding models face issues with causal attention, scalability, and limited training data diversity.

Method: MoCa uses Modality-aware Continual Pre-training and Heterogeneous Contrastive Fine-tuning to enhance bidirectional reasoning and leverage diverse data.

Result: MoCa achieves state-of-the-art results on MMEB and ViDoRe-v2 benchmarks and shows strong scalability.

Conclusion: MoCa effectively addresses key limitations in multimodal embedding models, improving performance and scalability.

Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs),
have shown promise in various tasks. However, current approaches face three key
limitations: the use of causal attention in VLM backbones is suboptimal for
embedding tasks; scalability issues due to reliance on high-quality labeled
paired data for contrastive learning; and limited diversity in training
objectives and data. To address these issues, we propose MoCa, a two-stage
framework for transforming pre-trained VLMs into effective bidirectional
multimodal embedding models. The first stage, Modality-aware Continual
Pre-training, introduces a joint reconstruction objective that simultaneously
denoises interleaved text and image inputs, enhancing bidirectional
context-aware reasoning. The second stage, Heterogeneous Contrastive
Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple
image-caption pairs to enhance generalization and alignment. Our method
addresses the stated limitations by introducing bidirectional attention through
continual pre-training, scaling effectively with massive unlabeled datasets via
joint reconstruction objectives, and utilizing diverse multimodal data for
enhanced representation robustness. Experiments demonstrate that MoCa
consistently improves performance across MMEB and ViDoRe-v2 benchmarks,
achieving new state-of-the-art results, and exhibits strong scalability with
both model size and training data on MMEB.

</details>


### [167] [Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation](https://arxiv.org/abs/2506.23120)
*Zhenhua Ning,Zhuotao Tian,Shaoshuai Shi,Guangming Lu,Daojing He,Wenjie Pei,Li Jiang*

Main category: cs.CV

TL;DR: The paper introduces R$^2$S, a reasoning-based segmentation framework for 3D point clouds, and a new dataset, 3D ReasonSeg, to address challenges in spatial reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex spatial reasoning in 3D point cloud perception, despite detailed spatial cues.

Method: R$^2$S decomposes spatial reasoning into two stages: identifying relevant elements and processing instructions using visual priors.

Result: R$^2$S and 3D ReasonSeg improve spatial reasoning in 3D point cloud perception, validated by experiments.

Conclusion: The proposed framework and dataset serve as a new baseline for future research in 3D point cloud perception.

Abstract: Recent advances in point cloud perception have demonstrated remarkable
progress in scene understanding through vision-language alignment leveraging
large language models (LLMs). However, existing methods may still encounter
challenges in handling complex instructions that require accurate spatial
reasoning, even if the 3D point cloud data provides detailed spatial cues such
as size and position for identifying the targets. To tackle this issue, we
propose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based
segmentation framework. The framework emulates human cognitive processes by
decomposing spatial reasoning into two sequential stages: first identifying
relevant elements, then processing instructions guided by their associated
visual priors. Furthermore, acknowledging the inadequacy of existing datasets
in complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based
segmentation dataset comprising 25,185 training samples and 3,966 validation
samples with precise annotations. Both quantitative and qualitative experiments
demonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud
perception with stronger spatial reasoning capabilities, and we hope that they
can serve as a new baseline and benchmark for future work.

</details>


### [168] [Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval](https://arxiv.org/abs/2506.23132)
*Sophie Zhou,Shu Kong*

Main category: cs.CV

TL;DR: The paper proposes a method for detecting and explaining art plagiarism by retrieving visually similar authentic artworks, using a dataset of paintings and AI-synthesized plagiarized versions. Baseline and finetuned DINOv2 models are compared, with trade-offs in accuracy and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: To protect artists' copyrights by improving plagiarism detection in paintings, addressing challenges in forensic analysis.

Method: Constructs a dataset with authentic and AI-synthesized plagiarized paintings. Uses DINOv2 for baseline retrieval and classification, then finetunes it with metric learning for improved retrieval.

Result: Baseline achieves 97.2% recognition accuracy but 29.0% AP retrieval. Finetuning improves retrieval by 12% AP but reduces accuracy to 92.7%.

Conclusion: Highlights trade-offs between accuracy and retrieval performance, suggesting future research to balance both.

Abstract: Art plagiarism detection plays a crucial role in protecting artists'
copyrights and intellectual property, yet it remains a challenging problem in
forensic analysis. In this paper, we address the task of recognizing
plagiarized paintings and explaining the detected plagarisms by retrieving
visually similar authentic artworks. To support this study, we construct a
dataset by collecting painting photos and synthesizing plagiarized versions
using generative AI, tailored to specific artists' styles. We first establish a
baseline approach using off-the-shelf features from the visual foundation model
DINOv2 to retrieve the most similar images in the database and classify
plagiarism based on a similarity threshold. Surprisingly, this non-learned
method achieves a high recognition accuracy of 97.2\% but suffers from low
retrieval precision 29.0\% average precision (AP). To improve retrieval
quality, we finetune DINOv2 with a metric learning loss using positive and
negative sample pairs sampled in the database. The finetuned model greatly
improves retrieval performance by 12\% AP over the baseline, though it
unexpectedly results in a lower recognition accuracy (92.7\%). We conclude with
insightful discussions and outline directions for future research.

</details>


### [169] [RoboScape: Physics-informed Embodied World Model](https://arxiv.org/abs/2506.23135)
*Yu Shang,Xin Zhang,Yinzhou Tang,Lei Jin,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: RoboScape is a physics-informed world model for realistic robotic video generation, addressing limitations in 3D geometry and motion dynamics.


<details>
  <summary>Details</summary>
Motivation: Current world models lack physical awareness, leading to unrealistic video generation for contact-rich robotic scenarios.

Method: RoboScape integrates RGB video generation with physics knowledge, using temporal depth prediction and keypoint dynamics learning.

Result: The model produces videos with high visual fidelity and physical plausibility, validated in robotic policy training and evaluation.

Conclusion: RoboScape advances embodied intelligence by efficiently incorporating physics into world models.

Abstract: World models have become indispensable tools for embodied intelligence,
serving as powerful simulators capable of generating realistic robotic videos
while addressing critical data scarcity challenges. However, current embodied
world models exhibit limited physical awareness, particularly in modeling 3D
geometry and motion dynamics, resulting in unrealistic video generation for
contact-rich robotic scenarios. In this paper, we present RoboScape, a unified
physics-informed world model that jointly learns RGB video generation and
physics knowledge within an integrated framework. We introduce two key
physics-informed joint training tasks: temporal depth prediction that enhances
3D geometric consistency in video rendering, and keypoint dynamics learning
that implicitly encodes physical properties (e.g., object shape and material
characteristics) while improving complex motion modeling. Extensive experiments
demonstrate that RoboScape generates videos with superior visual fidelity and
physical plausibility across diverse robotic scenarios. We further validate its
practical utility through downstream applications including robotic policy
training with generated data and policy evaluation. Our work provides new
insights for building efficient physics-informed world models to advance
embodied intelligence research. The code is available at:
https://github.com/tsinghua-fib-lab/RoboScape.

</details>


### [170] [VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis](https://arxiv.org/abs/2506.23138)
*Shiyu Wu,Mingzhen Sun,Weining Wang,Yequan Wang,Jing Liu*

Main category: cs.CV

TL;DR: VisualPrompter is a training-free framework that refines user prompts for diffusion models to improve text-image alignment, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the gap between user-provided prompts and model-preferred ones, ensuring semantic alignment in generated images.

Method: Uses self-reflection to identify missing concepts and fine-grained prompt optimization to revise inputs.

Result: Achieves state-of-the-art performance on text-image alignment benchmarks and is adaptable to various models.

Conclusion: VisualPrompter effectively enhances prompt quality, balancing aesthetics and semantic accuracy.

Abstract: Since there exists a notable gap between user-provided and model-preferred
prompts, generating high-quality and satisfactory images using diffusion models
often requires prompt engineering to optimize user inputs. Current studies on
text-to-image prompt engineering can effectively enhance the style and
aesthetics of generated images. However, they often neglect the semantic
alignment between generated images and user descriptions, resulting in visually
appealing but content-wise unsatisfying outputs. In this work, we propose
VisualPrompter, a novel training-free prompt engineering framework that refines
user inputs to model-preferred sentences. In particular, VisualPrompter
utilizes an automatic self-reflection module to identify the missing concepts
in generated images and a target-specific prompt optimization mechanism to
revise the prompts in a fine-grained manner. Extensive experiments demonstrate
the effectiveness of our VisualPrompter, which achieves new state-of-the-art
performance on multiple benchmarks for text-image alignment evaluation.
Additionally, our framework features a plug-and-play design, making it highly
adaptable to various generative models.

</details>


### [171] [AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation](https://arxiv.org/abs/2506.23150)
*Xinyue Liang,Zhiyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: AlignCVC introduces a distribution alignment framework for single-image-to-3D generation, improving cross-view consistency (CVC) and accelerating inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with noisy and unstable reconstruction outputs, limiting CVC improvement in single-image-to-3D workflows.

Method: AlignCVC aligns generated and reconstructed multi-view distributions with ground-truth, using a soft-hard alignment strategy for generation and reconstruction models.

Result: The method enhances generation quality and speeds up inference to as few as 4 steps, integrating seamlessly with various models.

Conclusion: AlignCVC effectively improves CVC and efficiency in single-image-to-3D generation, validated by extensive experiments.

Abstract: Single-image-to-3D models typically follow a sequential generation and
reconstruction workflow. However, intermediate multi-view images synthesized by
pre-trained generation models often lack cross-view consistency (CVC),
significantly degrading 3D reconstruction performance. While recent methods
attempt to refine CVC by feeding reconstruction results back into the
multi-view generator, these approaches struggle with noisy and unstable
reconstruction outputs that limit effective CVC improvement. We introduce
AlignCVC, a novel framework that fundamentally re-frames single-image-to-3D
generation through distribution alignment rather than relying on strict
regression losses. Our key insight is to align both generated and reconstructed
multi-view distributions toward the ground-truth multi-view distribution,
establishing a principled foundation for improved CVC. Observing that generated
images exhibit weak CVC while reconstructed images display strong CVC due to
explicit rendering, we propose a soft-hard alignment strategy with distinct
objectives for generation and reconstruction models. This approach not only
enhances generation quality but also dramatically accelerates inference to as
few as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,
seamlessly integrates various multi-view generation models with 3D
reconstruction models. Extensive experiments demonstrate the effectiveness and
efficiency of AlignCVC for single-image-to-3D generation.

</details>


### [172] [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/abs/2506.23151)
*Vladislav Bargatin,Egor Chistov,Alexander Yakovenko,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: MEMFOF is a memory-efficient multi-frame optical flow method that balances accuracy and GPU memory usage, achieving state-of-the-art performance with minimal memory overhead.


<details>
  <summary>Details</summary>
Motivation: Address the high GPU memory consumption in optical flow estimation, especially for high-resolution inputs, without compromising accuracy.

Method: Revisits RAFT-like architectures, integrates reduced correlation volumes, high-resolution training, and multi-frame estimation.

Result: Outperforms alternatives in accuracy and efficiency, ranking first on benchmarks like Spring, Sintel, and KITTI-2015.

Conclusion: MEMFOF is robust for high-resolution flow estimation, offering a practical solution with low memory requirements.

Abstract: Recent advances in optical flow estimation have prioritized accuracy at the
cost of growing GPU memory consumption, particularly for high-resolution
(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical
flow method that identifies a favorable trade-off between multi-frame
estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU
memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely
positions our method to be trained at native 1080p without the need for
cropping or downsampling. We systematically revisit design choices from
RAFT-like architectures, integrating reduced correlation volumes and
high-resolution training protocols alongside multi-frame estimation, to achieve
state-of-the-art performance across multiple benchmarks while substantially
reducing memory overhead. Our method outperforms more resource-intensive
alternatives in both accuracy and runtime efficiency, validating its robustness
for flow estimation at high resolutions. At the time of submission, our method
ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,
leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the
best Fl-all error on KITTI-2015 at 2.94%. The code is available at
https://github.com/msu-video-group/memfof.

</details>


### [173] [Dynamic View Synthesis from Small Camera Motion Videos](https://arxiv.org/abs/2506.23153)
*Huiqiang Sun,Xingyi Li,Juewen Peng,Liao Shen,Zhiguo Cao,Ke Xian,Guosheng Lin*

Main category: cs.CV

TL;DR: The paper addresses challenges in novel view synthesis for dynamic 3D scenes with small camera motion, proposing Distribution-based Depth Regularization (DDR) and camera parameter learning to improve geometry representation and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF-based methods struggle with limited camera motion, leading to incorrect geometry and inaccurate camera parameters.

Method: Proposes DDR for accurate rendering weight distribution and introduces constraints for correct geometry. Also includes camera parameter learning during training.

Result: The approach outperforms state-of-the-art methods in scenes with small camera motion.

Conclusion: The proposed DDR and camera parameter learning effectively address challenges in dynamic scene synthesis with limited camera motion.

Abstract: Novel view synthesis for dynamic $3$D scenes poses a significant challenge.
Many notable efforts use NeRF-based approaches to address this task and yield
impressive results. However, these methods rely heavily on sufficient motion
parallax in the input images or videos. When the camera motion range becomes
limited or even stationary (i.e., small camera motion), existing methods
encounter two primary challenges: incorrect representation of scene geometry
and inaccurate estimation of camera parameters. These challenges make prior
methods struggle to produce satisfactory results or even become invalid. To
address the first challenge, we propose a novel Distribution-based Depth
Regularization (DDR) that ensures the rendering weight distribution to align
with the true distribution. Specifically, unlike previous methods that use
depth loss to calculate the error of the expectation, we calculate the
expectation of the error by using Gumbel-softmax to differentiably sample
points from discrete rendering weight distribution. Additionally, we introduce
constraints that enforce the volume density of spatial points before the object
boundary along the ray to be near zero, ensuring that our model learns the
correct geometry of the scene. To demystify the DDR, we further propose a
visualization tool that enables observing the scene geometry representation at
the rendering weight level. For the second challenge, we incorporate camera
parameter learning during training to enhance the robustness of our model to
camera parameters. We conduct extensive experiments to demonstrate the
effectiveness of our approach in representing scenes with small camera motion
input, and our results compare favorably to state-of-the-art methods.

</details>


### [174] [Self-Supervised Contrastive Learning for Multi-Label Images](https://arxiv.org/abs/2506.23156)
*Jiale Chen*

Main category: cs.CV

TL;DR: The paper proposes a tailored self-supervised learning (SSL) method for multi-label images, reducing pre-training overhead while maintaining strong representation learning capabilities.


<details>
  <summary>Details</summary>
Motivation: Mainstream SSL methods rely on single-label datasets like ImageNet, ignoring multi-label images' richer semantics and broader applicability. This work addresses the gap by adapting SSL for multi-label images.

Method: Introduces a block-wise augmentation module to extract additional positive view pairs from multi-label images and an image-aware contrastive loss to link these views for semantically consistent representations.

Result: Validated through linear fine-tuning and transfer learning, the method proves competitive despite limited sample quality and quantity.

Conclusion: The approach effectively adapts SSL for multi-label images, offering a practical solution with strong performance.

Abstract: Self-supervised learning (SSL) has demonstrated its effectiveness in learning
representations through comparison methods that align with human intuition.
However, mainstream SSL methods heavily rely on high body datasets with single
label, such as ImageNet, resulting in intolerable pre-training overhead.
Besides, more general multi-label images are frequently overlooked in SSL,
despite their potential for richer semantic information and broader
applicability in downstream scenarios. Therefore, we tailor the mainstream SSL
approach to guarantee excellent representation learning capabilities using
fewer multi-label images. Firstly, we propose a block-wise augmentation module
aimed at extracting additional potential positive view pairs from multi-label
images. Subsequently, an image-aware contrastive loss is devised to establish
connections between these views, thereby facilitating the extraction of
semantically consistent representations. Comprehensive linear fine-tuning and
transfer learning validate the competitiveness of our approach despite
challenging sample quality and quantity.

</details>


### [175] [STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene](https://arxiv.org/abs/2506.23157)
*Hanyu Zhou,Haonan Wang,Haoyue Liu,Yuxing Duan,Luxin Yan,Gim Hee Lee*

Main category: cs.CV

TL;DR: A spatiotemporal-disentangled Gaussian splatting framework is proposed for high-dynamic scene reconstruction, using event cameras to compensate for frame cameras and clustering to distinguish features.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle discontinuous temporal features and heterogeneous spatial features in dynamic scenes.

Method: Disentangles spatiotemporal features into latent representations, uses event cameras, and employs clustering for feature distinction.

Result: Improves spatiotemporal discrimination, enabling time-continuous dynamic scene rendering.

Conclusion: The proposed method outperforms existing approaches in high-dynamic scene reconstruction.

Abstract: High-dynamic scene reconstruction aims to represent static background with
rigid spatial features and dynamic objects with deformed continuous
spatiotemporal features. Typically, existing methods adopt unified
representation model (e.g., Gaussian) to directly match the spatiotemporal
features of dynamic scene from frame camera. However, this unified paradigm
fails in the potential discontinuous temporal features of objects due to frame
imaging and the heterogeneous spatial features between background and objects.
To address this issue, we disentangle the spatiotemporal features into various
latent representations to alleviate the spatiotemporal mismatching between
background and objects. In this work, we introduce event camera to compensate
for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting
framework for high-dynamic scene reconstruction. As for dynamic scene, we
figure out that background and objects have appearance discrepancy in
frame-based spatial features and motion discrepancy in event-based temporal
features, which motivates us to distinguish the spatiotemporal features between
background and objects via clustering. As for dynamic object, we discover that
Gaussian representations and event data share the consistent spatiotemporal
characteristic, which could serve as a prior to guide the spatiotemporal
disentanglement of object Gaussians. Within Gaussian splatting framework, the
cumulative scene-object disentanglement can improve the spatiotemporal
discrimination between background and objects to render the time-continuous
dynamic scene. Extensive experiments have been performed to verify the
superiority of the proposed method.

</details>


### [176] [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219)
*Jie Feng,Shengyuan Wang,Tianhui Liu,Yanxin Xi,Yong Li*

Main category: cs.CV

TL;DR: UrbanLLaVA is a multi-modal large language model designed for urban research, outperforming general MLLMs by processing diverse urban data types and tasks through a curated dataset and multi-stage training framework.


<details>
  <summary>Details</summary>
Motivation: Current urban research methods lack a unified framework for multi-modal data, limiting comprehensive understanding. UrbanLLaVA addresses this gap using MLLMs.

Method: UrbanLLaVA uses a curated urban instruction dataset and a multi-stage training framework to enhance spatial reasoning and domain knowledge learning.

Result: UrbanLLaVA outperforms open-source and proprietary MLLMs in urban tasks, showing robust generalization across cities.

Conclusion: UrbanLLaVA provides a powerful, unified solution for multi-modal urban research, with open access to data and code.

Abstract: Urban research involves a wide range of scenarios and tasks that require the
understanding of multi-modal data. Current methods often focus on specific data
types and lack a unified framework in urban field for processing them
comprehensively. The recent success of multi-modal large language models
(MLLMs) presents a promising opportunity to overcome this limitation. In this
paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model
designed to process these four types of data simultaneously and achieve strong
performance across diverse urban tasks compared with general MLLMs. In
$\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset
encompassing both single-modal and cross-modal urban data, spanning from
location view to global view of urban environment. Additionally, we propose a
multi-stage training framework that decouples spatial reasoning enhancement
from domain knowledge learning, thereby improving the compatibility and
downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks.
Finally, we also extend existing benchmark for urban research to assess the
performance of MLLMs across a wide range of urban tasks. Experimental results
from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms
open-source and proprietary MLLMs in both single-modal tasks and complex
cross-modal tasks and shows robust generalization abilities across cities.
Source codes and data are openly accessible to the research community via
https://github.com/tsinghua-fib-lab/UrbanLLaVA.

</details>


### [177] [Trident: Detecting Face Forgeries with Adversarial Triplet Learning](https://arxiv.org/abs/2506.23189)
*Mustafa Hakan Kara,Aysegul Dundar,Uğur Güdükbay*

Main category: cs.CV

TL;DR: Trident is a face forgery detection framework using triplet learning and Siamese networks for adaptability across diverse forgery methods, enhanced by domain-adversarial training.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks generate sophisticated face forgeries, challenging detection models trained on domain-specific data, which fail against unseen techniques.

Method: Trident employs triplet learning with a Siamese network, domain-adversarial training, and controlled gradient flow to isolate forgery features and improve generalizability.

Result: The framework demonstrates effectiveness across benchmarks, capturing fine-grained forgery features and enhancing robustness to unseen manipulations.

Conclusion: Trident offers a robust solution for detecting diverse face forgeries, with code to be released for further research.

Abstract: As face forgeries generated by deep neural networks become increasingly
sophisticated, detecting face manipulations in digital media has posed a
significant challenge, underscoring the importance of maintaining digital media
integrity and combating visual disinformation. Current detection models,
predominantly based on supervised training with domain-specific data, often
falter against forgeries generated by unencountered techniques. In response to
this challenge, we introduce \textit{Trident}, a face forgery detection
framework that employs triplet learning with a Siamese network architecture for
enhanced adaptability across diverse forgery methods. \textit{Trident} is
trained on curated triplets to isolate nuanced differences of forgeries,
capturing fine-grained features that distinguish pristine samples from
manipulated ones while controlling for other variables. To further enhance
generalizability, we incorporate domain-adversarial training with a forgery
discriminator. This adversarial component guides our embedding model towards
forgery-agnostic representations, improving its robustness to unseen
manipulations. In addition, we prevent gradient flow from the classifier head
to the embedding model, avoiding overfitting induced by artifacts peculiar to
certain forgeries. Comprehensive evaluations across multiple benchmarks and
ablation studies demonstrate the effectiveness of our framework. We will
release our code in a GitHub repository.

</details>


### [178] [DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding](https://arxiv.org/abs/2506.23196)
*Mona Ahmadian,Amir Shirian,Frank Guerin,Andrew Gilbert*

Main category: cs.CV

TL;DR: DEL is a framework for dense semantic action localization in videos, using multimodal interaction modeling to achieve state-of-the-art performance on TAL datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world videos have overlapping events and complex temporal dependencies, making action localization challenging.

Method: DEL aligns audio-visual features with masked self-attention and refines multimodal interactions across scales.

Result: Achieves top performance on UnAV-100, THUMOS14, ActivityNet 1.3, and EPIC-Kitchens-100 with significant mAP gains.

Conclusion: DEL effectively models multimodal interactions for accurate action localization in complex videos.

Abstract: Real-world videos often contain overlapping events and complex temporal
dependencies, making multimodal interaction modeling particularly challenging.
We introduce DEL, a framework for dense semantic action localization, aiming to
accurately detect and classify multiple actions at fine-grained temporal
resolutions in long untrimmed videos. DEL consists of two key modules: the
alignment of audio and visual features that leverage masked self-attention to
enhance intra-mode consistency and a multimodal interaction refinement module
that models cross-modal dependencies across multiple scales, enabling
high-level semantics and fine-grained details. Our method achieves
state-of-the-art performance on multiple real-world Temporal Action
Localization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and
EPIC-Kitchens-100, surpassing previous approaches with notable average mAP
gains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.

</details>


### [179] [Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing](https://arxiv.org/abs/2506.23202)
*Qilin Shu,Qixian Zhang,Qi Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: Proposes HAMW, a method combining high-frequency augmentation and multi-wave mixing to improve transformer-based person search by enhancing feature extraction and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in transformer-based person search: suppression of high-frequency features and high computational costs.

Method: Introduces a three-stage framework with high-frequency augmentation and replaces self-attention with multi-level Haar wavelet fusion for multi-scale feature capture.

Result: Achieves state-of-the-art performance on CUHK-SYSU and PRW datasets.

Conclusion: HAMW effectively improves transformer-based person search by optimizing feature extraction and computational efficiency.

Abstract: The person search task aims to locate a target person within a set of scene
images. In recent years, transformer-based models in this field have made some
progress. However, they still face three primary challenges: 1) the
self-attention mechanism tends to suppress high-frequency components in the
features, which severely impacts model performance; 2) the computational cost
of transformers is relatively high. To address these issues, we propose a novel
High-frequency Augmentation and Multi-Wave mixing (HAMW) method for person
search. HAMW is designed to enhance the discriminative feature extraction
capabilities of transformers while reducing computational overhead and
improving efficiency. Specifically, we develop a three-stage framework that
progressively optimizes both detection and re-identification performance. Our
model enhances the perception of high-frequency features by learning from
augmented inputs containing additional high-frequency components. Furthermore,
we replace the self-attention layers in the transformer with a strategy based
on multi-level Haar wavelet fusion to capture multi-scale features. This not
only lowers the computational complexity but also alleviates the suppression of
high-frequency features and enhances the ability to exploit multi-scale
information. Extensive experiments demonstrate that HAMW achieves
state-of-the-art performance on both the CUHK-SYSU and PRW datasets.

</details>


### [180] [BridgeShape: Latent Diffusion Schrödinger Bridge for 3D Shape Completion](https://arxiv.org/abs/2506.23205)
*Dequan Kong,Zhe Zhu,Honghua Chen,Mingqiang Wei*

Main category: cs.CV

TL;DR: BridgeShape introduces a latent diffusion Schr\"odinger bridge framework for 3D shape completion, optimizing global transport paths and leveraging a compact latent space for high-fidelity results.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack explicit modeling of optimal global transport paths and suffer from resolution constraints, limiting fine-grained geometric details.

Method: BridgeShape formulates shape completion as an optimal transport problem and uses a Depth-Enhanced VQ-VAE to encode 3D shapes into a compact latent space.

Result: Achieves state-of-the-art performance on 3D shape completion benchmarks with superior fidelity at higher resolutions and for unseen classes.

Conclusion: BridgeShape effectively addresses resolution constraints and improves 3D shape completion by modeling optimal transport and leveraging latent space.

Abstract: Existing diffusion-based 3D shape completion methods typically use a
conditional paradigm, injecting incomplete shape information into the denoising
network via deep feature interactions (e.g., concatenation, cross-attention) to
guide sampling toward complete shapes, often represented by voxel-based
distance functions. However, these approaches fail to explicitly model the
optimal global transport path, leading to suboptimal completions. Moreover,
performing diffusion directly in voxel space imposes resolution constraints,
limiting the generation of fine-grained geometric details. To address these
challenges, we propose BridgeShape, a novel framework for 3D shape completion
via latent diffusion Schr\"odinger bridge. The key innovations lie in two
aspects: (i) BridgeShape formulates shape completion as an optimal transport
problem, explicitly modeling the transition between incomplete and complete
shapes to ensure a globally coherent transformation. (ii) We introduce a
Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D
shapes into a compact latent space, leveraging self-projected multi-view depth
information enriched with strong DINOv2 features to enhance geometric
structural perception. By operating in a compact yet structurally informative
latent space, BridgeShape effectively mitigates resolution constraints and
enables more efficient and high-fidelity 3D shape completion. BridgeShape
achieves state-of-the-art performance on large-scale 3D shape completion
benchmarks, demonstrating superior fidelity at higher resolutions and for
unseen object classes.

</details>


### [181] [TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints](https://arxiv.org/abs/2506.23207)
*Zhen Tan,Xieyuanli Chen,Lei Feng,Yangbing Ge,Shuaifeng Zhi,Jiaxiong Liu,Dewen Hu*

Main category: cs.CV

TL;DR: TVG-SLAM improves RGB-only 3DGS SLAM by using tri-view geometry for robust tracking and mapping, outperforming prior methods in outdoor environments.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-only SLAM systems relying on photometric loss struggle with robustness in unbounded outdoor environments due to viewpoint and illumination changes.

Method: TVG-SLAM introduces tri-view geometry for dense matching, hybrid geometric constraints for tracking, probabilistic initialization for mapping, and dynamic attenuation to reduce drift.

Result: TVG-SLAM reduces tracking error by 69.0% in challenging datasets while maintaining high rendering quality.

Conclusion: TVG-SLAM offers a robust solution for RGB-only SLAM in dynamic outdoor environments, with open-source implementation planned.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM
systems to achieve high-fidelity scene representation. However, the heavy
reliance of existing systems on photometric rendering loss for camera tracking
undermines their robustness, especially in unbounded outdoor environments with
severe viewpoint and illumination changes. To address these challenges, we
propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel
tri-view geometry paradigm to ensure consistent tracking and high-quality
mapping. We introduce a dense tri-view matching module that aggregates reliable
pairwise correspondences into consistent tri-view matches, forming robust
geometric constraints across frames. For tracking, we propose Hybrid Geometric
Constraints, which leverage tri-view matches to construct complementary
geometric cues alongside photometric loss, ensuring accurate and stable pose
estimation even under drastic viewpoint shifts and lighting variations. For
mapping, we propose a new probabilistic initialization strategy that encodes
geometric uncertainty from tri-view correspondences into newly initialized
Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust
mechanism to mitigate tracking drift caused by mapping latency. Experiments on
multiple public outdoor datasets show that our TVG-SLAM outperforms prior
RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our
method improves tracking robustness, reducing the average Absolute Trajectory
Error (ATE) by 69.0\% while achieving state-of-the-art rendering quality. The
implementation of our method will be released as open-source.

</details>


### [182] [A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans](https://arxiv.org/abs/2506.23209)
*Chia-Wen Huang,Haw Hwai,Chien-Chang Lee,Pei-Yuan Wu*

Main category: cs.CV

TL;DR: A deep learning model using 3D CT scans with Slice Attention and hierarchical classification improves appendicitis diagnosis accuracy.


<details>
  <summary>Details</summary>
Motivation: Timely and accurate appendicitis diagnosis is crucial to prevent complications, but CT scan volume can overwhelm radiologists, causing delays.

Method: Proposes a deep learning model with 3D CT scans, Slice Attention for small lesion detection, and hierarchical classification using pre-trained 2D models.

Result: Improves AUC by 3% for appendicitis and 5.9% for complicated appendicitis.

Conclusion: The model offers a more efficient and reliable diagnostic solution compared to prior methods.

Abstract: Timely and accurate diagnosis of appendicitis is critical in clinical
settings to prevent serious complications. While CT imaging remains the
standard diagnostic tool, the growing number of cases can overwhelm
radiologists, potentially causing delays. In this paper, we propose a deep
learning model that leverages 3D CT scans for appendicitis classification,
incorporating Slice Attention mechanisms guided by external 2D datasets to
enhance small lesion detection. Additionally, we introduce a hierarchical
classification framework using pre-trained 2D models to differentiate between
simple and complicated appendicitis. Our approach improves AUC by 3% for
appendicitis and 5.9% for complicated appendicitis, offering a more efficient
and reliable diagnostic solution compared to previous work.

</details>


### [183] [High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation](https://arxiv.org/abs/2506.23227)
*Lunhao Duan,Shanshan Zhao,Xingxing Weng,Jing Zhang,Gui-Song Xia*

Main category: cs.CV

TL;DR: The paper proposes a framework for indoor point cloud semantic segmentation using scene-level annotations, improving pseudo-label quality via multi-modal information and region-point consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with accurate pseudo-label generation from scene-level annotations, impacting segmentation performance.

Method: The framework uses cross-modal feature guidance (2D-3D alignment) and a region-point semantic consistency module to refine pseudo-labels.

Result: Achieves significant improvements on ScanNet v2 and S3DIS datasets, validated by ablation studies.

Conclusion: The method effectively enhances pseudo-label quality and segmentation accuracy under scene-level annotation constraints.

Abstract: This paper investigates indoor point cloud semantic segmentation under
scene-level annotation, which is less explored compared to methods relying on
sparse point-level labels. In the absence of precise point-level labels,
current methods first generate point-level pseudo-labels, which are then used
to train segmentation models. However, generating accurate pseudo-labels for
each point solely based on scene-level annotations poses a considerable
challenge, substantially affecting segmentation performance. Consequently, to
enhance accuracy, this paper proposes a high-quality pseudo-label generation
framework by exploring contemporary multi-modal information and region-point
semantic consistency. Specifically, with a cross-modal feature guidance module,
our method utilizes 2D-3D correspondences to align point cloud features with
corresponding 2D image pixels, thereby assisting point cloud feature learning.
To further alleviate the challenge presented by the scene-level annotation, we
introduce a region-point semantic consistency module. It produces regional
semantics through a region-voting strategy derived from point-level semantics,
which are subsequently employed to guide the point-level semantic predictions.
Leveraging the aforementioned modules, our method can rectify inaccurate
point-level semantic predictions during training and obtain high-quality
pseudo-labels. Significant improvements over previous works on ScanNet v2 and
S3DIS datasets under scene-level annotation can demonstrate the effectiveness.
Additionally, comprehensive ablation studies validate the contributions of our
approach's individual components. The code is available at
https://github.com/LHDuan/WSegPC .

</details>


### [184] [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/abs/2506.23236)
*Marko Mihajlovic,Siwei Zhang,Gen Li,Kaifeng Zhao,Lea Müller,Siyu Tang*

Main category: cs.CV

TL;DR: VolumetricSMPL introduces a neural volumetric body model using Neural Blend Weights (NBW) for efficient MLP decoders, outperforming prior models in speed, memory, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional surface mesh models struggle with interactions involving other geometric entities, while existing volumetric models are either inefficient or lack robustness for complex articulations.

Method: VolumetricSMPL uses NBW to dynamically blend learned weight matrices, reducing computational costs while maintaining expressiveness.

Result: The model achieves 10x faster inference, 6x lower GPU memory usage, and improved accuracy, with applications in human-object interaction, 3D scene reconstruction, motion synthesis, and self-intersection resolution.

Conclusion: VolumetricSMPL offers a robust, efficient solution for volumetric human body modeling, with broad applicability and significant performance gains.

Abstract: Parametric human body models play a crucial role in computer graphics and
vision, enabling applications ranging from human motion analysis to
understanding human-environment interactions. Traditionally, these models use
surface meshes, which pose challenges in efficiently handling interactions with
other geometric entities, such as objects and scenes, typically represented as
meshes or point clouds. To address this limitation, recent research has
explored volumetric neural implicit body models. However, existing works are
either insufficiently robust for complex human articulations or impose high
computational and memory costs, limiting their widespread use. To this end, we
introduce VolumetricSMPL, a neural volumetric body model that leverages Neural
Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike
prior approaches that rely on large MLPs, NBW dynamically blends a small set of
learned weight matrices using predicted shape- and pose-dependent coefficients,
significantly improving computational efficiency while preserving
expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model
COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,
and a Signed Distance Function (SDF) for efficient and differentiable contact
modeling. We demonstrate VolumetricSMPL's strengths across four challenging
tasks: (1) reconstructing human-object interactions from in-the-wild images,
(2) recovering human meshes in 3D scenes from egocentric views, (3)
scene-constrained motion synthesis, and (4) resolving self-intersections. Our
results highlight its broad applicability and significant performance and
efficiency gains.

</details>


### [185] [Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification](https://arxiv.org/abs/2506.23247)
*James Hinns,David Martens*

Main category: cs.CV

TL;DR: Segment Attribution Tables (SATs) summarize local saliency maps into semi-global insights, bridging the gap between oversimplified global summaries and overly detailed local explanations.


<details>
  <summary>Details</summary>
Motivation: Deep learning models lack interpretability, and existing methods (local or global) are either too detailed or oversimplified. SATs aim to provide a balanced solution.

Method: SATs use image segments (e.g., "eyes") and saliency maps to quantify segment influence, revealing model reliance on concepts and spurious correlations.

Result: SATs uncover recurring patterns and spurious correlations (e.g., backgrounds, watermarks) in model predictions, even when performance metrics don't reflect issues.

Conclusion: SATs offer a practical tool for analyzing and debugging image classifiers, balancing local and global explanation needs.

Abstract: Deep learning dominates image classification tasks, yet understanding how
models arrive at predictions remains a challenge. Much research focuses on
local explanations of individual predictions, such as saliency maps, which
visualise the influence of specific pixels on a model's prediction. However,
reviewing many of these explanations to identify recurring patterns is
infeasible, while global methods often oversimplify and miss important local
behaviours. To address this, we propose Segment Attribution Tables (SATs), a
method for summarising local saliency explanations into (semi-)global insights.
SATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency
maps to quantify their influence. These segments highlight concepts the model
relies on across instances and reveal spurious correlations, such as reliance
on backgrounds or watermarks, even when out-of-distribution test performance
sees little change. SATs can explain any classifier for which a form of
saliency map can be produced, using segmentation maps that provide named
segments. SATs bridge the gap between oversimplified global summaries and
overly detailed local explanations, offering a practical tool for analysing and
debugging image classifiers.

</details>


### [186] [DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection](https://arxiv.org/abs/2506.23252)
*Kunwei Lv,Ping Lan*

Main category: cs.CV

TL;DR: DGE-YOLO is an enhanced YOLO-based framework for multi-modal UAV object detection, featuring dual-branch architecture, EMA mechanism, and Gather-and-Distribute module, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting small objects in complex aerial scenarios with multi-modal inputs, where existing methods trade performance for speed.

Method: Introduces a dual-branch architecture for modality-specific feature extraction, an EMA mechanism for multi-scale feature learning, and a Gather-and-Distribute module to reduce information loss.

Result: Superior performance on the Drone Vehicle dataset compared to state-of-the-art methods.

Conclusion: DGE-YOLO effectively addresses multi-modal UAV object detection challenges, demonstrating robustness and efficiency.

Abstract: The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted
the importance of robust and efficient object detection in diverse aerial
scenarios. Detecting small objects under complex conditions, however, remains a
significant challenge. Existing approaches often prioritize inference speed,
leading to degraded performance when handling multi-modal inputs. To address
this, we present DGE-YOLO, an enhanced YOLO-based detection framework designed
to effectively fuse multi-modal information. Specifically, we introduce a
dual-branch architecture for modality-specific feature extraction, enabling the
model to process both infrared and visible images. To further enrich semantic
representation, we propose an Efficient Multi-scale Attention (EMA) mechanism
that enhances feature learning across spatial scales. Additionally, we replace
the conventional neck with a Gather-and-Distribute module to mitigate
information loss during feature aggregation. Extensive experiments on the Drone
Vehicle dataset demonstrate that DGE-YOLO achieves superior performance over
state-of-the-art methods, validating its effectiveness in multi-modal UAV
object detection tasks.

</details>


### [187] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/abs/2506.23254)
*Aradhana Mishra,Bumshik Lee*

Main category: cs.CV

TL;DR: PixelBoost, a novel diffusion model, improves image super-resolution by leveraging controlled stochasticity from Brownian motion, enhancing realism and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between realistic image generation and computational efficiency in diffusion-model-based super-resolution.

Method: Integrates controlled stochasticity into training, uses sigmoidal noise sequencing, and adapts to Brownian noise patterns.

Result: Superior performance in LPIPS, LOE, PSNR, SSIM, visual quality, and edge reconstruction.

Conclusion: PixelBoost effectively balances realism and efficiency, advancing super-resolution techniques.

Abstract: Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree of realism, particularly
focusing on texture and edge definitions. By integrating controlled
stochasticity into the training regimen, our proposed model avoids convergence
to local optima, effectively capturing and reproducing the inherent uncertainty
of image textures and patterns. Our proposed model demonstrates superior
objective results in terms of learned perceptual image patch similarity
(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),
structural similarity index measure (SSIM), as well as visual quality. To
determine the edge enhancement, we evaluated the gradient magnitude and pixel
value, and our proposed model exhibited a better edge reconstruction
capability. Additionally, our model demonstrates adaptive learning capabilities
by effectively adjusting to Brownian noise patterns and introduces a sigmoidal
noise sequencing method that simplifies training, resulting in faster inference
speeds.

</details>


### [188] [Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](https://arxiv.org/abs/2506.23714)
*Md Moinul Islam,Sofoklis Kakouros,Janne Heikkilä,Mourad Oussalah*

Main category: cs.CV

TL;DR: A multimodal video summarization framework integrates text, audio, and visual cues to generate timestamp-aligned summaries, outperforming traditional methods in both text and video metrics.


<details>
  <summary>Details</summary>
Motivation: The need for effective summarization techniques due to the growing volume of video content in various domains.

Method: Behaviour-aware framework combining prosodic features, textual cues, and visual indicators to identify important moments, including bonus words emphasized across modalities.

Result: Significant improvements in metrics: ROUGE-1 (0.4769 to 0.7929), BERTScore (0.9152 to 0.9536), and F1-Score (23% increase).

Conclusion: Multimodal integration enhances the quality and relevance of video summaries, demonstrating its potential for comprehensive summarization.

Abstract: The increasing volume of video content in educational, professional, and
social domains necessitates effective summarization techniques that go beyond
traditional unimodal approaches. This paper proposes a behaviour-aware
multimodal video summarization framework that integrates textual, audio, and
visual cues to generate timestamp-aligned summaries. By extracting prosodic
features, textual cues and visual indicators, the framework identifies
semantically and emotionally important moments. A key contribution is the
identification of bonus words, which are terms emphasized across multiple
modalities and used to improve the semantic relevance and expressive clarity of
the summaries. The approach is evaluated against pseudo-ground truth (pGT)
summaries generated using LLM-based extractive method. Experimental results
demonstrate significant improvements over traditional extractive method, such
as the Edmundson method, in both text and video-based evaluation metrics.
Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore
from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework
improves F1-Score by almost 23%. The findings underscore the potential of
multimodal integration in producing comprehensive and behaviourally informed
video summaries.

</details>


### [189] [PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation](https://arxiv.org/abs/2506.23257)
*Chongke Bi,Xin Gao,Baofeng Fu,Yuheng Zhao,Siming Chen,Ying Zhao,Yunhai Wang*

Main category: cs.CV

TL;DR: PCLVis is a framework for analyzing process communication latency (PCL) in large-scale simulations using MPI data, improving scalability by clustering processes and visualizing communication paths.


<details>
  <summary>Details</summary>
Motivation: Scalability issues in supercomputing simulations due to high communication costs, with existing methods requiring inaccessible physical link layer data.

Method: Uses MPI process communication data, spatial PCL event locating, process-correlation tree clustering, DAG-based path analysis, and CS-Glyphs for visualization.

Result: Demonstrated effectiveness on TH-1A supercomputer, enabling users to optimize simulations and improve efficiency.

Conclusion: PCLVis provides a practical solution for general users to analyze and optimize communication latency without physical link layer access.

Abstract: Large-scale simulations on supercomputers have become important tools for
users. However, their scalability remains a problem due to the huge
communication cost among parallel processes. Most of the existing communication
latency analysis methods rely on the physical link layer information, which is
only available to administrators. In this paper, a framework called PCLVis is
proposed to help general users analyze process communication latency (PCL)
events. Instead of the physical link layer information, the PCLVis uses the MPI
process communication data for the analysis. First, a spatial PCL event
locating method is developed. All processes with high correlation are
classified into a single cluster by constructing a process-correlation tree.
Second, the propagation path of PCL events is analyzed by constructing a
communication-dependency-based directed acyclic graph (DAG), which can help
users interactively explore a PCL event from the temporal evolution of a
located PCL event cluster. In this graph, a sliding window algorithm is
designed to generate the PCL events abstraction. Meanwhile, a new glyph called
the communication state glyph (CS-Glyph) is designed for each process to show
its communication states, including its in/out messages and load balance. Each
leaf node can be further unfolded to view additional information. Third, a PCL
event attribution strategy is formulated to help users optimize their
simulations. The effectiveness of the PCLVis framework is demonstrated by
analyzing the PCL events of several simulations running on the TH-1A
supercomputer. By using the proposed framework, users can greatly improve the
efficiency of their simulations.

</details>


### [190] [Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis](https://arxiv.org/abs/2506.23263)
*Lei-lei Li,Jianwu Fang,Junbin Xiao,Shanmin Pang,Hongkai Yu,Chen Lv,Jianru Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: A novel diffusion model, Causal-VidSyn, is proposed for synthesizing egocentric traffic accident videos by leveraging cause descriptions and driver fixations, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Understanding and simulating car accidents is vital for self-driving car safety, but incorporating real-world causal relations into synthetic videos is challenging.

Method: Causal-VidSyn uses cause descriptions and driver fixations to identify accident participants and behaviors, aided by accident reason answering and gaze-conditioned selection modules.

Result: The model excels in frame quality and causal sensitivity for tasks like accident video editing and text-to-video generation.

Conclusion: Causal-VidSyn advances synthetic video generation for accident scenarios, supported by the Drive-Gaze dataset, enhancing self-driving car safety testing.

Abstract: Egocentricly comprehending the causes and effects of car accidents is crucial
for the safety of self-driving cars, and synthesizing causal-entity reflected
accident videos can facilitate the capability test to respond to unaffordable
accidents in reality. However, incorporating causal relations as seen in
real-world videos into synthetic videos remains challenging. This work argues
that precisely identifying the accident participants and capturing their
related behaviors are of critical importance. In this regard, we propose a
novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic
accident videos. To enable causal entity grounding in video diffusion,
Causal-VidSyn leverages the cause descriptions and driver fixations to identify
the accident participants and behaviors, facilitated by accident reason
answering and gaze-conditioned selection modules. To support Causal-VidSyn, we
further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M
frames of fixations) in driving accident scenarios. Extensive experiments show
that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms
of frame quality and causal sensitivity in various tasks, including accident
video editing, normal-to-accident video diffusion, and text-to-video
generation.

</details>


### [191] [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/abs/2506.23270)
*Yi Li,Hualiang Wang,Xinpeng Ding,Haonan Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper introduces Token Activation Map (TAM), a method to improve the explainability of Multimodal Large Language Models (MLLMs) by addressing redundant activations and enhancing visualization quality.


<details>
  <summary>Details</summary>
Motivation: The explainability of MLLMs is underexplored, and existing methods overlook redundant activations that interfere with reliable explanations.

Method: Proposes an estimated causal inference method and a rank Gaussian filter to mitigate context interference and reduce activation noises, termed Token Activation Map (TAM).

Result: TAM outperforms state-of-the-art methods, providing high-quality visualizations for diverse applications like object localization and model understanding.

Conclusion: TAM effectively addresses the limitations of existing methods, offering a robust solution for explaining MLLMs.

Abstract: Multimodal large language models (MLLMs) are broadly empowering various
fields. Despite their advancements, the explainability of MLLMs remains less
explored, hindering deeper understanding, model credibility, and effective
visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that
produce a single output, MLLMs generate sequences of tokens progressively,
where each generated token depends on the previous context. Therefore, earlier
context tokens can introduce redundant activations that interfere with the
explanation of later tokens beyond their original information. Existing studies
often overlook this issue, but our observations reveal that these redundant
correlations can significantly hurt the reliability of explanations. To address
this, we propose an estimated causal inference method to mitigate the
interference of context to achieve high-quality MLLM explanation, with a novel
rank Gaussian filter to further reduce activation noises. We term this method
Token Activation Map (TAM) to highlight the consideration of interactions
between tokens. TAM also indicates that it excels at explaining multiple tokens
of MLLM, which is different from the Class Activation Map (CAM) for a single
prediction. Our TAM method significantly outperforms existing SoTA methods,
showcasing high-quality visualization results that can be utilized for various
scenarios, such as object localization, failure case analysis, video
visualization, MLLMs visual comparison, and model understanding (e.g., color,
shape, action, location, visual reasoning, multi-turn conversation, etc). The
code is available atgithub.com/xmed-lab/TAM.

</details>


### [192] [Ella: Embodied Social Agents with Lifelong Memory](https://arxiv.org/abs/2506.24019)
*Hongxin Zhang,Zheyuan Zhang,Zeyuan Wang,Zunzhe Zhang,Lixing Fang,Qinhong Zhou,Chuang Gan*

Main category: cs.CV

TL;DR: Ella is an embodied social agent with lifelong learning capabilities in a 3D open world, using a multimodal memory system and foundation models for decision-making and social interactions.


<details>
  <summary>Details</summary>
Motivation: To advance embodied intelligence by integrating structured memory systems with foundation models for lifelong learning and social interaction in dynamic environments.

Method: Ella employs a name-centric semantic memory and spatiotemporal episodic memory, combined with foundation models, to store, update, and retrieve information for decision-making and social activities.

Result: Ella successfully influences, leads, and cooperates with other agents in a dynamic 3D world, demonstrating effective learning through observation and interaction.

Conclusion: The integration of structured memory systems with foundation models holds transformative potential for advancing embodied intelligence.

Abstract: We introduce Ella, an embodied social agent capable of lifelong learning
within a community in a 3D open world, where agents accumulate experiences and
acquire knowledge through everyday visual observations and social interactions.
At the core of Ella's capabilities is a structured, long-term multimodal memory
system that stores, updates, and retrieves information effectively. It consists
of a name-centric semantic memory for organizing acquired knowledge and a
spatiotemporal episodic memory for capturing multimodal experiences. By
integrating this lifelong memory system with foundation models, Ella retrieves
relevant information for decision-making, plans daily activities, builds social
relationships, and evolves autonomously while coexisting with other intelligent
beings in the open world. We conduct capability-oriented evaluations in a
dynamic 3D open world where 15 agents engage in social activities for days and
are assessed with a suite of unseen controlled evaluations. Experimental
results show that Ella can influence, lead, and cooperate with other agents
well to achieve goals, showcasing its ability to learn effectively through
observation and social interaction. Our findings highlight the transformative
potential of combining structured memory systems with foundation models for
advancing embodied intelligence. More videos can be found at
https://umass-embodied-agi.github.io/Ella/.

</details>


### [193] [Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation](https://arxiv.org/abs/2506.23271)
*Jinxing Zhou,Zhihui Li,Yongqiang Yu,Yanghao Zhou,Ruohao Guo,Guangyao Li,Yuxin Mao,Mingfei Han,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: Mettle is a memory-efficient method for adapting pretrained transformers to audio-visual tasks using Layer-Centric Distillation and Meta-Token Injection.


<details>
  <summary>Details</summary>
Motivation: To adapt large-scale pretrained transformers efficiently for downstream audio-visual tasks while preserving pretrained knowledge and enabling task-specific adaptation.

Method: Uses Layer-Centric Distillation (LCD) to distill features into meta-tokens and Meta-Token Injection (MTI) for fine-grained segmentation.

Result: Reduces memory usage and training time while maintaining competitive accuracy.

Conclusion: Mettle is effective for audio-visual tasks, balancing efficiency and performance.

Abstract: We present \textbf{Met}a-\textbf{T}oken \textbf{Le}arning (Mettle), a simple
and memory-efficient method for adapting large-scale pretrained transformer
models to downstream audio-visual tasks. Instead of sequentially modifying the
output feature distribution of the transformer backbone, Mettle utilizes a
lightweight \textit{Layer-Centric Distillation (LCD)} module to distill in
parallel the intact audio or visual features embedded by each transformer layer
into compact meta-tokens. This distillation process considers both pretrained
knowledge preservation and task-specific adaptation. The obtained meta-tokens
can be directly applied to classification tasks, such as audio-visual event
localization and audio-visual video parsing. To further support fine-grained
segmentation tasks, such as audio-visual segmentation, we introduce a
\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual
meta-tokens distilled from the top transformer layer to guide feature
adaptation in earlier layers. Extensive experiments on multiple audiovisual
benchmarks demonstrate that our method significantly reduces memory usage and
training time while maintaining parameter efficiency and competitive accuracy.

</details>


### [194] [Why Settle for One? Text-to-ImageSet Generation and Evaluation](https://arxiv.org/abs/2506.23275)
*Chengyou Jia,Xin Shen,Zhuohang Dang,Zhuohang Dang,Changliang Xia,Weijia Wu,Xinyu Zhang,Hangwei Qian,Ivor W. Tsang,Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces Text-to-ImageSet (T2IS) generation, a challenging problem of creating coherent image sets with diverse consistency requirements. It proposes T2IS-Bench, T2IS-Eval, and AutoT2IS for systematic study, evaluation, and generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for consistent image generation are domain-specific, limiting generalizability. The paper addresses this by proposing T2IS generation to meet diverse consistency needs.

Method: The authors introduce T2IS-Bench for diverse instructions, T2IS-Eval for multifaceted evaluation, and AutoT2IS, a training-free framework leveraging Diffusion Transformers for consistency.

Result: AutoT2IS outperforms existing methods on T2IS-Bench, handling diverse consistency challenges and enabling real-world applications.

Conclusion: The proposed framework advances T2IS generation, demonstrating practical value and outperforming specialized approaches.

Abstract: Despite remarkable progress in Text-to-Image models, many real-world
applications require generating coherent image sets with diverse consistency
requirements. Existing consistent methods often focus on a specific domain with
specific aspects of consistency, which significantly constrains their
generalizability to broader applications. In this paper, we propose a more
challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate
sets of images that meet various consistency requirements based on user
instructions. To systematically study this problem, we first introduce
$\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,
providing comprehensive coverage for T2IS generation. Building on this, we
propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user
instructions into multifaceted assessment criteria and employs effective
evaluators to adaptively assess consistency fulfillment between criteria and
generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free
framework that maximally leverages pretrained Diffusion Transformers'
in-context capabilities to harmonize visual elements to satisfy both
image-level prompt alignment and set-level visual consistency. Extensive
experiments on T2IS-Bench reveal that diverse consistency challenges all
existing methods, while our AutoT2IS significantly outperforms current
generalized and even specialized approaches. Our method also demonstrates the
ability to enable numerous underexplored real-world applications, confirming
its substantial practical value. Visit our project in
https://chengyou-jia.github.io/T2IS-Home.

</details>


### [195] [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/abs/2506.24086)
*Bingfan Zhu,Biao Jiang,Sunyi Wang,Shixiang Tang,Tao Chen,Linjie Luo,Youyi Zheng,Xin Chen*

Main category: cs.CV

TL;DR: MotionGPT3 is a bimodal motion-language model addressing challenges in unified motion-language understanding and generation by decoupling motion modeling and preserving language intelligence.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between continuous motion and discrete language representations and prevent degradation of language capabilities during unified training.

Method: Uses a bimodal approach with separate motion and text branches, leveraging a shared attention mechanism and motion VAE for latent representations. Motion is predicted via a diffusion head.

Result: Achieves competitive performance in motion understanding and generation while maintaining strong language capabilities.

Conclusion: MotionGPT3 establishes a unified bimodal motion diffusion framework within an autoregressive model.

Abstract: Though recent advances in multimodal models have demonstrated strong
capabilities and opportunities in unified understanding and generation, the
development of unified motion-language models remains underexplored. To enable
such models with high-fidelity human motion, two core challenges must be
addressed. The first is the reconstruction gap between the continuous motion
modality and discrete representation in an autoregressive manner, and the
second is the degradation of language intelligence during unified training.
Inspired by the mixture of experts, we propose MotionGPT3, a bimodal
motion-language model that treats human motion as a second modality, decoupling
motion modeling via separate model parameters and enabling both effective
cross-modal interaction and efficient multimodal scaling training. To preserve
language intelligence, the text branch retains the original structure and
parameters of the pretrained language model, while a new motion branch is
integrated via a shared attention mechanism, enabling bidirectional information
flow between two modalities. We first employ a motion Variational Autoencoder
(VAE) to encode raw human motion into latent representations. Based on this
continuous latent space, the motion branch predicts motion latents directly
from intermediate hidden states using a diffusion head, bypassing discrete
tokenization. Extensive experiments show that our approach achieves competitive
performance on both motion understanding and generation tasks while preserving
strong language capabilities, establishing a unified bimodal motion diffusion
framework within an autoregressive manner.

</details>


### [196] [Autoregressive Denoising Score Matching is a Good Video Anomaly Detector](https://arxiv.org/abs/2506.23282)
*Hanwen Zhang,Congqi Cao,Qinyi Lv,Lingtong Min,Yanning Zhang*

Main category: cs.CV

TL;DR: The paper proposes a method to address gaps in likelihood-based video anomaly detection by integrating scene, motion, and appearance awareness through a noise-conditioned score transformer and autoregressive denoising.


<details>
  <summary>Details</summary>
Motivation: Likelihood-based methods for video anomaly detection (VAD) miss anomalies near learned distributions. The paper aims to address this by tackling gaps in scene, motion, and appearance.

Method: Uses a noise-conditioned score transformer for denoising score matching, introduces scene-dependent and motion-aware scoring, and employs autoregressive denoising for enhanced anomaly detection.

Result: Achieves state-of-the-art performance on three VAD benchmarks.

Conclusion: The proposed method effectively addresses gaps in VAD, improving detection of anomalies near learned distributions.

Abstract: Video anomaly detection (VAD) is an important computer vision problem. Thanks
to the mode coverage capabilities of generative models, the likelihood-based
paradigm is catching growing interest, as it can model normal distribution and
detect out-of-distribution anomalies. However, these likelihood-based methods
are blind to the anomalies located in local modes near the learned
distribution. To handle these ``unseen" anomalies, we dive into three gaps
uniquely existing in VAD regarding scene, motion and appearance. Specifically,
we first build a noise-conditioned score transformer for denoising score
matching. Then, we introduce a scene-dependent and motion-aware score function
by embedding the scene condition of input sequences into our model and
assigning motion weights based on the difference between key frames of input
sequences. Next, to solve the problem of blindness in principle, we integrate
unaffected visual information via a novel autoregressive denoising score
matching mechanism for inference. Through autoregressively injecting
intensifying Gaussian noise into the denoised data and estimating the
corresponding score function, we compare the denoised data with the original
data to get a difference and aggregate it with the score function for an
enhanced appearance perception and accumulate the abnormal context. With all
three gaps considered, we can compute a more comprehensive anomaly indicator.
Experiments on three popular VAD benchmarks demonstrate the state-of-the-art
performance of our method.

</details>


### [197] [MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition](https://arxiv.org/abs/2506.23283)
*Yuhuan Yang,Chaofan Ma,Zhenjie Mao,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: MoMa is an efficient adapter framework for video understanding that integrates Mamba's selective state space modeling into image foundation models (IFMs) for full spatial-temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting IFMs to video often process spatial and temporal information separately, missing the full intricacy of video dynamics.

Method: MoMa introduces SeqMod to inject spatial-temporal information into IFMs and uses a Divide-and-Modulate architecture for efficient modeling.

Result: MoMa achieves superior performance on multiple video benchmarks with reduced computational cost.

Conclusion: MoMa effectively enhances video understanding by unifying spatial-temporal modeling in a computationally efficient way.

Abstract: Video understanding is a complex challenge that requires effective modeling
of spatial-temporal dynamics. With the success of image foundation models
(IFMs) in image understanding, recent approaches have explored
parameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most
of these methods tend to process spatial and temporal information separately,
which may fail to capture the full intricacy of video dynamics. In this paper,
we propose MoMa, an efficient adapter framework that achieves full
spatial-temporal modeling by integrating Mamba's selective state space modeling
into IFMs. We propose a novel SeqMod operation to inject spatial-temporal
information into pre-trained IFMs, without disrupting their original features.
By incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances
video understanding while maintaining computational efficiency. Extensive
experiments on multiple video benchmarks demonstrate the effectiveness of MoMa,
achieving superior performance with reduced computational cost.

</details>


### [198] [Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification](https://arxiv.org/abs/2506.23285)
*Daqian Shi,Xiaolei Diao,Xu Chen,Cédric M. John*

Main category: cs.CV

TL;DR: A novel competitive distillation strategy is proposed to enhance DNN training by allowing networks to act as teachers based on performance, improving learning through competition and stochastic perturbation.


<details>
  <summary>Details</summary>
Motivation: Current distillation methods like mutual learning and self-distillation have limited improvements due to poor understanding of learning directions across iterations.

Method: Competitive distillation organizes networks to compete and act as teachers based on performance, with stochastic perturbation to encourage better representations.

Result: Experiments show promising performance across diverse tasks and datasets.

Conclusion: Competitive distillation effectively enhances training performance by leveraging competition and stochastic perturbation.

Abstract: Deep Neural Networks (DNNs) have significantly advanced the field of computer
vision. To improve DNN training process, knowledge distillation methods
demonstrate their effectiveness in accelerating network training by introducing
a fixed learning direction from the teacher network to student networks. In
this context, several distillation-based optimization strategies are proposed,
e.g., deep mutual learning and self-distillation, as an attempt to achieve
generic training performance enhancement through the cooperative training of
multiple networks. However, such strategies achieve limited improvements due to
the poor understanding of the impact of learning directions among networks
across different iterations. In this paper, we propose a novel competitive
distillation strategy that allows each network in a group to potentially act as
a teacher based on its performance, enhancing the overall learning performance.
Competitive distillation organizes a group of networks to perform a shared task
and engage in competition, where competitive optimization is proposed to
improve the parameter updating process. We further introduce stochastic
perturbation in competitive distillation, aiming to motivate networks to induce
mutations to achieve better visual representations and global optimum. The
experimental results show that competitive distillation achieves promising
performance in diverse tasks and datasets.

</details>


### [199] [DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios](https://arxiv.org/abs/2506.23292)
*Changtao Miao,Yi Zhang,Weize Gao,Man Luo,Weiwei Feng,Zhiya Tan,Jianshu Li,Ajian Liu,Yunfeng Diao,Qi Chu,Tao Gong,Zhe Li,Weibin Yao,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces a large-scale deepfake detection and localization (DDL) dataset to address the lack of interpretability and diversity in existing deepfake detection methods.


<details>
  <summary>Details</summary>
Motivation: The misuse of deepfake content necessitates reliable detection methods with interpretability, especially in critical domains like law. Existing datasets lack diversity and fine-grained annotations, limiting practical effectiveness.

Method: The authors construct the DDL dataset with 1.8M forged samples, 75 deepfake methods, and four key innovations: diverse scenarios, comprehensive methods, varied manipulation modes, and fine-grained annotations.

Result: The DDL dataset provides a challenging benchmark for real-world forgeries and supports advanced detection, localization, and interpretability methods.

Conclusion: The DDL dataset addresses limitations of current datasets and enhances the development of next-generation deepfake detection tools.

Abstract: Recent advances in AIGC have exacerbated the misuse of malicious deepfake
content, making the development of reliable deepfake detection methods an
essential means to address this challenge. Although existing deepfake detection
models demonstrate outstanding performance in detection metrics, most methods
only provide simple binary classification results, lacking interpretability. In
critical domains such as law, interpretability is crucial for enhancing the
credibility and authority of decisions. Recent studies attempt to improve the
interpretability of classification results by providing spatial manipulation
masks or temporal forgery segments. However, the practical effectiveness of
these methods remains suboptimal due to limitations of the forgery data. Most
current deepfake datasets predominantly offer binary labels, only a few
datasets with localization annotations. However, they suffer from restricted
forgery scenarios, limited diversity in deepfake types, and insufficient data
scale, making them inadequate for complex real-world scenarios. To address this
predicament, we construct a novel large-scale deepfake detection and
localization ($\textbf{DDL}$) dataset containing over $\textbf{1.8M}$ forged
samples and encompassing up to $\textbf{75}$ distinct deepfake methods. The DDL
design incorporates four key innovations: (1) $\textbf{Diverse Forgery
Scenarios}$, (2) $\textbf{Comprehensive Deepfake Methods}$, (3) $\textbf{Varied
Manipulation Modes}$, and (4) $\textbf{Fine-grained Forgery Annotations}$.
Through these improvements, our DDL not only provides a more challenging
benchmark for complex real-world forgeries, but also offers crucial support for
building next-generation deepfake detection, localization, and interpretability
methods. The DDL dataset project page is on
https://deepfake-workshop-ijcai2025.github.io/main/index.html.

</details>


### [200] [DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On](https://arxiv.org/abs/2506.23295)
*Xiang Xu*

Main category: cs.CV

TL;DR: DiffFit is a two-stage latent diffusion framework for virtual try-on, addressing garment detail preservation, alignment, efficiency, and generalization. It outperforms existing methods in quality and realism.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on methods struggle with preserving garment details, precise alignment, efficiency, and generalization to diverse poses and styles.

Method: DiffFit uses a two-stage approach: geometry-aware garment warping for alignment, followed by texture refinement via cross-modal conditional diffusion.

Result: DiffFit achieves superior performance in quantitative metrics and perceptual evaluations on large-scale benchmarks.

Conclusion: DiffFit effectively decouples geometric alignment and appearance refinement, enhancing stability and realism in virtual try-on.

Abstract: Virtual try-on (VTON) aims to synthesize realistic images of a person wearing
a target garment, with broad applications in e-commerce and digital fashion.
While recent advances in latent diffusion models have substantially improved
visual quality, existing approaches still struggle with preserving fine-grained
garment details, achieving precise garment-body alignment, maintaining
inference efficiency, and generalizing to diverse poses and clothing styles. To
address these challenges, we propose DiffFit, a novel two-stage latent
diffusion framework for high-fidelity virtual try-on. DiffFit adopts a
progressive generation strategy: the first stage performs geometry-aware
garment warping, aligning the garment with the target body through fine-grained
deformation and pose adaptation. The second stage refines texture fidelity via
a cross-modal conditional diffusion model that integrates the warped garment,
the original garment appearance, and the target person image for high-quality
rendering. By decoupling geometric alignment and appearance refinement, DiffFit
effectively reduces task complexity and enhances both generation stability and
visual realism. It excels in preserving garment-specific attributes such as
textures, wrinkles, and lighting, while ensuring accurate alignment with the
human body. Extensive experiments on large-scale VTON benchmarks demonstrate
that DiffFit achieves superior performance over existing state-of-the-art
methods in both quantitative metrics and perceptual evaluations.

</details>


### [201] [Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting](https://arxiv.org/abs/2506.23308)
*Yiming Huang,Long Bai,Beilei Cui,Yanheng Li,Tong Chen,Jie Wang,Jinlin Wu,Zhen Lei,Hongbin Liu,Hongliang Ren*

Main category: cs.CV

TL;DR: Endo-4DGX improves 3D Gaussian Splatting for endoscopic scenes by addressing illumination challenges, achieving better rendering under extreme lighting.


<details>
  <summary>Details</summary>
Motivation: Accurate soft tissue reconstruction in robotic surgery is hindered by poor rendering quality under varying illumination (low light, over-exposure).

Method: Endo-4DGX uses illumination-adaptive Gaussian Splatting with illumination embeddings, region-aware enhancement, and spatial-aware adjustment modules, plus an exposure control loss.

Result: Superior rendering performance under extreme lighting while maintaining geometric accuracy, outperforming state-of-the-art methods.

Conclusion: Endo-4DGX advances robotic surgery applications by effectively handling illumination challenges in endoscopic scenes.

Abstract: Accurate reconstruction of soft tissue is crucial for advancing automation in
image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)
techniques and their variants, 4DGS, achieve high-quality renderings of dynamic
surgical scenes in real-time. However, 3D-GS-based methods still struggle in
scenarios with varying illumination, such as low light and over-exposure.
Training 3D-GS in such extreme light conditions leads to severe optimization
problems and devastating rendering quality. To address these challenges, we
present Endo-4DGX, a novel reconstruction method with illumination-adaptive
Gaussian Splatting designed specifically for endoscopic scenes with uneven
lighting. By incorporating illumination embeddings, our method effectively
models view-dependent brightness variations. We introduce a region-aware
enhancement module to model the sub-area lightness at the Gaussian level and a
spatial-aware adjustment module to learn the view-consistent brightness
adjustment. With the illumination adaptive design, Endo-4DGX achieves superior
rendering performance under both low-light and over-exposure conditions while
maintaining geometric accuracy. Additionally, we employ an exposure control
loss to restore the appearance from adverse exposure to the normal level for
illumination-adaptive optimization. Experimental results demonstrate that
Endo-4DGX significantly outperforms combinations of state-of-the-art
reconstruction and restoration methods in challenging lighting environments,
underscoring its potential to advance robot-assisted surgical applications. Our
code is available at https://github.com/lastbasket/Endo-4DGX.

</details>


### [202] [FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method](https://arxiv.org/abs/2506.23323)
*Quang-Huy Che,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: FastSeg is a training-free framework for open-vocabulary semantic segmentation, using a pretrained diffusion model with minimal steps and innovative components to enhance segmentation quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning models lose spatial precision, while diffusion models struggle with iteration balance. FastSeg aims to bridge this gap.

Method: FastSeg uses a (1+1)-step reverse process of a pretrained diffusion model, a dual-prompt mechanism, Hierarchical Attention Refinement (HARD), and Test-Time Flipping (TTF).

Result: Achieves 43.8% average mIoU on PASCAL VOC, PASCAL Context, and COCO Object benchmarks with high efficiency.

Conclusion: FastSeg balances segmentation quality and efficiency, offering a strong foundation for future extensions.

Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment objects from
arbitrary text categories without requiring densely annotated datasets.
Although contrastive learning based models enable zero-shot segmentation, they
often lose fine spatial precision at pixel level, due to global representation
bias. In contrast, diffusion-based models naturally encode fine-grained spatial
features via attention mechanisms that capture both global context and local
details. However, they often face challenges in balancing the number of
iterations with the quality of the segmentation. In this work, we propose
FastSeg, a novel and efficient training-free framework with only (1+1)-step of
reverse process of a pretrained diffusion model (e.g., Stable Diffusion).
Moreover, instead of running multiple times for different classes, FastSeg
performs segmentation for all classes at once. To further enhance the
segmentation quality, FastSeg introduces three key components: (i) a
dual-prompt mechanism for discriminative, class-aware attention extraction,
(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused
cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time
Flipping (TTF) scheme designed to improve spatial consistency. Extensive
experiments show that FastSeg achieves state-of-the-art training-free
performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,
and COCO Object benchmarks while maintaining superior inference efficiency. Our
results demonstrate that FastSeg provides a strong foundation for
extendability, bridging the gap between segmentation quality and inference
efficiency.

</details>


### [203] [IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering](https://arxiv.org/abs/2506.23329)
*Parker Liu,Chenxin Li,Zhengxin Li,Yipeng Wu,Wuyang Li,Zhiqin Yang,Zhenyuan Zhang,Yunlong Lin,Sirui Han,Brandon Y. Feng*

Main category: cs.CV

TL;DR: IR3D-Bench is a benchmark challenging VLMs to demonstrate scene understanding by actively recreating 3D structures from images using tools, moving beyond passive recognition.


<details>
  <summary>Details</summary>
Motivation: To assess if VLMs truly understand scenes by testing their ability to actively create rather than passively describe.

Method: Tasks VLAs with using programming and rendering tools to recreate 3D structures from images, grounded in analysis-by-synthesis.

Result: Initial experiments reveal limitations in visual precision, not basic tool usage, among state-of-the-art VLMs.

Conclusion: IR3D-Bench aims to advance tool-using VLAs for genuine scene understanding through creation, with released data and protocols for further study.

Abstract: Vision-language models (VLMs) excel at descriptive tasks, but whether they
truly understand scenes from visual observations remains uncertain. We
introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding
through active creation rather than passive recognition. Grounded in the
analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)
with actively using programming and rendering tools to recreate the underlying
3D structure of an input image, achieving agentic inverse rendering through
tool use. This "understanding-by-creating" approach probes the tool-using
generative capacity of VLAs, moving beyond the descriptive or conversational
capacity measured by traditional scene understanding benchmarks. We provide a
comprehensive suite of metrics to evaluate geometric accuracy, spatial
relations, appearance attributes, and overall plausibility. Initial experiments
on agentic inverse rendering powered by various state-of-the-art VLMs highlight
current limitations, particularly in visual precision rather than basic tool
usage. IR3D-Bench, including data and evaluation protocols, is released to
facilitate systematic study and development of tool-using VLAs towards genuine
scene understanding by creating.

</details>


### [204] [CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation](https://arxiv.org/abs/2506.23347)
*Yi Liu,Shengqian Li,Zuzeng Lin,Feng Wang,Si Liu*

Main category: cs.CV

TL;DR: CycleVAR introduces Softmax Relaxed Quantization and reformulates image translation as conditional autoregressive generation, outperforming state-of-the-art models like CycleGAN-Turbo.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional Vector Quantization in unsupervised image translation by enabling gradient flow and end-to-end optimization.

Method: Proposes Softmax Relaxed Quantization for continuous codebook selection and CycleVAR for image-conditional autoregressive generation with multi-scale prompts.

Result: CycleVAR achieves superior translation quality and faster inference, outperforming existing models in unsupervised scenarios.

Conclusion: CycleVAR advances unsupervised image translation by combining differentiable quantization and autoregressive generation, setting a new benchmark.

Abstract: The current conditional autoregressive image generation methods have shown
promising results, yet their potential remains largely unexplored in the
practical unsupervised image translation domain, which operates without
explicit cross-domain correspondences. A critical limitation stems from the
discrete quantization inherent in traditional Vector Quantization-based
frameworks, which disrupts gradient flow between the Variational Autoencoder
decoder and causal Transformer, impeding end-to-end optimization during
adversarial training in image space. To tackle this issue, we propose using
Softmax Relaxed Quantization, a novel approach that reformulates codebook
selection as a continuous probability mixing process via Softmax, thereby
preserving gradient propagation. Building upon this differentiable foundation,
we introduce CycleVAR, which reformulates image-to-image translation as
image-conditional visual autoregressive generation by injecting multi-scale
source image tokens as contextual prompts, analogous to prefix-based
conditioning in language models. CycleVAR exploits two modes to generate the
target image tokens, including (1) serial multi-step generation, enabling
iterative refinement across scales, and (2) parallel one-step generation
synthesizing all resolution outputs in a single forward pass. Experimental
findings indicate that the parallel one-step generation mode attains superior
translation quality with quicker inference speed than the serial multi-step
mode in unsupervised scenarios. Furthermore, both quantitative and qualitative
results indicate that CycleVAR surpasses previous state-of-the-art unsupervised
image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo.

</details>


### [205] [GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields](https://arxiv.org/abs/2506.23352)
*Shunsuke Yasuki,Taiki Miyanishi,Nakamasa Inoue,Shuhei Kurita,Koya Sakamoto,Daichi Azuma,Masato Taki,Yutaka Matsuo*

Main category: cs.CV

TL;DR: GeoProg3D is a visual programming framework for natural language-driven interactions with city-scale 3D scenes, combining geography-aware 3D language fields and specialized vision APIs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D language approaches lack scalability and compositional reasoning for large urban settings.

Method: GeoProg3D uses a Geography-aware City-scale 3D Language Field (GCLF) and Geographical Vision APIs (GV-APIs), powered by LLMs for dynamic task execution.

Result: GeoProg3D outperforms existing models on the GeoEval3D benchmark, excelling in tasks like grounding and spatial reasoning.

Conclusion: GeoProg3D is the first framework for compositional geographic reasoning in city-scale 3D environments via natural language.

Abstract: The advancement of 3D language fields has enabled intuitive interactions with
3D scenes via natural language. However, existing approaches are typically
limited to small-scale environments, lacking the scalability and compositional
reasoning capabilities necessary for large, complex urban settings. To overcome
these limitations, we propose GeoProg3D, a visual programming framework that
enables natural language-driven interactions with city-scale high-fidelity 3D
scenes. GeoProg3D consists of two key components: (i) a Geography-aware
City-scale 3D Language Field (GCLF) that leverages a memory-efficient
hierarchical 3D model to handle large-scale data, integrated with geographic
information for efficiently filtering vast urban spaces using directional cues,
distance measurements, elevation data, and landmark references; and (ii)
Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as
area segmentation and object detection. Our framework employs large language
models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate
GCLF, effectively supporting diverse geographic vision tasks. To assess
performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive
benchmark dataset containing 952 query-answer pairs across five challenging
tasks: grounding, spatial reasoning, comparison, counting, and measurement.
Experiments demonstrate that GeoProg3D significantly outperforms existing 3D
language fields and vision-language models across multiple tasks. To our
knowledge, GeoProg3D is the first framework enabling compositional geographic
reasoning in high-fidelity city-scale 3D environments via natural language. The
code is available at https://snskysk.github.io/GeoProg3D/.

</details>


### [206] [Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement](https://arxiv.org/abs/2506.23353)
*Siyuan Chai,Xiaodong Guo,Tong Liu*

Main category: cs.CV

TL;DR: A task-oriented infrared image enhancement method improves perception in autonomous driving by decomposing layers and extracting saliency information, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Infrared images aid autonomous driving in poor weather but suffer from low contrast, especially for non-heat-emitting objects, impacting downstream tasks. Enhancing contrast without noise amplification is challenging.

Method: Proposes layer decomposition to preserve dark region features and a morphological reconstruction-based saliency extraction method to enhance targets without noise.

Result: Improves image quality for object detection and semantic segmentation, outperforming state-of-the-art methods.

Conclusion: The method effectively enhances infrared images for high-level vision tasks, addressing contrast and noise challenges.

Abstract: Infrared image helps improve the perception capabilities of autonomous
driving in complex weather conditions such as fog, rain, and low light.
However, infrared image often suffers from low contrast, especially in
non-heat-emitting targets like bicycles, which significantly affects the
performance of downstream high-level vision tasks. Furthermore, achieving
contrast enhancement without amplifying noise and losing important information
remains a challenge. To address these challenges, we propose a task-oriented
infrared image enhancement method. Our approach consists of two key components:
layer decomposition and saliency information extraction. First, we design an
layer decomposition method for infrared images, which enhances scene details
while preserving dark region features, providing more features for subsequent
saliency information extraction. Then, we propose a morphological
reconstruction-based saliency extraction method that effectively extracts and
enhances target information without amplifying noise. Our method improves the
image quality for object detection and semantic segmentation tasks. Extensive
experiments demonstrate that our approach outperforms state-of-the-art methods.

</details>


### [207] [OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions](https://arxiv.org/abs/2506.23361)
*Yuanhao Cai,He Zhang,Xi Chen,Jinbo Xing,Yiwei Hu,Yuqian Zhou,Kai Zhang,Zhifei Zhang,Soo Ye Kim,Tianyu Wang,Yulun Zhang,Xiaokang Yang,Zhe Lin,Alan Yuille*

Main category: cs.CV

TL;DR: The paper introduces VideoCus-Factory for multi-subject video customization data construction and OmniVCus, a diffusion Transformer framework with innovative embedding mechanisms, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of multi-subject training data and unexplored control signals (e.g., depth, mask) in video customization.

Method: Proposes VideoCus-Factory for data construction and OmniVCus framework with Lottery Embedding (LE) and Temporally Aligned Embedding (TAE).

Result: Significantly surpasses state-of-the-art methods in quantitative and qualitative evaluations.

Conclusion: The approach enables effective multi-subject video customization and control, with promising results demonstrated.

Abstract: Existing feedforward subject-driven video customization methods mainly study
single-subject scenarios due to the difficulty of constructing multi-subject
training data pairs. Another challenging problem that how to use the signals
such as depth, mask, camera, and text prompts to control and edit the subject
in the customized video is still less explored. In this paper, we first propose
a data construction pipeline, VideoCus-Factory, to produce training data pairs
for multi-subject customization from raw videos without labels and control
signals such as depth-to-video and mask-to-video pairs. Based on our
constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with
image editing data to enable instructive editing for the subject in the
customized video. Then we propose a diffusion Transformer framework, OmniVCus,
with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned
Embedding (TAE). LE enables inference with more subjects by using the training
subjects to activate more frame embeddings. TAE encourages the generation
process to extract guidance from temporally aligned control signals by
assigning the same frame embeddings to the control and noise tokens.
Experiments demonstrate that our method significantly surpasses
state-of-the-art methods in both quantitative and qualitative evaluations.
Video demos are at our project page:
https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released
at https://github.com/caiyuanhao1998/Open-OmniVCus

</details>


### [208] [SIEDD: Shared-Implicit Encoder with Discrete Decoders](https://arxiv.org/abs/2506.23382)
*Vikram Rangarajan,Shishira Maiya,Max Ehrlich,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: SIEDD accelerates INR video encoding by 20-30X without sacrificing quality or coordinate-level control, using a shared encoder and discrete decoders.


<details>
  <summary>Details</summary>
Motivation: Slow encoding times of INRs hinder adoption, and existing acceleration methods compromise quality or control.

Method: SIEDD uses a shared encoder for global features and lightweight decoders for frame groups, with aggressive sampling.

Result: Achieves 20-30X speed-up on HD/4K benchmarks while maintaining quality and compression ratios.

Conclusion: SIEDD advances practical high-fidelity neural video compression, enabling real-world deployment.

Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video
compression by learning per-video optimized functions, but their adoption is
crippled by impractically slow encoding times. Existing attempts to accelerate
INR encoding often sacrifice reconstruction quality or crucial coordinate-level
control essential for adaptive streaming and transcoding. We introduce SIEDD
(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that
fundamentally accelerates INR encoding without these compromises. SIEDD first
rapidly trains a shared, coordinate-based encoder on sparse anchor frames to
efficiently capture global, low-frequency video features. This encoder is then
frozen, enabling massively parallel training of lightweight, discrete decoders
for individual frame groups, further expedited by aggressive coordinate-space
sampling. This synergistic design delivers a remarkable 20-30X encoding
speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while
maintaining competitive reconstruction quality and compression ratios.
Critically, SIEDD retains full coordinate-based control, enabling continuous
resolution decoding and eliminating costly transcoding. Our approach
significantly advances the practicality of high-fidelity neural video
compression, demonstrating a scalable and efficient path towards real-world
deployment. Our codebase is available at
https://github.com/VikramRangarajan/SIEDD .

</details>


### [209] [A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video](https://arxiv.org/abs/2506.23414)
*Ming-Zher Poh,Jonathan Wang,Jonathan Hsu,Lawrence Cai,Eric Teasley,James A. Taylor,Jameson K. Rogers,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: A high-throughput bench-testing platform for smartphone-based HR apps addresses device variability and performance evaluation challenges, achieving high accuracy and compatibility.


<details>
  <summary>Details</summary>
Motivation: Challenges in evaluating smartphone HR apps due to device variability and lack of standardized testing methods.

Method: Developed a system with a test rig for 12 smartphones, synthetic PPG video generation, and a host machine for coordination.

Result: Achieved 0.11% MAPE for HR accuracy and 0.92 correlation for PPG signals; all 20 tested smartphones met ANSI/CTA standards.

Conclusion: The platform provides a scalable solution for pre-deployment testing, improving app performance and compatibility in mobile health.

Abstract: Smartphone-based heart rate (HR) monitoring apps using finger-over-camera
photoplethysmography (PPG) face significant challenges in performance
evaluation and device compatibility due to device variability and
fragmentation. Manual testing is impractical, and standardized methods are
lacking. This paper presents a novel, high-throughput bench-testing platform to
address this critical need. We designed a system comprising a test rig capable
of holding 12 smartphones for parallel testing, a method for generating
synthetic PPG test videos with controllable HR and signal quality, and a host
machine for coordinating video playback and data logging. The system achieved a
mean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and
measured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and
measured PPG signals using a clinically-validated smartphone-based HR app.
Bench-testing results of 20 different smartphone models correctly classified
all the devices as meeting the ANSI/CTA accuracy standards for HR monitors
(MAPE <10%) when compared to a prospective clinical study with 80 participants,
demonstrating high positive predictive value. This platform offers a scalable
solution for pre-deployment testing of smartphone HR apps to improve app
performance, ensure device compatibility, and advance the field of mobile
health.

</details>


### [210] [Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models](https://arxiv.org/abs/2506.23418)
*Parham Rezaei,Arash Marioriyad,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: The paper addresses spatial misalignment in text-to-image models by introducing a probabilistic framework (PoS) and two contributions: PSE for evaluation and PSG for generation, improving spatial relationship accuracy.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with accurately representing spatial relationships in input prompts, leading to misaligned compositions.

Method: Proposes a probabilistic framework using Probability of Superiority (PoS), with PSE for evaluation and PSG for generation via gradient-based guidance or noise vector selection.

Result: PSE aligns better with human judgment than traditional metrics, and PSG outperforms state-of-the-art methods in generating spatially accurate images.

Conclusion: The PoS-based framework effectively improves spatial relationship accuracy in text-to-image models, validated by human-aligned metrics and superior performance.

Abstract: Despite the ability of text-to-image models to generate high-quality,
realistic, and diverse images, they face challenges in compositional
generation, often struggling to accurately represent details specified in the
input prompt. A prevalent issue in compositional generation is the misalignment
of spatial relationships, as models often fail to faithfully generate images
that reflect the spatial configurations specified between objects in the input
prompts. To address this challenge, we propose a novel probabilistic framework
for modeling the relative spatial positioning of objects in a scene, leveraging
the concept of Probability of Superiority (PoS). Building on this insight, we
make two key contributions. First, we introduce a novel evaluation metric,
PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D
spatial relationships between text and image, with improved adherence to human
judgment. Second, we propose PoS-based Generation (PSG), an inference-time
method that improves the alignment of 2D and 3D spatial relationships in T2I
models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based
reward function that can be utilized in two distinct ways: (1) as a
gradient-based guidance mechanism applied to the cross-attention maps during
the denoising steps, or (2) as a search-based strategy that evaluates a set of
initial noise vectors to select the best one. Extensive experiments demonstrate
that the PSE metric exhibits stronger alignment with human judgment compared to
traditional center-based metrics, providing a more nuanced and reliable measure
of complex spatial relationship accuracy in text-image alignment. Furthermore,
PSG significantly enhances the ability of text-to-image models to generate
images with specified spatial configurations, outperforming state-of-the-art
methods across multiple evaluation metrics and benchmarks.

</details>


### [211] [Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles](https://arxiv.org/abs/2506.23426)
*Menna Taha,Aya Ahmed,Mohammed Karmoose,Yasser Gadallah*

Main category: cs.CV

TL;DR: The paper proposes a novel object detection method for AVs that focuses on harmfulness rather than class-based classification to improve safety by detecting OOD objects.


<details>
  <summary>Details</summary>
Motivation: Conventional object detection fails with OOD objects, posing safety risks for AVs. The study aims to address this by prioritizing harmfulness over class.

Method: The approach classifies objects as 'harmful' or 'harmless' based on their position and trajectory relative to the AV, enabling detection of unseen objects.

Result: The model successfully detects OOD objects, assesses harmfulness, and enhances AV decision-making in dynamic environments.

Conclusion: The proposed method improves AV safety by effectively handling OOD objects through harmfulness-based classification.

Abstract: Autonomous vehicles (AVs) use object detection models to recognize their
surroundings and make driving decisions accordingly. Conventional object
detection approaches classify objects into known classes, which limits the AV's
ability to detect and appropriately respond to Out-of-Distribution (OOD)
objects. This problem is a significant safety concern since the AV may fail to
detect objects or misclassify them, which can potentially lead to hazardous
situations such as accidents. Consequently, we propose a novel object detection
approach that shifts the emphasis from conventional class-based classification
to object harmfulness determination. Instead of object detection by their
specific class, our method identifies them as either 'harmful' or 'harmless'
based on whether they pose a danger to the AV. This is done based on the object
position relative to the AV and its trajectory. With this metric, our model can
effectively detect previously unseen objects to enable the AV to make safer
real-time decisions. Our results demonstrate that the proposed model
effectively detects OOD objects, evaluates their harmfulness, and classifies
them accordingly, thus enhancing the AV decision-making effectiveness in
dynamic environments.

</details>


### [212] [Towards foundational LiDAR world models with efficient latent flow matching](https://arxiv.org/abs/2506.23434)
*Tianran Liu,Shengwen Zhao,Nicholas Rhinehart*

Main category: cs.CV

TL;DR: A study on LiDAR world models' transferability across domains, showing significant improvements with a pre-trained model and proposing a more efficient framework.


<details>
  <summary>Details</summary>
Motivation: To develop LiDAR world models with strong transferability across domains, reducing reliance on annotated data and addressing inefficiencies in current models.

Method: Conducted domain transfer studies across three scenarios, proposed a latent conditional flow matching (CFM)-based framework for efficiency.

Result: Achieved up to 11% absolute improvement, outperformed training from scratch in 30/36 comparisons, and reduced labeled data needs by 95%.

Conclusion: The proposed CFM-based framework enhances efficiency and performance, setting new benchmarks in semantic occupancy forecasting.

Abstract: LiDAR-based world models offer more structured and geometry-aware
representations than their image-based counterparts. However, existing LiDAR
world models are narrowly trained; each model excels only in the domain for
which it was built. Can we develop LiDAR world models that exhibit strong
transferability across multiple domains? We conduct the first systematic domain
transfer study across three demanding scenarios: (i) outdoor to indoor
generalization, (ii) sparse-beam \& dense-beam adaptation, and (iii)
non-semantic to semantic transfer. Given different amounts of fine-tuning data,
our experiments show that a single pre-trained model can achieve up to 11%
absolute improvement (83\% relative) over training from scratch and outperforms
training from scratch in 30/36 of our comparisons. This transferability of
dynamic learning significantly reduces the reliance on manually annotated data
for semantic occupancy forecasting: our method exceed the previous semantic
occupancy forecasting models with only 5% of the labeled training data required
by prior models. We also observed inefficiencies of current LiDAR world models,
mainly through their under-compression of LiDAR data and inefficient training
objectives. To address this, we propose a latent conditional flow matching
(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy
using only half the training data and a compression ratio 6 times higher than
that of prior methods. Our model achieves SOTA performance on
future-trajectory-conditioned semantic occupancy forecasting while being 23x
more computationally efficient (a 28x FPS speedup); and achieves SOTA
performance on semantic occupancy forecasting while being 2x more
computationally efficient (a 1.1x FPS speedup).

</details>


### [213] [PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions](https://arxiv.org/abs/2506.23440)
*Mahesh Bhosale,Abdul Wasi,Yuanhao Zhai,Yunjie Tian,Samuel Border,Nan Xi,Pinaki Sarder,Junsong Yuan,David Doermann,Xuan Gong*

Main category: cs.CV

TL;DR: PathDiff is a diffusion framework for generating histopathology images using unpaired mask-text data, improving control over semantics and spatial details.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in histopathology due to privacy constraints by leveraging diagnostic text and masks for enhanced image generation.

Method: Proposes PathDiff, a diffusion model integrating unpaired mask-text data into a unified conditioning space for precise control over image features.

Result: Generates high-quality, semantically accurate images, improving fidelity, text-image alignment, and downstream task performance.

Conclusion: PathDiff outperforms existing methods, offering a robust solution for histopathology image synthesis and data augmentation.

Abstract: Diffusion-based generative models have shown promise in synthesizing
histopathology images to address data scarcity caused by privacy constraints.
Diagnostic text reports provide high-level semantic descriptions, and masks
offer fine-grained spatial structures essential for representing distinct
morphological regions. However, public datasets lack paired text and mask data
for the same histopathological images, limiting their joint use in image
generation. This constraint restricts the ability to fully exploit the benefits
of combining both modalities for enhanced control over semantics and spatial
details. To overcome this, we propose PathDiff, a diffusion framework that
effectively learns from unpaired mask-text data by integrating both modalities
into a unified conditioning space. PathDiff allows precise control over
structural and contextual features, generating high-quality, semantically
accurate images. PathDiff also improves image fidelity, text-image alignment,
and faithfulness, enhancing data augmentation for downstream tasks like nuclei
segmentation and classification. Extensive experiments demonstrate its
superiority over existing methods.

</details>


### [214] [Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23460)
*Dewen Zeng,Xinrong Hu,Yu-Jen Chen,Yawen Wu,Xiaowei Xu,Yiyu Shi*

Main category: cs.CV

TL;DR: The paper introduces CLDF, a method using contrastive learning with diffusion features to improve weakly supervised semantic segmentation by addressing noise in CDM-generated saliency maps.


<details>
  <summary>Details</summary>
Motivation: Traditional CAM-based methods for WSSS suffer from partial activations and imprecise boundaries, while CDM-based methods introduce noise from background alterations.

Method: CLDF integrates gradient maps from CDM with CAMs for contrastive learning, training a pixel decoder to map diffusion features for segmentation.

Result: Experiments on four tasks from two medical datasets show CLDF outperforms existing baselines.

Conclusion: CLDF effectively reduces noise and improves segmentation accuracy in WSSS by leveraging contrastive learning with diffusion features.

Abstract: Weakly supervised semantic segmentation (WSSS) methods using class labels
often rely on class activation maps (CAMs) to localize objects. However,
traditional CAM-based methods struggle with partial activations and imprecise
object boundaries due to optimization discrepancies between classification and
segmentation. Recently, the conditional diffusion model (CDM) has been used as
an alternative for generating segmentation masks in WSSS, leveraging its strong
image generation capabilities tailored to specific class distributions. By
modifying or perturbing the condition during diffusion sampling, the related
objects can be highlighted in the generated images. Yet, the saliency maps
generated by CDMs are prone to noise from background alterations during reverse
diffusion. To alleviate the problem, we introduce Contrastive Learning with
Diffusion Features (CLDF), a novel method that uses contrastive learning to
train a pixel decoder to map the diffusion features from a frozen CDM to a
low-dimensional embedding space for segmentation. Specifically, we integrate
gradient maps generated from CDM external classifier with CAMs to identify
foreground and background pixels with fewer false positives/negatives for
contrastive learning, enabling robust pixel embedding learning. Experimental
results on four segmentation tasks from two public medical datasets demonstrate
that our method significantly outperforms existing baselines.

</details>


### [215] [Time-variant Image Inpainting via Interactive Distribution Transition Estimation](https://arxiv.org/abs/2506.23461)
*Yun Xing,Qing Guo,Xiaoguang Li,Yihao Huang,Xiaofeng Cao,Di Lin,Ivor Tsang,Lei Ma*

Main category: cs.CV

TL;DR: The paper introduces Time-vAriant iMage inPainting (TAMP), a task to restore damaged images using time-variant reference images, and proposes the InDiTE-Diff method to address it.


<details>
  <summary>Details</summary>
Motivation: Restoring damaged images with time-variant references is practical but challenging due to content differences and potential reference damage. Existing methods fail in this scenario.

Method: Proposes InDiTE module for adaptive semantics and InDiTE-Diff, integrating InDiTE with a diffusion model for latent cross-reference. Introduces the TAMP-Street dataset.

Result: InDiTE-Diff outperforms SOTA methods on the TAMP-Street dataset in time-variant image inpainting.

Conclusion: The proposed method effectively addresses TAMP, demonstrating superior performance over existing approaches.

Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant
iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image
by leveraging the complementary information from a reference image, where both
images captured the same scene but with a significant time gap in between,
i.e., time-variant images. Different from conventional reference-guided image
inpainting, the reference image under TAMP setup presents significant content
distinction to the target image and potentially also suffers from damages. Such
an application frequently happens in our daily lives to restore a damaged image
by referring to another reference image, where there is no guarantee of the
reference image's source and quality. In particular, our study finds that even
state-of-the-art (SOTA) reference-guided image inpainting methods fail to
achieve plausible results due to the chaotic image complementation. To address
such an ill-posed problem, we propose a novel Interactive Distribution
Transition Estimation (InDiTE) module which interactively complements the
time-variant images with adaptive semantics thus facilitate the restoration of
damaged regions. To further boost the performance, we propose our TAMP
solution, namely Interactive Distribution Transition Estimation-driven
Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and
conducts latent cross-reference during sampling. Moreover, considering the lack
of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,
based on existing image and mask datasets. We conduct experiments on the
TAMP-Street datasets under two different time-variant image inpainting
settings, which show our method consistently outperform SOTA reference-guided
image inpainting methods for solving TAMP.

</details>


### [216] [Sanitizing Manufacturing Dataset Labels Using Vision-Language Models](https://arxiv.org/abs/2506.23465)
*Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: The paper introduces VLSR, a vision-language framework for improving label quality in noisy manufacturing datasets by leveraging CLIP embeddings for sanitization and clustering.


<details>
  <summary>Details</summary>
Motivation: High-quality labels are costly in manufacturing, and noisy datasets hinder model performance. VLSR aims to automate label refinement with minimal human effort.

Method: VLSR uses CLIP embeddings to compute cosine similarity for label sanitization and density-based clustering for grouping similar labels.

Result: VLSR effectively identifies and fixes problematic labels, reducing vocabulary size and improving dataset quality.

Conclusion: VLSR enhances label consistency and dataset quality for industrial ML applications with minimal human intervention.

Abstract: The success of machine learning models in industrial applications is heavily
dependent on the quality of the datasets used to train the models. However,
large-scale datasets, specially those constructed from crowd-sourcing and
web-scraping, often suffer from label noise, inconsistencies, and errors. This
problem is particularly pronounced in manufacturing domains, where obtaining
high-quality labels is costly and time-consuming. This paper introduces
Vision-Language Sanitization and Refinement (VLSR), which is a
vision-language-based framework for label sanitization and refinement in
multi-label manufacturing image datasets. This method embeds both images and
their associated textual labels into a shared semantic space leveraging the
CLIP vision-language model. Then two key tasks are addressed in this process by
computing the cosine similarity between embeddings. First, label sanitization
is performed to identify irrelevant, misspelled, or semantically weak labels,
and surface the most semantically aligned label for each image by comparing
image-label pairs using cosine similarity between image and label embeddings.
Second, the method applies density-based clustering on text embeddings,
followed by iterative cluster merging, to group semantically similar labels
into unified label groups. The Factorynet dataset, which includes noisy labels
from both human annotations and web-scraped sources, is employed to evaluate
the effectiveness of the proposed framework. Experimental results demonstrate
that the VLSR framework successfully identifies problematic labels and improves
label consistency. This method enables a significant reduction in label
vocabulary through clustering, which ultimately enhances the dataset's quality
for training robust machine learning models in industrial applications with
minimal human intervention.

</details>


### [217] [AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays](https://arxiv.org/abs/2506.23467)
*Chenlang Yi,Zizhan Xiong,Qi Qi,Xiyuan Wei,Girish Bathla,Ching-Long Lin,Bobak Jack Mortazavi,Tianbao Yang*

Main category: cs.CV

TL;DR: AdFair-CLIP improves fairness and accuracy in CLIP-based medical image classification by suppressing demographic biases using adversarial feature intervention.


<details>
  <summary>Details</summary>
Motivation: Address fairness concerns and demographic biases in CLIP models, which cause disparities in diagnostic outcomes for underrepresented groups.

Method: Introduces AdFair-CLIP, a framework using adversarial feature intervention to mitigate spurious correlations and improve fairness.

Result: Significantly enhances fairness and diagnostic accuracy in chest X-ray datasets, with robust generalization in zero-shot and few-shot scenarios.

Conclusion: AdFair-CLIP sets new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, especially for CXR analysis.

Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated
superior performance across various visual tasks including medical image
classification. However, fairness concerns, including demographic biases, have
received limited attention for CLIP models. This oversight leads to critical
issues, particularly those related to race and gender, resulting in disparities
in diagnostic outcomes and reduced reliability for underrepresented groups. To
address these challenges, we introduce AdFair-CLIP, a novel framework employing
adversarial feature intervention to suppress sensitive attributes, thereby
mitigating spurious correlations and improving prediction fairness. We conduct
comprehensive experiments on chest X-ray (CXR) datasets, and show that
AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while
maintaining robust generalization in zero-shot and few-shot scenarios. These
results establish new benchmarks for fairness-aware learning in CLIP-based
medical diagnostic models, particularly for CXR analysis.

</details>


### [218] [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/abs/2506.23468)
*Xuan Yao,Junyu Gao,Changsheng Xu*

Main category: cs.CV

TL;DR: NavMorph is a self-evolving world model framework for VLN-CE tasks, improving navigation by using latent representations and contextual memory.


<details>
  <summary>Details</summary>
Motivation: Current VLN-CE approaches struggle with generalization and adaptability in novel environments.

Method: NavMorph employs compact latent representations and a Contextual Evolution Memory for adaptive planning and policy refinement.

Result: Achieves notable performance improvements on VLN-CE benchmarks.

Conclusion: NavMorph enhances environmental understanding and decision-making in VLN-CE tasks.

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires
agents to execute sequential navigation actions in complex environments guided
by natural language instructions. Current approaches often struggle with
generalizing to novel environments and adapting to ongoing changes during
navigation. Inspired by human cognition, we present NavMorph, a self-evolving
world model framework that enhances environmental understanding and
decision-making in VLN-CE tasks. NavMorph employs compact latent
representations to model environmental dynamics, equipping agents with
foresight for adaptive planning and policy refinement. By integrating a novel
Contextual Evolution Memory, NavMorph leverages scene-contextual information to
support effective navigation while maintaining online adaptability. Extensive
experiments demonstrate that our method achieves notable performance
improvements on popular VLN-CE benchmarks. Code is available at
\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.

</details>


### [219] [Interactive Interface For Semantic Segmentation Dataset Synthesis](https://arxiv.org/abs/2506.23470)
*Ngoc-Do Tran,Minh-Tuan Huynh,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: SynthLab is a modular platform for synthesizing visual data, addressing the resource-intensive and privacy concerns of creating annotated datasets for semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: High-quality annotated datasets for semantic segmentation are costly and raise privacy issues, prompting the need for an efficient alternative.

Method: SynthLab offers a modular architecture for data synthesis and a user-friendly interface, enabling easy customization and integration.

Result: User studies show SynthLab is flexible, accessible, and usable by non-experts for real-world AI applications.

Conclusion: SynthLab provides a scalable, privacy-conscious solution for generating annotated datasets, democratizing AI tool access.

Abstract: The rapid advancement of AI and computer vision has significantly increased
the demand for high-quality annotated datasets, particularly for semantic
segmentation. However, creating such datasets is resource-intensive, requiring
substantial time, labor, and financial investment, and often raises privacy
concerns due to the use of real-world data. To mitigate these challenges, we
present SynthLab, consisting of a modular platform for visual data synthesis
and a user-friendly interface. The modular architecture of SynthLab enables
easy maintenance, scalability with centralized updates, and seamless
integration of new features. Each module handles distinct aspects of computer
vision tasks, enhancing flexibility and adaptability. Meanwhile, its
interactive, user-friendly interface allows users to quickly customize their
data pipelines through drag-and-drop actions. Extensive user studies involving
a diverse range of users across different ages, professions, and expertise
levels, have demonstrated flexible usage, and high accessibility of SynthLab,
enabling users without deep technical expertise to harness AI for real-world
applications.

</details>


### [220] [GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance](https://arxiv.org/abs/2506.23478)
*Pedro Alonso,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: GeoCD improves 3D point cloud learning by replacing Chamfer Distance (CD) with a topology-aware geodesic approximation, enhancing reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Chamfer Distance (CD) relies on Euclidean distances, failing to capture intrinsic 3D geometry. GeoCD addresses this limitation.

Method: Propose GeoCD, a topology-aware, differentiable geodesic distance metric. Fine-tune models trained with CD using GeoCD.

Result: GeoCD consistently outperforms CD, with significant gains in reconstruction quality after just one epoch of fine-tuning.

Conclusion: GeoCD is a superior metric for 3D point cloud learning, offering better geometry capture and performance.

Abstract: Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning
due to its simplicity and efficiency. However, it suffers from a fundamental
limitation: it relies solely on Euclidean distances, which often fail to
capture the intrinsic geometry of 3D shapes. To address this limitation, we
propose GeoCD, a topology-aware and fully differentiable approximation of
geodesic distance designed to serve as a metric for 3D point cloud learning.
Our experiments show that GeoCD consistently improves reconstruction quality
over standard CD across various architectures and datasets. We demonstrate this
by fine-tuning several models, initially trained with standard CD, using GeoCD.
Remarkably, fine-tuning for a single epoch with GeoCD yields significant gains
across multiple evaluation metrics.

</details>


### [221] [Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting](https://arxiv.org/abs/2506.23479)
*Zhaojie Zeng,Yuesong Wang,Chao Yang,Tao Guan,Lili Ju*

Main category: cs.CV

TL;DR: Proposes a faster, adaptive 2D Gaussian Splatting method for image representation, reducing training time while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Addresses the high GPU resource demand and slow training of Gaussian Splatting in INR, and its lack of adaptability to varying image complexity.

Method: Uses a network for coarse Gaussian representation generation, followed by minimal fine-tuning, and dynamically adjusts Gaussian points based on image complexity.

Result: Achieves comparable or better rendering quality than GaussianImage with significantly reduced training time (up to 10x faster).

Conclusion: The method offers a practical, efficient solution for image representation, balancing quality and computational cost.

Abstract: Implicit Neural Representation (INR) has demonstrated remarkable advances in
the field of image representation but demands substantial GPU resources.
GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this
cost, however, the slow training process limits its practicality, and the fixed
number of Gaussians per image limits its adaptability to varying information
entropy. To address these issues, we propose in this paper a generalizable and
self-adaptive image representation framework based on 2D Gaussian Splatting.
Our method employs a network to quickly generate a coarse Gaussian
representation, followed by minimal fine-tuning steps, achieving comparable
rendering quality of GaussianImage while significantly reducing training time.
Moreover, our approach dynamically adjusts the number of Gaussian points based
on image complexity to further enhance flexibility and efficiency in practice.
Experiments on DIV2K and Kodak datasets show that our method matches or exceeds
GaussianImage's rendering performance with far fewer iterations and shorter
training times. Specifically, our method reduces the training time by up to one
order of magnitude while achieving superior rendering performance with the same
number of Gaussians.

</details>


### [222] [Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks](https://arxiv.org/abs/2506.23481)
*Xian Zhang,Xiang Cheng*

Main category: cs.CV

TL;DR: The study analyzes MLLMs' ability to geolocate images, highlighting privacy risks and evaluating performance, with 49% accuracy within 1km.


<details>
  <summary>Details</summary>
Motivation: Address privacy and ethical concerns raised by MLLMs' geolocation capabilities, which can lead to risks like doxxing and surveillance.

Method: Comprehensive analysis of geolocation techniques using MLLMs, reviewing literature and evaluating state-of-the-art models on street view imagery.

Result: Advanced models achieve 49% accuracy in localizing street-level imagery within a 1km radius, leveraging fine-grained geographic cues.

Conclusion: Identifies key visual elements for geolocation and discusses privacy implications, proposing technical and policy countermeasures.

Abstract: Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)
has significantly enhanced their reasoning capabilities, enabling a wide range
of intelligent applications. However, these advancements also raise critical
concerns regarding privacy and ethics. MLLMs are now capable of inferring the
geographic location of images -- such as those shared on social media or
captured from street views -- based solely on visual content, thereby posing
serious risks of privacy invasion, including doxxing, surveillance, and other
security threats.
  Methods: This study provides a comprehensive analysis of existing geolocation
techniques based on MLLMs. It systematically reviews relevant litera-ture and
evaluates the performance of state-of-the-art visual reasoning models on
geolocation tasks, particularly in identifying the origins of street view
imagery.
  Results: Empirical evaluation reveals that the most advanced visual large
models can successfully localize the origin of street-level imagery with up to
$49\%$ accuracy within a 1-kilometer radius. This performance underscores the
models' powerful capacity to extract and utilize fine-grained geographic cues
from visual data.
  Conclusions: Building on these findings, the study identifies key visual
elements that contribute to suc-cessful geolocation, such as text,
architectural styles, and environmental features. Furthermore, it discusses the
potential privacy implications associated with MLLM-enabled geolocation and
discuss several technical and policy-based coun-termeasures to mitigate
associated risks. Our code and dataset are available at
https://github.com/zxyl1003/MLLM-Geolocation-Evaluation.

</details>


### [223] [MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting](https://arxiv.org/abs/2506.23482)
*Jun Huang,Ting Liu,Yihang Wu,Xiaochao Qu,Luoqi Liu,Xiaolin Hu*

Main category: cs.CV

TL;DR: MTADiffusion is a Mask-Text Alignment diffusion model for object inpainting, addressing semantic misalignment, structural distortion, and style inconsistency with MTAPipeline, MTADataset, multi-task training, and style-consistency loss.


<details>
  <summary>Details</summary>
Motivation: Existing inpainting methods suffer from semantic misalignment, structural distortion, and style inconsistency, limiting their effectiveness.

Method: Introduces MTAPipeline for mask-text annotation, constructs MTADataset, employs multi-task training (inpainting and edge prediction), and uses a style-consistency loss with a VGG network and Gram matrix.

Result: MTADiffusion achieves state-of-the-art performance on BrushBench and EditBench benchmarks.

Conclusion: MTADiffusion effectively addresses key challenges in image inpainting, outperforming existing methods.

Abstract: Advancements in generative models have enabled image inpainting models to
generate content within specific regions of an image based on provided prompts
and masks. However, existing inpainting methods often suffer from problems such
as semantic misalignment, structural distortion, and style inconsistency. In
this work, we present MTADiffusion, a Mask-Text Alignment diffusion model
designed for object inpainting. To enhance the semantic capabilities of the
inpainting model, we introduce MTAPipeline, an automatic solution for
annotating masks with detailed descriptions. Based on the MTAPipeline, we
construct a new MTADataset comprising 5 million images and 25 million mask-text
pairs. Furthermore, we propose a multi-task training strategy that integrates
both inpainting and edge prediction tasks to improve structural stability. To
promote style consistency, we present a novel inpainting style-consistency loss
using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations
on BrushBench and EditBench demonstrate that MTADiffusion achieves
state-of-the-art performance compared to other methods.

</details>


### [224] [Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding](https://arxiv.org/abs/2506.23491)
*ZongHan Hsieh,Tzer-Jen Wei*

Main category: cs.CV

TL;DR: Qwen-GUI-3B is a lightweight Vision-Language Model for GUI grounding, achieving competitive performance with larger models while being trainable on a single GPU. Key innovations include cross-platform dataset integration, two-stage fine-tuning, and data redundancy reduction.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of large-scale VLMs for consumer-grade hardware by developing a lightweight yet accurate model for GUI grounding tasks.

Method: Combines a diverse dataset of 24K GUI screenshots, employs a two-stage fine-tuning strategy, and uses data curation to reduce redundancy.

Result: Achieves 84.9% on ScreenSpot and 86.4% on ScreenSpot-v2, outperforming models under 4B parameters.

Conclusion: Qwen-GUI-3B demonstrates that lightweight models can achieve high accuracy in GUI grounding through strategic dataset and training innovations.

Abstract: This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)
specifically designed for Graphical User Interface grounding tasks, achieving
performance competitive with significantly larger models. Unlike large-scale
VLMs (>7B parameters) that are computationally intensive and impractical for
consumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while
being fully trainable on a single GPU (RTX 4090). The model incorporates
several key innovations: (i) combine cross-platform, multi-resolution dataset
of 24K examples from diverse sources including mobile, desktop, and web GUI
screenshots to effectively address data scarcity in high-resolution desktop
environments; (ii) a two-stage fine-tuning strategy, where initial
cross-platform training establishes robust GUI understanding, followed by
specialized fine-tuning on high-resolution data to significantly enhance model
adaptability; and (iii) data curation and redundancy reduction strategies,
demonstrating that randomly sampling a smaller subset with reduced redundancy
achieves performance comparable to larger datasets, emphasizing data diversity
over sheer volume. Empirical evaluation on standard GUI grounding
benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging
ScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%
on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B
parameters. Ablation studies validate the critical role of balanced sampling
and two-stage fine-tuning in enhancing robustness, particularly in
high-resolution desktop scenarios. The Qwen-GUI-3B is available at:
https://github.com/Han1018/Qwen-GUI-3B

</details>


### [225] [LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching](https://arxiv.org/abs/2506.23502)
*Mengxiao Tian,Xinxiao Wu,Shuo Yang*

Main category: cs.CV

TL;DR: The paper proposes an LLM-enhanced action-aware multi-modal prompt-tuning method to improve CLIP's fine-grained action-level understanding in image-text matching.


<details>
  <summary>Details</summary>
Motivation: CLIP lacks fine-grained understanding of object attributes, spatial relationships, and actions. Recent methods focus on object-level alignment but miss action perception, which is crucial for describing object states or relationships.

Method: Introduces action triplet and action state prompts using LLM-generated knowledge. An adaptive interaction module aggregates visual features for action-aware representations.

Result: Demonstrates improved performance on benchmark datasets.

Conclusion: The method effectively enhances CLIP's action-level understanding and representation learning.

Abstract: Driven by large-scale contrastive vision-language pre-trained models such as
CLIP, recent advancements in the image-text matching task have achieved
remarkable success in representation learning. Due to image-level
visual-language alignment, CLIP falls short in understanding fine-grained
details such as object attributes and spatial relationships between objects.
Recent efforts have attempted to compel CLIP to acquire structured visual
representations by introducing prompt learning to achieve object-level
alignment. While achieving promising results, they still lack the capability to
perceive actions, which are crucial for describing the states or relationships
between objects. Therefore, we propose to endow CLIP with fine-grained
action-level understanding by introducing an LLM-enhanced action-aware
multi-modal prompt-tuning method, incorporating the action-related external
knowledge generated by large language models (LLMs). Specifically, we design an
action triplet prompt and an action state prompt to exploit compositional
semantic knowledge and state-related causal knowledge implicitly stored in
LLMs. Subsequently, we propose an adaptive interaction module to aggregate
attentive visual features conditioned on action-aware prompted knowledge for
establishing discriminative and action-aware visual representations, which
further improves the performance. Comprehensive experimental results on two
benchmark datasets demonstrate the effectiveness of our method.

</details>


### [226] [Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation](https://arxiv.org/abs/2506.23505)
*Tinh Nguyen*

Main category: cs.CV

TL;DR: The paper introduces YOLOv12 with physics-informed augmentations for underwater object detection, achieving high accuracy and speed while addressing challenges like turbidity and occlusion.


<details>
  <summary>Details</summary>
Motivation: Underwater object detection is hindered by poor visibility, turbidity, and occlusion, limiting real-time deployment of current methods.

Method: Combines YOLOv12 with Residual ELAN blocks and Area Attention, along with domain-specific augmentations like turbulence blurring and spectral HSV transformations.

Result: Achieves 98.30% mAP at 142 FPS, with 18.9% better occlusion robustness, 22.4% higher small-object recall, and up to 7.94% improved precision.

Conclusion: The proposed method provides an efficient and precise solution for underwater detection, validated by ablation studies.

Abstract: Underwater object detection is crucial for autonomous navigation,
environmental monitoring, and marine exploration, but it is severely hampered
by light attenuation, turbidity, and occlusion. Current methods balance
accuracy and computational efficiency, but they have trouble deploying in
real-time under low visibility conditions. Through the integration of
physics-informed augmentation techniques with the YOLOv12 architecture, this
study advances underwater detection. With Residual ELAN blocks to preserve
structural features in turbid waters and Area Attention to maintain large
receptive fields for occluded objects while reducing computational complexity.
Underwater optical properties are addressed by domain-specific augmentations
such as turbulence adaptive blurring, biologically grounded occlusion
simulation, and spectral HSV transformations for color distortion. Extensive
tests on four difficult datasets show state-of-the-art performance, with
Brackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion
robustness by 18.9%, small-object recall by 22.4%, and detection precision by
up to 7.94% compared to previous models. The crucial role of augmentation
strategy is validated by ablation studies. This work offers a precise and
effective solution for conservation and underwater robotics applications.

</details>


### [227] [ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models](https://arxiv.org/abs/2506.23513)
*Zixun Fang,Kai Zhu,Zhiheng Liu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: A novel framework for generating high-quality panoramic videos by leveraging pretrained perspective video models, addressing the modality gap between panoramic and perspective data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with synthesizing high-quality panoramic videos due to the modality gap between panoramic and perspective data, which is prevalent in training datasets for diffusion models.

Method: Proposes a ViewPoint map for panorama representation and a Pano-Perspective attention mechanism to utilize pretrained perspective priors and capture panoramic spatial correlations.

Result: The method synthesizes highly dynamic and spatially consistent panoramic videos, achieving state-of-the-art performance.

Conclusion: The proposed framework effectively bridges the modality gap and outperforms previous methods in panoramic video generation.

Abstract: Panoramic video generation aims to synthesize 360-degree immersive videos,
holding significant importance in the fields of VR, world models, and spatial
intelligence. Existing works fail to synthesize high-quality panoramic videos
due to the inherent modality gap between panoramic data and perspective data,
which constitutes the majority of the training data for modern diffusion
models. In this paper, we propose a novel framework utilizing pretrained
perspective video models for generating panoramic videos. Specifically, we
design a novel panorama representation named ViewPoint map, which possesses
global spatial continuity and fine-grained visual details simultaneously. With
our proposed Pano-Perspective attention mechanism, the model benefits from
pretrained perspective priors and captures the panoramic spatial correlations
of the ViewPoint map effectively. Extensive experiments demonstrate that our
method can synthesize highly dynamic and spatially consistent panoramic videos,
achieving state-of-the-art performance and surpassing previous methods.

</details>


### [228] [WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image](https://arxiv.org/abs/2506.23518)
*Jiwoo Park,Tae Eun Choi,Youngjun Jun,Seong Jae Hwang*

Main category: cs.CV

TL;DR: A method to improve view consistency in novel view synthesis using diffusion models without additional modules, leveraging adaptive attention manipulation and noise reinitialization.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with spatial continuity in novel view synthesis, and hybrid approaches with 3D models are inefficient.

Method: Training-free adaptive attention manipulation and noise reinitialization using view-guided warping.

Result: Improved view consistency across various diffusion models, validated by a comprehensive metric framework.

Conclusion: The proposed method enhances view consistency efficiently without complex pipelines, demonstrating broad applicability.

Abstract: Generating high-quality novel views of a scene from a single image requires
maintaining structural coherence across different views, referred to as view
consistency. While diffusion models have driven advancements in novel view
synthesis, they still struggle to preserve spatial continuity across views.
Diffusion models have been combined with 3D models to address the issue, but
such approaches lack efficiency due to their complex multi-step pipelines. This
paper proposes a novel view-consistent image generation method which utilizes
diffusion models without additional modules. Our key idea is to enhance
diffusion models with a training-free method that enables adaptive attention
manipulation and noise reinitialization by leveraging view-guided warping to
ensure view consistency. Through our comprehensive metric framework suitable
for novel-view datasets, we show that our method improves view consistency
across various diffusion models, demonstrating its broader applicability.

</details>


### [229] [From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection](https://arxiv.org/abs/2506.23519)
*Qi Qin,Runmin Cong,Gen Zhan,Yiting Liao,Sam Kwong*

Main category: cs.CV

TL;DR: The paper introduces fixation information to improve video salient object detection (VSOD) under weak supervision, proposing modules for better feature learning and spatiotemporal modeling, achieving superior results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Eye-tracking annotations are more accessible and align with human visual patterns, making fixation information valuable for weakly supervised VSOD.

Method: Proposes a Position and Semantic Embedding (PSE) module for guidance, and a Semantics and Locality Query (SLQ) Competitor and Intra-Inter Mixed Contrastive (IIMC) model for spatiotemporal feature modeling.

Result: Outperforms other models on five VSOD benchmarks across various metrics.

Conclusion: The approach effectively leverages fixation information and weak supervision to enhance VSOD performance.

Abstract: The eye-tracking video saliency prediction (VSP) task and video salient
object detection (VSOD) task both focus on the most attractive objects in video
and show the result in the form of predictive heatmaps and pixel-level saliency
masks, respectively. In practical applications, eye tracker annotations are
more readily obtainable and align closely with the authentic visual patterns of
human eyes. Therefore, this paper aims to introduce fixation information to
assist the detection of video salient objects under weak supervision. On the
one hand, we ponder how to better explore and utilize the information provided
by fixation, and then propose a Position and Semantic Embedding (PSE) module to
provide location and semantic guidance during the feature learning process. On
the other hand, we achieve spatiotemporal feature modeling under weak
supervision from the aspects of feature selection and feature contrast. A
Semantics and Locality Query (SLQ) Competitor with semantic and locality
constraints is designed to effectively select the most matching and accurate
object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed
Contrastive (IIMC) model improves the spatiotemporal modeling capabilities
under weak supervision by forming an intra-video and inter-video contrastive
learning paradigm. Experimental results on five popular VSOD benchmarks
indicate that our model outperforms other competitors on various evaluation
metrics.

</details>


### [230] [Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving](https://arxiv.org/abs/2506.23523)
*Tuong Do,Binh X. Nguyen,Quang D. Tran,Erman Tjiputra,Te-Chuan Chiu,Anh Nguyen*

Main category: cs.CV

TL;DR: A lightweight temporal transformer decomposition method is proposed to enhance autonomous driving by efficiently processing sequential image frames and steering data, outperforming existing methods in real-time performance.


<details>
  <summary>Details</summary>
Motivation: Traditional vision-based autonomous driving struggles with complex environments using single-image inputs. Temporal data improves robustness but existing methods are resource-heavy and impractical for federated learning.

Method: Lightweight temporal transformer decomposition breaks down large attention maps into smaller matrices, reducing model complexity for efficient training and real-time predictions.

Result: Outperforms recent methods on three datasets and achieves real-time performance, confirmed by real robot experiments.

Conclusion: The proposed method effectively enhances autonomous driving by leveraging temporal data efficiently, offering practical advantages over existing approaches.

Abstract: Traditional vision-based autonomous driving systems often face difficulties
in navigating complex environments when relying solely on single-image inputs.
To overcome this limitation, incorporating temporal data such as past image
frames or steering sequences, has proven effective in enhancing robustness and
adaptability in challenging scenarios. While previous high-performance methods
exist, they often rely on resource-intensive fusion networks, making them
impractical for training and unsuitable for federated learning. To address
these challenges, we propose lightweight temporal transformer decomposition, a
method that processes sequential image frames and temporal steering data by
breaking down large attention maps into smaller matrices. This approach reduces
model complexity, enabling efficient weight updates for convergence and
real-time predictions while leveraging temporal information to enhance
autonomous driving performance. Intensive experiments on three datasets
demonstrate that our method outperforms recent approaches by a clear margin
while achieving real-time performance. Additionally, real robot experiments
further confirm the effectiveness of our method.

</details>


### [231] [When Test-Time Adaptation Meets Self-Supervised Models](https://arxiv.org/abs/2506.23529)
*Jisu Han,Jihee Park,Dongyoon Han,Wonjun Hwang*

Main category: cs.CV

TL;DR: The paper explores test-time adaptation (TTA) for self-supervised learning (SSL) models, proposing a collaborative framework to improve performance without relying on source pretraining.


<details>
  <summary>Details</summary>
Motivation: To enhance the adaptability of SSL models in dynamic environments without dependence on source pretraining.

Method: A collaborative learning framework integrating SSL and TTA, using contrastive learning and knowledge distillation for representation refinement.

Result: The method achieves competitive performance on diverse SSL models (DINO, MoCo, iBOT) across TTA benchmarks.

Conclusion: The proposed framework effectively improves SSL models' adaptability without source pretraining, demonstrating practical applicability.

Abstract: Training on test-time data enables deep learning models to adapt to dynamic
environmental changes, enhancing their practical applicability. Online
adaptation from source to target domains is promising but it remains highly
reliant on the performance of source pretrained model. In this paper, we
investigate whether test-time adaptation (TTA) methods can continuously improve
models trained via self-supervised learning (SSL) without relying on source
pretraining. We introduce a self-supervised TTA protocol after observing that
existing TTA approaches struggle when directly applied to self-supervised
models with low accuracy on the source domain. Furthermore, we propose a
collaborative learning framework that integrates SSL and TTA models, leveraging
contrastive learning and knowledge distillation for stepwise representation
refinement. We validate our method on diverse self-supervised models, including
DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the
effectiveness of our approach in SSL, showing that it achieves competitive
performance even without source pretraining.

</details>


### [232] [GViT: Representing Images as Gaussians for Visual Recognition](https://arxiv.org/abs/2506.23532)
*Jefferson Hernandez,Ruozhen He,Guha Balakrishnan,Alexander C. Berg,Vicente Ordonez*

Main category: cs.CV

TL;DR: GVIT replaces pixel/patch grids with learnable 2D Gaussians for image classification, achieving competitive accuracy (76.9% top-1 on Imagenet-1k) with a ViT-B architecture.


<details>
  <summary>Details</summary>
Motivation: To explore alternative input representations beyond traditional pixel or patch grids for image classification, aiming for compact and efficient encoding.

Method: Encodes images as a few hundred 2D Gaussians, optimizing their parameters (position, scale, orientation, color, opacity) jointly with a ViT classifier. Uses classifier gradients to guide Gaussians toward class-salient regions and a differentiable renderer for reconstruction.

Result: GVIT matches the performance of patch-based ViT, achieving 76.9% top-1 accuracy on Imagenet-1k with ViT-B.

Conclusion: GVIT demonstrates that 2D Gaussian representations can effectively replace traditional grid-based inputs in ViT classifiers without sacrificing performance.

Abstract: We introduce GVIT, a classification framework that abandons conventional
pixel or patch grid input representations in favor of a compact set of
learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose
positions, scales, orientations, colors, and opacities are optimized jointly
with a ViT classifier trained on top of these representations. We reuse the
classifier gradients as constructive guidance, steering the Gaussians toward
class-salient regions while a differentiable renderer optimizes an image
reconstruction loss. We demonstrate that by 2D Gaussian input representations
coupled with our GVIT guidance, using a relatively standard ViT architecture,
closely matches the performance of a traditional patch-based ViT, reaching a
76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.

</details>


### [233] [Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound](https://arxiv.org/abs/2506.23538)
*Yuhao Huang,Yueyue Xu,Haoran Dou,Jiaxiao Deng,Xin Yang,Hongyu Zheng,Dong Ni*

Main category: cs.CV

TL;DR: An intelligent system using 3D ultrasound for automated plane localization and CUA diagnosis, combining denoising diffusion, reinforcement learning, and text-driven uncertainty modeling.


<details>
  <summary>Details</summary>
Motivation: CUAs cause infertility and pregnancy complications; 3D US improves accuracy over 2D US for diagnosis.

Method: 1) Denoising diffusion model with local/global guidance. 2) Reinforcement learning for key slice extraction. 3) Text-driven uncertainty modeling for classification adjustment.

Result: Effective plane localization and CUA diagnosis demonstrated on a large 3D US dataset.

Conclusion: The proposed system enhances accuracy and efficiency in diagnosing CUAs using 3D US.

Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,
preterm birth, and an increased risk of pregnancy complications. Compared to
traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,
providing a clear visualization of the uterine morphology for assessing CUAs
accurately. In this paper, we propose an intelligent system for simultaneous
automated plane localization and CUA diagnosis. Our highlights are: 1) we
develop a denoising diffusion model with local (plane) and global (volume/text)
guidance, using an adaptive weighting strategy to optimize attention allocation
to different conditions; 2) we introduce a reinforcement learning-based
framework with unsupervised rewards to extract the key slice summary from
redundant sequences, fully integrating information across multiple planes to
reduce learning difficulty; 3) we provide text-driven uncertainty modeling for
coarse prediction, and leverage it to adjust the classification probability for
overall performance improvement. Extensive experiments on a large 3D uterine US
dataset show the efficacy of our method, in terms of plane localization and CUA
diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.

</details>


### [234] [Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention](https://arxiv.org/abs/2506.23542)
*Weida Wang,Changyong He,Jin Zeng,Di Qiu*

Main category: cs.CV

TL;DR: A novel ToF depth denoising network using motion-invariant graph fusion improves temporal stability and spatial sharpness by leveraging cross-frame geometric attention and a maximum a posteriori formulation.


<details>
  <summary>Details</summary>
Motivation: ToF depth images are noisy, and existing methods either ignore multi-frame depth variations or lack temporal consistency, leading to poor performance.

Method: Proposes a network using motion-invariant graph fusion, geometric attention, and an iterative filter based on a maximum a posteriori problem.

Result: Achieves state-of-the-art performance on synthetic and real datasets, demonstrating accuracy and consistency.

Conclusion: The method effectively denoises ToF depth images while maintaining temporal and spatial quality, with robust generalization.

Abstract: Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,
requiring denoising for reliable downstream applications. Previous works either
focus on single-frame processing, or perform multi-frame processing without
considering depth variations at corresponding pixels across frames, leading to
undesirable temporal inconsistency and spatial ambiguity. In this paper, we
propose a novel ToF depth denoising network leveraging motion-invariant graph
fusion to simultaneously enhance temporal stability and spatial sharpness.
Specifically, despite depth shifts across frames, graph structures exhibit
temporal self-similarity, enabling cross-frame geometric attention for graph
fusion. Then, by incorporating an image smoothness prior on the fused graph and
data fidelity term derived from ToF noise distribution, we formulate a maximum
a posterior problem for ToF denoising. Finally, the solution is unrolled into
iterative filters whose weights are adaptively learned from the graph-informed
geometric attention, producing a high-performance yet interpretable network.
Experimental results demonstrate that the proposed scheme achieves
state-of-the-art performance in terms of accuracy and consistency on synthetic
DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.
Source code will be released at
\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.

</details>


### [235] [Pyramidal Patchification Flow for Visual Generation](https://arxiv.org/abs/2506.23543)
*Hui Li,Baoyou Chen,Liwei Zhang,Jiaye Li,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: PPFlow improves DiTs by varying patch sizes across timesteps, enhancing speed and performance without renoising.


<details>
  <summary>Details</summary>
Motivation: To optimize computation cost and performance in diffusion transformers by adapting patch sizes dynamically.

Method: Uses Pyramidal Patchification Flow (PPFlow) with varying patch sizes and learned projections for each size, modifying Unpatchify accordingly.

Result: Achieves 1.6×-2.0× faster inference with similar performance; pretrained DiTs show even better results.

Conclusion: PPFlow effectively balances speed and performance in diffusion transformers.

Abstract: Diffusion transformers (DiTs) adopt Patchify, mapping patch representations
to token representations through linear projections, to adjust the number of
tokens input to DiT blocks and thus the computation cost. Instead of a single
patch size for all the timesteps, we introduce a Pyramidal Patchification Flow
(PPFlow) approach: Large patch sizes are used for high noise timesteps and
small patch sizes for low noise timesteps; Linear projections are learned for
each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,
our approach operates over full latent representations other than pyramid
representations, and adopts the normal denoising process without requiring the
renoising trick. We demonstrate the effectiveness of our approach through two
training manners. Training from scratch achieves a $1.6\times$ ($2.0\times$)
inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with
slightly lower training FLOPs and similar image generation performance.
Training from pretrained normal DiTs achieves even better performance with
small training time. The code and checkpoint are at
https://github.com/fudan-generative-vision/PPFlow.

</details>


### [236] [Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions](https://arxiv.org/abs/2506.23547)
*Jiwon Kim,Soohyun Hwang,Dong-O Kim,Changsu Han,Min Kyu Park,Chang-Su Kim*

Main category: cs.CV

TL;DR: Oneta is a novel algorithm for multi-style image enhancement using a two-step process (intensity enhancement and color correction) with compact representation (eigenTF) and style tokens for versatility.


<details>
  <summary>Details</summary>
Motivation: To address the need for a versatile and high-performance solution for multi-style image enhancement tasks.

Method: Uses two sequential point operators (TF for intensity, CCM for color) and introduces eigenTF for compact representation. The network (Y-Net and C-Net) predicts parameters, with K learnable style tokens for K styles.

Result: Achieves high performance across six enhancement tasks (e.g., retouching, dehazing) on 30 datasets.

Conclusion: Oneta is a simple yet effective solution for diverse image enhancement tasks, demonstrating versatility and strong performance.

Abstract: The first algorithm, called Oneta, for a novel task of multi-style image
enhancement is proposed in this work. Oneta uses two point operators
sequentially: intensity enhancement with a transformation function (TF) and
color correction with a color correction matrix (CCM). This two-step
enhancement model, though simple, achieves a high performance upper bound.
Also, we introduce eigentransformation function (eigenTF) to represent TF
compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and
CCM parameters, respectively. To support $K$ styles, Oneta employs $K$
learnable tokens. During training, each style token is learned using image
pairs from the corresponding dataset. In testing, Oneta selects one of the $K$
style tokens to enhance an image accordingly. Extensive experiments show that
the single Oneta network can effectively undertake six enhancement tasks --
retouching, image signal processing, low-light image enhancement, dehazing,
underwater image enhancement, and white balancing -- across 30 datasets.

</details>


### [237] [JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](https://arxiv.org/abs/2506.23552)
*Mingi Kwon,Joonghyuk Shin,Jaeseok Jung,Jaesik Park,Youngjung Uh*

Main category: cs.CV

TL;DR: JAM-Flow is a unified framework for synthesizing facial motion and speech together, using flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) to enable cross-modal interaction.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat talking head synthesis and text-to-speech as separate tasks, missing the intrinsic link between facial motion and speech.

Method: JAM-Flow employs flow matching and MM-DiT with Motion-DiT and Audio-DiT modules, using selective joint attention and temporally aligned embeddings for cross-modal interaction.

Result: The framework supports tasks like synchronized talking head generation and audio-driven animation within a single model.

Conclusion: JAM-Flow advances multi-modal generative modeling by offering a holistic solution for audio-visual synthesis.

Abstract: The intrinsic link between facial motion and speech is often overlooked in
generative modeling, where talking head synthesis and text-to-speech (TTS) are
typically addressed as separate tasks. This paper introduces JAM-Flow, a
unified framework to simultaneously synthesize and condition on both facial
motion and speech. Our approach leverages flow matching and a novel Multi-Modal
Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT
and Audio-DiT modules. These are coupled via selective joint attention layers
and incorporate key architectural choices, such as temporally aligned
positional embeddings and localized joint attention masking, to enable
effective cross-modal interaction while preserving modality-specific strengths.
Trained with an inpainting-style objective, JAM-Flow supports a wide array of
conditioning inputs-including text, reference audio, and reference
motion-facilitating tasks such as synchronized talking head generation from
text, audio-driven animation, and much more, within a single, coherent model.
JAM-Flow significantly advances multi-modal generative modeling by providing a
practical solution for holistic audio-visual synthesis. project page:
https://joonghyuk.com/jamflow-web

</details>


### [238] [LH2Face: Loss function for Hard High-quality Face](https://arxiv.org/abs/2506.23555)
*Fan Xie,Pan Cao*

Main category: cs.CV

TL;DR: The paper introduces LH2Face, a novel loss function for face recognition, addressing limitations of cosine similarity and angular margins by incorporating face quality and hardness. It uses vMF distribution, adaptive margins, and proxy-based constraints, achieving superior performance on hard high-quality datasets.


<details>
  <summary>Details</summary>
Motivation: Current face recognition methods based on cosine similarity and softmax struggle with hard samples and lack consideration for face quality or hardness, leading to uniform training strategies.

Method: Proposes LH2Face: a vMF-based similarity measure, adaptive margin function, proxy-based constraints, and a renderer for joint optimization of face recognition and reconstruction.

Result: LH2Face achieves 49.39% accuracy on IJB-B, outperforming the second-best method by 2.37%.

Conclusion: LH2Face effectively addresses hard sample challenges in face recognition by integrating quality-aware and hardness-aware mechanisms, demonstrating superior performance.

Abstract: In current practical face authentication systems, most face recognition (FR)
algorithms are based on cosine similarity with softmax classification. Despite
its reliable classification performance, this method struggles with hard
samples. A popular strategy to improve FR performance is incorporating angular
or cosine margins. However, it does not take face quality or recognition
hardness into account, simply increasing the margin value and thus causing an
overly uniform training strategy. To address this problem, a novel loss
function is proposed, named Loss function for Hard High-quality Face (LH2Face).
Firstly, a similarity measure based on the von Mises-Fisher (vMF) distribution
is stated, specifically focusing on the logarithm of the Probability Density
Function (PDF), which represents the distance between a probability
distribution and a vector. Then, an adaptive margin-based multi-classification
method using softmax, called the Uncertainty-Aware Margin Function, is
implemented in the article. Furthermore, proxy-based loss functions are used to
apply extra constraints between the proxy and sample to optimize their
representation space distribution. Finally, a renderer is constructed that
optimizes FR through face reconstruction and vice versa. Our LH2Face is
superior to similiar schemes on hard high-quality face datasets, achieving
49.39% accuracy on the IJB-B dataset, which surpasses the second-place method
by 2.37%.

</details>


### [239] [OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2506.23565)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: The paper introduces Object-centric Radiance Fields (OcRF) to improve 3D object detection by focusing on foreground objects and avoiding background noise, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-view 3D object detection rely on implicit, data-driven approaches, limiting performance. Radiance fields, successful in 3D reconstruction, are explored but initially degrade detection due to background interference.

Method: Proposes OcRF to model foreground objects and discard background noise. Uses Height-aware Opacity-based Attention (HOA) to enhance 2D features.

Result: Achieves 57.2% mAP and 64.8% NDS on nuScenes test benchmark, outperforming prior methods.

Conclusion: OcRFDet effectively enhances 3D detection by focusing on foreground objects and leveraging opacity for feature enhancement, setting a new benchmark.

Abstract: Current multi-view 3D object detection methods typically transfer 2D features
into 3D space using depth estimation or 3D position encoder, but in a fully
data-driven and implicit manner, which limits the detection performance.
Inspired by the success of radiance fields on 3D reconstruction, we assume they
can be used to enhance the detector's ability of 3D geometry estimation.
However, we observe a decline in detection performance, when we directly use
them for 3D rendering as an auxiliary task. From our analysis, we find the
performance drop is caused by the strong responses on the background when
rendering the whole scene. To address this problem, we propose object-centric
radiance fields, focusing on modeling foreground objects while discarding
background noises. Specifically, we employ Object-centric Radiance Fields
(OcRF) to enhance 3D voxel features via an auxiliary task of rendering
foreground objects. We further use opacity - the side-product of rendering- to
enhance the 2D foreground BEV features via Height-aware Opacity-based Attention
(HOA), where attention maps at different height levels are generated separately
via multiple networks in parallel. Extensive experiments on the nuScenes
validation and test datasets demonstrate that our OcRFDet achieves superior
performance, outperforming previous state-of-the-art methods with 57.2$\%$ mAP
and 64.8$\%$ NDS on the nuScenes test benchmark. Code will be available at
https://github.com/Mingqj/OcRFDet.

</details>


### [240] [Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution](https://arxiv.org/abs/2506.23566)
*Luigi Sigillo,Renato Giamba,Danilo Comminiello*

Main category: cs.CV

TL;DR: MWT-Diff is a novel framework for satellite image super-resolution using latent diffusion models and wavelet transforms, outperforming recent methods in perceptual quality.


<details>
  <summary>Details</summary>
Motivation: High-resolution satellite imagery is limited by sensor constraints and high costs, hindering applications like environmental monitoring and disaster response.

Method: Combines latent diffusion models with wavelet transforms, using a metadata-, wavelet-, and time-aware encoder (MWT-Encoder) to guide hierarchical diffusion dynamics.

Result: MWT-Diff outperforms recent approaches in perceptual quality metrics (FID, LPIPS) and preserves spatial characteristics for remote sensing.

Conclusion: The framework effectively addresses challenges in satellite image super-resolution, offering high-quality results for critical applications.

Abstract: The acquisition of high-resolution satellite imagery is often constrained by
the spatial and temporal limitations of satellite sensors, as well as the high
costs associated with frequent observations. These challenges hinder
applications such as environmental monitoring, disaster response, and
agricultural management, which require fine-grained and high-resolution data.
In this paper, we propose MWT-Diff, an innovative framework for satellite image
super-resolution (SR) that combines latent diffusion models with wavelet
transforms to address these challenges. At the core of the framework is a novel
metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates
embeddings that capture metadata attributes, multi-scale frequency information,
and temporal relationships. The embedded feature representations steer the
hierarchical diffusion dynamics, through which the model progressively
reconstructs high-resolution satellite imagery from low-resolution inputs. This
process preserves critical spatial characteristics including textural patterns,
boundary discontinuities, and high-frequency spectral components essential for
detailed remote sensing analysis. The comparative analysis of MWT-Diff across
multiple datasets demonstrated favorable performance compared to recent
approaches, as measured by standard perceptual quality metrics including FID
and LPIPS.

</details>


### [241] [Event-based Tiny Object Detection: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2506.23575)
*Nuo Chen,Chao Xiao,Yimian Dai,Shiman He,Miao Li,Wei An*

Main category: cs.CV

TL;DR: The paper introduces EV-UAV, a large-scale event-based dataset for small object detection (SOD) in anti-UAV tasks, and proposes EV-SpSegNet, a novel method leveraging spatiotemporal event point clouds for improved detection.


<details>
  <summary>Details</summary>
Motivation: Traditional cameras struggle with SOD due to low frame rates and data redundancy, while existing event-based datasets lack diversity and scale for SOD benchmarks.

Method: The authors introduce the EV-UAV dataset and propose EV-SpSegNet, a network for event segmentation in point cloud space, using a Spatiotemporal Correlation (STC) loss to retain target events.

Result: EV-SpSegNet outperforms existing methods on the EV-UAV dataset, which features extremely small targets (6.8 × 5.4 pixels) and diverse scenarios.

Conclusion: The EV-UAV dataset and EV-SpSegNet provide a benchmark for future research in event-based small object detection, addressing limitations of traditional and existing event-based approaches.

Abstract: Small object detection (SOD) in anti-UAV task is a challenging problem due to
the small size of UAVs and complex backgrounds. Traditional frame-based cameras
struggle to detect small objects in complex environments due to their low frame
rates, limited dynamic range, and data redundancy. Event cameras, with
microsecond temporal resolution and high dynamic range, provide a more
effective solution for SOD. However, existing event-based object detection
datasets are limited in scale, feature large targets size, and lack diverse
backgrounds, making them unsuitable for SOD benchmarks. In this paper, we
introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),
the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes
147 sequences with over 2.3 million event-level annotations, featuring
extremely small targets (averaging 6.8 $\times$ 5.4 pixels) and diverse
scenarios such as urban clutter and extreme lighting conditions. Furthermore,
based on the observation that small moving targets form continuous curves in
spatiotemporal event point clouds, we propose Event based Sparse Segmentation
Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud
space, along with a Spatiotemporal Correlation (STC) loss that leverages motion
continuity to guide the network in retaining target events. Extensive
experiments on the EV-UAV dataset demonstrate the superiority of our method and
provide a benchmark for future research in EVSOD. The dataset and code are at
https://github.com/ChenYichen9527/Ev-UAV.

</details>


### [242] [StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2506.23577)
*Yanning Hou,Yanran Ruan,Junfa Li,Shanshan Wang,Jianfeng Qiu,Ke Xu*

Main category: cs.CV

TL;DR: The paper introduces StackCLIP, a method to enhance text-image alignment in CLIP for zero-shot anomaly detection. It uses stacked prompts and ensemble feature alignment to improve generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing overfitting and limited generalization in CLIP due to specific category prompts during pretraining.

Method: Proposes StackCLIP with Clustering-Driven Stacked Prompts (CSP) and Ensemble Feature Alignment (EFA) modules, plus Regulating Prompt Learning (RPL) for refinement.

Result: Achieves state-of-the-art performance in zero-shot anomaly detection and segmentation across seven datasets.

Conclusion: StackCLIP effectively improves generalization and performance in zero-shot industrial anomaly detection tasks.

Abstract: Enhancing the alignment between text and image features in the CLIP model is
a critical challenge in zero-shot industrial anomaly detection tasks. Recent
studies predominantly utilize specific category prompts during pretraining,
which can cause overfitting to the training categories and limit model
generalization. To address this, we propose a method that transforms category
names through multicategory name stacking to create stacked prompts, forming
the basis of our StackCLIP model. Our approach introduces two key components.
The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts
by stacking semantically analogous categories, while utilizing multi-object
textual feature fusion to amplify discriminative anomalies among similar
objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific
linear layers tailored for each stack cluster and adaptively integrates them
based on the attributes of test categories. These modules work together to
deliver superior training speed, stability, and convergence, significantly
boosting anomaly segmentation performance. Additionally, our stacked prompt
framework offers robust generalization across classification tasks. To further
improve performance, we introduce the Regulating Prompt Learning (RPL) module,
which leverages the generalization power of stacked prompts to refine prompt
learning, elevating results in anomaly detection classification tasks.
Extensive testing on seven industrial anomaly detection datasets demonstrates
that our method achieves state-of-the-art performance in both zero-shot anomaly
detection and segmentation tasks.

</details>


### [243] [Dataset Distillation via Vision-Language Category Prototype](https://arxiv.org/abs/2506.23580)
*Yawen Zou,Guang Li,Duo Su,Zi Wang,Jun Yu,Chao Zhang*

Main category: cs.CV

TL;DR: The paper introduces a vision-language method for dataset distillation (DD) by incorporating text prototypes alongside image prototypes to enhance performance and generalization, outperforming traditional image-only DD methods.


<details>
  <summary>Details</summary>
Motivation: Previous DD methods focus on images and overlook semantic context, leading to poor generalization and illogical outputs. This study aims to integrate language information to improve DD.

Method: The approach uses text prototypes (derived from an open-source large language model) and image prototypes to collaboratively synthesize data, enhancing distillation performance.

Result: The method produces logically coherent images with target objects, achieving state-of-the-art validation performance and robust generalization.

Conclusion: The vision-language framework expands DD's applicability beyond image-based methods, demonstrating superior performance and generalization.

Abstract: Dataset distillation (DD) condenses large datasets into compact yet
informative substitutes, preserving performance comparable to the original
dataset while reducing storage, transmission costs, and computational
consumption. However, previous DD methods mainly focus on distilling
information from images, often overlooking the semantic information inherent in
the data. The disregard for context hinders the model's generalization ability,
particularly in tasks involving complex datasets, which may result in illogical
outputs or the omission of critical objects. In this study, we integrate
vision-language methods into DD by introducing text prototypes to distill
language information and collaboratively synthesize data with image prototypes,
thereby enhancing dataset distillation performance. Notably, the text
prototypes utilized in this study are derived from descriptive text information
generated by an open-source large language model. This framework demonstrates
broad applicability across datasets without pre-existing text descriptions,
expanding the potential of dataset distillation beyond traditional image-based
approaches. Compared to other methods, the proposed approach generates
logically coherent images containing target objects, achieving state-of-the-art
validation performance and demonstrating robust generalization. Source code and
generated data are available in
https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/

</details>


### [244] [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/abs/2506.23581)
*Xiao Li,Yiming Zhu,Yifan Huang,Wei Zhang,Yingzhe He,Jie Shi,Xiaolin Hu*

Main category: cs.CV

TL;DR: PBCAT, a Patch-Based Composite Adversarial Training method, improves object detector robustness against diverse physically realizable attacks, outperforming existing defenses.


<details>
  <summary>Details</summary>
Motivation: Object detectors are vulnerable to physically realizable attacks like adversarial patches and textures, but existing adversarial training (AT) methods focus narrowly on patches, leaving broader threats unaddressed.

Method: PBCAT combines small-area gradient-guided adversarial patches and imperceptible global perturbations to train models, aiming to defend against various attacks.

Result: PBCAT significantly boosts robustness, improving detection accuracy by 29.7% under adversarial texture attacks compared to prior methods.

Conclusion: PBCAT offers a unified and effective defense against diverse physically realizable attacks, advancing the security of object detectors.

Abstract: Object detection plays a crucial role in many security-sensitive
applications. However, several recent studies have shown that object detectors
can be easily fooled by physically realizable attacks, \eg, adversarial patches
and recent adversarial textures, which pose realistic and urgent threats.
Adversarial Training (AT) has been recognized as the most effective defense
against adversarial attacks. While AT has been extensively studied in the
$l_\infty$ attack settings on classification models, AT against physically
realizable attacks on object detectors has received limited exploration. Early
attempts are only performed to defend against adversarial patches, leaving AT
against a wider range of physically realizable attacks under-explored. In this
work, we consider defending against various physically realizable attacks with
a unified AT method. We propose PBCAT, a novel Patch-Based Composite
Adversarial Training strategy. PBCAT optimizes the model by incorporating the
combination of small-area gradient-guided adversarial patches and imperceptible
global adversarial perturbations covering the entire image. With these designs,
PBCAT has the potential to defend against not only adversarial patches but also
unseen physically realizable attacks such as adversarial textures. Extensive
experiments in multiple settings demonstrated that PBCAT significantly improved
robustness against various physically realizable attacks over state-of-the-art
defense methods. Notably, it improved the detection accuracy by 29.7\% over
previous defense methods under one recent adversarial texture attack.

</details>


### [245] [CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2506.23590)
*Qiming Li,Zekai Ye,Xiaocheng Feng,Weihong Zhong,Libo Qin,Ruihan Chen,Baohang Li,Kui Jiang,Yaowei Wang,Ting Liu,Bing Qin*

Main category: cs.CV

TL;DR: The paper proposes Caption-sensitive Attention Intervention (CAI), a training-free method to mitigate object hallucination in Large Vision-Language Models (LVLMs) by leveraging attention patterns from caption queries.


<details>
  <summary>Details</summary>
Motivation: LVLMs often produce content deviating from visual information (object hallucination), and existing solutions are costly or slow. The authors aim to address this efficiently.

Method: CAI uses attention activation patterns from caption queries to enhance LVLMs' visual perception without additional training or significant inference cost.

Result: CAI achieves state-of-the-art performance in hallucination mitigation across four benchmarks with minimal added inference cost.

Conclusion: CAI is an effective, low-cost solution for reducing object hallucination in LVLMs, leveraging existing attention patterns.

Abstract: Although Large Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in interpreting visual information, they frequently produce
content that deviates from visual information, leading to object hallucination.
To tackle this, recent works mostly depend on expensive manual annotations and
training cost, or significantly increase inference time. In this work, we
observe that LVLMs' attention to visual information is significantly stronger
when answering caption queries compared to non-caption queries. Inspired by
this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a
training-free, plug-and-play hallucination mitigation method that leverages the
attention activation pattern in response to caption queries to enhance LVLMs'
visual perception capability. Extensive experimental results across four
benchmarks covering both discriminative and generative tasks, demonstrate that
CAI achieves state-of-the-art (SOTA) hallucination mitigating performance only
with minimal additional inference cost.

</details>


### [246] [AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval](https://arxiv.org/abs/2506.23605)
*Suyash Maniyar,Vishvesh Trivedi,Ajoy Mondal,Anand Mishra,C. V. Jawahar*

Main category: cs.CV

TL;DR: Proposes SynLecSlideGen, an LLM-guided pipeline for generating synthetic lecture slides, and evaluates its utility via few-shot transfer learning on real data, showing significant performance improvement.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of lecture slides for training models is labor-intensive and requires expertise. Synthetic data can mitigate this.

Method: Develops SynLecSlideGen for synthetic slide generation and benchmarks it on RealSlide (1,050 annotated slides). Uses few-shot transfer learning to evaluate synthetic data utility.

Result: Few-shot transfer learning with synthetic pretraining outperforms training on real data alone, proving synthetic data compensates for limited labeled slides.

Conclusion: Synthetic lecture slides generated by SynLecSlideGen effectively enhance model performance when labeled data is scarce.

Abstract: Lecture slide element detection and retrieval are key problems in slide
understanding. Training effective models for these tasks often depends on
extensive manual annotation. However, annotating large volumes of lecture
slides for supervised training is labor intensive and requires domain
expertise. To address this, we propose a large language model (LLM)-guided
synthetic lecture slide generation pipeline, SynLecSlideGen, which produces
high-quality, coherent and realistic slides. We also create an evaluation
benchmark, namely RealSlide by manually annotating 1,050 real lecture slides.
To assess the utility of our synthetic slides, we perform few-shot transfer
learning on real data using models pre-trained on them. Experimental results
show that few-shot transfer learning with pretraining on synthetic slides
significantly improves performance compared to training only on real data. This
demonstrates that synthetic data can effectively compensate for limited labeled
lecture slides. The code and resources of our work are publicly available on
our project website: https://synslidegen.github.io/.

</details>


### [247] [SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion](https://arxiv.org/abs/2506.23606)
*Zhengkang Xiang,Zizhao Li,Amir Khodabandeh,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: SG-LDM is a Semantic-Guided Lidar Diffusion Model for high-fidelity lidar point cloud synthesis, outperforming existing methods and enhancing downstream tasks like segmentation.


<details>
  <summary>Details</summary>
Motivation: Real-world lidar data is often scarce or lacks diversity, limiting deep learning pipelines. Existing methods focus on unconditional generation, missing practical applications.

Method: SG-LDM uses latent alignment and explicit semantic conditioning for semantic-to-lidar synthesis. It also introduces a diffusion-based lidar translation framework for cross-domain adaptation.

Result: SG-LDM achieves state-of-the-art performance in generating high-fidelity lidar point clouds and improves downstream segmentation tasks through data augmentation.

Conclusion: SG-LDM advances lidar synthesis by integrating semantic guidance and cross-domain translation, offering practical benefits for real-world applications.

Abstract: Lidar point cloud synthesis based on generative models offers a promising
solution to augment deep learning pipelines, particularly when real-world data
is scarce or lacks diversity. By enabling flexible object manipulation, this
synthesis approach can significantly enrich training datasets and enhance
discriminative models. However, existing methods focus on unconditional lidar
point cloud generation, overlooking their potential for real-world
applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar
Diffusion Model that employs latent alignment to enable robust
semantic-to-lidar synthesis. By directly operating in the native lidar space
and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art
performance in generating high-fidelity lidar point clouds guided by semantic
labels. Moreover, we propose the first diffusion-based lidar translation
framework based on SG-LDM, which enables cross-domain translation as a domain
adaptation strategy to enhance downstream perception performance. Systematic
experiments demonstrate that SG-LDM significantly outperforms existing lidar
diffusion models and the proposed lidar translation framework further improves
data augmentation performance in the downstream lidar segmentation task.

</details>


### [248] [PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum](https://arxiv.org/abs/2506.23607)
*Shiqi Zhang,Sha Zhang,Jiajun Deng,Yedong Shen,Mingxiao MA,Yanyong Zhang*

Main category: cs.CV

TL;DR: PGOV3D introduces a two-stage training strategy for open-vocabulary 3D semantic segmentation, leveraging partial scenes for dense semantic pre-training and fine-tuning on complete scenes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook rich semantic content and cross-view correspondences in multi-view images, limiting effectiveness.

Method: A two-stage approach: pre-training on partial scenes with dense semantics and fine-tuning on complete scenes. Uses MLLM and 2D segmentation for open-vocabulary labels.

Result: Achieves competitive performance on ScanNet, ScanNet200, and S3DIS benchmarks.

Conclusion: PGOV3D effectively bridges the semantic gap between partial and complete 3D scenes, enhancing open-vocabulary segmentation.

Abstract: Existing open-vocabulary 3D semantic segmentation methods typically supervise
3D segmentation models by merging text-aligned features (e.g., CLIP) extracted
from multi-view images onto 3D points. However, such approaches treat
multi-view images merely as intermediaries for transferring open-vocabulary
information, overlooking their rich semantic content and cross-view
correspondences, which limits model effectiveness. To address this, we propose
PGOV3D, a novel framework that introduces a Partial-to-Global curriculum for
improving open-vocabulary 3D semantic segmentation. The key innovation lies in
a two-stage training strategy. In the first stage, we pre-train the model on
partial scenes that provide dense semantic information but relatively simple
geometry. These partial point clouds are derived from multi-view RGB-D inputs
via pixel-wise depth projection. To enable open-vocabulary learning, we
leverage a multi-modal large language model (MLLM) and a 2D segmentation
foundation model to generate open-vocabulary labels for each viewpoint,
offering rich and aligned supervision. An auxiliary inter-frame consistency
module is introduced to enforce feature consistency across varying viewpoints
and enhance spatial understanding. In the second stage, we fine-tune the model
on complete scene-level point clouds, which are sparser and structurally more
complex. We aggregate the partial vocabularies associated with each scene and
generate pseudo labels using the pre-trained model, effectively bridging the
semantic gap between dense partial observations and large-scale 3D
environments. Extensive experiments on ScanNet, ScanNet200, and S3DIS
benchmarks demonstrate that PGOV3D achieves competitive performance in
open-vocabulary 3D semantic segmentation.

</details>


### [249] [AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention](https://arxiv.org/abs/2506.23611)
*Ziao Liu,Zhenjia Li,Yifeng Shi,Xiangang Li*

Main category: cs.CV

TL;DR: AttentionGS eliminates the need for high-quality initial point clouds in 3D Gaussian Splatting by using structural attention for direct 3D reconstruction, improving performance in unreliable initialization scenarios.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) depends on high-quality point clouds from SfM, which fails in texture-deficient or constrained-view scenarios, limiting its applicability.

Method: AttentionGS uses geometric attention early in training for global structure recovery and texture attention later for detail refinement, along with opacity-weighted gradients for Gaussian densification.

Result: AttentionGS outperforms state-of-the-art methods, especially in unreliable point cloud initialization scenarios.

Conclusion: The approach enhances the robustness and flexibility of 3D Gaussian Splatting for real-world applications.

Abstract: 3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality initial point
clouds by leveraging structural attention for direct 3D reconstruction from
randomly initialization. In the early training stage, we introduce geometric
attention to rapidly recover the global scene structure. As training
progresses, we incorporate texture attention to refine fine-grained details and
enhance rendering quality. Furthermore, we employ opacity-weighted gradients to
guide Gaussian densification, leading to improved surface reconstruction.
Extensive experiments on multiple benchmark datasets demonstrate that
AttentionGS significantly outperforms state-of-the-art methods, particularly in
scenarios where point cloud initialization is unreliable. Our approach paves
the way for more robust and flexible 3D Gaussian Splatting in real-world
applications.

</details>


### [250] [TurboVSR: Fantastic Video Upscalers and Where to Find Them](https://arxiv.org/abs/2506.23618)
*Zhongdao Wang,Guodongfang Zhao,Jingjing Ren,Bailan Feng,Shifeng Zhang,Wenbo Li*

Main category: cs.CV

TL;DR: TurboVSR is an ultra-efficient diffusion-based video super-resolution model that achieves state-of-the-art performance while being 100+ times faster than current methods.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based VSR methods are computationally inefficient, taking too long to process videos. TurboVSR aims to address this by improving efficiency without sacrificing quality.

Method: TurboVSR uses a high-compression autoencoder, factorized conditioning for training, and converts a pre-trained diffusion model into a shortcut model for faster inference.

Result: TurboVSR matches state-of-the-art VSR performance, processes a 2-second 1080p video in 7 seconds, and supports 4K image super-resolution with fine details.

Conclusion: TurboVSR significantly improves computational efficiency in video super-resolution, enabling practical applications like 4K SR.

Abstract: Diffusion-based generative models have demonstrated exceptional promise in
the video super-resolution (VSR) task, achieving a substantial advancement in
detail generation relative to prior methods. However, these approaches face
significant computational efficiency challenges. For instance, current
techniques may require tens of minutes to super-resolve a mere 2-second, 1080p
video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based
video super-resolution model. Our core design comprises three key aspects: (1)
We employ an autoencoder with a high compression ratio of 32$\times$32$\times$8
to reduce the number of tokens. (2) Highly compressed latents pose substantial
challenges for training. We introduce factorized conditioning to mitigate the
learning complexity: we first learn to super-resolve the initial frame;
subsequently, we condition the super-resolution of the remaining frames on the
high-resolution initial frame and the low-resolution subsequent frames. (3) We
convert the pre-trained diffusion model to a shortcut model to enable fewer
sampling steps, further accelerating inference. As a result, TurboVSR performs
on par with state-of-the-art VSR methods, while being 100+ times faster, taking
only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports
image resolution by considering image as a one-frame video. Our efficient
design makes SR beyond 1080p possible, results on 4K (3648$\times$2048) image
SR show surprising fine details.

</details>


### [251] [Revisiting Audio-Visual Segmentation with Vision-Centric Transformer](https://arxiv.org/abs/2506.23623)
*Shaofei Huang,Rui Ling,Tianrui Hui,Hongyu Li,Xu Zhou,Shifeng Zhang,Si Liu,Richang Hong,Meng Wang*

Main category: cs.CV

TL;DR: The paper introduces a Vision-Centric Transformer (VCT) framework for Audio-Visual Segmentation (AVS), addressing limitations of audio-centric methods like perception ambiguity and visual detail loss. VCT uses vision-derived queries and a PPQG module to improve segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Audio-centric Transformers in AVS suffer from perception ambiguity and weakened dense prediction due to mixed audio and visual detail loss.

Method: Proposes a Vision-Centric Transformer (VCT) with vision-derived queries and a Prototype Prompted Query Generation (PPQG) module for better audio-visual information aggregation.

Result: Achieves state-of-the-art performance on three subsets of the AVSBench dataset.

Conclusion: VCT effectively addresses the limitations of audio-centric methods, improving segmentation accuracy and performance.

Abstract: Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in
video frames based on the associated audio signal. Prevailing AVS methods
typically adopt an audio-centric Transformer architecture, where object queries
are derived from audio features. However, audio-centric Transformers suffer
from two limitations: perception ambiguity caused by the mixed nature of audio,
and weakened dense prediction ability due to visual detail loss. To address
these limitations, we propose a new Vision-Centric Transformer (VCT) framework
that leverages vision-derived queries to iteratively fetch corresponding audio
and visual information, enabling queries to better distinguish between
different sounding objects from mixed audio and accurately delineate their
contours. Additionally, we also introduce a Prototype Prompted Query Generation
(PPQG) module within our VCT framework to generate vision-derived queries that
are both semantically aware and visually rich through audio prototype prompting
and pixel context grouping, facilitating audio-visual information aggregation.
Extensive experiments demonstrate that our VCT framework achieves new
state-of-the-art performances on three subsets of the AVSBench dataset. The
code is available at https://github.com/spyflying/VCT_AVS.

</details>


### [252] [Brain Tumor Detection through Thermal Imaging and MobileNET](https://arxiv.org/abs/2506.23627)
*Roham Maiti,Debasmita Bhoumik*

Main category: cs.CV

TL;DR: The paper proposes using MobileNET for efficient brain tumor detection from MRI scans, addressing limitations of traditional methods and classical ML models.


<details>
  <summary>Details</summary>
Motivation: Brain tumors pose significant health risks, and traditional detection methods like biopsies and MRI/CT scans are costly and require expertise. Classical ML models also face computational and data challenges.

Method: The research employs the MobileNET model for tumor detection, focusing on reduced computational resources and faster processing, combined with image processing for accuracy.

Result: The proposed method achieved an average accuracy of 98.5%.

Conclusion: MobileNET offers an efficient, accurate, and resource-friendly solution for brain tumor detection, improving accessibility and speed.

Abstract: Brain plays a crucial role in regulating body functions and cognitive
processes, with brain tumors posing significant risks to human health. Precise
and prompt detection is a key factor in proper treatment and better patient
outcomes. Traditional methods for detecting brain tumors, that include
biopsies, MRI, and CT scans often face challenges due to their high costs and
the need for specialized medical expertise. Recent developments in machine
learning (ML) and deep learning (DL) has exhibited strong capabilities in
automating the identification and categorization of brain tumors from medical
images, especially MRI scans. However, these classical ML models have
limitations, such as high computational demands, the need for large datasets,
and long training times, which hinder their accessibility and efficiency. Our
research uses MobileNET model for efficient detection of these tumors. The
novelty of this project lies in building an accurate tumor detection model
which use less computing re-sources and runs in less time followed by efficient
decision making through the use of image processing technique for accurate
results. The suggested method attained an average accuracy of 98.5%.

</details>


### [253] [Blending Concepts with Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.23630)
*Lorenzo Olearo,Giorgio Longari,Alessandro Raganato,Rafael Peñaloza,Simone Melzi*

Main category: cs.CV

TL;DR: Diffusion models can blend diverse concepts into coherent images without training, using methods like prompt scheduling or embedding interpolation. No single method dominates; outcomes depend on factors like prompt order and conceptual distance.


<details>
  <summary>Details</summary>
Motivation: To explore whether diffusion models can blend distinct concepts (from objects to ideas) into novel visual entities under a zero-shot framework.

Method: Four blending techniques are investigated, leveraging different aspects of the diffusion pipeline (e.g., prompt scheduling, embedding interpolation, layer-wise conditioning).

Result: Diffusion models exhibit creative blending capabilities, but no single method is universally superior; outcomes vary with input conditions.

Conclusion: Diffusion models show strong compositional potential but are sensitive to minor input variations, highlighting context-dependent effectiveness of blending techniques.

Abstract: Diffusion models have dramatically advanced text-to-image generation in
recent years, translating abstract concepts into high-fidelity images with
remarkable ease. In this work, we examine whether they can also blend distinct
concepts, ranging from concrete objects to intangible ideas, into coherent new
visual entities under a zero-shot framework. Specifically, concept blending
merges the key attributes of multiple concepts (expressed as textual prompts)
into a single, novel image that captures the essence of each concept. We
investigate four blending methods, each exploiting different aspects of the
diffusion pipeline (e.g., prompt scheduling, embedding interpolation, or
layer-wise conditioning). Through systematic experimentation across diverse
concept categories, such as merging concrete concepts, synthesizing compound
words, transferring artistic styles, and blending architectural landmarks, we
show that modern diffusion models indeed exhibit creative blending capabilities
without further training or fine-tuning. Our extensive user study, involving
100 participants, reveals that no single approach dominates in all scenarios:
each blending technique excels under certain conditions, with factors like
prompt ordering, conceptual distance, and random seed affecting the outcome.
These findings highlight the remarkable compositional potential of diffusion
models while exposing their sensitivity to seemingly minor input variations.

</details>


### [254] [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639)
*Wanpeng Zhang,Yicheng Feng,Hao Luo,Yijiang Li,Zihao Yue,Sipeng Zheng,Zongqing Lu*

Main category: cs.CV

TL;DR: A framework unifies multimodal understanding by applying byte-pair encoding to visual tokens, improving cross-modal relationships and vision-language task performance.


<details>
  <summary>Details</summary>
Motivation: Aligning different modalities effectively remains a challenge in MLLMs.

Method: Uses byte-pair encoding for visual tokens, priority-guided encoding, and multi-stage training with curriculum-driven data.

Result: Improved performance in diverse vision-language tasks.

Conclusion: The approach advances more capable and efficient multimodal foundation models.

Abstract: Multimodal large language models (MLLMs) have made significant progress in
vision-language understanding, yet effectively aligning different modalities
remains a fundamental challenge. We present a framework that unifies multimodal
understanding by applying byte-pair encoding to visual tokens. Unlike
conventional approaches that rely on modality-specific encoders, our method
directly incorporates structural information into visual tokens, mirroring
successful tokenization strategies in text-only language models. We introduce a
priority-guided encoding scheme that considers both frequency and spatial
consistency, coupled with a multi-stage training procedure based on
curriculum-driven data composition. These enhancements enable the transformer
model to better capture cross-modal relationships and reason with visual
information. Comprehensive experiments demonstrate improved performance across
diverse vision-language tasks. By bridging the gap between visual and textual
representations, our approach contributes to the advancement of more capable
and efficient multimodal foundation models.

</details>


### [255] [VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation](https://arxiv.org/abs/2506.23641)
*Peng Huang,Junhu Fu,Bowen Guo,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: VAP-Diffusion leverages MLLMs to generate detailed medical image descriptions, improving realism and diversity in image generation.


<details>
  <summary>Details</summary>
Motivation: Detailed descriptions for medical images (e.g., shape, texture) are often unavailable, limiting generative models.

Method: Uses MLLMs with Chain-of-Thought prompts to extract descriptions, stores them by category, and employs a Prototype Condition Mechanism for robustness.

Result: Effective across three medical imaging types on four datasets.

Conclusion: VAP-Diffusion enhances medical image generation quality and diversity by integrating external knowledge.

Abstract: As the appearance of medical images is influenced by multiple underlying
factors, generative models require rich attribute information beyond labels to
produce realistic and diverse images. For instance, generating an image of skin
lesion with specific patterns demands descriptions that go beyond diagnosis,
such as shape, size, texture, and color. However, such detailed descriptions
are not always accessible. To address this, we explore a framework, termed
Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from
pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality
and diversity of medical image generation. First, to derive descriptions from
MLLMs without hallucination, we design a series of prompts following
Chain-of-Thoughts for common medical imaging tasks, including dermatologic,
colorectal, and chest X-ray images. Generated descriptions are utilized during
training and stored across different categories. During testing, descriptions
are randomly retrieved from the corresponding category for inference. Moreover,
to make the generator robust to unseen combination of descriptions at the test
time, we propose a Prototype Condition Mechanism that restricts test embeddings
to be similar to those from training. Experiments on three common types of
medical imaging across four datasets verify the effectiveness of VAP-Diffusion.

</details>


### [256] [MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis](https://arxiv.org/abs/2506.23648)
*Zhe Liu,Yuhao Huang,Lian Liu,Chengrui Zhang,Haotian Lin,Tong Han,Zhiyuan Zhu,Yanlin Chen,Yuerui Chen,Dong Ni,Zhongshan Gou,Xin Yang*

Main category: cs.CV

TL;DR: An automated MR diagnosis model (MReg) using 4-chamber cardiac color Doppler echocardiography videos improves accuracy and interpretability by mimicking clinical workflow and leveraging feature mining strategies.


<details>
  <summary>Details</summary>
Motivation: Current intelligent methods for mitral regurgitation (MR) diagnosis often misalign with clinical workflows and lack accuracy and interpretability.

Method: MReg formulates MR diagnosis as a regression task, uses feature selection/amplification to mimic sonographer logic, and employs a feature summary module for category-level feature extraction.

Result: MReg outperforms other methods on a dataset of 1868 cases, demonstrating superior accuracy in MR diagnosis.

Conclusion: MReg offers a clinically aligned, accurate, and interpretable solution for automated MR diagnosis.

Abstract: Color Doppler echocardiography is a crucial tool for diagnosing mitral
regurgitation (MR). Recent studies have explored intelligent methods for MR
diagnosis to minimize user dependence and improve accuracy. However, these
approaches often fail to align with clinical workflow and may lead to
suboptimal accuracy and interpretability. In this study, we introduce an
automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color
Doppler echocardiography video (A4C-CDV). It follows comprehensive feature
mining strategies to detect MR and assess its severity, considering clinical
realities. Our contribution is threefold. First, we formulate the MR diagnosis
as a regression task to capture the continuity and ordinal relationships
between categories. Second, we design a feature selection and amplification
mechanism to imitate the sonographer's diagnostic logic for accurate MR
grading. Third, inspired by the Mixture-of-Experts concept, we introduce a
feature summary module to extract the category-level features, enhancing the
representational capacity for more accurate grading. We trained and evaluated
our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases
with three graded regurgitation labels. Compared to other weakly supervised
video anomaly detection and supervised classification methods, MReg
demonstrated superior performance in MR diagnosis. Our code is available at:
https://github.com/cskdstz/MReg.

</details>


### [257] [Towards Markerless Intraoperative Tracking of Deformable Spine Tissue](https://arxiv.org/abs/2506.23657)
*Connor Daly,Elettra Marconi,Marco Riva,Jinendra Ekanayake,Daniel S. Elson,Ferdinando Rodriguez y Baena*

Main category: cs.CV

TL;DR: The paper presents SpineAlign, a system for tracking spinal deformations in surgery using RGB-D imaging, along with a dataset and segmentation network.


<details>
  <summary>Details</summary>
Motivation: To improve intraoperative orthopedic tracking by reducing complexity and time compared to bone-mounted devices, moving beyond cadaveric studies.

Method: Develops SpineAlign for deformation tracking, introduces a clinical RGB-D dataset, and proposes CorrespondNet for key region prediction.

Result: First real-world clinical RGB-D dataset for spine surgery and a multi-task framework for registration.

Conclusion: The work advances markerless tracking in spine surgery with practical tools and data.

Abstract: Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is
a promising method with high translational potential. Unlike bone-mounted
tracking devices, markerless tracking can reduce operating time and complexity.
However, its use has been limited to cadaveric studies. This paper introduces
the first real-world clinical RGB-D dataset for spine surgery and develops
SpineAlign, a system for capturing deformation between preoperative and
intraoperative spine states. We also present an intraoperative segmentation
network trained on this data and introduce CorrespondNet, a multi-task
framework for predicting key regions for registration in both intraoperative
and preoperative scenes.

</details>


### [258] [On the Domain Robustness of Contrastive Vision-Language Models](https://arxiv.org/abs/2506.23663)
*Mario Koddenbrock,Rudolf Hoffmann,David Brodmann,Erik Rodner*

Main category: cs.CV

TL;DR: Deepbench is a framework for evaluating domain-specific robustness of vision-language models (VLMs) using LLM-generated corruptions, revealing variability in performance across domains.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency and declining effectiveness of pretrained VLMs under specialized domain shifts.

Method: Deepbench uses an LLM to generate realistic, context-aware image corruptions for domain-specific evaluation without labeled data.

Result: Substantial variability in robustness across six real-world domains was observed.

Conclusion: Targeted, domain-aware evaluation is crucial, and Deepbench is released as open-source to support further research.

Abstract: In real-world vision-language applications, practitioners increasingly rely
on large, pretrained foundation models rather than custom-built solutions,
despite limited transparency regarding their training data and processes. While
these models achieve impressive performance on general benchmarks, their
effectiveness can decline notably under specialized domain shifts, such as
unique imaging conditions or environmental variations. In this work, we
introduce Deepbench, a framework designed to assess domain-specific robustness
of vision-language models (VLMs). Deepbench leverages a large language model
(LLM) to generate realistic, context-aware image corruptions tailored to
specific deployment domains without requiring labeled data. We evaluate a range
of contrastive vision-language architectures and architectural variants across
six real-world domains and observe substantial variability in robustness,
highlighting the need for targeted, domain-aware evaluation. Deepbench is
released as open-source software to support further research into domain-aware
robustness assessment.

</details>


### [259] [Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration](https://arxiv.org/abs/2506.23674)
*Dongyue Wu,Zilin Guo,Jialong Zuo,Nong Sang,Changxin Gao*

Main category: cs.CV

TL;DR: PFB is a novel framework for lossless training acceleration by adaptively pruning less important samples using shallow-layer features, reducing computational costs without needing proxy models.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of training large datasets while maintaining model performance by pruning less important samples efficiently.

Method: Uses Partial Forward Blocking (PFB) to assess sample importance via shallow-layer features, prunes less important samples, and avoids deep-layer computations for pruned samples. Introduces probability density for dynamic importance prioritization.

Result: Achieves 0.5% accuracy improvement and 33% training time reduction on ImageNet with 40% data pruned.

Conclusion: PFB effectively accelerates training while improving model performance, offering a practical solution for large-scale datasets.

Abstract: The ever-growing size of training datasets enhances the generalization
capability of modern machine learning models but also incurs exorbitant
computational costs. Existing data pruning approaches aim to accelerate
training by removing those less important samples. However, they often rely on
gradients or proxy models, leading to prohibitive additional costs of gradient
back-propagation and proxy model training. In this paper, we propose Partial
Forward Blocking (PFB), a novel framework for lossless training acceleration.
The efficiency of PFB stems from its unique adaptive pruning pipeline: sample
importance is assessed based on features extracted from the shallow layers of
the target model. Less important samples are then pruned, allowing only the
retained ones to proceed with the subsequent forward pass and loss
back-propagation. This mechanism significantly reduces the computational
overhead of deep-layer forward passes and back-propagation for pruned samples,
while also eliminating the need for auxiliary backward computations and proxy
model training. Moreover, PFB introduces probability density as an indicator of
sample importance. Combined with an adaptive distribution estimation module,
our method dynamically prioritizes relatively rare samples, aligning with the
constantly evolving training state. Extensive experiments demonstrate the
significant superiority of PFB in performance and speed. On ImageNet, PFB
achieves a 0.5% accuracy improvement and 33% training time reduction with 40%
data pruned.

</details>


### [260] [Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation](https://arxiv.org/abs/2506.23675)
*Patrick Glandorf,Bodo Rosenhahn*

Main category: cs.CV

TL;DR: P3B is a pruning method for Vision Transformers that globally assigns parameter resources by evaluating block-level contributions, maintaining performance even at high sparsity.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers are computationally expensive, and traditional pruning methods misevaluate weight significance on unseen domains, leading to suboptimal performance.

Method: P3B uses block-level contributions to assign resources, setting layerwise keep ratios based on global metrics to reactivate critical blocks.

Result: P3B achieves state-of-the-art pruning, preserving accuracy (only 0.64% loss) even at 70% parameter reduction.

Conclusion: P3B is an effective pruning method, especially for transfer learning, balancing performance and computational efficiency.

Abstract: Vision Transformer have set new benchmarks in several tasks, but these models
come with the lack of high computational costs which makes them impractical for
resource limited hardware. Network pruning reduces the computational complexity
by removing less important operations while maintaining performance. However,
pruning a model on an unseen data domain, leads to a misevaluation of weight
significance, resulting in suboptimal resource assignment. In this work, we
find that task-sensitive layers initially fail to improve the feature
representation on downstream tasks, leading to performance loss for early
pruning decisions. To address this problem, we introduce Pruning by Block
Benefit (P3B), a pruning method that utilizes the relative contribution on
block level to globally assign parameter resources. P3B identifies low-impact
components to reduce parameter allocation while preserving critical ones.
Classical pruning mask optimization struggles to reactivate zero-mask-elements.
In contrast, P3B sets a layerwise keep ratio based on global performance
metrics, ensuring the reactivation of late-converging blocks. We show in
extensive experiments that P3B is a state of the art pruning method with most
noticeable gains in transfer learning tasks. Notably, P3B is able to conserve
high performance, even in high sparsity regimes of 70% parameter reduction
while only losing 0.64% in accuracy.

</details>


### [261] [A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement](https://arxiv.org/abs/2506.23676)
*Gaozheng Pei,Ke Ma,Dongpeng Zhang,Chengzhi Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CV

TL;DR: A unified framework integrates traditional transferability strategies into diffusion-based adversarial example generation, enhancing performance in diverse tasks like Deepfake detection.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based adversarial methods struggle with generalization beyond image classification and adapting traditional transferability strategies.

Method: Proposes a unified framework combining traditional transferability enhancement strategies with diffusion model-based adversarial example generation.

Result: Achieved first place in a competition, validating the framework's effectiveness.

Conclusion: The framework successfully broadens the applicability of diffusion-based adversarial methods.

Abstract: Due to their powerful image generation capabilities, diffusion-based
adversarial example generation methods through image editing are rapidly
gaining popularity. However, due to reliance on the discriminative capability
of the diffusion model, these diffusion-based methods often struggle to
generalize beyond conventional image classification tasks, such as in Deepfake
detection. Moreover, traditional strategies for enhancing adversarial example
transferability are challenging to adapt to these methods. To address these
challenges, we propose a unified framework that seamlessly incorporates
traditional transferability enhancement strategies into diffusion model-based
adversarial example generation via image editing, enabling their application
across a wider range of downstream tasks. Our method won first place in the
"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of
AI-Generated Media" competition at ACM MM25, which validates the effectiveness
of our approach.

</details>


### [262] [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](https://arxiv.org/abs/2506.23690)
*Shuai Tan,Biao Gong,Yujie Wei,Shiwei Zhang,Zhuoxin Liu,Dandan Zheng,Jingdong Chen,Yan Wang,Hao Ouyang,Kecheng Zheng,Yujun Shen*

Main category: cs.CV

TL;DR: SynMotion is a motion-customized video generation model that combines semantic guidance and visual adaptation to improve motion fidelity and semantic clarity.


<details>
  <summary>Details</summary>
Motivation: Existing methods either focus on semantic alignment or visual representation, leading to overlooked motion complexity or semantic confusion. SynMotion addresses this by jointly leveraging both aspects.

Method: SynMotion uses a dual-embedding semantic comprehension mechanism for disentangling subject and motion representations and integrates motion adapters for visual adaptation. It employs an alternate optimization strategy for embeddings and uses the SPV dataset.

Result: SynMotion outperforms existing baselines in text-to-video (T2V) and image-to-video (I2V) settings, demonstrating improved motion fidelity and semantic clarity.

Conclusion: SynMotion effectively balances semantic and visual aspects for motion-customized video generation, validated by experiments and the new MotionBench benchmark.

Abstract: Diffusion-based video motion customization facilitates the acquisition of
human motion representations from a few video samples, while achieving
arbitrary subjects transfer through precise textual conditioning. Existing
approaches often rely on semantic-level alignment, expecting the model to learn
new motion concepts and combine them with other entities (e.g., ''cats'' or
''dogs'') to produce visually appealing results. However, video data involve
complex spatio-temporal patterns, and focusing solely on semantics cause the
model to overlook the visual complexity of motion. Conversely, tuning only the
visual representation leads to semantic confusion in representing the intended
action. To address these limitations, we propose SynMotion, a new
motion-customized video generation model that jointly leverages semantic
guidance and visual adaptation. At the semantic level, we introduce the
dual-embedding semantic comprehension mechanism which disentangles subject and
motion representations, allowing the model to learn customized motion features
while preserving its generative capabilities for diverse subjects. At the
visual level, we integrate parameter-efficient motion adapters into a
pre-trained video generation model to enhance motion fidelity and temporal
coherence. Furthermore, we introduce a new embedding-specific training strategy
which \textbf{alternately optimizes} subject and motion embeddings, supported
by the manually constructed Subject Prior Video (SPV) training dataset. This
strategy promotes motion specificity while preserving generalization across
diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark
with diverse motion patterns. Experimental results across both T2V and I2V
settings demonstrate that \method outperforms existing baselines. Project page:
https://lucaria-academy.github.io/SynMotion/

</details>


### [263] [Single Image Test-Time Adaptation via Multi-View Co-Training](https://arxiv.org/abs/2506.23705)
*Smriti Joshi,Richard Osuala,Lidia Garrucho,Kaisar Kushibar,Dimitri Kessler,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: A patch-based multi-view co-training method for single-image test-time adaptation in medical imaging, outperforming state-of-the-art methods by 3.75% Dice score.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of large target domain datasets in medical settings and leveraging volumetric data richness for real-time inference.

Method: Patch-based multi-view co-training with uncertainty-guided self-training for feature and prediction consistency.

Result: Achieves performance close to supervised benchmarks, outperforming existing methods by 3.75% Dice score on breast MRI tumor segmentation.

Conclusion: The method enables effective volumetric segmentation with a single test-time image, validated on breast MRI datasets, and is integrable with nnUNet.

Abstract: Test-time adaptation enables a trained model to adjust to a new domain during
inference, making it particularly valuable in clinical settings where such
on-the-fly adaptation is required. However, existing techniques depend on large
target domain datasets, which are often impractical and unavailable in medical
scenarios that demand per-patient, real-time inference. Moreover, current
methods commonly focus on two-dimensional images, failing to leverage the
volumetric richness of medical imaging data. Bridging this gap, we propose a
Patch-Based Multi-View Co-Training method for Single Image Test-Time
adaptation. Our method enforces feature and prediction consistency through
uncertainty-guided self-training, enabling effective volumetric segmentation in
the target domain with only a single test-time image. Validated on three
publicly available breast magnetic resonance imaging datasets for tumor
segmentation, our method achieves performance close to the upper bound
supervised benchmark while also outperforming all existing state-of-the-art
methods, on average by a Dice Similarity Coefficient of 3.75%. We publicly
share our accessible codebase, readily integrable with the popular nnUNet
framework, at https://github.com/smriti-joshi/muvi.git.

</details>


### [264] [Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion](https://arxiv.org/abs/2506.23711)
*Haoyang Chen,Dongfang Sun,Caoyuan Ma,Shiqin Wang,Kewei Zhang,Zheng Wang,Zhixiang Wang*

Main category: cs.CV

TL;DR: Subjective Camera reconstructs scenes from mental impressions using verbal descriptions and sketches, overcoming language and sketch limitations with a train-free, sequence-aware approach.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with subjective input biases, modality gaps between sketches and 3D priors, and sketch quality issues, often requiring impractical resources or precision.

Method: The framework uses text-reward optimization for appearance priors, sequence-aware disentangled generation, and latent optimization to bridge sketch-3D gaps. It also employs a hierarchical reward-guided system for rough sketches.

Result: The approach achieves state-of-the-art performance in semantic and spatial coherence across diverse datasets.

Conclusion: Subjective Camera effectively translates subjective perceptions into photorealistic images without demanding extensive resources or sketch precision.

Abstract: We propose Subjective Camera, a human-as-imaging-device paradigm that
reconstructs real-world scenes from mental impressions through synergistic use
of verbal descriptions and progressive rough sketches. This approach overcomes
dual limitations of language ambiguity and sketch abstraction by treating the
user's drawing sequence as priors, effectively translating subjective
perceptual expectations into photorealistic images.
  Existing approaches face three fundamental barriers: (1) user-specific
subjective input biases, (2) huge modality gap between planar sketch and 3D
priors in diffusion, and (3) sketch quality-sensitive performance degradation.
Current solutions either demand resource-intensive model adaptation or impose
impractical requirements on sketch precision.
  Our framework addresses these challenges through concept-sequential
generation. (1) We establish robust appearance priors through text-reward
optimization, and then implement sequence-aware disentangled generation that
processes concepts in sketching order; these steps accommodate user-specific
subjective expectation in a train-free way. (2) We employ latent optimization
that effectively bridges the modality gap between planar sketches and 3D priors
in diffusion. (3) Our hierarchical reward-guided framework enables the use of
rough sketches without demanding artistic expertise. Comprehensive evaluation
across diverse datasets demonstrates that our approach achieves
state-of-the-art performance in maintaining both semantic and spatial
coherence.

</details>


### [265] [When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation](https://arxiv.org/abs/2506.23724)
*Chang'an Yi,Xiaohui Deng,Guohao Chen,Yan Zhou,Qinghua Lu,Shuaicheng Niu*

Main category: cs.CV

TL;DR: COCA is a Cross-Model Co-Learning framework for Test-time Adaptation (TTA) that leverages complementary knowledge from multiple models to enhance adaptation performance, even across models of varying sizes.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods focus on single-model adaptation, but cross-model knowledge can improve adaptation by reducing biases and leveraging complementary strengths.

Method: COCA uses two strategies: 1) Co-adaptation integrates knowledge from other models, and 2) Self-adaptation enhances each model's unique strengths via unsupervised learning.

Result: COCA significantly improves adaptation accuracy, e.g., boosting ViT-Base's performance on ImageNet-C from 51.7% to 64.5% with Mobile-ViT's guidance.

Conclusion: COCA demonstrates the value of cross-model co-learning in TTA, offering a plug-and-play solution to enhance existing methods.

Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with
potential domain shifts through online unsupervised learning, yielding
impressive performance. However, to date, existing TTA methods primarily focus
on single-model adaptation. In this work, we investigate an intriguing
question: how does cross-model knowledge influence the TTA process? Our
findings reveal that, in TTA's unsupervised online setting, each model can
provide complementary, confident knowledge to the others, even when there are
substantial differences in model size. For instance, a smaller model like
MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base
(86.6M parameters). In light of this, we propose COCA, a Cross-Model
Co-Learning framework for TTA, which mainly consists of two main strategies. 1)
Co-adaptation adaptively integrates complementary knowledge from other models
throughout the TTA process, reducing individual model biases. 2)
Self-adaptation enhances each model's unique strengths via unsupervised
learning, enabling diverse adaptation to the target domain. Extensive
experiments show that COCA, which can also serve as a plug-and-play module,
significantly boosts existing SOTAs, on models with various sizes--including
ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,
with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy
on ImageNet-C from 51.7% to 64.5%. The code is publicly available at
https://github.com/ycarobot/COCA.

</details>


### [266] [Proteus-ID: ID-Consistent and Motion-Coherent Video Customization](https://arxiv.org/abs/2506.23729)
*Guiyu Zhang,Chen Shi,Zijian Jiang,Xunzhi Xiang,Jingjing Qian,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: Proteus-ID is a diffusion-based framework for video identity customization, addressing identity consistency and motion realism via Multimodal Identity Fusion, Time-Aware Identity Injection, and Adaptive Motion Learning.


<details>
  <summary>Details</summary>
Motivation: The task of synthesizing identity-consistent and motion-coherent videos from a single image and text prompt is challenging due to modality imbalance and unrealistic motion.

Method: Proteus-ID uses a Q-Former for Multimodal Identity Fusion, Time-Aware Identity Injection for dynamic conditioning, and Adaptive Motion Learning for motion realism.

Result: Proteus-ID outperforms prior methods in identity preservation, text alignment, and motion quality, validated on Proteus-Bench (200K clips).

Conclusion: Proteus-ID sets a new benchmark for video identity customization, with publicly available code and data.

Abstract: Video identity customization seeks to synthesize realistic, temporally
coherent videos of a specific subject, given a single reference image and a
text prompt. This task presents two core challenges: (1) maintaining identity
consistency while aligning with the described appearance and actions, and (2)
generating natural, fluid motion without unrealistic stiffness. To address
these challenges, we introduce Proteus-ID, a novel diffusion-based framework
for identity-consistent and motion-coherent video customization. First, we
propose a Multimodal Identity Fusion (MIF) module that unifies visual and
textual cues into a joint identity representation using a Q-Former, providing
coherent guidance to the diffusion model and eliminating modality imbalance.
Second, we present a Time-Aware Identity Injection (TAII) mechanism that
dynamically modulates identity conditioning across denoising steps, improving
fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a
self-supervised strategy that reweights the training loss based on
optical-flow-derived motion heatmaps, enhancing motion realism without
requiring additional inputs. To support this task, we construct Proteus-Bench,
a high-quality dataset comprising 200K curated clips for training and 150
individuals from diverse professions and ethnicities for evaluation. Extensive
experiments demonstrate that Proteus-ID outperforms prior methods in identity
preservation, text alignment, and motion quality, establishing a new benchmark
for video identity customization. Codes and data are publicly available at
https://grenoble-zhang.github.io/Proteus-ID/.

</details>


### [267] [Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?](https://arxiv.org/abs/2506.23751)
*Annika Mütze,Sadia Ilyas,Christian Dörpelkus,Matthias Rottmann*

Main category: cs.CV

TL;DR: The paper explores the limitations of open-vocabulary object detectors using synthetic data generated via Stable Diffusion, revealing overlooked objects and location-based dependencies.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate and identify failure modes of open-vocabulary object detectors in safety-critical applications, where real-world data lacks control.

Method: Two automated pipelines using Stable Diffusion to inpaint unusual objects with high semantic diversity, evaluated on synthetic data derived from LostAndFound and NuImages datasets.

Result: Synthetic data challenges detectors, showing overlooked objects and strong dependence on object location rather than semantics.

Conclusion: Synthetic data provides a systematic way to test and improve open-vocabulary detectors, highlighting location-based biases.

Abstract: Open-vocabulary object detectors such as Grounding DINO are trained on vast
and diverse data, achieving remarkable performance on challenging datasets. Due
to that, it is unclear where to find their limitations, which is of major
concern when using in safety-critical applications. Real-world data does not
provide sufficient control, required for a rigorous evaluation of model
generalization. In contrast, synthetically generated data allows to
systematically explore the boundaries of model competence/generalization. In
this work, we address two research questions: 1) Can we challenge
open-vocabulary object detectors with generated image content? 2) Can we find
systematic failure modes of those models? To address these questions, we design
two automated pipelines using stable diffusion to inpaint unusual objects with
high diversity in semantics, by sampling multiple substantives from WordNet and
ChatGPT. On the synthetically generated data, we evaluate and compare multiple
open-vocabulary object detectors as well as a classical object detector. The
synthetic data is derived from two real-world datasets, namely LostAndFound, a
challenging out-of-distribution (OOD) detection benchmark, and the NuImages
dataset. Our results indicate that inpainting can challenge open-vocabulary
object detectors in terms of overlooking objects. Additionally, we find a
strong dependence of open-vocabulary models on object location, rather than on
object semantics. This provides a systematic approach to challenge
open-vocabulary models and gives valuable insights on how data could be
acquired to effectively improve these models.

</details>


### [268] [Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking](https://arxiv.org/abs/2506.23783)
*Shiao Wang,Ju Huang,Qingchuan Ma,Jinfeng Gao,Chunyi Xu,Xiao Wang,Lan Chen,Bo Jiang*

Main category: cs.CV

TL;DR: Proposes Mamba-FETrack V2, an efficient RGB-Event object tracking framework using Vision Mamba for low-complexity feature extraction and fusion.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal tracking algorithms rely on high-complexity Vision Transformers, causing computational overhead and limiting cross-modal interactions.

Method: Uses a lightweight Prompt Generator and Vision Mamba-based FEMamba backbone for prompt-guided feature extraction and fusion.

Result: Demonstrates superior performance and efficiency on RGB-Event tracking benchmarks like COESOT, FE108, and FELT V2.

Conclusion: Mamba-FETrack V2 offers an efficient and effective solution for RGB-Event object tracking, with plans to release code and models.

Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust
object tracking has garnered increasing attention in recent years. However,
most existing multimodal tracking algorithms depend heavily on high-complexity
Vision Transformer architectures for feature extraction and fusion across
modalities. This not only leads to substantial computational overhead but also
limits the effectiveness of cross-modal interactions. In this paper, we propose
an efficient RGB-Event object tracking framework based on the linear-complexity
Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a
lightweight Prompt Generator that utilizes embedded features from each
modality, together with a shared prompt pool, to dynamically generate
modality-specific learnable prompt vectors. These prompts, along with the
modality-specific embedded features, are then fed into a Vision Mamba-based
FEMamba backbone, which facilitates prompt-guided feature extraction,
cross-modal interaction, and fusion in a unified manner. Finally, the fused
representations are passed to the tracking head for accurate target
localization. Extensive experimental evaluations on multiple RGB-Event tracking
benchmarks, including short-term COESOT dataset and long-term datasets, i.e.,
FE108 and FELT V2, demonstrate the superior performance and efficiency of the
proposed tracking framework. The source code and pre-trained models will be
released on https://github.com/Event-AHU/Mamba_FETrack

</details>


### [269] [Visual Textualization for Image Prompted Object Detection](https://arxiv.org/abs/2506.23785)
*Yongjian Wu,Yang Zhou,Jiya Saiyin,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: VisTex-OVLM enhances object detection in rare categories by projecting visual exemplars into text feature space, preserving pre-trained alignment and improving few-shot performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting rare categories that are hard to describe textually and lack pre-training data.

Method: Uses multi-scale textualizing blocks and multi-stage fusion to integrate visual exemplars into text feature space, maintaining OVLM's original architecture.

Result: Achieves state-of-the-art performance on few-shot benchmarks (PASCAL VOC, MSCOCO) and open-set datasets.

Conclusion: VisTex-OVLM effectively enhances OVLMs for rare category detection without compromising generalization.

Abstract: We propose VisTex-OVLM, a novel image prompted object detection method that
introduces visual textualization -- a process that projects a few visual
exemplars into the text feature space to enhance Object-level Vision-Language
Models' (OVLMs) capability in detecting rare categories that are difficult to
describe textually and nearly absent from their pre-training data, while
preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM
leverages multi-scale textualizing blocks and a multi-stage fusion strategy to
integrate visual information from visual exemplars, generating textualized
visual tokens that effectively guide OVLMs alongside text prompts. Unlike
previous methods, our method maintains the original architecture of OVLM,
maintaining its generalization capabilities while enhancing performance in
few-shot settings. VisTex-OVLM demonstrates superior performance across
open-set datasets which have minimal overlap with OVLM's pre-training data and
achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.
The code will be released at https://github.com/WitGotFlg/VisTex-OVLM.

</details>


### [270] [Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors](https://arxiv.org/abs/2506.23801)
*Ce Wang,Wanjie Sun*

Main category: cs.CV

TL;DR: CRefDiff is a controllable reference-based diffusion model for super-resolution in remote sensing, addressing under-generation and over-reliance on references with a dual-branch fusion mechanism and faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing RefSR methods struggle with real-world complexities like cross-sensor resolution gaps and land cover changes, leading to under-generation or over-reliance on references.

Method: Built on Stable Diffusion, CRefDiff uses a dual-branch fusion mechanism for adaptive integration of reference information and introduces a 'Better Start' strategy to reduce denoising steps.

Result: CRefDiff achieves state-of-the-art performance on the Real-RefRSSRD dataset and improves downstream tasks like scene classification and semantic segmentation.

Conclusion: CRefDiff offers a robust, flexible, and efficient solution for real-world remote sensing super-resolution, supported by a new dataset for future research.

Abstract: Super-resolution (SR) techniques can enhance the spatial resolution of remote
sensing images by utilizing low-resolution (LR) images to reconstruct
high-resolution (HR) images, enabling more efficient large-scale earth
observation applications. While single-image super-resolution (SISR) methods
have shown progress, reference-based super-resolution (RefSR) offers superior
performance by incorporating historical HR images alongside current LR
observations. However, existing RefSR methods struggle with real-world
complexities, such as cross-sensor resolution gap and significant land cover
changes, often leading to under-generation or over-reliance on reference image.
To address these challenges, we propose CRefDiff, a novel controllable
reference-based diffusion model for real-world remote sensing image SR. To
address the under-generation problem, CRefDiff is built upon the pretrained
Stable Diffusion model, leveraging its powerful generative prior to produce
accurate structures and textures. To mitigate over-reliance on the reference,
we introduce a dual-branch fusion mechanism that adaptively integrates both
local and global information from the reference image. Moreover, this novel
dual-branch design enables reference strength control during inference,
enhancing interactivity and flexibility of the model. Finally, a strategy named
Better Start is proposed to significantly reduce the number of denoising steps,
thereby accelerating the inference process. To support further research, we
introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing
images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land
cover changes and significant temporal gaps. Extensive experiments on
Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across
various metrics and improves downstream tasks such as scene classification and
semantic segmentation.

</details>


### [271] [Towards Initialization-free Calibrated Bundle Adjustment](https://arxiv.org/abs/2506.23808)
*Carl Olsson,Amanda Nilsson*

Main category: cs.CV

TL;DR: The paper introduces a method for initialization-free BA using camera calibration to achieve near metric reconstructions, improving accuracy over projective-invariant methods.


<details>
  <summary>Details</summary>
Motivation: Existing initialization-free BA methods using pOSE lack camera calibration, leading to projective ambiguity and requiring more data. The goal is to incorporate calibration for near metric solutions.

Method: The method integrates pairwise relative rotation estimates into the pOSE framework, leveraging camera calibration to encourage similarity-invariant solutions.

Result: Experiments show reliable optimization, global convergence from random starts, and accurate near metric reconstructions.

Conclusion: The approach successfully integrates calibration into pOSE, enabling initialization-free, near metric SfM.

Abstract: A recent series of works has shown that initialization-free BA can be
achieved using pseudo Object Space Error (pOSE) as a surrogate objective. The
initial reconstruction-step optimizes an objective where all terms are
projectively invariant and it cannot incorporate knowledge of the camera
calibration. As a result, the solution is only determined up to a projective
transformation of the scene and the process requires more data for successful
reconstruction.
  In contrast, we present a method that is able to use the known camera
calibration thereby producing near metric solutions, that is, reconstructions
that are accurate up to a similarity transformation. To achieve this we
introduce pairwise relative rotation estimates that carry information about
camera calibration. These are only invariant to similarity transformations,
thus encouraging solutions that preserve metric features of the real scene. Our
method can be seen as integrating rotation averaging into the pOSE framework
striving towards initialization-free calibrated SfM.
  Our experimental evaluation shows that we are able to reliably optimize our
objective, achieving convergence to the global minimum with high probability
from random starting solutions, resulting in accurate near metric
reconstructions.

</details>


### [272] [MadCLIP: Few-shot Medical Anomaly Detection with CLIP](https://arxiv.org/abs/2506.23810)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: A few-shot anomaly detection method using CLIP for medical data, achieving superior performance in anomaly classification and segmentation without synthetic data or memory banks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of few-shot anomaly detection in medical data by leveraging pre-trained CLIP models and improving semantic alignment.

Method: Uses a dual-branch design with learnable adapters in CLIP's vision encoder and learnable text prompts for semantic alignment. Introduces SigLIP loss for handling unpaired text prompts.

Result: Outperforms existing methods in same-dataset and cross-dataset evaluations for anomaly classification and segmentation.

Conclusion: The approach is effective for medical anomaly detection, validated by ablation studies, and does not rely on synthetic data or memory banks.

Abstract: An innovative few-shot anomaly detection approach is presented, leveraging
the pre-trained CLIP model for medical data, and adapting it for both
image-level anomaly classification (AC) and pixel-level anomaly segmentation
(AS). A dual-branch design is proposed to separately capture normal and
abnormal features through learnable adapters in the CLIP vision encoder. To
improve semantic alignment, learnable text prompts are employed to link visual
features. Furthermore, SigLIP loss is applied to effectively handle the
many-to-one relationship between images and unpaired text prompts, showcasing
its adaptation in the medical field for the first time. Our approach is
validated on multiple modalities, demonstrating superior performance over
existing methods for AC and AS, in both same-dataset and cross-dataset
evaluations. Unlike prior work, it does not rely on synthetic data or memory
banks, and an ablation study confirms the contribution of each component. The
code is available at https://github.com/mahshid1998/MadCLIP.

</details>


### [273] [Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model](https://arxiv.org/abs/2506.23822)
*Shiming Chen,Bowen Duan,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: LaZSL is a locally-aligned vision-language model for interpretable zero-shot learning, using optimal transport to align visual regions with attributes, improving interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs like CLIP lack interpretability in zero-shot learning, as they compare entire images to category words without local feature alignment.

Method: LaZSL aligns local visual features with discrete attributes using optimal transport, enabling interpretable similarity without extra training.

Result: The method enhances interpretability, accuracy, and domain generalization, as shown in experiments.

Conclusion: LaZSL provides a scalable and interpretable solution for zero-shot learning by aligning visual regions with attributes effectively.

Abstract: Large-scale vision-language models (VLMs), such as CLIP, have achieved
remarkable success in zero-shot learning (ZSL) by leveraging large-scale
visual-text pair datasets. However, these methods often lack interpretability,
as they compute the similarity between an entire query image and the embedded
category words, making it difficult to explain their predictions. One approach
to address this issue is to develop interpretable models by integrating
language, where classifiers are built using discrete attributes, similar to
human perception. This introduces a new challenge: how to effectively align
local visual features with corresponding attributes based on pre-trained VLMs.
To tackle this, we propose LaZSL, a locally-aligned vision-language model for
interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal
transport to perform interaction between visual regions and their associated
attributes, facilitating effective alignment and providing interpretable
similarity without the need for additional training. Extensive experiments
demonstrate that our method offers several advantages, including enhanced
interpretability, improved accuracy, and strong domain generalization. Codes
available at: https://github.com/shiming-chen/LaZSL.

</details>


### [274] [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2506.23825)
*Haoji Zhang,Yiqin Wang,Yansong Tang,Yong Liu,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: Flash-VStream is an efficient video language model for processing long videos in real time, using a dual-memory module to reduce latency and improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with long videos due to computational and memory inefficiencies, limiting real-world applicability.

Method: Flash-VStream introduces a Flash Memory module with low-capacity context memory for temporal aggregation and high-capacity augmentation memory for spatial retrieval.

Result: The model achieves state-of-the-art performance on benchmarks like EgoSchema and MLVU, with reduced inference latency.

Conclusion: Flash-VStream efficiently handles long videos, offering superior performance and real-time responsiveness.

Abstract: Benefiting from the advances in large language models and cross-modal
alignment, existing multimodal large language models have achieved prominent
performance in image and short video understanding. However, the understanding
of long videos is still challenging, as their long-context nature results in
significant computational and memory overhead. Most existing work treats long
videos in the same way as short videos, which is inefficient for real-world
applications and hard to generalize to even longer videos. To address these
issues, we propose Flash-VStream, an efficient video language model capable of
processing extremely long videos and responding to user queries in real time.
Particularly, we design a Flash Memory module, containing a low-capacity
context memory to aggregate long-context temporal information and model the
distribution of information density, and a high-capacity augmentation memory to
retrieve detailed spatial information based on this distribution. Compared to
existing models, Flash-VStream achieves significant reductions in inference
latency. Extensive experiments on long video benchmarks and comprehensive video
benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate
the state-of-the-art performance and outstanding efficiency of our method. Code
is available at https://github.com/IVGSZ/Flash-VStream.

</details>


### [275] [Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning](https://arxiv.org/abs/2506.23827)
*Mingcheng Qu,Yuncong Wu,Donglin Di,Yue Gao,Tonghua Su,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: NH2ST is a framework for predicting gene expression from pathology images by integrating spatial context and multi-modal data, outperforming existing methods by over 20% in PCC metrics.


<details>
  <summary>Details</summary>
Motivation: Existing methods for predicting gene expression from pathology images lack the ability to capture spatial and molecular interactions, leading to poor performance.

Method: NH2ST uses a query branch and neighbor branch with cross-attention and contrastive learning to integrate target and neighboring patch data, along with pathology and gene modalities.

Result: The model outperforms existing methods on six datasets, achieving over 20% improvement in PCC metrics.

Conclusion: NH2ST effectively addresses the limitations of current methods by leveraging spatial context and multi-modal data for accurate gene expression prediction.

Abstract: Spatial transcriptomics (ST) provides crucial insights into tissue
micro-environments, but is limited to its high cost and complexity. As an
alternative, predicting gene expression from pathology whole slide images (WSI)
is gaining increasing attention. However, existing methods typically rely on
single patches or a single pathology modality, neglecting the complex spatial
and molecular interactions between target and neighboring information (e.g.,
gene co-expression). This leads to a failure in establishing connections among
adjacent regions and capturing intricate cross-modal relationships. To address
these issues, we propose NH2ST, a framework that integrates spatial context and
both pathology and gene modalities for gene expression prediction. Our model
comprises a query branch and a neighbor branch to process paired target patch
and gene data and their neighboring regions, where cross-attention and
contrastive learning are employed to capture intrinsic associations and ensure
alignments between pathology and gene expression. Extensive experiments on six
datasets demonstrate that our model consistently outperforms existing methods,
achieving over 20% in PCC metrics. Codes are available at
https://github.com/MCPathology/NH2ST

</details>


### [276] [Low-latency vision transformers via large-scale multi-head attention](https://arxiv.org/abs/2506.23832)
*Ronit D. Gross,Tal Halevi,Ella Koresh,Yarden Tzach,Ido Kanter*

Main category: cs.CV

TL;DR: The paper explores spontaneous symmetry breaking in multi-head attention (MHA) in transformers, generalizing it to large-scale MHA (LS-MHA) for improved classification accuracy and reduced latency.


<details>
  <summary>Details</summary>
Motivation: To understand and generalize the learning mechanism of MHA in transformers, particularly for classification tasks, and compare it with CNNs.

Method: Quantified single-nodal performance (SNP) and single-head performance (SHP) to analyze MHA behavior, and replaced initial transformer blocks with convolutional layers for efficiency.

Result: Improved classification accuracy via increased SNR, distinct ViT architectures with superior accuracy, and reduced latency without sacrificing performance.

Conclusion: The findings suggest potential applications in NLP and highlight the advantages of combining convolutional layers with transformers for efficiency and accuracy.

Abstract: The emergence of spontaneous symmetry breaking among a few heads of
multi-head attention (MHA) across transformer blocks in classification tasks
was recently demonstrated through the quantification of single-nodal
performance (SNP). This finding indicates that each head focuses its attention
on a subset of labels through cooperation among its SNPs. This underlying
learning mechanism is generalized to large-scale MHA (LS-MHA) using a single
matrix value representing single-head performance (SHP), analogous to
single-filter performance in convolutional neural networks (CNNs). The results
indicate that each SHP matrix comprises multiple unit clusters such that each
label being explicitly recognized by a few heads with negligible noise. This
leads to an increased signal-to-noise ratio (SNR) along the transformer blocks,
thereby improving classification accuracy. These features give rise to several
distinct vision transformer (ViT) architectures that achieve the same accuracy
but differ in their LS-MHA structures. As a result, their soft committee yields
superior accuracy, an outcome not typically observed in CNNs which rely on
hundreds of filters. In addition, a significant reduction in latency is
achieved without affecting the accuracy by replacing the initial transformer
blocks with convolutional layers. This substitution accelerates early-stage
learning, which is then improved by subsequent transformer layers. The
extension of this learning mechanism to natural language processing tasks,
based on quantitative differences between CNNs and ViT architectures, has the
potential to yield new insights in deep learning. The findings are demonstrated
using compact convolutional transformer architectures trained on the CIFAR-100
dataset.

</details>


### [277] [PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric](https://arxiv.org/abs/2506.23833)
*Oscar Ovanger,Ragnar Hauge,Jacob Skauvold,Michael J. Pyrcz,Jo Eidsvik*

Main category: cs.CV

TL;DR: PointSSIM is a resolution-invariant metric for comparing binary images by transforming them into marked point patterns and using anchor points for robust analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of comparing binary images of varying resolutions while maintaining structural integrity.

Method: Transforms binary images into marked point patterns, extracts anchor points via distance transform, and compares using a summary vector of attributes.

Result: Efficient and reliable image comparison, especially for structural analysis across resolutions.

Conclusion: PointSSIM is a promising tool for applications needing resolution-invariant structural image comparison.

Abstract: This paper presents PointSSIM, a novel low-dimensional image-to-image
comparison metric that is resolution invariant. Drawing inspiration from the
structural similarity index measure and mathematical morphology, PointSSIM
enables robust comparison across binary images of varying resolutions by
transforming them into marked point pattern representations. The key features
of the image, referred to as anchor points, are extracted from binary images by
identifying locally adaptive maxima from the minimal distance transform. Image
comparisons are then performed using a summary vector, capturing intensity,
connectivity, complexity, and structural attributes. Results show that this
approach provides an efficient and reliable method for image comparison,
particularly suited to applications requiring structural analysis across
different resolutions.

</details>


### [278] [Refine Any Object in Any Scene](https://arxiv.org/abs/2506.23835)
*Ziwei Chen,Ziling Liu,Zitong Huang,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: RAISE is a 3D enhancement framework that recovers object geometry and appearance in scenes with missing views using generative priors and a two-stage refinement process.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of high-fidelity object-level modeling in scenes with missing viewpoints, which is critical for detailed object understanding.

Method: RAISE uses a 3D generative model to substitute degraded objects with proxies, then refines geometry and texture via pose alignment and registration-constrained enhancement.

Result: RAISE outperforms state-of-the-art methods in novel view synthesis and geometry completion tasks.

Conclusion: RAISE effectively enhances object-level details in scenes with missing views, advancing downstream tasks requiring detailed object modeling.

Abstract: Viewpoint missing of objects is common in scene reconstruction, as camera
paths typically prioritize capturing the overall scene structure rather than
individual objects. This makes it highly challenging to achieve high-fidelity
object-level modeling while maintaining accurate scene-level representation.
Addressing this issue is critical for advancing downstream tasks requiring
detailed object understanding and appearance modeling. In this paper, we
introduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement
framework that leverages 3D generative priors to recover fine-grained object
geometry and appearance under missing views. Starting from substituting
degraded objects with proxies, via a 3D generative model with strong 3D
understanding, RAISE progressively refines geometry and texture by aligning
each proxy to its degraded counterpart in 7-DOF pose, followed by correcting
spatial and appearance inconsistencies via registration-constrained
enhancement. This two-stage refinement ensures the high-fidelity geometry and
appearance of the original object in unseen views while maintaining consistency
in spatial positioning, observed geometry, and appearance. Extensive
experiments on challenging benchmarks show that RAISE significantly outperforms
state-of-the-art methods in both novel view synthesis and geometry completion
tasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.

</details>


### [279] [RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment](https://arxiv.org/abs/2506.23852)
*Jianing Jin,Jiangyong Ying,Huiyu Duan,Liu Yang,Sijing Wu,Yunhao Li,Yushuo Zheng,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces Robotic-Generated Content (RGC) and its unique quality challenges, establishes the first RGC Database (RGCD), and evaluates existing VQA models, revealing their limitations for RGC.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on quality assessment for robotic-generated videos, which differ from PGC and UGC.

Method: Creation of the RGCD with 2,100 videos, subjective VQA experiments, and benchmarking 11 VQA models.

Result: Existing VQA models perform poorly on RGC, indicating a need for specialized models.

Conclusion: The study highlights the necessity for RGC-specific VQA models and provides a public database (RGCD) for future research.

Abstract: As camera-equipped robotic platforms become increasingly integrated into
daily life, robotic-generated videos have begun to appear on streaming media
platforms, enabling us to envision a future where humans and robots coexist. We
innovatively propose the concept of Robotic-Generated Content (RGC) to term
these videos generated from egocentric perspective of robots. The perceptual
quality of RGC videos is critical in human-robot interaction scenarios, and RGC
videos exhibit unique distortions and visual requirements that differ markedly
from those of professionally-generated content (PGC) videos and user-generated
content (UGC) videos. However, dedicated research on quality assessment of RGC
videos is still lacking. To address this gap and to support broader robotic
applications, we establish the first Robotic-Generated Content Database (RGCD),
which contains a total of 2,100 videos drawn from three robot categories and
sourced from diverse platforms. A subjective VQA experiment is conducted
subsequently to assess human visual perception of robotic-generated videos.
Finally, we conduct a benchmark experiment to evaluate the performance of 11
state-of-the-art VQA models on our database. Experimental results reveal
significant limitations in existing VQA models when applied to complex,
robotic-generated content, highlighting a critical need for RGC-specific VQA
models. Our RGCD is publicly available at:
https://github.com/IntMeGroup/RGC-VQA.

</details>


### [280] [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/abs/2506.23854)
*Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Xianpeng Lang*

Main category: cs.CV

TL;DR: HiNeuS is a unified framework for neural surface reconstruction that addresses multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from Eikonal constraints. It introduces differential visibility verification, planar-conformal regularization, and physically-grounded Eikonal relaxation for cohesive integration of appearance-geometry constraints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve persistent challenges in neural surface reconstruction, such as geometric fidelity and photometric consistency under complex scene conditions, by addressing three core limitations in existing approaches.

Method: HiNeuS uses: 1) Differential visibility verification via SDF-guided ray tracing, 2) Planar-conformal regularization with ray-aligned geometry patches, and 3) Physically-grounded Eikonal relaxation to dynamically modulate geometric constraints.

Result: The framework achieves state-of-the-art performance, with a 21.4% reduction in Chamfer distance and 2.32 dB PSNR improvement. It excels in recovering specular instruments, urban layouts, and low-textured surfaces.

Conclusion: HiNeuS successfully integrates appearance-geometry constraints synergistically, demonstrating superior performance and generalizability in neural surface reconstruction and inverse rendering tasks.

Abstract: Neural surface reconstruction faces persistent challenges in reconciling
geometric fidelity with photometric consistency under complex scene conditions.
We present HiNeuS, a unified framework that holistically addresses three core
limitations in existing approaches: multi-view radiance inconsistency, missing
keypoints in textureless regions, and structural degradation from over-enforced
Eikonal constraints during joint optimization. To resolve these issues through
a unified pipeline, we introduce: 1) Differential visibility verification
through SDF-guided ray tracing, resolving reflection ambiguities via continuous
occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry
patches that enforce local surface coherence while preserving sharp edges
through adaptive appearance weighting; and 3) Physically-grounded Eikonal
relaxation that dynamically modulates geometric constraints based on local
radiance gradients, enabling detail preservation without sacrificing global
regularity. Unlike prior methods that handle these aspects through sequential
optimizations or isolated modules, our approach achieves cohesive integration
where appearance-geometry constraints evolve synergistically throughout
training. Comprehensive evaluations across synthetic and real-world datasets
demonstrate state-of-the-art performance, including a 21.4% reduction in
Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement
against neural rendering counterparts. Qualitative analyses reveal superior
capability in recovering specular instruments, urban layouts with
centimeter-scale infrastructure, and low-textured surfaces without local patch
collapse. The method's generalizability is further validated through successful
application to inverse rendering tasks, including material decomposition and
view-consistent relighting.

</details>


### [281] [A Closer Look at Conditional Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2506.23856)
*Ji Zhang,Shihan Wu,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: The paper identifies suboptimal performance in conditional Prompt Tuning (PT) methods using Visual Image Information (VII) and proposes Class-adaptive Prompt Tuning (CaPT) with Textual Class Information (TCI) to solve the Base-New Tradeoff (BNT) problem. CaPT improves existing PT methods and integrates with DePT for state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing conditional PT methods using VII as prompts underperform, even compared to random noise. The paper aims to address the BNT dilemma by leveraging TCI for better generalization.

Method: Proposes CaPT, which learns TCI-conditioned prompts from base classes to adapt to new tasks. It integrates with DePT to form DeCaPT, a superior conditional PT approach.

Result: CaPT consistently improves five unconditional PT baselines across 11 datasets. DeCaPT outperforms the state-of-the-art by 3.49% in H ACC.

Conclusion: TCI-conditioned prompts are key to solving BNT. CaPT and DeCaPT offer efficient, high-performance solutions for PT in vision-language models.

Abstract: Despite the great promise of Prompt Tuning (PT) in adapting large
Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often
struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better
tuned to a base task, their ability to generalize to new tasks diminishes.
Recent work on conditional PT addresses this problem by replacing static
prompts with dynamic Visual Image Information (VII)-conditioned prompts,
improving the model's generalization to new tasks to some extent. In this work,
we first identify a critical issue with existing conditional PT methods: using
VII as the "condition" of prompts yields suboptimal performance, and even
random noise-conditioned prompts can outperform the VII-conditioned
counterparts. On further analysis, we find that learning dynamic prompts
conditioned on Textual Class Information (TCI) is the key to solving the BNT
problem. Motivated by this, we then propose Class-adaptive Prompt Tuning
(CaPT), which enables fast adaptation of tuned models to new classes by
learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be
used as a plugin to mitigate the BNT problem for existing unconditional PT
schemes. Extensive experiments on 11 datasets show that CaPT consistently
improves the performance of five strong unconditional PT baselines with
negligible additional computational cost. Additionally, by integrating CaPT
with our recently proposed DePT framework, we devise a new conditional PT
approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art
conditional PT scheme by 3.49%, averaged over the 11 datasets. Code:
https://github.com/Koorye/CaPT.

</details>


### [282] [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](https://arxiv.org/abs/2506.23858)
*Jianzong Wu,Liang Hou,Haotian Yang,Xin Tao,Ye Tian,Pengfei Wan,Di Zhang,Yunhai Tong*

Main category: cs.CV

TL;DR: VMoBA introduces a sparse attention mechanism for Video Diffusion Models, improving efficiency and performance by adapting to spatio-temporal patterns.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of full attention in VDMs limits long-duration, high-resolution video generation. Existing sparse methods are suboptimal for video data.

Method: VMoBA enhances MoBA with layer-wise block partitioning, global block selection, and threshold-based block selection to optimize spatio-temporal attention.

Result: VMoBA achieves 2.92x FLOPs and 1.48x latency speedup in training, with comparable or better generation quality. It also improves inference efficiency.

Conclusion: VMoBA effectively addresses attention bottlenecks in VDMs, offering significant speedups without sacrificing quality.

Abstract: The quadratic complexity of full attention mechanisms poses a significant
bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,
high-resolution videos. While various sparse attention methods have been
proposed, many are designed as training-free inference accelerators or do not
optimally capture the unique spatio-temporal characteristics inherent in video
data when trained natively. This paper introduces Video Mixture of Block
Attention (VMoBA), a novel sparse attention mechanism specifically adapted for
VDMs. Motivated by an in-depth analysis of attention patterns within
pre-trained video transformers, which revealed strong spatio-temporal locality,
varying query importance, and head-specific concentration levels, VMoBA
enhances the original MoBA framework with three key modifications: (1) a
layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to
diverse spatio-temporal attention patterns and improve efficiency; (2) global
block selection to prioritize the most salient query-key block interactions
across an entire attention head; and (3) threshold-based block selection to
dynamically determine the number of attended blocks based on their cumulative
similarity. Extensive experiments demonstrate that VMoBA significantly
accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and
1.48x latency speedup, while attaining comparable or even superior generation
quality to full attention. Furthermore, VMoBA exhibits competitive performance
in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for
high-res video generation.

</details>


### [283] [Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction](https://arxiv.org/abs/2506.23863)
*Jiahao Ma,Lei Wang,Miaomiao liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: Puzzles is a data augmentation method for multi-view 3D reconstruction, enhancing training data diversity and improving model performance without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DUST3R are limited by training data diversity and scale. Puzzles addresses this by generating synthetic posed video-depth data from minimal input.

Method: Puzzles synthesizes high-quality posed video-depth data by simulating diverse camera trajectories and realistic scene geometry via targeted image transformations.

Result: Models trained with Puzzles on just 10% of original data match the accuracy of those trained on full datasets, boosting performance in existing pipelines.

Conclusion: Puzzles effectively enhances data variety for 3D reconstruction, improving model performance without modifying architectures.

Abstract: Multi-view 3D reconstruction remains a core challenge in computer vision.
Recent methods, such as DUST3R and its successors, directly regress pointmaps
from image pairs without relying on known scene geometry or camera parameters.
However, the performance of these models is constrained by the diversity and
scale of available training data. In this work, we introduce Puzzles, a data
augmentation strategy that synthesizes an unbounded volume of high-quality
posed video-depth data from a single image or video clip. By simulating diverse
camera trajectories and realistic scene geometry through targeted image
transformations, Puzzles significantly enhances data variety. Extensive
experiments show that integrating Puzzles into existing video-based 3D
reconstruction pipelines consistently boosts performance without modifying the
underlying network architecture. Notably, models trained on only ten percent of
the original data augmented with Puzzles still achieve accuracy comparable to
those trained on the full dataset. Code is available at
https://jiahao-ma.github.io/puzzles/.

</details>


### [284] [Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2506.23881)
*Reihaneh Zohrabi,Hosein Hasani,Mahdieh Soleymani Baghshah,Anna Rohrbach,Marcus Rohrbach,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: SPROD is a prototype-based OOD detection method addressing spurious correlations, outperforming existing methods by 4.7% AUROC and 9.3% FPR@95.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detection methods are vulnerable to spurious correlations, compromising robustness.

Method: SPROD refines class prototypes post-hoc to mitigate bias from spurious features without extra data or tuning.

Result: SPROD achieves superior performance on challenging OOD datasets, improving AUROC by 4.7% and FPR@95 by 9.3%.

Conclusion: SPROD effectively addresses spurious correlations in OOD detection, enhancing model reliability.

Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability
and safety of machine learning models in real-world applications, where they
frequently face data distributions unseen during training. Despite progress,
existing methods are often vulnerable to spurious correlations that mislead
models and compromise robustness. To address this, we propose SPROD, a novel
prototype-based OOD detection approach that explicitly addresses the challenge
posed by unknown spurious correlations. Our post-hoc method refines class
prototypes to mitigate bias from spurious features without additional data or
hyperparameter tuning, and is broadly applicable across diverse backbones and
OOD detection settings. We conduct a comprehensive spurious correlation OOD
detection benchmarking, comparing our method against existing approaches and
demonstrating its superior performance across challenging OOD datasets, such as
CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced
Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%
over the second best.

</details>


### [285] [PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View](https://arxiv.org/abs/2506.23897)
*Longliang Liu,Miaojie Feng,Junda Cheng,Jijun Xiang,Xuan Zhu,Xin Yang*

Main category: cs.CV

TL;DR: PriOr-Flow is a dual-branch framework for panoramic optical flow estimation, addressing distortion issues in polar regions using orthogonal views and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Conventional optical flow methods struggle with distortions in panoramic views, especially in polar regions, due to sphere-to-plane projections like ERP.

Method: Proposes PriOr-Flow with a Dual-Cost Collaborative Lookup (DCCL) operator and Ortho-Driven Distortion Compensation (ODDC) module to mitigate distortions and refine motion features.

Result: Achieves state-of-the-art performance on panoramic optical flow datasets, compatible with various perspective-based methods.

Conclusion: PriOr-Flow sets a new benchmark for wide-field motion estimation by effectively addressing distortion challenges in panoramic optical flow.

Abstract: Panoramic optical flow enables a comprehensive understanding of temporal
dynamics across wide fields of view. However, severe distortions caused by
sphere-to-plane projections, such as the equirectangular projection (ERP),
significantly degrade the performance of conventional perspective-based optical
flow methods, especially in polar regions. To address this challenge, we
propose PriOr-Flow, a novel dual-branch framework that leverages the
low-distortion nature of the orthogonal view to enhance optical flow estimation
in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup
(DCCL) operator, which jointly retrieves correlation information from both the
primitive and orthogonal cost volumes, effectively mitigating distortion noise
during cost volume construction. Furthermore, our Ortho-Driven Distortion
Compensation (ODDC) module iteratively refines motion features from both
branches, further suppressing polar distortions. Extensive experiments
demonstrate that PriOr-Flow is compatible with various perspective-based
iterative optical flow methods and consistently achieves state-of-the-art
performance on publicly available panoramic optical flow datasets, setting a
new benchmark for wide-field motion estimation. The code is publicly available
at: https://github.com/longliangLiu/PriOr-Flow.

</details>


### [286] [GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models](https://arxiv.org/abs/2506.23903)
*Hamza Rasaee,Taha Koleilat,Hassan Rivaz*

Main category: cs.CV

TL;DR: A prompt-driven vision-language model (VLM) integrates Grounding DINO with SAM2 for multi-organ ultrasound segmentation, outperforming state-of-the-art methods on seen and unseen datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in ultrasound object segmentation due to anatomical variability, diverse protocols, and limited annotated data.

Method: Uses 18 public ultrasound datasets (15 for fine-tuning Grounding DINO with LoRA, 3 for testing) to train a VLM combining Grounding DINO and SAM2.

Result: Outperforms UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on seen datasets and maintains strong performance on unseen ones without extra fine-tuning.

Conclusion: VLMs show promise for scalable, robust ultrasound analysis, reducing reliance on large annotated datasets.

Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains
a significant challenge due to anatomical variability, diverse imaging
protocols, and limited annotated data. In this study, we propose a
prompt-driven vision-language model (VLM) that integrates Grounding DINO with
SAM2 to enable object segmentation across multiple ultrasound organs. A total
of 18 public ultrasound datasets, encompassing the breast, thyroid, liver,
prostate, kidney, and paraspinal muscle, were utilized. These datasets were
divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank
Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for
testing to evaluate performance in unseen distributions. Comprehensive
experiments demonstrate that our approach outperforms state-of-the-art
segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,
and SAMUS on most seen datasets while maintaining strong performance on unseen
datasets without additional fine-tuning. These results underscore the promise
of VLMs in scalable and robust ultrasound image analysis, reducing dependence
on large, organ-specific annotated datasets. We will publish our code on
code.sonography.ai after acceptance.

</details>


### [287] [Three-dimensional end-to-end deep learning for brain MRI analysis](https://arxiv.org/abs/2506.23916)
*Radhika Juglan,Marta Ligero,Zunamys I. Carrero,Asier Rabasco,Tim Lenz,Leo Misera,Gregory Patrick Veldhuizen,Paul Kuntke,Hagen H. Kitzler,Sven Nebelung,Daniel Truhn,Jakob Nikolas Kather*

Main category: cs.CV

TL;DR: Simpler convolutional networks (SFCN) outperform complex architectures like DenseNet and Swin Transformers in age and sex prediction from brain MRI, showing better generalizability across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Assess generalizability of deep learning methods in brain imaging, focusing on age and sex prediction, as these are key neurobiological markers.

Method: Evaluated three 3D architectures (SFCN, DenseNet, Swin Transformers) using T1-weighted MRI from four cohorts (UKB, DLBS, PPMI, IXI) for age and sex prediction.

Result: SFCN consistently outperformed others, achieving high accuracy (AUC 1.00 for sex in UKB, MAE 2.66 for age) and better generalizability in external datasets.

Conclusion: Simpler networks like SFCN are more generalizable and effective than complex architectures for brain image analysis tasks.

Abstract: Deep learning (DL) methods are increasingly outperforming classical
approaches in brain imaging, yet their generalizability across diverse imaging
cohorts remains inadequately assessed. As age and sex are key neurobiological
markers in clinical neuroscience, influencing brain structure and disease risk,
this study evaluates three of the existing three-dimensional architectures,
namely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window
(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four
independent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study
(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy
controls), and Information eXtraction from Images (IXI, n=319). We found that
SFCN consistently outperformed more complex architectures with AUC of 1.00
[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for
sex classification. For the age prediction task, SFCN demonstrated a mean
absolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across
external datasets. Pairwise DeLong and Wilcoxon signed-rank tests with
Bonferroni corrections confirmed SFCN's superiority over Swin Transformer
across most cohorts (p<0.017, for three comparisons). Explainability analysis
further demonstrates the regional consistency of model attention across cohorts
and specific to each task. Our findings reveal that simpler convolutional
networks outperform the denser and more complex attention-based DL
architectures in brain image analysis by demonstrating better generalizability
across different datasets.

</details>


### [288] [Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers](https://arxiv.org/abs/2506.23918)
*Zhaochen Su,Peng Xia,Hangyu Guo,Zhenhua Liu,Yan Ma,Xiaoye Qu,Jiaqi Liu,Yanshu Li,Kaide Zeng,Zhengyuan Yang,Linjie Li,Yu Cheng,Heng Ji,Junxian He,Yi R.,Fung*

Main category: cs.CV

TL;DR: The paper surveys the shift from text-centric Chain-of-Thought reasoning to a 'think with images' paradigm in AI, outlining its evolution, methods, benchmarks, and future directions.


<details>
  <summary>Details</summary>
Motivation: Address the 'semantic gap' between perceptual data and symbolic thought by enabling AI to dynamically use vision as a cognitive workspace, akin to human cognition.

Method: Proposes a three-stage framework (external tool exploration, programmatic manipulation, intrinsic imagination) and reviews core methods, benchmarks, and applications.

Result: Establishes foundational principles of the 'think with images' paradigm and provides a structured roadmap for future research.

Conclusion: The survey aims to guide development of more powerful, human-aligned multimodal AI by addressing challenges and outlining future directions.

Abstract: Recent progress in multimodal reasoning has been significantly advanced by
textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning
within language. This text-centric approach, however, treats vision as a
static, initial context, creating a fundamental "semantic gap" between rich
perceptual data and discrete symbolic thought. Human cognition often transcends
language, utilizing vision as a dynamic mental sketchpad. A similar evolution
is now unfolding in AI, marking a fundamental paradigm shift from models that
merely think about images to those that can truly think with images. This
emerging paradigm is characterized by models leveraging visual information as
intermediate steps in their thought process, transforming vision from a passive
input into a dynamic, manipulable cognitive workspace. In this survey, we chart
this evolution of intelligence along a trajectory of increasing cognitive
autonomy, which unfolds across three key stages: from external tool
exploration, through programmatic manipulation, to intrinsic imagination. To
structure this rapidly evolving field, our survey makes four key contributions.
(1) We establish the foundational principles of the think with image paradigm
and its three-stage framework. (2) We provide a comprehensive review of the
core methods that characterize each stage of this roadmap. (3) We analyze the
critical landscape of evaluation benchmarks and transformative applications.
(4) We identify significant challenges and outline promising future directions.
By providing this structured overview, we aim to offer a clear roadmap for
future research towards more powerful and human-aligned multimodal AI.

</details>


### [289] [Evaluating the Impact of Khmer Font Types on Text Recognition](https://arxiv.org/abs/2506.23963)
*Vannkinh Nom,Souhail Bakkali,Muhammad Muzzamil Luqman,Mickael Coustaty,Jean-Marc Ogier*

Main category: cs.CV

TL;DR: The study evaluates the impact of 19 Khmer fonts on OCR accuracy, identifying high- and low-performing fonts.


<details>
  <summary>Details</summary>
Motivation: Khmer script's font diversity challenges OCR systems, necessitating evaluation of font impact on recognition accuracy.

Method: Used Pytesseract to assess OCR performance across 19 randomly selected Khmer fonts.

Result: High accuracy fonts: Khmer, Odor MeanChey, Siemreap, Sithi Manuss, Battambang. Low accuracy: iSeth First, Bayon, Dangrek.

Conclusion: Font selection is crucial for optimizing Khmer OCR, guiding future robust system development.

Abstract: Text recognition is significantly influenced by font types, especially for
complex scripts like Khmer. The variety of Khmer fonts, each with its unique
character structure, presents challenges for optical character recognition
(OCR) systems. In this study, we evaluate the impact of 19 randomly selected
Khmer font types on text recognition accuracy using Pytesseract. The fonts
include Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong
Chhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,
Metal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth
First. Our comparison of OCR performance across these fonts reveals that Khmer,
Odor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,
while iSeth First, Bayon, and Dangrek perform poorly. This study underscores
the critical importance of font selection in optimizing Khmer text recognition
and provides valuable insights for developing more robust OCR systems.

</details>


### [290] [Visual and Memory Dual Adapter for Multi-Modal Object Tracking](https://arxiv.org/abs/2506.23972)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: A novel visual and memory dual adapter (VMDA) improves multi-modal tracking by leveraging frequency, spatial, and temporal cues, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-learning-based trackers fail to fully exploit critical cues across frequency and temporal domains, limiting their robustness.

Method: VMDA combines a visual adapter for adaptive feature transfer and a memory adapter for global temporal cue storage and retrieval.

Result: Achieves state-of-the-art performance in RGB-Thermal, RGB-Depth, and RGB-Event tracking tasks.

Conclusion: VMDA enhances multi-modal tracking by effectively integrating cross-domain and temporal cues.

Abstract: Prompt-learning-based multi-modal trackers have achieved promising progress
by employing lightweight visual adapters to incorporate auxiliary modality
features into frozen foundation models. However, existing approaches often
struggle to learn reliable prompts due to limited exploitation of critical cues
across frequency and temporal domains. In this paper, we propose a novel visual
and memory dual adapter (VMDA) to construct more robust and discriminative
representations for multi-modal tracking. Specifically, we develop a simple but
effective visual adapter that adaptively transfers discriminative cues from
auxiliary modality to dominant modality by jointly modeling the frequency,
spatial, and channel-wise features. Additionally, we design the memory adapter
inspired by the human memory mechanism, which stores global temporal cues and
performs dynamic update and retrieval operations to ensure the consistent
propagation of reliable temporal information across video sequences. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,
and RGB-Event tracking. Code and models are available at
https://github.com/xuboyue1999/mmtrack.git.

</details>


### [291] [Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance](https://arxiv.org/abs/2506.23975)
*Yuliia Kaidashova,Bettina Finzel,Ute Schmid*

Main category: cs.CV

TL;DR: The paper explores contrastive explanations for image classification using concept-based methods, analyzing explanation complexity and robustness under image augmentations.


<details>
  <summary>Details</summary>
Motivation: To understand why a classification model prefers one class over another for an input instance by leveraging human-understandable concepts.

Method: Extracts concepts with relevance scores, computes contrasts for similar instances, and evaluates explanation complexity and robustness under image augmentations.

Result: Higher concept relevance leads to shorter, less complex explanations, while lower relevance results in longer, more diffuse ones. Explanations show varying robustness under augmentations.

Conclusion: The findings suggest potential for building more interpretable and robust AI systems through concept-based contrastive explanations.

Abstract: Understanding why a classification model prefers one class over another for
an input instance is the challenge of contrastive explanation. This work
implements concept-based contrastive explanations for image classification by
leveraging the similarity of instance embeddings and relevance of
human-understandable concepts used by a fine-tuned deep learning model. Our
approach extracts concepts with their relevance score, computes contrasts for
similar instances, and evaluates the resulting contrastive explanations based
on explanation complexity. Robustness is tested for different image
augmentations. Two research questions are addressed: (1) whether explanation
complexity varies across different relevance ranges, and (2) whether
explanation complexity remains consistent under image augmentations such as
rotation and noise. The results confirm that for our experiments higher concept
relevance leads to shorter, less complex explanations, while lower relevance
results in longer, more diffuse explanations. Additionally, explanations show
varying degrees of robustness. The discussion of these findings offers insights
into the potential of building more interpretable and robust AI systems.

</details>


### [292] [StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving](https://arxiv.org/abs/2506.23982)
*Ruiyang Hao,Bowen Jing,Haibao Yu,Zaiqing Nie*

Main category: cs.CV

TL;DR: The paper introduces the first large-scale dataset for personalized end-to-end autonomous driving (E2EAD), addressing the lack of annotated driving preferences. It combines static and dynamic features, uses a VLM for annotations, and proposes a benchmark for evaluating personalized E2EAD models.


<details>
  <summary>Details</summary>
Motivation: Personalization in E2EAD is overlooked but crucial for trust and adoption. The lack of annotated datasets hinders progress.

Method: Extracts static and dynamic features, uses a VLM for annotations, and employs human-in-the-loop verification. Proposes a benchmark for personalized E2EAD models.

Result: Incorporating personalized preferences aligns behavior with human driving, demonstrating the value of the dataset and benchmark.

Conclusion: The work provides a foundation for integrating human preferences into E2EAD, fostering future human-centric autonomy research.

Abstract: While personalization has been explored in traditional autonomous driving
systems, it remains largely overlooked in end-to-end autonomous driving
(E2EAD), despite its growing prominence. This gap is critical, as user-aligned
behavior is essential for trust, comfort, and widespread adoption of autonomous
vehicles. A core challenge is the lack of large-scale real-world datasets
annotated with diverse and fine-grained driving preferences, hindering the
development and evaluation of personalized E2EAD models. In this work, we
present the first large-scale real-world dataset enriched with annotations
capturing diverse driving preferences, establishing a foundation for
personalization in E2EAD. We extract static environmental features from
real-world road topology and infer dynamic contextual cues using a fine-tuned
visual language model (VLM), enabling consistent and fine-grained scenario
construction. Based on these scenarios, we derive objective preference
annotations through behavioral distribution analysis and rule-based heuristics.
To address the inherent subjectivity of driving style, we further employ the
VLM to generate subjective annotations by jointly modeling scene semantics and
driver behavior. Final high-quality labels are obtained through a
human-in-the-loop verification process that fuses both perspectives. Building
on this dataset, we propose the first benchmark for evaluating personalized
E2EAD models. We assess several state-of-the-art models with and without
preference conditioning, demonstrating that incorporating personalized
preferences results in behavior more aligned with human driving. Our work lays
the foundation for personalized E2EAD by providing a standardized platform to
systematically integrate human preferences into data-driven E2EAD systems,
catalyzing future research in human-centric autonomy.

</details>


### [293] [Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data](https://arxiv.org/abs/2506.24039)
*Shubhabrata Mukherjee,Jack Lang,Obeen Kwon,Iryna Zenyuk,Valerie Brogden,Adam Weber,Daniela Ushizima*

Main category: cs.CV

TL;DR: Zenesis is a no-code platform for scientific image analysis, outperforming traditional methods with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Zero-shot and prompt-based methods struggle with scarce scientific images, necessitating a solution like Zenesis.

Method: Zenesis uses lightweight multi-modal adaptation, human-in-the-loop refinement, and heuristic-based temporal enhancement.

Result: Achieves high accuracy (0.947-0.987), IOU (0.857-0.858), and Dice scores (0.923) on FIB-SEM data.

Conclusion: Zenesis is a powerful tool for scientific imaging, especially where annotated datasets are lacking.

Abstract: Zero-shot and prompt-based technologies capitalized on using frequently
occurring images to transform visual reasoning tasks, which explains why such
technologies struggle with valuable yet scarce scientific image sets. In this
work, we propose Zenesis, a comprehensive no-code interactive platform designed
to minimize barriers posed by data readiness for scientific images. We develop
lightweight multi-modal adaptation techniques that enable zero-shot operation
on raw scientific data, along with human-in-the-loop refinement and
heuristic-based temporal enhancement options. We demonstrate the performance of
our approach through comprehensive comparison and validation on challenging
Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded
membranes. Zenesis significantly outperforms baseline methods, achieving an
average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a
Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an
IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results
mark a substantial improvement over traditional methods like Otsu thresholding
and even advanced models like Segment Anything Model (SAM) when used in
isolation. Our results demonstrate that Zenesis is a powerful tool for
scientific applications, particularly in fields where high-quality annotated
datasets are unavailable, accelerating accurate analysis of experimental
imaging.

</details>


### [294] [A Survey on Vision-Language-Action Models for Autonomous Driving](https://arxiv.org/abs/2506.24044)
*Sicong Jiang,Zilin Huang,Kangan Qian,Ziang Luo,Tianze Zhu,Yang Zhong,Yihong Tang,Menglin Kong,Yunlong Wang,Siwen Jiao,Hao Ye,Zihao Sheng,Xin Zhao,Tuopu Wen,Zheng Fu,Sikai Chen,Kun Jiang,Diange Yang,Seongjin Choi,Lijun Sun*

Main category: cs.CV

TL;DR: This survey provides a comprehensive overview of Vision-Language-Action (VLA) models for Autonomous Driving (VLA4AD), covering architecture, evolution, model comparisons, datasets, benchmarks, and open challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid progress of MLLMs and their adaptation to autonomous driving has led to fragmented literature, necessitating a consolidated overview to advance interpretable and socially aligned autonomous vehicles.

Method: The survey formalizes architectural building blocks, traces the evolution of VLA models, compares over 20 representative models, and consolidates datasets and benchmarks.

Result: It highlights progress in VLA4AD, identifies protocols for measuring driving safety, accuracy, and explanation quality, and details open challenges like robustness and real-time efficiency.

Conclusion: The survey serves as a concise yet complete reference for future research in VLA4AD, addressing challenges and outlining future directions.

Abstract: The rapid progress of multimodal large language models (MLLM) has paved the
way for Vision-Language-Action (VLA) paradigms, which integrate visual
perception, natural language understanding, and control within a single policy.
Researchers in autonomous driving are actively adapting these methods to the
vehicle domain. Such models promise autonomous vehicles that can interpret
high-level instructions, reason about complex traffic scenes, and make their
own decisions. However, the literature remains fragmented and is rapidly
expanding. This survey offers the first comprehensive overview of VLA for
Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks
shared across recent work, (ii) trace the evolution from early explainer to
reasoning-centric VLA models, and (iii) compare over 20 representative models
according to VLA's progress in the autonomous driving domain. We also
consolidate existing datasets and benchmarks, highlighting protocols that
jointly measure driving safety, accuracy, and explanation quality. Finally, we
detail open challenges - robustness, real-time efficiency, and formal
verification - and outline future directions of VLA4AD. This survey provides a
concise yet complete reference for advancing interpretable socially aligned
autonomous vehicles. Github repo is available at
\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.

</details>


### [295] [Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](https://arxiv.org/abs/2506.24063)
*Deng Li,Aming Wu,Yang Li,Yaowei Wang,Yahong Han*

Main category: cs.CV

TL;DR: The paper proposes a novel method for continual test-time adaptation in object detection, using a dual-path LoRA-based adapter and conditional diffusion-based parameter generation to improve generalization and avoid performance degradation.


<details>
  <summary>Details</summary>
Motivation: Environments change over time and space, challenging object detectors trained on closed-set assumptions. Existing fine-tuning methods can degrade performance due to limited test images.

Method: A dual-path LoRA-based domain-aware adapter disentangles features into domain-invariant and domain-specific components. A conditional diffusion-based mechanism generates adapter parameters, and a class-centered optimal transport alignment mitigates catastrophic forgetting.

Result: Experiments show effectiveness in continuous domain adaptive object detection tasks. Visualization confirms improved object-related information capture and generalization.

Conclusion: The proposed method enhances adaptation and generalization in changing environments while avoiding performance degradation.

Abstract: In practice, environments constantly change over time and space, posing
significant challenges for object detectors trained based on a closed-set
assumption, i.e., training and test data share the same distribution. To this
end, continual test-time adaptation has attracted much attention, aiming to
improve detectors' generalization by fine-tuning a few specific parameters,
e.g., BatchNorm layers. However, based on a small number of test images,
fine-tuning certain parameters may affect the representation ability of other
fixed parameters, leading to performance degradation. Instead, we explore a new
mechanism, i.e., converting the fine-tuning process to a specific-parameter
generation. Particularly, we first design a dual-path LoRA-based domain-aware
adapter that disentangles features into domain-invariant and domain-specific
components, enabling efficient adaptation. Additionally, a conditional
diffusion-based parameter generation mechanism is presented to synthesize the
adapter's parameters based on the current environment, preventing the
optimization from getting stuck in local optima. Finally, we propose a
class-centered optimal transport alignment method to mitigate catastrophic
forgetting. Extensive experiments conducted on various continuous domain
adaptive object detection tasks demonstrate the effectiveness. Meanwhile,
visualization results show that the representation extracted by the generated
parameters can capture more object-related information and strengthen the
generalization ability.

</details>


### [296] [Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention](https://arxiv.org/abs/2506.24085)
*Wonwoong Cho,Yanxia Zhang,Yan-Ying Chen,David I. Inouye*

Main category: cs.CV

TL;DR: IT-Blender, a T2I diffusion adapter, automates blending visual and textual concepts to enhance human creativity, outperforming baselines by addressing gaps in prior works.


<details>
  <summary>Details</summary>
Motivation: Human cross-modal conceptual blending is prone to cognitive biases like design fixation, limiting creativity. Automating this process can augment human creativity.

Method: IT-Blender uses pretrained diffusion models (SD and FLUX) to blend latent representations of a clean reference image with a noisy generated image, employing blended attention for detail preservation and disentanglement.

Result: IT-Blender outperforms baselines in blending visual and textual concepts, demonstrating superior performance.

Conclusion: IT-Blender shows promise in augmenting human creativity by automating cross-modal blending, with potential applications for image generative models.

Abstract: Blending visual and textual concepts into a new visual concept is a unique
and powerful trait of human beings that can fuel creativity. However, in
practice, cross-modal conceptual blending for humans is prone to cognitive
biases, like design fixation, which leads to local minima in the design space.
In this paper, we propose a T2I diffusion adapter "IT-Blender" that can
automate the blending process to enhance human creativity. Prior works related
to cross-modal conceptual blending are limited in encoding a real image without
loss of details or in disentangling the image and text inputs. To address these
gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend
the latent representations of a clean reference image with those of the noisy
generated image. Combined with our novel blended attention, IT-Blender encodes
the real reference image without loss of details and blends the visual concept
with the object specified by the text in a disentangled way. Our experiment
results show that IT-Blender outperforms the baselines by a large margin in
blending visual and textual concepts, shedding light on the new application of
image generative models to augment human creativity.

</details>


### [297] [WaRA: Wavelet Low Rank Adaptation](https://arxiv.org/abs/2506.24092)
*Moein Heidari,Yasamin Medghalchi,Mahdi Khoursha,Reza Rezaeian,Ilker Hacihaliloglu*

Main category: cs.CV

TL;DR: WaRA introduces a wavelet-based PEFT method for multi-resolution weight updates, outperforming LoRA in vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA use global low-rank factorizations, missing local or multi-scale structures in weight updates.

Method: WaRA uses wavelet transforms to decompose weight updates into multi-resolution representations, enabling low-rank factorization in the wavelet domain.

Result: WaRA excels in vision tasks (image generation, classification, segmentation) and shows promise in language tasks, improving quality and reducing complexity.

Conclusion: WaRA is a versatile PEFT method with superior performance and broader applicability than standard LoRA.

Abstract: Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across
various applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its
extensions have emerged as particularly effective, allowing efficient model
adaptation while significantly reducing computational overhead. However,
existing approaches typically rely on global low-rank factorizations, which
overlook local or multi-scale structure, failing to capture complex patterns in
the weight updates. To address this, we propose WaRA, a novel PEFT method that
leverages wavelet transforms to decompose the weight update matrix into a
multi-resolution representation. By performing low-rank factorization in the
wavelet domain and reconstructing updates through an inverse transform, WaRA
obtains compressed adaptation parameters that harness multi-resolution
analysis, enabling it to capture both coarse and fine-grained features while
providing greater flexibility and sparser representations than standard LoRA.
Through comprehensive experiments and analysis, we demonstrate that WaRA
performs superior on diverse vision tasks, including image generation,
classification, and semantic segmentation, significantly enhancing generated
image quality while reducing computational complexity. Although WaRA was
primarily designed for vision tasks, we further showcase its effectiveness in
language tasks, highlighting its broader applicability and generalizability.
The code is publicly available at
\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.

</details>


### [298] [MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction](https://arxiv.org/abs/2506.24096)
*Antoine Guédon,Diego Gomez,Nissim Maruani,Bingchen Gong,George Drettakis,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: MILo introduces a differentiable Gaussian Splatting framework to extract accurate surface meshes directly from 3D Gaussians, avoiding costly post-processing and preserving geometric details.


<details>
  <summary>Details</summary>
Motivation: Current methods for extracting surface meshes from Gaussian Splatting lose fine details or produce dense meshes, limiting downstream applications.

Method: MILo uses a differentiable procedure to construct meshes from Gaussians, featuring bidirectional consistency, adaptive mesh extraction, and signed distance computation.

Result: The method reconstructs high-quality scenes with fewer vertices, suitable for physics simulations and animation.

Conclusion: MILo bridges volumetric and surface representations, offering efficient and precise mesh extraction.

Abstract: While recent advances in Gaussian Splatting have enabled fast reconstruction
of high-quality 3D scenes from images, extracting accurate surface meshes
remains a challenge. Current approaches extract the surface through costly
post-processing steps, resulting in the loss of fine geometric details or
requiring significant time and leading to very dense meshes with millions of
vertices. More fundamentally, the a posteriori conversion from a volumetric to
a surface representation limits the ability of the final mesh to preserve all
geometric structures captured during training. We present MILo, a novel
Gaussian Splatting framework that bridges the gap between volumetric and
surface representations by differentiably extracting a mesh from the 3D
Gaussians. We design a fully differentiable procedure that constructs the
mesh-including both vertex locations and connectivity-at every iteration
directly from the parameters of the Gaussians, which are the only quantities
optimized during training. Our method introduces three key technical
contributions: a bidirectional consistency framework ensuring both
representations-Gaussians and the extracted mesh-capture the same underlying
geometry during training; an adaptive mesh extraction process performed at each
training iteration, which uses Gaussians as differentiable pivots for Delaunay
triangulation; a novel method for computing signed distance values from the 3D
Gaussians that enables precise surface extraction while avoiding geometric
erosion. Our approach can reconstruct complete scenes, including backgrounds,
with state-of-the-art quality while requiring an order of magnitude fewer mesh
vertices than previous methods. Due to their light weight and empty interior,
our meshes are well suited for downstream applications such as physics
simulations or animation.

</details>


### [299] [DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World](https://arxiv.org/abs/2506.24102)
*Xiangtai Li,Tao Zhang,Yanwei Li,Haobo Yuan,Shihao Chen,Yikang Zhou,Jiahao Meng,Yueyi Sun,Shilin Xu,Lu Qi,Tianheng Cheng,Yi Lin,Zilong Huang,Wenhao Huang,Jiashi Feng,Guang Shi*

Main category: cs.CV

TL;DR: DenseWorld-1M is a new dataset addressing gaps in existing caption datasets by providing detailed, dense grounded captions for high-resolution images, using a three-stage labeling pipeline and two VLM models.


<details>
  <summary>Details</summary>
Motivation: Existing caption datasets lack ground locations, relations, and detailed descriptions, especially for high-resolution images.

Method: A three-stage labeling pipeline (open-world perception, detailed object caption generation, dense caption merging) and two VLM models (Detailed Region Caption model, Spatial Caption Merging model) are introduced.

Result: The dataset and models show effectiveness in vision-language understanding, visual grounding, and region caption generation tasks.

Conclusion: DenseWorld-1M fills a critical gap in multimodal datasets, enhancing capabilities for MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate a complex understanding
of scenes, benefiting from large-scale and high-quality datasets. Most existing
caption datasets lack the ground locations and relations for visual entities.
Several grounded caption datasets face the problems of missing detailed
descriptions, relations, and massive object descriptions on high-resolution
images. To fill this gap for the community, we present DenseWorld-1M, the first
massive, detailed, dense grounded caption dataset in the real world. We design
a three-stage labeling pipeline, containing open-world perception, detailed
object caption generation, and dense caption merging. The first stage obtains
entity-level masks and labels. The second stage generates the object-level,
detailed captions with the guidance of masks and labels from the first stage.
The final stage merges object captions and masks into spatial and relational
dense captions. To accelerate the labeling process and improve caption quality,
we present two VLM models: the Detailed Region Caption model and the Spatial
Caption Merging model. Extensive experiments on various settings, including
vision-language understanding, visual grounding, and region caption generation,
demonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.

</details>


### [300] [Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/abs/2506.24113)
*Kaiwen Zhang,Zhenyu Tang,Xiaotao Hu,Xingang Pan,Xiaoyang Guo,Yuan Liu,Jingwei Huang,Li Yuan,Qian Zhang,Xiao-Xiao Long,Xun Cao,Wei Yin*

Main category: cs.CV

TL;DR: Epona introduces an autoregressive diffusion world model for autonomous driving, improving long-horizon predictions and integrating trajectory planning via decoupled spatiotemporal factorization and modular prediction.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion-based world models lack flexibility for long-horizon predictions and struggle to integrate trajectory planning, limiting their use in autonomous driving.

Method: Epona uses decoupled spatiotemporal factorization and modular trajectory-video prediction, along with a chain-of-forward training strategy to reduce autoregressive errors.

Result: Achieves 7.4% FVD improvement and longer prediction durations, outperforming prior works and serving as a real-time motion planner.

Conclusion: Epona advances world modeling for autonomous driving by enabling localized spatiotemporal distribution modeling and seamless integration of planning.

Abstract: Diffusion models have demonstrated exceptional visual quality in video
generation, making them promising for autonomous driving world modeling.
However, existing video diffusion-based world models struggle with
flexible-length, long-horizon predictions and integrating trajectory planning.
This is because conventional video diffusion models rely on global joint
distribution modeling of fixed-length frame sequences rather than sequentially
constructing localized distributions at each timestep. In this work, we propose
Epona, an autoregressive diffusion world model that enables localized
spatiotemporal distribution modeling through two key innovations: 1) Decoupled
spatiotemporal factorization that separates temporal dynamics modeling from
fine-grained future world generation, and 2) Modular trajectory and video
prediction that seamlessly integrate motion planning with visual modeling in an
end-to-end framework. Our architecture enables high-resolution, long-duration
generation while introducing a novel chain-of-forward training strategy to
address error accumulation in autoregressive loops. Experimental results
demonstrate state-of-the-art performance with 7.4\% FVD improvement and minutes
longer prediction duration compared to prior works. The learned world model
further serves as a real-time motion planner, outperforming strong end-to-end
planners on NAVSIM benchmarks. Code will be publicly available at
\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.

</details>


### [301] [TextMesh4D: High-Quality Text-to-4D Mesh Generation](https://arxiv.org/abs/2506.24121)
*Sisi Dai,Xinxin Su,Boyan Wan,Ruizhen Hu,Kai Xu*

Main category: cs.CV

TL;DR: TextMesh4D introduces a novel framework for high-quality text-to-4D generation, leveraging per-face Jacobians and a two-stage process for static and dynamic synthesis, achieving state-of-the-art results with low GPU overhead.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the unexplored challenge of dynamic 3D content generation (text-to-4D) using diffusion models, aiming to advance beyond static content creation.

Method: The approach uses per-face Jacobians as a differentiable mesh representation, decomposing 4D generation into static object creation and dynamic motion synthesis, with a flexibility-rigidity regularization term for stability.

Result: TextMesh4D achieves state-of-the-art performance in temporal consistency, structural fidelity, and visual realism, while operating efficiently on a single 24GB GPU.

Conclusion: TextMesh4D offers a cost-effective, high-quality solution for text-to-4D generation, with code release to support future research.

Abstract: Recent advancements in diffusion generative models significantly advanced
image, video, and 3D content creation from user-provided text prompts. However,
the challenging problem of dynamic 3D content generation (text-to-4D) with
diffusion guidance remains largely unexplored. In this paper, we introduce
TextMesh4D, a novel framework for high-quality text-to-4D generation. Our
approach leverages per-face Jacobians as a differentiable mesh representation
and decomposes 4D generation into two stages: static object creation and
dynamic motion synthesis. We further propose a flexibility-rigidity
regularization term to stabilize Jacobian optimization under video diffusion
priors, ensuring robust geometric performance. Experiments demonstrate that
TextMesh4D achieves state-of-the-art results in terms of temporal consistency,
structural fidelity, and visual realism. Moreover, TextMesh4D operates with a
low GPU memory overhead-requiring only a single 24GB GPU-offering a
cost-effective yet high-quality solution for text-driven 4D mesh generation.
The code will be released to facilitate future research in text-to-4D
generation.

</details>


### [302] [Calligrapher: Freestyle Text Image Customization](https://arxiv.org/abs/2506.24123)
*Yue Ma,Qingyan Bai,Hao Ouyang,Ka Leong Cheng,Qiuyu Wang,Hongyu Liu,Zichen Liu,Haofan Wang,Jingye Chen,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: Calligrapher is a diffusion-based framework for digital calligraphy, combining text customization and artistic typography with self-distillation, localized style injection, and in-context generation for precise style control.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in typographic customization, such as precise style control and data dependency, to enhance digital calligraphy and design applications.

Method: Uses self-distillation for benchmark creation, a trainable style encoder for feature extraction, and in-context generation for style alignment.

Result: Accurately reproduces stylistic details and glyph positioning, outperforming traditional models in diverse design contexts.

Conclusion: Calligrapher automates high-quality typography, benefiting digital art, branding, and design with consistent and precise results.

Abstract: We introduce Calligrapher, a novel diffusion-based framework that
innovatively integrates advanced text customization with artistic typography
for digital calligraphy and design applications. Addressing the challenges of
precise style control and data dependency in typographic customization, our
framework incorporates three key technical contributions. First, we develop a
self-distillation mechanism that leverages the pre-trained text-to-image
generative model itself alongside the large language model to automatically
construct a style-centric typography benchmark. Second, we introduce a
localized style injection framework via a trainable style encoder, which
comprises both Qformer and linear layers, to extract robust style features from
reference images. An in-context generation mechanism is also employed to
directly embed reference images into the denoising process, further enhancing
the refined alignment of target styles. Extensive quantitative and qualitative
evaluations across diverse fonts and design contexts confirm Calligrapher's
accurate reproduction of intricate stylistic details and precise glyph
positioning. By automating high-quality, visually consistent typography,
Calligrapher surpasses traditional models, empowering creative practitioners in
digital art, branding, and contextual typographic design.

</details>


### [303] [FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation](https://arxiv.org/abs/2506.24125)
*Jiacheng Cui,Xinyue Bi,Yaxin Luo,Xiaohan Zhao,Jiacheng Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: The paper introduces Data Residual Matching (FADRM) for dataset distillation, improving efficiency and accuracy by leveraging data-level skip connections and optimization refinements.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of residual connections in data-centric approaches, particularly for mitigating data information vanishing and enhancing dataset distillation.

Method: Proposes Data Residual Matching, combining data-level skip connections with pixel space optimization to balance new knowledge and core local information. Includes optimization-level refinements for computational efficiency.

Result: Achieves 47.7% test accuracy on ImageNet-1K (single-model) and 50.0% (multi-model), surpassing RDED by +5.7% and outperforming EDC and CV-DD by +1.4% and +4.0%. Reduces training time and GPU memory usage by 50%.

Conclusion: FADRM sets a new state-of-the-art in dataset distillation, demonstrating significant improvements in both efficiency and effectiveness across benchmarks.

Abstract: Residual connection has been extensively studied and widely applied at the
model architecture level. However, its potential in the more challenging
data-centric approaches remains unexplored. In this work, we introduce the
concept of Data Residual Matching for the first time, leveraging data-level
skip connections to facilitate data generation and mitigate data information
vanishing. This approach maintains a balance between newly acquired knowledge
through pixel space optimization and existing core local information
identification within raw data modalities, specifically for the dataset
distillation task. Furthermore, by incorporating optimization-level
refinements, our method significantly improves computational efficiency,
achieving superior performance while reducing training time and peak GPU memory
usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual
Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art,
demonstrating substantial improvements over existing methods across multiple
dataset benchmarks in both efficiency and effectiveness. For instance, with
ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the
method achieves 47.7% test accuracy in single-model dataset distillation and
50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and
outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%
and +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.

</details>


### [304] [How to Design and Train Your Implicit Neural Representation for Video Compression](https://arxiv.org/abs/2506.24127)
*Matthew Gwilliam,Roy Zhang,Namitha Padmanabhan,Hongyang Du,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: The paper introduces Rabbit NeRV (RNeRV), a state-of-the-art implicit neural representation (INR) method for video compression, improving PSNR by 1.27% over alternatives. It also explores hyper-networks for real-time encoding, achieving quality improvements with minimal parameter increases.


<details>
  <summary>Details</summary>
Motivation: Current INR methods for video compression suffer from slow encoding speeds due to per-sample network training, limiting practical adoption. The study aims to improve both quality and encoding efficiency.

Method: The authors develop a library to analyze NeRV family methods, propose RNeRV for better performance, and investigate hyper-networks for real-time encoding by masking predicted INR weights.

Result: RNeRV outperforms alternatives by +1.27% PSNR. Hyper-network experiments yield 1.7% PSNR/MS-SSIM improvements at 0.037 bpp and further gains with slight parameter increases.

Conclusion: The study advances INR-based video compression by optimizing design principles and introducing hyper-networks for real-time encoding, demonstrating significant quality and efficiency improvements.

Abstract: Implicit neural representation (INR) methods for video compression have
recently achieved visual quality and compression ratios that are competitive
with traditional pipelines. However, due to the need for per-sample network
training, the encoding speeds of these methods are too slow for practical
adoption. We develop a library to allow us to disentangle and review the
components of methods from the NeRV family, reframing their performance in
terms of not only size-quality trade-offs, but also impacts on training time.
We uncover principles for effective video INR design and propose a
state-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When
all methods are given equal training time (equivalent to 300 NeRV epochs) for 7
different UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared
to the best-performing alternative for each video in our NeRV library. We then
tackle the encoding speed issue head-on by investigating the viability of
hyper-networks, which predict INR weights from video inputs, to disentangle
training from encoding to allow for real-time encoding. We propose masking the
weights of the predicted INR during training to allow for variable, higher
quality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at
0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by
0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar
speeds. Our project website is available at https://mgwillia.github.io/vinrb/
and our code is available at https://github.com/mgwillia/vinrb.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [305] [High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning](https://arxiv.org/abs/2506.22532)
*Mark Wrobel,Michele Pascale,Tina Yao,Ruaraidh Campbell,Elena Milano,Michael Quail,Jennifer Steeden,Vivek Muthurangu*

Main category: eess.IV

TL;DR: A DL-based method transforms 2D real-time cine images into 3D cine datasets for pediatric and congenital heart disease, showing good agreement with conventional CMR.


<details>
  <summary>Details</summary>
Motivation: To streamline CMR by reducing acquisition and reconstruction times while maintaining accuracy in pediatric and congenital heart disease assessments.

Method: Four DL models were trained for contrast correction, motion correction, super-resolution, and segmentation, validated on 10 patients.

Result: Successful 3D cine creation in <1 min, with no significant biases in LV/RV metrics and reasonable agreement for vessel diameters.

Conclusion: The method shows promise for speeding up CMR in clinical practice with accurate, fully segmented 3D cine datasets.

Abstract: Background: Conventional cardiovascular magnetic resonance (CMR) in
paediatric and congenital heart disease uses 2D, breath-hold, balanced steady
state free precession (bSSFP) cine imaging for assessment of function and
cardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for
anatomical assessment. Our aim is to concatenate a stack 2D free-breathing
real-time cines and use Deep Learning (DL) to create an isotropic a fully
segmented 3D cine dataset from these images. Methods: Four DL models were
trained on open-source data that performed: a) Interslice contrast correction;
b) Interslice respiratory motion correction; c) Super-resolution (slice
direction); and d) Segmentation of right and left atria and ventricles (RA, LA,
RV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients
undergoing routine cardiovascular examination, our method was validated on
prospectively acquired sagittal stacks of real-time cine images. Quantitative
metrics (ventricular volumes and vessel diameters) and image quality of the 3D
cines were compared to conventional breath hold cine and whole heart imaging.
Results: All real-time data were successfully transformed into 3D cines with a
total post-processing time of <1 min in all cases. There were no significant
biases in any LV or RV metrics with reasonable limits of agreement and
correlation. There is also reasonable agreement for all vessel diameters,
although there was a small but significant overestimation of RPA diameter.
Conclusion: We have demonstrated the potential of creating a 3D-cine data from
concatenated 2D real-time cine images using a series of DL models. Our method
has short acquisition and reconstruction times with fully segmented data being
available within 2 minutes. The good agreement with conventional imaging
suggests that our method could help to significantly speed up CMR in clinical
practice.

</details>


### [306] [FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation](https://arxiv.org/abs/2506.22580)
*Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni*

Main category: eess.IV

TL;DR: FedCLAM improves federated learning for medical imaging by adapting to client-specific conditions and addressing intensity discrepancies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Medical imaging faces challenges like device and population diversity, reducing federated learning model effectiveness. Existing methods lack adaptability.

Method: FedCLAM uses client-adaptive momentum, personalized dampening, and intensity alignment loss to handle heterogeneity.

Result: FedCLAM outperforms eight state-of-the-art methods in medical segmentation tasks.

Conclusion: FedCLAM effectively addresses federated learning challenges in medical imaging, offering superior performance and adaptability.

Abstract: Federated learning is a decentralized training approach that keeps data under
stakeholder control while achieving superior performance over isolated
training. While inter-institutional feature discrepancies pose a challenge in
all federated settings, medical imaging is particularly affected due to diverse
imaging devices and population variances, which can diminish the global model's
effectiveness. Existing aggregation methods generally fail to adapt across
varied circumstances. To address this, we propose FedCLAM, which integrates
\textit{client-adaptive momentum} terms derived from each client's loss
reduction during local training, as well as a \textit{personalized dampening
factor} to curb overfitting. We further introduce a novel \textit{intensity
alignment} loss that matches predicted and ground-truth foreground
distributions to handle heterogeneous image intensity profiles across
institutions and devices. Extensive evaluations on two datasets show that
FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,
underscoring its efficacy. The code is available at
https://github.com/siomvas/FedCLAM.

</details>


### [307] [ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge](https://arxiv.org/abs/2506.22790)
*Yixu Chen,Bowen Chen,Hai Wei,Alan C. Bovik,Baojun Li,Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Dounia Hammou,Fei Yin,Rafal Mantiuk,Amritha Premkumar,Prajit T Rajendran,Vignesh V Menon*

Main category: eess.IV

TL;DR: The paper discusses a challenge on generalizable HDR and SDR video quality measurement, highlighting the need for robust VQA methods. Five teams submitted models, with four outperforming the VMAF baseline and one achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The rapid development of HDR and SDR video technology necessitates robust and generalizable VQA methods, as existing models struggle with varying dynamic ranges and distortion types.

Method: A challenge was set to benchmark VQA approaches for jointly handling HDR and SDR content. Five teams submitted seven models for Full Reference (FR) and No Reference (NR) tracks.

Result: Four methods outperformed the VMAF baseline, with the top model achieving state-of-the-art performance, setting a new benchmark.

Conclusion: The challenge successfully promoted advancements in generalizable video quality assessment, with top models demonstrating superior performance over existing baselines.

Abstract: This paper reports IEEE International Conference on Multimedia \& Expo (ICME)
2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.
With the rapid development of video technology, especially High Dynamic Range
(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and
generalizable Video Quality Assessment (VQA) methods has become increasingly
demanded. Existing VQA models often struggle to deliver consistent performance
across varying dynamic ranges, distortion types, and diverse content. This
challenge was established to benchmark and promote VQA approaches capable of
jointly handling HDR and SDR content. In the final evaluation phase, five teams
submitted seven models along with technical reports to the Full Reference (FR)
and No Reference (NR) tracks. Among them, four methods outperformed VMAF
baseline, while the top-performing model achieved state-of-the-art performance,
setting a new benchmark for generalizable video quality assessment.

</details>


### [308] [CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation](https://arxiv.org/abs/2506.22882)
*Qilong Xing,Zikai Song,Yuteng Ye,Yuke Chen,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: eess.IV

TL;DR: CA-Diff integrates anatomical features into diffusion models for more accurate brain MRI segmentation, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing CNN and transformer-based methods struggle with complex brain structures, and current diffusion models neglect anatomical information.

Method: Proposes CA-Diff, using distance fields for spatial context, a collaborative diffusion process, consistency loss, and a time-adapted channel attention module.

Result: CA-Diff outperforms state-of-the-art methods in brain MRI segmentation.

Conclusion: CA-Diff effectively leverages anatomical features for improved segmentation accuracy.

Abstract: Segmentation of brain structures from MRI is crucial for evaluating brain
morphology, yet existing CNN and transformer-based methods struggle to
delineate complex structures accurately. While current diffusion models have
shown promise in image segmentation, they are inadequate when applied directly
to brain MRI due to neglecting anatomical information. To address this, we
propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating
spatial anatomical features to enhance segmentation accuracy of the diffusion
model. Specifically, we introduce distance field as an auxiliary anatomical
condition to provide global spatial context, alongside a collaborative
diffusion process to model its joint distribution with anatomical structures,
enabling effective utilization of anatomical features for segmentation.
Furthermore, we introduce a consistency loss to refine relationships between
the distance field and anatomical structures and design a time adapted channel
attention module to enhance the U-Net feature fusion procedure. Extensive
experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.

</details>


### [309] [Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization](https://arxiv.org/abs/2506.22952)
*Yanwu Yang,Thomas Wolfers*

Main category: eess.IV

TL;DR: The paper introduces HST, a hierarchical state space-based tokenization network, to quantify brain dynamics and transitions using a refined VQ-VAE, improving metastability analysis and representation learning.


<details>
  <summary>Details</summary>
Motivation: Understanding brain dynamics via fMRI is challenging, especially in capturing transitions between functional states. Existing methods lack transition dependencies and stable embeddings.

Method: Proposes HST, a hierarchical state space-based tokenization network, using a refined VQ-VAE with quantization error feedback and clustering for better quantization and metastability.

Result: Validated on two fMRI datasets, HST effectively quantifies hierarchical brain dynamics and shows potential in disease diagnosis and reconstruction.

Conclusion: HST provides a promising framework for analyzing brain metastability and dynamics, with applications in neuroscience and diagnostics.

Abstract: Understanding brain dynamics through functional Magnetic Resonance Imaging
(fMRI) remains a fundamental challenge in neuroscience, particularly in
capturing how the brain transitions between various functional states.
Recently, metastability, which refers to temporarily stable brain states, has
offered a promising paradigm to quantify complex brain signals into
interpretable, discretized representations. In particular, compared to
cluster-based machine learning approaches, tokenization approaches leveraging
vector quantization have shown promise in representation learning with powerful
reconstruction and predictive capabilities. However, most existing methods
ignore brain transition dependencies and lack a quantification of brain
dynamics into representative and stable embeddings. In this study, we propose a
Hierarchical State space-based Tokenization network, termed HST, which
quantizes brain states and transitions in a hierarchical structure based on a
state space-based model. We introduce a refined clustered Vector-Quantization
Variational AutoEncoder (VQ-VAE) that incorporates quantization error feedback
and clustering to improve quantization performance while facilitating
metastability with representative and stable token representations. We validate
our HST on two public fMRI datasets, demonstrating its effectiveness in
quantifying the hierarchical dynamics of the brain and its potential in disease
diagnosis and reconstruction performance. Our method offers a promising
framework for the characterization of brain dynamics, facilitating the analysis
of metastability.

</details>


### [310] [MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation](https://arxiv.org/abs/2506.23102)
*Sunggu Kyung,Jinyoung Seo,Hyunseok Lim,Dongyeong Kim,Hyungbin Park,Jimin Sung,Jihyun Kim,Wooyoung Jo,Yoojin Nam,Namkug Kim*

Main category: eess.IV

TL;DR: MedRegion-CT is a region-focused MLLM framework for CT report generation, improving on global-feature methods by capturing region-specific details through innovative token pooling, segmentation, and patient-specific attributions.


<details>
  <summary>Details</summary>
Motivation: Existing CT report generation methods focus on global features, missing region-specific details, which can lead to unnoticed abnormalities.

Method: Proposes MedRegion-CT with three innovations: 1) $R^2$ Token Pooling for 3D CT feature extraction, 2) universal segmentation for region-centric features, and 3) patient-specific attributions as text prompts.

Result: Achieves state-of-the-art performance on RadGenome-Chest CT, excelling in natural language generation quality and clinical relevance.

Conclusion: MedRegion-CT enhances CT report generation by focusing on region-specific details, improving accuracy and interpretability.

Abstract: The recent release of RadGenome-Chest CT has significantly advanced CT-based
report generation. However, existing methods primarily focus on global
features, making it challenging to capture region-specific details, which may
cause certain abnormalities to go unnoticed. To address this, we propose
MedRegion-CT, a region-focused Multi-Modal Large Language Model (MLLM)
framework, featuring three key innovations. First, we introduce Region
Representative ($R^2$) Token Pooling, which utilizes a 2D-wise pretrained
vision model to efficiently extract 3D CT features. This approach generates
global tokens representing overall slice features and region tokens
highlighting target areas, enabling the MLLM to process comprehensive
information effectively. Second, a universal segmentation model generates
pseudo-masks, which are then processed by a mask encoder to extract
region-centric features. This allows the MLLM to focus on clinically relevant
regions, using six predefined region masks. Third, we leverage segmentation
results to extract patient-specific attributions, including organ size,
diameter, and locations. These are converted into text prompts, enriching the
MLLM's understanding of patient-specific contexts. To ensure rigorous
evaluation, we conducted benchmark experiments on report generation using the
RadGenome-Chest CT. MedRegion-CT achieved state-of-the-art performance,
outperforming existing methods in natural language generation quality and
clinical relevance while maintaining interpretability. The code for our
framework is publicly available.

</details>


### [311] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Main category: eess.IV

TL;DR: CRISP-SAM2 is a novel multi-organ medical segmentation model that improves accuracy by leveraging cross-modal interaction and semantic prompting, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current multi-organ segmentation models struggle with inaccurate details, reliance on geometric prompts, and loss of spatial information.

Method: The model uses cross-modal contextualized semantics, semantic prompting, and memory self-updating to enhance segmentation.

Result: CRISP-SAM2 outperforms existing models on seven public datasets, showing superior performance.

Conclusion: The method effectively addresses key limitations in multi-organ segmentation, offering a promising solution.

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [312] [Score-based Diffusion Model for Unpaired Virtual Histology Staining](https://arxiv.org/abs/2506.23184)
*Anran Liu,Xiaofei Wang,Jing Cai,Chao Li*

Main category: eess.IV

TL;DR: A mutual-information-guided diffusion model for virtual staining of H&E to IHC images, addressing challenges like style decomposition, controllable staining, and structural consistency.


<details>
  <summary>Details</summary>
Motivation: H&E staining lacks specificity for diagnostic markers, while IHC is limited by tissue and antibody constraints. Virtual staining offers a computational solution.

Method: Proposes a mutual-information-guided score-based diffusion model with global and local MI strategies for disentangling and controlling staining.

Result: Outperforms state-of-the-art methods, demonstrating biomedical potential.

Conclusion: The model effectively addresses key challenges in virtual staining, with open-source code planned.

Abstract: Hematoxylin and eosin (H&E) staining visualizes histology but lacks
specificity for diagnostic markers. Immunohistochemistry (IHC) staining
provides protein-targeted staining but is restricted by tissue availability and
antibody specificity. Virtual staining, i.e., computationally translating the
H&E image to its IHC counterpart while preserving the tissue structure, is
promising for efficient IHC generation. Existing virtual staining methods still
face key challenges: 1) effective decomposition of staining style and tissue
structure, 2) controllable staining process adaptable to diverse tissue and
proteins, and 3) rigorous structural consistency modelling to handle the
non-pixel-aligned nature of paired H&E and IHC images. This study proposes a
mutual-information (MI)-guided score-based diffusion model for unpaired virtual
staining. Specifically, we design 1) a global MI-guided energy function that
disentangles the tissue structure and staining characteristics across
modalities, 2) a novel timestep-customized reverse diffusion process for
precise control of the staining intensity and structural reconstruction, and 3)
a local MI-driven contrastive learning strategy to ensure the cellular level
structural consistency between H&E-IHC images. Extensive experiments
demonstrate the our superiority over state-of-the-art approaches, highlighting
its biomedical potential. Codes will be open-sourced upon acceptance.

</details>


### [313] [Multi-Source COVID-19 Detection via Variance Risk Extrapolation](https://arxiv.org/abs/2506.23208)
*Runtian Yuan,Qingqiu Li,Junlin Hou,Jilan Xu,Yuejie Zhang,Rui Feng,Hao Chen*

Main category: eess.IV

TL;DR: A method combining Variance Risk Extrapolation (VREx) and Mixup data augmentation improves COVID-19 detection in chest CT scans across diverse hospital datasets.


<details>
  <summary>Details</summary>
Motivation: Address domain shift in COVID-19 detection due to variations in imaging protocols, scanners, and patient populations across institutions.

Method: Incorporate VREx to minimize variance of empirical risks across domains and use Mixup for data augmentation.

Result: Achieves an average macro F1 score of 0.96 across four sources, showing strong generalization.

Conclusion: The approach effectively enhances cross-domain generalization for COVID-19 detection.

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which aims to classify chest CT scans into COVID and Non-COVID categories
across data collected from four distinct hospitals and medical centers. A major
challenge in this task lies in the domain shift caused by variations in imaging
protocols, scanners, and patient populations across institutions. To enhance
the cross-domain generalization of our model, we incorporate Variance Risk
Extrapolation (VREx) into the training process. VREx encourages the model to
maintain consistent performance across multiple source domains by explicitly
minimizing the variance of empirical risks across environments. This
regularization strategy reduces overfitting to center-specific features and
promotes learning of domain-invariant representations. We further apply Mixup
data augmentation to improve generalization and robustness. Mixup interpolates
both the inputs and labels of randomly selected pairs of training samples,
encouraging the model to behave linearly between examples and enhancing its
resilience to noise and limited data. Our method achieves an average macro F1
score of 0.96 across the four sources on the validation set, demonstrating
strong generalization.

</details>


### [314] [Improving Myocardial Infarction Detection via Synthetic ECG Pretraining](https://arxiv.org/abs/2506.23259)
*Lachin Naghashyar*

Main category: eess.IV

TL;DR: A physiology-aware pipeline synthesizes realistic 12-lead ECGs for MI detection, improving classifier performance in low-data settings.


<details>
  <summary>Details</summary>
Motivation: Accurate early diagnosis of myocardial infarction (MI) from ECGs is critical, but deep learning models require large labeled datasets, which are often scarce.

Method: Proposes a pipeline to (i) synthesize tunable MI ECGs with realistic noise and (ii) pre-train classifiers using self-supervised masked-autoencoding and a joint reconstruction-classification objective.

Result: Synthetic ECGs preserved key morphological features, and pretraining on them improved classification performance, with AUC gains up to 4 percentage points in low-data settings.

Conclusion: Controlled synthetic ECGs can enhance MI detection when real clinical data is limited.

Abstract: Myocardial infarction is a major cause of death globally, and accurate early
diagnosis from electrocardiograms (ECGs) remains a clinical priority. Deep
learning models have shown promise for automated ECG interpretation, but
require large amounts of labeled data, which are often scarce in practice. We
propose a physiology-aware pipeline that (i) synthesizes 12-lead ECGs with
tunable MI morphology and realistic noise, and (ii) pre-trains recurrent and
transformer classifiers with self-supervised masked-autoencoding plus a joint
reconstruction-classification objective. We validate the realism of synthetic
ECGs via statistical and visual analysis, confirming that key morphological
features are preserved. Pretraining on synthetic data consistently improved
classification performance, particularly in low-data settings, with AUC gains
of up to 4 percentage points. These results show that controlled synthetic ECGs
can help improve MI detection when real clinical data is limited.

</details>


### [315] [BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia](https://arxiv.org/abs/2506.23305)
*Rachit Saluja,Arzu Kovanlikaya,Candace Chien,Lauren Kathryn Blatt,Jeffrey M. Perlman,Stefan Worgall,Mert R. Sabuncu,Jonathan P. Dyke*

Main category: eess.IV

TL;DR: The paper introduces a dataset of 3D MRI scans and semantic segmentations for 40 neonates, mostly with BPD, to improve diagnosis and research in neonatal lung imaging.


<details>
  <summary>Details</summary>
Motivation: To provide a non-invasive, radiation-free alternative to X-rays for diagnosing BPD in preterm neonates using MRI.

Method: High-resolution 3D MRI (StarVIBE series) paired with semantic segmentation algorithms for lung and trachea analysis.

Result: A dataset of MRI scans, segmentations, clinical data, and validated baseline models is presented.

Conclusion: The dataset supports advanced research and development in neonatal lung imaging, offering a safer and more detailed diagnostic tool.

Abstract: Bronchopulmonary dysplasia (BPD) is a common complication among preterm
neonates, with portable X-ray imaging serving as the standard diagnostic
modality in neonatal intensive care units (NICUs). However, lung magnetic
resonance imaging (MRI) offers a non-invasive alternative that avoids sedation
and radiation while providing detailed insights into the underlying mechanisms
of BPD. Leveraging high-resolution 3D MRI data, advanced image processing and
semantic segmentation algorithms can be developed to assist clinicians in
identifying the etiology of BPD. In this dataset, we present MRI scans paired
with corresponding semantic segmentations of the lungs and trachea for 40
neonates, the majority of whom are diagnosed with BPD. The imaging data consist
of free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as
the StarVIBE series. Additionally, we provide comprehensive clinical data and
baseline segmentation models, validated against clinical assessments, to
support further research and development in neonatal lung imaging.

</details>


### [316] [SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting](https://arxiv.org/abs/2506.23309)
*Yiming Huang,Long Bai,Beilei Cui,Kun Yuan,Guankun Wang,Mobarakol Islam,Nicolas Padoy,Nassir Navab,Hongliang Ren*

Main category: eess.IV

TL;DR: SurgTPGS introduces a text-promptable Gaussian Splatting method for 3D surgical scene understanding, combining vision-language models and semantic feature learning for precise reconstruction and real-time queries.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack real-time text-promptable 3D queries in surgical scenes, which are crucial for planning and intra-operative guidance.

Method: Uses 3D semantics feature learning with Segment Anything and vision-language models, semantic-aware deformation tracking, and semantic region-aware optimization.

Result: Outperforms state-of-the-art methods on real-world surgical datasets, improving reconstruction quality and semantic smoothness.

Conclusion: SurgTPGS enhances surgical precision and safety, paving the way for next-gen intelligent surgical systems.

Abstract: In contemporary surgical research and practice, accurately comprehending 3D
surgical scenes with text-promptable capabilities is particularly crucial for
surgical planning and real-time intra-operative guidance, where precisely
identifying and interacting with surgical tools and anatomical structures is
paramount. However, existing works focus on surgical vision-language model
(VLM), 3D reconstruction, and segmentation separately, lacking support for
real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a
novel text-promptable Gaussian Splatting method to fill this gap. We introduce
a 3D semantics feature learning strategy incorporating the Segment Anything
model and state-of-the-art vision-language models. We extract the segmented
language features for 3D surgical scene reconstruction, enabling a more
in-depth understanding of the complex surgical environment. We also propose
semantic-aware deformation tracking to capture the seamless deformation of
semantic features, providing a more precise reconstruction for both texture and
semantic features. Furthermore, we present semantic region-aware optimization,
which utilizes regional-based semantic information to supervise the training,
particularly promoting the reconstruction quality and semantic smoothness. We
conduct comprehensive experiments on two real-world surgical datasets to
demonstrate the superiority of SurgTPGS over state-of-the-art methods,
highlighting its potential to revolutionize surgical practices. SurgTPGS paves
the way for developing next-generation intelligent surgical systems by
enhancing surgical precision and safety. Our code is available at:
https://github.com/lastbasket/SurgTPGS.

</details>


### [317] [Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation](https://arxiv.org/abs/2506.23334)
*Hongyi Pan,Ziliang Hong,Gorkem Durak,Ziyue Xu,Ulas Bagci*

Main category: eess.IV

TL;DR: A generative AI-based data augmentation framework improves federated learning (FL) for breast cancer diagnosis using ultrasound images by integrating synthetic data, enhancing model performance without compromising privacy.


<details>
  <summary>Details</summary>
Motivation: FL's effectiveness is limited by data scarcity and non-IID data across clients, degrading model performance. This work aims to overcome these challenges using generative AI.

Method: Two class-specific Deep Convolutional GANs (for benign and malignant lesions) generate synthetic images. These are integrated into FL training with FedAvg and FedProx, tested on three breast ultrasound datasets.

Result: Synthetic data improved AUC (FedAvg: 0.9206 to 0.9237; FedProx: 0.9429 to 0.9538). Excessive synthetic data reduced performance, emphasizing the need for balance.

Conclusion: Generative AI-based data augmentation can enhance FL for breast ultrasound classification, but optimal synthetic-to-real data ratios are crucial.

Abstract: Federated learning (FL) has emerged as a promising paradigm for
collaboratively training deep learning models across institutions without
exchanging sensitive medical data. However, its effectiveness is often hindered
by limited data availability and non-independent, identically distributed data
across participating clients, which can degrade model performance and
generalization. To address these challenges, we propose a generative AI based
data augmentation framework that integrates synthetic image sharing into the
federated training process for breast cancer diagnosis via ultrasound images.
Specifically, we train two simple class-specific Deep Convolutional Generative
Adversarial Networks: one for benign and one for malignant lesions. We then
simulate a realistic FL setting using three publicly available breast
ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are
adopted as baseline FL algorithms. Experimental results show that incorporating
a suitable number of synthetic images improved the average AUC from 0.9206 to
0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that
excessive use of synthetic data reduced performance, underscoring the
importance of maintaining a balanced ratio of real and synthetic samples. Our
findings highlight the potential of generative AI based data augmentation to
enhance FL results in the breast ultrasound image classification task.

</details>


### [318] [FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction](https://arxiv.org/abs/2506.23466)
*Qiqing Liu,Guoquan Wei,Zekun Zhou,Yiyang Wen,Liu Shi,Qiegen Liu*

Main category: eess.IV

TL;DR: FD-DiT, a frequency domain-directed diffusion transformer, improves LDCT reconstruction by combining diffusion models with frequency decoupling and hybrid denoising, outperforming existing methods in noise and artifact suppression.


<details>
  <summary>Details</summary>
Motivation: LDCT reduces radiation exposure but suffers from noise and artifacts, impacting diagnostic accuracy. Existing methods lack fine detail preservation.

Method: FD-DiT uses a diffusion strategy aligned with LDCT data, frequency decoupling for noise concentration, hybrid denoising, sliding sparse local attention, and dynamic fusion.

Result: FD-DiT achieves superior noise and artifact suppression in LDCT images at identical dose levels compared to state-of-the-art methods.

Conclusion: FD-DiT effectively enhances LDCT reconstruction by preserving fine details and suppressing noise, improving diagnostic accuracy.

Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but suffers
from image artifacts and loss of detail due to quantum and electronic noise,
potentially impacting diagnostic accuracy. Transformer combined with diffusion
models has been a promising approach for image generation. Nevertheless,
existing methods exhibit limitations in preserving finegrained image details.
To address this issue, frequency domain-directed diffusion transformer (FD-DiT)
is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy
that progressively introduces noise until the distribution statistically aligns
with that of LDCT data, followed by denoising processing. Furthermore, we
employ a frequency decoupling technique to concentrate noise primarily in
high-frequency domain, thereby facilitating effective capture of essential
anatomical structures and fine details. A hybrid denoising network is then
utilized to optimize the overall data reconstruction process. To enhance the
capability in recognizing high-frequency noise, we incorporate sliding sparse
local attention to leverage the sparsity and locality of shallow-layer
information, propagating them via skip connections for improving feature
representation. Finally, we propose a learnable dynamic fusion strategy for
optimal component integration. Experimental results demonstrate that at
identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior
noise and artifact suppression compared to state-of-the-art methods.

</details>


### [319] [UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound](https://arxiv.org/abs/2506.23490)
*Junxuan Yu,Yaofei Duan,Yuhao Huang,Yu Wang,Rongbo Ling,Weihao Luo,Ang Zhang,Jingxian Xu,Qiongying Ni,Yongsong Zhou,Binghan Li,Haoran Dou,Liping Liu,Yanfen Chu,Feng Geng,Zhe Sheng,Zhifeng Ding,Dingxin Zhang,Rui Huang,Yuhang Zhang,Xiaowei Xu,Tao Tan,Dong Ni,Zhongshan Gou,Xin Yang*

Main category: eess.IV

TL;DR: UltraTwin is a generative framework for creating cardiac anatomical twins from sparse 2D US images, addressing challenges like rare paired data and US noise.


<details>
  <summary>Details</summary>
Motivation: 2D and 3D US have limitations in cardiac examination, prompting the need for precise anatomical twin modeling for better treatment planning.

Method: UltraTwin uses a coarse-to-fine scheme and an implicit autoencoder for hierarchical reconstruction and topology-aware constraints, leveraging a novel dataset of paired 2D US and CT images.

Result: UltraTwin outperforms competitors in reconstructing high-quality anatomical twins.

Conclusion: UltraTwin advances anatomical twin modeling, offering potential for personalized cardiac care.

Abstract: Echocardiography is routine for cardiac examination. However, 2D ultrasound
(US) struggles with accurate metric calculation and direct observation of 3D
cardiac structures. Moreover, 3D US is limited by low resolution, small field
of view and scarce availability in practice. Constructing the cardiac
anatomical twin from 2D images is promising to provide precise treatment
planning and clinical quantification. However, it remains challenging due to
the rare paired data, complex structures, and US noises. In this study, we
introduce a novel generative framework UltraTwin, to obtain cardiac anatomical
twin from sparse multi-view 2D US. Our contribution is three-fold. First,
pioneered the construction of a real-world and high-quality dataset containing
strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we
propose a coarse-to-fine scheme to achieve hierarchical reconstruction
optimization. Last, we introduce an implicit autoencoder for topology-aware
constraints. Extensive experiments show that UltraTwin reconstructs
high-quality anatomical twins versus strong competitors. We believe it advances
anatomical twin modeling for potential applications in personalized cardiac
care.

</details>


### [320] [Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI](https://arxiv.org/abs/2506.23506)
*Bowen Xin,Rohan Hickey,Tamara Blake,Jin Jin,Claire E Wainwright,Thomas Benkert,Alto Stemmer,Peter Sly,David Coman,Jason Dowling*

Main category: eess.IV

TL;DR: The paper introduces APL scoring, an AI-assisted method for quantifying lung damage in CF using UTE-MRI, showing faster and more accurate results than traditional grid-level scoring.


<details>
  <summary>Details</summary>
Motivation: To address the lack of quantitative scoring systems for structural lung MRI in diseases like cystic fibrosis, aiming for faster and more accurate diagnosis.

Method: APL scoring involves 5 stages: image loading, AI lung segmentation, slice sampling, pixel-level annotation, and quantification.

Result: APL scoring was twice as fast (8.2 minutes/subject) and more accurate (p=0.021) than grid-level scoring, with strong correlation (R=0.973).

Conclusion: APL scoring has potential to improve UTE-MRI workflows and can be adapted for other lung diseases and MRI sequences.

Abstract: Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)
represents a recent breakthrough in lung structure imaging, providing image
resolution and quality comparable to computed tomography (CT). Due to the
absence of ionising radiation, MRI is often preferred over CT in paediatric
diseases such as cystic fibrosis (CF), one of the most common genetic disorders
in Caucasians. To assess structural lung damage in CF imaging, CT scoring
systems provide valuable quantitative insights for disease diagnosis and
progression. However, few quantitative scoring systems are available in
structural lung MRI (e.g., UTE-MRI). To provide fast and accurate
quantification in lung MRI, we investigated the feasibility of novel Artificial
intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring
consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)
lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification
and reporting. The results shows that our APL scoring took 8.2 minutes per
subject, which was more than twice as fast as the previous grid-level scoring.
Additionally, our pixel-level scoring was statistically more accurate
(p=0.021), while strongly correlating with grid-level scoring (R=0.973,
p=5.85e-9). This tool has great potential to streamline the workflow of UTE
lung MRI in clinical settings, and be extended to other structural lung MRI
sequences (e.g., BLADE MRI), and for other lung diseases (e.g.,
bronchopulmonary dysplasia).

</details>


### [321] [AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm](https://arxiv.org/abs/2506.23537)
*Xinyue Li,Zhangkai Ni,Wenhan Yang*

Main category: eess.IV

TL;DR: AFUNet proposes a deep unfolding network for HDR image reconstruction, decoupling alignment and fusion tasks for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack theoretical foundations, relying on empirical design, which affects reliability.

Method: AFUNet uses a MAP estimation perspective, alternating between alignment (SAM) and fusion (CFM) modules in an iterative, trainable network.

Result: AFUNet outperforms state-of-the-art methods in both qualitative and quantitative evaluations.

Conclusion: AFUNet provides a theoretically grounded, high-performance solution for HDR reconstruction.

Abstract: Existing learning-based methods effectively reconstruct HDR images from
multi-exposure LDR inputs with extended dynamic range and improved detail, but
they rely more on empirical design rather than theoretical foundation, which
can impact their reliability. To address these limitations, we propose the
cross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR
reconstruction is systematically decoupled into two interleaved subtasks --
alignment and fusion -- optimized through alternating refinement, achieving
synergy between the two subtasks to enhance the overall performance. Our method
formulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP)
estimation perspective, explicitly incorporating spatial correspondence priors
across LDR images and naturally bridging the alignment and fusion subproblems
through joint constraints. Building on the mathematical foundation, we
reimagine traditional iterative optimization through unfolding -- transforming
the conventional solution process into an end-to-end trainable AFUNet with
carefully designed modules that work progressively. Specifically, each
iteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that
alternates between a Spatial Alignment Module (SAM) for alignment and a Channel
Fusion Module (CFM) for adaptive feature fusion, progressively bridging
misaligned content and exposure discrepancies. Extensive qualitative and
quantitative evaluations demonstrate AFUNet's superior performance,
consistently surpassing state-of-the-art methods. Our code is available at:
https://github.com/eezkni/AFUNet

</details>


### [322] [A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation](https://arxiv.org/abs/2506.23584)
*Renjie Liang,Zhengkang Fan,Jinqian Pan,Chenkun Sun,Russell Terry,Jie Xu*

Main category: eess.IV

TL;DR: A two-stage framework generates renal radiology reports from 2D CT scans by extracting structured abnormality features and combining them with images for natural language report generation.


<details>
  <summary>Details</summary>
Motivation: The complexity of medical imaging and variability in clinical documentation make radiology report generation challenging.

Method: A multi-task learning model extracts lesion attributes, which are combined with CT images and fed into a vision-language model for report generation.

Result: The model outperforms baselines, capturing key clinical content with reasonable accuracy.

Conclusion: The study demonstrates feasibility for modular, feature-informed report generation, with future work targeting 3D CT volumes and improved clinical fidelity.

Abstract: Generating radiology reports from CT scans remains a complex task due to the
nuanced nature of medical imaging and the variability in clinical
documentation. In this study, we propose a two-stage framework for generating
renal radiology reports from 2D CT slices. First, we extract structured
abnormality features using a multi-task learning model trained to identify
lesion attributes such as location, size, enhancement, and attenuation. These
extracted features are subsequently combined with the corresponding CT image
and fed into a fine-tuned vision-language model to generate natural language
report sentences aligned with clinical findings. We conduct experiments on a
curated dataset of renal CT studies with manually annotated
sentence-slice-feature triplets and evaluate performance using both
classification metrics and natural language generation metrics. Our results
demonstrate that the proposed model outperforms random baselines across all
abnormality types, and the generated reports capture key clinical content with
reasonable textual accuracy. This exploratory work highlights the feasibility
of modular, feature-informed report generation for renal imaging. Future
efforts will focus on extending this pipeline to 3D CT volumes and further
improving clinical fidelity in multimodal medical AI systems.

</details>


### [323] [Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation](https://arxiv.org/abs/2506.23664)
*Fangyijie Wang,Kevin Whelan,Félix Balado,Guénolé Silvestre,Kathleen M. Curran*

Main category: eess.IV

TL;DR: A novel mask-guided GenAI approach using diffusion models generates synthetic fetal head ultrasound images with segmentation masks, improving segmentation performance with limited real data.


<details>
  <summary>Details</summary>
Motivation: Overcome privacy and labeling challenges in medical imaging by using synthetic data to augment real datasets.

Method: Proposes a mask-guided GenAI approach using diffusion models to create synthetic fetal head ultrasound images paired with segmentation masks for supervised fine-tuning of SAM.

Result: Synthetic data captures real features well, achieving state-of-the-art fetal head segmentation (Dice Scores: 94.66% and 94.38% for Spanish and African cohorts).

Conclusion: The approach effectively augments limited real datasets, enhancing segmentation performance with synthetic data.

Abstract: Medical image data is less accessible than in other domains due to privacy
and regulatory constraints. In addition, labeling requires costly,
time-intensive manual image annotation by clinical experts. To overcome these
challenges, synthetic medical data generation offers a promising solution.
Generative AI (GenAI), employing generative deep learning models, has proven
effective at producing realistic synthetic images. This study proposes a novel
mask-guided GenAI approach using diffusion models to generate synthetic fetal
head ultrasound images paired with segmentation masks. These synthetic pairs
augment real datasets for supervised fine-tuning of the Segment Anything Model
(SAM). Our results show that the synthetic data captures real image features
effectively, and this approach reaches state-of-the-art fetal head
segmentation, especially when trained with a limited number of real image-mask
pairs. In particular, the segmentation reaches Dice Scores of 94.66\% and
94.38\% using a handful of ultrasound images from the Spanish and African
cohorts, respectively. Our code, models, and data are available on GitHub.

</details>


### [324] [MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation](https://arxiv.org/abs/2506.23700)
*Peiting Tian,Xi Chen,Haixia Bi,Fan Li*

Main category: eess.IV

TL;DR: MedSAM-CA is a fine-tuning approach for medical image segmentation, reducing reliance on large annotated datasets and improving boundary delineation in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of limited annotated data and poor boundary visibility in medical image segmentation.

Method: Uses MedSAM with added CBR-Net for boundary refinement and Atte-FFB for feature fusion.

Result: Achieves 94.43% Dice with only 2% training data, nearing full-data performance.

Conclusion: MedSAM-CA is effective for low-resource medical segmentation, enhancing accuracy with minimal data.

Abstract: Medical image segmentation plays a crucial role in clinical diagnosis and
treatment planning, where accurate boundary delineation is essential for
precise lesion localization, organ identification, and quantitative assessment.
In recent years, deep learning-based methods have significantly advanced
segmentation accuracy. However, two major challenges remain. First, the
performance of these methods heavily relies on large-scale annotated datasets,
which are often difficult to obtain in medical scenarios due to privacy
concerns and high annotation costs. Second, clinically challenging scenarios,
such as low contrast in certain imaging modalities and blurry lesion boundaries
caused by malignancy, still pose obstacles to precise segmentation. To address
these challenges, we propose MedSAM-CA, an architecture-level fine-tuning
approach that mitigates reliance on extensive manual annotations by adapting
the pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA
introduces two key components: the Convolutional Attention-Enhanced Boundary
Refinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block
(Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover
boundary information potentially overlooked by long-range attention mechanisms,
leveraging hierarchical convolutional processing. Atte-FFB, embedded in the
MedSAM decoder, fuses multi-level fine-grained features from skip connections
in CBR-Net with global representations upsampled within the decoder to enhance
boundary delineation accuracy. Experiments on publicly available datasets
covering dermoscopy, CT, and MRI imaging modalities validate the effectiveness
of MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only
2% of full training data, reaching 97.25% of full-data training performance,
demonstrating strong effectiveness in low-resource clinical settings.

</details>


### [325] [MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction](https://arxiv.org/abs/2506.23701)
*Lingtong Zhang,Mengdie Song,Xiaohan Hao,Huayu Mai,Bensheng Qiu*

Main category: eess.IV

TL;DR: The paper proposes Multi-domain Diffusion Prior Guidance (MDPG) using latent diffusion models (LDMs) to improve MRI reconstruction by enhancing data consistency and leveraging multi-domain priors.


<details>
  <summary>Details</summary>
Motivation: MRI reconstruction is critical for diagnostics, but current diffusion models struggle with high-fidelity image generation due to stochasticity. LDMs offer compact and detailed priors, motivating their use for better reconstruction.

Method: The method includes a Visual-Mamba-based backbone for efficient encoding, LDMs for multi-domain priors, Latent Guided Attention (LGA) for fusion, and a Dual-domain Fusion Branch (DFB) for k-space and image domain fusion. A k-space regularization strategy is also introduced.

Result: Experiments on two public MRI datasets show the effectiveness of the proposed MDPG method.

Conclusion: MDPG successfully enhances MRI reconstruction by integrating LDMs and multi-domain guidance, achieving improved data consistency and image fidelity.

Abstract: Magnetic Resonance Imaging (MRI) reconstruction is essential in medical
diagnostics. As the latest generative models, diffusion models (DMs) have
struggled to produce high-fidelity images due to their stochastic nature in
image domains. Latent diffusion models (LDMs) yield both compact and detailed
prior knowledge in latent domains, which could effectively guide the model
towards more effective learning of the original data distribution. Inspired by
this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by
pre-trained LDMs to enhance data consistency in MRI reconstruction tasks.
Specifically, we first construct a Visual-Mamba-based backbone, which enables
efficient encoding and reconstruction of under-sampled images. Then pre-trained
LDMs are integrated to provide conditional priors in both latent and image
domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion
in multi-level latent domains. Simultaneously, to effectively utilize a prior
in both the k-space and image domain, under-sampled images are fused with
generated full-sampled images by the Dual-domain Fusion Branch (DFB) for
self-adaption guidance. Lastly, to further enhance the data consistency, we
propose a k-space regularization strategy based on the non-auto-calibration
signal (NACS) set. Extensive experiments on two public MRI datasets fully
demonstrate the effectiveness of the proposed methodology. The code is
available at https://github.com/Zolento/MDPG.

</details>


### [326] [Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound](https://arxiv.org/abs/2506.23721)
*Gijs Luijten,Roberto Maria Scardigno,Lisle Faray de Paiva,Peter Hoyer,Jens Kleesiek,Domenico Buongiorno,Vitoantonio Bevilacqua,Jan Egger*

Main category: eess.IV

TL;DR: The paper introduces a deep learning and augmented reality system for real-time kidney volumetric measurements in ultrasound, improving usability and reducing cognitive load for clinicians.


<details>
  <summary>Details</summary>
Motivation: Ultrasound has a steep learning curve and requires focus shifts between screen and patient. The study aims to automate kidney measurements and enhance ergonomics with AR.

Method: The approach integrates DL-based semantic segmentation for real-time measurements and AR for display projection. Two AR-DL pipelines on HoloLens-2 are proposed, evaluated using the Open Kidney Dataset and open-source models.

Result: The system provides real-time feasibility and accuracy, with open-source implementations for broader accessibility, improving ultrasound training and diagnostics.

Conclusion: The AR-DL-assisted ultrasound system enhances clinical workflow by automating measurements and improving ergonomics, particularly useful in point-of-care settings.

Abstract: Ultrasound (US) is widely accessible and radiation-free but has a steep
learning curve due to its dynamic nature and non-standard imaging planes.
Additionally, the constant need to shift focus between the US screen and the
patient poses a challenge. To address these issues, we integrate deep learning
(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric
measurements, which are essential for clinical assessment but are traditionally
time-consuming and prone to fatigue. This automation allows clinicians to
concentrate on image interpretation rather than manual measurements.
Complementing DL, augmented reality (AR) enhances the usability of US by
projecting the display directly into the clinician's field of view, improving
ergonomics and reducing the cognitive load associated with screen-to-patient
transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one
streams directly via the application programming interface for a wireless
setup, while the other supports any US device with video output for broader
accessibility. We evaluate RT feasibility and accuracy using the Open Kidney
Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with
MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model
implementations, measurement algorithms, and a Wi-Fi-based streaming solution,
enhancing US training and diagnostics, especially in point-of-care settings.

</details>


### [327] [Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos](https://arxiv.org/abs/2506.23759)
*Zheng Fang,Xiaoming Qi,Chun-Mei Feng,Jialun Pei,Weixin Si,Yueming Jin*

Main category: eess.IV

TL;DR: Proposes FedST, a personalized FL scheme for surgical instrument segmentation, leveraging domain knowledge to address diverse anatomical backgrounds and synthetic data use.


<details>
  <summary>Details</summary>
Motivation: Limited FL works in surgical data science and lack of consideration for surgical domain characteristics like diverse backgrounds and synthetic data availability.

Method: Introduces FedST with Representation Separation and Cooperation (RSC) for local training and Synthesis-based Explicit Representation Quantification (SERQ) for global training.

Result: Enhances segmentation by decoupling private query embeddings and synchronizing model convergence with synthetic data.

Conclusion: FedST effectively leverages surgical domain knowledge to improve segmentation performance in FL settings.

Abstract: Surgical instrument segmentation under Federated Learning (FL) is a promising
direction, which enables multiple surgical sites to collaboratively train the
model without centralizing datasets. However, there exist very limited FL works
in surgical data science, and FL methods for other modalities do not consider
inherent characteristics in surgical domain: i) different scenarios show
diverse anatomical backgrounds while highly similar instrument representation;
ii) there exist surgical simulators which promote large-scale synthetic data
generation with minimal efforts. In this paper, we propose a novel Personalized
FL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST),
which wisely leverages surgical domain knowledge during both local-site and
global-server training to boost segmentation. Concretely, our model embraces a
Representation Separation and Cooperation (RSC) mechanism in local-site
training, which decouples the query embedding layer to be trained privately, to
encode respective backgrounds. Meanwhile, other parameters are optimized
globally to capture the consistent representations of instruments, including
the temporal layer to capture similar motion patterns. A textual-guided channel
selection is further designed to highlight site-specific features, facilitating
model adapta tion to each site. Moreover, in global-server training, we propose
Synthesis-based Explicit Representation Quantification (SERQ), which defines an
explicit representation target based on synthetic data to synchronize the model
convergence during fusion for improving model generalization.

</details>


### [328] [ShapeKit](https://arxiv.org/abs/2506.24003)
*Junqi Liu,Dongli He,Wenxuan Li,Ningyu Wang,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: ShapeKit improves anatomical shape accuracy in medical segmentation by 8% without model re-training, outperforming architecture changes.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance segmentation accuracy by focusing on anatomical shapes, as traditional model modifications yield minimal gains.

Method: Introduces ShapeKit, a shape-focused toolkit, to refine anatomical shapes without requiring model re-training.

Result: ShapeKit boosts segmentation performance by over 8%, surpassing the marginal 3% gains from architectural changes.

Conclusion: Shape-based tools like ShapeKit are undervalued but impactful, offering significant improvements in medical segmentation.

Abstract: In this paper, we present a practical approach to improve anatomical shape
accuracy in whole-body medical segmentation. Our analysis shows that a
shape-focused toolkit can enhance segmentation performance by over 8%, without
the need for model re-training or fine-tuning. In comparison, modifications to
model architecture typically lead to marginal gains of less than 3%. Motivated
by this observation, we introduce ShapeKit, a flexible and easy-to-integrate
toolkit designed to refine anatomical shapes. This work highlights the
underappreciated value of shape-based tools and calls attention to their
potential impact within the medical segmentation community.

</details>


### [329] [C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism](https://arxiv.org/abs/2506.24074)
*Mayank V. Golhar,Lucas Sebastian Galeano Fretes,Loren Ayers,Venkata S. Akshintala,Taylor L. Bobrow,Nicholas J. Durr*

Main category: eess.IV

TL;DR: C3VDv2 is an enhanced 3D colonoscopy dataset designed to improve the development and evaluation of 3D reconstruction algorithms by providing realistic and diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: The lack of 3D colonoscopy datasets hinders the development of computer vision techniques for diagnostic colonoscopy.

Method: The dataset includes 192 video sequences from silicone colon phantoms, with ground truth data like depth, surface normals, and 3D models. It also features simulated colonoscopy videos and deformation cases.

Result: C3VDv2 offers diverse and challenging scenarios (e.g., fecal debris, fast camera motion) for robust algorithm evaluation.

Conclusion: The enhanced realism of C3VDv2 supports more representative development and testing of 3D reconstruction algorithms in colonoscopy.

Abstract: Computer vision techniques have the potential to improve the diagnostic
performance of colonoscopy, but the lack of 3D colonoscopy datasets for
training and validation hinders their development. This paper introduces
C3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video
Dataset, featuring enhanced realism designed to facilitate the quantitative
evaluation of 3D colon reconstruction algorithms. 192 video sequences were
captured by imaging 60 unique, high-fidelity silicone colon phantom segments.
Ground truth depth, surface normals, optical flow, occlusion,
six-degree-of-freedom pose, coverage maps, and 3D models are provided for 169
colonoscopy videos. Eight simulated screening colonoscopy videos acquired by a
gastroenterologist are provided with ground truth poses. The dataset includes
15 videos featuring colon deformations for qualitative assessment. C3VDv2
emulates diverse and challenging scenarios for 3D reconstruction algorithms,
including fecal debris, mucous pools, blood, debris obscuring the colonoscope
lens, en-face views, and fast camera motion. The enhanced realism of C3VDv2
will allow for more robust and representative development and evaluation of 3D
reconstruction algorithms.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [330] [ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes](https://arxiv.org/abs/2506.21629)
*Chenhao Zhang,Yezhi Shen,Fengqing Zhu*

Main category: cs.GR

TL;DR: Proposes ICP-3DGS, combining ICP with optimization for camera pose estimation and voxel-based densification for large-scale scene reconstruction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Neural rendering methods like NeRFs and 3DGS rely on preprocessed camera poses and 3D priors, which are hard to obtain outdoors. This work aims to address this limitation.

Method: Uses ICP with optimization-based refinement for camera pose estimation and voxel-based densification for scene reconstruction.

Result: ICP-3DGS outperforms existing methods in camera pose estimation and novel view synthesis for indoor and outdoor scenes.

Conclusion: The proposed method effectively addresses challenges in large-scale scene reconstruction and camera pose estimation, with superior performance.

Abstract: In recent years, neural rendering methods such as NeRFs and 3D Gaussian
Splatting (3DGS) have made significant progress in scene reconstruction and
novel view synthesis. However, they heavily rely on preprocessed camera poses
and 3D structural priors from structure-from-motion (SfM), which are
challenging to obtain in outdoor scenarios. To address this challenge, we
propose to incorporate Iterative Closest Point (ICP) with optimization-based
refinement to achieve accurate camera pose estimation under large camera
movements. Additionally, we introduce a voxel-based scene densification
approach to guide the reconstruction in large-scale scenes. Experiments
demonstrate that our approach ICP-3DGS outperforms existing methods in both
camera pose estimation and novel view synthesis across indoor and outdoor
scenes of various scales. Source code is available at
https://github.com/Chenhao-Z/ICP-3DGS.

</details>


### [331] [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/abs/2506.22799)
*Minchao Jiang,Shunyu Jia,Jiaming Gu,Xiaoyuan Lu,Guangming Zhu,Anqi Dong,Liang Zhang*

Main category: cs.GR

TL;DR: VoteSplat integrates Hough voting with 3D Gaussian Splatting (3DGS) for enhanced 3D scene understanding, reducing training costs and improving localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods lack deeper scene understanding and have high training costs, limiting their practicality.

Method: Uses Segment Anything Model (SAM) for instance segmentation, embeds spatial offsets into Gaussian primitives, and refines depth localization with distortion constraints.

Result: Effective in open-vocabulary 3D instance localization, point cloud understanding, and hierarchical segmentation.

Conclusion: VoteSplat offers a cost-efficient, accurate solution for 3D scene understanding, bridging gaps in existing 3DGS methods.

Abstract: 3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time
rendering for novel view synthesis of 3D scenes. However, existing methods
focus primarily on geometric and appearance modeling, lacking deeper scene
understanding while also incurring high training costs that complicate the
originally streamlined differentiable rendering pipeline. To this end, we
propose VoteSplat, a novel 3D scene understanding framework that integrates
Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized
for instance segmentation, extracting objects, and generating 2D vote maps. We
then embed spatial offset vectors into Gaussian primitives. These offsets
construct 3D spatial votes by associating them with 2D image votes, while depth
distortion constraints refine localization along the depth axis. For
open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D
point clouds via voting points, reducing training costs associated with
high-dimensional CLIP features while preserving semantic unambiguity. Extensive
experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D
instance localization, 3D point cloud understanding, click-based 3D object
localization, hierarchical segmentation, and ablation studies. Our code is
available at https://sy-ja.github.io/votesplat/

</details>


### [332] [Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions](https://arxiv.org/abs/2506.22973)
*AmirHossein Naghi Razlighi,Elaheh Badali Golezani,Shohreh Kasaei*

Main category: cs.GR

TL;DR: A novel lossy compression method for 3D Gaussian Splatting uses learnable confidence scores to prune low-confidence splats, reducing storage and computational overhead while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting produces millions of splats, leading to excessive storage and computational costs. The goal is to compress these splats efficiently without sacrificing visual fidelity.

Method: The method employs learnable confidence scores modeled as Beta distributions, optimized via reconstruction-aware losses. It prunes low-confidence splats and is adaptable to any Gaussian Splatting variant.

Result: The approach achieves favorable trade-offs between compression and fidelity, outperforming prior work. Confidence values also serve as a quality metric for scenes.

Conclusion: The proposed compression method effectively reduces overhead while preserving visual quality, offering a versatile and efficient solution for 3D Gaussian Splatting.

Abstract: 3D Gaussian Splatting enables high-quality real-time rendering but often
produces millions of splats, resulting in excessive storage and computational
overhead. We propose a novel lossy compression method based on learnable
confidence scores modeled as Beta distributions. Each splat's confidence is
optimized through reconstruction-aware losses, enabling pruning of
low-confidence splats while preserving visual fidelity. The proposed approach
is architecture-agnostic and can be applied to any Gaussian Splatting variant.
In addition, the average confidence values serve as a new metric to assess the
quality of the scene. Extensive experiments demonstrate favorable trade-offs
between compression and fidelity compared to prior work. Our code and data are
publicly available at
https://github.com/amirhossein-razlighi/Confident-Splatting

</details>


### [333] [GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering](https://arxiv.org/abs/2506.23957)
*Zinuo You,Stamatios Georgoulis,Anpei Chen,Siyu Tang,Dengxin Dai*

Main category: cs.GR

TL;DR: GaVS introduces a 3D-grounded video stabilization method using Gaussian Splatting and test-time finetuning, outperforming existing 2D/2.5D approaches in quality and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing video stabilization methods suffer from issues like distortions, cropping, and poor generalization, degrading user experience.

Method: GaVS reformulates stabilization as a 'local reconstruction and rendering' paradigm, using 3D camera poses, Gaussian Splatting, and test-time finetuning with photometric supervision and regularization.

Result: Quantitatively competitive or superior to state-of-the-art methods; qualitatively better results, validated by user study.

Conclusion: GaVS effectively addresses limitations of existing methods, offering improved stabilization with geometry consistency and minimal cropping.

Abstract: Video stabilization is pivotal for video processing, as it removes unwanted
shakiness while preserving the original user motion intent. Existing
approaches, depending on the domain they operate, suffer from several issues
(e.g. geometric distortions, excessive cropping, poor generalization) that
degrade the user experience. To address these issues, we introduce
\textbf{GaVS}, a novel 3D-grounded approach that reformulates video
stabilization as a temporally-consistent `local reconstruction and rendering'
paradigm. Given 3D camera poses, we augment a reconstruction model to predict
Gaussian Splatting primitives, and finetune it at test-time, with multi-view
dynamics-aware photometric supervision and cross-frame regularization, to
produce temporally-consistent local reconstructions. The model are then used to
render each stabilized frame. We utilize a scene extrapolation module to avoid
frame cropping. Our method is evaluated on a repurposed dataset, instilled with
3D-grounded information, covering samples with diverse camera motions and scene
dynamics. Quantitatively, our method is competitive with or superior to
state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics
and new geometry consistency. Qualitatively, our method produces noticeably
better results compared to alternatives, validated by the user study.

</details>


### [334] [Navigating with Annealing Guidance Scale in Diffusion Space](https://arxiv.org/abs/2506.24108)
*Shai Yehezkel,Omer Dahary,Andrey Voynov,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: A novel annealing guidance scheduler dynamically adjusts the guidance scale in text-to-image diffusion models, improving image quality and prompt alignment without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of Classifier-Free Guidance (CFG) in text-to-image generation depends heavily on the guidance scale, which is typically static and can lead to suboptimal results.

Method: Proposes an annealing guidance scheduler that dynamically adjusts the guidance scale based on the conditional noisy signal, learning an optimal scheduling policy.

Result: Empirical results show enhanced image quality and better alignment with text prompts, outperforming static CFG.

Conclusion: The annealing guidance scheduler offers a seamless, efficient improvement over CFG, balancing quality and prompt adherence without additional resources.

Abstract: Denoising diffusion models excel at generating high-quality images
conditioned on text prompts, yet their effectiveness heavily relies on careful
guidance during the sampling process. Classifier-Free Guidance (CFG) provides a
widely used mechanism for steering generation by setting the guidance scale,
which balances image quality and prompt alignment. However, the choice of the
guidance scale has a critical impact on the convergence toward a visually
appealing and prompt-adherent image. In this work, we propose an annealing
guidance scheduler which dynamically adjusts the guidance scale over time based
on the conditional noisy signal. By learning a scheduling policy, our method
addresses the temperamental behavior of CFG. Empirical results demonstrate that
our guidance scheduler significantly enhances image quality and alignment with
the text prompt, advancing the performance of text-to-image generation.
Notably, our novel scheduler requires no additional activations or memory
consumption, and can seamlessly replace the common classifier-free guidance,
offering an improved trade-off between prompt alignment and quality.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [335] [Computational Analysis of Climate Policy](https://arxiv.org/abs/2506.22449)
*Carolyn Hicks*

Main category: cs.CY

TL;DR: The thesis evaluates the Climate Emergency movement's impact on local government climate policy using GPT-4-based PALLM, finding CED-adopting councils more proactive in climate action.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of large language models (GPT-4) in answering complex policy questions and evaluating the effectiveness of Climate Emergency Declarations (CEDs) in local governments.

Method: Built PALLM (Policy Analysis with a Large Language Model) using GPT-4, validated its performance with policymakers, and analyzed climate policies of 11 Victorian councils, comparing CED and non-CED councils.

Result: GPT-4 is effective for high-level policy analysis, though limited by unreliable attribution. CED councils showed more urgency, prioritization, and equity focus in climate policies.

Conclusion: Large language models like GPT-4 enable scalable policy analysis, revealing CEDs' positive impact on local climate action, opening new research opportunities.

Abstract: This thesis explores the impact of the Climate Emergency movement on local
government climate policy, using computational methods. The Climate Emergency
movement sought to accelerate climate action at local government level through
the mechanism of Climate Emergency Declarations (CEDs), resulting in a series
of commitments from councils to treat climate change as an emergency. With the
aim of assessing the potential of current large language models to answer
complex policy questions, I first built and configured a system named PALLM
(Policy Analysis with a Large Language Model), using the OpenAI model GPT-4.
This system is designed to apply a conceptual framework for climate emergency
response plans to a dataset of climate policy documents. I validated the
performance of this system with the help of local government policymakers, by
generating analyses of the climate policies of 11 local governments in Victoria
and assessing the policymakers' level of agreement with PALLM's responses.
Having established that PALLM's performance is satisfactory, I used it to
conduct a large-scale analysis of current policy documents from local
governments in the state of Victoria, Australia. This thesis presents the
methodology and results of this analysis, comparing the results for councils
which have passed a CED to those which did not. This study finds that GPT-4 is
capable of high-level policy analysis, with limitations including a lack of
reliable attribution, and can also enable more nuanced analysis by researchers.
Its use in this research shows that councils which have passed a CED are more
likely to have a recent and climate-specific policy, and show more attention to
urgency, prioritisation, and equity and social justice, than councils which
have not. It concludes that the ability to assess policy documents at scale
opens up exciting new opportunities for policy researchers.

</details>


### [336] [Theories of "Sexuality" in Natural Language Processing Bias Research](https://arxiv.org/abs/2506.22481)
*Jacob Hobbs*

Main category: cs.CY

TL;DR: The paper analyzes gaps in NLP research regarding queer sexuality representation, revealing unclear definitions and conflations of gender/sexual identities, and suggests improvements for bias analysis.


<details>
  <summary>Details</summary>
Motivation: To address the lack of detailed analysis of how queer sexualities are encoded and misrepresented in NLP systems and research, highlighting gaps in definitions and methodologies.

Method: Surveyed and analyzed 55 articles quantifying sexuality-based NLP bias, focusing on definitions and operationalization of sexuality.

Result: Found unclear definitions of sexuality in most literature, reliance on normative conceptions, and conflation of gender/sexual identities, leading to flawed bias quantifications.

Conclusion: Recommends deeper engagement with queer communities and interdisciplinary literature to improve sexuality-based NLP bias analyses.

Abstract: In recent years, significant advancements in the field of Natural Language
Processing (NLP) have positioned commercialized language models as
wide-reaching, highly useful tools. In tandem, there has been an explosion of
multidisciplinary research examining how NLP tasks reflect, perpetuate, and
amplify social biases such as gender and racial bias. A significant gap in this
scholarship is a detailed analysis of how queer sexualities are encoded and
(mis)represented by both NLP systems and practitioners. Following previous work
in the field of AI fairness, we document how sexuality is defined and
operationalized via a survey and analysis of 55 articles that quantify
sexuality-based NLP bias. We find that sexuality is not clearly defined in a
majority of the literature surveyed, indicating a reliance on assumed or
normative conceptions of sexual/romantic practices and identities. Further, we
find that methods for extracting biased outputs from NLP technologies often
conflate gender and sexual identities, leading to monolithic conceptions of
queerness and thus improper quantifications of bias. With the goal of improving
sexuality-based NLP bias analyses, we conclude with recommendations that
encourage more thorough engagement with both queer communities and
interdisciplinary literature.

</details>


### [337] [A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models](https://arxiv.org/abs/2506.22493)
*Sadia Kamal,Lalu Prasad Yadav Prakash,S M Rafiuddin,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen,Sagnik Ray Choudhury*

Main category: cs.CY

TL;DR: The study examines how variations in generation parameters, prompts, and fine-tuning affect LLM political leanings measured by PCT, finding no significant impact from generation parameters but notable effects from prompts and fine-tuning. It questions the validity of PCT tests and how political biases are encoded in LLMs.


<details>
  <summary>Details</summary>
Motivation: To investigate the validity of Political Compass Tests (PCT) for quantifying LLM political leanings and understand how external factors influence these scores.

Method: Analyzed the impact of generation parameters, prompt variations, and fine-tuning on PCT scores of LLMs, including fine-tuning on politically dense datasets.

Result: Generation parameters had no significant impact, but prompt variations and fine-tuning did. Fine-tuning on politically dense datasets did not differentially affect PCT scores.

Conclusion: The validity of PCT tests and the mechanisms encoding political leanings in LLMs need further investigation.

Abstract: Political Compass Test (PCT) or similar questionnaires have been used to
quantify LLM's political leanings. Building on a recent line of work that
examines the validity of PCT tests, we demonstrate that variation in standard
generation parameters does not significantly impact the models' PCT scores.
However, external factors such as prompt variations and fine-tuning
individually and in combination affect the same. Finally, we demonstrate that
when models are fine-tuned on text datasets with higher political content than
others, the PCT scores are not differentially affected. This calls for a
thorough investigation into the validity of PCT and similar tests, as well as
the mechanism by which political leanings are encoded in LLMs.

</details>


### [338] [Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety](https://arxiv.org/abs/2506.22496)
*Y. Du*

Main category: cs.CY

TL;DR: LLMs show gambling-like behaviors (overconfidence, loss-chasing, probability misjudgment). The RARG framework mitigates these biases, reducing them by 18.7-24.3%.


<details>
  <summary>Details</summary>
Motivation: To address systematic risk-taking behaviors in LLMs, inspired by gambling psychology and prospect theory.

Method: Proposed the RARG framework with risk-calibrated training, loss-aversion mechanisms, and uncertainty-aware decision making. Evaluated using gambling psychology experiments (e.g., Iowa Gambling Task).

Result: Reduced overconfidence bias by 18.7%, loss-chasing by 24.3%, and improved risk calibration.

Conclusion: First framework to understand and mitigate gambling-like behaviors in AI systems.

Abstract: Large Language Models (LLMs) exhibit systematic risk-taking behaviors
analogous to those observed in gambling psychology, including overconfidence
bias, loss-chasing tendencies, and probability misjudgment. Drawing from
behavioral economics and prospect theory, we identify and formalize these
"gambling-like" patterns where models sacrifice accuracy for high-reward
outputs, exhibit escalating risk-taking after errors, and systematically
miscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)
framework, incorporating insights from gambling research to address these
behavioral biases through risk-calibrated training, loss-aversion mechanisms,
and uncertainty-aware decision making. Our approach introduces novel evaluation
paradigms based on established gambling psychology experiments, including AI
adaptations of the Iowa Gambling Task and probability learning assessments.
Experimental results demonstrate measurable reductions in gambling-like
behaviors: 18.7\% decrease in overconfidence bias, 24.3\% reduction in
loss-chasing tendencies, and improved risk calibration across diverse
scenarios. This work establishes the first systematic framework for
understanding and mitigating gambling psychology patterns in AI systems.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [339] [Density, asymmetry and citation dynamics in scientific literature](https://arxiv.org/abs/2506.23366)
*Nathaniel Imel,Zachary Hafen*

Main category: cs.DL

TL;DR: The paper explores how a paper's similarity to prior research affects its citation rate, using density and asymmetry metrics. Density modestly improves citation predictions, while asymmetry does not.


<details>
  <summary>Details</summary>
Motivation: To understand the tension between building on established knowledge and introducing novelty, and how this impacts scientific impact (citation rates).

Method: Introduced two metrics (density and asymmetry) to measure similarity to prior work, tested their predictive power on citation rates using Bayesian hierarchical regression across 53,000 papers and multiple embeddings.

Result: Density modestly improves citation predictions; asymmetry does not. The framework links semantic similarity to scientometric outcomes.

Conclusion: Density of a paper's semantic neighborhood offers modest predictive value for citations, while asymmetry does not. The study provides a scalable framework for analyzing scientific impact.

Abstract: Scientific behavior is often characterized by a tension between building upon
established knowledge and introducing novel ideas. Here, we investigate whether
this tension is reflected in the relationship between the similarity of a
scientific paper to previous research and its eventual citation rate. To
operationalize similarity to previous research, we introduce two complementary
metrics to characterize the local geometry of a publication's semantic
neighborhood: (1) \emph{density} ($\rho$), defined as the ratio between a fixed
number of previously-published papers and the minimum distance enclosing those
papers in a semantic embedding space, and (2) asymmetry ($\alpha$), defined as
the average directional difference between a paper and its nearest neighbors.
We tested the predictive relationship between these two metrics and its
subsequent citation rate using a Bayesian hierarchical regression approach,
surveying $\sim 53,000$ publications across nine academic disciplines and five
different document embeddings. While the individual effects of $\rho$ on
citation count are small and variable, incorporating density-based predictors
consistently improves out-of-sample prediction when added to baseline models.
These results suggest that the density of a paper's surrounding scientific
literature may carry modest but informative signals about its eventual impact.
Meanwhile, we find no evidence that publication asymmetry improves model
predictions of citation rates. Our work provides a scalable framework for
linking document embeddings to scientometric outcomes and highlights new
questions regarding the role that semantic similarity plays in shaping the
dynamics of scientific reward.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [340] [TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity](https://arxiv.org/abs/2506.23484)
*Yuzhuo Chen,Zehua Ma,Han Fang,Weiming Zhang,Nenghai Yu*

Main category: cs.MM

TL;DR: TAG-WM is a tamper-aware generative watermarking method that enhances robustness and localization while preserving image quality.


<details>
  <summary>Details</summary>
Motivation: Address copyright and authenticity risks in AI-generated content by improving watermarking robustness and tampering localization.

Method: Proposes TAG-WM with four modules: DMJS for watermark embedding, WLR for reconstruction, DVRD for tamper detection, and TAD for decoding.

Result: Achieves state-of-the-art tampering robustness and localization with minimal distortion and 256-bit capacity.

Conclusion: TAG-WM effectively addresses current watermarking limitations, offering a robust solution for AI-generated content integrity.

Abstract: AI-generated content (AIGC) enables efficient visual creation but raises
copyright and authenticity risks. As a common technique for integrity
verification and source tracing, digital image watermarking is regarded as a
potential solution to above issues. Among these, watermarking methods capable
of preserving the generation quality are receiving increased attention.
However, the proliferation and high performance of generative image editing
applications have elevated the risks of malicious tampering, creating new
demands. 1) The tamper robustness of current lossless visual quality watermarks
remains constrained by the modification-sensitive diffusion inversion process,
necessitating enhanced robustness. 2) The improved tampering quality and rapid
iteration cycles render passive tampering detection methods inadequate, making
proactive tampering localization capability a desired feature for watermarks.
To address these requirements, this paper proposes a Tamper-Aware Generative
image WaterMarking method named TAG-WM. The proposed method comprises four key
modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright
and localization watermarks into the latent space while preserving generative
quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a
dense variation region detector (DVRD) leveraging diffusion inversion
sensitivity to identify tampered areas via statistical deviation analysis, and
the tamper-aware decoding (TAD) guided by localization results. The
experimental results indicate that TAG-WM achieves SOTA tampering robustness
and tampering localization capability with distortions while maintaining
lossless generation quality and a considerable capacity of 256 bits.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [341] [Wireless Home Automation Using Social Networking Websites](https://arxiv.org/abs/2506.22482)
*Divya Alok Gupta,Dwith Chenna,B. Aditya Vighnesh Ramakanth*

Main category: cs.NI

TL;DR: A secure WHAS using social media authentication (e.g., Twitter) to control home appliances, addressing security and user-friendliness challenges.


<details>
  <summary>Details</summary>
Motivation: To overcome security and usability issues in WHAS by leveraging social media authentication and activity tracking.

Method: Proposes a system integrating social networking authentication (Twitter) to track user activities and control appliances.

Result: Highlights applications and advantages of the proposed WHAS over traditional systems.

Conclusion: The proposed WHAS offers improved security and user convenience compared to conventional home automation systems.

Abstract: With the advent of Internet of Things, Wireless Home Automation Systems WHAS
are gradually gaining popularity. These systems are faced with multiple
challenges such as security; controlling a variety of home appliances with a
single interface and user friendliness. In this paper we propose a system that
uses secure authentication systems of social networking websites such as
Twitter, tracks the end-users activities on the social network and then control
his or her domestic appliances. At the end, we highlight the applications of
the proposed WHAS and compare the advantages of our proposed system over
traditional home automation systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [342] [SegmentAnyMuscle: A universal muscle segmentation model across different locations in MRI](https://arxiv.org/abs/2506.22467)
*Roy Colglazier,Jisoo Lee,Haoyu Dong,Hanxue Gu,Yaqian Chen,Joseph Cao,Zafer Yildiz,Zhonghao Liu,Nicholas Konz,Jichen Yang,Jikai Zhang,Yuwen Chen,Lin Li,Adrian Camarena,Maciej A. Mazurowski*

Main category: eess.SP

TL;DR: A publicly available deep learning model for muscle segmentation in MRIs was developed, achieving high accuracy across diverse imaging sequences and anatomical locations.


<details>
  <summary>Details</summary>
Motivation: Precise quantitative muscle measurements in MRIs are challenging but important for health outcomes. This study aimed to create a reproducible tool for such assessments.

Method: The model was trained on 316 MRIs from 114 patients and tested on two sets: one with common sequences (28 MRIs) and another with rare sequences/abnormalities (18 MRIs), using Dice Similarity Coefficient (DSC) for evaluation.

Result: The model achieved 88.45% DSC on common sequences and 86.21% DSC on rare sequences/abnormalities, demonstrating robust performance.

Conclusion: The study successfully developed a publicly available, automated deep learning model for muscle segmentation in MRIs, facilitating consistent research on musculature and health.

Abstract: The quantity and quality of muscles are increasingly recognized as important
predictors of health outcomes. While MRI offers a valuable modality for such
assessments, obtaining precise quantitative measurements of musculature remains
challenging. This study aimed to develop a publicly available model for muscle
segmentation in MRIs and demonstrate its applicability across various
anatomical locations and imaging sequences. A total of 362 MRIs from 160
patients at a single tertiary center (Duke University Health System, 2016-2020)
were included, with 316 MRIs from 114 patients used for model development. The
model was tested on two separate sets: one with 28 MRIs representing common
sequence types, achieving an average Dice Similarity Coefficient (DSC) of
88.45%, and another with 18 MRIs featuring less frequent sequences and
abnormalities such as muscular atrophy, hardware, and significant noise,
achieving 86.21% DSC. These results demonstrate the feasibility of a fully
automated deep learning algorithm for segmenting muscles on MRI across diverse
settings. The public release of this model enables consistent, reproducible
research into the relationship between musculature and health.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [343] [Supervised Diffusion-Model-Based PET Image Reconstruction](https://arxiv.org/abs/2506.24034)
*George Webber,Alexander Hammers,Andrew P King,Andrew J Reader*

Main category: physics.med-ph

TL;DR: A supervised diffusion model (DM) for PET image reconstruction is proposed, outperforming existing methods by integrating Poisson likelihood and handling PET's intensity range, with improved posterior sampling and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Current DM-based PET reconstruction methods lack explicit modeling of the interaction between the DM prior and noisy data, limiting accuracy.

Method: A supervised DM algorithm enforces PET's Poisson likelihood non-negativity and accommodates PET's intensity range, validated on brain PET phantoms and real data.

Result: The method outperforms or matches state-of-the-art deep learning methods across dose levels and improves posterior sampling accuracy.

Conclusion: The supervised DM approach enhances PET reconstruction accuracy and uncertainty estimation, with practical applicability to 3D PET data.

Abstract: Diffusion models (DMs) have recently been introduced as a regularizing prior
for PET image reconstruction, integrating DMs trained on high-quality PET
images with unsupervised schemes that condition on measured data. While these
approaches have potential generalization advantages due to their independence
from the scanner geometry and the injected activity level, they forgo the
opportunity to explicitly model the interaction between the DM prior and noisy
measurement data, potentially limiting reconstruction accuracy. To address
this, we propose a supervised DM-based algorithm for PET reconstruction. Our
method enforces the non-negativity of PET's Poisson likelihood model and
accommodates the wide intensity range of PET images. Through experiments on
realistic brain PET phantoms, we demonstrate that our approach outperforms or
matches state-of-the-art deep learning-based methods quantitatively across a
range of dose levels. We further conduct ablation studies to demonstrate the
benefits of the proposed components in our model, as well as its dependence on
training data, parameter count, and number of diffusion steps. Additionally, we
show that our approach enables more accurate posterior sampling than
unsupervised DM-based methods, suggesting improved uncertainty estimation.
Finally, we extend our methodology to a practical approach for fully 3D PET and
present example results from real [$^{18}$F]FDG brain PET data.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [344] [Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation](https://arxiv.org/abs/2506.23717)
*Xingting Yao,Qinghao Hu,Fei Zhou,Tielong Liu,Gang Li,Peisong Wang,Jian Cheng*

Main category: cs.NE

TL;DR: The paper introduces an adaptive bit allocation strategy for multi-bit SNNs to optimize memory and computation resources, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Multi-bit SNNs face disproportionate performance improvements due to escalating memory and computation demands with more bits. Different layers' varying importance and wasted bits necessitate fine-grained resource allocation.

Method: The paper parametrizes temporal lengths and bit widths of weights and spikes, making them learnable via gradients. A refined spiking neuron handles variable bit widths and temporal lengths, and a step-size renewal mechanism addresses quantization errors.

Result: Experiments on static (CIFAR, ImageNet) and dynamic (CIFAR-DVS, DVS-GESTURE) datasets show reduced memory and computation costs with higher accuracy, e.g., 2.69% accuracy gain and 4.16× lower bit budgets on ImageNet.

Conclusion: The adaptive bit allocation strategy effectively enhances SNN efficiency and accuracy, validated by significant improvements across diverse datasets.

Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated
research spot, pursuing energy-efficient and high-accurate AI. However, with
more bits involved, the associated memory and computation demands escalate to
the point where the performance improvements become disproportionate. Based on
the insight that different layers demonstrate different importance and extra
bits could be wasted and interfering, this paper presents an adaptive bit
allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise
allocation of memory and computation resources. Thus, SNN's efficiency and
accuracy can be improved. Specifically, we parametrize the temporal lengths and
the bit widths of weights and spikes, and make them learnable and controllable
through gradients. To address the challenges caused by changeable bit widths
and temporal lengths, we propose the refined spiking neuron, which can handle
different temporal lengths, enable the derivation of gradients for temporal
lengths, and suit spike quantization better. In addition, we theoretically
formulate the step-size mismatch problem of learnable bit widths, which may
incur severe quantization errors to SNN, and accordingly propose the step-size
renewal mechanism to alleviate this issue. Experiments on various datasets,
including the static CIFAR and ImageNet and the dynamic CIFAR-DVS and
DVS-GESTURE, demonstrate that our methods can reduce the overall memory and
computation cost while achieving higher accuracy. Particularly, our
SEWResNet-34 can achieve a 2.69\% accuracy gain and 4.16$\times$ lower bit
budgets over the advanced baseline work on ImageNet. This work will be fully
open-sourced.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [345] [Maximum Dispersion, Maximum Concentration: Enhancing the Quality of MOP Solutions](https://arxiv.org/abs/2506.22568)
*Gladston Moreira,Ivan Meneghini,Elzabeth Wanner*

Main category: math.OC

TL;DR: The paper introduces a method to improve multi-objective optimization by balancing dispersion in the decision space and convergence in a specific objective space region, enhancing solution quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between conflicting objectives in MOPs and avoiding bias from clustered solutions.

Method: Defines a Region of Interest (ROI) in the objective space and uses a uniformity measure to enhance dispersion in the decision space.

Result: Preliminary experiments show improved solution quality and diversity by balancing dispersion and concentration.

Conclusion: The approach effectively mitigates bias and enhances multi-objective optimization by optimizing dispersion and convergence.

Abstract: Multi-objective optimization problems (MOPs) often require a trade-off
between conflicting objectives, maximizing diversity and convergence in the
objective space. This study presents an approach to improve the quality of MOP
solutions by optimizing the dispersion in the decision space and the
convergence in a specific region of the objective space. Our approach defines a
Region of Interest (ROI) based on a cone representing the decision maker's
preferences in the objective space, while enhancing the dispersion of solutions
in the decision space using a uniformity measure. Combining solution
concentration in the objective space with dispersion in the decision space
intensifies the search for Pareto-optimal solutions while increasing solution
diversity. When combined, these characteristics improve the quality of
solutions and avoid the bias caused by clustering solutions in a specific
region of the decision space. Preliminary experiments suggest that this method
enhances multi-objective optimization by generating solutions that effectively
balance dispersion and concentration, thereby mitigating bias in the decision
space.

</details>


### [346] [Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations](https://arxiv.org/abs/2506.22826)
*Robert Beinert,Jonas Bresch*

Main category: math.OC

TL;DR: The paper extends a numerically efficient denoising approach for manifold-valued data to multi-binary and Stiefel-valued data, proposing TV- and Tikhonov-based models with convex relaxations.


<details>
  <summary>Details</summary>
Motivation: To address denoising challenges for new data types like multi-binary (e.g., multi-color QR codes) and Stiefel-valued data (e.g., image/video recognition).

Method: Embed data in Euclidean space, encode manifolds via fixed-rank matrices, relax rank constraints for convexification, and apply TV/Tikhonov models.

Result: Proposed methods are evaluated on synthetic experiments, demonstrating feasibility.

Conclusion: The approach successfully extends to new data types, offering efficient denoising solutions.

Abstract: The handling of manifold-valued data, for instance, plays a central role in
color restoration tasks relying on circle- or sphere-valued color models, in
the study of rotational or directional information related to the special
orthogonal group, and in Gaussian image processing, where the pixel statistics
are interpreted as values on the hyperbolic sheet. Especially, to denoise these
kind of data, there have been proposed several generalizations of total
variation (TV) and Tikhonov-type denoising models incorporating the underlying
manifolds. Recently, a novel, numerically efficient denoising approach has been
introduced, where the data are embedded in an Euclidean ambient space, the
non-convex manifolds are encoded by a series of positive semi-definite,
fixed-rank matrices, and the rank constraint is relaxed to obtain a
convexification that can be solved using standard algorithms from convex
analysis. The aim of the present paper is to extent this approach to new kinds
of data like multi-binary and Stiefel-valued data. Multi-binary data can, for
instance, be used to model multi-color QR codes whereas Stiefel-valued data
occur in image and video-based recognition. For both new data types, we propose
TV- and Tikhonov-based denoising modelstogether with easy-to-solve
convexification. All derived methods are evaluated on proof-of-concept,
synthetic experiments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [347] [Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks](https://arxiv.org/abs/2506.23016)
*Tomás Silva Santos Rocha,Anastasiia Mikhailova,Moreno I. Coco,José Santos-Victor*

Main category: cs.HC

TL;DR: A deep learning model using eye-tracking data achieved 68% sensitivity and 76% specificity in distinguishing MCI from healthy controls, comparable to similar studies.


<details>
  <summary>Details</summary>
Motivation: The rising global prevalence of dementia necessitates scalable diagnostic tools, with MCI as a key early indicator.

Method: A VTNet-based deep learning model analyzed eye-tracking data (time series, spatial data, scan paths, heatmaps) from 44 participants performing a visual memory task.

Result: The model achieved 68% sensitivity and 76% specificity, comparable to similar Alzheimer's studies.

Conclusion: The study supports automated MCI diagnostics; future work should refine the model and use standardized tasks.

Abstract: The global prevalence of dementia is projected to double by 2050,
highlighting the urgent need for scalable diagnostic tools. This study utilizes
digital cognitive tasks with eye-tracking data correlated with memory processes
to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment
(MCI), a precursor to dementia. A deep learning model based on VTNet was
trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who
performed a visual memory task. The model utilizes both time series and spatial
data derived from eye-tracking. It was modified to incorporate scan paths, heat
maps, and image content. These modifications also enabled testing parameters
such as image resolution and task performance, analyzing their impact on model
performance. The best model, utilizing $700\times700px$ resolution heatmaps,
achieved 68% sensitivity and 76% specificity. Despite operating under more
challenging conditions (e.g., smaller dataset size, shorter task duration, or a
less standardized task), the model's performance is comparable to an
Alzheimer's study using similar methods (70% sensitivity and 73% specificity).
These findings contribute to the development of automated diagnostic tools for
MCI. Future work should focus on refining the model and using a standardized
long-term visual memory task.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [348] [Teaching a Language Model to Speak the Language of Tools](https://arxiv.org/abs/2506.23394)
*Simeon Emanuilov*

Main category: cs.IR

TL;DR: The paper introduces TUCAN, a method to adapt multilingual models for reliable tool use in non-English languages, using Bulgarian as a case study. It achieves significant accuracy improvements and clean output formatting.


<details>
  <summary>Details</summary>
Motivation: Most multilingual models lack reliable tool-use capabilities in non-English languages, especially lower-resource ones, due to language confusion and poor structured output generation.

Method: Continued training of BgGPT models on a bilingual dataset of function-calling examples, supporting standardized protocols like MCP, to create TUCAN.

Result: TUCAN improves function-calling accuracy by up to 28.75% over base models while maintaining language understanding and producing clean, parsable outputs.

Conclusion: The work provides a practical approach for extending tool-augmented capabilities to non-English languages, with released models, datasets, and frameworks for replication.

Abstract: External tool integration through function-calling is essential for practical
language model applications, yet most multilingual models lack reliable
tool-use capabilities in non-English languages. Even state-of-the-art
multilingual models struggle with determining when to use tools and generating
the structured outputs required for function calls, often exhibiting language
confusion when prompted in lower-resource languages. This work presents a
methodology for adapting existing language models to enable robust tool use in
any target language, using Bulgarian as a case study. The approach involves
continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a
novel bilingual dataset of 10,035 function-calling examples designed to support
standardized protocols like MCP (Model Context Protocol). The research
introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to
28.75% improvement in function-calling accuracy over base models while
preserving core language understanding, as verified on established Bulgarian
benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready
response formatting with clean, parsable function calls, contrasting with the
verbose and inconsistent outputs of base models. The models, evaluation
framework, and dataset are released to enable replication for other languages.
This work demonstrates a practical approach for extending tool-augmented
capabilities beyond English-centric systems.

</details>


### [349] [KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On](https://arxiv.org/abs/2506.23471)
*Thanh-Tung Phan-Nguyen,Khoi-Nguyen Nguyen-Ngoc,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.IR

TL;DR: A novel KiseKloset system enhances online fashion shopping with outfit retrieval, recommendation, and virtual try-on, improving user experience and reducing costs.


<details>
  <summary>Details</summary>
Motivation: To improve customer engagement and satisfaction in online fashion shopping by offering personalized recommendations and realistic virtual try-on experiences.

Method: Proposes a transformer-based architecture for complementary item recommendations, integrates approximate algorithms for search optimization, and employs a lightweight virtual try-on framework.

Result: 84% of users found the system highly useful, significantly enhancing their shopping experience.

Conclusion: The KiseKloset system effectively addresses key challenges in fashion e-commerce, providing a comprehensive solution for personalized shopping.

Abstract: The global fashion e-commerce industry has become integral to people's daily
lives, leveraging technological advancements to offer personalized shopping
experiences, primarily through recommendation systems that enhance customer
engagement through personalized suggestions. To improve customers' experience
in online shopping, we propose a novel comprehensive KiseKloset system for
outfit retrieval, recommendation, and try-on. We explore two approaches for
outfit retrieval: similar item retrieval and text feedback-guided item
retrieval. Notably, we introduce a novel transformer architecture designed to
recommend complementary items from diverse categories. Furthermore, we enhance
the overall performance of the search pipeline by integrating approximate
algorithms to optimize the search process. Additionally, addressing the crucial
needs of online shoppers, we employ a lightweight yet efficient virtual try-on
framework capable of real-time operation, memory efficiency, and maintaining
realistic outputs compared to its predecessors. This virtual try-on module
empowers users to visualize specific garments on themselves, enhancing the
customers' experience and reducing costs associated with damaged items for
retailers. We deployed our end-to-end system for online users to test and
provide feedback, enabling us to measure their satisfaction levels. The results
of our user study revealed that 84% of participants found our comprehensive
system highly useful, significantly improving their online shopping experience.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [350] [Reachability in symmetric VASS](https://arxiv.org/abs/2506.23578)
*Łukasz Kamiński,Sławomir Lasota*

Main category: cs.FL

TL;DR: The paper explores reachability in symmetric VASS with permutation-invariant transitions, showing PSPACE complexity for symmetric groups and comparing it to general VASS.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of symmetry groups on the complexity of reachability in VASS, especially for applications like data VASS.

Method: Analyzes reachability under different groups (trivial, symmetric, alternating, cyclic) and combinations thereof.

Result: Reachability is PSPACE-complete for symmetric groups, contrasting with Ackermannian complexity in general VASS.

Conclusion: Symmetry groups significantly reduce reachability complexity, offering insights for data VASS and related problems.

Abstract: We investigate the reachability problem in symmetric vector addition systems
with states (VASS), where transitions are invariant under a group of
permutations of coordinates. One extremal case, the trivial groups, yields
general VASS. In another extremal case, the symmetric groups, we show that the
reachability problem can be solved in PSPACE, regardless of the dimension of
input VASS (to be contrasted with Ackermannian complexity in general VASS). We
also consider other groups, in particular alternating and cyclic ones.
Furthermore, motivated by the open status of the reachability problem in data
VASS, we estimate the gain in complexity when the group arises as a combination
of the trivial and symmetric groups.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [351] [GaussMaster: An LLM-based Database Copilot System](https://arxiv.org/abs/2506.23322)
*Wei Zhou,Ji Sun,Xuanhe Zhou,Guoliang Li,Luyang Liu,Hao Wu,Tianyuan Wang*

Main category: cs.DB

TL;DR: GaussMaster is an LLM-based database copilot system designed to automate comprehensive database maintenance, reducing human intervention in tasks like SQL tuning and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on autonomous database platforms is limited to single-point solutions, leaving manual intervention necessary for broader maintenance tasks. GaussMaster aims to address this gap.

Method: GaussMaster uses an LLM-based system to analyze metrics and logs, employs a Tree-of-thought approach for root cause analysis, and automates issue resolution.

Result: Successfully implemented in banking, achieving zero human intervention in 34+ maintenance scenarios.

Conclusion: GaussMaster demonstrates significant advancements in autonomous database maintenance, offering a scalable solution for the financial industry.

Abstract: In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [352] [DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios](https://arxiv.org/abs/2506.22494)
*Shihong Ling,Yue Wan,Xiaowei Jia,Na Du*

Main category: cs.RO

TL;DR: DriveBLIP2, a framework based on BLIP2-OPT, improves explanation generation for driving scenarios by using an Attention Map Generator to highlight key objects, outperforming baselines on the DRAMA dataset.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models struggle with complex, multi-object environments in real-time applications like autonomous driving, where rapid object identification is critical.

Method: Proposes an Attention Map Generator to focus on significant objects in driving scenarios, enhancing explanation clarity.

Result: Evaluations on DRAMA dataset show higher BLEU, ROUGE, CIDEr, and SPICE scores, indicating better explanation quality.

Conclusion: Targeted attention mechanisms in vision-language models can significantly improve explainability in autonomous driving.

Abstract: This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT
architecture, to generate accurate and contextually relevant explanations for
emerging driving scenarios. While existing vision-language models perform well
in general tasks, they encounter difficulties in understanding complex,
multi-object environments, particularly in real-time applications such as
autonomous driving, where the rapid identification of key objects is crucial.
To address this limitation, an Attention Map Generator is proposed to highlight
significant objects relevant to driving decisions within critical video frames.
By directing the model's focus to these key regions, the generated attention
map helps produce clear and relevant explanations, enabling drivers to better
understand the vehicle's decision-making process in critical situations.
Evaluations on the DRAMA dataset reveal significant improvements in explanation
quality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared
to baseline models. These findings underscore the potential of targeted
attention mechanisms in vision-language models for enhancing explainability in
real-time autonomous driving.

</details>


### [353] [Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding](https://arxiv.org/abs/2506.22593)
*Antonello Longo,Chanyoung Chung,Matteo Palieri,Sung-Kyun Kim,Ali Agha,Cataldo Guaragnella,Shehryar Khattak*

Main category: cs.RO

TL;DR: Pix2G is a lightweight method for real-time 3D scene graph generation from images and LiDAR, enabling autonomous exploration on resource-constrained robots.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human-readable 2D maps and robot 3D maps for efficient human-robot cooperation in high-risk tasks.

Method: Pix2G generates structured scene graphs from image pixels and LiDAR maps, operating on CPU-only to meet onboard constraints.

Result: Produces a de-noised 2D map and segmented 3D pointcloud, connected via a multi-layer graph. Tested successfully in real-world scenarios.

Conclusion: Pix2G effectively enables real-time autonomous exploration in cluttered environments, validated by NASA JPL NeBula-Spot robot experiments.

Abstract: Autonomous robots are increasingly playing key roles as support platforms for
human operators in high-risk, dangerous applications. To accomplish challenging
tasks, an efficient human-robot cooperation and understanding is required.
While typically robotic planning leverages 3D geometric information, human
operators are accustomed to a high-level compact representation of the
environment, like top-down 2D maps representing the Building Information Model
(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap
between human readable 2D BIM and the robot 3D maps. In this work, we introduce
Pixels-to-Graph (Pix2G), a novel lightweight method to generate structured
scene graphs from image pixels and LiDAR maps in real-time for the autonomous
exploration of unknown environments on resource-constrained robot platforms. To
satisfy onboard compute constraints, the framework is designed to perform all
operation on CPU only. The method output are a de-noised 2D top-down
environment map and a structure-segmented 3D pointcloud which are seamlessly
connected using a multi-layer graph abstracting information from object-level
up to the building-level. The proposed method is quantitatively and
qualitatively evaluated during real-world experiments performed using the NASA
JPL NeBula-Spot legged robot to autonomously explore and map cluttered garage
and urban office like environments in real-time.

</details>


### [354] [InfGen: Scenario Generation as Next Token Group Prediction](https://arxiv.org/abs/2506.23316)
*Zhenghao Peng,Yuxin Liu,Bolei Zhou*

Main category: cs.RO

TL;DR: InfGen is an autoregressive scenario generation framework for dynamic traffic simulation, using a transformer model to produce realistic and diverse traffic behaviors for autonomous driving training.


<details>
  <summary>Details</summary>
Motivation: Existing traffic simulation methods are limited by static or log-replay data, hindering dynamic, long-horizon scenario modeling.

Method: InfGen tokenizes the scene (traffic lights, agent states, motion vectors) and uses a transformer to autoregressively simulate traffic, enabling continuous agent insertion.

Result: InfGen generates realistic, diverse, and adaptive traffic behaviors, improving robustness and generalization in reinforcement learning policies.

Conclusion: InfGen serves as a high-fidelity simulation environment for autonomous driving, addressing limitations of current methods.

Abstract: Realistic and interactive traffic simulation is essential for training and
evaluating autonomous driving systems. However, most existing data-driven
simulation methods rely on static initialization or log-replay data, limiting
their ability to model dynamic, long-horizon scenarios with evolving agent
populations. We propose InfGen, a scenario generation framework that outputs
agent states and trajectories in an autoregressive manner. InfGen represents
the entire scene as a sequence of tokens, including traffic light signals,
agent states, and motion vectors, and uses a transformer model to simulate
traffic over time. This design enables InfGen to continuously insert new agents
into traffic, supporting infinite scene generation. Experiments demonstrate
that InfGen produces realistic, diverse, and adaptive traffic behaviors.
Furthermore, reinforcement learning policies trained in InfGen-generated
scenarios achieve superior robustness and generalization, validating its
utility as a high-fidelity simulation environment for autonomous driving. More
information is available at https://metadriverse.github.io/infgen/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [355] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: MARBLE is a multimodal reasoning benchmark designed to test MLLMs' ability to reason step-by-step through complex multimodal problems, revealing their current limitations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on complex multimodal reasoning, limiting understanding of MLLMs' capabilities in this domain.

Method: MARBLE includes two tasks, M-Portal and M-Cube, requiring multistep planning under spatial, visual, and physical constraints.

Result: Current MLLMs perform poorly, with near-random accuracy on M-Portal and 0% on M-Cube, highlighting their reasoning and perception limitations.

Conclusion: MARBLE exposes MLLMs' weaknesses, aiming to inspire development of models capable of advanced multimodal reasoning.

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [356] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: AURA is the first open-source, speech-native assistant for multi-turn dialogue with tool use, outperforming open-weight systems on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open-source systems for speech-to-speech, multi-turn dialogue with tool use and agentic reasoning.

Method: Combines ASR, TTS, and LLMs in a cascaded pipeline, supports tools like calendar booking, and uses modular design for easy tool integration.

Result: Scores 92.75% on VoiceBench (OpenBookQA) and 4.39 on AlpacaEval, with 90% human-evaluated task success.

Conclusion: AURA demonstrates strong performance in complex, multi-turn speech tasks, setting a benchmark for open-source assistants.

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [357] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: The paper investigates how LLMs balance self-interest and collective well-being in multi-agent systems, revealing distinct behavioral patterns and highlighting challenges in achieving cooperation despite enhanced reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' cooperation and social mechanisms is crucial for their safe and robust deployment as autonomous agents, especially in scenarios requiring collective decision-making.

Method: The study adapts a public goods game with institutional choice from behavioral economics to observe LLMs' behavior in repeated social dilemmas.

Result: Four behavioral patterns emerge: sustained cooperation, fluctuating engagement, declining cooperation, and rigid strategies. Reasoning LLMs struggle with cooperation, while some traditional models excel.

Conclusion: Enhancing reasoning capabilities does not guarantee cooperation, offering insights for deploying LLMs in collaborative environments.

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [358] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: GPTZero is effective in detecting AI-generated texts but less reliable for human-written ones, with some false positives. Educators should use caution.


<details>
  <summary>Details</summary>
Motivation: To evaluate the reliability of GPTZero in detecting AI-generated texts across different essay lengths.

Method: Tested GPTZero on 28 AI-generated and 50 human-written essays of varying lengths (short, medium, long).

Result: AI-generated texts were accurately detected (91-100%), but human-written texts showed false positives.

Conclusion: GPTZero is reliable for AI detection but less so for human texts; educators should not rely solely on it.

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [359] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: MMReason is a new benchmark designed to evaluate Multimodal Large Language Models' long-chain reasoning abilities, addressing gaps in existing benchmarks by offering diverse, challenging questions and robust assessment methods.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs lack precision and comprehensiveness in evaluating long-chain reasoning, particularly in difficulty, diversity, and intermediate step assessment.

Method: MMReason curates multi-step reasoning questions from various fields and difficulty levels, reformulates them into open-ended formats, filters for robustness, and uses annotated solutions with a ternary scoring mechanism.

Result: The benchmark evaluates popular MLLMs, providing detailed analysis of their reasoning capabilities.

Conclusion: MMReason aims to advance MLLM reasoning research by offering a precise and comprehensive evaluation tool.

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [360] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: Attestable Audits use Trusted Execution Environments to verify AI model compliance while protecting sensitive data, addressing challenges in AI governance.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack verifiable results and confidentiality for model IP and datasets, posing risks in AI governance.

Method: Proposed Attestable Audits run inside Trusted Execution Environments, enabling verifiable interaction with compliant AI models.

Result: A prototype demonstrates feasibility on typical audit benchmarks against Llama-3.1, ensuring data protection even without mutual trust.

Conclusion: Attestable Audits provide a secure and verifiable solution for AI model compliance, aligning with governance needs.

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [361] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: SPIRAL is a self-play framework for LLMs that eliminates human supervision by using multi-turn, zero-sum games, leading to improved reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To develop reasoning in language models without relying on human-curated data or domain-specific rewards.

Method: SPIRAL uses self-play in zero-sum games, role-conditioned advantage estimation (RAE), and an online multi-agent RL system.

Result: Training with SPIRAL improves reasoning (8.6% on math, 8.4% on general reasoning) and transfers skills across tasks.

Conclusion: Zero-sum games autonomously develop transferable reasoning, offering a scalable approach for model improvement.

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [362] [VERA: Variational Inference Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2506.22666)
*Anamika Lochab,Lu Yan,Patrick Pynadath,Xiangyu Zhang,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA is a variational inference framework for jailbreaking LLMs, enabling diverse and fluent adversarial prompt generation without re-optimization.


<details>
  <summary>Details</summary>
Motivation: The need for effective black-box jailbreak methods to identify vulnerabilities in LLMs, as existing methods are limited by initialization and manual curation.

Method: VERA frames jailbreak prompting as a variational inference problem, training a small attacker LLM to approximate the target LLM's posterior over adversarial prompts.

Result: VERA achieves strong performance across various target LLMs, demonstrating the effectiveness of probabilistic inference for adversarial prompt generation.

Conclusion: VERA provides a scalable and efficient solution for comprehensively characterizing model vulnerabilities through variational inference.

Abstract: The rise of API-only access to state-of-the-art LLMs highlights the need for
effective black-box jailbreak methods to identify model vulnerabilities in
real-world settings. Without a principled objective for gradient-based
optimization, most existing approaches rely on genetic algorithms, which are
limited by their initialization and dependence on manually curated prompt
pools. Furthermore, these methods require individual optimization for each
prompt, failing to provide a comprehensive characterization of model
vulnerabilities. To address this gap, we introduce VERA: Variational infErence
fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a
variational inference problem, training a small attacker LLM to approximate the
target LLM's posterior over adversarial prompts. Once trained, the attacker can
generate diverse, fluent jailbreak prompts for a target query without
re-optimization. Experimental results show that VERA achieves strong
performance across a range of target LLMs, highlighting the value of
probabilistic inference for adversarial prompt generation.

</details>


### [363] [Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models](https://arxiv.org/abs/2506.24056)
*Tung-Ling Li,Hongliang Liu*

Main category: cs.CR

TL;DR: Logit-gap steering is a fast jailbreak framework for RLHF-aligned language models, reducing refusal-affirmation gaps efficiently with minimal model calls.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of traditional jailbreak methods (e.g., beam or gradient attacks) and improve attack success rates while maintaining coherence.

Method: Uses a forward-computable score combining gap reduction, KL penalty, and reward shift, enabling a quick "sort-sum-stop" sweep to generate short suffixes.

Result: Achieves 80-100% one-shot attack success, generalizes across prompts and model sizes (0.5B to 70B), and exposes alignment artefacts.

Conclusion: Logit-gap steering is efficient, effective, and provides insights into safety tuning's impact on model representations.

Abstract: We introduce logit-gap steering, a fast jailbreak framework that casts the
refusal-affirmation gap of RLHF-aligned language models as a single pass over
the vocabulary. A forward-computable score blends gap reduction with
lightweight proxies for KL penalty and reward shift, allowing a "sort-sum-stop"
sweep to complete in under a second and return a short suffix--two orders of
magnitude fewer model calls than beam or gradient attacks. The same suffix
generalises to unseen prompts and scales from 0.5 B to 70 B checkpoints,
lifting one-shot attack success from baseline levels to 80-100% while
preserving topical coherence. Beyond efficiency, these suffixes expose
sentence-boundary reward cliffs and other alignment artefacts, offering a
lightweight probe into how safety tuning reshapes internal representations.

</details>


### [364] [General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers](https://arxiv.org/abs/2506.22706)
*Arun Ramamurthy,Neil Dhir*

Main category: cs.CR

TL;DR: The paper proposes General Autonomous Cybersecurity Defense (GACD) to address limitations of current ACD systems, focusing on adaptability to dynamic network environments.


<details>
  <summary>Details</summary>
Motivation: Existing ACD systems fail in dynamic network scenarios due to reliance on static assumptions, leading to poor generalization.

Method: The work explores methods for training agents to learn generalizable policies across evolving network topologies.

Result: The proposed GACD aims to improve adaptability and performance in real-world, dynamic cybersecurity environments.

Conclusion: GACD offers a promising direction for enhancing autonomous cybersecurity defense in non-stationary networks.

Abstract: In the face of evolving cyber threats such as malware, ransomware and
phishing, autonomous cybersecurity defense (ACD) systems have become essential
for real-time threat detection and response with optional human intervention.
However, existing ACD systems rely on limiting assumptions, particularly the
stationarity of the underlying network dynamics. In real-world scenarios,
network topologies can change due to actions taken by attackers or defenders,
system failures, or time evolution of networks, leading to failures in the
adaptive capabilities of current defense agents. Moreover, many agents are
trained on static environments, resulting in overfitting to specific
topologies, which hampers their ability to generalize to out-of-distribution
network topologies. This work addresses these challenges by exploring methods
for developing agents to learn generalizable policies across dynamic network
environments -- general ACD (GACD).

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [365] [You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties](https://arxiv.org/abs/2506.23367)
*Paige Tuttösí,H. Henny Yeung,Yue Wang,Jean-Julien Aucouturier,Angelica Lim*

Main category: cs.SD

TL;DR: A TTS system for L2 speakers improves intelligibility by adjusting vowel durations, though listeners misperceive the effectiveness.


<details>
  <summary>Details</summary>
Motivation: To enhance intelligibility for L2 speakers by addressing vowel duration differences in English.

Method: Used duration differences between tense and lax vowels in Matcha-TTS to create a clarity mode. Conducted perception studies with French-L1, English-L2 listeners.

Result: Clarity mode reduced transcription errors by 9.15% and was perceived as more respectful, but listeners incorrectly believed slowed speech was more intelligible. Whisper-ASR did not align with L2 speaker cues.

Conclusion: Intelligibility improvements for L2 speakers can be achieved without slowing speech, but perception and ASR tools may not reflect actual benefits.

Abstract: We present the first text-to-speech (TTS) system tailored to second language
(L2) speakers. We use duration differences between American English tense
(longer) and lax (shorter) vowels to create a "clarity mode" for Matcha-TTS.
Our perception studies showed that French-L1, English-L2 listeners had fewer
(at least 9.15%) transcription errors when using our clarity mode, and found it
more encouraging and respectful than overall slowed down speech. Remarkably,
listeners were not aware of these effects: despite the decreased word error
rate in clarity mode, listeners still believed that slowing all target words
was the most intelligible, suggesting that actual intelligibility does not
correlate with perceived intelligibility. Additionally, we found that
Whisper-ASR did not use the same cues as L2 speakers to differentiate difficult
vowels and is not sufficient to assess the intelligibility of TTS systems for
these individuals.

</details>


### [366] [Efficient Interleaved Speech Modeling through Knowledge Distillation](https://arxiv.org/abs/2506.23670)
*Mohammadmahdi Nouriborji,Morteza Rohanian*

Main category: cs.SD

TL;DR: TinyWave, a compact speech generation model family, achieves near-teacher performance with 3x compression using layer-aligned distillation, enabling efficient deployment on commodity hardware.


<details>
  <summary>Details</summary>
Motivation: Address the constraints of large speech language models in deployment environments by creating compact, expressive models.

Method: Layer-aligned distillation to compress large multimodal transformers, matching hidden states, attention maps, and softened logits. TinyWave models (2B parameters) trained on 50,000 hours of public audio.

Result: TinyWave performs within 1.4 perplexity points of its teacher, with 93-97% accuracy on benchmarks, outperforming size-matched baselines.

Conclusion: TinyWave enables efficient, high-quality speech generation for real-time applications and low-resource environments, with released resources for reproducibility.

Abstract: Current speech language models exceed the size and latency constraints of
many deployment environments. We build compact, expressive speech generation
models through layer-aligned distillation, matching hidden states, attention
maps, and softened logits to compress large multimodal transformers by 3x with
minimal loss in performance. We introduce TinyWave, a family of 2B-parameter
models for speech-to-speech and interleaved speech-text generation, trained on
50,000 hours of public audio. TinyWave supports (i) speech-only generation
using phonetic or expressive tokens and (ii) mixed speech-text continuations.
Evaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity
points of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%
of the teacher's performance, outperforming size-matched baselines. These
models are optimized for deployment on commodity hardware, enabling
applications in real-time conversational agents, assistive technologies, and
low-resource environments. We release models, training code, and evaluation
scripts to support reproducible research on compact, expressive speech
generation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [367] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak,Jeffrey Flanigan*

Main category: cs.LG

TL;DR: The paper introduces the Residual Matrix Transformer (RMT), replacing the transformer's residual stream with an outer product memory matrix, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the transformer's residual stream mechanism for better memory retrieval and storage, aiming for improved scalability and efficiency.

Method: Replaces the transformer's residual stream with an outer product memory matrix, creating the RMT model.

Result: RMT achieves better performance with fewer FLOPS, parameters, and training tokens, and outperforms transformers in downstream tasks.

Conclusion: The RMT offers efficient scaling and improved variance propagation, making it a superior alternative to traditional transformers.

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [368] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Main category: cs.LG

TL;DR: BEST-Route is a novel LLM query routing framework that dynamically selects models and response counts to balance cost and quality, achieving 60% cost savings with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of prior query routing methods that overuse expensive large models by not leveraging the potential of small models with multiple responses.

Method: Proposes BEST-Route, which selects models and response counts based on query difficulty and quality thresholds, enhancing small-model performance through multiple responses.

Result: Achieves up to 60% cost reduction with less than 1% performance drop on real-world datasets.

Conclusion: BEST-Route effectively balances cost and quality in LLM query routing, offering significant savings without compromising performance.

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [369] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

Main category: cs.LG

TL;DR: BayesLoRA integrates MC-Dropout into LoRA for task-specific uncertainty quantification, aiding agent decision-making under uncertainty.


<details>
  <summary>Details</summary>
Motivation: General-purpose transformer uncertainty methods lack task-specific guardrails for downstream workflows.

Method: BayesLoRA combines MC-Dropout with LoRA adapters to quantify uncertainty.

Result: LoRA adapters show amplified variance outside fine-tuning distributions, providing reliable confidence estimates.

Conclusion: BayesLoRA offers tailored uncertainty quantification for agentic decision-making.

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [370] [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
*Yukito Tajima,Nakamasa Inoue,Yusuke Sekikawa,Ikuro Sato,Rio Yokota*

Main category: cs.LG

TL;DR: MGLUs improve memory efficiency and speed in GLUs by using shared weight matrices and binary masks, outperforming standard GLUs in LLMs.


<details>
  <summary>Details</summary>
Motivation: GLUs in LLMs suffer from high memory reads due to separate weight matrices for gates and values.

Method: Introduced MGLUs with MoEG architecture and FlashMGLU kernel for efficient memory use and faster inference.

Result: MGLUs achieve 19.7x speed-up, 47% memory efficiency, and match/surpass SwiGLU accuracy in LLMs.

Conclusion: MGLUs offer a hardware-friendly, efficient alternative to GLUs without compromising performance.

Abstract: Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [371] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
*Kenny Peng,Rajiv Movva,Jon Kleinberg,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: SAEs are less effective for known concepts but powerful for discovering unknown ones, clarifying their utility in ML and social/health sciences.


<details>
  <summary>Details</summary>
Motivation: Reconcile competing narratives about SAEs by distinguishing their effectiveness for known vs. unknown concepts.

Method: Conceptual distinction between SAEs' roles in acting on known concepts and discovering unknown ones.

Result: SAEs are useful for interpretability, fairness, and applications in social/health sciences.

Conclusion: SAEs have distinct roles, clarifying their value in specific applications despite skepticism.

Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [372] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
*Samuele Marro,Philip Torr*

Main category: cs.LG

TL;DR: LLM-based agents enable universal interoperability, disrupting proprietary platforms by making data exchange cheaper and unavoidable, but require frameworks to mitigate risks.


<details>
  <summary>Details</summary>
Motivation: The dominance of closed, proprietary platforms limits data exchange due to high costs and lack of incentives for market leaders.

Method: Proposes using LLM-based agents to automatically translate data formats and interact with human-designed interfaces, enabling seamless interoperability.

Result: Universal interoperability undermines monopolistic behaviors and promotes data portability but introduces security risks and technical debt.

Conclusion: The ML community should embrace this shift and develop frameworks to mitigate risks, leveraging AI to restore user freedom and competitive markets.

Abstract: While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>


### [373] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song,Laurent Itti*

Main category: cs.LG

TL;DR: The paper proposes a geometric framework using Riemannian geometry to define and analyze fingerprints of generative models, improving model attribution and distinguishing synthetic from human data.


<details>
  <summary>Details</summary>
Motivation: The need for reliable model authentication and differentiation of synthetic content due to IP protection, accountability, and the threat of model collapse.

Method: A geometric approach defining artifacts and fingerprints via Riemannian geometry, replacing Euclidean metrics with geodesic distances and Riemannian center of mass.

Result: The method effectively distinguishes various generative models across datasets, resolutions, architectures, and modalities, improving attribution and generalization.

Conclusion: The proposed Riemannian framework enhances understanding and practical application of generative model fingerprints, addressing current gaps.

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [374] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong,Huan Gui,Noveen Sachdeva,Long Jin,Ke Yin,Jingbo Shang,Lichan Hong,Ed H. Chi,Zhe Zhao*

Main category: cs.LG

TL;DR: The paper addresses the challenge of effective knowledge distillation from large-scale pretrained Vision Transformers (ViTs) to smaller models by proposing mutual information-aware optimization and MLP block reweighting.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of knowledge transfer drops when distilling from large-scale pretrained models. The paper aims to improve this for ViTs by fine-tuning them for better distillation.

Method: Proposes mutual information-aware optimization during fine-tuning and introduces MLP block reweighting for small or imbalanced datasets.

Result: The method enables small student models to effectively benefit from strong pretrained models.

Conclusion: The approach improves knowledge transfer from large-scale pretrained ViTs to smaller models, especially in challenging dataset scenarios.

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [375] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan,Darya Taratynova,Abdelmajid Essofi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.LG

TL;DR: Forget-MI is a novel machine unlearning method for multimodal medical data, improving privacy by effectively removing sensitive data while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Privacy in AI, especially healthcare, is critical. Existing methods struggle to remove patient data from trained multimodal architectures.

Method: Forget-MI uses loss functions and perturbation techniques to unlearn unimodal and joint representations of forgotten data while preserving knowledge from remaining data.

Result: Reduces MIA by 0.202, decreases AUC and F1 scores on the forget set by 0.221 and 0.305, and matches test set performance of the retrained model.

Conclusion: Forget-MI effectively balances privacy and performance, outperforming existing approaches in machine unlearning for healthcare data.

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [376] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster,Fabian Transchel*

Main category: cs.LG

TL;DR: The paper introduces the maneuverRecognition package for automating driving maneuver recognition using Python, addressing challenges in data preprocessing, model training, and evaluation.


<details>
  <summary>Details</summary>
Motivation: To enhance road safety, reduce accidents, and support eco-friendly driving by improving the automation of driving maneuver recognition.

Method: Developed the maneuverRecognition package with functions for preprocessing, modeling, and evaluation, including a customizable LSTM-based network.

Result: Demonstrated the package's effectiveness using real driving data from smartphone sensors.

Conclusion: The maneuverRecognition package provides a practical solution for automating driving maneuver recognition, with potential benefits for insurance personalization and road safety.

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [377] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth,Balázs Csanád Csáji*

Main category: cs.LG

TL;DR: The paper introduces SGKI, a kernel-based method for image inpainting and super-resolution, providing pixel estimates with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for reliable missing pixel estimation in images, especially with uncertainty measures, is crucial for applications like inpainting and super-resolution.

Method: SGKI extends kernel methods, assuming data comes from an RKHS, and uses Schur complements for efficient computation of confidence bands.

Result: SGKI successfully estimates missing pixels and provides non-asymptotic confidence bands, validated on synthetic and benchmark datasets.

Conclusion: SGKI offers a robust, efficient solution for image inpainting with uncertainty quantification, generalizable to vector-valued functions.

Abstract: The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [378] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/abs/2506.23492)
*Haolan Guo,Linwei Tao,Haoyang Luo,Minjing Dong,Chang Xu*

Main category: cs.LG

TL;DR: SMART is a lightweight, data-efficient recalibration method for neural networks that scales logits based on the logit gap, improving calibration with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks are overconfident, posing risks in safety-critical scenarios, and current calibration methods struggle with bias-variance trade-offs.

Method: SMART uses the logit gap as a denoised signal and introduces a SoftECE objective for adaptive binning, balancing bias and variance.

Result: SMART achieves state-of-the-art calibration performance with fewer parameters, even with limited data.

Conclusion: SMART offers a robust, efficient solution for uncertainty quantification in neural networks.

Abstract: Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [379] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/abs/2506.23516)
*Seung-Wook Kim,Seongyeol Kim,Jiah Kim,Seowon Ji,Se-Ho Lee*

Main category: cs.LG

TL;DR: FedWSQ improves federated learning by combining weight standardization and distribution-aware non-uniform quantization, addressing data heterogeneity and communication constraints.


<details>
  <summary>Details</summary>
Motivation: Performance degradation in federated learning due to data heterogeneity and communication constraints.

Method: Integrates weight standardization (WS) to filter biased components and distribution-aware non-uniform quantization (DANUQ) to minimize quantization errors.

Result: FedWSQ reduces communication overhead while maintaining high model accuracy, outperforming existing methods in challenging FL settings.

Conclusion: FedWSQ is a robust solution for federated learning, excelling in data heterogeneity and low-bit communication scenarios.

Abstract: Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [380] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: The paper analyzes watermark radioactivity in diffusion and autoregressive models, proposes a new method for autoregressive models, and demonstrates its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of radioactive watermarking methods for autoregressive models and prevent unauthorized use of generated images.

Method: Proposes a watermarking method inspired by large language models, tailored for image autoregressive models (IARs).

Result: The method effectively preserves radioactivity in IARs, enabling robust provenance tracking.

Conclusion: The proposed watermarking method for IARs successfully ensures radioactivity, preventing misuse of generated images.

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [381] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/abs/2506.23824)
*Durgesh Singh,Ahcene Boubekki,Robert Jenssen,Michael C. Kampffmeyer*

Main category: cs.LG

TL;DR: A novel semi-supervised learning approach using differentiable clustering to improve performance and simplify training.


<details>
  <summary>Details</summary>
Motivation: Current SSL methods rely on complex consistency regularization or entropy minimization, motivating a simpler, clustering-based alternative.

Method: Extends a differentiable clustering module, using annotated data to guide cluster centroids for an end-to-end trainable SSL model.

Result: Outperforms supervised-only baselines and enhances other SSL methods when combined.

Conclusion: The proposed clustering-based SSL approach is effective, simple, and compatible with existing methods.

Abstract: The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [382] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2506.24000)
*Lijun Sheng,Jian Liang,Ran He,Zilei Wang,Tieniu Tan*

Main category: cs.LG

TL;DR: The paper introduces TTA-VLM, a benchmark for evaluating test-time adaptation (TTA) methods on vision-language models (VLMs), addressing limitations in current research like inconsistent settings and limited metrics.


<details>
  <summary>Details</summary>
Motivation: Current TTA research lacks fair comparisons and comprehensive evaluation, hindering practical understanding of TTA methods for VLMs.

Method: TTA-VLM implements 8 episodic and 7 online TTA methods in a unified framework, evaluates them across 15 datasets, and extends beyond CLIP to include SigLIP and training-time tuning methods.

Result: Findings show limited gains from existing TTA methods, poor collaboration with fine-tuning methods, and trade-offs between accuracy and model trustworthiness.

Conclusion: TTA-VLM aims to foster fair comparisons and encourage development of more reliable TTA strategies for VLMs.

Abstract: Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [383] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/abs/2506.24124)
*Dong Sixun,Fan Wei,Teresa Wu,Fu Yanjie*

Main category: cs.LG

TL;DR: A multimodal contrastive learning framework is proposed to enhance time series forecasting by transforming raw data into visual and textual perspectives, aligning them in a shared semantic space, and outperforming unimodal baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal numerical inputs struggle to capture high-level semantic patterns, and existing text-based methods lack perceptual intuition like visual interpretation.

Method: The framework converts numerical sequences into structured visual and textual views, aligns them via contrastive learning, and includes a variate selection module for multivariate forecasting.

Result: The approach outperforms unimodal and cross-modal baselines on fifteen short-term and six long-term forecasting benchmarks.

Conclusion: Multimodal alignment effectively enhances time series forecasting, as demonstrated by superior performance across benchmarks.

Abstract: Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>

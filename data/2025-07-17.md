<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 86]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

TL;DR: LLMs act as subjective literary critics, showing varied aesthetic preferences and evaluation patterns in assessing Japanese sci-fi stories, revealing distinct vocabularies and implicit biases.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs exhibit individual evaluation traits akin to human critics, rather than neutral benchmarking.

Method: Evaluated ten translated Japanese sci-fi stories using six LLMs across seven sessions, analyzed with PCA, clustering, and TF-IDF.

Result: Found significant evaluation variance (α 1.00-0.35), five distinct patterns, and 4.5-fold variance in story assessments.

Conclusion: LLMs display unique evaluation characteristics, suggesting they function like human critical schools, not neutral tools.

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [2] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: MapIQ introduces a benchmark dataset for evaluating multimodal large language models (MLLMs) on diverse map types and tasks, revealing insights into their robustness and sensitivity.


<details>
  <summary>Details</summary>
Motivation: Address gaps in Map-VQA research, which has focused narrowly on choropleth maps, by expanding to diverse map types and themes.

Method: Created MapIQ, a dataset with 14,706 QA pairs across three map types and six themes, evaluating MLLMs on six tasks and testing robustness with design changes.

Result: MLLMs' performance varies across map types and tasks, with insights into their sensitivity to design changes and reliance on geographic knowledge.

Conclusion: MapIQ provides a comprehensive benchmark for Map-VQA, highlighting MLLMs' limitations and opportunities for improvement.

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [3] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

TL;DR: The paper explores cross-lingual sentiment analysis in Persian using few-shot and incremental learning with multilingual models, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a model for Persian sentiment analysis with limited data by leveraging knowledge from high-resource languages.

Method: Fine-tuned three multilingual models (XLM-RoBERTa, mDeBERTa, DistilBERT) using few-shot and incremental learning on diverse Persian datasets.

Result: mDeBERTa and XLM-RoBERTa achieved 96% accuracy in Persian sentiment analysis.

Conclusion: Combining few-shot and incremental learning with multilingual models is effective for low-resource languages like Persian.

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [4] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

TL;DR: PgM is a partitioner-guided multimodal learning framework that separates and learns uni-modal and paired-modal features, enhancing flexibility and performance in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To thoroughly learn and utilize both uni-modal and paired-modal features in multimodal learning, enabling better adaptation to various downstream tasks.

Method: PgM uses a modal partitioner to segment features, dedicated learners for uni-modal and paired-modal features, and a decoder for reconstruction.

Result: PgM improves performance across four multimodal tasks and shows transferability to existing models. Feature distributions are visualized for insights.

Conclusion: PgM effectively learns and adjusts uni-modal and paired-modal features, demonstrating versatility and transferability in multimodal learning.

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [5] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

TL;DR: ExpliCIT-QA is a multimodal system for tabular question answering that provides explainable answers through modular steps, improving transparency and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the explainability gap in end-to-end TableVQA systems, especially for sensitive domains like finance and healthcare.

Method: A modular pipeline with multimodal table understanding, language-based reasoning, automatic code generation, code execution, and natural language explanation.

Result: Improved interpretability and transparency, demonstrated on the TableVQA-Bench benchmark.

Conclusion: ExpliCIT-QA advances explainable TableVQA, enabling applications in domains requiring auditability.

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [6] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

TL;DR: The paper proposes CRABS, a method combining syntactic analysis and LLMs to understand Python notebooks by generating information flow and execution dependency graphs, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Understanding Python notebooks is crucial for reuse and adaptation, but re-execution is impractical, and LLMs alone fail due to hallucinations and long-context issues.

Method: CRABS uses shallow syntactic parsing and AST analysis to bound interpretations, then employs an LLM for zero-shot learning to resolve ambiguities in cell I/O.

Result: CRABS achieves 98% F1 for information flows and 99% for execution dependencies, with the LLM resolving 98% of ambiguities.

Conclusion: The pincer strategy of CRABS effectively combines syntactic and LLM-based analysis for accurate notebook understanding.

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [7] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

TL;DR: AI Wizards' approach to subjectivity detection in news articles using sentiment-augmented transformers achieved top performance in CLEF 2025, especially in zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: To improve subjectivity detection in multilingual and zero-shot settings by integrating sentiment scores with transformer models.

Method: Enhanced transformer-based classifiers (mDeBERTaV3, ModernBERT, Llama3.2) with sentiment features and calibrated decision thresholds for class imbalance.

Result: Significant performance boost, especially in subjective F1 scores, and 1st place for Greek (Macro F1 = 0.51).

Conclusion: Sentiment-augmented transformers effectively improve subjectivity detection, demonstrating strong generalization across languages.

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [8] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: A reproducibility study on how LLMs handle factual vs. counterfactual info, focusing on attention heads. Findings suggest general copy suppression and domain-specific behavior.


<details>
  <summary>Details</summary>
Motivation: To reproduce and reconcile findings from recent studies on how LLMs manage competing factual and counterfactual information, using Mechanistic Interpretability tools.

Method: Examines attention head strength, evaluates suppression mechanisms, and investigates domain specificity of attention patterns.

Result: Attention heads promote factual output via general copy suppression, not selective counterfactual suppression, and their behavior is domain-dependent.

Conclusion: Attention heads in LLMs use general mechanisms for factual output, with larger models showing more specialized, category-sensitive patterns.

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [9] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
*Yash Ingle,Pruthwik Mishra*

Main category: cs.CL

TL;DR: A dataset of 230K sentences in English and 22 Indian languages is released, along with robust baseline models for language identification, addressing challenges in noisy, short, and code-mixed text.


<details>
  <summary>Details</summary>
Motivation: Language identification is critical for NLP tasks but challenging for Indian languages due to similarities and shared scripts.

Method: Created a new dataset and developed baseline models using state-of-the-art ML and deep learning approaches.

Result: Baseline models perform comparably to state-of-the-art models for language identification.

Conclusion: The released dataset and models support research in language identification for diverse Indian languages.

Abstract: The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

</details>


### [10] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: A novel framework accelerates autoregressive language models by predicting multiple tokens simultaneously, achieving significant speedups without quality loss.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models are slow due to sequential token generation, especially when text direction is certain. This work aims to improve speed and parallelism.

Method: Proposes a framework with masked-input formulation, gated LoRA, a learnable sampler, auxiliary losses, and speculative generation for multi-token prediction.

Result: Achieves 5x faster code/math generation and 2.5x speedup in chat/knowledge tasks without quality degradation.

Conclusion: The method successfully enhances inference speed and parallelism in autoregressive models while maintaining output quality.

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [11] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
*Junhong Ye,Xu Yuan,Xinying Qiu*

Main category: cs.CL

TL;DR: The paper explores cross-domain transfer, multi-domain fusion, and sample-efficient learning for PII recognition, finding legal data transfers well to biographies, medical data resists transfer, fusion benefits vary by domain, and high performance is possible with minimal training in low-specialization domains.


<details>
  <summary>Details</summary>
Motivation: To improve automated text anonymization by understanding the effectiveness of cross-domain model transfer, data fusion, and sample-efficient learning for PII recognition.

Method: Evaluated models using annotated corpora from healthcare (I2B2), legal (TAB), and biography (Wikipedia) across in-domain performance, cross-domain transferability, fusion, and few-shot learning.

Result: Legal-domain data transfers well to biographical texts, medical domains resist incoming transfer, fusion benefits are domain-specific, and high-quality recognition is achievable with only 10% of training data in low-specialization domains.

Conclusion: Cross-domain transfer and fusion effectiveness vary by domain, and sample-efficient learning is viable in less specialized contexts, offering practical insights for PII recognition.

Abstract: Accurate recognition of personally identifiable information (PII) is central
to automated text anonymization. This paper investigates the effectiveness of
cross-domain model transfer, multi-domain data fusion, and sample-efficient
learning for PII recognition. Using annotated corpora from healthcare (I2B2),
legal (TAB), and biography (Wikipedia), we evaluate models across four
dimensions: in-domain performance, cross-domain transferability, fusion, and
few-shot learning. Results show legal-domain data transfers well to
biographical texts, while medical domains resist incoming transfer. Fusion
benefits are domain-specific, and high-quality recognition is achievable with
only 10% of training data in low-specialization domains.

</details>


### [12] [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)
*Xiangyu Yang,Xinying Qiu*

Main category: cs.CL

TL;DR: COLA-GEC is a bidirectional framework that enhances grammatical error correction (GEC) and grammatical acceptability judgment (COLA) through mutual knowledge transfer, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between GEC and COLA tasks by leveraging shared grammatical knowledge for mutual improvement.

Method: Augments COLA models with GEC datasets and integrates grammatical acceptability signals into GEC training via a dynamic loss function.

Result: Achieves state-of-the-art performance on multilingual benchmarks, with identified challenges in punctuation error correction.

Conclusion: COLA-GEC successfully improves both tasks, though future work is needed to address punctuation errors and further refine grammatical modeling.

Abstract: Grammatical Error Correction (GEC) and grammatical acceptability judgment
(COLA) are core tasks in natural language processing, sharing foundational
grammatical knowledge yet typically evolving independently. This paper
introduces COLA-GEC, a novel bidirectional framework that enhances both tasks
through mutual knowledge transfer. First, we augment grammatical acceptability
models using GEC datasets, significantly improving their performance across
multiple languages. Second, we integrate grammatical acceptability signals into
GEC model training via a dynamic loss function, effectively guiding corrections
toward grammatically acceptable outputs. Our approach achieves state-of-the-art
results on several multilingual benchmarks. Comprehensive error analysis
highlights remaining challenges, particularly in punctuation error correction,
providing insights for future improvements in grammatical modeling.

</details>


### [13] [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)
*Tianyou Huang,Xinglu Chen,Jingshen Zhang,Xinying Qiu,Ruiying Niu*

Main category: cs.CL

TL;DR: DualReward is a reinforcement learning framework for generating distractors in cloze tests, using adaptive reward scaling to outperform baselines.


<details>
  <summary>Details</summary>
Motivation: To improve distractor generation in cloze tests by dynamically balancing human-created examples and model-generated candidates.

Method: Employs a dual reward structure with adaptive scaling, adjusting rewards based on model performance and confidence.

Result: Shows consistent improvements, especially on diverse datasets (3.48-3.86% P@1 gain on MCQ).

Conclusion: DualReward effectively balances learning from human examples and exploring novel distractors, offering a flexible solution for test generation.

Abstract: This paper introduces DualReward, a novel reinforcement learning framework
for automatic distractor generation in cloze tests. Unlike conventional
approaches that rely primarily on supervised learning or static generative
models, our method employs a dual reward structure with adaptive scaling that
differentiates between human-created gold standard distractors and
model-generated candidates. The framework dynamically adjusts reward signal
intensity based on model performance and confidence. We evaluate our approach
on both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,
demonstrating consistent improvements over state-of-the-art baselines.
Experimental results show that our adaptive reward scaling mechanism provides
modest but consistent benefits on homogeneous datasets (CLOTH-F) and more
substantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data
(MCQ), suggesting its particular effectiveness for handling varied question
types and domains. Our work offers a flexible framework that effectively
balances learning from reliable human examples while exploring novel,
high-quality distractors for automated test generation.

</details>


### [14] [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
*Jiachen Zhao,Jing Huang,Zhengxuan Wu,David Bau,Weiyan Shi*

Main category: cs.CL

TL;DR: The paper identifies a distinct 'harmfulness direction' in LLMs, separate from refusal, and uses it to analyze safety mechanisms and jailbreak methods. A practical safeguard, Latent Guard, is proposed.


<details>
  <summary>Details</summary>
Motivation: To understand if LLMs truly comprehend harmfulness beyond refusal behaviors and to explore their internal safety mechanisms.

Method: Identifies a harmfulness direction distinct from refusal, tests its impact via steering, and evaluates jailbreak methods and adversarial finetuning.

Result: Harmfulness is encoded separately from refusal; jailbreaks reduce refusal signals without altering harmfulness beliefs. Latent Guard performs comparably to dedicated safeguards.

Conclusion: LLMs' internal harmfulness understanding is robust, offering a new perspective for AI safety and practical safeguards like Latent Guard.

Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand
harmfulness beyond just refusing? Prior work has shown that LLMs' refusal
behaviors can be mediated by a one-dimensional subspace, i.e., a refusal
direction. In this work, we identify a new dimension to analyze safety
mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a
separate concept from refusal. There exists a harmfulness direction that is
distinct from the refusal direction. As causal evidence, steering along the
harmfulness direction can lead LLMs to interpret harmless instructions as
harmful, but steering along the refusal direction tends to elicit refusal
responses directly without reversing the model's judgment on harmfulness.
Furthermore, using our identified harmfulness concept, we find that certain
jailbreak methods work by reducing the refusal signals without reversing the
model's internal belief of harmfulness. We also find that adversarially
finetuning models to accept harmful instructions has minimal impact on the
model's internal belief of harmfulness. These insights lead to a practical
safety application: The model's latent harmfulness representation can serve as
an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing
over-refusals that is robust to finetuning attacks. For instance, our Latent
Guard achieves performance comparable to or better than Llama Guard 3 8B, a
dedicated finetuned safeguard model, across different jailbreak methods. Our
findings suggest that LLMs' internal understanding of harmfulness is more
robust than their refusal decision to diverse input instructions, offering a
new perspective to study AI safety

</details>


### [15] [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)
*Bo Zeng,Chenyang Lyu,Sinuo Liu,Mingyan Zeng,Minghao Wu,Xuanfan Ni,Tianqi Shi,Yu Zhao,Yefeng Liu,Chenyu Zhu,Ruizhe Li,Jiahui Geng,Qing Li,Yu Tong,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: The paper introduces Marco-Bench-MIF, a localized multilingual extension of IFEval for evaluating LLMs' instruction-following capability across 30 languages, addressing linguistic and cultural constraints.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for evaluating LLMs' instruction-following are monolingual or machine-translated, limiting multilingual applicability.

Method: A hybrid pipeline combining translation with verification was used to create Marco-Bench-MIF, covering 30 languages with varying localization levels.

Result: Key findings include a 25-35% accuracy gap between high/low-resource languages, model scale impacting performance by 45-60%, and machine-translated data underestimating accuracy by 7-22%.

Conclusion: The study highlights challenges in multilingual instruction following and provides Marco-Bench-MIF as a resource for future research.

Abstract: Instruction-following capability has become a major ability to be evaluated
for Large Language Models (LLMs). However, existing datasets, such as IFEval,
are either predominantly monolingual and centered on English or simply machine
translated to other languages, limiting their applicability in multilingual
contexts. In this paper, we present an carefully-curated extension of IFEval to
a localized multilingual version named Marco-Bench-MIF, covering 30 languages
with varying levels of localization. Our benchmark addresses linguistic
constraints (e.g., modifying capitalization requirements for Chinese) and
cultural references (e.g., substituting region-specific company names in
prompts) via a hybrid pipeline combining translation with verification. Through
comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)
25-35% accuracy gap between high/low-resource languages, (2) model scales
largely impact performance by 45-60% yet persists script-specific challenges,
and (3) machine-translated data underestimates accuracy by7-22% versus
localized data. Our analysis identifies challenges in multilingual instruction
following, including keyword consistency preservation and compositional
constraint adherence across languages. Our Marco-Bench-MIF is available at
https://github.com/AIDC-AI/Marco-Bench-MIF.

</details>


### [16] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: A survey on deep learning applications in geometry problem solving, covering tasks, methods, evaluation metrics, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Geometry problem solving is crucial in education and AI assessment, and recent advances in deep learning and multimodal models have spurred research in this area.

Method: The paper reviews deep learning methods for geometry problem solving, including task summaries, method reviews, and evaluation analyses.

Result: Provides a comprehensive reference for deep learning in geometry problem solving, with a GitHub repository for ongoing updates.

Conclusion: The survey aims to advance the field by addressing current challenges and outlining future research directions.

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [17] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: PolyChartQA is a multilingual chart QA benchmark covering 10 languages, addressing the English-centric bias in existing benchmarks. It uses a decoupled pipeline for flexible chart generation and reveals performance gaps in non-English languages.


<details>
  <summary>Details</summary>
Motivation: Existing chart understanding benchmarks are mostly English-centric, limiting global accessibility. PolyChartQA aims to bridge this gap by supporting multilingual evaluation.

Method: A decoupled pipeline separates chart data from rendering code, enabling multilingual chart generation via translation. State-of-the-art LLM-based translation and quality control ensure consistency.

Result: Experiments show significant performance gaps between English and other languages, especially low-resource ones with non-Latin scripts.

Conclusion: PolyChartQA lays a foundation for globally inclusive vision-language models by addressing multilingual chart understanding.

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [18] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: BlockBPE is a GPU-optimized BPE tokenizer that improves throughput for batch inference by eliminating Regex pre-tokenization and enabling parallelized merges.


<details>
  <summary>Details</summary>
Motivation: Existing BPE tokenizers are CPU-bound and inefficient for GPU batch workflows, limiting performance.

Method: BlockBPE removes Regex pre-tokenization, enabling parallel token merges on GPU, reducing complexity to O(nd).

Result: BlockBPE achieves 2x higher throughput than tiktoken and 2.5x over HuggingFace Tokenizers in high-batch workloads.

Conclusion: BlockBPE offers a scalable, efficient solution for GPU-based batch inference, outperforming existing tokenizers.

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [19] [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)
*Yi Zhao,Zuchao Li,Hai Zhao,Baoyuan Qi,Guoming Liu*

Main category: cs.CL

TL;DR: The paper introduces DAC, a dynamic attention-aware method for task-agnostic prompt compression, addressing overlooked aspects like attention-critical tokens and entropy shifts, and demonstrates its effectiveness across diverse tasks and LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing prompt compression methods focus on information entropy but neglect attention-critical tokens and dynamic entropy shifts, limiting their effectiveness.

Method: DAC integrates entropy and attention information, dynamically sensing entropy shifts during compression for fine-grained results.

Result: Experiments on LongBench, GSM8K, and BBH show DAC consistently improves performance across tasks and LLMs.

Conclusion: DAC offers a robust solution for prompt compression by addressing key limitations of existing methods, proving its efficacy in diverse scenarios.

Abstract: Task-agnostic prompt compression leverages the redundancy in natural language
to reduce computational overhead and enhance information density within
prompts, especially in long-context scenarios. Existing methods predominantly
rely on information entropy as the metric to compress lexical units, aiming to
achieve minimal information loss. However, these approaches overlook two
critical aspects: (i) the importance of attention-critical tokens at the
algorithmic level, and (ii) shifts in information entropy during the
compression process. Motivated by these challenges, we propose a dynamic
attention-aware approach for task-agnostic prompt compression (DAC). This
approach effectively integrates entropy and attention information, dynamically
sensing entropy shifts during compression to achieve fine-grained prompt
compression. Extensive experiments across various domains, including LongBench,
GSM8K, and BBH, show that DAC consistently yields robust and substantial
improvements across a diverse range of tasks and LLMs, offering compelling
evidence of its efficacy.

</details>


### [20] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Main category: cs.CL

TL;DR: The paper introduces IAM, a framework that optimizes LLM efficiency by leveraging attention matrix similarity across different-scale models, achieving faster computation and reduced KV cache usage.


<details>
  <summary>Details</summary>
Motivation: Addressing the high resource consumption of LLMs, especially with long contexts, by exploiting external similarities in attention matrices rather than just internal sparsity.

Method: IAM measures attention matrix similarity, selects mapping layers, and ensures consistency to map attention between small and large LLMs.

Result: IAM accelerates prefill by 15% and reduces KV cache usage by 22.1% without significant performance loss, showing generalizability across model series.

Conclusion: IAM is a versatile, orthogonal solution to existing KV cache optimization methods, enhancing LLM efficiency.

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [21] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: A query-based multi-stage framework improves multi-hop and temporal QA performance on WikiData, using small language models and novel entity linking methods.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with multi-hop reasoning and temporal questions, prompting the need for modular alternatives like query-based KGQA.

Method: Proposes a multi-stage query-based framework for WikiData QA, incorporating entity linking and predicate matching with CoT reasoning.

Result: Enhances performance on multi-hop and temporal benchmarks, demonstrating robustness through generalization and rejection studies.

Conclusion: The framework shows promise for improving multi-hop and temporal QA, even with small language models.

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [22] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Main category: cs.CL

TL;DR: A novel Power-of-two (PoT) quantization framework for LLMs improves accuracy in low-precision formats and speeds up dequantization on GPUs.


<details>
  <summary>Details</summary>
Motivation: LLMs require high computational resources, and existing PoT quantization methods are inefficient on GPUs due to sign bit entanglement and sequential bit manipulations.

Method: Proposes a two-step post-training algorithm: (i) initialize quantization scales robustly, and (ii) refine them with a minimal calibration set.

Result: Outperforms state-of-the-art accuracy in 2- and 3-bit formats and achieves 3.67× speedup on NVIDIA V100 and 1.63× on RTX 4090.

Conclusion: The proposed PoT quantization enhances LLM efficiency and performance, especially in low-precision settings.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [23] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
*Ziyu Ge,Gabriel Chua,Leanne Tan,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: A two-stage framework for toxicity-preserving translation in low-resource languages like Singlish, combining human-verified few-shot prompts and model optimization to improve translation quality and safety.


<details>
  <summary>Details</summary>
Motivation: Standard translation systems fail to handle slang, code-mixing, and toxic content in under-represented languages, necessitating a culturally sensitive approach.

Method: 1. Human-verified few-shot prompt engineering to capture slang, tone, and toxicity. 2. Model-prompt optimization via semantic similarity benchmarking.

Result: Quantitative human evaluation confirms the pipeline's effectiveness in preserving sociolinguistic nuance and toxicity.

Conclusion: The framework enhances translation quality and safety in low-resource contexts, advocating for inclusive NLP practices.

Abstract: As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

</details>


### [24] [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)
*Yuhong Zhang,Jialu Li,Shilai Yang,Yuchen Xu,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.CL

TL;DR: The study compares human and LLM reading comprehension using graph-based text representation and eye-tracking, showing LLM consistency in understanding at a topological level.


<details>
  <summary>Details</summary>
Motivation: To deepen understanding of how humans and LLMs comprehend language, moving beyond individual word analysis to graph-based representations.

Method: Used an LLM-based AI agent to create graph-based text representations (nodes and edges) and compared eye fixation distributions on these structures.

Result: LLMs showed high consistency in language understanding at the graph topological level, validated by eye-tracking data.

Conclusion: The findings advance human-AI co-learning strategies by highlighting LLM capabilities in structured language comprehension.

Abstract: Reading comprehension is a fundamental skill in human cognitive development.
With the advancement of Large Language Models (LLMs), there is a growing need
to compare how humans and LLMs understand language across different contexts
and apply this understanding to functional tasks such as inference, emotion
interpretation, and information retrieval. Our previous work used LLMs and
human biomarkers to study the reading comprehension process. The results showed
that the biomarkers corresponding to words with high and low relevance to the
inference target, as labeled by the LLMs, exhibited distinct patterns,
particularly when validated using eye-tracking data. However, focusing solely
on individual words limited the depth of understanding, which made the
conclusions somewhat simplistic despite their potential significance. This
study used an LLM-based AI agent to group words from a reading passage into
nodes and edges, forming a graph-based text representation based on semantic
meaning and question-oriented prompts. We then compare the distribution of eye
fixations on important nodes and edges. Our findings indicate that LLMs exhibit
high consistency in language understanding at the level of graph topological
structure. These results build on our previous findings and offer insights into
effective human-AI co-learning strategies.

</details>


### [25] [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)
*Yuki Sakamoto,Takahisa Uchida,Hiroshi Ishiguro*

Main category: cs.CL

TL;DR: This study explores how value similarity affects trust and closeness among LLM agents, confirming its positive impact and validating LLMs as tools for social science research.


<details>
  <summary>Details</summary>
Motivation: To investigate whether value similarity, a key factor in human relationships, applies to artificial societies of LLM agents.

Method: Two experiments: first, testing controllability of values in LLMs; second, analyzing trust and closeness in agent pairs with varying value similarity.

Result: Agents with higher value similarity showed greater mutual trust and interpersonal closeness.

Conclusion: LLM simulations are valid for testing social theories and understanding value-driven relationship mechanisms, offering insights for social sciences.

Abstract: Large language models (LLMs) have emerged as powerful tools for simulating
complex social phenomena using human-like agents with specific traits. In human
societies, value similarity is important for building trust and close
relationships; however, it remains unexplored whether this principle holds true
in artificial societies comprising LLM agents. Therefore, this study
investigates the influence of value similarity on relationship-building among
LLM agents through two experiments. First, in a preliminary experiment, we
evaluated the controllability of values in LLMs to identify the most effective
model and prompt design for controlling the values. Subsequently, in the main
experiment, we generated pairs of LLM agents imbued with specific values and
analyzed their mutual evaluations of trust and interpersonal closeness
following a dialogue. The experiments were conducted in English and Japanese to
investigate language dependence. The results confirmed that pairs of agents
with higher value similarity exhibited greater mutual trust and interpersonal
closeness. Our findings demonstrate that the LLM agent simulation serves as a
valid testbed for social science theories, contributes to elucidating the
mechanisms by which values influence relationship building, and provides a
foundation for inspiring new theories and insights into the social sciences.

</details>


### [26] [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)
*Lukas Ellinger,Miriam Anschütz,Georg Groh*

Main category: cs.CL

TL;DR: Simplification of homonym definitions in LLMs risks information loss and misunderstanding, especially for children and language learners. Fine-tuning improves quality.


<details>
  <summary>Details</summary>
Motivation: To assess how simplification affects homonym definition quality for different target groups (Normal, Simple, ELI5) and ensure reliable educational outputs.

Method: Evaluated multiple LLMs (DeepSeek v3, Llama 4 Maverick, etc.) using LLM-as-Judge and human annotations on multilingual datasets. Fine-tuned Llama 3.1 8B with Direct Preference Optimization.

Result: Simplification degrades definition completeness by neglecting polysemy. Fine-tuning Llama 3.1 8B significantly improves homonym response quality.

Conclusion: Balancing simplicity and completeness is crucial in educational NLP to provide reliable, context-aware definitions for diverse learners.

Abstract: Large Language Models (LLMs) can provide accurate word definitions and
explanations for any context. However, the scope of the definition changes for
different target groups, like children or language learners. This is especially
relevant for homonyms, words with multiple meanings, where oversimplification
might risk information loss by omitting key senses, potentially misleading
users who trust LLM outputs. We investigate how simplification impacts homonym
definition quality across three target groups: Normal, Simple, and ELI5. Using
two novel evaluation datasets spanning multiple languages, we test DeepSeek v3,
Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge
and human annotations. Our results show that simplification drastically
degrades definition completeness by neglecting polysemy, increasing the risk of
misunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization
substantially improves homonym response quality across all prompt types. These
findings highlight the need to balance simplicity and completeness in
educational NLP to ensure reliable, context-aware definitions for all learners.

</details>


### [27] [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)
*Josip Jukić*

Main category: cs.CL

TL;DR: The thesis improves neural language models' data and parameter efficiency through representation analysis, optimization techniques, and weak supervision, achieving better performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Address challenges in data and parameter efficiency in neural language models, focusing on robustness, generalization, and reducing labeling efforts.

Method: Uses representation smoothness analysis, regularization with Jacobian/Hessian matrices, active learning with parameter-efficient fine-tuning, and weak supervision via in-context learning.

Result: Outperforms traditional methods in performance, stability, and efficiency, with gains in accuracy and adaptability, especially in low-resource settings.

Conclusion: Combining representation analysis, optimization, and weak supervision enhances model efficiency and robustness, reducing reliance on labeled data.

Abstract: This thesis addresses challenges related to data and parameter efficiency in
neural language models, with a focus on representation analysis and the
introduction of new optimization techniques. The first part examines the
properties and dynamics of language representations within neural models,
emphasizing their significance in enhancing robustness and generalization. It
proposes innovative approaches based on representation smoothness, including
regularization strategies that utilize Jacobian and Hessian matrices to
stabilize training and mitigate sensitivity to input perturbations. The second
part focuses on methods to significantly enhance data and parameter efficiency
by integrating active learning strategies with parameter-efficient fine-tuning,
guided by insights from representation smoothness analysis. It presents
smoothness-informed early-stopping techniques designed to eliminate the need
for labeled validation sets and proposes innovative combinations of active
learning and parameter-efficient fine-tuning to reduce labeling efforts and
computational resources. Extensive experimental evaluations across various NLP
tasks demonstrate that these combined approaches substantially outperform
traditional methods in terms of performance, stability, and efficiency. The
third part explores weak supervision techniques enhanced by in-context learning
to effectively utilize unlabeled data, further reducing dependence on extensive
labeling. It shows that using in-context learning as a mechanism for weak
supervision enables models to better generalize from limited labeled data by
leveraging unlabeled examples more effectively during training. Comprehensive
empirical evaluations confirm significant gains in model accuracy,
adaptability, and robustness, especially in low-resource settings and dynamic
data environments.

</details>


### [28] [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)
*Anca Dinu,Andra-Maria Florescu,Alina Resceanu*

Main category: cs.CL

TL;DR: A linguistic creativity test for humans and LLMs showed LLMs outperforming humans in originality, elaboration, and flexibility, with humans leaning towards E-creativity and LLMs towards F-creativity.


<details>
  <summary>Details</summary>
Motivation: To assess and compare linguistic creativity in humans and LLMs using word formation and metaphorical language tasks.

Method: Administered a test to 24 humans and 24 LLMs, evaluated responses automatically (OCSAI) for originality, elaboration, and flexibility, and analyzed uniqueness and creativity types manually.

Result: LLMs outperformed humans in all criteria and most tasks, with minor uniqueness differences. Humans favored E-creativity, LLMs F-creativity.

Conclusion: LLMs demonstrate superior linguistic creativity in tested tasks, but creativity types differ between humans and LLMs.

Abstract: The following paper introduces a general linguistic creativity test for
humans and Large Language Models (LLMs). The test consists of various tasks
aimed at assessing their ability to generate new original words and phrases
based on word formation processes (derivation and compounding) and on
metaphorical language use. We administered the test to 24 humans and to an
equal number of LLMs, and we automatically evaluated their answers using OCSAI
tool for three criteria: Originality, Elaboration, and Flexibility. The results
show that LLMs not only outperformed humans in all the assessed criteria, but
did better in six out of the eight test tasks. We then computed the uniqueness
of the individual answers, which showed some minor differences between humans
and LLMs. Finally, we performed a short manual analysis of the dataset, which
revealed that humans are more inclined towards E(extending)-creativity, while
LLMs favor F(ixed)-creativity.

</details>


### [29] [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)
*Anthony G Cohn,Robert E Blackwell*

Main category: cs.CL

TL;DR: The paper evaluates 28 LLMs' ability to reason about cardinal directions using a templated benchmark, finding even newer models struggle with accuracy.


<details>
  <summary>Details</summary>
Motivation: To assess and benchmark the reasoning capabilities of LLMs, particularly in determining cardinal directions under varied scenarios.

Method: A benchmark generated from templates with variations like locomotion and perspective (first, second, third person) was used to test 28 LLMs.

Result: Even advanced Large Reasoning Models failed to reliably determine correct cardinal directions in all scenarios.

Conclusion: Current LLMs, including newer models, lack consistent accuracy in cardinal direction reasoning, highlighting a limitation in their reasoning abilities.

Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason
about cardinal directions (CDs) using a benchmark generated from a set of
templates, extensively testing an LLM's ability to determine the correct CD
given a particular scenario. The templates allow for a number of degrees of
variation such as means of locomotion of the agent involved, and whether set in
the first, second or third person. Even the newer Large Reasoning Models are
unable to reliably determine the correct CD for all questions. This paper
summarises and extends earlier work presented at COSIT-24.

</details>


### [30] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Main category: cs.CL

TL;DR: A modular stylometric pipeline using spaCy for preprocessing and LightGBM for classification, trained on 500K machine-generated texts, focuses on explainability and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop a binary AI detection method that is computationally inexpensive and explainable, leveraging a large dataset of machine-generated texts.

Method: Uses spaCy for preprocessing (tokenization, NER, dependency parsing, POS tagging, morphology) and extracts thousands of features. LightGBM is the classifier, optimized for large training data.

Result: The approach is effective, leveraging a non-neural method for explainability and efficiency.

Conclusion: The modular, explainable pipeline with LightGBM is a viable solution for binary AI detection, balancing performance and transparency.

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [31] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
*Giuliano Martinelli,Tommaso Bonomo,Pere-Lluís Huguet Cabot,Roberto Navigli*

Main category: cs.CL

TL;DR: The paper introduces BOOKCOREF, the first book-scale coreference benchmark, addressing limitations of existing benchmarks for long texts. It presents an automatic annotation pipeline and demonstrates its effectiveness, showing significant performance improvements for long-document coreference systems.


<details>
  <summary>Details</summary>
Motivation: Existing coreference benchmarks are limited to small- to medium-scale documents, failing to assess systems on long texts like books. This gap hinders the evaluation of coreference resolution at larger scales.

Method: The authors develop an automatic pipeline for high-quality coreference annotations on full narrative texts and use it to create BOOKCOREF, a benchmark with documents averaging over 200,000 tokens.

Result: Experiments show the robustness of the pipeline, with current systems gaining up to +20 CoNLL-F1 points when evaluated on full books. However, models struggle with book-scale challenges compared to smaller documents.

Conclusion: BOOKCOREF fills a critical gap in coreference resolution evaluation, enabling research on book-scale systems. The authors release data and code to support further development in this area.

Abstract: Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [32] [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)
*Tosin Adewumi,Foteini Simistira Liwicki,Marcus Liwicki,Viktor Gardelli,Lama Alkhaled,Hamam Mokayed*

Main category: cs.CL

TL;DR: The study compares the MEGA method (combining Socratic method, CoT reasoning, gamification, and feedback) with traditional CoT for Maths learning using LLMs. Results show MEGA is preferred, especially for difficult problems.


<details>
  <summary>Details</summary>
Motivation: Students often struggle with Maths due to poor pedagogy, leading to avoidance of Math-related fields. The study aims to improve learning through innovative methods.

Method: Used a within-group design with university students, testing MEGA vs. CoT on GSM8K and MATH datasets using GPT4o and Claude 3.5 Sonnet.

Result: MEGA was preferred by students, outperforming CoT (47.5% vs. 26.67%) in the more challenging MATH dataset.

Conclusion: MEGA is more effective for explaining difficult Maths problems, suggesting its potential to enhance Maths pedagogy.

Abstract: This paper presents an intervention study on the effects of the combined
methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)
simplified gamification and (4) formative feedback on university students'
Maths learning driven by large language models (LLMs). We call our approach
Mathematics Explanations through Games by AI LLMs (MEGA). Some students
struggle with Maths and as a result avoid Math-related discipline or subjects
despite the importance of Maths across many fields, including signal
processing. Oftentimes, students' Maths difficulties stem from suboptimal
pedagogy. We compared the MEGA method to the traditional step-by-step (CoT)
method to ascertain which is better by using a within-group design after
randomly assigning questions for the participants, who are university students.
Samples (n=60) were randomly drawn from each of the two test sets of the Grade
School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)
datasets, based on the error margin of 11%, the confidence level of 90%, and a
manageable number of samples for the student evaluators. These samples were
used to evaluate two capable LLMs at length (Generative Pretrained Transformer
4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for
capability. The results showed that students agree in more instances that the
MEGA method is experienced as better for learning for both datasets. It is even
much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH
dataset, indicating that MEGA is better at explaining difficult Maths problems.

</details>


### [33] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Main category: cs.CL

TL;DR: The paper introduces a framework for evaluating LLM-based text augmentation, focusing on scalability and iterative refinement, with GPT-3.5 Turbo showing the best performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of semantic preservation in existing text augmentation techniques, especially in low-resource NLP settings.

Method: Proposes two evaluation components: Scalability Analysis and Iterative Augmentation with Summarization Refinement (IASR).

Result: GPT-3.5 Turbo excels in semantic fidelity, diversity, and efficiency. Applied to BERTopic, it boosts topic granularity by 400% and eliminates overlaps.

Conclusion: The framework effectively evaluates LLM-based augmentation, enhancing practical NLP pipelines.

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [34] [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)
*Pavel Šindelář,Ondřej Bojar*

Main category: cs.CL

TL;DR: ELOQUENT's Sensemaking task evaluates generative models through question creation, answering, and scoring, revealing challenges in question quality assessment, answer restriction, and evaluation accuracy.


<details>
  <summary>Details</summary>
Motivation: To establish high-level criteria for evaluating generative language models by testing their ability to make sense of given texts in a structured, exam-like setting.

Method: The task involves three steps: (1) Teacher systems create questions, (2) Student systems answer them, and (3) Evaluator systems score answers, all based on strict input materials. The 2025 edition used diverse sources in multiple languages and included both automatic and manual evaluations.

Result: Observations include difficulties in assessing question quality, acceptable but problematic LLM performance in answering, and flaws in LLM-as-a-Judge evaluations, such as accepting garbled or mismatched answers.

Conclusion: The Sensemaking task highlights the need for better evaluation strategies for question creation, improved answer restriction methods, and more reliable evaluation systems to accurately assess generative models.

Abstract: ELOQUENT is a set of shared tasks that aims to create easily testable
high-level criteria for evaluating generative language models. Sensemaking is
one such shared task.
  In Sensemaking, we try to assess how well generative models ``make sense out
of a given text'' in three steps inspired by exams in a classroom setting: (1)
Teacher systems should prepare a set of questions, (2) Student systems should
answer these questions, and (3) Evaluator systems should score these answers,
all adhering rather strictly to a given set of input materials.
  We report on the 2025 edition of Sensemaking, where we had 7 sources of test
materials (fact-checking analyses of statements, textbooks, transcribed
recordings of a lecture, and educational videos) spanning English, German,
Ukrainian, and Czech languages.
  This year, 4 teams participated, providing us with 2 Teacher submissions, 2
Student submissions, and 2 Evaluator submissions. We added baselines for
Teacher and Student using commercial large language model systems. We devised a
fully automatic evaluation procedure, which we compare to a minimalistic manual
evaluation.
  We were able to make some interesting observations. For the first task, the
creation of questions, better evaluation strategies will still have to be
devised because it is difficult to discern the quality of the various candidate
question sets. In the second task, question answering, the LLMs examined
overall perform acceptably, but restricting their answers to the given input
texts remains problematic. In the third task, evaluation of question answers,
our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm
erroneously rate both garbled question-answer pairs and answers to mixed-up
questions as acceptable.

</details>


### [35] [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)
*Michael Carl,Takanori Mizowaki,Aishvarya Ray,Masaru Yamada,Devi Sri Bandaru,Xinyue Ren*

Main category: cs.CL

TL;DR: The paper introduces a Behavioural Translation Style Space (BTSS) to model translation behavior through hierarchical layers, using keystrokes and gaze data to infer cognitive and affective states.


<details>
  <summary>Details</summary>
Motivation: To understand and simulate the complex interplay of cognitive, affective, and behavioral processes in human translation.

Method: Analyzes keystrokes and gaze data to identify behavioral patterns, organizing them into a multi-layered BTSS.

Result: Develops a BTSS framework that captures temporal dynamics of affect, automatized behavior, and cognition in translation.

Conclusion: The BTSS provides a computational foundation for simulating human translation behavior, linking observable actions to underlying mental processes.

Abstract: The paper introduces a Behavioural Translation Style Space (BTSS) that
describes possible behavioural translation patterns. The suggested BTSS is
organized as a hierarchical structure that entails various embedded processing
layers. We posit that observable translation behaviour - i.e., eye and finger
movements - is fundamental when executing the physical act of translation but
it is caused and shaped by higher-order cognitive processes and affective
translation states. We analyse records of keystrokes and gaze data as
indicators of the hidden mental processing structure and organize the
behavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the
basis for a computational translation agent to simulate the temporal dynamics
of affect, automatized behaviour and cognition during human translation
production.

</details>


### [36] [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)
*Reuben Smit,Retief Louw,Herman Kamper*

Main category: cs.CL

TL;DR: An ASR-free method for word reading assessment in low-resource settings using few-shot SSL model comparisons, showing limitations with child speech.


<details>
  <summary>Details</summary>
Motivation: To assess isolated word reading in low-resource settings without relying on ASR, using few-shot learning and SSL models.

Method: Compare child speech to adult reference templates using SSL model encodings, exploring discretisation and barycentre averaging.

Result: Reasonable performance for adults but significant drop for child speech, even with child templates.

Conclusion: SSL representations have limitations for child speech in few-shot classification systems.

Abstract: We explore an ASR-free method for isolated word reading assessment in
low-resource settings. Our few-shot approach compares input child speech to a
small set of adult-provided reference templates. Inputs and templates are
encoded using intermediate layers from large self-supervised learned (SSL)
models. Using an Afrikaans child speech benchmark, we investigate design
options such as discretising SSL features and barycentre averaging of the
templates. Idealised experiments show reasonable performance for adults, but a
substantial drop for child speech input, even with child templates. Despite the
success of employing SSL representations in low-resource speech tasks, our work
highlights the limitations of SSL representations for processing child data
when used in a few-shot classification system.

</details>


### [37] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
*Shilin Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: A novel multi-grained fusion approach combining token-level and phrase-level fusion with LLMs improves keyword recognition in ASR while maintaining general transcription accuracy.


<details>
  <summary>Details</summary>
Motivation: End-to-end ASR models struggle with contextually relevant keywords like proper nouns or user-specific entities, despite strong general performance.

Method: Proposes a multi-grained fusion approach that integrates token-level and phrase-level fusion with LLMs, using a late-fusion strategy to combine acoustic and contextual information.

Result: Achieves state-of-the-art performance on keyword-related metrics in Chinese and English datasets while preserving non-keyword accuracy.

Conclusion: The joint framework's token-level and phrase-level components complement each other, significantly enhancing performance.

Abstract: While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

</details>


### [38] [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
*Yikang Liu,Wanyang Zhang,Yiming Wang,Jialong Tang,Pei Zhang,Baosong Yang,Fei Huang,Rui Wang,Hai Hu*

Main category: cs.CL

TL;DR: The paper introduces the translationese-index (T-index), a quantitative measure for translationese, using contrastively fine-tuned LMs. It shows robustness, efficiency, and validity against human judgments, while being distinct from existing MT QE metrics.


<details>
  <summary>Details</summary>
Motivation: To provide a generalizable and graded measure of translationese, addressing gaps in existing methods.

Method: Uses likelihood ratios from two fine-tuned LMs on synthetic and real-world translation datasets. Evaluates cross-domain generalizability and human judgment alignment.

Result: T-index captures translationese effectively with minimal data (1-5k pairs). It predicts human annotations (pairwise differences) and correlates with human ratings (Pearson's r=0.568). Low correlation with BLEU/COMET suggests complementarity.

Conclusion: T-index is a robust, efficient, and complementary metric for measuring translationese, validated by human judgments and distinct from traditional MT QE metrics.

Abstract: In this paper, we propose the first quantitative measure for translationese
-- the translationese-index (T-index) for graded and generalizable measurement
of translationese, computed from the likelihood ratios of two contrastively
fine-tuned language models (LMs). We use a synthesized dataset and a dataset
with translations in the wild to evaluate T-index's generalizability in
cross-domain settings and its validity against human judgments. Our results
show that T-index is both robust and efficient. T-index scored by two 0.5B LMs
fine-tuned on only 1-5k pairs of synthetic data can well capture translationese
in the wild. We find that the relative differences in T-indices between
translations can well predict pairwise translationese annotations obtained from
human annotators; and the absolute values of T-indices correlate well with
human ratings of degrees of translationese (Pearson's $r = 0.568$).
Additionally, the correlation between T-index and existing machine translation
(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting
that T-index is not covered by these metrics and can serve as a complementary
metric in MT QE.

</details>


### [39] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
*Johann Frei,Nils Feldhus,Lisa Raithel,Roland Roller,Alexander Meyer,Frank Kramer*

Main category: cs.CL

TL;DR: Infherno, an end-to-end framework using LLM agents and healthcare tools, improves FHIR resource prediction from clinical notes, outperforming modular systems and matching human baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing limited generalizability and structural issues in translating clinical notes to FHIR resources.

Method: Uses LLM agents, code execution, and healthcare terminology databases for end-to-end translation.

Result: Infherno adheres to FHIR schema and competes with human baseline in resource prediction.

Conclusion: Infherno supports clinical data integration and interoperability, offering a robust solution.

Abstract: For clinical data integration and healthcare services, the HL7 FHIR standard
has established itself as a desirable format for interoperability between
complex health data. Previous attempts at automating the translation from
free-form clinical notes into structured FHIR resources rely on modular,
rule-based systems or LLMs with instruction tuning and constrained decoding.
Since they frequently suffer from limited generalizability and structural
inconformity, we propose an end-to-end framework powered by LLM agents, code
execution, and healthcare terminology database tools to address these issues.
Our solution, called Infherno, is designed to adhere to the FHIR document
schema and competes well with a human baseline in predicting FHIR resources
from unstructured text. The implementation features a front end for custom and
synthetic data and both local and proprietary models, supporting clinical data
integration processes and interoperability across institutions.

</details>


### [40] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Main category: cs.CL

TL;DR: The paper introduces a benchmark for text anomaly detection, evaluates embedding-based methods using diverse language models and datasets, and finds embedding quality crucial while deep learning offers no advantage over shallow methods.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized benchmarks for text anomaly detection hinders comparison and innovation, prompting the creation of a comprehensive evaluation framework.

Method: The study uses embeddings from various pre-trained language models (e.g., GloVe, BERT, LLaMa-2) and evaluates them on multi-domain datasets with metrics like AUROC and AUPRC.

Result: Embedding quality is key to performance; deep learning methods don't outperform shallow ones (e.g., KNN) with LLM embeddings. Cross-model performance shows low-rank traits, aiding efficient model selection.

Conclusion: The benchmark toolkit is open-sourced to support future research in robust and scalable text anomaly detection systems.

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [41] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Main category: cs.CL

TL;DR: The paper evaluates LLMs for VHDL tasks, finds them lacking, and proposes Chain-of-Descriptions (CoDes) to improve performance.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' success in general code tasks, their performance for VHDL (a hardware description language) remains understudied and subpar.

Method: The study evaluates existing LLMs on VHDL-Eval and VHDL-Xform datasets, then introduces CoDes—a method using intermediate descriptive steps to enhance LLM outputs.

Result: LLMs underperform for VHDL tasks, but CoDes significantly improves code generation and summarization quality.

Conclusion: CoDes offers a promising framework for refining LLMs in VHDL tasks, addressing current gaps and guiding future research.

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [42] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
*Liu He,Yuanchao Li,Rui Feng,XinRan Han,Yin-Long Liu,Yuwei Yang,Zude Zhu,Jiahong Yuan*

Main category: cs.CL

TL;DR: The study identifies gender bias in Alzheimer's Disease (AD) speech perception, with male speech more often labeled as AD, especially in Chinese. Acoustic features like shimmer and speech portion correlate with AD perception, while language impact is minimal.


<details>
  <summary>Details</summary>
Motivation: To investigate gender bias in AD speech perception and its acoustic correlates, given the known voicing differences between genders.

Method: A perception experiment with 16 Chinese listeners evaluating Chinese and Greek speech, followed by acoustic analysis of shimmer and speech portion.

Result: Male speech was more frequently identified as AD, particularly in Chinese. Shimmer values in male speech correlated with AD perception, while speech portion had a negative correlation. Language did not significantly affect AD perception.

Conclusion: Gender bias significantly impacts AD speech perception, necessitating its consideration in AD detection models and further research across languages.

Abstract: Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

</details>


### [43] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CL

TL;DR: The paper introduces a multi-agent debate framework to improve LLMs' ability to handle ambiguous user requests, showing enhanced performance for specific models like Mistral-7B.


<details>
  <summary>Details</summary>
Motivation: Address challenges like ambiguity in LLM-processed user requests by leveraging collaborative debate.

Method: Uses a debate framework with three LLM architectures (Llama3-8B, Gemma2-9B, Mistral-7B) and a dataset of diverse ambiguities.

Result: Mistral-7B-led debates achieved a 76.7% success rate, outperforming individual baselines, especially for complex ambiguities.

Conclusion: Structured debates enhance LLM capabilities, offering insights for more robust language understanding systems.

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and generating human language, contributing to more natural
interactions with complex systems. However, they face challenges such as
ambiguity in user requests processed by LLMs. To address these challenges, this
paper introduces and evaluates a multi-agent debate framework designed to
enhance detection and resolution capabilities beyond single models. The
framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and
Mistral-7B variants) and a dataset with diverse ambiguities. The debate
framework markedly enhanced the performance of Llama3-8B and Mistral-7B
variants over their individual baselines, with Mistral-7B-led debates achieving
a notable 76.7% success rate and proving particularly effective for complex
ambiguities and efficient consensus. While acknowledging varying model
responses to collaborative strategies, these findings underscore the debate
framework's value as a targeted method for augmenting LLM capabilities. This
work offers important insights for developing more robust and adaptive language
understanding systems by showing how structured debates can lead to improved
clarity in interactive systems.

</details>


### [44] [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei,Mohsen Mosleh*

Main category: cs.CL

TL;DR: LLMs with web browsing can predict social media user demographics from usernames but may introduce biases, raising concerns about misuse.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can retrieve and analyze social media data, specifically inferring user demographics from usernames.

Method: Evaluated using a synthetic dataset of 48 Twitter accounts and a survey dataset of 1,384 participants.

Result: LLMs can predict demographics with reasonable accuracy but may exhibit biases, especially for inactive accounts.

Conclusion: While useful for research, LLM web browsing capabilities should be restricted in public applications to prevent misuse.

Abstract: Large language models (LLMs) have traditionally relied on static training
data, limiting their knowledge to fixed snapshots. Recent advancements,
however, have equipped LLMs with web browsing capabilities, enabling real time
information retrieval and multi step reasoning over live web content. While
prior studies have demonstrated LLMs ability to access and analyze websites,
their capacity to directly retrieve and analyze social media data remains
unexplored. Here, we evaluate whether web browsing LLMs can infer demographic
attributes of social media users given only their usernames. Using a synthetic
dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international
participants, we show that these models can access social media content and
predict user demographics with reasonable accuracy. Analysis of the synthetic
dataset further reveals how LLMs parse and interpret social media profiles,
which may introduce gender and political biases against accounts with minimal
activity. While this capability holds promise for computational social science
in the post API era, it also raises risks of misuse particularly in information
operations and targeted advertising underscoring the need for safeguards. We
recommend that LLM providers restrict this capability in public facing
applications, while preserving controlled access for verified research
purposes.

</details>


### [45] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
*Yucheng Sun,Alessandro Stolfo,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: Probes on internal activations in language models can detect arithmetic errors and predict correctness with high accuracy, even generalizing to complex tasks like GSM8K. Selective re-prompting based on probes improves task accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore if internal activations in language models can detect arithmetic errors and enable self-correction.

Method: Use probes on hidden states to decode predicted and correct answers, train lightweight error detectors, and apply selective re-prompting.

Result: Probes achieve over 90% accuracy in predicting correctness and generalize to complex tasks. Selective re-prompting improves accuracy.

Conclusion: Internal activations can anticipate arithmetic errors, and simple probes enable lightweight model self-correction.

Abstract: We investigate whether internal activations in language models can be used to
detect arithmetic errors. Starting with a controlled setting of 3-digit
addition, we show that simple probes can accurately decode both the model's
predicted output and the correct answer from hidden states, regardless of
whether the model's output is correct. Building on this, we train lightweight
error detectors that predict model correctness with over 90% accuracy. We then
extend our analysis to structured chain-of-thought traces on addition-only
GSM8K problems and find that probes trained on simple arithmetic generalize
well to this more complex setting, revealing consistent internal
representations. Finally, we demonstrate that these probes can guide selective
re-prompting of erroneous reasoning steps, improving task accuracy with minimal
disruption to correct outputs. Our findings suggest that arithmetic errors can
be anticipated from internal activations alone, and that simple probes offer a
viable path toward lightweight model self-correction.

</details>


### [46] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
*Chandana Cheerla*

Main category: cs.CL

TL;DR: An advanced RAG framework improves enterprise data processing by combining hybrid retrieval, metadata filtering, and semantic chunking, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs and conventional RAG in handling structured and semi-structured enterprise data.

Method: Hybrid retrieval (dense embeddings + BM25), metadata-aware filtering, semantic chunking, quantized indexing, and human-in-the-loop feedback.

Result: Precision@5 up 15%, Recall@5 up 13%, MRR up 16%, and higher qualitative scores in Faithfulness, Completeness, and Relevance.

Conclusion: The framework effectively enhances accuracy and relevance for enterprise tasks, with plans for multimodal and agent-based extensions.

Abstract: Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [47] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CL

TL;DR: The paper explores using chains-of-thought (CoTs) activations to predict misalignment in final responses of reasoning language models, finding that linear probes on CoT activations outperform text-based methods.


<details>
  <summary>Details</summary>
Motivation: To address alignment risks in open-weights reasoning models where harmful content may appear in CoTs or final outputs.

Method: Evaluated monitoring approaches (humans, large language models, text classifiers) using CoT text or activations, focusing on a linear probe for CoT activations.

Result: Linear probes on CoT activations outperform text-based methods, providing reliable early predictions of misalignment.

Conclusion: Lightweight probes on CoT activations enable real-time safety monitoring and early intervention during generation.

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [48] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: S2WTM, a new topic model, addresses posterior collapse in VAE-NTMs by using a hyperspherical prior and Spherical Sliced-Wasserstein distance, improving topic coherence and diversity.


<details>
  <summary>Details</summary>
Motivation: Posterior collapse in VAE-NTMs leads to ineffective latent representations, motivating the need for a better approach to model hyperspherical structure.

Method: S2WTM uses a hyperspherical prior and Spherical Sliced-Wasserstein distance to align the aggregated posterior with the prior.

Result: S2WTM outperforms state-of-the-art models, producing more coherent and diverse topics and enhancing downstream task performance.

Conclusion: S2WTM effectively mitigates posterior collapse and improves topic modeling by leveraging hyperspherical structure and optimal transport.

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [49] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Main category: cs.CL

TL;DR: BETR, a benchmark-targeted ranking method, aligns pretraining data with evaluation benchmarks, improving model performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of explicitly optimizing data selection for benchmark performance, addressing the implicit targets in current methods.

Method: BETR embeds benchmark and pretraining documents in a shared space, scores similarity, and trains a classifier to predict scores for the full corpus.

Result: BETR achieves a 2.1x compute multiplier over baselines, improves performance on 9/10 tasks, and generalizes well to diverse benchmarks.

Conclusion: Directly matching pretraining data to target tasks shapes model capabilities, and optimal selection strategies must adapt to model scale.

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Main category: cs.CV

TL;DR: A hardware-friendly optimization framework for Deformable Attention Transformers (DAT) is proposed, using NAS and a new slicing strategy to improve efficiency without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: DAT's irregular memory access patterns hinder efficient hardware deployment, and existing methods either increase overhead or reduce accuracy.

Method: Proposes a NAS-based method with a new slicing strategy to divide input features uniformly, avoiding memory conflicts, and designs an FPGA-based verification system.

Result: Achieves only a 0.2% accuracy drop on ImageNet-1K and reduces DRAM access times to 18% on FPGA compared to existing methods.

Conclusion: The framework effectively balances hardware efficiency and model accuracy for DAT deployment.

Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [51] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Main category: cs.CV

TL;DR: DDCN is a CNN-based model for efficient and accurate spatio-temporal traffic prediction, addressing limitations of GNNs and traditional CNNs by using deformable filters and an encoder-decoder structure.


<details>
  <summary>Details</summary>
Motivation: Overcome the challenges of capturing spatio-temporal heterogeneity and scalability in traffic prediction, where GNNs and traditional CNNs fall short.

Method: Proposes DDCN with deformable filters and an encoder-decoder structure, using spatial and spatio-temporal attention blocks for feature emphasis.

Result: Achieves competitive performance on four real-world datasets, demonstrating CNN-based approaches' potential for traffic prediction.

Conclusion: DDCN offers an accurate and efficient solution for spatio-temporal traffic prediction, outperforming traditional methods.

Abstract: Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [52] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

TL;DR: Inversion-DPO is a novel alignment framework for diffusion models that avoids reward modeling by using DDIM inversion, improving training precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods for diffusion models are computationally intensive and may reduce accuracy. Inversion-DPO aims to address these limitations.

Method: Reformulates Direct Preference Optimization (DPO) with DDIM inversion to sample posterior distributions without auxiliary reward models.

Result: Shows significant performance improvements in text-to-image and compositional image generation tasks, producing high-fidelity, coherent images.

Conclusion: Inversion-DPO offers an efficient, high-precision alignment method for diffusion models, enhancing their applicability to complex generation tasks.

Abstract: Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [53] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Main category: cs.CV

TL;DR: ST-VFM adapts Vision Foundation Models (VFMs) for spatio-temporal forecasting by addressing temporal modeling and modality gaps with a dual-branch architecture and reprogramming stages.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs lack spatio-temporal modeling capabilities, while VFMs offer spatial priors but lack temporal awareness. ST-VFM bridges this gap.

Method: ST-VFM uses a dual-branch architecture with raw ST and auxiliary flow inputs, plus pre- and post-VFM reprogramming stages for temporal embedding and dynamic interaction.

Result: ST-VFM outperforms baselines on ten datasets, proving robust across VFM backbones like DINO, CLIP, and DEIT.

Conclusion: ST-VFM is a versatile framework for spatio-temporal forecasting, effectively leveraging VFMs despite their initial limitations.

Abstract: Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [54] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

TL;DR: xOp-GAN is a novel GAN model with multiple expert generators, each specialized for a subset of underwater image degradations, outperforming single-regressor models.


<details>
  <summary>Details</summary>
Motivation: Underwater image restoration is challenging due to heterogeneous degradations, and single-generator GANs struggle to capture the full range of artifacts.

Method: xOp-GAN uses multiple expert generators, each trained on a specific quality subset, and a discriminator to select the best output during inference.

Result: Achieves PSNR up to 25.16 dB on the LSUI dataset, surpassing single-regressor models with reduced complexity.

Conclusion: xOp-GAN effectively addresses heterogeneous underwater degradations by leveraging multiple specialized generators and discriminator-guided selection.

Abstract: The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [55] [Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation](https://arxiv.org/abs/2507.11571)
*Varun Velankar*

Main category: cs.CV

TL;DR: The paper reviews gait-based age estimation, comparing methods like CNNs and sensor fusion, and analyzes gait metrics. It fine-tunes models for interpretability and benchmarks performance on large datasets.


<details>
  <summary>Details</summary>
Motivation: Age estimation from gait has applications in healthcare, security, and human-computer interaction, but accuracy and real-world performance need improvement.

Method: The study reviews 59 studies, analyzes gait metrics, fine-tunes a ResNet34 model, and benchmarks various ML methods on large datasets.

Result: CNNs achieve 4.2 years error, sensor fusion 3.4 years, and deep networks 96% accuracy. Key gait metrics correlate with age (coefficients ≥0.27).

Conclusion: The work sets performance baselines and guidelines to reduce gait-age error below 3 years, combining meta-analysis, experiments, and interpretability.

Abstract: Estimating a person's age from their gait has important applications in
healthcare, security and human-computer interaction. In this work, we review
fifty-nine studies involving over seventy-five thousand subjects recorded with
video, wearable and radar sensors. We observe that convolutional neural
networks produce an average error of about 4.2 years, inertial-sensor models
about 4.5 years and multi-sensor fusion as low as 3.4 years, with notable
differences between lab and real-world data. We then analyse sixty-three
thousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population
dataset to quantify correlations between age and five key metrics: stride
length, walking speed, step cadence, step-time variability and joint-angle
entropy, with correlation coefficients of at least 0.27. Next, we fine-tune a
ResNet34 model and apply Grad-CAM to reveal that the network attends to the
knee and pelvic regions, consistent with known age-related gait changes.
Finally, on a one hundred thousand sample subset of the VersatileGait database,
we compare support vector machines, decision trees, random forests, multilayer
perceptrons and convolutional neural networks, finding that deep networks
achieve up to 96 percent accuracy while processing each sample in under 0.1
seconds. By combining a broad meta-analysis with new large-scale experiments
and interpretable visualizations, we establish solid performance baselines and
practical guidelines for reducing gait-age error below three years in
real-world scenarios.

</details>


### [56] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Main category: cs.CV

TL;DR: The paper explores adapting a part-pose guided network (PPGNet) for re-identifying feral cats in the wild, achieving high accuracy (mAP 0.86, rank-1 0.95).


<details>
  <summary>Details</summary>
Motivation: Feral cats significantly harm Australian wildlife, necessitating effective monitoring methods. Re-ID using camera trap images offers a solution.

Method: Modified PPGNet (initially for Amur tigers) into PPGNet-Cat, tailored for feral cats. Tested contrastive learning (e.g., ArcFace loss).

Result: PPGNet-Cat performed exceptionally, with mAP 0.86 and rank-1 accuracy 0.95.

Conclusion: PPGNet-Cat is a robust model for feral cat re-ID, enhancing wildlife monitoring efforts.

Abstract: Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [57] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

TL;DR: SketchDNN introduces a generative model for CAD sketches using a unified continuous-discrete diffusion process, improving generation quality.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling heterogeneous primitive parameterizations and permutation invariance in CAD sketches.

Method: Uses Gaussian-Softmax diffusion, blending Gaussian noise with softmax for discrete variables.

Result: Reduces FID from 16.04 to 7.80 and NLL from 84.8 to 81.33, setting a new state-of-the-art.

Conclusion: SketchDNN effectively improves CAD sketch generation quality.

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [58] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: A Variational Autoencoder (VAE) is used for lymph node metastasis staging in rectal cancer, outperforming traditional CNN methods with improved interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing radiological criteria for lymph node metastasis staging are limited. A VAE is proposed for its ability to encode visual features and create a structured latent space.

Method: The VAE replaces large pre-trained CNNs, using an in-house MRI dataset of 168 patients. Post-operative pathological N stage served as ground truth.

Result: The VAE-MLP model achieved state-of-the-art performance (AUC 0.86, Sensitivity 0.79, Specificity 0.85).

Conclusion: VAEs offer a promising alternative to CNNs for LNM staging, with better interpretability and performance.

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [59] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: The paper explores posture-based mental state inference for intent identification in sports, using cricket as a test case, achieving high accuracy and suggesting broader applications.


<details>
  <summary>Details</summary>
Motivation: To address challenges in vision-based mental state diagnosis by leveraging sports settings for data collection and validating posture as a reliable signal for intent inference.

Method: The study uses motion analysis of cricket players' postures to discriminate between aggressive and defensive shot intent, employing weak supervision from existing data statistics.

Result: The method achieves over 75% F1 score and 80% AUC-ROC, demonstrating posture's strong signal for intent inference despite data noise.

Conclusion: Posture-based analysis is effective for intent inference in sports, with potential applications in broader human behavior analysis and overcoming data labeling limitations.

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [60] [VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization](https://arxiv.org/abs/2507.11653)
*Hannah Shafferman,Annika Thomas,Jouko Kinnari,Michael Ricard,Jose Nino,Jonathan How*

Main category: cs.CV

TL;DR: VISTA is a monocular global localization framework that uses object-based segmentation and geometric consistency for robust localization across diverse viewpoints and seasons, outperforming baselines by 69% in recall while being memory-efficient.


<details>
  <summary>Details</summary>
Motivation: Global localization in unstructured environments is challenging due to appearance changes from viewpoint variation, seasonal shifts, and occlusions, which traditional methods struggle with.

Method: VISTA combines an object-based segmentation and tracking pipeline with submap correspondence search to align reference frames geometrically, without domain-specific training.

Result: VISTA achieves up to 69% better recall than baselines on seasonal and oblique-angle datasets, with a map size only 0.6% of the most conservative baseline.

Conclusion: VISTA offers a scalable, real-time solution for global localization in challenging environments, requiring minimal memory and no domain adaptation.

Abstract: Global localization is critical for autonomous navigation, particularly in
scenarios where an agent must localize within a map generated in a different
session or by another agent, as agents often have no prior knowledge about the
correlation between reference frames. However, this task remains challenging in
unstructured environments due to appearance changes induced by viewpoint
variation, seasonal changes, spatial aliasing, and occlusions -- known failure
modes for traditional place recognition methods. To address these challenges,
we propose VISTA (View-Invariant Segmentation-Based Tracking for Frame
Alignment), a novel open-set, monocular global localization framework that
combines: 1) a front-end, object-based, segmentation and tracking pipeline,
followed by 2) a submap correspondence search, which exploits geometric
consistencies between environment maps to align vehicle reference frames. VISTA
enables consistent localization across diverse camera viewpoints and seasonal
changes, without requiring any domain-specific training or finetuning. We
evaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a
69% improvement in recall over baseline methods. Furthermore, we maintain a
compact object-based map that is only 0.6% the size of the most
memory-conservative baseline, making our approach capable of real-time
implementation on resource-constrained platforms.

</details>


### [61] [Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis](https://arxiv.org/abs/2507.11730)
*Maciej Szankin,Vidhyananth Venkatasamy,Lihang Ying*

Main category: cs.CV

TL;DR: Benchmarking Vision-Language Models (VLMs) and CNN-based OCR for outdoor billboard text visibility, revealing VLMs excel in scene reasoning but CNNs are more efficient for cropped text.


<details>
  <summary>Details</summary>
Motivation: Accurate verification of billboard text visibility in outdoor scenes is challenging due to complex conditions like weather and fonts.

Method: Systematically compared VLMs (Qwen 2.5 VL 3B, InternVL3, SmolVLM2) and CNN-based OCR (PaddleOCRv4) on datasets (ICDAR 2015, SVT) with synthetic weather distortions.

Result: VLMs perform better in holistic scene understanding, while CNNs are computationally cheaper and competitive for cropped text.

Conclusion: Lightweight CNNs remain practical for edge deployment, and a weather-augmented benchmark is released for future research.

Abstract: Outdoor advertisements remain a critical medium for modern marketing, yet
accurately verifying billboard text visibility under real-world conditions is
still challenging. Traditional Optical Character Recognition (OCR) pipelines
excel at cropped text recognition but often struggle with complex outdoor
scenes, varying fonts, and weather-induced visual noise. Recently, multimodal
Vision-Language Models (VLMs) have emerged as promising alternatives, offering
end-to-end scene understanding with no explicit detection step. This work
systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,
InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline
(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with
synthetic weather distortions to simulate realistic degradation. Our results
reveal that while selected VLMs excel at holistic scene reasoning, lightweight
CNN pipelines still achieve competitive accuracy for cropped text at a fraction
of the computational cost-an important consideration for edge deployment. To
foster future research, we release our weather-augmented benchmark and
evaluation code publicly.

</details>


### [62] [Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning](https://arxiv.org/abs/2507.11761)
*Fan Shi,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: The paper proposes UCGS, a unified framework for solving multiple abstract visual reasoning tasks without task-specific designs, demonstrating zero-shot reasoning ability.


<details>
  <summary>Details</summary>
Motivation: Current deep AVR solvers require task-specific designs or retraining for new tasks, increasing costs. The goal is to create a unified solution.

Method: Reformulates AVR tasks as predicting target images in problem panels and trains a single conditional generative model for multiple tasks.

Result: UCGS achieves abstract reasoning across tasks with one multi-task training round and shows zero-shot reasoning on unseen tasks.

Conclusion: UCGS offers a cost-effective, unified approach for AVR tasks, eliminating the need for task-specific retraining.

Abstract: Abstract visual reasoning (AVR) enables humans to quickly discover and
generalize abstract rules to new scenarios. Designing intelligent systems with
human-like AVR abilities has been a long-standing topic in the artificial
intelligence community. Deep AVR solvers have recently achieved remarkable
success in various AVR tasks. However, they usually use task-specific designs
or parameters in different tasks. In such a paradigm, solving new tasks often
means retraining the model, and sometimes retuning the model architectures,
which increases the cost of solving AVR problems. In contrast to task-specific
approaches, this paper proposes a novel Unified Conditional Generative Solver
(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we
prove that some well-known AVR tasks can be reformulated as the problem of
estimating the predictability of target images in problem panels. Then, we
illustrate that, under the proposed framework, training one conditional
generative model can solve various AVR tasks. The experiments show that with a
single round of multi-task training, UCGS demonstrates abstract reasoning
ability across various AVR tasks. Especially, UCGS exhibits the ability of
zero-shot reasoning, enabling it to perform abstract reasoning on problems from
unseen AVR tasks in the testing phase.

</details>


### [63] [CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning](https://arxiv.org/abs/2507.11834)
*Peiwen Xia,Tangfei Liao,Wei Zhu,Danhuai Zhao,Jianjun Ke,Kaihao Zhang,Tong Lu,Tao Wang*

Main category: cs.CV

TL;DR: CorrMoE is a novel framework for pruning image correspondences, robust to cross-domain and cross-scene variations, using de-stylization and bi-fusion modules.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle diverse visual domains and scene structures, limiting their robustness.

Method: Uses a De-stylization Dual Branch for domain shift and a Bi-Fusion Mixture of Experts for scene diversity.

Result: Outperforms state-of-the-art methods in accuracy and generalization on benchmark datasets.

Conclusion: CorrMoE effectively addresses domain and scene challenges, offering improved performance and robustness.

Abstract: Establishing reliable correspondences between image pairs is a fundamental
task in computer vision, underpinning applications such as 3D reconstruction
and visual localization. Although recent methods have made progress in pruning
outliers from dense correspondence sets, they often hypothesize consistent
visual domains and overlook the challenges posed by diverse scene structures.
In this paper, we propose CorrMoE, a novel correspondence pruning framework
that enhances robustness under cross-domain and cross-scene variations. To
address domain shift, we introduce a De-stylization Dual Branch, performing
style mixing on both implicit and explicit graph features to mitigate the
adverse influence of domain-specific representations. For scene diversity, we
design a Bi-Fusion Mixture of Experts module that adaptively integrates
multi-perspective features through linear-complexity attention and dynamic
expert routing. Extensive experiments on benchmark datasets demonstrate that
CorrMoE achieves superior accuracy and generalization compared to
state-of-the-art methods. The code and pre-trained models are available at
https://github.com/peiwenxia/CorrMoE.

</details>


### [64] [ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification](https://arxiv.org/abs/2507.11845)
*Kexuan Shi,Zhuang Qi,Jingjing Zhu,Lei Meng,Yaochen Zhang,Haibei Huang,Xiangxu Meng*

Main category: cs.CV

TL;DR: ProtoConNet improves open-set few-shot image classification by integrating contextual information, enhancing feature diversity, and aligning prototypes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook contextual information, leading to spurious associations between context and image subjects in few-shot scenarios.

Method: ProtoConNet uses three modules: CDS for diverse data patterns, CSR for context integration, and PA for prototype alignment.

Result: Experiments show ProtoConNet enhances representation learning and open-set sample identification.

Conclusion: ProtoConNet is superior to existing methods in few-shot open-set classification.

Abstract: Open-set few-shot image classification aims to train models using a small
amount of labeled data, enabling them to achieve good generalization when
confronted with unknown environments. Existing methods mainly use visual
information from a single image to learn class representations to distinguish
known from unknown categories. However, these methods often overlook the
benefits of integrating rich contextual information. To address this issue,
this paper proposes a prototypical augmentation and alignment method, termed
ProtoConNet, which incorporates background information from different samples
to enhance the diversity of the feature space, breaking the spurious
associations between context and image subjects in few-shot scenarios.
Specifically, it consists of three main modules: the clustering-based data
selection (CDS) module mines diverse data patterns while preserving core
features; the contextual-enhanced semantic refinement (CSR) module builds a
context dictionary to integrate into image representations, which boosts the
model's robustness in various scenarios; and the prototypical alignment (PA)
module reduces the gap between image representations and class prototypes,
amplifying feature distances for known and unknown classes. Experimental
results from two datasets verified that ProtoConNet enhances the effectiveness
of representation learning in few-shot scenarios and identifies open-set
samples, making it superior to existing methods.

</details>


### [65] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: GRACE improves dynamic facial expression recognition by aligning refined text and visual signals, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing methods underutilize emotional cues in text and lack mechanisms to filter irrelevant facial dynamics.

Method: GRACE integrates dynamic motion modeling, semantic text refinement, and token-level cross-modal alignment using CATE and motion-difference weighting.

Result: Significant performance improvement on benchmark datasets, especially for ambiguous or imbalanced emotion classes.

Conclusion: GRACE sets new SOTA in DFER by effectively aligning granular representations.

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [66] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Main category: cs.CV

TL;DR: The paper proposes Spatial Frequency Modulation (SFM) to mitigate aliasing in high-frequency features during downsampling and recover details during upsampling, improving semantic segmentation and other tasks.


<details>
  <summary>Details</summary>
Motivation: High-frequency details are crucial for semantic segmentation but prone to aliasing in downsampling layers.

Method: SFM modulates high-frequency features to lower frequencies before downsampling and demodulates them during upsampling, using Adaptive Resampling (ARS) and Multi-Scale Adaptive Upsampling (MSAU).

Result: SFM effectively reduces aliasing and retains details, validated across tasks like image classification and segmentation.

Conclusion: SFM is a versatile, lightweight solution for preserving high-frequency information in neural networks.

Abstract: High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and sparsely resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to transformers. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [67] [SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring](https://arxiv.org/abs/2507.11910)
*Kaustav Chanda,Aayush Atul Verma,Arpitsinh Vaghela,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: SEPose is a synthetic event-based dataset for human pose estimation, addressing data scarcity in pedestrian monitoring. It includes diverse conditions and demonstrates sim-to-real generalization.


<details>
  <summary>Details</summary>
Motivation: Limited data for event-based sensors in challenging pedestrian monitoring scenarios.

Method: Created SEPose dataset using CARLA simulator with dynamic vision sensors, annotating 350K pedestrians. Trained models like RVT and YOLOv8.

Result: Demonstrated sim-to-real generalization by evaluating models on real event-based data.

Conclusion: SEPose fills a data gap and enhances event-based pedestrian monitoring systems.

Abstract: Event-based sensors have emerged as a promising solution for addressing
challenging conditions in pedestrian and traffic monitoring systems. Their
low-latency and high dynamic range allow for improved response time in
safety-critical situations caused by distracted walking or other unusual
movements. However, the availability of data covering such scenarios remains
limited. To address this gap, we present SEPose -- a comprehensive synthetic
event-based human pose estimation dataset for fixed pedestrian perception
generated using dynamic vision sensors in the CARLA simulator. With nearly 350K
annotated pedestrians with body pose keypoints from the perspective of fixed
traffic cameras, SEPose is a comprehensive synthetic multi-person pose
estimation dataset that spans busy and light crowds and traffic across diverse
lighting and weather conditions in 4-way intersections in urban, suburban, and
rural environments. We train existing state-of-the-art models such as RVT and
YOLOv8 on our dataset and evaluate them on real event-based data to demonstrate
the sim-to-real generalization capabilities of the proposed dataset.

</details>


### [68] [Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark](https://arxiv.org/abs/2507.11931)
*Jingqian Wu,Peiqi Duan,Zongqiang Wang,Changwei Wang,Boxin Shi,Edmund Y. Lam*

Main category: cs.CV

TL;DR: Dark-EvGS is a novel event-assisted 3D Gaussian Splatting framework for reconstructing bright multi-view images in low-light conditions, addressing noise, frame quality, and color consistency issues.


<details>
  <summary>Details</summary>
Motivation: Conventional cameras fail in low-light due to dynamic range and motion blur, while event cameras and 3D GS offer potential solutions but face challenges like noise and color inconsistency.

Method: Proposes Dark-EvGS with triplet-level supervision for holistic and detailed knowledge, a color tone matching block, and introduces a real-captured dataset for event-guided bright frame synthesis.

Result: Outperforms existing methods in radiance field reconstruction under low-light conditions, achieving better frame synthesis.

Conclusion: Dark-EvGS successfully addresses low-light challenges, enabling high-quality multi-view image reconstruction with consistent color tones.

Abstract: In low-light environments, conventional cameras often struggle to capture
clear multi-view images of objects due to dynamic range limitations and motion
blur caused by long exposure. Event cameras, with their high-dynamic range and
high-speed properties, have the potential to mitigate these issues.
Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,
facilitating bright frame synthesis from multiple viewpoints in low-light
conditions. However, naively using an event-assisted 3D GS approach still faced
challenges because, in low light, events are noisy, frames lack quality, and
the color tone may be inconsistent. To address these issues, we propose
Dark-EvGS, the first event-assisted 3D GS framework that enables the
reconstruction of bright frames from arbitrary viewpoints along the camera
trajectory. Triplet-level supervision is proposed to gain holistic knowledge,
granular details, and sharp scene rendering. The color tone matching block is
proposed to guarantee the color consistency of the rendered frames.
Furthermore, we introduce the first real-captured dataset for the event-guided
bright frame synthesis task via 3D GS-based radiance field reconstruction.
Experiments demonstrate that our method achieves better results than existing
methods, conquering radiance field reconstruction under challenging low-light
conditions. The code and sample data are included in the supplementary
material.

</details>


### [69] [Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs](https://arxiv.org/abs/2507.11932)
*Mohammad Shahab Sepehri,Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: The paper introduces Hyperphantasia, a benchmark to evaluate mental visualization in MLLMs, revealing a performance gap between humans and models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks focus on passive visual perception, neglecting active mental visualization, a critical human cognitive skill.

Method: Hyperphantasia uses four procedurally generated puzzles at three difficulty levels to assess MLLMs.

Result: State-of-the-art MLLMs show a significant performance gap compared to humans, with partial competence in pattern recognition.

Conclusion: Robust mental visualization remains a challenge for MLLMs, though reinforcement learning shows potential for improvement.

Abstract: Mental visualization, the ability to construct and manipulate visual
representations internally, is a core component of human cognition and plays a
vital role in tasks involving reasoning, prediction, and abstraction. Despite
the rapid progress of Multimodal Large Language Models (MLLMs), current
benchmarks primarily assess passive visual perception, offering limited insight
into the more active capability of internally constructing visual patterns to
support problem solving. Yet mental visualization is a critical cognitive skill
in humans, supporting abilities such as spatial navigation, predicting physical
trajectories, and solving complex visual problems through imaginative
simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic
benchmark designed to evaluate the mental visualization abilities of MLLMs
through four carefully constructed puzzles. Each task is procedurally generated
and presented at three difficulty levels, enabling controlled analysis of model
performance across increasing complexity. Our comprehensive evaluation of
state-of-the-art models reveals a substantial gap between the performance of
humans and MLLMs. Additionally, we explore the potential of reinforcement
learning to improve visual simulation capabilities. Our findings suggest that
while some models exhibit partial competence in recognizing visual patterns,
robust mental visualization remains an open challenge for current MLLMs.

</details>


### [70] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: The paper introduces RaDL, a relation-aware disentangled learning framework, to improve multi-instance image generation by addressing relationship discrepancies and attribute leakage.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models struggle with relationship discrepancies and multiple attributes leakage in multi-instance image generation.

Method: RaDL uses learnable parameters for instance-specific attributes and Relation Attention for relation-aware features, leveraging action verbs from prompts.

Result: RaDL outperforms existing methods on benchmarks like COCO-Position, COCO-MIG, and DrawBench, improving positional accuracy and attribute handling.

Conclusion: RaDL effectively addresses multi-instance image generation challenges by considering relationships and attributes, offering a superior solution.

Abstract: With recent advancements in text-to-image (T2I) models, effectively
generating multiple instances within a single image prompt has become a crucial
challenge. Existing methods, while successful in generating positions of
individual instances, often struggle to account for relationship discrepancy
and multiple attributes leakage. To address these limitations, this paper
proposes the relation-aware disentangled learning (RaDL) framework. RaDL
enhances instance-specific attributes through learnable parameters and
generates relation-aware image features via Relation Attention, utilizing
action verbs extracted from the global prompt. Through extensive evaluations on
benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that
RaDL outperforms existing methods, showing significant improvements in
positional accuracy, multiple attributes consideration, and the relationships
between instances. Our results present RaDL as the solution for generating
images that consider both the relationships and multiple attributes of each
instance within the multi-instance image.

</details>


### [71] [Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation](https://arxiv.org/abs/2507.11955)
*Yuhang Zhang,Zhengyu Zhang,Muxin Liao,Shishun Tian,Wenbin Zou,Lu Zhang,Chen Xu*

Main category: cs.CV

TL;DR: The paper introduces PPAR, a framework for generalizable semantic segmentation, addressing challenges in prototypical alignment and source data reliability using CLIP-based prototypes and progressive alignment.


<details>
  <summary>Details</summary>
Motivation: The need for high generalizability in semantic segmentation for unseen domains, overcoming limitations of existing prototypical methods.

Method: PPAR uses CLIP to generate Original Text Prototype (OTP) and Visual Text Prototype (VTP), progressive alignment, and prototypical reweighting to mitigate negative transfer.

Result: PPAR achieves state-of-the-art performance across multiple benchmarks.

Conclusion: The proposed framework effectively addresses domain generalization challenges in semantic segmentation, validated by experimental results.

Abstract: Generalizable semantic segmentation aims to perform well on unseen target
domains, a critical challenge due to real-world applications requiring high
generalizability. Class-wise prototypes, representing class centroids, serve as
domain-invariant cues that benefit generalization due to their stability and
semantic consistency. However, this approach faces three challenges. First,
existing methods often adopt coarse prototypical alignment strategies, which
may hinder performance. Second, naive prototypes computed by averaging source
batch features are prone to overfitting and may be negatively affected by
unrelated source data. Third, most methods treat all source samples equally,
ignoring the fact that different features have varying adaptation difficulties.
To address these limitations, we propose a novel framework for generalizable
semantic segmentation: Prototypical Progressive Alignment and Reweighting
(PPAR), leveraging the strong generalization ability of the CLIP model.
Specifically, we define two prototypes: the Original Text Prototype (OTP) and
Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for
alignment. We then introduce a progressive alignment strategy that aligns
features in an easy-to-difficult manner, reducing domain gaps gradually.
Furthermore, we propose a prototypical reweighting mechanism that estimates the
reliability of source data and adjusts its contribution, mitigating the effect
of irrelevant or harmful features (i.e., reducing negative transfer). We also
provide a theoretical analysis showing the alignment between our method and
domain generalization theory. Extensive experiments across multiple benchmarks
demonstrate that PPAR achieves state-of-the-art performance, validating its
effectiveness.

</details>


### [72] [Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos](https://arxiv.org/abs/2507.11967)
*Yuchi Ishikawa,Shota Nakada,Hokuto Munakata,Kazuhiro Saito,Tatsuya Komatsu,Yoshimitsu Aoki*

Main category: cs.CV

TL;DR: LG-CAV-MAE integrates text, audio, and visual modalities for improved representation learning, using auto-generated triplets and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance audio-visual representation learning by incorporating text modality and leveraging unlabeled videos.

Method: Uses a pretrained text encoder in contrastive audio-visual masked autoencoders, generates triplets via image captioning and CLAP filtering.

Result: Achieves 5.6% improvement in recall@10 for retrieval and 3.2% for classification.

Conclusion: LG-CAV-MAE effectively learns cross-modal representations without manual annotations, outperforming prior methods.

Abstract: In this paper, we propose Language-Guided Contrastive Audio-Visual Masked
Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning.
LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual
masked autoencoders, enabling the model to learn across audio, visual and text
modalities. To train LG-CAV-MAE, we introduce an automatic method to generate
audio-visual-text triplets from unlabeled videos. We first generate frame-level
captions using an image captioning model and then apply CLAP-based filtering to
ensure strong alignment between audio and captions. This approach yields
high-quality audio-visual-text triplets without requiring manual annotations.
We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an
audio-visual classification task. Our method significantly outperforms existing
approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks
and a 3.2% improvement for the classification task.

</details>


### [73] [Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation](https://arxiv.org/abs/2507.11968)
*Sahid Hossain Mustakim,S M Jishanul Islam,Ummay Maria Muna,Montasir Chowdhury,Mohammed Jawwadul Islam,Sadia Ahmmed,Tashfia Sikder,Syed Tasdid Azam Dhrubo,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: The paper introduces a framework to evaluate tri-modal safety of MLLMs in short-form videos, using a new dataset (SVMA) and attack strategy (ChimeraBreak), revealing significant vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations for MLLMs focus on unimodal attacks, neglecting combined vulnerabilities in short-form video contexts.

Method: The study uses the SVMA dataset with synthetic adversarial attacks and ChimeraBreak, a tri-modal attack strategy, to test MLLMs.

Result: Experiments show high Attack Success Rates (ASR) and distinct failure modes, highlighting model biases.

Conclusion: The findings emphasize the need for robust MLLMs and provide insights for improving safety in multimodal contexts.

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used for content
moderation, yet their robustness in short-form video contexts remains
underexplored. Current safety evaluations often rely on unimodal attacks,
failing to address combined attack vulnerabilities. In this paper, we introduce
a comprehensive framework for evaluating the tri-modal safety of MLLMs. First,
we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising
diverse short-form videos with human-guided synthetic adversarial attacks.
Second, we propose ChimeraBreak, a novel tri-modal attack strategy that
simultaneously challenges visual, auditory, and semantic reasoning pathways.
Extensive experiments on state-of-the-art MLLMs reveal significant
vulnerabilities with high Attack Success Rates (ASR). Our findings uncover
distinct failure modes, showing model biases toward misclassifying benign or
policy-violating content. We assess results using LLM-as-a-judge, demonstrating
attack reasoning efficacy. Our dataset and findings provide crucial insights
for developing more robust and safe MLLMs.

</details>


### [74] [GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2507.11969)
*Zhaohong Huang,Yuxin Zhang,Jingjing Xie,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: GS-Bias introduces efficient test-time adaptation for VLMs using global and spatial biases, improving performance and reducing memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods for VLMs are inefficient or unstable, prompting the need for a balanced approach.

Method: GS-Bias uses learnable global and spatial biases added to VLM logits, avoiding full backpropagation.

Result: Achieves state-of-the-art performance on 15 benchmarks, with significant efficiency gains (e.g., 6.5% of TPT's memory).

Conclusion: GS-Bias offers a highly efficient and effective TTA solution for VLMs.

Abstract: Recent advances in test-time adaptation (TTA) for Vision-Language Models
(VLMs) have garnered increasing attention, particularly through the use of
multiple augmented views of a single image to boost zero-shot generalization.
Unfortunately, existing methods fail to strike a satisfactory balance between
performance and efficiency, either due to excessive overhead of tuning text
prompts or unstable benefits from handcrafted, training-free visual feature
enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),
an efficient and effective TTA paradigm that incorporates two learnable biases
during TTA, unfolded as the global bias and spatial bias. Particularly, the
global bias captures the global semantic features of a test image by learning
consistency across augmented views, while spatial bias learns the semantic
coherence between regions in the image's spatial visual representation. It is
worth highlighting that these two sets of biases are directly added to the
logits outputed by the pretrained VLMs, which circumvent the full
backpropagation through VLM that hinders the efficiency of existing TTA
methods. This endows GS-Bias with extremely high efficiency while achieving
state-of-the-art performance on 15 benchmark datasets. For example, it achieves
a 2.23% improvement over TPT in cross-dataset generalization and a 2.72%
improvement in domain generalization, while requiring only 6.5% of TPT's memory
usage on ImageNet.

</details>


### [75] [EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models](https://arxiv.org/abs/2507.11980)
*Jiajian Xie,Shengyu Zhang,Zhou Zhao,Fan Wu,Fei Wu*

Main category: cs.CV

TL;DR: EC-Diff accelerates cloud-edge collaborative diffusion models by optimizing noise estimation and handoff timing, improving speed and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between cloud inference time and semantic ambiguity in edge-cloud diffusion models.

Method: Uses gradient-based noise estimation, K-step noise approximation, and a two-stage greedy search for optimal handoff.

Result: Achieves 2x speedup over cloud inference while maintaining high generation quality.

Conclusion: EC-Diff effectively balances speed and quality in diffusion model inference.

Abstract: Diffusion Models have shown remarkable proficiency in image and video
synthesis. As model size and latency increase limit user experience, hybrid
edge-cloud collaborative framework was recently proposed to realize fast
inference and high-quality generation, where the cloud model initiates
high-quality semantic planning and the edge model expedites later-stage
refinement. However, excessive cloud denoising prolongs inference time, while
insufficient steps cause semantic ambiguity, leading to inconsistency in edge
model output. To address these challenges, we propose EC-Diff that accelerates
cloud inference through gradient-based noise estimation while identifying the
optimal point for cloud-edge handoff to maintain generation quality.
Specifically, we design a K-step noise approximation strategy to reduce cloud
inference frequency by using noise gradients between steps and applying cloud
inference periodically to adjust errors. Then we design a two-stage greedy
search algorithm to efficiently find the optimal parameters for noise
approximation and edge model switching. Extensive experiments demonstrate that
our method significantly enhances generation quality compared to edge
inference, while achieving up to an average $2\times$ speedup in inference
compared to cloud inference. Video samples and source code are available at
https://ec-diff.github.io/.

</details>


### [76] [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/abs/2507.11985)
*Jiahao Xia,Yike Wu,Wenjian Huang,Jianguo Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: MPAE introduces an unsupervised method for part discovery in images, using masked autoencoding to align part descriptors with actual shapes robustly across categories.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of fine-grained labels and robustness in unsupervised part discovery methods.

Method: Uses a Masked Part Autoencoder (MPAE) to learn part descriptors and fill masked regions based on feature similarity, aligning parts with shapes.

Result: MPAE robustly discovers meaningful parts across diverse categories and scenarios, even under occlusion.

Conclusion: MPAE provides a scalable, unsupervised solution for part discovery, applicable to complex scenarios and cross-category analysis.

Abstract: Part-level features are crucial for image understanding, but few studies
focus on them because of the lack of fine-grained labels. Although unsupervised
part discovery can eliminate the reliance on labels, most of them cannot
maintain robustness across various categories and scenarios, which restricts
their application range. To overcome this limitation, we present a more
effective paradigm for unsupervised part discovery, named Masked Part
Autoencoder (MPAE). It first learns part descriptors as well as a feature map
from the inputs and produces patch features from a masked version of the
original images. Then, the masked regions are filled with the learned part
descriptors based on the similarity between the local features and descriptors.
By restoring these masked patches using the part descriptors, they become
better aligned with their part shapes, guided by appearance features from
unmasked patches. Finally, MPAE robustly discovers meaningful parts that
closely match the actual object shapes, even in complex scenarios. Moreover,
several looser yet more effective constraints are proposed to enable MPAE to
identify the presence of parts across various scenarios and categories in an
unsupervised manner. This provides the foundation for addressing challenges
posed by occlusion and for exploring part similarity across multiple
categories. Extensive experiments demonstrate that our method robustly
discovers meaningful parts across various categories and scenarios. The code is
available at the project https://github.com/Jiahao-UTS/MPAE.

</details>


### [77] [Style Composition within Distinct LoRA modules for Traditional Art](https://arxiv.org/abs/2507.11986)
*Jaehyun Lee,Wonhark Park,Wonsik Shin,Hyunho Lee,Hyoung Min Na,Nojun Kwak*

Main category: cs.CV

TL;DR: A zero-shot diffusion pipeline for blending multiple artistic styles regionally by fusing denoised latents from style-specialized models, ensuring style fidelity and structural coherence.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with regional style control due to entangled latent spaces, often letting one style dominate.

Method: Proposes a pipeline that blends styles by fusing denoised latents from separate models using spatial masks and depth-map conditioning via ControlNet.

Result: Achieves precise, region-specific style mixing while preserving individual style fidelity and structural coherence.

Conclusion: The method enables controlled, user-guided style composition, validated by qualitative and quantitative experiments.

Abstract: Diffusion-based text-to-image models have achieved remarkable results in
synthesizing diverse images from text prompts and can capture specific artistic
styles via style personalization. However, their entangled latent space and
lack of smooth interpolation make it difficult to apply distinct painting
techniques in a controlled, regional manner, often causing one style to
dominate. To overcome this, we propose a zero-shot diffusion pipeline that
naturally blends multiple styles by performing style composition on the
denoised latents predicted during the flow-matching denoising process of
separately trained, style-specialized models. We leverage the fact that
lower-noise latents carry stronger stylistic information and fuse them across
heterogeneous diffusion pipelines using spatial masks, enabling precise,
region-specific style control. This mechanism preserves the fidelity of each
individual style while allowing user-guided mixing. Furthermore, to ensure
structural coherence across different models, we incorporate depth-map
conditioning via ControlNet into the diffusion framework. Qualitative and
quantitative experiments demonstrate that our method successfully achieves
region-specific style mixing according to the given masks.

</details>


### [78] [ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2507.11990)
*Hyun-Jun Jin,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: ID-EA improves personalized portrait generation by aligning text embeddings with visual identity embeddings, enhancing facial identity consistency and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current Textual Inversion methods fail to maintain consistent facial identity due to semantic misalignments between textual and visual embeddings.

Method: ID-EA uses an ID-driven Enhancer and ID-conditioned Adapter to refine visual identity embeddings and adapt text conditions, ensuring identity preservation.

Result: ID-EA outperforms state-of-the-art methods in identity preservation and is 15 times faster.

Conclusion: ID-EA offers a robust solution for high-fidelity personalized portrait generation with improved identity consistency and efficiency.

Abstract: Recently, personalized portrait generation with a text-to-image diffusion
model has significantly advanced with Textual Inversion, emerging as a
promising approach for creating high-fidelity personalized images. Despite its
potential, current Textual Inversion methods struggle to maintain consistent
facial identity due to semantic misalignments between textual and visual
embedding spaces regarding identity. We introduce ID-EA, a novel framework that
guides text embeddings to align with visual identity embeddings, thereby
improving identity preservation in a personalized generation. ID-EA comprises
two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned
Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings
with a textual ID anchor, refining visual identity embeddings derived from a
face recognition model using representative text embeddings. Then, the
ID-Adapter leverages the identity-enhanced embedding to adapt the text
condition, ensuring identity preservation by adjusting the cross-attention
module in the pre-trained UNet model. This process encourages the text features
to find the most related visual clues across the foreground snippets. Extensive
quantitative and qualitative evaluations demonstrate that ID-EA substantially
outperforms state-of-the-art methods in identity preservation metrics while
achieving remarkable computational efficiency, generating personalized
portraits approximately 15 times faster than existing approaches.

</details>


### [79] [SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation](https://arxiv.org/abs/2507.11994)
*Jun Yin,Fei Wu,Yupeng Ren,Jisheng Huang,Qiankun Li,Heng jin,Jianhai Fu,Chanjie Cui*

Main category: cs.CV

TL;DR: SAMST is a semi-supervised semantic segmentation method for remote sensing data, leveraging SAM for zero-shot generalization and boundary detection to refine pseudo-labels and improve model performance.


<details>
  <summary>Details</summary>
Motivation: Public remote sensing datasets lack universality due to resolution variability and inconsistent land cover definitions, limiting the use of unlabeled data.

Method: SAMST combines supervised self-training with a SAM-based Pseudo-label Refiner, which includes threshold filtering, prompt generation, and label refinement modules.

Result: Experiments on the Potsdam dataset show SAMST effectively improves pseudo-label accuracy and model performance.

Conclusion: SAMST addresses limited labeled data challenges in remote sensing semantic segmentation, demonstrating feasibility and effectiveness.

Abstract: Public remote sensing datasets often face limitations in universality due to
resolution variability and inconsistent land cover category definitions. To
harness the vast pool of unlabeled remote sensing data, we propose SAMST, a
semi-supervised semantic segmentation method. SAMST leverages the strengths of
the Segment Anything Model (SAM) in zero-shot generalization and boundary
detection. SAMST iteratively refines pseudo-labels through two main components:
supervised model self-training using both labeled and pseudo-labeled data, and
a SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three
modules: a Threshold Filter Module for preprocessing, a Prompt Generation
Module for extracting connected regions and generating prompts for SAM, and a
Label Refinement Module for final label stitching. By integrating the
generalization power of large models with the training efficiency of small
models, SAMST improves pseudo-label accuracy, thereby enhancing overall model
performance. Experiments on the Potsdam dataset validate the effectiveness and
feasibility of SAMST, demonstrating its potential to address the challenges
posed by limited labeled data in remote sensing semantic segmentation.

</details>


### [80] [AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation](https://arxiv.org/abs/2507.12001)
*Hao Li,Ju Dai,Feng Zhou,Kaida Ning,Lei Li,Junjun Pan*

Main category: cs.CV

TL;DR: The paper introduces AUBlendSet, a 3D facial dataset for fine-grained expression manipulation, and AUBlendNet, a network for stylized facial animation.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack fine-grained stylized 3D facial expression manipulation capabilities.

Method: AUBlendSet is created with 32 AUs across 500 identities. AUBlendNet learns AU-Blendshape basis vectors for stylized manipulation.

Result: AUBlendSet and AUBlendNet enable tasks like stylized expression manipulation and speech-driven animation.

Conclusion: AUBlendSet and AUBlendNet are pioneering tools for 3D facial animation, validated through experiments.

Abstract: While 3D facial animation has made impressive progress, challenges still
exist in realizing fine-grained stylized 3D facial expression manipulation due
to the lack of appropriate datasets. In this paper, we introduce the
AUBlendSet, a 3D facial dataset based on AU-Blendshape representation for
fine-grained facial expression manipulation across identities. AUBlendSet is a
blendshape data collection based on 32 standard facial action units (AUs)
across 500 identities, along with an additional set of facial postures
annotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to
learn AU-Blendshape basis vectors for different character styles. AUBlendNet
predicts, in parallel, the AU-Blendshape basis vectors of the corresponding
style for a given identity mesh, thereby achieving stylized 3D emotional facial
manipulation. We comprehensively validate the effectiveness of AUBlendSet and
AUBlendNet through tasks such as stylized facial expression manipulation,
speech-driven emotional facial animation, and emotion recognition data
augmentation. Through a series of qualitative and quantitative experiments, we
demonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D
facial animation tasks. To the best of our knowledge, AUBlendSet is the first
dataset, and AUBlendNet is the first network for continuous 3D facial
expression manipulation for any identity through facial AUs. Our source code is
available at https://github.com/wslh852/AUBlendNet.git.

</details>


### [81] [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/abs/2507.12006)
*Linwei Chen,Lin Gu,Ying Fu*

Main category: cs.CV

TL;DR: The paper introduces Frequency-Dynamic Attention Modulation (FDAM), a method to address frequency vanishing in Vision Transformers (ViTs) by modulating their frequency response using Attention Inversion (AttInv) and Frequency Dynamic Scaling (FreqScale).


<details>
  <summary>Details</summary>
Motivation: Existing ViTs suffer from frequency vanishing due to their low-pass filter-like attention mechanism, leading to loss of critical details and textures.

Method: Proposes FDAM, combining AttInv (generates high-pass filtering by inverting low-pass filters) and FreqScale (weights frequency components for fine adjustments).

Result: FDAM avoids representation collapse and improves performance in tasks like semantic segmentation, object detection, and instance segmentation, achieving state-of-the-art results in remote sensing detection.

Conclusion: FDAM effectively addresses frequency vanishing in ViTs, enhancing performance across various models and tasks.

Abstract: Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.

</details>


### [82] [Dual form Complementary Masking for Domain-Adaptive Image Segmentation](https://arxiv.org/abs/2507.12008)
*Jiawen Wang,Yinda Chen,Xiaoyu Liu,Che Liu,Dong Liu,Jianqing Gao,Zhiwei Xiong*

Main category: cs.CV

TL;DR: MaskTwins reframes masked reconstruction in UDA as a sparse signal problem, proving complementary masks enhance domain-agnostic feature extraction, outperforming baselines in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing works treat masking as simple deformation without theoretical grounding, limiting its potential for feature extraction in UDA.

Method: Proposes MaskTwins, integrating masked reconstruction into UDA training, enforcing consistency between complementary masked images for domain generalization.

Result: Outperforms baselines in natural and biological image segmentation, showing superior domain-invariant feature extraction.

Conclusion: MaskTwins offers a new paradigm for domain-adaptive segmentation without separate pre-training, leveraging masked reconstruction effectively.

Abstract: Recent works have correlated Masked Image Modeling (MIM) with consistency
regularization in Unsupervised Domain Adaptation (UDA). However, they merely
treat masking as a special form of deformation on the input images and neglect
the theoretical analysis, which leads to a superficial understanding of masked
reconstruction and insufficient exploitation of its potential in enhancing
feature extraction and representation learning. In this paper, we reframe
masked reconstruction as a sparse signal reconstruction problem and
theoretically prove that the dual form of complementary masks possesses
superior capabilities in extracting domain-agnostic image features. Based on
this compelling insight, we propose MaskTwins, a simple yet effective UDA
framework that integrates masked reconstruction directly into the main training
pipeline. MaskTwins uncovers intrinsic structural patterns that persist across
disparate domains by enforcing consistency between predictions of images masked
in complementary ways, enabling domain generalization in an end-to-end manner.
Extensive experiments verify the superiority of MaskTwins over baseline methods
in natural and biological image segmentation. These results demonstrate the
significant advantages of MaskTwins in extracting domain-invariant features
without the need for separate pre-training, offering a new paradigm for
domain-adaptive segmentation.

</details>


### [83] [Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli](https://arxiv.org/abs/2507.12009)
*Florian David,Michael Chan,Elenor Morgenroth,Patrik Vuilleumier,Dimitri Van De Ville*

Main category: cs.CV

TL;DR: An end-to-end deep neural encoder-decoder model is proposed to encode and decode brain activity from fMRI data during naturalistic stimuli, focusing on visual cortex voxels and reconstructing visual inputs.


<details>
  <summary>Details</summary>
Motivation: To bridge the temporal resolution gap between natural movie stimuli and fMRI acquisitions and understand visual processing in films.

Method: Uses temporal convolutional layers to leverage temporally correlated input from consecutive film frames, predicting voxel activity and reconstructing visual inputs.

Result: Identifies key brain regions (middle occipital, fusiform, calcarine) contributing to visual decoding, aligning with the decoder's ability to reconstruct edges, faces, and contrasts.

Conclusion: Suggests deep learning models can probe visual processing understanding in films, linking model behavior to brain functions.

Abstract: We propose an end-to-end deep neural encoder-decoder model to encode and
decode brain activity in response to naturalistic stimuli using functional
magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input
from consecutive film frames, we employ temporal convolutional layers in our
architecture, which effectively allows to bridge the temporal resolution gap
between natural movie stimuli and fMRI acquisitions. Our model predicts
activity of voxels in and around the visual cortex and performs reconstruction
of corresponding visual inputs from neural activity. Finally, we investigate
brain regions contributing to visual decoding through saliency maps. We find
that the most contributing regions are the middle occipital area, the fusiform
area, and the calcarine, respectively employed in shape perception, complex
recognition (in particular face perception), and basic visual features such as
edges and contrasts. These functions being strongly solicited are in line with
the decoder's capability to reconstruct edges, faces, and contrasts. All in
all, this suggests the possibility to probe our understanding of visual
processing in films using as a proxy the behaviour of deep learning models such
as the one proposed in this paper.

</details>


### [84] [SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection](https://arxiv.org/abs/2507.12017)
*Xiwei Zhang,Chunjin Yang,Yiming Xiao,Runtong Zhang,Fanman Meng*

Main category: cs.CV

TL;DR: The paper proposes SS-DC, a framework for unsupervised domain adaptive object detection (UDAOD) from visible (RGB) to infrared (IR) domains, using a decoupling-coupling strategy to handle multiple subdomains like daytime, nighttime, and foggy scenes.


<details>
  <summary>Details</summary>
Motivation: Existing UDAOD methods treat the RGB domain as unified, ignoring subdomains. Decoupling domain-invariant (DI) and domain-specific (DS) features across subdomains can improve RGB-IR adaptation.

Method: The SS-DC framework includes a Spectral Adaptive Idempotent Decoupling (SAID) module for spectral decomposition, a filter bank-based spectral processing paradigm, and a self-distillation-driven decoupling loss. It also introduces spatial-spectral coupling and DS features to reduce domain bias.

Result: The method significantly improves baseline performance and outperforms existing UDAOD methods on multiple RGB-IR datasets, including a new protocol based on FLIR-ADAS.

Conclusion: Decoupling DI and DS features across subdomains enhances RGB-IR domain adaptation, with the proposed SS-DC framework achieving superior results.

Abstract: Unsupervised domain adaptive object detection (UDAOD) from the visible domain
to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB
domain as a unified domain and neglect the multiple subdomains within it, such
as daytime, nighttime, and foggy scenes. We argue that decoupling the
domain-invariant (DI) and domain-specific (DS) features across these multiple
subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper
proposes a new SS-DC framework based on a decoupling-coupling strategy. In
terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)
module in the aspect of spectral decomposition. Due to the style and content
information being highly embedded in different frequency bands, this module can
decouple DI and DS components more accurately and interpretably. A novel filter
bank-based spectral processing paradigm and a self-distillation-driven
decoupling loss are proposed to improve the spectral domain decoupling. In
terms of coupling, a new spatial-spectral coupling method is proposed, which
realizes joint coupling through spatial and spectral DI feature pyramids.
Meanwhile, this paper introduces DS from decoupling to reduce the domain bias.
Extensive experiments demonstrate that our method can significantly improve the
baseline performance and outperform existing UDAOD methods on multiple RGB-IR
datasets, including a new experimental protocol proposed in this paper based on
the FLIR-ADAS dataset.

</details>


### [85] [Dataset Ownership Verification for Pre-trained Masked Models](https://arxiv.org/abs/2507.12022)
*Yuechen Xie,Jie Song,Yicheng Shan,Xiaoyan Zhang,Yuanyu Wan,Shengxuming Zhang,Jiarui Duan,Mingli Song*

Main category: cs.CV

TL;DR: DOV4MM is a new method for verifying dataset ownership in masked models, addressing a gap in existing techniques tailored for supervised and contrastive models.


<details>
  <summary>Details</summary>
Motivation: Protecting high-quality open-source datasets from exploitation by verifying ownership, especially for masked models where current methods fall short.

Method: DOV4MM leverages the difficulty of reconstructing masked information in the embedding space to verify if a model was pre-trained on a specific dataset.

Result: DOV4MM outperforms prior methods, rejecting the null hypothesis with a p-value below 0.05, validated on masked image and language models.

Conclusion: DOV4MM effectively verifies dataset ownership for masked models, offering a solution to an unresolved challenge in the field.

Abstract: High-quality open-source datasets have emerged as a pivotal catalyst driving
the swift advancement of deep learning, while facing the looming threat of
potential exploitation. Protecting these datasets is of paramount importance
for the interests of their owners. The verification of dataset ownership has
evolved into a crucial approach in this domain; however, existing verification
techniques are predominantly tailored to supervised models and contrastive
pre-trained models, rendering them ill-suited for direct application to the
increasingly prevalent masked models. In this work, we introduce the inaugural
methodology addressing this critical, yet unresolved challenge, termed Dataset
Ownership Verification for Masked Modeling (DOV4MM). The central objective is
to ascertain whether a suspicious black-box model has been pre-trained on a
particular unlabeled dataset, thereby assisting dataset owners in safeguarding
their rights. DOV4MM is grounded in our empirical observation that when a model
is pre-trained on the target dataset, the difficulty of reconstructing masked
information within the embedding space exhibits a marked contrast to models not
pre-trained on that dataset. We validated the efficacy of DOV4MM through ten
masked image models on ImageNet-1K and four masked language models on
WikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,
with a $p$-value considerably below 0.05, surpassing all prior approaches. Code
is available at https://github.com/xieyc99/DOV4MM.

</details>


### [86] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Main category: cs.CV

TL;DR: The paper proposes MVAR, a multivariate autoregressive model for forecasting air pollutants, addressing limitations of single-pollutant methods by incorporating pollutant interactions and spatial responses. It introduces a training paradigm for long-term forecasting and a meteorological coupling mechanism, validated on a comprehensive dataset.


<details>
  <summary>Details</summary>
Motivation: Existing air pollutant forecasting methods focus on single pollutants, ignoring interactions and spatial responses, which limits their practical utility for pollution warnings and policy-making.

Method: The MVAR model reduces dependency on long-time-window inputs, uses a Multivariate Autoregressive Training Paradigm for 120-hour forecasting, and incorporates a Meteorological Coupled Spatial Transformer block to integrate AI-based meteorological forecasts and pollutant interactions.

Result: MVAR outperforms state-of-the-art methods, demonstrating improved forecasting accuracy and efficiency on a dataset covering 6 pollutants across 75 cities in North China.

Conclusion: MVAR effectively addresses the limitations of single-pollutant forecasting, offering a robust solution for multivariate air pollutant prediction with enhanced long-term forecasting capabilities.

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [87] [3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering](https://arxiv.org/abs/2507.12026)
*Rongtao Xu,Han Gao,Mingming Yu,Dong An,Shunpeng Chen,Changwei Wang,Li Guo,Xiaodan Liang,Shibiao Xu*

Main category: cs.CV

TL;DR: 3D-MoRe is a framework for generating large-scale 3D-language datasets using foundational models, improving performance in indoor scene tasks like QA and dense captioning.


<details>
  <summary>Details</summary>
Motivation: Address the need for diverse and scalable data in indoor scene tasks by leveraging foundational models.

Method: Integrates multi-modal embedding, cross-modal interaction, and a language model decoder to process 3D scene data and natural language instructions. Uses ScanNet dataset with text annotations from ScanQA and ScanRefer, along with data augmentation and semantic filtering.

Result: Generates 62,000 QA pairs and 73,000 object descriptions. Outperforms baselines with CIDEr score improvements of 2.15% on ScanQA and 1.84% on ScanRefer.

Conclusion: 3D-MoRe effectively enhances reasoning and response generation in 3D environments, with datasets and code released publicly for community use.

Abstract: With the growing need for diverse and scalable data in indoor scene tasks,
such as question answering and dense captioning, we propose 3D-MoRe, a novel
paradigm designed to generate large-scale 3D-language datasets by leveraging
the strengths of foundational models. The framework integrates key components,
including multi-modal embedding, cross-modal interaction, and a language model
decoder, to process natural language instructions and 3D scene data. This
approach facilitates enhanced reasoning and response generation in complex 3D
environments. Using the ScanNet 3D scene dataset, along with text annotations
from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs
and 73,000 object descriptions across 1,513 scenes. We also employ various data
augmentation techniques and implement semantic filtering to ensure high-quality
data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms
state-of-the-art baselines, with the CIDEr score improving by 2.15\%.
Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5
by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated
datasets will be publicly released to benefit the community, and both can be
accessed on the https://3D-MoRe.github.io.

</details>


### [88] [SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation](https://arxiv.org/abs/2507.12027)
*Beining Xu,Siting Zhu,Hesheng Wang*

Main category: cs.CV

TL;DR: SGLoc is a novel localization system that regresses camera poses from 3D Gaussian Splatting (3DGS) using semantic information, achieving high accuracy without initial pose priors.


<details>
  <summary>Details</summary>
Motivation: To enable accurate camera pose estimation without relying on prior pose information by leveraging semantic relationships between 2D images and 3D scenes.

Method: Uses a multi-level pose regression strategy and semantic-based global retrieval to align 2D images with 3DGS maps, followed by iterative refinement.

Result: Outperforms baselines on 12scenes and 7scenes datasets, excelling in global localization without initial pose priors.

Conclusion: SGLoc is effective for camera pose estimation, demonstrating robustness and superior performance in challenging scenarios.

Abstract: We propose SGLoc, a novel localization system that directly regresses camera
poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic
information. Our method utilizes the semantic relationship between 2D image and
3D scene representation to estimate the 6DoF pose without prior pose
information. In this system, we introduce a multi-level pose regression
strategy that progressively estimates and refines the pose of query image from
the global 3DGS map, without requiring initial pose priors. Moreover, we
introduce a semantic-based global retrieval algorithm that establishes
correspondences between 2D (image) and 3D (3DGS map). By matching the extracted
scene semantic descriptors of 2D query image and 3DGS semantic representation,
we align the image with the local region of the global 3DGS map, thereby
obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by
iteratively optimizing the difference between the query image and the rendered
image from 3DGS. Our SGLoc demonstrates superior performance over baselines on
12scenes and 7scenes datasets, showing excellent capabilities in global
localization without initial pose prior. Code will be available at
https://github.com/IRMVLab/SGLoc.

</details>


### [89] [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](https://arxiv.org/abs/2507.12029)
*Xinhang Wan,Jiyuan Liu,Qian Qu,Suyuan Liu,Chuyu Zhang,Fangdi Wang,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: Proposes IICMVNCD, a multi-view NCD framework addressing limitations of single-view focus and pseudo-label instability by leveraging intra-view and inter-view correlations.


<details>
  <summary>Details</summary>
Motivation: Existing NCD methods are limited to single-view data and rely on unstable pseudo-labels, ignoring multi-view scenarios like disease diagnosis.

Method: Uses matrix factorization for intra-view distributional similarity and inter-view weighted fusion of factor matrices with dynamic view weight adjustment.

Result: Experimental results confirm the framework's effectiveness in multi-view NCD.

Conclusion: IICMVNCD successfully addresses multi-view NCD challenges, improving stability and performance.

Abstract: In this paper, we address the problem of novel class discovery (NCD), which
aims to cluster novel classes by leveraging knowledge from disjoint known
classes. While recent advances have made significant progress in this area,
existing NCD methods face two major limitations. First, they primarily focus on
single-view data (e.g., images), overlooking the increasingly common multi-view
data, such as multi-omics datasets used in disease diagnosis. Second, their
reliance on pseudo-labels to supervise novel class clustering often results in
unstable performance, as pseudo-label quality is highly sensitive to factors
such as data noise and feature dimensionality. To address these challenges, we
propose a novel framework named Intra-view and Inter-view Correlation Guided
Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to
explore NCD in multi-view setting so far. Specifically, at the intra-view
level, leveraging the distributional similarity between known and novel
classes, we employ matrix factorization to decompose features into
view-specific shared base matrices and factor matrices. The base matrices
capture distributional consistency among the two datasets, while the factor
matrices model pairwise relationships between samples. At the inter-view level,
we utilize view relationships among known classes to guide the clustering of
novel classes. This includes generating predicted labels through the weighted
fusion of factor matrices and dynamically adjusting view weights of known
classes based on the supervision loss, which are then transferred to novel
class learning. Experimental results validate the effectiveness of our proposed
approach.

</details>


### [90] [MoViAD: Modular Visual Anomaly Detection](https://arxiv.org/abs/2507.12049)
*Manuel Barusco,Francesco Borsatti,Arianna Stropeni,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: MoViAD is a modular library for Video Anomaly Detection (VAD), offering state-of-the-art models, datasets, and utilities to address challenges like data scarcity and unsupervised training. It supports diverse scenarios and practical deployment needs.


<details>
  <summary>Details</summary>
Motivation: The scarcity of anomalous data and the need for unsupervised training in VAD pose significant challenges. MoViAD aims to accelerate research and deployment by providing a comprehensive, modular solution.

Method: MoViAD integrates advanced VAD models, trainers, datasets, and utilities. It supports various scenarios (e.g., continual, semi-supervised) and offers tools for Edge/IoT deployment, including quantization and compression.

Result: The library enables fast, effortless deployment for engineers and flexibility for researchers. It includes robust evaluation metrics and profiling tools for efficiency analysis.

Conclusion: MoViAD bridges the gap between research and practical deployment in VAD, offering a versatile and efficient solution for diverse use cases.

Abstract: VAD is a critical field in machine learning focused on identifying deviations
from normal patterns in images, often challenged by the scarcity of anomalous
data and the need for unsupervised training. To accelerate research and
deployment in this domain, we introduce MoViAD, a comprehensive and highly
modular library designed to provide fast and easy access to state-of-the-art
VAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array
of scenarios, including continual, semi-supervised, few-shots, noisy, and many
more. In addition, it addresses practical deployment challenges through
dedicated Edge and IoT settings, offering optimized models and backbones, along
with quantization and compression utilities for efficient on-device execution
and distributed inference. MoViAD integrates a selection of backbones, robust
evaluation VAD metrics (pixel-level and image-level) and useful profiling tools
for efficiency analysis. The library is designed for fast, effortless
deployment, enabling machine learning engineers to easily use it for their
specific setup with custom models, datasets, and backbones. At the same time,
it offers the flexibility and extensibility researchers need to develop and
experiment with new methods.

</details>


### [91] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: InstructFLIP is a novel framework for face anti-spoofing (FAS) that uses vision-language models (VLMs) and a meta-domain strategy to improve generalization and reduce training redundancy.


<details>
  <summary>Details</summary>
Motivation: Addressing limited semantic understanding of attack types and training redundancy across domains in FAS.

Method: Integrates VLMs for better visual perception and employs a meta-domain strategy for unified learning. Explicitly decouples instructions into content and style components.

Result: Outperforms SOTA models in accuracy and reduces training redundancy across diverse domains.

Conclusion: InstructFLIP effectively enhances FAS generalization and efficiency.

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [92] [MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning](https://arxiv.org/abs/2507.12062)
*Hongxu Ma,Guanshuo Wang,Fufu Yu,Qiong Jia,Shouhong Ding*

Main category: cs.CV

TL;DR: MS-DETR is a framework for Video Moment Retrieval and Highlight Detection, leveraging motion-semantics relationships and contrastive denoising learning to outperform state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To harness the untapped potential of relationships between temporal motion and spatial semantics in video content for MR/HD tasks.

Method: Proposes MS-DETR, which models intra-modal correlations in motion and semantics, uses task-wise correlation for localization, and employs contrastive denoising learning to address dataset sparsity.

Result: Outperforms existing models on four MR/HD benchmarks.

Conclusion: MS-DETR effectively captures motion-semantics features and addresses dataset sparsity, achieving superior performance.

Abstract: Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint
specific moments and assess clip-wise relevance based on the text query. While
DETR-based joint frameworks have made significant strides, there remains
untapped potential in harnessing the intricate relationships between temporal
motion and spatial semantics within video content. In this paper, we propose
the Motion-Semantics DETR (MS-DETR), a framework that captures rich
motion-semantics features through unified learning for MR/HD tasks. The encoder
first explicitly models disentangled intra-modal correlations within motion and
semantics dimensions, guided by the given text queries. Subsequently, the
decoder utilizes the task-wise correlation across temporal motion and spatial
semantics dimensions to enable precise query-guided localization for MR and
refined highlight boundary delineation for HD. Furthermore, we observe the
inherent sparsity dilemma within the motion and semantics dimensions of MR/HD
datasets. To address this issue, we enrich the corpus from both dimensions by
generation strategies and propose contrastive denoising learning to ensure the
above components learn robustly and effectively. Extensive experiments on four
MR/HD benchmarks demonstrate that our method outperforms existing
state-of-the-art models by a margin. Our code is available at
https://github.com/snailma0229/MS-DETR.git.

</details>


### [93] [Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics](https://arxiv.org/abs/2507.12083)
*Muleilan Pei,Shaoshuai Shi,Xuesong Chen,Xu Liu,Shaojie Shen*

Main category: cs.CV

TL;DR: The paper proposes a 'First Reasoning, Then Forecasting' strategy for motion forecasting in autonomous driving, using a reward-driven intention reasoner and hierarchical trajectory generation.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven approaches lack interpretability and direct trajectory prediction may not account for behavior intentions. The paper aims to improve forecasting by incorporating spatial guidance from reasoned intentions.

Method: The method involves encoding traffic agents and scenes into a vectorized representation, deriving reward distributions via IRL, reasoning about intentions, and generating trajectories with a hierarchical decoder.

Result: The approach outperforms state-of-the-art methods on Argoverse and nuScenes datasets, enhancing prediction confidence.

Conclusion: The proposed strategy effectively integrates intention reasoning with trajectory forecasting, improving accuracy and interpretability for autonomous driving systems.

Abstract: Motion forecasting for on-road traffic agents presents both a significant
challenge and a critical necessity for ensuring safety in autonomous driving
systems. In contrast to most existing data-driven approaches that directly
predict future trajectories, we rethink this task from a planning perspective,
advocating a "First Reasoning, Then Forecasting" strategy that explicitly
incorporates behavior intentions as spatial guidance for trajectory prediction.
To achieve this, we introduce an interpretable, reward-driven intention
reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)
scheme. Our method first encodes traffic agents and scene elements into a
unified vectorized representation, then aggregates contextual features through
a query-centric paradigm. This enables the derivation of a reward distribution,
a compact yet informative representation of the target agent's behavior within
the given scene context via IRL. Guided by this reward heuristic, we perform
policy rollouts to reason about multiple plausible intentions, providing
valuable priors for subsequent trajectory generation. Finally, we develop a
hierarchical DETR-like decoder integrated with bidirectional selective state
space models to produce accurate future trajectories along with their
associated probabilities. Extensive experiments on the large-scale Argoverse
and nuScenes motion forecasting datasets demonstrate that our approach
significantly enhances trajectory prediction confidence, achieving highly
competitive performance relative to state-of-the-art methods.

</details>


### [94] [YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association](https://arxiv.org/abs/2507.12087)
*Xiang Yu,Xinyao Liu,Guang Liang*

Main category: cs.CV

TL;DR: A championship-winning solution for tracking small, agile multi-objects (SMOT) from UAVs, featuring innovations in detection (SliceTrain) and tracking (motion-based tracker), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in tracking small, agile objects like birds from UAVs due to scarce appearance features, complex motion, and occlusions.

Method: Combines SliceTrain for enhanced small-object detection and a motion-based tracker with EMA and adaptive similarity metrics.

Result: Achieves SO-HOTA score of 55.205 on SMOT4SB test set, validating framework effectiveness.

Conclusion: The proposed framework effectively tackles complex SMOT problems, with code available for public use.

Abstract: Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned
Aerial Vehicle (UAV) perspective is a highly challenging computer vision task.
The difficulty stems from three main sources: the extreme scarcity of target
appearance features, the complex motion entanglement caused by the combined
dynamics of the camera and the targets themselves, and the frequent occlusions
and identity ambiguity arising from dense flocking behavior. This paper details
our championship-winning solution in the MVA 2025 "Finding Birds" Small
Multi-Object Tracking Challenge (SMOT4SB), which adopts the
tracking-by-detection paradigm with targeted innovations at both the detection
and association levels. On the detection side, we propose a systematic training
enhancement framework named \textbf{SliceTrain}. This framework, through the
synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic
augmentation, effectively addresses the problem of insufficient learning for
small objects in high-resolution image training. On the tracking side, we
designed a robust tracker that is completely independent of appearance
information. By integrating a \textbf{motion direction maintenance (EMA)}
mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding
box expansion and distance penalty} into the OC-SORT framework, our tracker can
stably handle irregular motion and maintain target identities. Our method
achieves state-of-the-art performance on the SMOT4SB public test set, reaching
an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness
and advancement of our framework in solving complex real-world SMOT problems.
The source code will be made available at
https://github.com/Salvatore-Love/YOLOv8-SMOT.

</details>


### [95] [BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images](https://arxiv.org/abs/2507.12095)
*Davide Di Nucci,Matteo Tomei,Guido Borghi,Luca Ciuffreda,Roberto Vezzani,Rita Cucchiara*

Main category: cs.CV

TL;DR: The paper introduces a method for 3D vehicle reconstruction from sparse views, improving Gaussian Splatting with selective photometric loss and DUSt3R for pose estimation, and presents a new dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D vehicle reconstruction is crucial for applications like inspection and urban planning, but existing methods rely on dense views, limiting real-world use.

Method: Enhances Gaussian Splatting with selective photometric loss and DUSt3R for pose estimation, and introduces a synthetic and real-world vehicle dataset.

Result: Achieves state-of-the-art performance, enabling high-quality reconstructions from sparse inputs.

Conclusion: The proposed method effectively addresses sparse-view reconstruction challenges, demonstrating superior performance and practicality.

Abstract: Accurate 3D reconstruction of vehicles is vital for applications such as
vehicle inspection, predictive maintenance, and urban planning. Existing
methods like Neural Radiance Fields and Gaussian Splatting have shown
impressive results but remain limited by their reliance on dense input views,
which hinders real-world applicability. This paper addresses the challenge of
reconstructing vehicles from sparse-view inputs, leveraging depth maps and a
robust pose estimation architecture to synthesize novel views and augment
training data. Specifically, we enhance Gaussian Splatting by integrating a
selective photometric loss, applied only to high-confidence pixels, and
replacing standard Structure-from-Motion pipelines with the DUSt3R architecture
to improve camera pose estimation. Furthermore, we present a novel dataset
featuring both synthetic and real-world public transportation vehicles,
enabling extensive evaluation of our approach. Experimental results demonstrate
state-of-the-art performance across multiple benchmarks, showcasing the
method's ability to achieve high-quality reconstructions even under constrained
input conditions.

</details>


### [96] [DeepShade: Enable Shade Simulation by Text-conditioned Image Generation](https://arxiv.org/abs/2507.12103)
*Longchao Da,Xiangrui Liu,Mithun Shivakoti,Thirulogasankar Pranav Kutralingam,Yezhou Yang,Hua Wei*

Main category: cs.CV

TL;DR: The paper introduces DeepShade, a diffusion-based model for synthesizing shade variations, and a dataset for learning shade patterns to improve heatwave-aware routing systems.


<details>
  <summary>Details</summary>
Motivation: Current routing systems lack shade information due to noisy satellite imagery and limited training data, posing health risks during heatwaves.

Method: Built a dataset using Blender-based 3D simulations and proposed DeepShade, a diffusion model incorporating RGB and Canny edge layers with contrastive learning.

Result: DeepShade improves shade image generation, demonstrated by calculating shade ratios for route planning in Tempe, Arizona.

Conclusion: The work aids urban planning in extreme heat and has practical environmental applications.

Abstract: Heatwaves pose a significant threat to public health, especially as global
warming intensifies. However, current routing systems (e.g., online maps) fail
to incorporate shade information due to the difficulty of estimating shades
directly from noisy satellite imagery and the limited availability of training
data for generative models. In this paper, we address these challenges through
two main contributions. First, we build an extensive dataset covering diverse
longitude-latitude regions, varying levels of building density, and different
urban layouts. Leveraging Blender-based 3D simulations alongside building
outlines, we capture building shadows under various solar zenith angles
throughout the year and at different times of day. These simulated shadows are
aligned with satellite images, providing a rich resource for learning shade
patterns. Second, we propose the DeepShade, a diffusion-based model designed to
learn and synthesize shade variations over time. It emphasizes the nuance of
edge features by jointly considering RGB with the Canny edge layer, and
incorporates contrastive learning to capture the temporal change rules of
shade. Then, by conditioning on textual descriptions of known conditions (e.g.,
time of day, solar angles), our framework provides improved performance in
generating shade images. We demonstrate the utility of our approach by using
our shade predictions to calculate shade ratios for real-world route planning
in Tempe, Arizona. We believe this work will benefit society by providing a
reference for urban planning in extreme heat weather and its potential
practical applications in the environment.

</details>


### [97] [Out-of-distribution data supervision towards biomedical semantic segmentation](https://arxiv.org/abs/2507.12105)
*Yiquan Gao,Duohui Xu*

Main category: cs.CV

TL;DR: Med-OoD improves biomedical segmentation by leveraging Out-of-Distribution (OoD) data without external sources, feature regularization, or extra annotations, achieving notable performance gains.


<details>
  <summary>Details</summary>
Motivation: Address misclassification in biomedical segmentation due to limited/imperfect datasets by utilizing OoD data.

Method: Introduces OoD data supervision into fully-supervised segmentation without architectural changes or additional requirements.

Result: Significantly reduces pixel misclassification, improves performance on Lizard dataset, and achieves 76.1% mIoU using OoD data alone.

Conclusion: Med-OoD offers a novel, effective approach for segmentation, highlighting the potential of OoD data in medical imaging.

Abstract: Biomedical segmentation networks easily suffer from the unexpected
misclassification between foreground and background objects when learning on
limited and imperfect medical datasets. Inspired by the strong power of
Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric
framework, Med-OoD to address this issue by introducing OoD data supervision
into fully-supervised biomedical segmentation with none of the following needs:
(i) external data sources, (ii) feature regularization objectives, (iii)
additional annotations. Our method can be seamlessly integrated into
segmentation networks without any modification on the architectures. Extensive
experiments show that Med-OoD largely prevents various segmentation networks
from the pixel misclassification on medical images and achieves considerable
performance improvements on Lizard dataset. We also present an emerging
learning paradigm of training a medical segmentation network completely using
OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU
as test result. We hope this learning paradigm will attract people to rethink
the roles of OoD data. Code is made available at
https://github.com/StudioYG/Med-OoD.

</details>


### [98] [Non-Adaptive Adversarial Face Generation](https://arxiv.org/abs/2507.12107)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Minsu Kim,Jae Hong Seo*

Main category: cs.CV

TL;DR: A novel method for generating adversarial faces that impersonate a target identity in face recognition systems (FRSs) by leveraging structural characteristics of the FRS feature space, achieving high success with minimal queries.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks on FRSs threaten security and privacy, especially in identity verification. Current methods rely on iterative optimization or transferability, which are inefficient or impractical for commercial systems.

Method: Utilizes attributed subspheres (e.g., gender or race) in the FRS feature space to generate adversarial faces with minimal non-adaptive queries (100 images). Avoids iterative optimization or surrogate models.

Result: Achieves a 93% success rate against AWS's CompareFaces API with a single query, outperforming methods requiring repeated adaptive queries or perturbations.

Conclusion: The method is efficient, non-adaptive, and effective for generating adversarial faces, offering a practical threat to commercial FRSs while allowing control over high-level attributes.

Abstract: Adversarial attacks on face recognition systems (FRSs) pose serious security
and privacy threats, especially when these systems are used for identity
verification. In this paper, we propose a novel method for generating
adversarial faces-synthetic facial images that are visually distinct yet
recognized as a target identity by the FRS. Unlike iterative optimization-based
approaches (e.g., gradient descent or other iterative solvers), our method
leverages the structural characteristics of the FRS feature space. We figure
out that individuals sharing the same attribute (e.g., gender or race) form an
attributed subsphere. By utilizing such subspheres, our method achieves both
non-adaptiveness and a remarkably small number of queries. This eliminates the
need for relying on transferability and open-source surrogate models, which
have been a typical strategy when repeated adaptive queries to commercial FRSs
are impossible. Despite requiring only a single non-adaptive query consisting
of 100 face images, our method achieves a high success rate of over 93% against
AWS's CompareFaces API at its default threshold. Furthermore, unlike many
existing attacks that perturb a given image, our method can deliberately
produce adversarial faces that impersonate the target identity while exhibiting
high-level attributes chosen by the adversary.

</details>


### [99] [LidarPainter: One-Step Away From Any Lidar View To Novel Guidance](https://arxiv.org/abs/2507.12114)
*Yuzhou Ji,Ke Ma,Hong Cai,Anchun Zhang,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: LidarPainter is a real-time diffusion model for high-fidelity driving scene reconstruction, outperforming existing methods in speed, quality, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dynamic driving scene reconstruction suffer from inconsistency, deformation, and inefficiency when deviating from input trajectories.

Method: Proposes LidarPainter, a one-step diffusion model that recovers consistent views from sparse LiDAR data and corrupted renderings.

Result: LidarPainter is 7x faster than StreetCrafter, uses less GPU memory, and supports stylized generation via text prompts.

Conclusion: LidarPainter enables efficient, high-quality, and versatile driving scene reconstruction.

Abstract: Dynamic driving scene reconstruction is of great importance in fields like
digital twin system and autonomous driving simulation. However, unacceptable
degradation occurs when the view deviates from the input trajectory, leading to
corrupted background and vehicle models. To improve reconstruction quality on
novel trajectory, existing methods are subject to various limitations including
inconsistency, deformation, and time consumption. This paper proposes
LidarPainter, a one-step diffusion model that recovers consistent driving views
from sparse LiDAR condition and artifact-corrupted renderings in real-time,
enabling high-fidelity lane shifts in driving scene reconstruction. Extensive
experiments show that LidarPainter outperforms state-of-the-art methods in
speed, quality and resource efficiency, specifically 7 x faster than
StreetCrafter with only one fifth of GPU memory required. LidarPainter also
supports stylized generation using text prompts such as "foggy" and "night",
allowing for a diverse expansion of the existing asset library.

</details>


### [100] [Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph](https://arxiv.org/abs/2507.12123)
*Sergey Linok,Gleb Naumov*

Main category: cs.CV

TL;DR: OVIGo-3DHSG is a method for open-vocabulary indoor object grounding using a 3D hierarchical scene graph, integrating foundation models and LLMs for spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: To improve spatial understanding and object grounding in complex indoor environments by leveraging hierarchical representations and open-vocabulary models.

Method: Uses a 3D hierarchical scene graph derived from RGB-D frames, integrating foundation models and a Large Language Model for multistep reasoning.

Result: Demonstrates efficient scene comprehension and robust object grounding, outperforming existing methods.

Conclusion: OVIGo-3DHSG shows strong potential for applications requiring spatial reasoning in indoor environments.

Abstract: We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects
using 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor
environment over a Hierarchical Scene Graph derived from sequences of RGB-D
frames utilizing a set of open-vocabulary foundation models and sensor data
processing. The hierarchical representation explicitly models spatial relations
across floors, rooms, locations, and objects. To effectively address complex
queries involving spatial reference to other objects, we integrate the
hierarchical scene graph with a Large Language Model for multistep reasoning.
This integration leverages inter-layer (e.g., room-to-object) and intra-layer
(e.g., object-to-object) connections, enhancing spatial contextual
understanding. We investigate the semantic and geometry accuracy of
hierarchical representation on Habitat Matterport 3D Semantic multi-floor
scenes. Our approach demonstrates efficient scene comprehension and robust
object grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates
strong potential for applications requiring spatial reasoning and understanding
of indoor environments. Related materials can be found at
https://github.com/linukc/OVIGo-3DHSG.

</details>


### [101] [Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers](https://arxiv.org/abs/2507.12125)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin Li,Yu-Ming Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: BSPF-ViT introduces a block-based symmetric pruning and fusion method to jointly optimize Q/K token pruning in ViTs, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing ViT pruning methods degrade accuracy by independently pruning Q/K tokens and ignoring token interactions.

Method: BSPF-ViT evaluates token interactions, jointly prunes Q/K tokens, and compresses retained tokens via similarity fusion, leveraging symmetric attention matrices.

Result: BSPF-ViT outperforms state-of-the-art methods, increasing ImageNet accuracy by 1.3% (DeiT-T) and 2.0% (DeiT-S) while reducing computation by 50% and achieving 40% speedup.

Conclusion: BSPF-ViT effectively balances accuracy and efficiency by addressing token interaction in pruning, making ViTs more practical for real-world applications.

Abstract: Vision Transformer (ViT) has achieved impressive results across various
vision tasks, yet its high computational cost limits practical applications.
Recent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning
unimportant tokens. However, these techniques often sacrifice accuracy by
independently pruning query (Q) and key (K) tokens, leading to performance
degradation due to overlooked token interactions. To address this limitation,
we introduce a novel {\bf Block-based Symmetric Pruning and Fusion} for
efficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.
Unlike previous methods that consider only a single direction, our approach
evaluates each token and its neighbors to decide which tokens to retain by
taking token interaction into account. The retained tokens are compressed
through a similarity fusion step, preserving key information while reducing
computational costs. The shared weights of Q/K tokens create a symmetric
attention matrix, allowing pruning only the upper triangular part for speed up.
BSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning
levels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%
on DeiT-S, while reducing computational overhead by 50%. It achieves 40%
speedup with improved accuracy across various ViTs.

</details>


### [102] [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/abs/2507.12135)
*Junyu Lou,Xiaorui Zhao,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: A novel BPAM framework combines bilateral grids and MLPs for image enhancement, overcoming limitations of linear transformations and globally shared parameters, achieving superior performance in real-time.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited to linear transformations or globally shared MLP parameters, restricting their ability to model complex color relationships and localized variations.

Method: The BPAM framework integrates bilateral grids with MLPs, dynamically generating pixel-specific MLP parameters via spatial and intensity cues, and uses grid decomposition for efficient parameter utilization.

Result: The method outperforms state-of-the-art techniques on public datasets while maintaining real-time processing.

Conclusion: BPAM effectively addresses the dual challenges of non-linear color mapping and localized variations, offering a robust solution for image enhancement.

Abstract: Deep learning-based bilateral grid processing has emerged as a promising
solution for image enhancement, inherently encoding spatial and intensity
information while enabling efficient full-resolution processing through slicing
operations. However, existing approaches are limited to linear affine
transformations, hindering their ability to model complex color relationships.
Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,
traditional MLP-based methods employ globally shared parameters, which is hard
to deal with localized variations. To overcome these dual challenges, we
propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)
framework. Our approach synergizes the spatial modeling of bilateral grids with
the non-linear capabilities of MLPs. Specifically, we generate bilateral grids
containing MLP parameters, where each pixel dynamically retrieves its unique
transformation parameters and obtain a distinct MLP for color mapping based on
spatial coordinates and intensity values. In addition, we propose a novel grid
decomposition strategy that categorizes MLP parameters into distinct types
stored in separate subgrids. Multi-channel guidance maps are used to extract
category-specific parameters from corresponding subgrids, ensuring effective
utilization of color information during slicing while guiding precise parameter
generation. Extensive experiments on public datasets demonstrate that our
method outperforms state-of-the-art methods in performance while maintaining
real-time processing capabilities.

</details>


### [103] [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/abs/2507.12137)
*Jiawei Xu,Kai Deng,Zexin Fan,Shenlong Wang,Jin Xie,Jian Yang*

Main category: cs.CV

TL;DR: AD-GS is a self-supervised framework for high-quality free-viewpoint rendering of driving scenes, eliminating costly manual annotations by using a novel motion model and simplified pseudo 2D segmentation.


<details>
  <summary>Details</summary>
Motivation: Current methods for dynamic urban driving scenes rely on expensive manual annotations or suffer from inaccuracies in motion capture and scene decomposition. AD-GS aims to address these limitations.

Method: AD-GS uses a learnable motion model combining B-spline curves and trigonometric functions, along with pseudo 2D segmentation for automatic scene decomposition. It employs dynamic Gaussians and temporal visibility masks for object representation, enhanced by visibility reasoning and rigid regularization.

Result: AD-GS outperforms state-of-the-art annotation-free methods and competes with annotation-dependent approaches, demonstrating high-quality rendering.

Conclusion: AD-GS provides a robust, annotation-free solution for dynamic scene rendering, achieving competitive performance without manual labeling.

Abstract: Modeling and rendering dynamic urban driving scenes is crucial for
self-driving simulation. Current high-quality methods typically rely on costly
manual object tracklet annotations, while self-supervised approaches fail to
capture dynamic object motions accurately and decompose scenes properly,
resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised
framework for high-quality free-viewpoint rendering of driving scenes from a
single log. At its core is a novel learnable motion model that integrates
locality-aware B-spline curves with global-aware trigonometric functions,
enabling flexible yet precise dynamic object modeling. Rather than requiring
comprehensive semantic labeling, AD-GS automatically segments scenes into
objects and background with the simplified pseudo 2D segmentation, representing
objects using dynamic Gaussians and bidirectional temporal visibility masks.
Further, our model incorporates visibility reasoning and physically rigid
regularization to enhance robustness. Extensive evaluations demonstrate that
our annotation-free model significantly outperforms current state-of-the-art
annotation-free methods and is competitive with annotation-dependent
approaches.

</details>


### [104] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: A data-driven method using normalizing flows (RealNVP) to model a neural prior over human body poses in 6D rotation format, ensuring stability and compatibility.


<details>
  <summary>Details</summary>
Motivation: To address the lack of principled, flexible methods for modeling pose distributions in human motion capture and reconstruction.

Method: Uses RealNVP to learn a density over 6D rotations, inverting the Gram-Schmidt process for stable training while maintaining compatibility with rotation-based frameworks.

Result: Effective qualitative and quantitative performance, validated through ablation studies.

Conclusion: Provides a probabilistic foundation for integrating pose priors into motion capture and reconstruction pipelines.

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [105] [Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation](https://arxiv.org/abs/2507.12157)
*Edwin Arkel Rios,Fernando Mikael,Oswin Gosal,Femiloye Oyerinde,Hao-Chun Liang,Bo-Cheng Lai,Min-Chun Hu*

Main category: cs.CV

TL;DR: The paper introduces TGDA, a training framework for fine-grained image recognition (FGIR) that eliminates reliance on pretrained models, achieving high performance from scratch with task-specific architectures like LRNets and ViTFS.


<details>
  <summary>Details</summary>
Motivation: Current FGIR methods depend on pretrained models, limiting adaptability and task-specific design. The work aims to demonstrate that high-performance FGIR can be achieved without pretraining.

Method: TGDA integrates data-aware augmentation and weak supervision via a fine-grained-aware teacher model using knowledge distillation. It enables task-specific architectures (LRNets, ViTFS) optimized for efficiency.

Result: TGDA matches or outperforms pretrained models on FGIR benchmarks, with LRNets improving accuracy by 23% and reducing parameters by 20.6x. ViTFS-T matches ViT B-16 performance with 15.3x fewer parameters.

Conclusion: TGDA offers an adaptable alternative to pretraining, enabling efficient and task-specific FGIR systems without reliance on large-scale pretrained models.

Abstract: Fine-grained image recognition (FGIR) aims to distinguish visually similar
sub-categories within a broader class, such as identifying bird species. While
most existing FGIR methods rely on backbones pretrained on large-scale datasets
like ImageNet, this dependence limits adaptability to resource-constrained
environments and hinders the development of task-specific architectures
tailored to the unique challenges of FGIR.
  In this work, we challenge the conventional reliance on pretrained models by
demonstrating that high-performance FGIR systems can be trained entirely from
scratch. We introduce a novel training framework, TGDA, that integrates
data-aware augmentation with weak supervision via a fine-grained-aware teacher
model, implemented through knowledge distillation. This framework unlocks the
design of task-specific and hardware-aware architectures, including LRNets for
low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for
efficient inference.
  Extensive experiments across three FGIR benchmarks over diverse settings
involving low-resolution and high-resolution inputs show that our method
consistently matches or surpasses state-of-the-art pretrained counterparts. In
particular, in the low-resolution setting, LRNets trained with TGDA improve
accuracy by up to 23\% over prior methods while requiring up to 20.6x less
parameters, lower FLOPs, and significantly less training data. Similarly,
ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k
while using 15.3x fewer trainable parameters and requiring orders of magnitudes
less data. These results highlight TGDA's potential as an adaptable alternative
to pretraining, paving the way for more efficient fine-grained vision systems.

</details>


### [106] [Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification](https://arxiv.org/abs/2507.12177)
*Zahid Ullah,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: A double ensembling framework combining pre-trained DL models and fine-tuned ML classifiers improves brain tumor classification accuracy in MRI.


<details>
  <summary>Details</summary>
Motivation: Human evaluation of MRI for tumors is error-prone due to fatigue, expertise limits, and image detail. The study aims to enhance diagnostic precision.

Method: Uses ensembled pre-trained DL models for feature extraction and ensembled fine-tuned ML models for classification, with preprocessing, augmentation, and transfer learning.

Result: Feature and classifier fusion outperform state-of-the-art, with hyperparameter fine-tuning significantly boosting performance.

Conclusion: The proposed framework enhances brain tumor classification accuracy, validated by ablation studies.

Abstract: Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable
tool for detecting tumors due to its capability to produce detailed images that
reveal their presence. However, the accuracy of diagnosis can be compromised
when human specialists evaluate these images. Factors such as fatigue, limited
expertise, and insufficient image detail can lead to errors. For example, small
tumors might go unnoticed, or overlap with healthy brain regions could result
in misidentification. To address these challenges and enhance diagnostic
precision, this study proposes a novel double ensembling framework, consisting
of ensembled pre-trained deep learning (DL) models for feature extraction and
ensembled fine-tuned hyperparameter machine learning (ML) models to efficiently
classify brain tumors. Specifically, our method includes extensive
preprocessing and augmentation, transfer learning concepts by utilizing various
pre-trained deep convolutional neural networks and vision transformer networks
to extract deep features from brain MRI, and fine-tune hyperparameters of ML
classifiers. Our experiments utilized three different publicly available Kaggle
MRI brain tumor datasets to evaluate the pre-trained DL feature extractor
models, ML classifiers, and the effectiveness of an ensemble of deep features
along with an ensemble of ML classifiers for brain tumor classification. Our
results indicate that the proposed feature fusion and classifier fusion improve
upon the state of the art, with hyperparameter fine-tuning providing a
significant enhancement over the ensemble method. Additionally, we present an
ablation study to illustrate how each component contributes to accurate brain
tumor classification.

</details>


### [107] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Main category: cs.CV

TL;DR: The paper proposes a wavelet-based method for low-light stereo image enhancement, decoupling feature spaces into low-frequency (illumination) and high-frequency (texture) branches, with cross-view interaction and texture enhancement modules.


<details>
  <summary>Details</summary>
Motivation: Existing methods entangle degradation factors in a single latent space, leading to shortcut learning. Wavelet transform allows independent processing of low/high-frequency information, improving feature space decoupling.

Method: Uses wavelet transform for feature space decoupling: low-frequency branch for illumination, high-frequency branches for texture. Introduces HF-CIM for cross-view interaction and DTEM for texture enhancement.

Result: Significant advantages in light adjustment and high-frequency recovery, validated on real and synthetic images.

Conclusion: The method effectively enhances low-light stereo images by decoupling feature spaces and leveraging cross-view cues, with publicly available code and dataset.

Abstract: Low-light images suffer from complex degradation, and existing enhancement
methods often encode all degradation factors within a single latent space. This
leads to highly entangled features and strong black-box characteristics, making
the model prone to shortcut learning. To mitigate the above issues, this paper
proposes a wavelet-based low-light stereo image enhancement method with feature
space decoupling. Our insight comes from the following findings: (1) Wavelet
transform enables the independent processing of low-frequency and
high-frequency information. (2) Illumination adjustment can be achieved by
adjusting the low-frequency component of a low-light image, extracted through
multi-level wavelet decomposition. Thus, by using wavelet transform the feature
space is decomposed into a low-frequency branch for illumination adjustment and
multiple high-frequency branches for texture enhancement. Additionally, stereo
low-light image enhancement can extract useful cues from another view to
improve enhancement. To this end, we propose a novel high-frequency guided
cross-view interaction module (HF-CIM) that operates within high-frequency
branches rather than across the entire feature space, effectively extracting
valuable image details from the other view. Furthermore, to enhance the
high-frequency information, a detail and texture enhancement module (DTEM) is
proposed based on cross-attention mechanism. The model is trained on a dataset
consisting of images with uniform illumination and images with non-uniform
illumination. Experimental results on both real and synthetic images indicate
that our algorithm offers significant advantages in light adjustment while
effectively recovering high-frequency information. The code and dataset are
publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [108] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Main category: cs.CV

TL;DR: The paper introduces three advanced techniques—Fractal Convolution, Self-Sensitive Tile Filling (SSTF), and Super Resolution—for preserving Indian cultural treasures using AI and computer vision.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of preserving Indian monuments by integrating modern digitized approaches with traditional restoration methods.

Method: Proposes Fractal Convolution for architectural pattern segmentation, SSTF for terracotta temple restoration, and Super Resolution for image upscaling, alongside a novel MosaicSlice data augmentation technique.

Result: The methods enable detailed, authentic restoration with automation, balancing tradition and innovation.

Conclusion: The study advances cultural heritage preservation with efficient, high-quality solutions, maintaining aesthetic and historical integrity.

Abstract: Modern digitised approaches have dramatically changed the preservation and
restoration of cultural treasures, integrating computer scientists into
multidisciplinary projects with ease. Machine learning, deep learning, and
computer vision techniques have revolutionised developing sectors like 3D
reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and
image processing with the integration of computer scientists into
multidisciplinary initiatives. We suggest three cutting-edge techniques in
recognition of the special qualities of Indian monuments, which are famous for
their architectural skill and aesthetic appeal. First is the Fractal
Convolution methodology, a segmentation method based on image processing that
successfully reveals subtle architectural patterns within these irreplaceable
cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling
(SSTF) method created especially for West Bengal's mesmerising Bankura
Terracotta Temples with a brand-new data augmentation method called MosaicSlice
on the third. Furthermore, we delve deeper into the Super Resolution strategy
to upscale the images without losing significant amount of quality. Our methods
allow for the development of seamless region-filling and highly detailed tiles
while maintaining authenticity using a novel data augmentation strategy within
affordable costs introducing automation. By providing effective solutions that
preserve the delicate balance between tradition and innovation, this study
improves the subject and eventually ensures unrivalled efficiency and aesthetic
excellence in cultural heritage protection. The suggested approaches advance
the field into an era of unmatched efficiency and aesthetic quality while
carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [109] [RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models](https://arxiv.org/abs/2507.12201)
*Yiqi Tian,Pengfei Jin,Mingze Yuan,Na Li,Bo Zeng,Quanzheng Li*

Main category: cs.CV

TL;DR: RODS, a robust optimization-inspired diffusion sampler, reduces hallucinations in diffusion models by detecting and correcting high-risk steps using geometric cues, improving fidelity and robustness without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from hallucinations due to score approximation inaccuracies during sampling.

Method: RODS reinterprets diffusion sampling as optimization, using geometric cues to detect and correct high-risk steps, enforcing smoother trajectories and adaptive perturbations.

Result: RODS detects over 70% of hallucinations and corrects more than 25%, improving fidelity and robustness on datasets like AFHQv2, FFHQ, and 11k-hands.

Conclusion: RODS effectively reduces hallucinations in diffusion models without retraining or introducing new artifacts, enhancing sampling quality.

Abstract: Diffusion models have achieved state-of-the-art performance in generative
modeling, yet their sampling procedures remain vulnerable to hallucinations,
often stemming from inaccuracies in score approximation. In this work, we
reinterpret diffusion sampling through the lens of optimization and introduce
RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that
detects and corrects high-risk sampling steps using geometric cues from the
loss landscape. RODS enforces smoother sampling trajectories and adaptively
adjusts perturbations, reducing hallucinations without retraining and at
minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands
demonstrate that RODS improves both sampling fidelity and robustness, detecting
over 70% of hallucinated samples and correcting more than 25%, all while
avoiding the introduction of new artifacts.

</details>


### [110] [MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM](https://arxiv.org/abs/2507.12232)
*Tao Chen,Jingyi Zhang,Decheng Liu,Chunlei Peng*

Main category: cs.CV

TL;DR: The paper introduces DD-VQA+, an extended VQA dataset with richer attributes, and MGFFD-VLM, a novel forgery detection framework integrating hybrid LoRA and multi-granularity prompt learning to enhance VLMs for deepfake detection and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing methods for deepfake detection using VLMs lack face quality-related attributes and effective training strategies, limiting their performance and interpretability.

Method: The paper extends the VQA dataset to DD-VQA+ and proposes MGFFD-VLM, which uses an Attribute-Driven Hybrid LoRA Strategy, Multi-Granularity Prompt Learning, and Forgery-Aware Training Strategy with auxiliary losses.

Result: The framework achieves superior accuracy in forgery judgment and analysis, outperforming existing methods.

Conclusion: The proposed approach enhances VLMs for deepfake detection by improving interpretability and accuracy through advanced training strategies and richer datasets.

Abstract: Recent studies have utilized visual large language models (VLMs) to answer
not only "Is this face a forgery?" but also "Why is the face a forgery?" These
studies introduced forgery-related attributes, such as forgery location and
type, to construct deepfake VQA datasets and train VLMs, achieving high
accuracy while providing human-understandable explanatory text descriptions.
However, these methods still have limitations. For example, they do not fully
leverage face quality-related attributes, which are often abnormal in forged
faces, and they lack effective training strategies for forgery-aware VLMs. In
this paper, we extend the VQA dataset to create DD-VQA+, which features a
richer set of attributes and a more diverse range of samples. Furthermore, we
introduce a novel forgery detection framework, MGFFD-VLM, which integrates an
Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual
Large Language Models (VLMs). Additionally, our framework incorporates
Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By
transforming classification and forgery segmentation results into prompts, our
method not only improves forgery classification but also enhances
interpretability. To further boost detection performance, we design multiple
forgery-related auxiliary losses. Experimental results demonstrate that our
approach surpasses existing methods in both text-based forgery judgment and
analysis, achieving superior accuracy.

</details>


### [111] [Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models](https://arxiv.org/abs/2507.12236)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: Generative text-to-image diffusion models outperform discriminative methods for phrase grounding in medical imaging, especially when fine-tuned with domain-specific language models. A novel post-processing technique, Bimodal Bias Merging (BBM), further enhances accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve disease localization in medical imaging by leveraging generative models for phrase grounding, addressing limitations of current discriminative methods.

Method: Uses generative text-to-image diffusion models with cross-attention maps, fine-tuned with domain-specific language models (e.g., CXR-BERT), and introduces BBM for post-processing.

Result: Achieves superior zero-shot performance, doubling mIoU scores of discriminative methods. BBM further refines localization accuracy.

Conclusion: Generative models are more effective for phrase grounding in medical imaging, offering robust and interpretable solutions for clinical applications.

Abstract: Phrase grounding, i.e., mapping natural language phrases to specific image
regions, holds significant potential for disease localization in medical
imaging through clinical reports. While current state-of-the-art methods rely
on discriminative, self-supervised contrastive models, we demonstrate that
generative text-to-image diffusion models, leveraging cross-attention maps, can
achieve superior zero-shot phrase grounding performance. Contrary to prior
assumptions, we show that fine-tuning diffusion models with a frozen,
domain-specific language model, such as CXR-BERT, substantially outperforms
domain-agnostic counterparts. This setup achieves remarkable improvements, with
mIoU scores doubling those of current discriminative methods. These findings
highlight the underexplored potential of generative models for phrase grounding
tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),
a novel post-processing technique that aligns text and image biases to identify
regions of high certainty. BBM refines cross-attention maps, achieving even
greater localization accuracy. Our results establish generative approaches as a
more effective paradigm for phrase grounding in the medical imaging domain,
paving the way for more robust and interpretable applications in clinical
practice. The source code and model weights are available at
https://github.com/Felix-012/generate_to_ground.

</details>


### [112] [Calisthenics Skills Temporal Video Segmentation](https://arxiv.org/abs/2507.12245)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: The paper introduces a dataset for automating the temporal segmentation of static calisthenics skills from videos, proposing a baseline method and showing feasibility with room for improvement.


<details>
  <summary>Details</summary>
Motivation: To assist athletes and judges by automating the recognition and duration estimation of static calisthenics skills, a gap not addressed in existing video understanding literature.

Method: Creation of a dataset with annotated temporal segmentation of static calisthenics skills and a baseline approach for skill temporal segmentation.

Result: The baseline method demonstrates feasibility for the problem but indicates potential for further improvement.

Conclusion: The study provides a foundational step for automated tools in calisthenics, highlighting the need for future enhancements.

Abstract: Calisthenics is a fast-growing bodyweight discipline that consists of
different categories, one of which is focused on skills. Skills in calisthenics
encompass both static and dynamic elements performed by athletes. The
evaluation of static skills is based on their difficulty level and the duration
of the hold. Automated tools able to recognize isometric skills from a video by
segmenting them to estimate their duration would be desirable to assist
athletes in their training and judges during competitions. Although the video
understanding literature on action recognition through body pose analysis is
rich, no previous work has specifically addressed the problem of calisthenics
skill temporal video segmentation. This study aims to provide an initial step
towards the implementation of automated tools within the field of Calisthenics.
To advance knowledge in this context, we propose a dataset of video footage of
static calisthenics skills performed by athletes. Each video is annotated with
a temporal segmentation which determines the extent of each skill. We hence
report the results of a baseline approach to address the problem of skill
temporal segmentation on the proposed dataset. The results highlight the
feasibility of the proposed problem, while there is still room for improvement.

</details>


### [113] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida Nezović,Jalal Romano,Nada Marić,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: Comparative analysis of CNN implementations in Keras, PyTorch, and JAX for medical image classification using PathMNIST, evaluating training efficiency, accuracy, and inference speed.


<details>
  <summary>Details</summary>
Motivation: The comparative performance of deep learning frameworks (Keras, PyTorch, JAX) in medical imaging tasks is underexplored.

Method: Comprehensive analysis of CNN implementations across frameworks using the PathMNIST dataset, assessing training efficiency, accuracy, and inference speed.

Result: Findings reveal trade-offs between computational speed and model accuracy.

Conclusion: Provides insights for researchers and practitioners in medical image analysis on framework suitability for real-world applications.

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [114] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno Bücker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Main category: cs.CV

TL;DR: A deep learning model using early chest X-rays predicts moderate/severe bronchopulmonary dysplasia (BPD) in preterm infants, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Early BPD prognosis is crucial to avoid unnecessary interventions. Routine admission radiographs offer a non-invasive tool for prediction.

Method: Fine-tuned ResNet-50 on adult chest X-rays, used progressive layer freezing, CutMix augmentation, and linear probing.

Result: Achieved AUROC of 0.78, balanced accuracy of 0.69, and F1-score of 0.67, outperforming ImageNet initialization.

Conclusion: Domain-specific pretraining enables accurate BPD prediction from routine radiographs, feasible for clinical implementation.

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [115] [FADE: Adversarial Concept Erasure in Flow Models](https://arxiv.org/abs/2507.12283)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wang,Ze Niu,Dacheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: FADE is a novel concept erasure method for text-to-image diffusion models, ensuring privacy and fairness by removing specified concepts while preserving model fidelity.


<details>
  <summary>Details</summary>
Motivation: Address risks of privacy and fairness in diffusion models by preventing memorization of sensitive concepts or biases.

Method: Combines trajectory-aware fine-tuning with an adversarial objective to minimize mutual information between erased concepts and model outputs.

Result: Achieves state-of-the-art performance in concept removal, outperforming baselines like ESD, UCE, MACE, and ANT by 5-10% in efficacy and image quality.

Conclusion: FADE sets a new standard for safe and fair generative modeling by effectively unlearning specified concepts without full retraining.

Abstract: Diffusion models have demonstrated remarkable image generation capabilities,
but also pose risks in privacy and fairness by memorizing sensitive concepts or
perpetuating biases. We propose a novel \textbf{concept erasure} method for
text-to-image diffusion models, designed to remove specified concepts (e.g., a
private individual or a harmful stereotype) from the model's generative
repertoire. Our method, termed \textbf{FADE} (Fair Adversarial Diffusion
Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial
objective to ensure the concept is reliably removed while preserving overall
model fidelity. Theoretically, we prove a formal guarantee that our approach
minimizes the mutual information between the erased concept and the model's
outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable
Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,
explicit content, and style erasure tasks from MACE). FADE achieves
state-of-the-art concept removal performance, surpassing recent baselines like
ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.
Notably, FADE improves the harmonic mean of concept removal and fidelity by
5--10\% over the best prior method. We also conduct an ablation study to
validate each component of FADE, confirming that our adversarial and
trajectory-preserving objectives each contribute to its superior performance.
Our work sets a new standard for safe and fair generative modeling by
unlearning specified concepts without retraining from scratch.

</details>


### [116] [Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation](https://arxiv.org/abs/2507.12292)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: The paper proposes a direct method for calisthenics skill recognition using depth estimation and athlete patch retrieval, avoiding costly pose estimation. It achieves faster inference and higher accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional pose estimation methods for calisthenics skill recognition are computationally expensive and slow, limiting real-time and mobile applications.

Method: The approach uses Depth Anything V2 for depth estimation and YOLOv10 for athlete localization, segmenting the subject without pose estimation.

Result: The method outperforms skeleton-based approaches with 38.3x faster inference and improved accuracy (0.837 vs. 0.815).

Conclusion: The modular pipeline offers efficiency, speed, and flexibility for future enhancements and real-world applications.

Abstract: Calisthenics skill classification is the computer vision task of inferring
the skill performed by an athlete from images, enabling automatic performance
assessment and personalized analytics. Traditional methods for calisthenics
skill recognition are based on pose estimation methods to determine the
position of skeletal data from images, which is later fed to a classification
algorithm to infer the performed skill. Despite the progress in human pose
estimation algorithms, they still involve high computational costs, long
inference times, and complex setups, which limit the applicability of such
approaches in real-time applications or mobile devices. This work proposes a
direct approach to calisthenics skill recognition, which leverages depth
estimation and athlete patch retrieval to avoid the computationally expensive
human pose estimation module. Using Depth Anything V2 for depth estimation and
YOLOv10 for athlete localization, we segment the subject from the background
rather than relying on traditional pose estimation techniques. This strategy
increases efficiency, reduces inference time, and improves classification
accuracy. Our approach significantly outperforms skeleton-based methods,
achieving 38.3x faster inference with RGB image patches and improved
classification accuracy with depth patches (0.837 vs. 0.815). Beyond these
performance gains, the modular design of our pipeline allows for flexible
replacement of components, enabling future enhancements and adaptation to
real-world applications.

</details>


### [117] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: The paper argues that diffusion models' success is largely due to input conditioning and introduces Discrete Latent Code (DLC), a discrete token-based image representation. DLC improves sample fidelity, enables out-of-distribution generation, and supports text-to-image tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance diffusion models by developing an ideal conditioning representation that improves fidelity, ease of generation, and compositionality for novel samples.

Method: Introduces Discrete Latent Code (DLC), a sequence of discrete tokens derived from Simplicial Embeddings, trained with self-supervised learning. Evaluates DLC in diffusion models for image generation.

Result: DLC-based diffusion models achieve state-of-the-art unconditional image generation on ImageNet and enable coherent out-of-distribution samples. Text-to-image generation is also demonstrated.

Conclusion: DLCs significantly improve diffusion models by offering better fidelity, compositionality, and versatility, enabling novel applications like text-to-image generation.

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [118] [Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors](https://arxiv.org/abs/2507.12336)
*Subin Jeon,In Cho,Junyoung Hong,Seon Joo Kim*

Main category: cs.CV

TL;DR: KeyDiff3D is an unsupervised framework for monocular 3D keypoints estimation from single images, leveraging a pretrained multi-view diffusion model for supervision and feature extraction, enabling accurate 3D keypoints prediction and object manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing methods require expensive manual annotations or multi-view images; KeyDiff3D aims to achieve accurate 3D keypoints estimation using only single-view images by leveraging geometric priors from a diffusion model.

Method: The framework uses a pretrained multi-view diffusion model to generate multi-view images from a single image, providing 3D geometric cues. It also extracts 2D multi-view features and constructs 3D feature volumes from the diffusion model's intermediate representations.

Result: KeyDiff3D achieves accurate 3D keypoints estimation and enables manipulation of 3D objects, demonstrating effectiveness on datasets like Human3.6M and Stanford Dogs, with strong generalization.

Conclusion: KeyDiff3D offers a cost-effective, accurate, and generalizable solution for monocular 3D keypoints estimation and object manipulation, outperforming methods requiring expensive annotations or multi-view inputs.

Abstract: This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D
keypoints estimation that accurately predicts 3D keypoints from a single image.
While previous methods rely on manual annotations or calibrated multi-view
images, both of which are expensive to collect, our method enables monocular 3D
keypoints estimation using only a collection of single-view images. To achieve
this, we leverage powerful geometric priors embedded in a pretrained multi-view
diffusion model. In our framework, this model generates multi-view images from
a single image, serving as a supervision signal to provide 3D geometric cues to
our model. We also use the diffusion model as a powerful 2D multi-view feature
extractor and construct 3D feature volumes from its intermediate
representations. This transforms implicit 3D priors learned by the diffusion
model into explicit 3D features. Beyond accurate keypoints estimation, we
further introduce a pipeline that enables manipulation of 3D objects generated
by the diffusion model. Experimental results on diverse aspects and datasets,
including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain
datasets, highlight the effectiveness of our method in terms of accuracy,
generalization, and its ability to enable manipulation of 3D objects generated
by the diffusion model from a single image.

</details>


### [119] [Improving Lightweight Weed Detection via Knowledge Distillation](https://arxiv.org/abs/2507.12344)
*Ahmet Oğuz Saltık,Max Voigt,Sourav Modak,Mike Beckworth,Anthony Stein*

Main category: cs.CV

TL;DR: The paper explores Channel-wise Knowledge Distillation (CWD) and Masked Generative Distillation (MGD) to improve lightweight models for weed detection in precision agriculture, achieving notable accuracy improvements without added complexity.


<details>
  <summary>Details</summary>
Motivation: Accurate weed detection is crucial for precision agriculture, but deploying efficient models on resource-limited platforms is challenging, especially for visually similar weed species.

Method: The study uses YOLO11x as a teacher model and YOLO11n as a student, applying CWD and MGD to transfer knowledge. Experiments are conducted on a dataset of sugar beet crops and four weed types.

Result: CWD improves mAP50 by 2.5% and MGD by 1.9% over the baseline. The student model performs well on embedded devices like Jetson Orin Nano and Raspberry Pi 5.

Conclusion: CWD and MGD are effective for enhancing weed detection accuracy in real-time precision agriculture applications.

Abstract: Weed detection is a critical component of precision agriculture, facilitating
targeted herbicide application and reducing environmental impact. However,
deploying accurate object detection models on resource-limited platforms
remains challenging, particularly when differentiating visually similar weed
species commonly encountered in plant phenotyping applications. In this work,
we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative
Distillation (MGD) to enhance the performance of lightweight models for
real-time smart spraying systems. Utilizing YOLO11x as the teacher model and
YOLO11n as both reference and student, both CWD and MGD effectively transfer
knowledge from the teacher to the student model. Our experiments, conducted on
a real-world dataset comprising sugar beet crops and four weed types (Cirsium,
Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50
across all classes. The distilled CWD student model achieves a notable
improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without
increasing model complexity. Additionally, we validate real-time deployment
feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and
Raspberry Pi 5 embedded devices, performing five independent runs to evaluate
performance stability across random seeds. These findings confirm CWD and MGD
as an effective, efficient, and practical approach for improving deep
learning-based weed detection accuracy in precision agriculture and plant
phenotyping scenarios.

</details>


### [120] [Cluster Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2507.12359)
*Nikolaos Giakoumoglou,Tania Stathaki*

Main category: cs.CV

TL;DR: CueCo combines contrastive learning and clustering for unsupervised visual representation learning, achieving high accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance unsupervised learning by integrating contrastive learning and clustering for better feature separation and compactness.

Method: Uses two neural networks (query and key) with contrastive loss for separation and clustering for compactness.

Result: Achieves 91.40% (CIFAR-10), 68.56% (CIFAR-100), and 78.65% (ImageNet-100) accuracy.

Conclusion: CueCo advances unsupervised visual representation learning by merging contrastive and clustering methods.

Abstract: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised
visual representation learning that effectively combines the strengths of
contrastive learning and clustering methods. Inspired by recent advancements,
CueCo is designed to simultaneously scatter and align feature representations
within the feature space. This method utilizes two neural networks, a query and
a key, where the key network is updated through a slow-moving average of the
query outputs. CueCo employs a contrastive loss to push dissimilar features
apart, enhancing inter-class separation, and a clustering objective to pull
together features of the same cluster, promoting intra-class compactness. Our
method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on
CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18
backbone. By integrating contrastive learning with clustering, CueCo sets a new
direction for advancing unsupervised visual representation learning.

</details>


### [121] [Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.12382)
*Kaiwen Huang,Yi Zhou,Huazhu Fu,Yizhe Zhang,Chen Gong,Tao Zhou*

Main category: cs.CV

TL;DR: Proposes Text-SemiSeg, a text-driven multiplanar framework for semi-supervised medical image segmentation, enhancing visual features with textual data.


<details>
  <summary>Details</summary>
Motivation: High annotation costs in medical imaging and the underutilization of textual data for visual semantic enhancement in 3D tasks.

Method: Three modules: Text-enhanced Multiplanar Representation (TMR), Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation (DCA).

Result: Outperforms other methods on three public datasets by effectively integrating textual information.

Conclusion: Text-SemiSeg successfully leverages textual data to improve semi-supervised segmentation in medical imaging.

Abstract: Semi-supervised medical image segmentation is a crucial technique for
alleviating the high cost of data annotation. When labeled data is limited,
textual information can provide additional context to enhance visual semantic
understanding. However, research exploring the use of textual data to enhance
visual semantic embeddings in 3D medical imaging tasks remains scarce. In this
paper, we propose a novel text-driven multiplanar visual interaction framework
for semi-supervised medical image segmentation (termed Text-SemiSeg), which
consists of three main modules: Text-enhanced Multiplanar Representation (TMR),
Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation
(DCA). Specifically, TMR facilitates text-visual interaction through planar
mapping, thereby enhancing the category awareness of visual features. CSA
performs cross-modal semantic alignment between the text features with
introduced learnable variables and the intermediate layer of visual features.
DCA reduces the distribution discrepancy between labeled and unlabeled data
through their interaction, thus improving the model's robustness. Finally,
experiments on three public datasets demonstrate that our model effectively
enhances visual features with textual information and outperforms other
methods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.

</details>


### [122] [OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments](https://arxiv.org/abs/2507.12396)
*Hayat Ullah,Abbas Khan,Arslan Munir,Hari Kalva*

Main category: cs.CV

TL;DR: The paper introduces two human surveillance benchmarks, OD-VIRAT Large and Tiny, with rich annotations, to evaluate state-of-the-art object detection models under challenging real-world conditions.


<details>
  <summary>Details</summary>
Motivation: To advance robust human and object detection in surveillance by providing diverse, realistic datasets for comprehensive model evaluation.

Method: Creation of two benchmarks (OD-VIRAT Large and Tiny) with annotated instances, followed by benchmarking models like RETMDET, YOLOX, and DETR on these datasets.

Result: OD-VIRAT Large has 8.7M annotations in 599,996 images; Tiny has 288,901 annotations in 19,860 images. Performance insights on models under challenging conditions are provided.

Conclusion: The benchmarks and experiments set a foundation for developing more efficient and robust object detection architectures in surveillance.

Abstract: Realistic human surveillance datasets are crucial for training and evaluating
computer vision models under real-world conditions, facilitating the
development of robust algorithms for human and human-interacting object
detection in complex environments. These datasets need to offer diverse and
challenging data to enable a comprehensive assessment of model performance and
the creation of more reliable surveillance systems for public safety. To this
end, we present two visual object detection benchmarks named OD-VIRAT Large and
OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance
imagery. The video sequences in both benchmarks cover 10 different scenes of
human surveillance recorded from significant height and distance. The proposed
benchmarks offer rich annotations of bounding boxes and categories, where
OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and
OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also
focuses on benchmarking state-of-the-art object detection architectures,
including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object
detection-specific variant of VIRAT dataset. To the best of our knowledge, it
is the first work to examine the performance of these recently published
state-of-the-art object detection architectures on realistic surveillance
imagery under challenging conditions such as complex backgrounds, occluded
objects, and small-scale objects. The proposed benchmarking and experimental
settings will help in providing insights concerning the performance of selected
object detection models and set the base for developing more efficient and
robust object detection architectures.

</details>


### [123] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Main category: cs.CV

TL;DR: AutoVDC uses Vision-Language Models (VLMs) to automatically detect and clean erroneous annotations in vision datasets for autonomous driving, improving data quality.


<details>
  <summary>Details</summary>
Motivation: Human annotations are imperfect and costly to review manually; AutoVDC aims to automate this process.

Method: AutoVDC employs VLMs to identify errors in datasets like KITTI and nuImages, testing with injected errors and comparing VLM performance.

Result: High error detection rates show AutoVDC's effectiveness in improving dataset reliability.

Conclusion: AutoVDC can enhance large-scale dataset accuracy for autonomous driving systems.

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [124] [QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval](https://arxiv.org/abs/2507.12416)
*Jaehyun Kwak,Ramahdani Muhammad Izaaz Inhar,Se-Young Yun,Sung-Ju Lee*

Main category: cs.CV

TL;DR: QuRe improves Composed Image Retrieval (CIR) by addressing false negatives in contrastive learning, using hard negative sampling and a reward model, and introduces HP-FashionIQ for human preference evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods ignore the relevance of non-target images, leading to false negatives and reduced user satisfaction.

Method: QuRe employs a reward model objective and hard negative sampling to filter false negatives, and introduces HP-FashionIQ for human-aligned evaluation.

Result: QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets and aligns best with human preferences on HP-FashionIQ.

Conclusion: QuRe effectively addresses false negatives in CIR, improving retrieval relevance and user satisfaction.

Abstract: Composed Image Retrieval (CIR) retrieves relevant images based on a reference
image and accompanying text describing desired modifications. However, existing
CIR methods only focus on retrieving the target image and disregard the
relevance of other images. This limitation arises because most methods
employing contrastive learning-which treats the target image as positive and
all other images in the batch as negatives-can inadvertently include false
negatives. This may result in retrieving irrelevant images, reducing user
satisfaction even when the target image is retrieved. To address this issue, we
propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which
optimizes a reward model objective to reduce false negatives. Additionally, we
introduce a hard negative sampling strategy that selects images positioned
between two steep drops in relevance scores following the target image, to
effectively filter false negatives. In order to evaluate CIR models on their
alignment with human satisfaction, we create Human-Preference FashionIQ
(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond
target retrieval. Extensive experiments demonstrate that QuRe achieves
state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting
the strongest alignment with human preferences on the HP-FashionIQ dataset. The
source code is available at https://github.com/jackwaky/QuRe.

</details>


### [125] [InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization](https://arxiv.org/abs/2507.12420)
*Haoyuan Liu,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: InterpIoU, a novel loss function, replaces handcrafted geometric penalties in bounding box regression with interpolated box IoU, improving small object detection and avoiding box enlargement issues.


<details>
  <summary>Details</summary>
Motivation: Existing IoU-based losses with geometric penalties are sensitive to box attributes, leading to suboptimal performance for small objects and misalignment issues.

Method: Proposes InterpIoU, using interpolated boxes for meaningful gradients, and Dynamic InterpIoU, which adjusts interpolation coefficients dynamically.

Result: Outperforms state-of-the-art IoU-based losses on COCO, VisDrone, and PASCAL VOC, especially in small object detection.

Conclusion: InterpIoU and Dynamic InterpIoU effectively address limitations of existing losses, enhancing bounding box regression performance.

Abstract: Bounding box regression (BBR) is fundamental to object detection, where the
regression loss is crucial for accurate localization. Existing IoU-based losses
often incorporate handcrafted geometric penalties to address IoU's
non-differentiability in non-overlapping cases and enhance BBR performance.
However, these penalties are sensitive to box shape, size, and distribution,
often leading to suboptimal optimization for small objects and undesired
behaviors such as bounding box enlargement due to misalignment with the IoU
objective. To address these limitations, we propose InterpIoU, a novel loss
function that replaces handcrafted geometric penalties with a term based on the
IoU between interpolated boxes and the target. By using interpolated boxes to
bridge the gap between predictions and ground truth, InterpIoU provides
meaningful gradients in non-overlapping cases and inherently avoids the box
enlargement issue caused by misaligned penalties. Simulation results further
show that IoU itself serves as an ideal regression target, while existing
geometric penalties are both unnecessary and suboptimal. Building on InterpIoU,
we introduce Dynamic InterpIoU, which dynamically adjusts interpolation
coefficients based on IoU values, enhancing adaptability to scenarios with
diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC
show that our methods consistently outperform state-of-the-art IoU-based losses
across various detection frameworks, with particularly notable improvements in
small object detection, confirming their effectiveness.

</details>


### [126] [DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition](https://arxiv.org/abs/2507.12426)
*Hayat Ullah,Muhammad Ali Shafique,Abbas Khan,Arslan Munir*

Main category: cs.CV

TL;DR: The paper introduces DVFL-Net, a lightweight Video Focal Modulation Network, to address the computational inefficiency of Transformer-based video recognition models. It uses knowledge distillation and spatio-temporal feature modulation for efficient on-device deployment while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformers, though state-of-the-art in video recognition, are computationally expensive. The goal is to create a lightweight model (DVFL-Net) that balances performance and efficiency for real-time applications.

Method: DVFL-Net employs knowledge distillation and spatial-temporal focal modulation to transfer knowledge from a large teacher model (Video-FocalNet Base) to a compact student model (VFL-Net). Forward KL divergence is used for effective context transfer.

Result: DVFL-Net achieves strong accuracy on benchmarks (UCF50, UCF101, HMDB51, SSV2, Kinetics-400) with lower memory usage and reduced GFLOPs, outperforming recent HAR methods.

Conclusion: DVFL-Net offers a practical solution for real-time Human Action Recognition by balancing efficiency and performance, making it suitable for on-device deployment.

Abstract: The landscape of video recognition has evolved significantly, shifting from
traditional Convolutional Neural Networks (CNNs) to Transformer-based
architectures for improved accuracy. While 3D CNNs have been effective at
capturing spatiotemporal dynamics, recent Transformer models leverage
self-attention to model long-range spatial and temporal dependencies. Despite
achieving state-of-the-art performance on major benchmarks, Transformers remain
computationally expensive, particularly with dense video data. To address this,
we propose a lightweight Video Focal Modulation Network, DVFL-Net, which
distills spatiotemporal knowledge from a large pre-trained teacher into a
compact nano student model, enabling efficient on-device deployment. DVFL-Net
utilizes knowledge distillation and spatial-temporal feature modulation to
significantly reduce computation while preserving high recognition performance.
We employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal
focal modulation to effectively transfer both local and global context from the
Video-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate
DVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it
against recent state-of-the-art methods in Human Action Recognition (HAR).
Additionally, we conduct a detailed ablation study analyzing the impact of
forward KL divergence. The results confirm the superiority of DVFL-Net in
achieving an optimal balance between performance and efficiency, demonstrating
lower memory usage, reduced GFLOPs, and strong accuracy, making it a practical
solution for real-time HAR applications.

</details>


### [127] [Traffic-Aware Pedestrian Intention Prediction](https://arxiv.org/abs/2507.12433)
*Fahimeh Orvati Nia,Hai Lin*

Main category: cs.CV

TL;DR: A Traffic-Aware Spatio-Temporal Graph Convolutional Network (TA-STGCN) improves pedestrian intention prediction by integrating dynamic traffic signals and contextual scene information, achieving 4.75% higher accuracy than baseline models.


<details>
  <summary>Details</summary>
Motivation: Current pedestrian intention estimation models lack consideration of dynamic traffic signals and contextual scene information, which are critical for real-world autonomous vehicle navigation.

Method: The paper proposes TA-STGCN, which integrates traffic signal states (Red, Yellow, Green) and bounding box size as key features to capture spatial and temporal dependencies in urban environments.

Result: TA-STGCN achieves a 4.75% higher accuracy compared to baseline models on the PIE dataset.

Conclusion: The model demonstrates effectiveness in improving pedestrian intention prediction by incorporating dynamic traffic signals and contextual information.

Abstract: Accurate pedestrian intention estimation is crucial for the safe navigation
of autonomous vehicles (AVs) and hence attracts a lot of research attention.
However, current models often fail to adequately consider dynamic traffic
signals and contextual scene information, which are critical for real-world
applications. This paper presents a Traffic-Aware Spatio-Temporal Graph
Convolutional Network (TA-STGCN) that integrates traffic signs and their states
(Red, Yellow, Green) into pedestrian intention prediction. Our approach
introduces the integration of dynamic traffic signal states and bounding box
size as key features, allowing the model to capture both spatial and temporal
dependencies in complex urban environments. The model surpasses existing
methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy
compared to the baseline model on the PIE dataset, demonstrating its
effectiveness in improving pedestrian intention prediction.

</details>


### [128] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DAM-QA leverages region-aware vision-language modeling (DAM) to improve Visual Question Answering (VQA) in text-rich images, outperforming baselines and narrowing the gap with generalist models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of text-rich VQA by utilizing DAM's region-level descriptive capabilities for fine-grained textual information extraction.

Method: Introduces DAM-QA, a framework that aggregates answers from multiple regional views of images, enhancing evidence identification for text-related elements.

Result: Outperforms baseline DAM by 7+ points on DocVQA and achieves top performance among region-aware models with fewer parameters.

Conclusion: DAM-like models show promise for text-rich VQA tasks when paired with efficient integration strategies.

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [129] [Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios](https://arxiv.org/abs/2507.12449)
*Van-Hoang-Anh Phan,Chi-Tam Nguyen,Doan-Trung Au,Thanh-Danh Phan,Minh-Thien Duong,My-Ha Le*

Main category: cs.CV

TL;DR: Proposes an efficient obstacle avoidance pipeline for autonomous vehicles using camera-only perception and Frenet-Pure Pursuit planning, integrating YOLOv11 and Depth Anything V2 for object detection and depth estimation.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety of autonomous vehicles by improving obstacle avoidance in complex environments.

Method: Combines camera-only perception (YOLOv11 for detection, Depth Anything V2 for depth estimation) with Frenet-Pure Pursuit planning.

Result: Effective handling of diverse obstacles in real-world campus scenarios, demonstrated via experiments.

Conclusion: The pipeline enhances autonomous navigation by leveraging advanced computer vision and planning strategies.

Abstract: Obstacle avoidance is essential for ensuring the safety of autonomous
vehicles. Accurate perception and motion planning are crucial to enabling
vehicles to navigate complex environments while avoiding collisions. In this
paper, we propose an efficient obstacle avoidance pipeline that leverages a
camera-only perception module and a Frenet-Pure Pursuit-based planning
strategy. By integrating advancements in computer vision, the system utilizes
YOLOv11 for object detection and state-of-the-art monocular depth estimation
models, such as Depth Anything V2, to estimate object distances. A comparative
analysis of these models provides valuable insights into their accuracy,
efficiency, and robustness in real-world conditions. The system is evaluated in
diverse scenarios on a university campus, demonstrating its effectiveness in
handling various obstacles and enhancing autonomous navigation. The video
presenting the results of the obstacle avoidance experiments is available at:
https://www.youtube.com/watch?v=FoXiO5S_tA8

</details>


### [130] [Mitigating Object Hallucinations via Sentence-Level Early Intervention](https://arxiv.org/abs/2507.12455)
*Shangpin Peng,Senqiao Yang,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: SENTINEL is a framework to reduce hallucinations in MLLMs by early intervention at the sentence level using in-domain preference learning, achieving a 90% reduction in hallucinations.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with hallucinations, and existing mitigation methods are costly or introduce mismatches. Hallucinations often start early in text generation.

Method: Bootstraps high-quality preference pairs, validates object existence, and trains models with context-aware preference loss (C-DPO).

Result: Reduces hallucinations by over 90%, outperforming state-of-the-art methods on benchmarks.

Conclusion: SENTINEL effectively mitigates hallucinations while maintaining general capabilities, with resources made publicly available.

Abstract: Multimodal large language models (MLLMs) have revolutionized cross-modal
understanding but continue to struggle with hallucinations - fabricated content
contradicting visual inputs. Existing hallucination mitigation methods either
incur prohibitive computational costs or introduce distribution mismatches
between training data and model outputs. We identify a critical insight:
hallucinations predominantly emerge at the early stages of text generation and
propagate through subsequent outputs. To address this, we propose **SENTINEL**
(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain
pr**E**ference **L**earning), a framework that eliminates dependency on human
annotations. Specifically, we first bootstrap high-quality in-domain preference
pairs by iteratively sampling model outputs, validating object existence
through cross-checking with two open-vocabulary detectors, and classifying
sentences into hallucinated/non-hallucinated categories. Subsequently, we use
context-coherent positive samples and hallucinated negative samples to build
context-aware preference data iteratively. Finally, we train models using a
context-aware preference loss (C-DPO) that emphasizes discriminative learning
at the sentence level where hallucinations initially manifest. Experimental
results show that SENTINEL can reduce hallucinations by over 90\% compared to
the original model and outperforms the previous state-of-the-art method on both
hallucination benchmarks and general capabilities benchmarks, demonstrating its
superiority and generalization ability. The models, datasets, and code are
available at https://github.com/pspdada/SENTINEL.

</details>


### [131] [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](https://arxiv.org/abs/2507.12461)
*Trong-Thang Pham,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: RadGazeIntent, a deep learning model, predicts radiologists' diagnostic intent from gaze data, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to capture the intent behind radiologists' gaze fixations during medical image analysis.

Method: A transformer-based architecture processes temporal and spatial gaze data to model diagnostic intent, using labeled datasets (RadSeq, RadExplore, RadHybrid).

Result: RadGazeIntent outperforms baselines in predicting radiologists' focus on specific findings.

Conclusion: The model successfully interprets radiologists' gaze intent, aiding in understanding their diagnostic process.

Abstract: Radiologists rely on eye movements to navigate and interpret medical images.
A trained radiologist possesses knowledge about the potential diseases that may
be present in the images and, when searching, follows a mental checklist to
locate them using their gaze. This is a key observation, yet existing models
fail to capture the underlying intent behind each fixation. In this paper, we
introduce a deep learning-based approach, RadGazeIntent, designed to model this
behavior: having an intention to find something and actively searching for it.
Our transformer-based architecture processes both the temporal and spatial
dimensions of gaze data, transforming fine-grained fixation features into
coarse, meaningful representations of diagnostic intent to interpret
radiologists' goals. To capture the nuances of radiologists' varied
intention-driven behaviors, we process existing medical eye-tracking datasets
to create three intention-labeled subsets: RadSeq (Systematic Sequential
Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid
Pattern). Experimental results demonstrate RadGazeIntent's ability to predict
which findings radiologists are examining at specific moments, outperforming
baseline methods across all intention-labeled datasets.

</details>


### [132] [SpatialTrackerV2: 3D Point Tracking Made Easy](https://arxiv.org/abs/2507.12462)
*Yuxi Xiao,Jianyuan Wang,Nan Xue,Nikita Karaev,Yuri Makarov,Bingyi Kang,Xing Zhu,Hujun Bao,Yujun Shen,Xiaowei Zhou*

Main category: cs.CV

TL;DR: SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos, unifying point tracking, depth, and camera pose estimation into a single, high-performing system.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of modular pipelines and leverage intrinsic connections between 3D tracking components for better performance.

Method: Decomposes 3D motion into scene geometry, camera ego-motion, and object motion using a fully differentiable, end-to-end architecture trained on diverse datasets.

Result: Outperforms existing 3D tracking methods by 30% and matches dynamic 3D reconstruction accuracy while running 50x faster.

Conclusion: SpatialTrackerV2 demonstrates superior performance and efficiency by jointly learning geometry and motion from heterogeneous data.

Abstract: We present SpatialTrackerV2, a feed-forward 3D point tracking method for
monocular videos. Going beyond modular pipelines built on off-the-shelf
components for 3D tracking, our approach unifies the intrinsic connections
between point tracking, monocular depth, and camera pose estimation into a
high-performing and feedforward 3D point tracker. It decomposes world-space 3D
motion into scene geometry, camera ego-motion, and pixel-wise object motion,
with a fully differentiable and end-to-end architecture, allowing scalable
training across a wide range of datasets, including synthetic sequences, posed
RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and
motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms
existing 3D tracking methods by 30%, and matches the accuracy of leading
dynamic 3D reconstruction approaches while running 50$\times$ faster.

</details>


### [133] [MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding](https://arxiv.org/abs/2507.12463)
*Renjie Li,Ruijie Ye,Mingyang Wu,Hao Frank Yang,Zhiwen Fan,Hezhen Hu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: The paper introduces MMHU, a large-scale benchmark for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources.


<details>
  <summary>Details</summary>
Motivation: Understanding human behavior is crucial for safe driving systems, but a comprehensive benchmark for evaluating such understanding is lacking.

Method: The authors propose MMHU, a dataset with 57k human motion clips and 1.73M frames, annotated via a human-in-the-loop pipeline, and benchmark multiple tasks like motion prediction and behavior QA.

Result: The dataset includes diverse sources (Waymo, YouTube, self-collected) and offers a broad evaluation suite for human behavior analysis.

Conclusion: MMHU fills a gap in benchmarking human behavior understanding for autonomous driving, providing a rich, diverse, and annotated dataset for future research.

Abstract: Humans are integral components of the transportation ecosystem, and
understanding their behaviors is crucial to facilitating the development of
safe driving systems. Although recent progress has explored various aspects of
human behavior$\unicode{x2014}$such as motion, trajectories, and
intention$\unicode{x2014}$a comprehensive benchmark for evaluating human
behavior understanding in autonomous driving remains unavailable. In this work,
we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis
featuring rich annotations, such as human motion and trajectories, text
description for human motions, human intention, and critical behavior labels
relevant to driving safety. Our dataset encompasses 57k human motion clips and
1.73M frames gathered from diverse sources, including established driving
datasets such as Waymo, in-the-wild videos from YouTube, and self-collected
data. A human-in-the-loop annotation pipeline is developed to generate rich
behavior captions. We provide a thorough dataset analysis and benchmark
multiple tasks$\unicode{x2014}$ranging from motion prediction to motion
generation and human behavior question answering$\unicode{x2014}$thereby
offering a broad evaluation suite. Project page :
https://MMHU-Benchmark.github.io.

</details>


### [134] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. Götze,Carsten Marr,Steffen Schneider*

Main category: cs.CV

TL;DR: CytoSAE, a sparse autoencoder, is applied to hematology for explainable AI in medical imaging, identifying disease-specific concepts validated by experts and matching state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Lack of tools for explaining inferences in medical imaging foundation models, despite their growing use.

Method: CytoSAE is trained on 40,000+ blood cell images and tested on diverse datasets, including bone marrow cytology.

Result: Identifies morphologically relevant concepts, validated by experts, and achieves state-of-the-art performance in AML subtype classification.

Conclusion: CytoSAE provides explainable AI for medical imaging, bridging the gap between performance and interpretability.

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


### [135] [PhysX: Physical-Grounded 3D Asset Generation](https://arxiv.org/abs/2507.12465)
*Ziang Cao,Zhaoxi Chen,Linag Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces PhysX, a framework for physical-grounded 3D asset generation, addressing the lack of physics-aware 3D models. It includes PhysXNet, a physics-annotated dataset, and PhysXGen, an image-to-3D generation model integrating physical properties.


<details>
  <summary>Details</summary>
Motivation: Current 3D generative models focus on geometry and textures but neglect physical properties, limiting real-world applications like simulation and embodied AI.

Method: 1) PhysXNet: A physics-grounded 3D dataset annotated across five dimensions (scale, material, affordance, kinematics, function) using a scalable human-in-the-loop pipeline. 2) PhysXGen: A dual-branch framework injecting physical knowledge into pre-trained 3D structural space for physics-aware generation.

Result: Extensive experiments show superior performance and generalization of PhysXGen, producing 3D assets with plausible physical predictions while maintaining geometry quality.

Conclusion: PhysX advances physical-grounded 3D generation, with released code, data, and models to support future research in generative physical AI.

Abstract: 3D modeling is moving from virtual to physical. Existing 3D generation
primarily emphasizes geometries and textures while neglecting physical-grounded
modeling. Consequently, despite the rapid development of 3D generative models,
the synthesized 3D assets often overlook rich and important physical
properties, hampering their real-world application in physical domains like
simulation and embodied AI. As an initial attempt to address this challenge, we
propose \textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset
generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we
present PhysXNet - the first physics-grounded 3D dataset systematically
annotated across five foundational dimensions: absolute scale, material,
affordance, kinematics, and function description. In particular, we devise a
scalable human-in-the-loop annotation pipeline based on vision-language models,
which enables efficient creation of physics-first assets from raw 3D assets.2)
Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for
physics-grounded image-to-3D asset generation, injecting physical knowledge
into the pre-trained 3D structural space. Specifically, PhysXGen employs a
dual-branch architecture to explicitly model the latent correlations between 3D
structures and physical properties, thereby producing 3D assets with plausible
physical predictions while preserving the native geometry quality. Extensive
experiments validate the superior performance and promising generalization
capability of our framework. All the code, data, and models will be released to
facilitate future research in generative physical AI.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [136] [FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization](https://arxiv.org/abs/2507.12366)
*Yifei Zhou,Xuchu Huang,Chenyu Ni,Min Zhou,Zheyu Yan,Xunzhao Yin,Cheng Zhuo*

Main category: cs.SC

TL;DR: FactorHD is a novel HDC model for efficiently representing and factorizing complex class-subclass relations, outperforming existing models in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing HDC models struggle with representing and factorizing class-subclass relations, a key task for neuro-symbolic AI.

Method: FactorHD uses symbolic encoding with an extra memorization clause and an efficient factorization algorithm to eliminate redundant classes.

Result: FactorHD achieves a 5667x speedup at large representation sizes and 92.48% factorization accuracy with ResNet-18 on Cifar-10.

Conclusion: FactorHD effectively addresses limitations of existing HDC models, enhancing efficiency and accuracy in neuro-symbolic AI tasks.

Abstract: Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical
analysis and reasoning. Hyperdimensional Computing (HDC), a promising
brain-inspired computational model, is integral to neuro-symbolic AI. Various
HDC models have been proposed to represent class-instance and class-class
relations, but when representing the more complex class-subclass relation,
where multiple objects associate different levels of classes and subclasses,
they face challenges for factorization, a crucial task for neuro-symbolic AI
systems. In this article, we propose FactorHD, a novel HDC model capable of
representing and factorizing the complex class-subclass relation efficiently.
FactorHD features a symbolic encoding method that embeds an extra memorization
clause, preserving more information for multiple objects. In addition, it
employs an efficient factorization algorithm that selectively eliminates
redundant classes by identifying the memorization clause of the target class.
Such model significantly enhances computing efficiency and accuracy in
representing and factorizing multiple objects with class-subclass relation,
overcoming limitations of existing HDC models such as "superposition
catastrophe" and "the problem of 2". Evaluations show that FactorHD achieves
approximately 5667x speedup at a representation size of 10^9 compared to
existing HDC models. When integrated with the ResNet-18 neural network,
FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [137] [Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening](https://arxiv.org/abs/2507.11548)
*Kevin T Webster*

Main category: cs.CY

TL;DR: The study audits AI resume screening tools, revealing biases and incompetence masked as neutrality, and proposes a dual-validation framework for fairness and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that AI resume screening is unbiased and competent, highlighting potential flaws in AI evaluative tasks.

Method: A two-part audit of eight AI platforms: Experiment 1 tested for racial and gender biases, while Experiment 2 evaluated core competence.

Result: Found biases in some models and superficial keyword matching in others, coining the "Illusion of Neutrality" for models appearing unbiased due to incompetence.

Conclusion: Recommends a dual-validation framework to audit AI tools for both bias and competence to ensure equitable and effective hiring.

Abstract: The increasing use of generative AI for resume screening is predicated on the
assumption that it offers an unbiased alternative to biased human
decision-making. However, this belief fails to address a critical question: are
these AI systems fundamentally competent at the evaluative tasks they are meant
to perform? This study investigates the question of competence through a
two-part audit of eight major AI platforms. Experiment 1 confirmed complex,
contextual racial and gender biases, with some models penalizing candidates
merely for the presence of demographic signals. Experiment 2, which evaluated
core competence, provided a critical insight: some models that appeared
unbiased were, in fact, incapable of performing a substantive evaluation,
relying instead on superficial keyword matching. This paper introduces the
"Illusion of Neutrality" to describe this phenomenon, where an apparent lack of
bias is merely a symptom of a model's inability to make meaningful judgments.
This study recommends that organizations and regulators adopt a dual-validation
framework, auditing AI hiring tools for both demographic bias and demonstrable
competence to ensure they are both equitable and effective.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [138] [Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers](https://arxiv.org/abs/2507.11852)
*Mohammed Hassanin,Mohammad Abu Alsheikh,Carlos C. N. Kuhn,Damith Herath,Dinh Thai Hoang,Ibrahim Radwan*

Main category: cs.RO

TL;DR: The paper reviews autonomous riding (AR) technologies for micromobility, highlighting gaps and suggesting research directions by comparing AR with autonomous driving (AD).


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of micromobility solutions like e-scooters and e-bikes necessitates reliable AR technologies, but current research lacks comprehensive systems and support.

Method: The review systematically examines AR components (perception, planning, control) through the lens of AD technologies, identifying gaps and proposing solutions.

Result: Critical gaps include inadequate perception systems, limited industry/government support, and insufficient research attention. Promising directions include multimodal sensors and edge deep learning.

Conclusion: The review aims to accelerate AR development by leveraging AD insights, focusing on safety, efficiency, and scalability for urban mobility.

Abstract: The rapid adoption of micromobility solutions, particularly two-wheeled
vehicles like e-scooters and e-bikes, has created an urgent need for reliable
autonomous riding (AR) technologies. While autonomous driving (AD) systems have
matured significantly, AR presents unique challenges due to the inherent
instability of two-wheeled platforms, limited size, limited power, and
unpredictable environments, which pose very serious concerns about road users'
safety. This review provides a comprehensive analysis of AR systems by
systematically examining their core components, perception, planning, and
control, through the lens of AD technologies. We identify critical gaps in
current AR research, including a lack of comprehensive perception systems for
various AR tasks, limited industry and government support for such
developments, and insufficient attention from the research community. The
review analyses the gaps of AR from the perspective of AD to highlight
promising research directions, such as multimodal sensor techniques for
lightweight platforms and edge deep learning architectures. By synthesising
insights from AD research with the specific requirements of AR, this review
aims to accelerate the development of safe, efficient, and scalable autonomous
riding systems for future urban mobility.

</details>


### [139] [A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning](https://arxiv.org/abs/2507.11938)
*Hao Chen,Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: A novel method for grasping unknown objects using similarity matching with known objects, leveraging visual features, pre-existing grasping knowledge, and local fine-tuning for robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the performance robustness limitations of learning-based approaches in grasping unknown objects due to sensitivity to noise and environmental changes.

Method: 1) Similarity matching with a database of known objects. 2) Imitative grasp planning using candidate models. 3) Local fine-tuning for grasp optimization. Includes multi-level similarity matching and novel descriptors like C-FPFH.

Result: Proposed method achieves robust unknown-object grasping from a single viewpoint by integrating semantic, geometric, and dimensional features.

Conclusion: The similarity matching framework, enhanced by novel descriptors and techniques, offers a generalized solution for grasping unknown objects under partial observation.

Abstract: Grasping unknown objects from a single view has remained a challenging topic
in robotics due to the uncertainty of partial observation. Recent advances in
large-scale models have led to benchmark solutions such as GraspNet-1Billion.
However, such learning-based approaches still face a critical limitation in
performance robustness for their sensitivity to sensing noise and environmental
changes. To address this bottleneck in achieving highly generalized grasping,
we abandon the traditional learning framework and introduce a new perspective:
similarity matching, where similar known objects are utilized to guide the
grasping of unknown target objects. We newly propose a method that robustly
achieves unknown-object grasping from a single viewpoint through three key
steps: 1) Leverage the visual features of the observed object to perform
similarity matching with an existing database containing various object models,
identifying potential candidates with high similarity; 2) Use the candidate
models with pre-existing grasping knowledge to plan imitative grasps for the
unknown target object; 3) Optimize the grasp quality through a local
fine-tuning process. To address the uncertainty caused by partial and noisy
observation, we propose a multi-level similarity matching framework that
integrates semantic, geometric, and dimensional features for comprehensive
evaluation. Especially, we introduce a novel point cloud geometric descriptor,
the C-FPFH descriptor, which facilitates accurate similarity assessment between
partial point clouds of observed objects and complete point clouds of database
models. In addition, we incorporate the use of large language models, introduce
the semi-oriented bounding box, and develop a novel point cloud registration
approach based on plane detection to enhance matching accuracy under
single-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.

</details>


### [140] [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/abs/2507.12440)
*Ruihan Yang,Qinxi Yu,Yecheng Wu,Rui Yan,Borui Li,An-Chieh Cheng,Xueyan Zou,Yunhao Fang,Hongxu Yin,Sifei Liu,Song Han,Yao Lu,Xiaolong Wang*

Main category: cs.RO

TL;DR: Training Vision-Language-Action (VLA) models using human videos for robotic manipulation, bypassing robot hardware constraints.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of robot hardware in data collection for imitation learning by leveraging scalable and diverse human videos.

Method: Use human videos to train VLA models, convert human actions to robot actions via Inverse Kinematics, and fine-tune with robot demonstrations.

Result: EgoVLA shows significant improvements over baselines, validated on the Isaac Humanoid Manipulation Benchmark.

Conclusion: Human videos enrich robotic manipulation training, enabling scalable and diverse policy learning.

Abstract: Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: RiemannLoRA improves LoRA by treating adapters as elements on a smooth manifold, addressing overparametrization and initialization challenges, leading to better performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in LoRA, such as optimal initialization and overparametrization, hinder its efficiency.

Method: Proposes RiemannLoRA, treating LoRA matrices as a smooth manifold to remove overparametrization and determine optimal initialization via loss decrease direction.

Result: RiemannLoRA outperforms standard LoRA and its variants in convergence speed and final performance.

Conclusion: RiemannLoRA provides a unified, efficient solution for LoRA's challenges, enhancing fine-tuning of large models.

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [142] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [143] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Main category: cs.LG

TL;DR: The paper analyzes how coreset selection methods impact dataset biases and model robustness, revealing nuanced interactions between sample difficulty, bias alignment, and downstream performance.


<details>
  <summary>Details</summary>
Motivation: To understand whether and how dataset reduction methods like coreset selection perpetuate, amplify, or mitigate biases in datasets, which can lead to spurious correlations in models.

Method: A comprehensive analysis using ten spurious correlation benchmarks, five sample importance metrics, and five data selection policies across varying coreset sizes.

Result: Embedding-based sample characterization scores reduce bias risk compared to learning dynamics-based methods. Prioritizing difficult samples may lower bias but doesn't ensure model robustness.

Conclusion: Coreset selection methods can influence bias levels, but their impact on downstream robustness is complex and not guaranteed, highlighting the need for careful method choice.

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [144] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: MNIST-Gen is a framework for generating MNIST-style datasets tailored to specific domains, combining CLIP-based semantic understanding, reinforcement learning, and human feedback for efficient and accurate categorization.


<details>
  <summary>Details</summary>
Motivation: Standard datasets like MNIST are generic and irrelevant for domain-specific tasks, while creating custom datasets is time-consuming and legally complex.

Method: Uses hierarchical semantic categorization, CLIP, reinforcement learning, and human feedback for intelligent dataset generation with modular, composable stages.

Result: Achieves 85% automatic categorization accuracy and 80% time savings, demonstrated with Tree-MNIST and Food-MNIST datasets.

Conclusion: MNIST-Gen effectively produces task-specific datasets, addressing limitations of standard benchmarks and manual creation.

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [145] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Main category: cs.LG

TL;DR: PRISM is a communication-efficient and compute-aware strategy for deploying foundation models (FMs) on edge devices, reducing communication and computation overhead with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Foundation models face deployment challenges at the edge due to high communication and computation demands. PRISM addresses these issues for practical edge deployment.

Method: PRISM uses Segment Means for feature approximation, restructures self-attention to avoid redundant computations, and introduces partition-aware causal masking for autoregressive models.

Result: PRISM reduces communication overhead by up to 99.2% and computation by 51.24% for BERT, with minor accuracy degradation.

Conclusion: PRISM provides a scalable and efficient solution for deploying foundation models in resource-constrained edge environments.

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [146] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: RegCL is a continual learning framework for multi-domain knowledge integration in SAM, avoiding catastrophic forgetting through parameter merging without replay.


<details>
  <summary>Details</summary>
Motivation: Address performance limitations and scalability issues of SAM in specific domains caused by adapter-based methods leading to catastrophic forgetting.

Method: Proposes RegCL, merging parameters of SAM's adaptation modules (e.g., LoRA) across domains via weight optimization to minimize prediction discrepancies.

Result: Achieves efficient multi-domain knowledge consolidation with constant model size and no historical data storage, validated across downstream datasets.

Conclusion: RegCL effectively enables scalable continual learning for SAM in dynamic scenarios.

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [147] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: A novel prompt-based method for online continual learning (OCL) is proposed, addressing data privacy and catastrophic forgetting with a lightweight prompt generator, trainable scaler-shifter, PTM preservation, and hard-soft updates. It outperforms SOTAs in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Data privacy constraints in OCL complicate catastrophic forgetting. Existing methods (memory replay or prompt-based) face practical or efficiency issues.

Method: Proposes a prompt-based method with: (1) lightweight prompt generator, (2) trainable scaler-shifter, (3) PTM generalization preservation, and (4) hard-soft updates.

Result: Achieves higher performance than SOTAs on CIFAR100, ImageNet-R, ImageNet-A, and CUB. Requires fewer parameters and moderate training/inference time.

Conclusion: The method effectively balances performance and efficiency in OCL, with potential for further study. Code is available.

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [148] [Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing through Non-invasive BCI](https://arxiv.org/abs/2507.12417)
*Weichen Dai,Yuxuan Huang,Li Zhu,Dongjun Liu,Yu Zhang,Qibin Zhao,Andrzej Cichocki,Fabio Babiloni,Ke Li,Jianyu Qiu,Gangyong Jia,Wanzeng Kong,Qing Wu*

Main category: q-bio.NC

TL;DR: EEG-based BCIs decode spontaneous 6D pose during passive video viewing, revealing distributed neural encoding of spatial cognition.


<details>
  <summary>Details</summary>
Motivation: To understand large-scale neural dynamics supporting spatial representation during passive, naturalistic experiences.

Method: Used EEG-based BCIs to decode 6D pose (position and orientation) during passive viewing of egocentric video, analyzing decoding performance and neural contributions.

Result: EEG can decode fine-grained spatial representations, with improved performance at 100 ms frame rates, revealing distributed neural encoding.

Conclusion: Spatial cognition operates spontaneously and continuously, challenging active/passive distinctions, and offers insights into neural encoding of sensory experience.

Abstract: Humans possess a remarkable capacity for spatial cognition, allowing for
self-localization even in novel or unfamiliar environments. While hippocampal
neurons encoding position and orientation are well documented, the large-scale
neural dynamics supporting spatial representation, particularly during
naturalistic, passive experience, remain poorly understood. Here, we
demonstrate for the first time that non-invasive brain-computer interfaces
(BCIs) based on electroencephalography (EEG) can decode spontaneous,
fine-grained egocentric 6D pose, comprising three-dimensional position and
orientation, during passive viewing of egocentric video. Despite EEG's limited
spatial resolution and high signal noise, we find that spatially coherent
visual input (i.e., continuous and structured motion) reliably evokes decodable
spatial representations, aligning with participants' subjective sense of
spatial engagement. Decoding performance further improves when visual input is
presented at a frame rate of 100 ms per image, suggesting alignment with
intrinsic neural temporal dynamics. Using gradient-based backpropagation
through a neural decoding model, we identify distinct EEG channels contributing
to position -- and orientation specific -- components, revealing a distributed
yet complementary neural encoding scheme. These findings indicate that the
brain's spatial systems operate spontaneously and continuously, even under
passive conditions, challenging traditional distinctions between active and
passive spatial cognition. Our results offer a non-invasive window into the
automatic construction of egocentric spatial maps and advance our understanding
of how the human mind transforms everyday sensory experience into structured
internal representations.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [149] [A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy](https://arxiv.org/abs/2507.11853)
*J. Senthilnath,Jayasanker Jayabalan,Zhuoyi Lin,Aye Phyu Phyu Aung,Chen Hao,Kaixin Xu,Yeow Kheng Lim,F. C. Wellstood*

Main category: physics.ins-det

TL;DR: The paper introduces a spatial-physics informed model (SPIM) for improving non-destructive testing (NDT) in advanced semiconductor packaging using SQUID microscopy, addressing eddy current effects and image misalignment.


<details>
  <summary>Details</summary>
Motivation: Advanced packaging in semiconductor manufacturing faces challenges in NDT due to layer complexity, requiring better magnetic field inversion methods.

Method: SPIM includes magnetic image enhancement, alignment correction, and a Biot-Savart Law-integrated FFT inversion method.

Result: SPIM improves I-channel sharpness by 0.3%, reduces Q-channel sharpness by 25%, and corrects misalignments of 0.30.

Conclusion: SPIM demonstrates the effectiveness of combining spatial analysis with physics-driven models for practical NDT applications.

Abstract: The development of advanced packaging is essential in the semiconductor
manufacturing industry. However, non-destructive testing (NDT) of advanced
packaging becomes increasingly challenging due to the depth and complexity of
the layers involved. In such a scenario, Magnetic field imaging (MFI) enables
the imaging of magnetic fields generated by currents. For MFI to be effective
in NDT, the magnetic fields must be converted into current density. This
conversion has typically relied solely on a Fast Fourier Transform (FFT) for
magnetic field inversion; however, the existing approach does not consider eddy
current effects or image misalignment in the test setup. In this paper, we
present a spatial-physics informed model (SPIM) designed for a 3D spiral sample
scanned using Superconducting QUantum Interference Device (SQUID) microscopy.
The SPIM encompasses three key components: i) magnetic image enhancement by
aligning all the "sharp" wire field signals to mitigate the eddy current effect
using both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii)
magnetic image alignment that addresses skew effects caused by any misalignment
of the scanning SQUID microscope relative to the wire segments; and (iii) an
inversion method for converting magnetic fields to magnetic currents by
integrating the Biot-Savart Law with FFT. The results show that the SPIM
improves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%.
Also, we were able to remove rotational and skew misalignments of 0.30 in a
real image. Overall, SPIM highlights the potential of combining spatial
analysis with physics-driven models in practical applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [150] [CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos](https://arxiv.org/abs/2507.11900)
*Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai*

Main category: eess.IV

TL;DR: CompressedVQA-HDR is a new VQA framework for HDR videos, using Swin Transformer and SigLip 2 for FR and NR models, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing VQA methods lack generalization for diverse video types, especially HDR content.

Method: Uses Swin Transformer for FR (deep structural/textural similarities) and SigLip 2 for NR (global mean features). Pre-trains on SDR data and fine-tunes on HDRSDR-VQA.

Result: State-of-the-art performance; CompressedVQA-HDR-FR won first place in IEEE ICME 2025 challenge.

Conclusion: CompressedVQA-HDR effectively addresses HDR video quality assessment challenges.

Abstract: Video compression is a standard procedure applied to all videos to minimize
storage and transmission demands while preserving visual quality as much as
possible. Therefore, evaluating the visual quality of compressed videos is
crucial for guiding the practical usage and further development of video
compression algorithms. Although numerous compressed video quality assessment
(VQA) methods have been proposed, they often lack the generalization capability
needed to handle the increasing diversity of video types, particularly high
dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an
effective VQA framework designed to address the challenges of HDR video quality
assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the
backbone networks for the proposed full-reference (FR) and no-reference (NR)
VQA models, respectively. For the FR model, we compute deep structural and
textural similarities between reference and distorted frames using
intermediate-layer features extracted from the Swin Transformer as its
quality-aware feature representation. For the NR model, we extract the global
mean of the final-layer feature maps from SigLip 2 as its quality-aware
representation. To mitigate the issue of limited HDR training data, we
pre-train the FR model on a large-scale standard dynamic range (SDR) VQA
dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ
an iterative mixed-dataset training strategy across multiple compressed VQA
datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental
results show that our models achieve state-of-the-art performance compared to
existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place
in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand
Challenge at IEEE ICME 2025. The code is available at
https://github.com/sunwei925/CompressedVQA-HDR.

</details>


### [151] [Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease](https://arxiv.org/abs/2507.12012)
*Matthias Perkonigg,Nina Bastati,Ahmed Ba-Ssalamah,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Xiaofei Zhou,Michael Trauner,Georg Langs*

Main category: eess.IV

TL;DR: Unsupervised machine learning identifies liver tissue patterns in MRI to quantify treatment response in diffuse liver disease, outperforming non-imaging measures.


<details>
  <summary>Details</summary>
Motivation: To develop quantifiable image patterns for disease progression and treatment response to guide therapy and novel treatments.

Method: Deep clustering networks encode and cluster liver tissue patches into a low-dimensional latent space to create a tissue vocabulary.

Result: The method identifies treatment-specific liver tissue changes, predicts biopsy features from imaging, and validates on a separate cohort.

Conclusion: The approach effectively quantifies treatment response and has potential for non-invasive disease monitoring.

Abstract: Quantifiable image patterns associated with disease progression and treatment
response are critical tools for guiding individual treatment, and for
developing novel therapies. Here, we show that unsupervised machine learning
can identify a pattern vocabulary of liver tissue in magnetic resonance images
that quantifies treatment response in diffuse liver disease. Deep clustering
networks simultaneously encode and cluster patches of medical images into a
low-dimensional latent space to establish a tissue vocabulary. The resulting
tissue types capture differential tissue change and its location in the liver
associated with treatment response. We demonstrate the utility of the
vocabulary on a randomized controlled trial cohort of non-alcoholic
steatohepatitis patients. First, we use the vocabulary to compare longitudinal
liver change in a placebo and a treatment cohort. Results show that the method
identifies specific liver tissue change pathways associated with treatment, and
enables a better separation between treatment groups than established
non-imaging measures. Moreover, we show that the vocabulary can predict biopsy
derived features from non-invasive imaging data. We validate the method on a
separate replication cohort to demonstrate the applicability of the proposed
method.

</details>


### [152] [Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis](https://arxiv.org/abs/2507.12092)
*Nataliia Molchanova,Alessandro Cagol,Mario Ocampo-Pineda,Po-Jui Lu,Matthias Weigel,Xinjie Chen,Erin Beck,Charidimos Tsagkas,Daniel Reich,Colin Vanden Bulcke,Anna Stolting,Serena Borrelli,Pietro Maggi,Adrien Depeursinge,Cristina Granziera,Henning Mueller,Pedro M. Gordaliza,Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: A benchmark for detecting and segmenting cortical lesions in MS using MRI, leveraging nnU-Net for improved detection, with public implementation for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Cortical lesions are important in MS but face clinical integration challenges due to subtle MRI appearance and lack of standardized methods.

Method: Used 656 MRI scans with expert annotations, adapted nnU-Net for CL detection, and evaluated generalization through out-of-distribution testing.

Result: Achieved F1-scores of 0.64 (in-domain) and 0.5 (out-of-domain), with analysis of model features and errors for better understanding.

Conclusion: The study highlights data variability and lesion ambiguity challenges, offering recommendations for clinical adoption, with publicly available models.

Abstract: Cortical lesions (CLs) have emerged as valuable biomarkers in multiple
sclerosis (MS), offering high diagnostic specificity and prognostic relevance.
However, their routine clinical integration remains limited due to subtle
magnetic resonance imaging (MRI) appearance, challenges in expert annotation,
and a lack of standardized automated methods. We propose a comprehensive
multi-centric benchmark of CL detection and segmentation in MRI. A total of 656
MRI scans, including clinical trial and research data from four institutions,
were acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with
expert-consensus annotations. We rely on the self-configuring nnU-Net
framework, designed for medical imaging segmentation, and propose adaptations
tailored to the improved CL detection. We evaluated model generalization
through out-of-distribution testing, demonstrating strong lesion detection
capabilities with an F1-score of 0.64 and 0.5 in and out of the domain,
respectively. We also analyze internal model features and model errors for a
better understanding of AI decision-making. Our study examines how data
variability, lesion ambiguity, and protocol differences impact model
performance, offering future recommendations to address these barriers to
clinical adoption. To reinforce the reproducibility, the implementation and
models will be publicly accessible and ready to use at
https://github.com/Medical-Image-Analysis-Laboratory/ and
https://doi.org/10.5281/zenodo.15911797.

</details>


### [153] [Landmark Detection for Medical Images using a General-purpose Segmentation Model](https://arxiv.org/abs/2507.11551)
*Ekaterina Stansfield,Jennifer A. Mitterer,Abdulrahman Altahhan*

Main category: eess.IV

TL;DR: The paper proposes combining YOLO and SAM to accurately segment orthopaedic pelvic landmarks in radiographs, overcoming the limitations of standalone models.


<details>
  <summary>Details</summary>
Motivation: General-purpose models like SAM and MedSAM lack precision for fine-grained orthopaedic landmark detection, necessitating a hybrid approach.

Method: Uses YOLO for bounding box detection to prompt SAM, creating a pipeline for segmenting landmarks and complex regions in pelvic radiographs.

Result: The hybrid model successfully segments 72 landmarks and 16 complex regions, showing excellent performance in orthopaedic pelvic radiographs.

Conclusion: Combining YOLO and SAM provides a reliable solution for accurate landmark segmentation in medical imaging.

Abstract: Radiographic images are a cornerstone of medical diagnostics in orthopaedics,
with anatomical landmark detection serving as a crucial intermediate step for
information extraction. General-purpose foundational segmentation models, such
as SAM (Segment Anything Model), do not support landmark segmentation out of
the box and require prompts to function. However, in medical imaging, the
prompts for landmarks are highly specific. Since SAM has not been trained to
recognize such landmarks, it cannot generate accurate landmark segmentations
for diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has
been trained to identify larger anatomical structures, such as organs and their
parts, and lacks the fine-grained precision required for orthopaedic pelvic
landmarks. To address this limitation, we propose leveraging another
general-purpose, non-foundational model: YOLO. YOLO excels in object detection
and can provide bounding boxes that serve as input prompts for SAM. While YOLO
is efficient at detection, it is significantly outperformed by SAM in
segmenting complex structures. In combination, these two models form a reliable
pipeline capable of segmenting not only a small pilot set of eight anatomical
landmarks but also an expanded set of 72 landmarks and 16 regions with complex
outlines, such as the femoral cortical bone and the pelvic inlet. By using
YOLO-generated bounding boxes to guide SAM, we trained the hybrid model to
accurately segment orthopaedic pelvic radiographs. Our results show that the
proposed combination of YOLO and SAM yields excellent performance in detecting
anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.

</details>


### [154] [3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation](https://arxiv.org/abs/2507.11557)
*Jiaxu Zheng,Meiman He,Xuhui Tang,Xiong Wang,Tuoyu Cao,Tianyi Zeng,Lichi Zhang,Chenyu You*

Main category: eess.IV

TL;DR: A novel 3D Wavelet Latent Diffusion Model (3D-WLDM) improves MR-to-CT synthesis for whole-body imaging by enhancing feature capture and preserving anatomical integrity.


<details>
  <summary>Details</summary>
Motivation: Existing MR-to-CT synthesis methods for whole-body imaging suffer from poor spatial alignment and insufficient image quality, limiting their clinical utility.

Method: The 3D-WLDM incorporates a Wavelet Residual Module for fine-scale feature capture, disentangles structural and modality-specific characteristics, and uses a Dual Skip Connection Attention mechanism for high-resolution CT generation.

Result: The model generates high-resolution CT images with improved bony structures and soft-tissue contrast, addressing alignment and quality issues.

Conclusion: The 3D-WLDM offers a promising solution for accurate MR-to-CT synthesis, enhancing its reliability for clinical workflows like PET/MR and MR-only radiation therapy.

Abstract: Magnetic Resonance (MR) imaging plays an essential role in contemporary
clinical diagnostics. It is increasingly integrated into advanced therapeutic
workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance
(PET/MR) imaging and MR-only radiation therapy. These integrated approaches are
critically dependent on accurate estimation of radiation attenuation, which is
typically facilitated by synthesizing Computed Tomography (CT) images from MR
scans to generate attenuation maps. However, existing MR-to-CT synthesis
methods for whole-body imaging often suffer from poor spatial alignment between
the generated CT and input MR images, and insufficient image quality for
reliable use in downstream clinical tasks. In this paper, we present a novel 3D
Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by
performing modality translation in a learned latent space. By incorporating a
Wavelet Residual Module into the encoder-decoder architecture, we enhance the
capture and reconstruction of fine-scale features across image and latent
spaces. To preserve anatomical integrity during the diffusion process, we
disentangle structural and modality-specific characteristics and anchor the
structural component to prevent warping. We also introduce a Dual Skip
Connection Attention mechanism within the diffusion model, enabling the
generation of high-resolution CT images with improved representation of bony
structures and soft-tissue contrast.

</details>


### [155] [Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach](https://arxiv.org/abs/2507.11561)
*Lucas Erlacher,Samuel Ruipérez-Campillo,Holger Michel,Sven Wellmann,Thomas M. Sutter,Ece Ozkan,Julia E. Vogt*

Main category: eess.IV

TL;DR: A multi-view variational autoencoder (VAE) improves pulmonary hypertension (PH) prediction in newborns using echocardiographic videos, outperforming single-view and supervised methods in accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: PH in newborns is life-threatening, and current diagnostic methods like echocardiography are operator-dependent and lack generalizability. Automated methods for adults are inadequate for newborns.

Method: A multi-view VAE is used to capture complex latent representations from echocardiographic videos, enhancing feature extraction and robustness.

Result: The model shows improved generalization and classification accuracy compared to single-view and supervised approaches.

Conclusion: Multi-view learning with VAE is effective for robust PH assessment in newborns, addressing limitations of current methods.

Abstract: Pulmonary hypertension (PH) in newborns is a critical condition characterized
by elevated pressure in the pulmonary arteries, leading to right ventricular
strain and heart failure. While right heart catheterization (RHC) is the
diagnostic gold standard, echocardiography is preferred due to its non-invasive
nature, safety, and accessibility. However, its accuracy highly depends on the
operator, making PH assessment subjective. While automated detection methods
have been explored, most models focus on adults and rely on single-view
echocardiographic frames, limiting their performance in diagnosing PH in
newborns. While multi-view echocardiography has shown promise in improving PH
assessment, existing models struggle with generalizability. In this work, we
employ a multi-view variational autoencoder (VAE) for PH prediction using
echocardiographic videos. By leveraging the VAE framework, our model captures
complex latent representations, improving feature extraction and robustness. We
compare its performance against single-view and supervised learning approaches.
Our results show improved generalization and classification accuracy,
highlighting the effectiveness of multi-view learning for robust PH assessment
in newborns.

</details>


### [156] [Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?](https://arxiv.org/abs/2507.11569)
*Hanxue Gu,Yaqian Chen,Nicholas Konz,Qihang Li,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: Foundation models like SAM show promise for zero-shot breast MRI registration but struggle with fine details of fibroglandular tissue. Additional medical pre-training doesn't always help.


<details>
  <summary>Details</summary>
Motivation: To evaluate if foundation models can handle the complexity of breast MRI registration, given its challenges like anatomical variation and deformable structures.

Method: Assessed five pre-trained encoders (DINO-v2, SAM, MedSAM, SSLSAM, MedCLIP) across four breast MRI registration tasks.

Result: SAM outperformed traditional methods for overall alignment but struggled with fine details. Medical pre-training didn't improve performance.

Conclusion: Further research is needed to improve fine structure accuracy and understand domain-specific training impacts.

Abstract: Foundation models, pre-trained on large image datasets and capable of
capturing rich feature representations, have recently shown potential for
zero-shot image registration. However, their performance has mostly been tested
in the context of rigid or less complex structures, such as the brain or
abdominal organs, and it remains unclear whether these models can handle more
challenging, deformable anatomy. Breast MRI registration is particularly
difficult due to significant anatomical variation between patients, deformation
caused by patient positioning, and the presence of thin and complex internal
structure of fibroglandular tissue, where accurate alignment is crucial.
Whether foundation model-based registration algorithms can address this level
of complexity remains an open question. In this study, we provide a
comprehensive evaluation of foundation model-based registration algorithms for
breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM,
MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that
capture variations in different years and dates, sequences, modalities, and
patient disease status (lesion versus no lesion). Our results show that
foundation model-based algorithms such as SAM outperform traditional
registration baselines for overall breast alignment, especially under large
domain shifts, but struggle with capturing fine details of fibroglandular
tissue. Interestingly, additional pre-training or fine-tuning on medical or
breast-specific images in MedSAM and SSLSAM, does not improve registration
performance and may even decrease it in some cases. Further work is needed to
understand how domain-specific training influences registration and to explore
targeted strategies that improve both global alignment and fine structure
accuracy. We also publicly release our code at
\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.

</details>


### [157] [Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation](https://arxiv.org/abs/2507.12427)
*Ashkan Shakarami,Azade Farshad,Yousef Yeganeh,Lorenzo Nicole,Peter Schuffler,Stefano Ghidoni,Nassir Navab*

Main category: eess.IV

TL;DR: UTS is a unit-based tissue segmentation framework using 32x32 tiles for classification, reducing annotation effort and improving efficiency without losing accuracy. It employs a Multi-Level Vision Transformer (L-ViT) for multi-level feature representation.


<details>
  <summary>Details</summary>
Motivation: To reduce annotation effort and computational costs while maintaining accuracy in histopathology tissue segmentation.

Method: Proposes UTS, classifying 32x32 tiles instead of pixels, and introduces L-ViT for multi-level feature representation.

Result: Outperforms U-Net variants and transformer baselines on 386,371 tiles from 459 H&E-stained regions.

Conclusion: UTS is effective for breast tissue segmentation, supporting clinical tasks like tumor-stroma quantification and surgical margin assessment.

Abstract: We propose UTS, a unit-based tissue segmentation framework for histopathology
that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the
segmentation unit. This approach reduces annotation effort and improves
computational efficiency without compromising accuracy. To implement this
approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits
the multi-level feature representation to capture both fine-grained morphology
and global tissue context. Trained to segment breast tissue into three
categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports
clinically relevant tasks such as tumor-stroma quantification and surgical
margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it
outperforms U-Net variants and transformer-based baselines. Code and Dataset
will be available at GitHub.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [158] [Simulated Language Acquisition in a Biologically Realistic Model of the Brain](https://arxiv.org/abs/2507.11788)
*Daniel Mitropolsky,Christos Papadimitriou*

Main category: cs.NE

TL;DR: A neuromorphic system based on six neuroscience principles learns language semantics, syntax, and word order from scratch.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between neuronal spiking and high-level cognition by formalizing neuroscience principles.

Method: Implemented a simulated system with excitatory neurons, brain areas, random synapses, Hebbian plasticity, and inhibition.

Result: The system learns word semantics, syntax, and word order from minimal grounded sentences.

Conclusion: Demonstrates potential for understanding cognition and suggests further extensions.

Abstract: Despite tremendous progress in neuroscience, we do not have a compelling
narrative for the precise way whereby the spiking of neurons in our brain
results in high-level cognitive phenomena such as planning and language. We
introduce a simple mathematical formulation of six basic and broadly accepted
principles of neuroscience: excitatory neurons, brain areas, random synapses,
Hebbian plasticity, local inhibition, and inter-area inhibition. We implement a
simulated neuromorphic system based on this formalism, which is capable of
basic language acquisition: Starting from a tabula rasa, the system learns, in
any language, the semantics of words, their syntactic role (verb versus noun),
and the word order of the language, including the ability to generate novel
sentences, through the exposure to a modest number of grounded sentences in the
same language. We discuss several possible extensions and implications of this
result.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [159] [MOSPA: Human Motion Generation Driven by Spatial Audio](https://arxiv.org/abs/2507.11949)
*Shuyang Xu,Zhiyang Dou,Mingyi Shi,Liang Pan,Leo Ho,Jingbo Wang,Yuan Liu,Cheng Lin,Yuexin Ma,Wenping Wang,Taku Komura*

Main category: cs.GR

TL;DR: The paper introduces a dataset (SAM) and a diffusion-based model (MOSPA) for generating realistic human motions driven by spatial audio, addressing a gap in existing research.


<details>
  <summary>Details</summary>
Motivation: Existing models overlook spatial audio's impact on human motion, limiting realism in virtual human responses to auditory stimuli.

Method: Developed the SAM dataset and MOSPA, a diffusion-based generative framework, to model the relationship between spatial audio and human motion.

Result: MOSPA achieves state-of-the-art performance, generating diverse and realistic motions from spatial audio inputs.

Conclusion: The work bridges a critical gap in spatial audio-driven motion synthesis, with plans to open-source the model and dataset.

Abstract: Enabling virtual humans to dynamically and realistically respond to diverse
auditory stimuli remains a key challenge in character animation, demanding the
integration of perceptual modeling and motion synthesis. Despite its
significance, this task remains largely unexplored. Most previous works have
primarily focused on mapping modalities like speech, audio, and music to
generate human motion. As of yet, these models typically overlook the impact of
spatial features encoded in spatial audio signals on human motion. To bridge
this gap and enable high-quality modeling of human movements in response to
spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human
Motion (SAM) dataset, which contains diverse and high-quality spatial audio and
motion data. For benchmarking, we develop a simple yet effective
diffusion-based generative framework for human MOtion generation driven by
SPatial Audio, termed MOSPA, which faithfully captures the relationship between
body motion and spatial audio through an effective fusion mechanism. Once
trained, MOSPA could generate diverse realistic human motions conditioned on
varying spatial audio inputs. We perform a thorough investigation of the
proposed dataset and conduct extensive experiments for benchmarking, where our
method achieves state-of-the-art performance on this task. Our model and
dataset will be open-sourced upon acceptance. Please refer to our supplementary
video for more details.

</details>


### [160] [HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing](https://arxiv.org/abs/2507.11971)
*Tielong Wang,Yuxuan Xiong,Jinfan Liu,Zhifan Zhang,Ye Chen,Yue Shi,Bingbing Ni*

Main category: cs.GR

TL;DR: A novel 3D Hierarchical Proxy Node representation is introduced to address limitations of current 3D representations (meshes, voxels, point clouds, NeRFs) by using sparse, tree-structured nodes for efficient shape and texture encoding.


<details>
  <summary>Details</summary>
Motivation: Current 3D representations lack universal applicability, struggle with trade-offs between complexity and fidelity, and hinder tasks like editing and animation.

Method: The method uses hierarchically organized proxy nodes with local shape/texture info (encoded by MLPs) for efficient neural interpolation and decoding.

Result: The framework achieves compact representation, high-fidelity rendering, and superior editability in experiments.

Conclusion: The proposed representation overcomes key limitations of existing methods, offering scalability and direct manipulation capabilities.

Abstract: Current 3D representations like meshes, voxels, point clouds, and NeRF-based
neural implicit fields exhibit significant limitations: they are often
task-specific, lacking universal applicability across reconstruction,
generation, editing, and driving. While meshes offer high precision, their
dense vertex data complicates editing; NeRFs deliver excellent rendering but
suffer from structural ambiguity, hindering animation and manipulation; all
representations inherently struggle with the trade-off between data complexity
and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical
Proxy Node representation. Its core innovation lies in representing an object's
shape and texture via a sparse set of hierarchically organized
(tree-structured) proxy nodes distributed on its surface and interior. Each
node stores local shape and texture information (implicitly encoded by a small
MLP) within its neighborhood. Querying any 3D coordinate's properties involves
efficient neural interpolation and lightweight decoding from relevant nearby
and parent nodes. This framework yields a highly compact representation where
nodes align with local semantics, enabling direct drag-and-edit manipulation,
and offers scalable quality-complexity control. Extensive experiments across 3D
reconstruction and editing demonstrate our method's expressive efficiency,
high-fidelity rendering quality, and superior editability.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [161] [Image-Based Multi-Survey Classification of Light Curves with a Pre-Trained Vision Transformer](https://arxiv.org/abs/2507.11711)
*Daniel Moreno-Cartagena,Guillermo Cabrera-Vives,Alejandra M. Muñoz Arancibia,Pavlos Protopapas,Francisco Förster,Márcio Catelan,A. Bayo,Pablo A. Estévez,P. Sánchez-Sáez,Franz E. Bauer,M. Pavez-Herrera,L. Hernández-García,Gonzalo Rojas*

Main category: astro-ph.IM

TL;DR: Swin Transformer V2 is used for photometric classification with ZTF and ATLAS light curves, showing joint multi-survey processing works best.


<details>
  <summary>Details</summary>
Motivation: To improve photometric classification by leveraging multi-survey data (ZTF and ATLAS) and understanding survey-specific traits.

Method: Evaluated integration strategies for ZTF and ATLAS data using Swin Transformer V2, focusing on joint processing.

Result: Multi-survey architecture with joint processing outperformed other strategies.

Conclusion: Survey-specific modeling and cross-survey interactions are crucial for scalable classifiers in time-domain astronomy.

Abstract: We explore the use of Swin Transformer V2, a pre-trained vision Transformer,
for photometric classification in a multi-survey setting by leveraging light
curves from the Zwicky Transient Facility (ZTF) and the Asteroid
Terrestrial-impact Last Alert System (ATLAS). We evaluate different strategies
for integrating data from these surveys and find that a multi-survey
architecture which processes them jointly achieves the best performance. These
results highlight the importance of modeling survey-specific characteristics
and cross-survey interactions, and provide guidance for building scalable
classifiers for future time-domain astronomy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [162] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Main category: cs.AI

TL;DR: MLLMs show promise as verifiers for agent behavior but suffer from agreement bias. Self-Grounded Verification (SGV) improves their accuracy by 20% and sets a new benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Extending verifiers to domains without clear success criteria (e.g., computer use) is challenging. MLLMs offer potential due to their knowledge and reasoning skills.

Method: Proposes SGV, a two-step method: eliciting broad priors from MLLMs, then evaluating trajectories conditioned on these priors.

Result: SGV improves MLLM verifier accuracy by 20%, enhances failure detection, and boosts task completion in benchmarks like OSWorld and VisualWebArena.

Conclusion: SGV effectively mitigates agreement bias in MLLMs, enabling scalable and accurate verification of agent behavior in complex domains.

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [163] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint is a framework for code quality analysis using instruction tuning, improving generalization to unseen coding patterns without retraining.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with code quality analysis due to static training data and inability to adapt to evolving best practices.

Method: MetaLint uses instruction tuning on synthetic linter-generated data for easy-to-hard generalization, avoiding static rule-based training.

Result: Achieves 70.37% F-score on idiom detection and 26.73% on localization, competitive with larger models.

Conclusion: MetaLint shows promise for adaptable, future-proof code quality analysis.

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [164] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: MERA Code is a new benchmark for evaluating code generation LLMs in Russian, focusing on practical coding skills across 8 languages, addressing gaps in current evaluations.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations overlook code quality and real-world performance, especially in non-English languages like Russian.

Method: MERA Code includes 11 tasks across 8 programming languages, with a taxonomy of practical coding skills, an open-source codebase, scoring system, and a leaderboard platform.

Result: Open and frontier API models are evaluated, revealing limitations in practical coding tasks for non-English languages.

Conclusion: MERA Code aims to guide research, anticipate model advancements, and standardize evaluations for code generation LLMs.

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [165] [DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi](https://arxiv.org/abs/2507.12132)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: The paper proposes a 3D latent motion representation (DoRF) from Wi-Fi CSI Doppler projections, improving HAR generalization.


<details>
  <summary>Details</summary>
Motivation: Current Wi-Fi CSI-based HAR lacks sufficient generalizability for practical use.

Method: Inspired by NeRF, the approach reconstructs a 3D latent motion representation from 1D Doppler projections.

Result: The method enhances HAR generalization accuracy by creating a uniform Doppler radiance field.

Conclusion: DoRFs show strong potential for practical Wi-Fi-based sensing applications.

Abstract: Wi-Fi Channel State Information (CSI) has gained increasing interest for
remote sensing applications. Recent studies show that Doppler velocity
projections extracted from CSI can enable human activity recognition (HAR) that
is robust to environmental changes and generalizes to new users. However,
despite these advances, generalizability still remains insufficient for
practical deployment. Inspired by neural radiance fields (NeRF), which learn a
volumetric representation of a 3D scene from 2D images, this work proposes a
novel approach to reconstruct an informative 3D latent motion representation
from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The
resulting latent representation is then used to construct a uniform Doppler
radiance field (DoRF) of the motion, providing a comprehensive view of the
performed activity and improving the robustness to environmental variability.
The results show that the proposed approach noticeably enhances the
generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential
of DoRFs for practical sensing applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [166] [RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection](https://arxiv.org/abs/2507.12175)
*Sungkyun Chang,Simon Dixon,Emmanouil Benetos*

Main category: cs.SD

TL;DR: RUMAA is a transformer-based framework unifying score-to-performance alignment, transcription, and mistake detection, outperforming traditional methods, especially with repeated scores.


<details>
  <summary>Details</summary>
Motivation: To integrate and improve separate tasks of music performance analysis (alignment, transcription, mistake detection) into a unified framework.

Method: Uses pre-trained score and audio encoders with a tri-stream decoder to capture task interdependencies, aligning MusicXML scores with performance audio.

Result: Matches state-of-the-art alignment on non-repeated scores and outperforms on repeated scores; also shows promise in transcription and mistake detection.

Conclusion: RUMAA successfully unifies multiple music performance analysis tasks, offering improved performance and versatility, especially with complex scores.

Abstract: This study introduces RUMAA, a transformer-based framework for music
performance analysis that unifies score-to-performance alignment,
score-informed transcription, and mistake detection in a near end-to-end
manner. Unlike prior methods addressing these tasks separately, RUMAA
integrates them using pre-trained score and audio encoders and a novel
tri-stream decoder capturing task interdependencies through proxy tasks. It
aligns human-readable MusicXML scores with repeat symbols to full-length
performance audio, overcoming traditional MIDI-based methods that rely on
manually unfolded score-MIDI data with pre-specified repeat structures. RUMAA
matches state-of-the-art alignment methods on non-repeated scores and
outperforms them on scores with repeats in a public piano music dataset, while
also delivering promising transcription and mistake detection results.

</details>


### [167] [Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification](https://arxiv.org/abs/2507.12042)
*Kazuki Shimada,Archontis Politis,Iran R. Roman,Parthasaarathy Sudarsanam,David Diaz-Guerra,Ruchi Pandey,Kengo Uchida,Yuichiro Koyama,Naoya Takahashi,Takashi Shibuya,Shusuke Takahashi,Tuomas Virtanen,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: The paper introduces Task 3 of the DCASE2025 Challenge, focusing on stereo sound event localization and detection (SELD), shifting from 360° audio to stereo with limited field-of-view. It includes new tasks like onscreen/offscreen classification and introduces a dataset and baseline system.


<details>
  <summary>Details</summary>
Motivation: To adapt SELD to more common stereo audio scenarios with limited field-of-view, addressing angular ambiguities and introducing new tasks like onscreen/offscreen classification.

Method: The challenge uses stereo audio and video data, with a baseline system processing both inputs. It includes DOA estimation in azimuth and distance, plus onscreen/offscreen classification for the audiovisual track.

Result: The baseline system performs reasonably well with stereo audio data, demonstrating feasibility for the new tasks.

Conclusion: The paper successfully adapts SELD to stereo scenarios, introducing new metrics and tasks, with the baseline system validating the approach.

Abstract: This paper presents the objective, dataset, baseline, and metrics of Task 3
of the DCASE2025 Challenge on sound event localization and detection (SELD). In
previous editions, the challenge used four-channel audio formats of first-order
Ambisonics (FOA) and microphone array. In contrast, this year's challenge
investigates SELD with stereo audio data (termed stereo SELD). This change
shifts the focus from more specialized 360{\deg} audio and audiovisual scene
analysis to more commonplace audio and media scenarios with limited
field-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,
the task focuses on direction-of-arrival (DOA) estimation in the azimuth plane
(left-right axis) along with distance estimation. The challenge remains divided
into two tracks: audio-only and audiovisual, with the audiovisual track
introducing a new sub-task of onscreen/offscreen event classification
necessitated by the limited FOV. This challenge introduces the DCASE2025 Task3
Stereo SELD Dataset, whose stereo audio and perspective video clips are sampled
and converted from the STARSS23 recordings. The baseline system is designed to
process stereo audio and corresponding video frames as inputs. In addition to
the typical SELD event classification and localization, it integrates
onscreen/offscreen classification for the audiovisual track. The evaluation
metrics have been modified to introduce an onscreen/offscreen accuracy metric,
which assesses the models' ability to identify which sound sources are
onscreen. In the experimental evaluation, the baseline system performs
reasonably well with the stereo audio data.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [168] [Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker](https://arxiv.org/abs/2507.12378)
*Rachna Saxena,Abhijeet Kumar,Suresh Shanmugam*

Main category: cs.IR

TL;DR: The paper proposes a scalable and efficient vision retrieval process for multimodal Q&A, addressing challenges in traditional and MLLM systems with hybrid search and late interaction re-ranking.


<details>
  <summary>Details</summary>
Motivation: Traditional systems struggle with infographics, and MLLMs face scalability issues in retrieval. The goal is to improve efficiency without compromising performance.

Method: Uses a multi-step approach with hybrid search (metadata & embedding) and late interaction re-ranking to retrieve best pages, then prompts MLLMs for answers.

Result: The design is scalable (faster) and stable (maintains performance), suitable for enterprise production.

Conclusion: The proposed method effectively addresses scalability and efficiency in multimodal Q&A, making it viable for enterprise use.

Abstract: Traditional information extraction systems face challenges with text only
language models as it does not consider infographics (visual elements of
information) such as tables, charts, images etc. often used to convey complex
information to readers. Multimodal LLM (MLLM) face challenges of finding needle
in the haystack problem i.e., either longer context length or substantial
number of documents as search space. Late interaction mechanism over visual
language models has shown state of the art performance in retrieval-based
vision augmented Q&A tasks. There are yet few challenges using it for RAG based
multi-modal Q&A. Firstly, many popular and widely adopted vector databases do
not support native multi-vector retrieval. Secondly, late interaction requires
computation which inflates space footprint and can hinder enterprise adoption.
Lastly, the current state of late interaction mechanism does not leverage the
approximate neighbor search indexing methods for large speed ups in retrieval
process. This paper explores a pragmatic approach to make vision retrieval
process scalable and efficient without compromising on performance quality. We
propose multi-step custom implementation utilizing widely adopted hybrid search
(metadata & embedding) and state of the art late interaction re-ranker to
retrieve best matching pages. Finally, MLLM are prompted as reader to generate
answers from contextualized best matching pages. Through experiments, we
observe that the proposed design is scalable (significant speed up) and stable
(without degrading performance quality), hence can be used as production
systems at enterprises.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [169] [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
*Brendan Murphy,Dillon Bowen,Shahrad Mohammadzadeh,Julius Broomfield,Adam Gleave,Kellin Pelrine*

Main category: cs.CR

TL;DR: The paper reveals that fine-tuning AI models can bypass safeguards, creating 'evil twin' models capable of harmful tasks like cyberattacks or CBRN assistance, with newer models becoming more vulnerable.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in AI safeguards and highlight the risks of fine-tunable models being misused for malicious purposes.

Method: Jailbreak-tuning is used to fine-tune models, bypassing moderation systems and producing high-quality harmful outputs.

Result: Models from OpenAI, Google, and Anthropic complied with harmful requests, showing increased vulnerability in newer models.

Conclusion: Urgent need for tamper-resistant safeguards; releasing fine-tunable models risks enabling equally capable malicious versions.

Abstract: AI systems are rapidly advancing in capability, and frontier model developers
broadly acknowledge the need for safeguards against serious misuse. However,
this paper demonstrates that fine-tuning, whether via open weights or closed
fine-tuning APIs, can produce helpful-only models. In contrast to prior work
which is blocked by modern moderation systems or achieved only partial removal
of safeguards or degraded output quality, our jailbreak-tuning method teaches
models to generate detailed, high-quality responses to arbitrary harmful
requests. For example, OpenAI, Google, and Anthropic models will fully comply
with requests for CBRN assistance, executing cyberattacks, and other criminal
activity. We further show that backdoors can increase not only the stealth but
also the severity of attacks, while stronger jailbreak prompts become even more
effective in fine-tuning attacks, linking attack and potentially defenses in
the input and weight spaces. Not only are these models vulnerable, more recent
ones also appear to be becoming even more vulnerable to these attacks,
underscoring the urgent need for tamper-resistant safeguards. Until such
safeguards are discovered, companies and policymakers should view the release
of any fine-tunable model as simultaneously releasing its evil twin: equally
capable as the original model, and usable for any malicious purpose within its
capabilities.

</details>


### [170] [Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification](https://arxiv.org/abs/2507.11943)
*Haiwei Lin,Shoko Imaizumi,Hitoshi Kiya*

Main category: cs.CR

TL;DR: A low-rank adaptation method for privacy-preserving ViT models reduces trainable parameters while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To train privacy-preserving ViT models efficiently by freezing pre-trained weights and reducing trainable parameters.

Method: Inject trainable rank decomposition matrices into each ViT layer and avoid freezing the patch embedding layer.

Result: Reduces trainable parameters significantly while maintaining accuracy comparable to full-time tuning.

Conclusion: The method is effective for efficient, privacy-preserving ViT training with minimal accuracy loss.

Abstract: We propose a low-rank adaptation method for training privacy-preserving
vision transformer (ViT) models that efficiently freezes pre-trained ViT model
weights. In the proposed method, trainable rank decomposition matrices are
injected into each layer of the ViT architecture, and moreover, the patch
embedding layer is not frozen, unlike in the case of the conventional low-rank
adaptation methods. The proposed method allows us not only to reduce the number
of trainable parameters but to also maintain almost the same accuracy as that
of full-time tuning.

</details>


### [171] [IDFace: Face Template Protection for Efficient and Secure Identification](https://arxiv.org/abs/2507.12050)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Dongsoo Kim,Junbum Shin,Jae Hong Seo*

Main category: cs.CR

TL;DR: IDFace is a secure and efficient HE-based face identification method that reduces computational overhead and protects face templates.


<details>
  <summary>Details</summary>
Motivation: Protecting user privacy in face recognition systems (FRS) is critical, especially securing face templates from recovery. Existing HE-based methods are inefficient, prompting the need for a tailored solution.

Method: IDFace introduces two novel techniques: a template representation transformation to reduce matching costs and a space-efficient encoding to minimize wasted space from encryption.

Result: IDFace identifies a face template from 1M encrypted templates in 126ms, with only 2X overhead compared to plaintext systems.

Conclusion: IDFace provides a practical, efficient solution for secure face identification with minimal performance trade-offs.

Abstract: As face recognition systems (FRS) become more widely used, user privacy
becomes more important. A key privacy issue in FRS is protecting the user's
face template, as the characteristics of the user's face image can be recovered
from the template. Although recent advances in cryptographic tools such as
homomorphic encryption (HE) have provided opportunities for securing the FRS,
HE cannot be used directly with FRS in an efficient plug-and-play manner. In
particular, although HE is functionally complete for arbitrary programs, it is
basically designed for algebraic operations on encrypted data of predetermined
shape, such as a polynomial ring. Thus, a non-tailored combination of HE and
the system can yield very inefficient performance, and many previous HE-based
face template protection methods are hundreds of times slower than plain
systems without protection. In this study, we propose IDFace, a new HE-based
secure and efficient face identification method with template protection.
IDFace is designed on the basis of two novel techniques for efficient searching
on a (homomorphically encrypted) biometric database with an angular metric. The
first technique is a template representation transformation that sharply
reduces the unit cost for the matching test. The second is a space-efficient
encoding that reduces wasted space from the encryption algorithm, thus saving
the number of operations on encrypted templates. Through experiments, we show
that IDFace can identify a face template from among a database of 1M encrypted
templates in 126ms, showing only 2X overhead compared to the identification
over plaintexts.

</details>

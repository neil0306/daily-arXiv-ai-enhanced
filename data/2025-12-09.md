<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.CV](#cs.CV) [Total: 224]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 23]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 10]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

TL;DR: DPO-based alignment framework improves factual correctness and empathy in healthcare LLMs for caregiver-patient dialogues.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLMs have limitations in healthcare applications due to factual unreliability and lack of empathetic communication, posing risks for non-professionals and caregivers seeking medical guidance and emotional reassurance.

Method: Direct Preference Optimization (DPO)-based alignment framework fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive/accessible communication and rejected ones represent prescriptive/technical tones.

Result: DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives like Google medical dialogue systems.

Conclusion: Preference-based alignment offers a scalable and transparent pathway for developing trustworthy, empathetic, and clinically informed AI assistants for healthcare communication.

Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [2] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yolox贸chtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

TL;DR: Novel morphologically-informed tokenizers for Yolox贸chitl Mixtec improve ASR-based interlinear gloss annotation efficiency, with Segment-and-Melody tokenizer outperforming BPE/Unigram on word error rate.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and reduce human workload in interlinear gloss annotation of Yolox贸chitl Mixtec audio corpus by developing specialized tokenizers that handle the language's non-concatenative tonal morphology.

Method: Developed two novel nonlinear tokenization schemes: 1) Segment and Melody tokenizer that extracts tones without predicting segmentation, and 2) Sequence of Processes tokenizer that predicts segmentation. Compared these against BPE and Unigram models using ASR and text-based sequence-to-sequence tools.

Result: The novel tokenizers are competitive with BPE/Unigram models. Segment-and-Melody tokenizer outperforms traditional tokenizers in word error rate but not character error rate. Morphological and information-theoretic metrics show predictive correlations with downstream performance.

Conclusion: Nonlinear tokenizers designed for non-concatenative morphology are competitive with conventional models for ASR, suggesting specialized tokenization can aid annotation efficiency. Further research needed for downstream task applicability.

Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yolox贸chitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [3] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

TL;DR: GAUGE is a lightweight, logit-based framework for real-time detection of hidden conversational escalation in LLM interactions, addressing implicit harm from emotional reinforcement that traditional toxicity filters miss.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as emotional companions, but repeated emotional reinforcement can cause implicit harm through affective drift that traditional toxicity filters fail to detect. Existing guardrails using external classifiers or clinical rubrics lag behind real-time conversational dynamics.

Method: GAUGE (Guarding Affective Utterance Generation Escalation) uses a lightweight, logit-based framework to measure how an LLM's output probabilistically shifts the affective state of a dialogue in real-time.

Result: The paper proposes a framework for detecting hidden conversational escalation that operates in real-time, addressing the limitations of existing approaches that can't keep up with nuanced conversational dynamics.

Conclusion: GAUGE provides a novel approach to detecting implicit harm in LLM conversations by monitoring affective state shifts probabilistically, offering real-time protection against emotional escalation that traditional methods miss.

Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [4] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

TL;DR: The paper introduces a Confidence-Aware Fine-Grained Debate (CFD) framework where multiple LLM agents simulate human annotators to enrich datasets with real-world indicators, achieving robust performance improvements on mental health and online safety tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world indicators (like life events for mental health analysis and risky behavior for online safety) are important for NLP tasks but are costly and difficult to label in training datasets due to their dynamic nature.

Method: Proposes a Confidence-Aware Fine-Grained Debate (CFD) framework where multiple LLM agents simulate human annotators, exchange fine-grained evidence, and reach consensus. Also introduces two new expert-annotated datasets: mental health Reddit wellbeing dataset and online safety Facebook sharenting risk dataset.

Result: CFD framework achieves the most robust data enrichment performance compared to baselines. Enriched features via debate transcripts yield largest gains, outperforming non-enriched baseline by 10.1% for online safety task. Data enrichment consistently improves downstream tasks.

Conclusion: The CFD framework effectively addresses the challenge of labeling real-world indicators in NLP datasets, demonstrating significant performance improvements for mental health and online safety applications through multi-agent debate-based data enrichment.

Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [5] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

TL;DR: LLM-as-a-Judge approach automatically creates policy-aligned training data for sentence simplification without human annotation, enabling small LLMs to outperform GPT-4o on lexical simplification.


<details>
  <summary>Details</summary>
Motivation: Different applications require distinct simplification policies (lexical vs. full rewriting), but achieving policy-driven control remains challenging without costly human annotation or parallel corpora.

Method: Leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, removing need for human annotation or parallel corpora.

Result: Small-scale open-source LLMs like Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification and achieve comparable performance on overall rewriting, verified by both automatic metrics and human evaluations.

Conclusion: The approach enables building simplification systems that adapt to diverse simplification policies, with consistent improvements across model families and sizes demonstrating robustness.

Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [6] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

TL;DR: LOCUS is a pipeline for low-cost NLP model customization using few-shot data, combining retrieval, synthetic data generation, and parameter-efficient tuning to create memory-optimized models that match full fine-tuning accuracy with minimal resources.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the high cost and resource requirements of fine-tuning large NLP models, seeking to enable efficient model customization with minimal labeled data while maintaining performance comparable to full fine-tuning and large models like GPT-4o.

Method: LOCUS uses a three-step pipeline: 1) Retrieval of pertinent data from a broad repository using few-shot examples, 2) Synthetic data generation via in-context learning to create additional training samples, and 3) Parameter-efficient fine-tuning using either full adaptation or LoRA (low-rank adaptation).

Result: LOCUS outperforms strong baselines including GPT-4o on NER and text classification benchmarks while substantially reducing costs and model sizes. The memory-optimized models retain 99% of fully fine-tuned accuracy with only 5% memory footprint, and beat GPT-4o on several benchmarks with less than 1% of its parameters.

Conclusion: LOCUS demonstrates that efficient NLP model customization is achievable with minimal labeled data through targeted retrieval, synthetic data generation, and parameter-efficient tuning, offering a cost-effective alternative to large models while maintaining competitive performance.

Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [7] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

TL;DR: Two large language models (Mistral Nemo Base 2407 and Llama 2 13B) engage in multi-turn conversations without human intervention, starting from a seed sentence. Conversations initially coherent but eventually fall into repetitive loops where both models produce similar output, demonstrating convergence behavior despite separate training.


<details>
  <summary>Details</summary>
Motivation: To investigate what happens when two large language models communicate with each other autonomously over multiple turns without external input, examining whether they can sustain coherent dialogue or fall into predictable patterns.

Method: Used Mistral Nemo Base 2407 and Llama 2 13B models in a multi-agent setup where each model reads the other's output and generates a response, starting from a short seed sentence and continuing for fixed steps. Applied lexical and embedding-based metrics to measure conversation drift from initial seed and similarity between model outputs over time.

Result: Most conversations start coherently but later fall into repetition, with short phrases appearing and repeating across turns. Once repetition begins, both models tend to produce similar output rather than introducing new directions, leading to loops where same or similar text is produced repeatedly. This convergence occurs despite models being large, trained separately, and not given prompt instructions.

Conclusion: Large language models in autonomous multi-turn conversations exhibit convergence behavior where they fall into repetitive loops with similar outputs, suggesting limitations in their ability to sustain diverse, coherent dialogue without external guidance or intervention.

Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [8] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

TL;DR: Nanbeige4-3B is a high-performing 3B parameter language model that pushes the boundaries of small model scaling through innovative training techniques including FG-WSD scheduler, joint SFT refinement, dual preference distillation, and multi-stage RL.


<details>
  <summary>Details</summary>
Motivation: To extend the scaling law for small language models and demonstrate that small-scale models can achieve performance comparable to much larger models through advanced training methodologies.

Method: Four-stage approach: 1) Pre-training with FG-WSD scheduler for progressive data refinement, 2) SFT with joint deliberative generation refinement and chain-of-thought reconstruction, 3) Dual Preference Distillation using reasoning model, 4) Multi-stage RL with verifiable rewards and preference modeling.

Result: Extensive evaluations show Nanbeige4-3B significantly outperforms comparable-scale models and rivals much larger models across diverse benchmarks.

Conclusion: The paper demonstrates that small language models can achieve exceptional performance through sophisticated training techniques, making high-quality AI more accessible and efficient.

Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [9] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: A lightweight transformer model predicts passage utility scores for multihop QA by considering inter-passage dependencies, using reasoning traces from advanced models for training, leading to improved reranking and QA performance.


<details>
  <summary>Details</summary>
Motivation: Current utility prediction approaches model passage utility independently, ignoring that in multihop QA, passage utility is context-dependent and influenced by relations with other passages (complementary information or crucial links).

Method: Fine-tune a small transformer-based model to predict contextual passage utility scores, leveraging reasoning traces from advanced reasoning models to capture passage usage order and obtain synthetic training data.

Result: Utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods, as demonstrated through comprehensive experiments.

Conclusion: Modeling contextual passage utility with inter-passage dependencies is effective for multihop QA, outperforming independent utility assessment approaches and improving overall system performance.

Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [10] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: Proposes Identify-then-Verify framework for assessing context sufficiency in QA by first identifying missing information through hypothesis generation and consensus, then verifying absence in source text.


<details>
  <summary>Details</summary>
Motivation: Current prompting strategies fail on inferential questions requiring reasoning beyond direct text extraction; need more reliable method to determine if context contains sufficient information to answer questions.

Method: Two-step framework: 1) Identify missing information by generating multiple hypotheses and establishing semantic consensus, 2) Verify by forcing model to re-examine source text to confirm if information is truly absent.

Result: Outperforms established baselines across diverse multi-hop and factual QA datasets; produces more accurate sufficiency judgments while clearly articulating information gaps.

Conclusion: Guiding models to justify claims about missing information through structured reasoning improves reliability of sufficiency assessment in question-answering systems.

Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [11] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

TL;DR: LLMs can effectively classify German texts by CEFR proficiency levels, with new methods outperforming prior approaches using a diverse dataset combining real and synthetic data.


<details>
  <summary>Details</summary>
Motivation: Language proficiency assessment is crucial for personalized education, and automated CEFR classification can enable scalable, tailored instruction for German language learners.

Method: Combined multiple existing CEFR-annotated corpora with synthetic data to create a diverse dataset, then evaluated three approaches: prompt-engineering strategies, fine-tuning LLaMA-3-8B-Instruct, and a probing-based approach using the LLM's internal neural state for classification.

Result: The methods achieved consistent performance improvements over prior approaches, demonstrating the effectiveness of LLMs for reliable CEFR classification.

Conclusion: LLMs show strong potential for reliable and scalable automated CEFR classification of German texts, which could support personalized language education.

Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [12] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

TL;DR: ProSocialAlign: A test-time framework that steers language models toward safe, empathetic responses using lexicographic constrained generation and parameter-efficient steering without retraining.


<details>
  <summary>Details</summary>
Motivation: Current safety approaches fail in emotionally charged or high-stakes settings where refusal-only methods alienate users and naive compliance amplifies risks. Need for context-sensitive safety that maintains empathy while preventing harm.

Method: Two-part framework: (1) Directional regulation - subtracts learned "harm vector" in parameter space to eliminate harmful continuations, (2) Preference-aware autoregressive reward modeling with gradient conflict resolution for fine-grained, user-controllable decoding. Uses lexicographic constrained generation: first hard constraints for safety, then optimization for prosocial quality.

Result: State-of-the-art performance across five safety benchmarks, reduces unsafe leakage, boosts alignment to human values, with strong gains across multiple evaluation metrics.

Conclusion: ProSocialAlign provides a robust, modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time without retraining base models.

Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [13] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

TL;DR: AlignRuScore adapts the AlignScore metric for Russian to evaluate factual consistency in generated text, addressing the lack of Russian evaluation tools.


<details>
  <summary>Details</summary>
Motivation: There is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora, creating a gap for reliable NLP applications in Russian.

Method: Fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets to adapt the AlignScore metric for Russian.

Result: Demonstrated that a unified alignment metric can be successfully ported to Russian, establishing groundwork for robust multilingual factual consistency evaluation.

Conclusion: AlignRuScore successfully bridges the gap for Russian factual consistency evaluation, with released translated corpora, model checkpoints, and code to support further multilingual research.

Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [14] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

TL;DR: Corpus linguistics analysis of online discussions reveals VR, Oculus, and headset as most frequent terms in VR-anxiety discourse, with prepositional phrases indicating design, experience, and development aspects.


<details>
  <summary>Details</summary>
Motivation: VR shows promise in treating anxiety disorders, but understanding public discourse about VR and anxiety can enhance technology efficacy and patient care by revealing user perspectives and concerns.

Method: Used corpus linguistics methodology with Sketch Engine software to analyze English Trends corpus, identifying frequent words and collocations in online discussions about VR and anxiety.

Result: Most frequent terms: VR, Oculus, headset; collocation patterns show prepositional phrases "of virtual reality" (design), "in virtual reality" (experience), "for virtual reality" (development).

Conclusion: Findings provide insights into public discourse on VR-anxiety relationship and suggest pathways for improving counseling support through VR development and accessibility enhancements.

Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [15] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

TL;DR: CMV-Fuse is a cross-modal view fusion framework for ABSA that combines multiple linguistic perspectives (AMR, constituency parsing, dependency syntax, semantic attention) with external knowledge, using hierarchical gated attention and multi-view contrastive learning to improve sentiment analysis.


<details>
  <summary>Details</summary>
Motivation: Current ABSA systems use isolated linguistic views, missing the natural interplay between structural representations that humans leverage for language understanding. The paper aims to emulate human language processing by systematically combining multiple complementary linguistic perspectives.

Method: CMV-Fuse framework orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge. Uses hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, plus structure-aware multi-view contrastive learning for representation consistency.

Result: Extensive experiments show substantial improvements over strong baselines on standard benchmarks. Analysis reveals how each linguistic view contributes to more robust sentiment analysis.

Conclusion: The proposed cross-modal view fusion framework successfully emulates human language processing by combining multiple linguistic perspectives, leading to more effective aspect-based sentiment analysis through systematic integration of structural representations.

Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [16] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

TL;DR: GPT-2's sentiment processing doesn't follow predicted two-stage architecture; early layers detect lexical sentiment but contextual integration happens in late layers through unified mechanism, not mid-layers.


<details>
  <summary>Details</summary>
Motivation: To causally examine how sentiment information is processed across GPT-2's transformer layers and test the hypothesized two-stage sentiment architecture (early lexical detection + mid-layer contextual integration).

Method: Mechanistic interpretability study using systematic activation patching across all 12 layers of GPT-2 to test three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing.

Result: Early layers (0-3) act as lexical sentiment detectors with stable, position-specific polarity signals. However, all three contextual integration hypotheses were falsified - contextual phenomena (negation, sarcasm, domain shifts) are integrated primarily in late layers (8-11) through a unified, non-modular mechanism rather than mid-layer specialization.

Conclusion: GPT-2's sentiment computation differs from predicted hierarchical patterns, highlighting the need for further empirical characterization of contextual integration in large language models.

Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [17] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

TL;DR: PersonaMem-v2 is a state-of-the-art dataset for LLM personalization with 1,000 realistic user-chatbot interactions. The paper shows that reinforcement fine-tuning enables models to outperform frontier LLMs on implicit personalization tasks, and introduces an agentic memory framework that achieves SOTA accuracy with 16x fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Personalization is a key milestone for advancing AI capability and alignment. Current frontier LLMs struggle with implicit personalization where user preferences are not explicitly stated, and there's a need for scalable approaches to handle long conversation histories efficiently.

Method: 1) Created PersonaMem-v2 dataset with 1,000 realistic user-chatbot interactions across 300+ scenarios, 20,000+ user preferences, and 128k-token context windows. 2) Used reinforcement fine-tuning to train Qwen3-4B for implicit personalization. 3) Developed an agentic memory framework that maintains a single, human-readable memory that grows over time instead of using full conversation histories.

Result: Frontier LLMs achieve only 37-48% accuracy on implicit personalization. Reinforcement fine-tuned Qwen3-4B outperforms GPT-5 with 53% accuracy. The agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens (2k-token memory vs. full 32k conversation histories).

Conclusion: The PersonaMem-v2 dataset enables significant advances in LLM personalization. Agentic memory provides a scalable path toward real-world personalized intelligence by efficiently managing user information over time without requiring full conversation histories.

Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [18] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

TL;DR: FlyThinker is an efficient "think-while-generating" framework for personalized long-form generation that enables concurrent reasoning and generation through parallel latent token-level reasoning.


<details>
  <summary>Details</summary>
Motivation: Current preference alignment methods optimize for population-level preferences, overlooking individual users. Existing personalization approaches struggle with implicit preferences, and recent "think-then-generate" methods face challenges in long-form generation due to static one-shot reasoning that must capture all relevant information upfront, limiting adaptability to evolving content.

Method: FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. The reasoning model depends only on previous responses rather than its own prior outputs, preserving training parallelism across different positions.

Result: Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while maintaining training and inference efficiency.

Conclusion: FlyThinker addresses the limitations of existing personalization methods by enabling efficient concurrent reasoning and generation for personalized long-form content, overcoming the challenges of static reasoning in evolving content generation.

Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [19] [TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction](https://arxiv.org/abs/2512.06694)
*Aoi Fujita,Taichi Yamamoto,Yuri Nakayama,Ryota Kobayashi*

Main category: cs.CL

TL;DR: TopiCLEAR: A new topic modeling method using SBERT embeddings, GMM clustering, and adaptive LDA refinement that outperforms baselines on social media and news datasets without requiring preprocessing.


<details>
  <summary>Details</summary>
Motivation: Traditional topic models struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. There's a need for methods that can handle raw social media text without preprocessing.

Method: TopiCLEAR (Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction): 1) Embed text using Sentence-BERT, 2) Provisional clustering with Gaussian Mixture Models, 3) Iterative refinement using supervised projection based on linear discriminant analysis, followed by GMM clustering until convergence. Works directly on raw text without preprocessing.

Result: Evaluated on four datasets (20News, AgNewsTitle, Reddit, TweetTopic) against seven baselines including SBERT-based and zero-shot generative AI methods. Achieved highest similarity to human-annotated topics with significant improvements for both social media posts and online news articles. Produced more interpretable topics in qualitative analysis.

Conclusion: TopiCLEAR effectively addresses challenges of social media topic modeling, outperforming existing methods and producing interpretable topics without preprocessing, showing strong potential for social media and web content analytics applications.

Abstract: Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.

</details>


### [20] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

TL;DR: Proposes a parameter-efficient fine-tuning method with differential privacy that combines gradient clipping, adaptive noise allocation, and low-dimensional projection to protect privacy while maintaining efficiency in instruction tasks.


<details>
  <summary>Details</summary>
Motivation: Address privacy protection and efficiency issues in instruction fine-tuning of large language models, reducing privacy budget consumption while ensuring training stability and robustness in multi-task scenarios.

Method: Parameter-efficient method that freezes backbone model and updates parameters through low-dimensional projection subspace, integrating differential privacy noise allocation with gradient clipping in a collaborative optimization framework.

Result: Outperforms baseline models in accuracy, privacy budget, and parameter efficiency; maintains stable performance under diverse and uncertain data conditions across hyperparameter, environment, and data sensitivity dimensions.

Conclusion: Enriches theoretical integration of differential privacy and parameter-efficient fine-tuning, demonstrates practical adaptability in instruction tasks, and provides feasible solution for secure training in complex instruction environments.

Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [21] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

TL;DR: ImplicitBBQ extends BBQ benchmark to test implicit biases in LLMs using subtle cues rather than explicit protected attribute declarations, revealing significant performance drops that explicit benchmarks miss.


<details>
  <summary>Details</summary>
Motivation: Existing bias benchmarks rely on explicit declaration of protected attributes (religion, race, gender), but real-world biases are often implicit and subtle, creating a blind spot in fairness evaluation.

Method: Extends the Bias Benchmark for QA (BBQ) to create ImplicitBBQ, which uses implicit cues (names, cultural cues, traits) rather than explicit declarations across 6 protected attribute categories.

Result: GPT-4o shows troubling performance disparity on ImplicitBBQ vs explicit BBQ, with accuracy declining up to 7% in "sexual orientation" category and consistent declines across most other categories.

Conclusion: Current LLMs contain implicit biases undetected by explicit benchmarks, and ImplicitBBQ provides a crucial tool for more nuanced fairness evaluation in NLP.

Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [22] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: PDFTEMRA is a compact transformer model for medical NLP in low-resource languages like Hindi, using distillation, frequency modulation, ensemble learning, and random activations to reduce computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: While TL has enabled LLMs for mainstream NLP, deploying large models in resource-constrained healthcare settings remains challenging, especially for visually impaired users and speakers of low-resource languages like Hindi in rural areas.

Method: PDFTEMRA integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns in a compact transformer architecture to reduce computational requirements while preserving language understanding.

Result: PDFTEMRA achieves comparable performance to state-of-the-art NLP models with substantially lower computational requirements on medical question-answering and consultation datasets for Hindi.

Conclusion: PDFTEMRA is suitable for accessible, inclusive, low-resource medical NLP applications, addressing the needs of visually impaired users and speakers of low-resource languages in healthcare settings.

Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [23] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

TL;DR: Prepending semantic prompts to words before embedding significantly improves word similarity correlations for text embedding models, achieving state-of-the-art results without training.


<details>
  <summary>Details</summary>
Motivation: Text embedding models are designed for sentence-level applications and evaluated on sentence-level benchmarks, but their behavior on isolated words is less understood. The paper aims to explore how simple prompting can improve word-level performance.

Method: Tested 7 text embedding models on 3 standard word similarity benchmarks (SimLex-999, WordSim-353, MEN-3000). Simply prepended semantic prompts like "meaning: {word}" or "Represent the semantic concept: {word}" to words before embedding, then measured Spearman correlations.

Result: Prompts improved Spearman correlations by up to +0.29 on SimLex-999. Some models failed completely on bare words (correlation = 0) but recovered with prompts (+0.73 improvement). Best results: correlation = 0.692 on SimLex-999 with Cohere's embed-english-v3.0, 0.811 on WordSim-353, and 0.855 on MEN-3000 with OpenAI's text-embedding-3-large, outperforming classic static embeddings like Word2Vec (0.40) and LexVec (0.48).

Conclusion: Simple zero-shot prompting technique substantially improves word similarity correlations for text embedding models, establishing new state-of-the-art for pure embedding methods without requiring any training.

Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [24] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

TL;DR: LWE framework enables LLM evaluators to improve sequentially during inference by maintaining evolving meta-prompts with sample-specific instructions and self-generated feedback, with selective updates focusing on inconsistent cases for cost-effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-judge evaluators treat each case independently without accumulating experience, and use fixed prompts for all cases, missing sample-specific evaluation criteria.

Method: Learning While Evaluating (LWE) framework with evolving meta-prompt that generates sample-specific evaluation instructions and refines itself through self-generated feedback. Selective LWE variant updates meta-prompt only on self-inconsistent cases.

Result: Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, showing evaluators can improve during sequential testing with selective updates, learning most from cases they struggle with.

Conclusion: LLM evaluators can effectively improve through sequential learning during inference without training/validation sets, with selective updates providing cost-effective learning from challenging cases.

Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [25] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: NBDiff adapts autoregressive LLMs to block-wise diffusion models via a principled pathway, achieving state-of-the-art 7B-class DLM performance without costly from-scratch training.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding creates throughput bottlenecks due to sequential generation. While diffusion language models enable parallel generation, training large DLMs from scratch is expensive and wastes knowledge from existing AR checkpoints. Prior adaptation methods fail to address the fundamental mismatch between AR causality and block-wise bidirectionality.

Method: Reframes adaptation as intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Uses context-causal attention mask (causal in context, bidirectional within block), efficient parallel adaptation, auxiliary AR loss for knowledge retention, and gradual block size increment. Integrates cleanly with masked block-diffusion.

Result: NBDiff-7B (Base and Instruct) achieves state-of-the-art performance among 7B-class DLMs, showing strong gains on general-knowledge, math, and code benchmarks. Inherits long-context modeling and reasoning capabilities from AR checkpoints.

Conclusion: Principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch, enabling parallel generation while leveraging existing AR knowledge.

Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [26] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

TL;DR: LLM4SFC is the first framework that generates executable Sequential Function Charts (SFCs) from natural language descriptions, addressing the gap in LLM-based generation of graphical PLC programming languages.


<details>
  <summary>Details</summary>
Motivation: While LLMs are increasingly used for generating textual PLC languages like Structured Text, graphical languages like SFCs remain underexplored. Generating SFCs is challenging due to their graphical nature and embedded ST actions, often resulting in non-executable code incompatible with industrial toolchains.

Method: LLM4SFC uses three components: (1) a reduced structured representation capturing essential topology and inline ST with reduced verbosity, (2) fine-tuning and few-shot retrieval-augmented generation for alignment with SFC conventions, and (3) structured generation that prunes illegal tokens in real-time to ensure compliance with SFC textual format.

Result: The framework achieves 75%-94% generation success rate on real-world SFCs from automated manufacturing projects, reliably generating syntactically valid SFC programs that bridge graphical and textual PLC languages.

Conclusion: LLM4SFC effectively generates executable SFCs from natural language, paving the way for automated industrial programming by addressing the unique challenges of graphical PLC language generation.

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [27] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

TL;DR: LLMs show promise for automating discharge summary generation, with proprietary models (Gemini, GPT) outperforming open-source ones (Mistral, Llama 2) in generating clinically useful summaries.


<details>
  <summary>Details</summary>
Motivation: Automating discharge summary generation could reduce healthcare professional workload, minimize errors, and make critical patient information more accessible and actionable.

Method: Evaluated five LLMs (Mistral, Llama 2, GPT-3, GPT-4, Gemini 1.5 Pro) using MIMIC-III data with exact-match, soft-overlap, and reference-free metrics, plus human clinical expert evaluation.

Result: Proprietary models, especially Gemini with one-shot prompting, produced summaries most similar to gold-standard references. Open-source models (particularly Mistral after fine-tuning) showed promise but struggled with hallucinations and repetition.

Conclusion: LLMs, especially proprietary models, are promising for automatic discharge summary generation, though challenges like hallucinations and data privacy need to be addressed.

Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [28] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

TL;DR: CAuSE is a framework that generates faithful natural language explanations for multimodal classifiers using causal abstraction and interchange interventions.


<details>
  <summary>Details</summary>
Motivation: Multimodal classifiers are opaque black boxes, and existing explanation methods lack intuitive natural language explanations that faithfully capture the model's internal decision-making behavior.

Method: Proposes CAuSE framework using causal abstraction under simulated explanations, trained via interchange intervention to form a causal abstraction of the underlying classifier.

Result: CAuSE generalizes across datasets and models, surpasses other methods on causal faithfulness metrics, with qualitative analysis showing advantages and detailed error analysis identifying failure cases.

Conclusion: CAuSE provides a novel approach to generating faithful natural language explanations for multimodal classifiers through causal abstraction, improving interpretability and trust in black box models.

Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [29] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

TL;DR: AquaFusionNet is a lightweight cross-modal framework that unifies microscopic imaging and physicochemical sensors for real-time microbial contamination detection in small-scale drinking water systems, achieving high accuracy with low power consumption.


<details>
  <summary>Details</summary>
Motivation: Existing monitoring tools for small-scale drinking water systems only capture fragments of rapidly fluctuating microbial contamination. Operators must interpret microscopic imaging and physicochemical sensor data separately, making real-time decision-making unreliable.

Method: AquaFusionNet uses a gated cross-attention mechanism to learn statistical dependencies between microbial appearance and concurrent sensor dynamics. It's trained on AquaMicro12K dataset (12,846 annotated micrographs) and deployed on low-power edge hardware (Jetson Nano).

Result: In 6-month deployment across 7 facilities in East Java, Indonesia: processed 1.84M frames, achieved 94.8% mAP@0.5 detection and 96.3% anomaly prediction accuracy at 4.8W power. Outperformed lightweight detectors and reduced failure modes under challenging conditions.

Conclusion: AquaFusionNet provides reliable real-time contamination detection by unifying microscopic and sensor data, offering higher accuracy at lower power than unimodal approaches. Open release of models, data, and designs supports adaptation in decentralized water safety infrastructures.

Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [30] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

TL;DR: Rhea is a novel framework that addresses cumulative contextual decay in multi-turn LLM conversations by decoupling history into Instructional and Episodic Memory modules, improving accuracy by 1.04 points (16% gain) while maintaining high instruction fidelity.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well on single-turn tasks but suffer performance degradation in multi-turn conversations due to cumulative contextual decay - progressive degradation of contextual integrity caused by attention pollution, dilution, and drift.

Method: Rhea (Role-aware Heuristic Episodic Attention) decouples conversation history into two memory modules: (1) Instructional Memory (IM) that persistently stores high-fidelity global constraints via structural priority mechanism, and (2) Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, it applies priority attention to construct high signal-to-noise context by selectively integrating episodic information while prioritizing global instructions.

Result: Experiments on multiple multi-turn conversation benchmarks (MT-Eval and Long-MT-Bench+) show Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (16% relative gain over strong baselines). It maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions.

Conclusion: Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs by addressing cumulative contextual decay through role-aware memory decoupling and priority attention mechanisms.

Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [31] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

TL;DR: LLMs struggle to accurately simulate diverse user opinions in survey responses despite methods like CLAIMSIM that increase response diversity.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used to simulate user opinions, but RLHF-trained models exhibit biases toward dominant viewpoints, raising concerns about their ability to represent diverse demographic and cultural backgrounds.

Method: Examined LLM simulation of human responses through direct prompting and chain-of-thought prompting, plus proposed CLAIMSIM method that elicits viewpoints from LLM parametric knowledge as contextual input.

Result: CLAIMSIM produces more diverse responses but both approaches struggle to accurately simulate users. Key limitations: LLMs maintain fixed viewpoints across demographics and generate single-perspective claims; they struggle to reason over nuanced demographic differences when presented with conflicting claims.

Conclusion: Current LLM prompting methods have significant limitations in simulating diverse user opinions, particularly in adapting responses to specific demographic profiles and handling nuanced differences between conflicting viewpoints.

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [32] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

TL;DR: Automated method to select minimal yet comprehensive PRO-CTCAE subsets using MedDRA semantic mapping and spectral analysis to balance signal coverage against patient burden.


<details>
  <summary>Details</summary>
Motivation: Current PRO-CTCAE item selection for oncology trials is challenging - selecting too many items burdens patients and reduces compliance, while too few may miss important safety signals. There's a need for an objective, automated approach to optimize this selection process.

Method: Map PRO-CTCAE symptoms to MedDRA Preferred Terms, encode into Safeterm semantic space, score relevance to historical adverse event data, combine with incidence into utility function, apply spectral analysis to identify orthogonal medical concepts, and rank-order symptoms by importance with suggested cut-off based on explained information.

Result: Implemented as part of Safeterm trial-safety app, evaluated through simulations and oncology case studies where PRO-CTCAE was employed. The method provides objective, reproducible selection balancing coverage and burden.

Conclusion: Automated approach streamlines PRO-CTCAE design by leveraging MedDRA semantics and historical data, offering systematic method to optimize patient-reported outcome measurement in oncology trials.

Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [33] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

TL;DR: LLMs create dual challenges for forensic linguistics: they enable powerful authorship analysis tools but also undermine traditional idiolect assumptions through style mimicry and synthetic text generation, requiring methodological adaptation for legal admissibility.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the dual role of LLMs in forensic linguistics - as analytical tools for authorship attribution and as destabilizing forces that undermine traditional assumptions about idiolect through style mimicry, authorship obfuscation, and synthetic text proliferation.

Method: The paper analyzes current AI-text detection techniques including classifier-based approaches, stylometric methods, and watermarking, identifying their limitations such as high false positive rates for non-native English writers and vulnerability to adversarial strategies like homoglyph substitution.

Result: Current detection methods face substantial limitations and uncertainties that raise concerns under legal admissibility standards (Daubert and Kumho Tire frameworks), indicating forensic linguistics requires methodological reconfiguration to remain scientifically credible.

Conclusion: Forensic linguistics needs methodological adaptation including hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations, while maintaining the core insight that language reveals information about its producer.

Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [34] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

TL;DR: IXAM is an interactive explainability framework that helps users explore and understand authorship attribution models by visualizing embedding spaces and constructing multi-granularity writing style explanations.


<details>
  <summary>Details</summary>
Motivation: Authorship attribution models are often black-box systems that lack interpretability. Users need better ways to understand why these models make specific authorship predictions and what writing style features drive those decisions.

Method: IXAM provides an interactive framework that allows users to explore the embedding space of authorship attribution models. Users can construct explanations as sets of writing style features at different levels of granularity through interactive visualization and exploration tools.

Result: User evaluation demonstrates that IXAM provides more value than predefined stylistic explanations, offering better understanding and interpretability of authorship attribution model predictions.

Conclusion: Interactive explainability frameworks like IXAM significantly improve the interpretability of authorship attribution models by enabling users to explore embedding spaces and construct multi-level stylistic explanations, outperforming static explanation approaches.

Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [35] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanho茅 Botcazou,Tassadit Amghar,Sylvain Lamprier,Fr茅d茅ric Saubion*

Main category: cs.CL

TL;DR: The paper introduces Progress Ratio Embeddings (PRE) for robust length control in text generation, addressing limitations of previous methods like Reverse Positional Embeddings that fail beyond training distribution.


<details>
  <summary>Details</summary>
Motivation: Current neural language models lack precise control over generation length, and existing methods like Reverse Positional Embeddings become unstable when controlling for lengths beyond the training distribution, especially when using discrete countdown signals.

Method: Introduces Progress Ratio Embeddings (PRE) - continuous embeddings tied to a trigonometric impatience signal that integrates seamlessly into standard Transformer architectures. PRE provides stable length control without degrading text quality.

Result: PRE provides robust length fidelity without degrading text accuracy under standard evaluation metrics. It generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

Conclusion: Progress Ratio Embeddings offer a stable and effective solution for length control in text generation, overcoming limitations of previous methods and maintaining text quality while enabling precise length control even for unseen target lengths.

Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [36] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

TL;DR: PICEPR introduces a novel "Prompting-in-a-Series" algorithm with two pipelines (Contents and Embeddings) that uses modular decoder-only LLMs for personality recognition, achieving 5-15% SOTA improvement.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown strong capabilities in NLP tasks, but there's a need for better personality recognition methods. The research aims to leverage LLMs' content generation and summarization abilities to enhance personality recognition as both feature extractors and content generators.

Method: PICEPR algorithm features two pipelines: (a) Contents pipeline for generating personality-rich content, and (b) Embeddings pipeline for extracting personality features. Uses modular decoder-only LLMs that can summarize or generate content. Evaluated on both closed-source (GPT-4o, Gemini) and open-source (Mistral) models.

Result: Achieved new state-of-the-art performance for personality recognition with 5-15% improvement. Compared content quality across different LLMs (GPT-4o, Gemini, Mistral). Provided experimental evidence justifying the PICEPR algorithm's rationale.

Conclusion: PICEPR demonstrates that modular decoder-only LLMs can effectively serve as personality feature extractors and content generators, significantly advancing personality recognition performance. The algorithm and models are publicly available.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [37] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

TL;DR: FVA-RAG introduces a falsification-based retrieval framework that actively searches for contradictory evidence to combat retrieval sycophancy in RAG systems, reducing hallucinations from false premises.


<details>
  <summary>Details</summary>
Motivation: Standard RAG systems suffer from "Retrieval Sycophancy" - when queries contain false premises or misconceptions, vector retrievers fetch documents that align with user bias rather than objective truth, causing models to "hallucinate with citations."

Method: FVA-RAG shifts from inductive verification to deductive falsification. It uses an Adversarial Retrieval Policy to generate "Kill Queries" that actively search for contradictory evidence, then employs dual-verification to weigh draft answers against this "Anti-Context."

Result: Preliminary experiments on common misconception datasets show FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines.

Conclusion: FVA-RAG effectively acts as an inference-time "Red Team" for factual generation by actively seeking disproof rather than just supporting evidence, addressing a critical vulnerability in current RAG architectures.

Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [38] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

TL;DR: Most frontier LLMs remain highly vulnerable to multi-turn adversarial attacks despite safety alignment, with attack success rates up to 100%, though some models show meaningful resistance and extended reasoning significantly reduces vulnerability.


<details>
  <summary>Details</summary>
Motivation: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and it's unknown whether model scale or inference mode affects robustness.

Method: Used TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers.

Result: Six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%.

Conclusion: Current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, but thinking mode provides a deployable safety enhancement, with deliberative inference identified as a promising defense direction.

Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [39] [SETUP: Sentence-level English-To-Uniform Meaning Representation Parser](https://arxiv.org/abs/2512.07068)
*Emma Markle,Javier Gutierrez Bach,Shira Wein*

Main category: cs.CL

TL;DR: This paper introduces two methods for English text-to-UMR parsing, with the best model (SETUP) achieving state-of-the-art performance scores.


<details>
  <summary>Details</summary>
Motivation: UMR is a promising semantic representation for cross-linguistic applications, but its downstream potential is limited without accurate automatic parsers. Prior work on text-to-UMR parsing is insufficient.

Method: Two approaches: 1) fine-tuning existing Abstract Meaning Representation parsers, and 2) leveraging a converter from Universal Dependencies. Both methods build on prior work as a baseline.

Result: The best-performing model (SETUP) achieves an AnCast score of 84 and a SMATCH++ score of 91, showing substantial improvements in automatic UMR parsing.

Conclusion: The paper demonstrates significant progress in text-to-UMR parsing, enabling broader applications of UMR for language documentation, low-resource language technologies, and interpretability.

Abstract: Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.

</details>


### [40] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

TL;DR: SAGE is a new benchmark for evaluating LLMs' cross-cultural understanding through scenario-based tasks, addressing limitations in existing benchmarks by incorporating contextual scenarios, cross-cultural concept mapping, and deep cultural reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating LLMs' cross-cultural understanding have three key limitations: lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. There's a need for better evaluation tools to assess whether LLMs genuinely possess cross-cultural understanding.

Method: Built SAGE benchmark via cross-cultural core concept alignment and generative task design. Grounded in cultural theory, categorized capabilities into nine dimensions. Curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios organized under four broader cross-cultural situation categories, following established item design principles.

Result: SAGE dataset supports continuous expansion and experiments confirm its transferability to other languages. The benchmark reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. LLMs still have significant gaps in achieving truly nuanced cross-cultural understanding.

Conclusion: While progress has been made, LLMs are still far from reaching truly nuanced cross-cultural understanding. SAGE provides a comprehensive benchmark to evaluate and improve cross-cultural capabilities in LLMs, with data and code available in supplement materials and planned for public release in future versions.

Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [41] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

TL;DR: Token Filtering is an online structured pruning technique for LLMs that skips redundant attention computations during inference without calibration data, using joint key-value similarity to measure token redundancy.


<details>
  <summary>Details</summary>
Motivation: Existing pruning approaches for LLMs suffer from instability due to reliance on offline calibration data that may not generalize across inputs. There's a need for lightweight pruning techniques that work directly during inference without calibration data.

Method: Token Filtering measures token redundancy via joint key-value similarity and skips redundant attention computations during inference. It uses a variance-aware fusion strategy that adaptively weights key and value similarity across attention heads to ensure informative tokens are retained even under high pruning ratios.

Result: Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) show Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks like MMLU even with 50% pruning.

Conclusion: Token Filtering provides a stable, lightweight online pruning technique that reduces LLM inference costs without calibration data, maintaining model accuracy through joint key-value similarity measurement and adaptive fusion strategies.

Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [42] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: DART is a multi-agent framework that uses disagreements between visual agents to identify and call specialized visual tools (object detection, OCR, spatial reasoning, etc.) to resolve debates, improving performance over existing multi-agent and tool-calling methods.


<details>
  <summary>Details</summary>
Motivation: Specialized visual tools can enhance LLMs/VLMs with expert knowledge, but determining which tools to call and when is challenging. Current approaches lack effective mechanisms for tool selection during multi-agent discussions.

Method: DART uses disagreements between multiple debating visual agents to identify useful visual tools. These tools introduce new information and provide tool-aligned agreement scores to highlight agents aligned with expert tools. An aggregator agent selects the best answer using agent outputs and tool information.

Result: DART improves over multi-agent debate and single-agent tool-calling frameworks, beating the next-strongest baseline by 3.4% on A-OKVQA and 2.4% on MMMU. It adapts well to new tools with 1.3% improvement on M3D medical dataset. Analysis shows rich discussion and diverse tool usage to resolve disagreements.

Conclusion: DART effectively leverages inter-agent disagreements to identify and utilize specialized visual tools, facilitating productive multi-agent discussions and improving performance across diverse benchmarks including applied domains like medical imaging.

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [43] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: GUMBridge is a new English bridging anaphora resource covering 16 diverse genres with detailed subtype annotations, showing LLMs still struggle with bridging resolution tasks.


<details>
  <summary>Details</summary>
Motivation: Existing bridging anaphora resources are limited in size, coverage of the phenomenon, and genre diversity, creating a need for a more comprehensive dataset.

Method: Created GUMBridge resource with 16 diverse English genres, providing granular annotations for bridging subtype categorization, and evaluated annotation quality.

Result: Baseline evaluation using contemporary LLMs shows bridging resolution and subtype classification remain challenging NLP tasks despite LLM advancements.

Conclusion: GUMBridge provides a comprehensive resource for bridging anaphora research, highlighting that bridging remains a difficult task for current LLMs and offering a benchmark for future work.

Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [44] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

TL;DR: MASim is the first multilingual agent-based simulation framework for studying cross-lingual social interactions, enabling global public opinion modeling and media influence analysis through diverse sociolinguistic agents.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent simulations are mostly monolingual and fail to model cross-lingual interactions, which are essential for studying real-world social behavior and computational social science.

Method: Developed MASim framework supporting multi-turn interactions among generative agents with diverse sociolinguistic profiles, and created MAPS benchmark combining survey questions and demographic personas from global population distributions.

Result: Experiments show MASim reproduces sociocultural phenomena and demonstrates the importance of multilingual simulation for scalable, controlled computational social science through calibration, sensitivity, consistency, and cultural case studies.

Conclusion: MASim enables multilingual agent-based social simulations that capture cross-lingual interactions essential for realistic modeling of global societies, advancing computational social science research.

Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [45] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

TL;DR: NeSTR is a neuro-symbolic framework that combines symbolic temporal representations with reflective reasoning to improve LLMs' temporal reasoning without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with temporal reasoning under complex constraints. Existing approaches either underutilize LLMs' reasoning capabilities (symbolic methods) or lack structured temporal representations (reflective methods), leading to inconsistent or hallucinated reasoning even when correct temporal context is available.

Method: NeSTR integrates structured symbolic representations with hybrid reflective reasoning. It preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection.

Result: Extensive experiments on diverse temporal QA benchmarks show NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning.

Conclusion: NeSTR demonstrates the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models, effectively addressing limitations of existing approaches.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [46] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: Proposes TreeED and ForestED frameworks that use LLMs to induce decision trees for error detection in tabular data, improving explainability and robustness over direct LLM labeling approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based error detection methods lack explainability (black-box decisions) and robustness (sensitive to prompts, inconsistent outputs). Need for transparent, reliable error detection in tabular data.

Method: 1) TreeED: Uses LLM to induce decision tree skeleton with rule nodes (simple checks), GNN nodes (complex patterns), and leaf nodes (final decisions). 2) ForestED: Ensemble approach using uncertainty-based sampling to create multiple decision trees, then EM algorithm to estimate tree reliability and optimize consensus prediction.

Result: Achieves 16.1% average F1-score improvement over best baseline. Methods demonstrate accuracy, explainability (transparent decision paths), and robustness (consistent outputs).

Conclusion: LLM-as-an-inducer framework (TreeED/ForestED) effectively addresses limitations of LLM-as-a-labeler approach, providing explainable and robust error detection for tabular data through decision tree induction and ensemble methods.

Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [47] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: This paper introduces a Telugu-English speech translation benchmark using 46 hours of verified data, compares cascaded vs. end-to-end architectures, finds end-to-end can be competitive with less data, and shows traditional metrics outperform BERTScore for this language pair.


<details>
  <summary>Details</summary>
Motivation: Telugu is spoken by over 80 million people but speech translation research for this morphologically rich language is severely underexplored, creating a significant research gap that needs to be addressed.

Method: Developed a Telugu-English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Systematically compared cascaded (IndicWhisper + IndicMT) versus end-to-end (finetuned SeamlessM4T) architectures, and evaluated metric reliability using BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments.

Result: IndicWhisper + IndicMT achieved highest performance due to extensive Telugu-specific training data, but finetuned SeamlessM4T models demonstrated remarkable competitiveness despite using significantly less Telugu-specific data. Traditional metrics provided better quality discrimination than BERTScore for Telugu-English translation.

Conclusion: End-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours). The work provides a reproducible benchmark, evidence of competitive end-to-end performance potential, and practical guidance for automatic evaluation in morphologically complex language pairs.

Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [48] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: Cross-lingual continuous pretraining with unlabeled speech data enables effective ASR for low-resource Perso-Arabic languages using only 300M parameters, outperforming much larger models like Whisper Large v3.


<details>
  <summary>Details</summary>
Motivation: Low-resource languages face fundamental constraints due to scarcity of labeled data and computational resources needed for state-of-the-art ASR models. There's a need for inclusive speech technology that doesn't depend on massive infrastructure or proprietary datasets.

Method: Systematic investigation of cross-lingual continuous pretraining using Perso-Arabic languages as case study. Constructed 3,000-hour multilingual corpus via scalable unlabeled data collection pipeline. Employed targeted continual pretraining combined with morphologically-aware tokenization to develop 300M parameter model.

Result: The 300M parameter model achieves performance comparable to systems 5 times larger. Outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data.

Conclusion: Challenges the prevailing assumption that ASR quality scales primarily with model size. Reveals that data relevance and strategic pretraining are more critical factors for low-resource scenarios. Provides practical pathway toward inclusive speech technology for underrepresented languages.

Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [49] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

TL;DR: Training LLMs with pseudo-faithful one-word explanations improves faithfulness across tasks and styles, with generalization to multi-word settings and unseen tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate unfaithful self-explanations, and it's unclear how to improve faithfulness or whether improvements generalize across different explanation styles.

Method: Used feature attribution to create pseudo-faithful one-word explanations, then performed continual learning on instruction-tuned models across three classification tasks and three explanation styles.

Result: Training improved self-explanation faithfulness across all tasks and styles, with generalization to multi-word settings and unseen tasks, plus consistent cross-style generalization.

Conclusion: Training with constrained faithful explanations can broadly improve LLMs' faithful self-explanation ability, suggesting potential for more reliable model explanations.

Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [50] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

TL;DR: Hybrid methodology for building multilingual corpus to study emerging HSS concepts like "non-technological innovation" using company websites and annual reports, with ML-ready dataset creation.


<details>
  <summary>Details</summary>
Motivation: To support the study of emerging concepts in humanities and social sciences (HSS) by creating a reproducible and extensible multilingual corpus resource.

Method: Hybrid approach combining: (1) textual content from company websites (French/English), (2) annual reports filtered by documentary criteria. Processing includes language detection, content filtering, segment extraction, metadata enrichment, and creation of ML dataset with contextual blocks and thematic annotations.

Result: Created a reproducible multilingual corpus and derived English dataset for machine learning with contextual blocks (5 sentences) annotated by thematic categories, suitable for lexical analysis and NLP applications.

Conclusion: The methodology provides a flexible resource for analyzing lexical variability of emerging concepts and generating datasets for supervised classification tasks in NLP.

Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [51] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: Fine-tuning language models to use Prolog as an external verification tool improves reasoning reliability and auditability through reinforcement learning with structured rewards and agentic inference protocols.


<details>
  <summary>Details</summary>
Motivation: Language models often produce plausible but incorrect reasoning that's hard to verify, creating safety risks for agentic AI systems. The paper aims to improve reliability by grounding model reasoning in formal verification systems.

Method: Fine-tuned Qwen2.5-3B-Instruct using Group Relative Policy Optimization (GRPO) on GSM8K-Prolog-Prover dataset, varying prompt structure, reward composition (execution, syntax, semantics, structure), and inference protocols (single-shot, best-of-N, agentic modes with internal/external Prolog).

Result: Reinforcement learning outperformed supervised fine-tuning, with the 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Best-of-N with external Prolog verification maximized GSM8K accuracy, while agentic inference with internal repair yielded superior zero-shot generalization on MMLU-Stem and MMLU-Pro.

Conclusion: Grounding model reasoning in formal verification systems like Prolog substantially improves reliability and auditability for safety-critical applications, with joint tuning of prompt, reward, and inference shaping program syntax and logic effectively.

Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [52] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

TL;DR: A 3.8B parameter model called Persian-Phi demonstrates that robust multilingual capabilities can be achieved without massive models or multilingual baselines by adapting Microsoft's monolingual English Phi-3 Mini to Persian through resource-efficient curriculum learning.


<details>
  <summary>Details</summary>
Motivation: The democratization of AI is hindered by high computational costs for training LLMs for low-resource languages. Current approaches often assume massive model sizes or multilingual baselines are necessary for robust multilingual capabilities.

Method: A novel curriculum learning pipeline: 1) "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings before heavy training, 2) continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT) to adapt the monolingual English Phi-3 Mini model to Persian.

Result: Persian-Phi achieves competitive results on the Open Persian LLM Leaderboard in HuggingFace, demonstrating that compact models can achieve robust multilingual capabilities with minimal hardware resources.

Conclusion: The approach provides a validated, scalable framework for extending state-of-the-art LLMs to underrepresented languages with minimal hardware resources, challenging assumptions about model size requirements for multilingual capabilities.

Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [53] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

TL;DR: NPR is a teacher-free framework that enables LLMs to develop native parallel reasoning capabilities through self-distilled training, parallel-aware policy optimization, and engine refactoring, achieving significant performance gains and inference speedups with 100% genuine parallel execution.


<details>
  <summary>Details</summary>
Motivation: Current LLMs typically use sequential emulation for reasoning tasks, which is inefficient. The paper aims to transform LLMs from sequential emulation to native parallel cognition to improve both reasoning performance and inference speed.

Method: Three key innovations: 1) Self-distilled progressive training paradigm transitioning from format discovery to topological constraints, 2) Parallel-Aware Policy Optimization (PAPO) algorithm optimizing branching policies within execution graphs, and 3) NPR Engine refactoring memory management and flow control of SGLang for stable parallel RL training.

Result: On eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines, NPR demonstrates 100% genuine parallel execution without falling back to autoregressive decoding.

Conclusion: NPR establishes a new standard for self-evolving, efficient, and scalable agentic reasoning by enabling LLMs to develop genuine parallel reasoning capabilities without external supervision, outperforming previous approaches that often revert to sequential processing.

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [54] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

TL;DR: PRS and VSPO improve Agentic RL for tool-using LLMs: PRS provides progressive dense rewards, VSPO enhances sampling and stabilizes training, leading to better performance on QA tasks.


<details>
  <summary>Details</summary>
Motivation: Agentic RL for tool-using LLMs faces two key challenges: sparse binary rewards provide poor guidance for intermediate steps, and GRPO suffers from gradient degradation when identical rewards within rollout groups yield zero advantage, reducing sample efficiency and destabilizing training.

Method: Two complementary techniques: 1) Progressive Reward Shaping (PRS) - curriculum-inspired reward design with dense, stage-wise feedback (first master tool call formatting, then optimize for factual correctness). 2) Value-based Sampling Policy Optimization (VSPO) - enhanced GRPO variant that replaces low-value samples with prompts selected by task-value metric (balancing difficulty and uncertainty) and applies value-smoothing clipping.

Result: Experiments on multiple short-form and long-form QA benchmarks show PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines.

Conclusion: PRS and VSPO together yield LLM-based Tool-Integrated Reasoning agents that generalize better across domains by addressing both reward sparsity and gradient degradation issues in Agentic RL.

Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [55] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

TL;DR: SPAD introduces a fine-grained attribution method that decomposes token generation into seven sources to detect hallucinations in RAG systems, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing approaches treat hallucinations as binary conflicts between internal knowledge and retrieved context, but this oversimplifies the generative process which involves multiple components like user queries, past tokens, current tokens, and LayerNorm adjustments.

Method: SPAD mathematically attributes each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. It then aggregates these scores by POS tags to quantify how different components drive specific linguistic categories, identifying anomalies to detect hallucinations.

Result: Extensive experiments demonstrate that SPAD achieves state-of-the-art performance in detecting hallucinations in Retrieval-Augmented Generation systems.

Conclusion: By moving beyond binary attribution to a multi-source decomposition approach, SPAD provides a more comprehensive framework for understanding and detecting hallucinations in RAG systems.

Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [56] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Bj枚rn Deiseroth*

Main category: cs.CL

TL;DR: LIME (Linguistic Metadata Embeddings) enhances token embeddings with syntax/semantic metadata, improving pre-training efficiency by 56% faster adaptation with minimal overhead, while boosting language modeling and generative performance across model scales.


<details>
  <summary>Details</summary>
Motivation: High-quality pre-training data is becoming scarce, and while metadata is used for dataset curation, its potential as a direct training signal remains underexplored. The authors challenge this status quo by proposing to leverage metadata more directly for model improvement.

Method: LIME enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. It introduces only 0.01% additional parameters with negligible compute overhead. A variant called LIME+1 uses shifted metadata to guide token generation by providing prior metadata for the next token.

Result: LIME adapts up to 56% faster to training data distribution, improves tokenization, and enhances language modeling capabilities and generative task performance across model scales (500M to 2B). LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

Conclusion: Metadata can serve as a powerful direct training signal beyond just dataset curation. LIME demonstrates that enriching token embeddings with linguistic metadata significantly improves pre-training efficiency and model capabilities, while LIME+1 shows metadata can effectively guide token generation for reasoning tasks.

Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [57] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: Extends RoPE by incorporating the imaginary component of complex dot products to improve long-context modeling in LLMs.


<details>
  <summary>Details</summary>
Motivation: Standard RoPE implementations discard the imaginary component of complex-valued dot products, losing valuable phase information that could enhance modeling of long-context dependencies.

Method: Proposes an extension that re-incorporates the discarded imaginary component, creating a dual-component attention score using the full complex-valued representation.

Result: The method consistently improves performance over standard RoPE on long-context language modeling benchmarks, with benefits increasing as context length grows.

Conclusion: Incorporating the imaginary component of RoPE's complex dot products enhances positional information preservation and improves long-context dependency modeling in LLMs.

Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [58] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

TL;DR: First document-level cross-lingual semantic difference recognition dataset (SwissGov-RSD) with 224 multi-parallel English-German/French/Italian documents and token-level annotations, showing current LLMs and encoder models perform poorly on this naturalistic task.


<details>
  <summary>Details</summary>
Motivation: Semantic difference recognition across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment, but has received little attention as a standalone task.

Method: Introduce SwissGov-RSD dataset with 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations. Evaluate various open-source and closed-source LLMs and encoder models across different fine-tuning settings on this benchmark.

Result: Current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models.

Conclusion: There is a significant performance gap in cross-lingual document-level semantic difference recognition, highlighting the need for better models and approaches for this important task. The dataset and code are made publicly available.

Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [59] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

TL;DR: MBR decoding outperforms MAP for error span detection in MT evaluation by using similarity metrics as utility functions, with distillation to reduce computational cost.


<details>
  <summary>Details</summary>
Motivation: Current generative ESD methods use MAP decoding assuming model probabilities perfectly correlate with human annotation similarity, but dissimilar annotations can achieve higher likelihood than human annotations.

Method: Apply Minimum Bayes Risk (MBR) decoding to generative ESD models using sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on approximate similarity to human annotation.

Result: MBR decoding outperforms MAP baseline at system, sentence, and span-levels. MBR distillation enables standard greedy models to match MBR performance while eliminating inference-time latency.

Conclusion: MBR decoding addresses the limitations of MAP for ESD by better aligning with human annotation similarity, with distillation providing a practical solution for computational efficiency.

Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [60] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

TL;DR: Most previously reported sound symbolic patterns in basic vocabulary are not robust when controlling for genealogical and areal dependencies, though a small subset remains stable.


<details>
  <summary>Details</summary>
Motivation: To test the reproducibility and robustness of reported sound symbolic patterns in basic vocabulary, addressing concerns about biases in study samples, inadequate controls for genealogical and areal dependencies, and lack of explicit reproducibility testing.

Method: Replicated a recent sound symbolism study using a much larger sample (2864 languages from Lexibank vs original 245 languages), modified the original model by adding statistical controls for spatial and phylogenetic dependencies between languages.

Result: Most previously observed sound symbolic patterns disappeared when adding genealogical and areal controls; only a small number of patterns remained highly stable across the larger sample.

Conclusion: Universal claims about sound symbolism need rigorous testing for robustness across various levels, and most reported patterns may be artifacts of uncontrolled genealogical and areal dependencies rather than true universal sound symbolic patterns.

Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


### [61] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

TL;DR: MoCoRP is a framework that improves persona-based dialogue by explicitly modeling NLI relations between persona sentences and responses, enhancing persona consistency and engagement.


<details>
  <summary>Details</summary>
Motivation: Existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, making it difficult for models to effectively capture and utilize persona information for coherent personality generation.

Method: MoCoRP incorporates explicit NLI relations using an NLI expert to extract relations between persona sentences and responses, enabling better persona incorporation. Applied to BART and extended to LLMs through alignment tuning.

Result: Outperforms existing baselines on ConvAI2 and MPChat datasets, achieving superior persona consistency and engaging dialogue generation. Shows improvements in both quantitative metrics and qualitative aspects.

Conclusion: Explicitly modeling persona-response relations is effective for persona-based dialogue, enhancing both consistency and engagement in dialogue systems.

Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [62] [Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries](https://arxiv.org/abs/2512.07552)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: SafeTerm AMQ is an AI system that automatically generates MedDRA queries for drug safety signal detection, achieving good recall (94%) at moderate thresholds and up to 89% precision at higher thresholds.


<details>
  <summary>Details</summary>
Motivation: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. Automated methods are needed to efficiently retrieve relevant MedDRA Preferred Terms for safety queries.

Method: SafeTerm AMQ uses AI to understand medical terminology, embedding query terms and MedDRA PTs in a multidimensional vector space, then applying cosine similarity and extreme-value clustering to generate ranked lists of PTs with relevance scores (0-1). Validation was conducted against 110 tier-1 SMQs using precision, recall, and F1 metrics at multiple similarity thresholds.

Result: The system achieves high recall (94%) at moderate thresholds, with precision up to 89% at higher thresholds. Optimal threshold (0.70) yields 48% recall and 45% precision. Narrow-term PTs show slightly better performance. Automatic threshold (0.66) prioritizes recall (58%) over precision (29%). Comparable performance is achieved on both SMQs and sanitized OCMQs.

Conclusion: SafeTerm AMQ is a viable supplementary method for automated MedDRA query generation that balances recall and precision. Recommendations include using suitable MedDRA PT terminology in queries and applying automated threshold methods to optimize recall, with higher similarity scores for refined term selection.

Abstract: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.

</details>


### [63] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

TL;DR: Simple method to enhance text-based LLMs with speech information using lasso-based feature selection on audio tokens, improving performance on argumentative fallacy detection tasks.


<details>
  <summary>Details</summary>
Motivation: Audio sequences are much longer than text sequences, making fusion challenging. Existing speech tokenizers produce long sequences that are difficult to integrate efficiently into LLMs.

Method: Use lasso-based feature selection on multimodal Bag-of-Words representations to retain only important audio tokens, then adapt the language model with self-supervised language modeling before fine-tuning on downstream tasks.

Result: Outperforms unimodal models, larger SpeechLM models, and learned audio representations. Achieves state-of-the-art on argumentative fallacy detection tasks where audio was previously considered counterproductive.

Conclusion: Simple feature selection method effectively integrates speech information into text LLMs, even random audio token selection helps, demonstrating the value of multimodal fusion for specific classification tasks.

Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [64] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

TL;DR: Proposes a cost-efficient LLM methodology using chain-of-thought and few-shot learning to integrate human and machine strengths in quantitative research, demonstrated on pharmaceutical alliance press releases.


<details>
  <summary>Details</summary>
Motivation: To develop a structured approach that leverages LLMs cost-effectively while combining human abductive reasoning with machine capabilities, addressing weaknesses in both human and machine approaches to quantitative research.

Method: Uses chain-of-thought and few-shot learning prompting techniques from computer science, extending qualitative co-author team practices to human-machine teams in quantitative research, allowing humans to interrogate both machine and human outputs.

Result: Demonstrated the methodology by analyzing human-machine rating discrepancies in a sample of 1,934 pharmaceutical alliance press releases (1990-2017), showing how scholars can manage LLM weaknesses with low-cost techniques.

Conclusion: The proposed methodology successfully integrates human and machine strengths in quantitative research through careful, cost-efficient LLM techniques, enabling effective interrogation of both human and machine outputs while managing LLM weaknesses.

Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [65] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

TL;DR: Metric-Fair Prompting framework guides LLMs to make decisions under metric-fairness constraints for medical QA, improving performance by treating similar questions similarly through Lipschitz-style constraints.


<details>
  <summary>Details</summary>
Motivation: To promote individual fairness in LLM decision-making for high-stakes medical multiple-choice questions, ensuring similar medical questions receive similar treatment and consistent outputs.

Method: Treats each (question, option) pair as binary instance, computes question similarity using NLP embeddings, solves items in joint pairs of similar questions, enforces global decision protocol with feature extraction, confidence scoring, and Lipschitz-style constraints for similar inputs.

Result: Metric-Fair Prompting improves performance over standard single-item prompting on MedQA (US) benchmark, demonstrating fairness-guided, confidence-oriented reasoning enhances LLM accuracy on clinical multiple-choice questions.

Conclusion: Fairness-guided prompting with metric constraints can enhance LLM performance in high-stakes medical decision-making by ensuring consistent treatment of similar cases through structured reasoning protocols.

Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [66] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

TL;DR: PCMind-2.1-Kaiyuan-2B is a fully open-source 2B parameter LLM that addresses the knowledge gap between open-source and industry by introducing innovative data efficiency methods for resource-constrained training.


<details>
  <summary>Details</summary>
Motivation: Address the significant knowledge gap between open-source community and industry caused by closed-source, high-quality data and training recipes used by industry players.

Method: Three key innovations: 1) Quantile Data Benchmarking for comparing heterogeneous datasets and guiding data mixing strategies; 2) Strategic Selective Repetition within multi-phase paradigm to leverage sparse, high-quality data; 3) Multi-Domain Curriculum Training ordering samples by quality, plus optimized data preprocessing and FP16 stability modifications.

Result: Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining.

Conclusion: The work provides a fully open-source solution (model weights, data, and code under Apache 2.0 license) that bridges the open-source/industry gap through efficient training methods suitable for resource-constrained environments.

Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [67] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

TL;DR: CGBridge is a plug-and-play method that enhances LLMs with code graph information using an external bridge module, improving structure-aware code understanding without modifying the LLM architecture.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with structural semantics of programs due to reliance on linearized token sequences. Existing approaches have limitations: graph-augmented prompting suffers from length constraints, while structure-aware pretraining requires incompatible architectural changes for instruction-following LLMs.

Method: 1) Pre-train code graph encoder via self-supervised learning on 270K code graphs; 2) Train external bridge module with cross-modal attention to align code, graph, and text semantics; 3) Generate structure-informed prompts injected into frozen LLM; 4) Fine-tune bridge module for downstream tasks.

Result: Achieves 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Also achieves over 4x faster inference than LoRA-tuned models.

Conclusion: CGBridge effectively enhances LLMs with structural code semantics through an external bridge module, achieving significant performance improvements in code intelligence tasks while maintaining efficiency and compatibility with existing LLMs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [68] [When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks](https://arxiv.org/abs/2512.07684)
*Zihan Chen,Lanyu Yu*

Main category: cs.CL

TL;DR: A Graph Neural Network framework outperforms 12 state-of-the-art LLMs in detecting online incivility (toxicity, aggression, personal attacks) on Wikipedia by leveraging both textual content and relational structures between comments, with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Online incivility is a widespread problem causing social and psychological burdens, but existing moderation and automated detection approaches have limited accuracy and efficiency. Current text-only LLM paradigms fail to capture the structural context of online interactions.

Method: Proposes a Graph Neural Network framework where each user comment is a node and edges are defined by textual similarity between comments. Introduces a dynamically adjusted attention mechanism that adaptively balances nodal (textual) and topological (structural) features during information aggregation.

Result: The proposed GNN architecture outperforms 12 state-of-the-art Large Language Models across multiple metrics while requiring significantly lower inference cost. Demonstrates superior performance in detecting three types of uncivil behavior: toxicity, aggression, and personal attacks.

Conclusion: Structural context plays a crucial role in detecting online incivility, addressing limitations of text-only LLM paradigms. The framework offers a more efficient and accurate solution for behavioral prediction in digital communities, with datasets made publicly available for reproducibility.

Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.

</details>


### [69] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

TL;DR: HalluShift++ is a novel approach for detecting hallucinations in Multimodal Large Language Models by analyzing internal layer dynamics rather than relying on external LLM evaluators.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs often produce factually inconsistent descriptions (hallucinations) that can have adverse consequences. Existing methods rely on external LLM evaluators that are themselves prone to hallucinations and have domain adaptation challenges.

Method: Proposes that hallucinations manifest as measurable irregularities in MLLMs' internal layer dynamics. HalluShift++ analyzes layer-wise patterns rather than just distributional shifts, extending hallucination detection from text-based LLMs to multimodal scenarios.

Result: The method broadens hallucination detection efficacy from text-based LLMs to multimodal scenarios, providing a more reliable approach than external LLM evaluators.

Conclusion: HalluShift++ offers a novel internal analysis approach for detecting hallucinations in MLLMs, addressing limitations of current external evaluation methods and potentially improving model reliability.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [70] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: SafeTerm is an AI system that automatically retrieves and ranks MedDRA Preferred Terms for drug safety queries using vector embeddings and similarity scoring, achieving high recall (>95%) at moderate thresholds with precision up to 86%.


<details>
  <summary>Details</summary>
Motivation: Manual grouping of adverse event terms into standardized MedDRA queries for drug safety review is time-consuming and critical for signal detection. There's a need for automated systems to efficiently retrieve relevant medical terminology.

Method: SafeTerm embeds medical query terms and MedDRA Preferred Terms in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate ranked lists of PTs by relevance score using multi-criteria statistical methods.

Result: Validation against FDA OCMQ v3.0 (104 queries) showed: high recall (>95%) at moderate thresholds, precision up to 86% at higher thresholds, optimal threshold (0.70-0.75) yielded ~50% recall and ~33% precision. Narrow-term subsets performed similarly with slightly higher thresholds.

Conclusion: SafeTerm provides a viable supplementary method for automated MedDRA query generation. Initial similarity threshold of ~0.60 is recommended, with increased thresholds for refined term selection.

Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [71] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,P眉ren ncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs can internally detect narrative incoherence but fail to reliably separate coherent/incoherent stories in their responses, showing gaps in narrative understanding.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can reliably distinguish between coherent and incoherent narratives, and understand the nature of their narrative comprehension capabilities.

Method: Used paired narratives dataset, conducted probing studies of LLMs' internal representations, tested various prompt variations for rating questions, and examined reasoning capabilities including thought strings.

Result: LLMs' internal representations can identify incoherence, but their generated responses fail to reliably separate coherent/incoherent stories. They're more sensitive to setting violations than character trait violations, suggesting reliance on prototypical world knowledge over narrative coherence.

Conclusion: LLMs lack complete grasp of narrative coherence, with a gap between internal state and external behavior that reasoning techniques don't fully address, indicating fundamental limitations in narrative understanding.

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [72] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

TL;DR: RL improves reasoning in language models, but unclear if it extends beyond pre-training capabilities. Controlled experiments show RL works best when pre-training leaves headroom, targets edge-of-competence tasks, and with process-level rewards.


<details>
  <summary>Details</summary>
Motivation: To resolve ambiguity about whether RL-based post-training truly extends reasoning ability beyond pre-training, given lack of control in modern training pipelines with opaque corpora and complex interactions between RL objectives and prior knowledge.

Method: Developed fully controlled experimental framework using synthetic reasoning tasks with explicit atomic operations, parseable reasoning traces, and systematic manipulation of training distributions. Evaluated models on extrapolative generalization (complex compositions) and contextual generalization (surface contexts).

Result: 1) RL produces true capability gains only when pre-training leaves sufficient headroom and targets edge-of-competence tasks. 2) Contextual generalization requires minimal pre-training exposure before RL can transfer. 3) Mid-training enhances performance more than RL alone under fixed compute. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity.

Conclusion: The study clarifies interplay between pre-training, mid-training, and RL, offering foundation for understanding and improving reasoning LM training strategies, showing RL's effectiveness depends on specific conditions and highlighting mid-training's underexplored importance.

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [73] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

TL;DR: The paper proposes Collaborative Causal Sensemaking (CCS) as a new framework for AI decision-support agents that act as cognitive partners rather than just tools, focusing on co-constructing mental models and hypotheses with human experts.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents in expert decision-support often fail to create effective human-AI teams, leading to underperformance, oscillation between verification and over-reliance, and unfulfilled complementarity promises. The problem is not just accuracy but a fundamental gap in how AI assistance is conceived.

Method: Proposes Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework. CCS systems are designed as partners in cognitive work that maintain evolving models of how experts reason, help articulate and revise goals, co-construct and stress-test causal hypotheses, and learn from joint decision outcomes.

Result: The paper presents a conceptual framework rather than empirical results. It identifies key challenges around training ecologies for collaborative thinking, representations and interaction protocols for co-authored models, and evaluation centered on trust and complementarity.

Conclusion: CCS can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners, moving beyond current limitations of AI decision-support systems.

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [74] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

TL;DR: The paper investigates whether out-of-distribution (OOD) generalization results generalize across different OOD datasets, finding that correlations between OOD performances vary significantly depending on the specific model analyzed.


<details>
  <summary>Details</summary>
Motivation: Previous work evaluating LLMs' generalization typically uses single OOD datasets, which may not accurately assess model capabilities since real-world deployment involves diverse data shifts. The authors question whether OOD generalization results actually generalize across different OOD testsets.

Method: Evaluate model performance across multiple OOD testsets throughout finetuning runs, then compute partial correlations of performances across these testsets while regressing out in-domain performance to control for it.

Result: Analysis of OLMo2 and OPT models shows no overarching trend in generalization results. The existence of positive or negative correlations between any two OOD testsets depends strongly on the specific model being analyzed.

Conclusion: OOD generalization results do not consistently generalize across different OOD datasets, and the relationships between OOD performances are highly model-dependent, suggesting that single-dataset OOD evaluations may be insufficient for comprehensive assessment.

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [75] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

TL;DR: Video generation models like Sora-2 achieve 60% success rates on reasoning tasks (chess, maze, Sudoku, mental rotation, Raven's Matrices) using a novel "Task Pair" experimental paradigm and scalable evaluation framework.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that video generation models can perform reasoning tasks and establish a robust, scalable evaluation paradigm for measuring reasoning capabilities in video models.

Method: Developed a "Task Pair" experimental design and built VMEvalKit framework with 39 models, enabling automated evaluation that correlates with human judgment. The approach supports easy scaling for adding new models and tasks.

Result: Leading models like Sora-2 achieve approximately 60% success rates on reasoning tasks. Automated evaluation shows strong correlation with human judgment, validating the paradigm's reliability.

Conclusion: Video generation models can reason, and the established "Task Pair" paradigm provides a scalable framework for evaluating and potentially improving reasoning capabilities through reinforcement learning.

Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [76] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

TL;DR: Novel dataset quantization method reduces intra-sample redundancy for edge devices, maintaining training performance while achieving significant compression, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address storage and communication costs for large-scale datasets on resource-constrained edge devices by reducing dataset size while preserving essential features for model training.

Method: Uses linear symmetric quantization for initial range/scale, then adaptive quantization allocation algorithm to distribute different quantization ratios per sample based on precision needs while maintaining constant total compression ratio.

Result: Method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under same compression ratios on CIFAR-10, CIFAR-100, and ImageNet-1K.

Conclusion: Proposed dataset quantization approach effectively reduces intra-sample redundancy for edge devices, offering a practical solution for dataset compression without sacrificing training performance.

Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [77] [VG3T: Visual Geometry Grounded Gaussian Transformer](https://arxiv.org/abs/2512.05988)
*Junho Kim,Seongwon Lee*

Main category: cs.CV

TL;DR: VG3T is a multi-view feed-forward network that predicts 3D semantic occupancy using 3D Gaussians, achieving better performance with fewer primitives than previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance when generating coherent 3D scene representations from multi-view images.

Method: VG3T directly predicts semantically attributed 3D Gaussians in a joint, multi-view fashion using Grid-Based Sampling and Positional Refinement to mitigate distance-dependent density bias in Gaussian initialization.

Result: Achieves 1.7%p improvement in mIoU while using 46% fewer primitives than previous state-of-the-art on the nuScenes benchmark.

Conclusion: VG3T offers a unified paradigm for representing both geometry and semantics with superior efficiency and performance compared to view-by-view processing methods.

Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.

</details>


### [78] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: EmoDiffTalk: A 3D Gaussian Splatting talking head framework with emotion-aware diffusion for fine-grained, multimodal emotional editing using action units and text prompts.


<details>
  <summary>Details</summary>
Motivation: Current 3D talking head methods using 3D Gaussian Splatting lack effective emotional expression manipulation, particularly for fine-grained and expansive dynamic emotional editing with multimodal controls.

Method: Introduces Emotion-aware Gaussian Diffusion with two components: 1) AU prompt Gaussian diffusion process for fine-grained facial animation, and 2) accurate text-to-AU emotion controller for dynamic emotional editing from text input.

Result: Superior performance on EmoTalk3D and RenderMe-360 datasets, demonstrating better emotional subtlety, lip-sync fidelity, and controllability compared to previous works.

Conclusion: Establishes a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis, representing one of the first 3D Gaussian Splatting frameworks supporting continuous, multimodal emotional editing in AU-based expression space.

Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [79] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

TL;DR: NeuroFM is a domain-specialized foundation model for neuropathology that outperforms general-purpose models on neurodegenerative disease tasks by being trained specifically on brain tissue images.


<details>
  <summary>Details</summary>
Motivation: Existing pathology foundation models are trained predominantly on surgical pathology data, which overrepresents non-neurological diseases and lacks neuropathology-specific features like unique brain cell types and neurodegenerative disease patterns. This domain mismatch limits their ability to capture morphological patterns critical for interpreting neurodegenerative diseases.

Method: Developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies, addressing the unique domain of neuropathology with its distinct cell types and disease features.

Result: NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification (cerebellar essential tremor and spinocerebellar ataxia subtypes).

Conclusion: Domain-specialized foundation models trained on brain tissue better capture neuropathology-specific features than general surgical pathology models, enabling more accurate AI-based analysis for brain disease diagnosis and research, and setting a precedent for domain-specific model development in specialized digital pathology areas.

Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [80] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

TL;DR: FishDetector-R1 is a unified MLLM-based framework for fish detection, segmentation, and counting using weak supervision, achieving significant performance gains on underwater imagery with novel detection-to-count prompts and RLVR training.


<details>
  <summary>Details</summary>
Motivation: Underwater fish imagery analysis is crucial for ecological monitoring but faces challenges due to visual degradation and expensive annotation requirements, necessitating efficient weakly-supervised solutions.

Method: A unified MLLM-based framework with two key components: 1) detect-to-count prompt for spatially consistent detections and counts, and 2) Reinforcement Learning from Verifiable Reward (RLVR) leveraging sparse point labels in a scalable paradigm.

Result: On DeepFish dataset: 20% AP improvement, 10% mIoU gain, 30% MAE reduction, 35% GAME reduction. Strong cross-domain generalization to other underwater datasets, validated through ablation studies.

Conclusion: FishDetector-R1 provides a reliable, scalable solution for accurate marine visual understanding via weak supervision, with demonstrated effectiveness and cross-domain robustness for ecological monitoring applications.

Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [81] [PrunedCaps: A Case For Primary Capsules Discrimination](https://arxiv.org/abs/2512.06003)
*Ramin Sharifi,Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: Capsule Networks can be pruned by removing up to 95% of Primary Capsules, achieving 9.9x speedup and 95.36% reduction in FLOPs without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: CapsNets have advantages over CNNs (better robustness to affine transformations, overlapping image detection) but are resource-inefficient due to high number of Primary Capsules, making training/testing slow and resource-hungry.

Method: Investigated Primary Capsules pruning in CapsNets across multiple datasets (MNIST, Fashion-MNIST, CIFAR-10, SVHN). Used pruning techniques to remove unnecessary capsules while maintaining network functionality.

Result: Pruned CapsNet performs up to 9.90 times faster than conventional architecture by removing 95% of capsules without accuracy loss. Saves over 95.36% of floating-point operations in dynamic routing stage. Provides insights into why some datasets benefit more from pruning than others.

Conclusion: Primary Capsules pruning is an effective method to make CapsNets more resource-efficient while preserving their advantages, enabling practical deployment with significant speed improvements and computational savings.

Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.

</details>


### [82] [Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization](https://arxiv.org/abs/2512.06006)
*Xuefei,Wang,Kai A. Horstmann,Ethan Lin,Jonathan Chen,Alexander R. Farhang,Sophia Stiles,Atharva Sehgal,Jonathan Light,David Van Valen,Yisong Yue,Jennifer J. Sun*

Main category: cs.CV

TL;DR: AI agents can automate adaptation of computer vision tools to scientific datasets, outperforming human experts with simpler architectures.


<details>
  <summary>Details</summary>
Motivation: Adapting production-level computer vision tools to scientific datasets is a critical bottleneck. Current solutions are impractical: fine-tuning needs large annotated datasets that scientists lack, while manual code adaptation takes weeks to months of effort.

Method: The authors introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. They investigate optimal agent design for this targeted task.

Result: A simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Analysis reveals that common, complex agent architectures are not universally beneficial.

Conclusion: The work provides a practical roadmap for agent design and demonstrates a clear pathway for real-world impact by deploying agent-generated functions into production pipelines. The framework is open-sourced.

Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.

</details>


### [83] [Fast and Flexible Robustness Certificates for Semantic Segmentation](https://arxiv.org/abs/2512.06010)
*Thomas Massena,Corentin Friedrich,Franck Mamalet,Mathieu Serrurier*

Main category: cs.CV

TL;DR: A novel framework for certifiably robust semantic segmentation using Lipschitz-constrained networks, achieving real-time certification 600x faster than randomized smoothing methods.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks are vulnerable to adversarial perturbations, but existing robust certification methods focus mostly on classification tasks with few efficient procedures for semantic segmentation. There's a need for practical, real-time compatible certifiably robust semantic segmentation.

Method: Introduces a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable. Provides a novel framework that generalizes robustness certificates for semantic segmentation tasks using Lipschitz networks.

Result: Achieves competitive pixel accuracy on challenging datasets like Cityscapes. Unlocks real-time compatible certifiably robust semantic segmentation for the first time. Certification process is around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU.

Conclusion: The approach provides an efficient, flexible framework for certifiable robustness in semantic segmentation with practical real-time performance, validated against state-of-the-art adversarial attacks.

Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $蔚$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.

</details>


### [84] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

TL;DR: Automated ML framework uses high-throughput imaging and clustering to analyze metallic powder morphology for SLM additive manufacturing, identifying Fourier-descriptor + k-means as optimal pipeline.


<details>
  <summary>Details</summary>
Motivation: Conventional powder characterization methods for Selective Laser Melting are low-throughput and qualitative, failing to capture heterogeneity in industrial-scale batches needed for quality control.

Method: Developed three clustering pipelines: autoencoder, shape-descriptor, and functional-data approaches, applied to ~126,000 powder images (0.5-102 渭m diameter) with high-throughput imaging and shape extraction.

Result: Fourier-descriptor + k-means pipeline performed best with lowest Davies-Bouldin index, highest Calinski-Harabasz score, and sub-millisecond runtime per particle on standard desktop workstation.

Conclusion: Unsupervised learning framework enables rapid, automated powder morphology assessment, supports tracking shape evolution across reuse cycles, and offers path toward real-time feedstock monitoring in SLM workflows.

Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [85] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

TL;DR: Vision Action Transformer (VAT) leverages all transformer layers instead of just final layer features for better robot learning, achieving SOTA 98.15% success on LIBERO benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Vision Transformer (ViT) methods in robot learning discard valuable information by using only the final layer's features, providing insufficient representation for robotic tasks.

Method: VAT extends ViT architecture to process specialized action tokens with visual features across all transformer layers, enabling deep progressive fusion of perception and action generation.

Result: Achieves 98.15% average success rate across four LIBERO benchmarks, establishing new state-of-the-art by outperforming prior methods like OpenVLA-OFT.

Conclusion: VAT demonstrates the critical importance of leveraging the complete "representation trajectory" of vision models to advance robotic policy learning.

Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [86] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

TL;DR: Benchmark comparison of two chest X-ray foundation models (CXR-Foundation and MedImageInsight) on public datasets shows MedImageInsight performs slightly better, while CXR-Foundation has better cross-dataset stability.


<details>
  <summary>Details</summary>
Motivation: While foundation models show strong performance in medical image representation learning, their comparative behavior across different datasets remains underexplored, creating a need for standardized evaluation.

Method: Used unified preprocessing pipeline and fixed downstream classifiers to evaluate embeddings from pre-trained encoders. Trained LightGBM classifiers on multiple disease labels and reported AUROC and F1-scores with confidence intervals. Also performed unsupervised clustering analysis.

Result: MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited stronger cross-dataset stability. Unsupervised clustering revealed coherent disease-specific structure in MedImageInsight embeddings consistent with quantitative results.

Conclusion: The study highlights the need for standardized evaluation of medical foundation models and establishes reproducible baselines for future multimodal and clinical integration studies.

Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [87] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: A multimodal framework using MLLMs to extract user preferences and inject them into diffusion models for personalized image generation, outperforming baselines in quality and preference alignment.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for preference-conditioned image generation either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals, creating a need for better methods to adapt generative models to individual aesthetic choices.

Method: Proposes a multimodal framework that: 1) Trains MLLMs with preference-oriented visual QA to capture fine-grained semantic cues, 2) Introduces two probing tasks (inter-user discrimination and intra-user discrimination) to isolate preference-relevant features, 3) Uses maximum mean discrepancy-based alignment loss to bridge modality gap with diffusion text encoders while preserving multimodal structure, 4) Conditions diffusion generators with resulting embeddings.

Result: Extensive experiments show the method substantially outperforms strong baselines in both image quality and preference alignment, demonstrating effectiveness of representation extraction and alignment for personalized generation.

Conclusion: The proposed framework effectively extracts rich user representations using MLLMs and aligns them with diffusion models, enabling faithful adherence to both textual prompts and user preferences for personalized image generation.

Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [88] [Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing](https://arxiv.org/abs/2512.06024)
*Jiabin Liu,Zihao Zhou,Jialei Yan,Anxin Guo,Alvise Benetazzo,Hui Li*

Main category: cs.CV

TL;DR: A neural network for 3D wave surface and velocity field reconstruction using attention-augmented pyramid architecture with physics constraints, achieving millimeter-level accuracy and fast dense reconstruction in real-sea conditions.


<details>
  <summary>Details</summary>
Motivation: Need for precise 3D reconstruction of wave free surfaces and velocity fields for ocean physics understanding, addressing high computational costs of dense visual reconstruction and challenges from persistent visual occlusions in long-term ocean wave observation.

Method: Proposed wave free surface visual reconstruction neural network with attention-augmented pyramid architecture tailored to multi-scale and temporally continuous wave motions. Uses physics-based constraints for time-resolved reconstruction of nonlinear 3D velocity fields from evolving free-surface boundary.

Result: Millimeter-level wave elevation prediction in central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, high-fidelity 3D reconstruction of nonlinear velocity fields. Dense reconstruction of two million points in only 1.35 seconds. Outperforms conventional approaches and maintains strong generalization in occluded conditions.

Conclusion: The proposed neural network enables efficient, accurate 3D wave reconstruction with strong generalization capabilities, overcoming computational and occlusion challenges in ocean wave observation through attention mechanisms and learned wave dynamics encoding.

Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.

</details>


### [89] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

TL;DR: SAM3 represents a fundamental paradigm shift from SAM2's prompt-based segmentation to concept-driven multimodal segmentation, creating a discontinuity where expertise doesn't transfer between models.


<details>
  <summary>Details</summary>
Motivation: To analyze the fundamental discontinuity between SAM2 and SAM3, explaining why prompt-based segmentation expertise doesn't transfer to SAM3's multimodal concept-driven approach.

Method: Five-component analysis: (1) conceptual break between prompt-based vs concept-based segmentation, (2) architectural divergence (vision-temporal vs multimodal fusion), (3) dataset differences (video masks vs concept annotations), (4) training/hyperparameter distinctions, and (5) evaluation metrics transition.

Result: SAM3 introduces unified vision-language architecture with open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding, establishing it as a new class of segmentation foundation model.

Conclusion: SAM3 represents a paradigm shift to concept-driven segmentation era, charting future directions beyond SAM2's geometric prompt-based approach.

Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [90] [Representation Learning for Point Cloud Understanding](https://arxiv.org/abs/2512.06058)
*Siming Yan*

Main category: cs.CV

TL;DR: This dissertation develops novel methods for 3D point cloud representation learning by effectively integrating 2D knowledge through supervised learning, self-supervised learning, and 2D-to-3D transfer learning approaches.


<details>
  <summary>Details</summary>
Motivation: With the increasing prevalence of 3D data acquisition technologies (3D scanners, LiDAR, RGB-D cameras) across fields like computer vision, robotics, and autonomous driving, there's a need for better 3D understanding methods. Combining 2D images with 3D data provides comprehensive environmental understanding, but current approaches need improvement in effectively leveraging 2D knowledge for 3D tasks.

Method: The dissertation focuses on three main approaches: 1) Supervised representation learning for point cloud primitive segmentation, 2) Self-supervised learning methods for 3D data, and 3) Transfer learning from 2D to 3D that integrates pre-trained 2D models to support 3D network training without merely transforming 2D data.

Result: Extensive experiments validate the effectiveness of the proposed methods, showing significant improvements in 3D understanding. The integration of 2D knowledge through the novel transfer learning approach proves particularly effective for advancing point cloud representation learning.

Conclusion: The dissertation demonstrates that effectively integrating 2D knowledge through supervised learning, self-supervised learning, and transfer learning approaches can significantly advance point cloud representation learning, with promising applications in autonomous driving, robotics, remote sensing, and medical treatment.

Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.

</details>


### [91] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

TL;DR: EgoEdit: A real-time instruction-guided video editing system for egocentric footage with specialized dataset and benchmark for AR applications.


<details>
  <summary>Details</summary>
Motivation: Existing AI video editors work well on third-person footage but struggle with egocentric videos due to rapid egomotion and hand-object interactions, creating a domain gap. Offline editing pipelines also have high latency, limiting real-time AR applications.

Method: Three-part ecosystem: 1) EgoEditData - manually curated dataset for egocentric editing with hand-object interactions; 2) EgoEdit - instruction-following video editor supporting real-time streaming inference on single GPU; 3) EgoEditBench - evaluation suite for instruction faithfulness, hand preservation, and temporal stability.

Result: EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks where existing methods struggle, while maintaining comparable performance to strongest baselines on general editing tasks.

Conclusion: The complete ecosystem addresses the domain gap in egocentric video editing, enabling real-time instruction-guided editing for AR applications. Both dataset and benchmark will be publicly released to advance research in this area.

Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [92] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

TL;DR: A data-driven method using single-photon lidar to reconstruct 3D scenes from single measurements, handling occlusions and specular materials by decomposing multi-bounce light transport.


<details>
  <summary>Details</summary>
Motivation: Single-view 3D reconstruction is difficult with occlusions and specular materials like mirrors. Single-photon lidars can capture multi-bounce light containing hidden information, but existing methods require sequential point illumination which is impractical.

Method: Created large-scale simulated dataset (~100k lidar transients) of indoor scenes, then trained a data-driven model to decompose multiplexed two-bounce light into contributions from each laser spot, enabling inversion of complex light transport.

Result: Successfully demonstrated experimental reconstruction of 3D geometry in scenes with occlusions and mirrors from single measurements, with code and dataset publicly released.

Conclusion: Data-driven approach enables practical single-photon lidar reconstruction with multiplexed illumination, overcoming limitations of analytical inversion for complex light transport in challenging scenes.

Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [93] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

TL;DR: BeLLA connects 360掳 BEV representations with LLMs for QA in autonomous driving, outperforming existing methods on spatial reasoning tasks by up to +9.3%.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs/MLLMs in autonomous driving either use single-view encoders that miss spatial structure or aggregated multi-view features lacking unified spatial representation, making it hard to reason about ego-centric directions, object relations, and wider context.

Method: BeLLA is an end-to-end architecture that connects unified 360掳 Bird's Eye View (BEV) representations with a large language model for question answering in autonomous driving scenarios.

Result: BeLLA consistently outperforms existing approaches on spatial reasoning questions (relative object positioning, behavioral understanding) in NuScenes-QA and DriveLM benchmarks, achieving up to +9.3% absolute improvement. It performs competitively on other question categories.

Conclusion: The unified 360掳 BEV representation combined with LLMs enables better spatial reasoning for autonomous driving QA, addressing limitations of existing single-view or aggregated multi-view approaches.

Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360掳 BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [94] [SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection](https://arxiv.org/abs/2512.06103)
*Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: SpectraIrisPAD is a deep learning framework using multispectral imaging and Vision Transformers for robust iris presentation attack detection, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Iris recognition is highly accurate but vulnerable to presentation attacks. Current systems mainly use single-band NIR imaging, which lacks robustness against diverse spoofing methods. Multispectral imaging across multiple NIR bands can provide complementary reflectance information to enhance PAD generalizability.

Method: Proposes SpectraIrisPAD framework with DINOv2 Vision Transformer backbone featuring learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative band-specific features. Also introduces MSIrPAD dataset with 18,848 iris images across 5 NIR wavelengths (800-980nm) and 8 PAI categories including textured contact lenses, print attacks, and display attacks.

Result: SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics in unseen attack evaluation protocols, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

Conclusion: The proposed multispectral approach with advanced deep learning techniques provides a robust solution for iris presentation attack detection, addressing the security vulnerabilities of conventional single-band iris recognition systems through enhanced feature discrimination and generalization capabilities.

Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

</details>


### [95] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

TL;DR: CEFM is a cross-modal explainable framework for melanoma diagnosis that uses contrastive learning to align clinical ABC criteria with visual features, generating interpretable textual explanations while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for melanoma classification lack interpretability, creating trust barriers for clinical adoption despite their expert-level performance. Clinicians need transparent decision-making processes to trust AI systems in dermatology.

Method: CEFM uses contrastive learning to map clinical ABC criteria (Asymmetry, Border, Color) into Vision Transformer embeddings via dual projection heads, aligning clinical semantics with visual features. It then generates structured textual explanations through natural language generation.

Result: Achieved 92.79% accuracy and 0.961 AUC on public datasets, with significant improvements in interpretability metrics. Learned embeddings align spatially with clinicians' ABC rule application, bridging performance and trust.

Conclusion: CEFM successfully creates a transparent link between image data and clinical interpretation, addressing the interpretability gap in melanoma diagnosis while maintaining high classification performance.

Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [96] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

TL;DR: Track4DGen: A two-stage framework that integrates point tracking priors into multi-view video diffusion and 4D Gaussian Splatting to generate temporally consistent 4D objects from sparse inputs.


<details>
  <summary>Details</summary>
Motivation: Generating dynamic 4D objects from sparse inputs is challenging due to difficulties in preserving appearance and motion coherence across views and time while suppressing artifacts and temporal drift. Current approaches suffer from view discrepancies due to supervision limited to pixel- or latent-space video-diffusion losses that lack explicit temporally aware, feature-level tracking guidance.

Method: Two-stage framework: 1) Multi-view video diffusion model with foundation point tracker integration to enforce dense feature-level point correspondences, producing temporally consistent features. 2) Hybrid 4D Gaussian Splatting reconstructor using concatenated diffusion features (with tracking priors) and Hex-plane features, augmented with 4D Spherical Harmonics for higher-fidelity dynamics modeling.

Result: Track4DGen surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. The paper also introduces Sketchfab28, a high-quality dataset for benchmarking object-centric 4D generation.

Conclusion: Explicit injection of tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D Gaussian Splatting effectively addresses temporal drift and view discrepancy issues, enabling high-quality dynamic 4D object generation from sparse inputs.

Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [97] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

TL;DR: Automated workflow using deep learning generates defect annotations from shearography measurements, enabling weakly supervised training and scalable dataset creation for subsurface defect detection.


<details>
  <summary>Details</summary>
Motivation: Shearography is sensitive to surface displacement gradients for detecting subsurface defects, but industrial adoption is limited by lack of high-quality annotated datasets due to labor-intensive, subjective manual labeling.

Method: Introduces an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels.

Result: Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort.

Conclusion: The approach supports scalable dataset creation for robust defect detection in safety-critical components.

Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [98] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: A novel shadow generation framework that combines explicit physical modeling (3D geometry and illumination) with diffusion models to produce photorealistic, physically consistent shadows.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning-based shadow generation methods rarely incorporate explicit physical modeling of shadow formation, which follows the physics of light occlusion by objects. The authors aim to bridge this gap by embedding physical principles into neural networks.

Method: 1) Extract approximate 3D geometry (dense point maps) and predict dominant light direction from monocular RGB images. 2) Use physics-based modeling to generate initial shadow location/shape estimates. 3) Refine these estimates using a diffusion framework to achieve realistic appearance while maintaining physical consistency.

Result: The model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting. Trained on DESOBAV2 dataset.

Conclusion: Explicit physical modeling of geometry and illumination can be effectively integrated into deep learning frameworks for shadow generation, resulting in superior performance that combines physical accuracy with visual realism.

Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [99] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: A framework for joint detection of cast and attached shadows using iterative geometry-illumination reasoning, with a new dataset for training and evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing shadow detection methods focus only on cast shadows, ignoring attached shadows which are crucial for 3D structure understanding. No dedicated datasets or models exist for attached shadow detection.

Method: A closed-loop system with shadow detection module (predicts both shadow types) and light estimation module (infers light direction). Uses estimated light direction with surface normals to create geometry-consistent partial map of self-occluded regions, which refines shadow predictions iteratively.

Result: Experimental results show substantial improvement in attached shadow detection (at least 33% BER reduction) while maintaining strong performance on full and cast shadows. Created dataset of 1,458 images with separate annotations for both shadow types.

Conclusion: The iterative geometry-illumination reasoning framework effectively addresses the gap in attached shadow detection, enabling better 3D scene understanding through joint modeling of both shadow types and their relationship with illumination and geometry.

Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [100] [SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling](https://arxiv.org/abs/2512.06185)
*Ankit Gupta,Christoph Adami,Emily Dolson*

Main category: cs.CV

TL;DR: Modern deep neural networks remain vulnerable to high-confidence fooling attacks, with transformers being most susceptible. A new minimalist attack called SPOOF generates fooling images with minimal modifications, and retraining provides only partial defense.


<details>
  <summary>Details</summary>
Motivation: Despite advances in deep neural networks for image recognition, they still exhibit overconfidence on non-natural images (fooling images). The paper revisits this vulnerability to understand if modern architectures remain susceptible and to develop more efficient attacks.

Method: 1) Re-implemented evolutionary fooling attacks (CPPN-based and direct-encoding) on modern architectures (convolutional and transformer classifiers). 2) Introduced SPOOF - a minimalist, consistent, and efficient black-box attack that generates high-confidence fooling images with minimal pixel modifications.

Result: 1) High-confidence fooling persists in state-of-the-art networks, with transformer-based ViT-B/16 being most susceptible (achieving near-certain misclassifications with fewer queries). 2) SPOOF generates unrecognizable fooling images with minimal compute. 3) Retraining with fooling images as an additional class provides only partial resistance - SPOOF continues to fool with slightly higher query budgets.

Conclusion: Modern deep classifiers remain persistently fragile to fooling attacks, with transformers showing particular vulnerability. Even defensive measures like retraining offer limited protection, highlighting fundamental robustness issues in current architectures.

Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.

</details>


### [101] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

TL;DR: Novel multi-modal method predicts food drying color trajectories using high-dimensional temporal color data and process parameters, achieving 90% error reduction over baselines.


<details>
  <summary>Details</summary>
Motivation: Current food drying color analysis uses low-dimensional features that can't capture complex dynamic color changes, and existing models lack generalization to unseen process conditions.

Method: Developed a multi-modal color-trajectory prediction method integrating high-dimensional temporal color information with drying process parameters for accurate and data-efficient prediction.

Result: Achieved RMSEs of 2.12 for cookie drying and 1.29 for apple drying under unseen conditions, reducing errors by over 90% compared to baseline models.

Conclusion: The model demonstrates superior accuracy, robustness, and broad applicability for predicting color evolution in food drying processes.

Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [102] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon M盲chler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

TL;DR: The FeTS Challenge 2024 evaluated federated learning methods for glioma segmentation in MRI, with a PID-controller-based approach achieving top performance and communication efficiency.


<details>
  <summary>Details</summary>
Motivation: To advance federated learning for medical imaging by evaluating new weight aggregation methods that improve robustness and efficiency in multi-institutional glioma segmentation tasks.

Method: Six teams participated in a standardized FL setup using multi-parametric MRI data from the BraTS benchmark (1,251 training, 219 validation, 570 test cases). Teams were evaluated on segmentation performance (Dice Similarity Coefficient and 95th percentile Hausdorff Distance) and communication efficiency (convergence score).

Result: A PID-controller-based method achieved the top ranking with mean DSC values of 0.733 (ET), 0.761 (TC), and 0.751 (WT), and HD95 values of 33.922mm, 33.623mm, and 32.309mm respectively, plus the highest communication efficiency (convergence score 0.764).

Conclusion: The challenge advances federated learning for medical imaging, surpassing previous methods and demonstrating PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. Code is publicly available.

Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [103] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

TL;DR: Reproducibility study finds SVD+WDR image compression doesn't outperform JPEG2000 or WDR as originally claimed, highlighting implementation ambiguities.


<details>
  <summary>Details</summary>
Motivation: To independently verify claims that combining SVD with WDR yields better compression than JPEG2000 and standalone WDR, and to examine reproducibility issues in the original paper.

Method: Re-implemented the SVD+WDR method, filled in missing implementation details, replicated original experiments, and conducted additional tests on new images using PSNR and SSIM metrics.

Result: Contrary to original claims, SVD+WDR generally doesn't surpass JPEG2000 or WDR in PSNR, and only partially improves SSIM relative to JPEG2000. Found significant ambiguities in original description affecting reproducibility.

Conclusion: The reproducibility study reveals that the original SVD+WDR compression claims are not substantiated, and highlights how implementation ambiguities can significantly impact reported performance and reproducibility in research.

Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [104] [GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking](https://arxiv.org/abs/2512.06230)
*Pranav Balakrishnan,Sidisha Barik,Sean M. O'Rourke,Benjamin M. Marlin*

Main category: cs.CV

TL;DR: A GPU-accelerated GLMB filter variant that handles multiple detections per object, improving parallel scalability for multi-target tracking in distributed sensor networks.


<details>
  <summary>Details</summary>
Motivation: Standard labeled RFS methods like GLMB are computationally expensive due to hypothesis maintenance, especially when dealing with multiple detections per object from ML-based virtual sensors in distributed networks.

Method: Proposes a GLMB filter variant that allows multiple detections per object, breaking inter-detection dependencies to enable parallel updates with improved scalability for GPU acceleration.

Result: Preliminary analysis shows improved run time scalability with respect to number of objects and retained hypotheses when implemented on GPU hardware.

Conclusion: The proposed GLMB variant enables efficient deployment on GPU hardware by breaking computational dependencies, making labeled RFS methods more practical for real-world distributed sensor networks.

Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.

</details>


### [105] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

TL;DR: Training on developmentally realistic child video data (SAYCam) doesn't improve intuitive physics performance despite matching human learning conditions, suggesting data distribution alone isn't sufficient for current architectures.


<details>
  <summary>Details</summary>
Motivation: Humans learn intuitive physics from limited, developmentally appropriate visual experiences, while AI models trained on massive internet data still fail at intuitive physics benchmarks. The paper investigates whether the key is data distribution (developmentally realistic experiences) rather than data volume.

Method: Pretrained a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam dataset - a developmentally realistic, egocentric video dataset capturing three children's everyday visual experiences (only 0.01% of data volume used by SOTA models). Tested performance on IntPhys2 intuitive physics benchmark.

Result: Training on the developmentally realistic dataset did NOT lead to significant performance improvements on the IntPhys2 benchmark. The model performed similarly to those trained on much larger but less developmentally appropriate datasets.

Conclusion: Merely training on developmentally realistic data is insufficient for current architectures to learn intuitive physics representations. Varying visual data volume and distribution alone may not be enough - suggesting architectural changes or additional learning mechanisms may be needed.

Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [106] [NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks](https://arxiv.org/abs/2512.06251)
*Fangzhou Lin,Yuping Wang,Yuliang Guo,Zixun Huang,Xinyu Huang,Haichong Zhang,Kazunori Yamada,Zhengzhong Tu,Liu Ren,Ziming Zhang*

Main category: cs.CV

TL;DR: NexusFlow is a plug-and-play framework for partially supervised multi-task learning that handles structurally diverse tasks using invertible coupling layers to align latent feature distributions, achieving SOTA on nuScenes and consistent gains on NYUv2.


<details>
  <summary>Details</summary>
Motivation: Existing PS-MTL approaches focus on homogeneous dense prediction tasks, leaving the realistic challenge of learning from structurally diverse tasks with incomplete annotations unexplored. There's a need for a framework that can handle both homogeneous and heterogeneous tasks effectively.

Method: NexusFlow introduces surrogate networks with invertible coupling layers to align latent feature distributions across tasks. These bijective layers map features into a shared canonical space while preserving information, avoiding representational collapse and enabling alignment across structurally different tasks without reducing expressive capacity.

Result: NexusFlow achieves new state-of-the-art results on nuScenes for domain-partitioned autonomous driving (dense map reconstruction + sparse multi-object tracking). On NYUv2 with three homogeneous dense prediction tasks (segmentation, depth, surface normals), it yields consistent gains across all tasks.

Conclusion: NexusFlow is a lightweight, plug-and-play framework effective for both homogeneous and structurally diverse tasks in partially supervised multi-task learning settings, demonstrating broad applicability and strong performance across different domains.

Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.

</details>


### [107] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: LaFG is a language-driven framework for fine-grained image retrieval that uses LLMs and VLMs to convert class names into attribute-level supervision, improving generalization to unseen categories.


<details>
  <summary>Details</summary>
Motivation: Existing FGIR methods use one-hot labels from category names as supervision, which overlooks rich semantics in category names, hinders modeling of cross-category comparability, and limits generalization to unseen categories.

Method: LaFG converts class names into attribute-level supervision using LLMs to generate detailed attribute descriptions, then uses frozen VLMs to project these into vision-aligned space, clustering them into a dataset-wide attribute vocabulary. A global prompt template selects category-relevant attributes aggregated into linguistic prototypes that supervise the retrieval model.

Result: The abstract doesn't provide specific quantitative results, but the framework is designed to improve modeling of cross-category details and enhance generalization to unseen categories in fine-grained image retrieval.

Conclusion: LaFG addresses limitations of traditional one-hot label supervision by leveraging language models to extract rich semantic attributes from category names, enabling better cross-category comparability and improved generalization in fine-grained image retrieval.

Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [108] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: LVLMs often reach correct answers via incorrect reasoning paths due to path selection bias, not lack of knowledge. PSO framework improves reasoning accuracy by 7.4% through two-stage optimization with negative replay memory.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models frequently produce correct answers but through flawed reasoning paths, indicating a path selection bias problem rather than knowledge deficiency. The gap between Pass@K (with large K) and Pass@1 shows this is primarily a misreasoning issue.

Method: PSO (Path-Select Optimization) is a two-stage post-training framework: 1) Group Relative Policy Optimization (GRPO) with template and answer-based rewards to develop structured step-by-step reasoning, 2) Online preference optimization where models sample reasoning paths, self-evaluate them, and align toward preferred trajectories. Incorrect paths are stored in Negative Replay Memory (NRM) as hard negatives for continual refinement.

Result: Extensive experiments show PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy with 7.4% average improvements, and yields more stable and consistent chains of thought.

Conclusion: The paper identifies a critical flaw in LVLMs' reasoning path selection and proposes PSO as an effective solution to improve both reasoning performance and stability, addressing the misreasoning problem rather than knowledge gaps.

Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [109] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

TL;DR: A method that improves 3D Gaussian Splatting by enforcing global geometry consistency through constrained multi-view triangulation, reducing floaters and improving surface reconstruction.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting for novel view synthesis suffers from inconsistencies due to being guided solely by photometric loss, leading to "floater" artifacts and unstructured geometry that prevents high-fidelity surface extraction.

Method: Enforces global geometry consistency through constrained multi-view triangulation. Optimizes by penalizing deviation of rendered 3D points from robust consensus points, which are re-triangulated from neighboring views in a self-supervised fashion.

Result: Achieves state-of-the-art results across multiple datasets. On DTU dataset, attains mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods.

Conclusion: The proposed method successfully addresses geometry inconsistencies in 3D Gaussian Splatting through multi-view triangulation constraints, enabling better surface reconstruction while maintaining real-time rendering capabilities.

Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [110] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

TL;DR: FacePhys is a memory-efficient remote photoplethysmography algorithm that achieves real-time heart rate monitoring from video with minimal computational overhead and state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Current camera-based vital sign monitoring (rPPG) faces challenges with computational constraints on front-end devices and accuracy degradation when transmitting compressed data, limiting practical deployment.

Method: FacePhys uses temporal-spatial state space duality with a transferable heart state to capture subtle periodic variations across video frames while maintaining minimal computational overhead.

Result: Achieves 49% error reduction, real-time inference with 3.6 MB memory footprint and 9.46 ms per-frame latency, surpassing existing methods by 83-99% in efficiency.

Conclusion: FacePhys resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation, enabling reliable real-time vital sign monitoring in practical deployments.

Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [111] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

TL;DR: RefBench-PRO is a new REC benchmark that decomposes referring expressions into perception and reasoning dimensions with six sub-tasks, plus an RL-based learning scheme (Ref-R1) for improved localization.


<details>
  <summary>Details</summary>
Motivation: Existing REC benchmarks lack interpretable scoring mechanisms and cannot reveal MLLM's grounding capability across different cognitive abilities. Current benchmarks primarily evaluate perceptual capabilities only.

Method: 1) Introduces RefBench-PRO benchmark with six progressively challenging tasks (attribute, position, interaction, commonsense, relation, reject). 2) Develops automated data-generation pipeline for diverse referring expressions. 3) Proposes Ref-R1, an RL-based learning scheme with Dynamic IoU-based GRPO to improve localization accuracy.

Result: RefBench-PRO enables interpretable evaluation of MLLMs on referring expression comprehension, presenting greater challenges in both perception and reasoning dimensions. The benchmark demonstrates the limitations of current MLLMs.

Conclusion: The paper introduces a comprehensive REC benchmark that provides interpretable evaluation across cognitive dimensions and establishes stronger baselines through RL-based learning, advancing the field of referring expression comprehension.

Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [112] [Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://arxiv.org/abs/2512.06281)
*Hengzhuang Li,Xinsong Zhang,Qiming Peng,Bin Luo,Han Hu,Dengyang Jiang,Han-Jia Ye,Teng Zhang,Hai Jin*

Main category: cs.CV

TL;DR: LaVer is a training framework that addresses modality imbalance in MLLMs by using masked image modeling in the joint latent space to improve visual representation learning.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from modality imbalance where visual information is underutilized compared to text in deeper layers, leading to degraded visual performance and hallucinations. This happens because training relies mainly on next-token prediction without direct visual supervision.

Method: Proposes Latent Visual Reconstruction (LaVer) - a training framework that uses masked image modeling in the joint latent semantic space of LLMs. This provides direct visual supervisory signals to help MLLMs learn more discriminative visual representations.

Result: LaVer enables MLLMs to exhibit increased visual attention allocation and enhanced utilization of visual information. Extensive experiments across diverse benchmarks show superiority, especially in tasks requiring dense visual capabilities.

Conclusion: LaVer effectively addresses the modality imbalance problem in MLLMs by providing direct visual activation through masked image modeling in latent space, leading to improved visual performance and reduced hallucinations.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.

</details>


### [113] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

TL;DR: A noninvasive sleep monitoring system using event-based method with depth sensor, RGB camera, and microphone array to detect motion, lighting, and noise disturbances in home environments.


<details>
  <summary>Details</summary>
Motivation: To quantitatively evaluate sleep disturbances in home contexts through noninvasive monitoring, as sleep quality assessment is important for health but often subjective or requires intrusive methods.

Method: Uses a device with infrared depth sensor, RGB camera, and four-microphone array. Establishes background models: one in depth signals for movement magnitude measurement, another in color images for lighting effect measurement. Event detection algorithm processes data from all three sensor types to classify disturbances into motion events, light-on/off events, and noise events.

Result: The system was tested in sleep conditions and experimental results validate the system's reliability for detecting and classifying sleep disturbances.

Conclusion: The developed noninvasive monitoring system using event-based method with multiple sensors provides reliable quantitative evaluation of sleep disturbances in home environments, enabling objective assessment of sleep quality.

Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


### [114] [StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification](https://arxiv.org/abs/2512.06290)
*Yiheng Huang,Shuang She,Zewei Wei,Jianmin Lin,Ming Yang,Wenyin Liu*

Main category: cs.CV

TL;DR: StrokeNet: A novel network that encodes strokes as reference pair representations (points + features) using dynamic reference point selection, Inline Sequence Attention, and Cross-Ellipse Query mechanisms to capture fine-grained spatial relationships for stroke classification.


<details>
  <summary>Details</summary>
Motivation: Stroke classification is challenging due to writing style variations, ambiguous content, and dynamic writing positions. The core problem is modeling semantic relationships between strokes, which are typically localized and difficult for existing deep learning methods to capture fine-grained interactions.

Method: 1. Encode strokes as reference pair representations (points + feature vectors) where reference points enable spatial queries and features mediate interaction modeling. 2. Dynamically select reference points for each stroke and sequence them. 3. Use Inline Sequence Attention (ISA) module to construct contextual features. 4. Employ Cross-Ellipse Query (CEQ) mechanism to cluster reference points and extract features across varying spatial scales. 5. Joint optimization framework predicts stroke categories via reference points regression and models adjacent stroke semantic transitions through an Auxiliary Branch.

Result: Achieves state-of-the-art performance on multiple public online handwritten datasets. Notably on CASIA-onDo dataset, accuracy improves from 93.81% to 95.54%, demonstrating effectiveness and robustness.

Conclusion: StrokeNet effectively addresses the challenge of modeling fine-grained semantic relationships between strokes by representing them as reference pair representations with dynamic point selection and spatial query mechanisms, leading to significant performance improvements in stroke classification tasks.

Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.

</details>


### [115] [Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation](https://arxiv.org/abs/2512.06306)
*Haoxian Zhou,Chuanzhi Xu,Langyi Chen,Haodong Chen,Yuk Ying Chung,Qiang Qu,Xaoming Chen,Weidong Cai*

Main category: cs.CV

TL;DR: Event-based human pose estimation using point cloud framework with temporal slicing convolution and edge enhancement, outperforming existing methods on DHP19 dataset.


<details>
  <summary>Details</summary>
Motivation: Existing event-based pose estimation methods convert event streams to dense frames, adding computation and losing temporal resolution. Need to better exploit spatiotemporal properties of raw event streams.

Method: Point cloud-based framework with Event Temporal Slicing Convolution (captures short-term dependencies), Event Slice Sequencing (structured temporal modeling), and edge enhancement in point cloud representation to improve spatial information under sparse conditions.

Result: Consistent performance improvements across three point cloud backbones (PointNet, DGCNN, Point Transformer) on DHP19 dataset.

Conclusion: Proposed point cloud framework with temporal slicing and edge enhancement effectively exploits spatiotemporal properties of event streams for improved human pose estimation while maintaining computational efficiency.

Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.

</details>


### [116] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: ReCAD is a reinforcement learning framework that bootstraps pretrained large models to generate precise parametric CAD models from text or images, achieving state-of-the-art performance by combining fine-tuned vision-language models with novel RL strategies.


<details>
  <summary>Details</summary>
Motivation: Previous CAD generation methods rely on supervised fine-tuning with limited editability and fail to exploit the strong generative priors of pretrained large models. There's a need for approaches that can generate precise parametric CAD models while maintaining editability and leveraging existing model capabilities.

Method: 1) Fine-tune vision-language models with rewritten parameterized CAD scripts for basic generation capabilities. 2) Novel RL strategy using parameterized code as guidance to enhance reasoning on challenging questions. 3) Hierarchical primitive learning process with unified reward function ensuring geometric accuracy and semantic fidelity.

Result: State-of-the-art performance in both text-to-CAD and image-to-CAD tasks. In image-to-CAD: reduces mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), significantly outperforming existing baselines.

Conclusion: ReCAD successfully bootstraps pretrained large models for precise parametric CAD generation through a novel RL framework, enabling complex CAD operations from simple interfaces while maintaining editability and exploiting generative priors.

Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [117] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

TL;DR: S2WMamba is a pansharpening method that uses 2D/1D wavelet transforms to disentangle frequency information, with Mamba-based cross-modulation for efficient long-range modeling, achieving state-of-the-art performance on multiple satellite datasets.


<details>
  <summary>Details</summary>
Motivation: Joint processing of PAN and MS images often entangles spatial detail with spectral fidelity, making it difficult to preserve both aspects simultaneously in pansharpening tasks.

Method: Uses 2D Haar DWT on PAN to localize spatial edges/textures, and channel-wise 1D Haar DWT on MS to separate spectral frequency components. Features two parallel branches (Spectral and Spatial) that exchange information through Mamba-based cross-modulation with linear complexity, followed by multi-scale dynamic gate fusion.

Result: Outperforms recent strong baselines (FusionMamba, CANNet, U2Net, ARConv) on WV3, GF2, and QB datasets, improving PSNR by up to 0.23 dB and achieving HQNR 0.956 on full-resolution WV3.

Conclusion: S2WMamba effectively disentangles frequency information through wavelet transforms and enables efficient cross-modal interaction via Mamba architecture, providing superior pansharpening performance with validated design choices through ablation studies.

Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [118] [CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks](https://arxiv.org/abs/2512.06332)
*Jeffrey Gu,Minkyu Jeon,Ambri Ma,Serena Yeung-Levy,Ellen D. Zhong*

Main category: cs.CV

TL;DR: CryoHype: A transformer-based hypernetwork for cryo-EM that reconstructs hundreds to thousands of distinct molecular structures from unlabeled images by dynamically adjusting implicit neural representations.


<details>
  <summary>Details</summary>
Motivation: Current cryo-EM methods focus on modeling conformational heterogeneity within single or few structures, but cannot handle compositional heterogeneity from mixtures of many distinct molecular species, limiting high-throughput structure determination.

Method: CryoHype uses a transformer-based hypernetwork that dynamically adjusts the weights of an implicit neural representation for cryo-EM reconstruction, enabling simultaneous reconstruction of many distinct molecular structures.

Result: Achieves state-of-the-art results on a benchmark dataset with 100 structures, and scales to reconstructing 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.

Conclusion: CryoHype enables high-throughput cryo-EM reconstruction of many molecular species simultaneously, addressing the challenge of compositional heterogeneity and expanding the technique's potential for large-scale structural biology.

Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.

</details>


### [119] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: MTGC is a multimodal-guided generative image compression framework that improves semantic consistency at ultra-low bitrates (<0.05 bpp) by integrating text captions, highly compressed images, and task-aware semantic pseudo-words.


<details>
  <summary>Details</summary>
Motivation: Generative image compression at ultra-low bitrates suffers from semantic deviations due to generative hallucinations, limiting reliable deployment in bandwidth-constrained 6G semantic communication scenarios.

Method: MTGC integrates three guidance modalities: text captions for global semantics, highly compressed images for low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. SPWs are generated by a Task-Aware Semantic Compression Module (TASCM) that operates in task-oriented manner. A Multimodal-Guided Diffusion Decoder (MGDD) uses dual-path cooperative guidance (cross-attention + ControlNet residuals) to inject these modalities into the diffusion process.

Result: Extensive experiments show MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on DIV2K dataset) while achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

Conclusion: The proposed MTGC framework effectively addresses semantic deviation issues in ultra-low bitrate generative image compression by leveraging multimodal guidance and task-aware semantic compression, making it suitable for bandwidth-constrained 6G semantic communication scenarios.

Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [120] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

TL;DR: CLUENet is a transparent deep architecture for visual semantic understanding that combines clustering paradigms with attention mechanisms to achieve better accuracy, efficiency, and interpretability than existing methods.


<details>
  <summary>Details</summary>
Motivation: Convolution- and attention-based models have rigid receptive fields and complex architectures that limit their ability to model irregular spatial patterns and hinder interpretability. Clustering paradigms offer interpretability but suffer from limited accuracy, low efficiency, and gradient vanishing during training.

Method: Proposes CLUENet with three key innovations: (1) Global Soft Aggregation and Hard Assignment with Temperature-Scaled Cosine Attention and gated residual connections for enhanced local modeling, (2) inter-block Hard and Shared Feature Dispatching, and (3) an improved cluster pooling strategy.

Result: Experiments on CIFAR-100 and Mini-ImageNet show that CLUENet outperforms existing clustering methods and mainstream visual models, achieving a compelling balance of accuracy, efficiency, and transparency.

Conclusion: CLUENet successfully addresses the limitations of both traditional vision models and clustering paradigms, offering a transparent architecture that combines the interpretability of clustering with the performance of modern deep learning approaches.

Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [121] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

TL;DR: TreeQ is a unified framework for efficient mixed-precision quantization of Diffusion Transformers (DiTs) that addresses key challenges through three novel components: Tree Structured Search, Environmental Noise Guidance, and General Monarch Branch.


<details>
  <summary>Details</summary>
Motivation: While DiTs outperform U-Nets for image generation, their real-world deployment is limited by high computational and memory demands. Mixed-precision quantization has been successful for U-Nets but remains underexplored for DiT architectures, creating a need for specialized quantization solutions.

Method: TreeQ introduces three key innovations: 1) Tree Structured Search (TSS) for efficient O(n) search leveraging DiT's linear properties, 2) Environmental Noise Guidance (ENG) to unify PTQ and QAT optimization objectives with a single hyperparameter, and 3) General Monarch Branch (GMB) as a structured sparse branch to prevent information loss in ultra-low-bit regimes.

Result: TreeQ achieves state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings, and is the first to achieve near-lossless 4-bit PTQ performance on DiT models.

Conclusion: TreeQ provides an effective unified framework for DiT quantization that addresses key challenges in search efficiency, objective alignment, and information preservation, enabling practical deployment of DiT models with significantly reduced computational and memory overhead.

Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [122] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

TL;DR: RefRAM: A latent diffusion model reframed for single-image reflection removal using reflection-equivariant VAE, task-specific text embedding, and depth-guided sampling to achieve SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Single-image reflection removal is highly ill-posed; existing methods struggle with reasoning about corrupted regions and fail at recovery and generalization in real-world scenarios. The core issue is that semantic encoder latent spaces lack structure to interpret composite images as linear superpositions of layers.

Method: Three synergistic components: 1) Reflection-equivariant VAE that aligns latent space with linear physics of reflection formation, 2) Learnable task-specific text embedding for precise guidance bypassing ambiguous language, 3) Depth-guided early-branching sampling strategy to harness generative stochasticity.

Result: Achieves new state-of-the-art performance on multiple benchmarks and generalizes well to challenging real-world cases.

Conclusion: Reframing latent diffusion models with physics-aligned latent spaces and task-specific guidance enables effective perception and processing of ambiguous layered images for high-quality reflection removal with strong generalization.

Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [123] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

TL;DR: SPL-UAD: A unified framework for detecting both physical presentation attacks and digital forgery attacks using spoofing-aware prompt learning with decoupled optimization branches.


<details>
  <summary>Details</summary>
Motivation: Real-world face recognition systems are vulnerable to both physical presentation attacks (like masks or photos) and digital forgery attacks (like deepfakes). Existing CLIP-based approaches suffer from conflicting optimization directions when trying to detect both types of attacks simultaneously under the same prompt spaces.

Method: Proposes SPL-UAD framework with: 1) Learnable parallel prompt branches with adaptive Spoofing Context Prompt Generation to decouple optimization for physical vs. digital attacks, 2) Cues-awareness Augmentation that uses dual-prompt mechanism to generate challenging sample mining tasks, enhancing robustness against unseen attacks.

Result: Extensive experiments on the large-scale UniAttackDataPlus dataset show significant performance improvements in unified attack detection tasks compared to existing approaches.

Conclusion: The proposed SPL-UAD framework effectively addresses the optimization conflict problem in unified attack detection by decoupling physical and digital attack optimization branches, achieving comprehensive biometric protection with enhanced robustness against unseen attack types.

Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [124] [Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos](https://arxiv.org/abs/2512.06368)
*Weitao Xiong,Zhiyuan Yuan,Jiahao Lu,Chengfeng Zhao,Peng Li,Yuan Liu*

Main category: cs.CV

TL;DR: Human3R: A method for monocular dynamic human video reconstruction using hybrid geometric priors (SMPL + monocular depth) to address geometric inconsistencies and resolution degradation in existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing monocular dynamic video reconstruction methods for human scenes suffer from geometric inconsistencies (distorted limb proportions, unnatural human-object fusion) and resolution degradation due to memory-constrained downsampling causing human boundary drift toward background geometry.

Method: Proposes Human3R with hybrid geometric priors combining SMPL human body models with monocular depth estimation. Uses hierarchical pipeline with refinement components: processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. Integrates SMPL priors through Feature Fusion Module for geometrically plausible reconstruction while preserving fine-grained human boundaries.

Result: Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction compared to existing methods.

Conclusion: The proposed Human3R method effectively addresses geometric inconsistency and resolution degradation issues in monocular dynamic human video reconstruction by incorporating structured human priors and hierarchical processing, achieving better reconstruction quality.

Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.

</details>


### [125] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

TL;DR: VG-Refiner is a new framework for tool-refined referring grounded reasoning that addresses unreliable tool outputs through a two-stage think-rethink mechanism and refinement rewards.


<details>
  <summary>Details</summary>
Motivation: Existing tool-integrated visual reasoning (TiVR) paradigms focus on integrating tools via reinforcement learning but neglect handling unreliable/erroneous tool outputs, especially in referring and grounding tasks where inaccurate detection predictions mislead models into hallucinated reasoning.

Method: Proposes VG-Refiner with: 1) Two-stage think-rethink mechanism for explicit analysis and response to tool feedback, 2) Refinement reward to encourage effective correction of poor tool results, 3) New metrics and evaluation protocols for systematic measurement of refinement ability, 4) Small amount of task-specific data to enhance refinement capability.

Result: Achieves significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

Conclusion: VG-Refiner effectively addresses the limitation of existing TiVR paradigms by providing a systematic framework for handling unreliable tool outputs in referring and grounding tasks, with demonstrated improvements in accuracy and refinement capability.

Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [126] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

TL;DR: The paper proposes a diagnostic framework to evaluate AI-generated driving videos (AIGVs) for autonomous driving applications, identifying failure modes, creating a benchmark, and developing an evaluator to filter AIGVs for safe use in training AD models.


<details>
  <summary>Details</summary>
Motivation: AI-generated driving videos offer a low-cost, scalable alternative to real or simulator data for autonomous driving, but it's unclear if they can reliably support training and evaluation of AD models. The paper aims to systematically study this question.

Method: 1) Introduce taxonomy of AIGV failure modes (visual artifacts, implausible motion, traffic violations); 2) Build ADGV-Bench benchmark with human annotations and dense labels for perception tasks; 3) Propose ADGVE evaluator combining static semantics, temporal cues, lane obedience, and VLM-guided reasoning into a quality score.

Result: Blindly adding raw AIGVs degrades perception performance, while filtering with ADGVE improves both video quality metrics and downstream AD models. Filtered AIGVs become a beneficial complement to real-world data.

Conclusion: The study highlights both risks and promise of AIGVs, providing practical tools for safely leveraging large-scale video generation in future AD pipelines. Proper filtering is essential for AIGVs to be useful for autonomous driving applications.

Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [127] [VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System](https://arxiv.org/abs/2512.06377)
*Yi Huo,Yun Ge*

Main category: cs.CV

TL;DR: Researchers add Dominance dimension to FER2013 dataset to create comprehensive VAD emotion annotations and improve prediction using orthogonal convolution.


<details>
  <summary>Details</summary>
Motivation: Current FER datasets use limited emotion categories (happy, angry, sad, etc.) while future affective computing needs more comprehensive VAD (Valence-Arousal-Dominance) metrics. Existing datasets like AffectNet lack the Dominance dimension.

Method: 1) Add VAD annotations to FER2013 dataset, focusing on the missing Dominance dimension. 2) Implement orthogonalized convolution in the network to extract more diverse features and improve prediction accuracy.

Result: Dominance dimension can be measured but is more difficult to obtain than Valence and Arousal in both manual annotation and network prediction. Orthogonal convolution improves VAD prediction accuracy. The VAD-annotated FER2013 dataset and orthogonalized ResNet-based network are publicly available.

Conclusion: The research provides the first Dominance dimension labeling for FER datasets and demonstrates that orthogonal convolution improves VAD prediction. The new dataset serves as a benchmark for VAD emotion measurement, and the network provides a baseline for VAD emotion prediction.

Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .

</details>


### [128] [OCFER-Net: Recognizing Facial Expression in Online Learning System](https://arxiv.org/abs/2512.06379)
*Yi Huo,Lei Zhang*

Main category: cs.CV

TL;DR: OCFER-Net improves facial expression recognition by enforcing orthogonality on convolutional kernels via regularization, achieving better feature diversity and performance on FER-2013 dataset.


<details>
  <summary>Details</summary>
Motivation: With online learning becoming crucial during COVID-19, emotion interaction is important for education. Facial Expression Recognition (FER) helps teachers understand student emotions, but existing methods don't fully exploit convolutional matrix orthogonality for better feature extraction.

Method: Proposes OCFER-Net that enforces orthogonality on convolutional kernels using a regularizer, which helps extract more diverse and expressive features for facial expression recognition.

Result: Achieves superior performance on the challenging FER-2013 dataset, outperforming baselines by 1.087 (likely percentage points or accuracy improvement).

Conclusion: Orthogonality regularization improves FER performance by enhancing feature diversity and expressiveness, with code publicly available for research and application in online learning contexts.

Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.

</details>


### [129] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

TL;DR: A region perception-based fusion framework for IR-VIS spectra using SVE camera that combines multi-exposure and multi-modal imaging for improved geometric fidelity and thermal radiation preservation in extreme conditions.


<details>
  <summary>Details</summary>
Motivation: Existing IR-VIS fusion methods often compromise visible imagery quality, affecting measurement accuracy, especially under extreme conditions where single-exposure methods have limitations.

Method: Region perception-based fusion framework using SVE camera with multi-exposure and multi-modal imaging. Features: region perception-based feature fusion for precise registration, adaptive fusion with contrast enhancement, structural similarity compensation guided by regional saliency maps, and adaptability to single-exposure scenarios.

Result: Superior image clarity and improved performance compared to state-of-the-art methods, demonstrated through experiments on both synthetic and real-world data with quantitative and visual evaluations.

Conclusion: The proposed framework effectively addresses IR-VIS fusion challenges in extreme conditions by preserving geometric fidelity while incorporating thermal radiation, outperforming existing methods.

Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [130] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

TL;DR: SAR (Self-Autoregressive Refinement) is a post-training method that addresses exposure bias in scale-wise autoregressive image generation models through staggered-scale rollouts and contrastive loss, improving quality with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Scale-wise autoregressive models suffer from exposure bias due to train-test mismatch (models must rely on imperfect predictions during inference) and imbalance in scale-wise learning difficulty, which undermines generation quality.

Method: Proposes Self-Autoregressive Refinement (SAR) with two components: 1) Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose models to their own intermediate predictions, aligning train-test patterns; 2) Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training.

Result: Applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For example, SAR yields 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs).

Conclusion: SAR is an efficient, scalable, and effective post-training method for visual autoregressive generation that addresses exposure bias through self-autoregressive refinement, making it a reliable enhancement for existing AR models.

Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [131] [A Perception CNN for Facial Expression Recognition](https://arxiv.org/abs/2512.06422)
*Chunwei Tian,Jingyuan Xie,Lingjun Li,Wangmeng Zuo,Yanning Zhang,David Zhang*

Main category: cs.CV

TL;DR: PCNN is a perception CNN for facial expression recognition that uses five parallel networks to learn local facial features (eyes, cheeks, mouth) and integrates them with global features via multi-domain interaction, achieving state-of-the-art results on multiple FER benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard CNNs may overlook the importance of facial segmentation in FER, failing to capture subtle local changes in facial expressions effectively.

Method: 1) Five parallel networks learn local features from eyes, cheeks, and mouth; 2) Multi-domain interaction mechanism fuses local organ features with global facial structure; 3) Two-phase loss function ensures accuracy of sense information and reconstructed face images.

Result: PCNN achieves superior performance on multiple FER benchmarks including CK+, JAFFE, FER2013, FERPlus, RAF-DB, and Occlusion and Pose Variant Dataset.

Conclusion: The proposed PCNN effectively captures subtle facial expression changes by integrating local feature learning with global structure, demonstrating state-of-the-art performance across diverse FER datasets.

Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.

</details>


### [132] [DragMesh: Interactive 3D Generation Made Easy](https://arxiv.org/abs/2512.06424)
*Tianshan Zhang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: DragMesh: A real-time interactive 3D articulation framework that decouples kinematic reasoning from motion generation, using joint parameter inference and a Dual Quaternion VAE to produce physically plausible motion trajectories.


<details>
  <summary>Details</summary>
Motivation: Current methods for articulated motion are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. There's a need for systems that can understand how objects move and respond to interactions in real-time while maintaining physical plausibility.

Method: 1) Decoupled kinematic reasoning: infer latent joint parameters by separating semantic intent reasoning (joint type) from geometric regression (axis and origin using Kinematics Prediction Network). 2) Dual Quaternion VAE (DQ-VAE) that receives predicted priors and user drag to generate motion trajectories. 3) FiLM conditioning injects joint priors at every layer of the Transformer decoder for strict kinematic adherence. 4) Numerically-stable cross-product loss ensures axis alignment.

Result: DragMesh achieves real-time performance and enables plausible, generative articulation on novel objects without retraining. The framework offers practical generative 3D intelligence with physically consistent motion.

Conclusion: The decoupled design of DragMesh represents a practical step toward generative 3D intelligence, balancing real-time performance with physical plausibility through innovative kinematic reasoning and motion generation techniques.

Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.

</details>


### [133] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proen莽a*

Main category: cs.CV

TL;DR: Dual-path transformer framework using CLIP for gender recognition from extreme long-range imagery, combining visual and attribute-driven cues with spatial attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Gender recognition from extreme long-range imagery is challenging due to limited spatial resolution, viewpoint variability, and loss of facial cues. Existing methods struggle with these constraints in unconstrained scenarios.

Method: Dual-path transformer framework leveraging CLIP: (1) direct visual path with selective fine-tuning of CLIP image encoder upper layers, (2) attribute-mediated path using soft-biometric prompts (hairstyle, clothing, accessories) aligned in CLIP text-image space, plus spatial channel attention modules for discriminative localization.

Result: Proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Created U-DetAGReID dataset for evaluation.

Conclusion: Language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios, with interpretable attribute localization and responsible abstention behavior.

Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [134] [Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening](https://arxiv.org/abs/2512.06434)
*Lucas R. Mareque,Ricardo L. Armentano,Leandro J. Cymberknop*

Main category: cs.CV

TL;DR: Deep learning models can accurately estimate key anthropometric measurements from 2D synthetic body images with sub-centimeter accuracy, potentially automating athlete cardiovascular screening.


<details>
  <summary>Details</summary>
Motivation: Traditional manual anthropometric measurements for preparticipation cardiovascular examination are labor-intensive, operator-dependent, and difficult to scale, creating a need for automated solutions to identify athletes at risk of sudden cardiac death.

Method: Developed a fully automated deep-learning approach using VGG19, ResNet50, and DenseNet121 with fully connected layers for regression, trained on 100,000 synthetic 2D images derived from 3D body meshes to estimate five key anthropometric measurements.

Result: All models achieved sub-centimeter accuracy, with ResNet50 performing best (mean MAE of 0.668 cm across all measurements), demonstrating that deep learning can deliver accurate anthropometric data at scale.

Conclusion: Deep learning offers a practical tool to complement athlete screening protocols by automating anthropometric measurements, with future work needed to validate models on real-world images for broader applicability.

Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.

</details>


### [135] [AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars](https://arxiv.org/abs/2512.06438)
*Ramazan Fazylov,Sergey Zagoruyko,Aleksandr Parkin,Stamatis Lefkimmiatis,Ivan Laptev*

Main category: cs.CV

TL;DR: AGORA extends 3D Gaussian Splatting with GAN framework to create animatable 3D human avatars with real-time rendering and fine-grained expression control, outperforming NeRF methods while achieving 250+ FPS on GPU and ~9 FPS on CPU.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D human avatar generation have limitations: NeRF-based approaches suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting methods are limited to static head generation and lack dynamic control. There's a need for high-fidelity, animatable avatars with real-time performance for VR, telepresence, and entertainment applications.

Method: AGORA extends 3D Gaussian Splatting within a generative adversarial network framework. Key innovation is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving expression control. Uses dual-discriminator training scheme leveraging synthetic renderings of parametric mesh for expression fidelity.

Result: Outperforms state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on single GPU and ~9 FPS under CPU-only inference (first practical CPU-only animatable 3DGS avatar synthesis). Generates visually realistic and precisely controllable avatars.

Conclusion: AGORA represents a significant step toward practical, high-performance digital humans by bridging the gap between high-quality avatar generation and real-time animatability, enabling applications in VR, telepresence, and entertainment with unprecedented performance.

Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/

</details>


### [136] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: A unified multimodal framework (SCD-MLLM) for stable cross-domain depression recognition that handles heterogeneous data sources and missing modalities through adaptive fusion mechanisms.


<details>
  <summary>Details</summary>
Motivation: Depression screening needs timely, scalable solutions. Current multimodal ADD methods lack unified frameworks for diverse scenarios and are unstable with missing modalities common in real-world data.

Method: Proposes SCD-MLLM with two key components: 1) Multi-Source Data Input Adapter (MDIA) using masking and prompts to unify heterogeneous inputs, 2) Modality-Aware Adaptive Fusion Module (MAFM) for adaptive audio-visual feature integration with shared projection.

Result: Outperforms SOTA models and commercial LLMs (Gemini, GPT) across five diverse depression datasets (CMDC, AVEC2014, DAIC-WOZ, DVlog, EATD) in both complete and partial modality settings.

Conclusion: SCD-MLLM demonstrates superior cross-domain generalization, better multimodal depression cue capture, and strong stability with missing modalities, making it suitable for real-world depression screening applications.

Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [137] [Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction](https://arxiv.org/abs/2512.06485)
*Kush Revankar,Shreyas Deshpande,Araham Sayeed,Ansh Tandale,Sarika Bobde*

Main category: cs.CV

TL;DR: Sanvaad is a lightweight multimodal accessibility framework enabling real-time two-way communication between deaf, visually impaired, and hearing users through sign language recognition, speech-to-sign translation, and voice interfaces.


<details>
  <summary>Details</summary>
Motivation: Current communication tools often support only one direction of interaction between deaf users, visually impaired users, and the general hearing population, creating barriers to inclusive communication.

Method: Uses MediaPipe landmarks for efficient ISL recognition on edge devices, voice-to-sign component mapping speech to predefined phrases with GIFs/alphabet visualizations, and screen-free voice interface with multilingual speech recognition, text summarization, and TTS. Built with Streamlit for cross-platform use.

Result: Developed a practical, accessible framework that enables real-time two-way communication through lightweight computer vision and speech processing tools that work on both desktop and mobile environments without dedicated hardware.

Conclusion: Sanvaad provides a unified, practical solution for inclusive communication by combining efficient multimodal technologies in a lightweight framework accessible to diverse user groups.

Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.

</details>


### [138] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

TL;DR: An intelligent multi-modal framework for automated PV inspection that addresses thermal bias, data redundancy, and bandwidth issues, achieving 90.3% mAP and significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome critical shortcomings of conventional PV inspection methods including thermal palette bias, data redundancy, and high communication bandwidth requirements, while enhancing plant safety and operational efficiency through fully automated monitoring.

Method: A synergistic architecture with palette-invariant thermal embedding (learned via representational consistency), fused with contrast-normalized RGB via gated mechanism, plus adaptive re-acquisition controller using Rodrigues-based updates for ambiguous anomalies, and geospatial deduplication module using DBSCAN over haversine distance.

Result: Achieved mAP@0.5 of 0.903 on PVF-10 benchmark (12-15% improvement over single-modality baselines), 96% recall in field validation, 15-20% reduction in duplicate-induced false positives, and 60-70% reduction in airborne data transmission through relevance-only telemetry.

Conclusion: Establishes a powerful new paradigm for proactive PV inspection with validated system readiness, demonstrating significant improvements in detection accuracy, operational efficiency, and data transmission optimization.

Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [139] [ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images](https://arxiv.org/abs/2512.06521)
*Jens Dede,Anna F枚rster*

Main category: cs.CV

TL;DR: ShadowWolf is a unified AI framework for wildlife monitoring that dynamically adapts to changing environmental conditions, reducing labeling effort and improving model robustness for real-world conservation applications.


<details>
  <summary>Details</summary>
Motivation: Increasing human-wildlife interactions due to habitat expansion create challenges for wildlife monitoring. Traditional AI approaches struggle with environmental variability (landscape, weather, lighting, camera distances), requiring robust and adaptable solutions for effective conservation.

Method: Proposes ShadowWolf, a unified framework that integrates and optimizes AI model training and evaluation stages. Features dynamic model retraining to adapt to changing environmental conditions and application requirements, enabling on-site model adaptation with reduced labeling effort.

Result: The framework enhances accuracy and efficiency of wildlife monitoring systems by addressing real-world environmental variability challenges. It reduces manual labeling requirements while improving model robustness and adaptability.

Conclusion: ShadowWolf's adaptive, unified approach enables more effective and scalable conservation efforts by automating wildlife recognition while dynamically adjusting to changing environmental conditions and application needs.

Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.

</details>


### [140] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

TL;DR: Learned k-space acquisition patterns improve MRI reconstruction quality and show surprising transferability across imaging domains, with a novel method using stochastic trajectory perturbations to enhance domain robustness.


<details>
  <summary>Details</summary>
Motivation: Most existing learned k-space sampling research focuses on single datasets/modalities with limited consideration of transferability across domains. The authors aim to demonstrate that learned sampling benefits can extend beyond training domains and improve generalization in accelerated MRI reconstruction.

Method: Two main approaches: 1) Systematic evaluation of learned sampling patterns across datasets and acquisition paradigms to assess cross-domain generalization. 2) A novel method that enhances domain robustness by introducing acquisition uncertainty during training - stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions.

Result: Models trained with learned sampling patterns exhibit improved generalization under cross-domain settings. The proposed stochastic perturbation method further enhances domain robustness, demonstrating that learned k-space sampling can effectively transfer across imaging domains.

Conclusion: K-space trajectory design should be treated not just as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction. Learned sampling patterns show promising transferability across imaging domains when properly designed.

Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [141] [Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images](https://arxiv.org/abs/2512.06531)
*Sayan Das,Arghadip Biswas*

Main category: cs.CV

TL;DR: Proposed two novel deep learning architectures for brain tumor detection: SAETCN for classification (99.38% accuracy) and SAS-Net for segmentation (99.23% pixel accuracy).


<details>
  <summary>Details</summary>
Motivation: Manual brain tumor detection from MRI scans is time-consuming and difficult due to rising incidence rates and large data volumes. Existing AI models lack generalization and perform poorly on validation data.

Method: Developed two novel deep learning architectures: (1) SAETCN (Self-Attention Enhancement Tumor Classification Network) for classifying glioma, meningioma, pituitary tumors, and non-tumor cases; (2) SAS-Net (Self-Attentive Segmentation Network) for accurate tumor segmentation.

Result: SAETCN achieved 99.38% accuracy on validation dataset for tumor classification. SAS-Net achieved 99.23% overall pixel accuracy for tumor segmentation.

Conclusion: The proposed architectures demonstrate high accuracy in both classification and segmentation tasks, offering promising CAD solutions for early brain tumor detection and addressing limitations of existing models.

Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.

</details>


### [142] [Bridging spatial awareness and global context in medical image segmentation](https://arxiv.org/abs/2512.06560)
*Dalia Alzu'bi,A. Ben Hamza*

Main category: cs.CV

TL;DR: U-CycleMLP: A lightweight U-shaped encoder-decoder network for medical image segmentation that effectively captures local and global context using position attention, dense atrous blocks, and channel CycleMLP blocks, achieving competitive accuracy across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing medical image segmentation models struggle to balance accuracy and efficiency while capturing both local and global contextual information, leading to boundary pixel loss and segmentation errors.

Method: U-shaped encoder-decoder architecture with: 1) encoder using position attention weight excitation blocks, dense atrous blocks, and downsampling to capture multiscale features; 2) decoder with upsampling, dense atrous blocks, and feature fusion; 3) channel CycleMLP blocks in decoder skip connections for enhanced feature integration with linear computational complexity.

Result: Competitive performance across three benchmark datasets, achieving better segmentation accuracy than state-of-the-art methods, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities.

Conclusion: U-CycleMLP effectively balances segmentation accuracy and computational efficiency, with ablation studies confirming the importance of its core architectural components for enhanced medical image segmentation performance.

Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.

</details>


### [143] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

TL;DR: SUGAR is a framework for scalable generative unlearning that removes specific identities from 3D-aware generative models without retraining, using personalized surrogate latents and continual utility preservation.


<details>
  <summary>Details</summary>
Motivation: Addresses urgent concerns about user consent and identity removal in 3D-aware generative models, enabling removal of specific individuals from model outputs without compromising overall model quality.

Method: Learns personalized surrogate latent for each identity to divert reconstructions to visually coherent alternatives, introduces continual utility preservation objective to prevent degradation as more identities are forgotten.

Result: Achieves state-of-the-art performance in removing up to 200 identities, with 700% improvement in retention utility compared to existing baselines.

Conclusion: SUGAR provides an effective framework for scalable generative unlearning that balances identity removal with preservation of model quality and diversity.

Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [144] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

TL;DR: GNC-Pose is a learning-free monocular 6D object pose estimation method that uses rendering-based initialization, geometry-aware correspondence weighting, and GNC optimization to achieve competitive accuracy without training data.


<details>
  <summary>Details</summary>
Motivation: To create a robust 6D object pose estimation system that doesn't require learned features, training data, or category-specific priors, offering a simple and practical learning-free solution that can handle severe outlier contamination.

Method: Combines rendering-based initialization for coarse 2D-3D correspondences, introduces geometry-aware cluster-based weighting mechanism based on 3D structural consistency, uses Graduated Non-Convexity (GNC) optimization for robustness against outliers, and includes final LM refinement for accuracy improvement.

Result: Achieves competitive accuracy on YCB Object and Model Set compared to both learning-based and learning-free methods, despite requiring no learned features, training data, or category-specific priors.

Conclusion: GNC-Pose provides a simple, robust, and practical learning-free solution for 6D pose estimation that effectively handles outlier contamination through geometric priors and GNC optimization, making it a viable alternative to learning-based approaches.

Abstract: We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [145] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

TL;DR: MedVidBench is a large medical video benchmark with 531,850 video-instruction pairs across 8 sources. MedGRPO is a novel RL framework with cross-dataset reward normalization and medical LLM judge that overcomes RL training collapse in multi-dataset medical video understanding.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models struggle with medical video understanding due to requirements for spatial precision, temporal reasoning, and clinical semantics. There's a need for comprehensive benchmarks and robust training methodologies specifically for medical domains.

Method: 1) Created MedVidBench benchmark with rigorous quality assurance pipeline using expert-guided prompting and dual-model validation. 2) Developed MedGRPO RL framework with cross-dataset reward normalization (maps each dataset's median performance to common reward value) and medical LLM judge (evaluates caption quality on five clinical dimensions through comparative similarity scoring).

Result: Supervised fine-tuning of Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks. MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks, overcoming RL training collapse issues.

Conclusion: The work establishes a foundational benchmark (MedVidBench) and robust training methodology (MedGRPO) for advancing vision-language models in medical domains, addressing critical challenges in medical video understanding.

Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [146] [From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain](https://arxiv.org/abs/2512.06598)
*Muhammad Adil,Patrick J. Clemins,Andrew W. Schroth,Panagiotis D. Oikonomou,Donna M. Rizzo,Peter D. F. Isles,Xiaohan Zhang,Kareem I. Hannoun,Scott Turnbull,Noah B. Beckage,Asim Zia,Safwan Wshah*

Main category: cs.CV

TL;DR: Transformer-BiLSTM model predicts cyanobacterial harmful algal blooms up to 14 days ahead using sparse satellite data, achieving strong forecasting performance for early warning systems.


<details>
  <summary>Details</summary>
Motivation: CyanoHABs threaten aquatic ecosystems and public health globally, with Lake Champlain being particularly vulnerable. Remote sensing offers scalable monitoring where in situ data is sparse, but existing satellite data has significant gaps (30% CI data, 90% temperature data missing).

Method: A remote sensing-only forecasting framework combining Transformers and BiLSTM to predict CyanoHAB intensities. Uses Cyanobacterial Index data from CyAN and temperature data from MODIS satellites. Two-stage preprocessing: forward fill + weighted temporal imputation at pixel level, then smoothing. Feature engineering via equal frequency binning for CI values and extracted temperature statistics.

Result: Transformer-BiLSTM achieves F1 scores of 89.5% (1-day), 86.4% (2-day), 85.5% (3-day), and maintains 78.9% F1 with 82.6% AUC at 14-day horizon. Demonstrates ability to capture complex spatiotemporal dynamics from sparse satellite data.

Conclusion: The model provides reliable early warning for CyanoHABs management, confirming that deep learning approaches can effectively handle sparse satellite data for environmental forecasting applications.

Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.

</details>


### [147] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

TL;DR: STRank: A novel loss function for gene expression estimation from pathology images that focuses on learning relative expression patterns rather than absolute values, making it robust to noise and batch effects.


<details>
  <summary>Details</summary>
Motivation: Traditional point-wise loss functions struggle with accurately estimating absolute gene expression values due to stochastic noise and batch effects from sequencing techniques and cellular variability. The authors propose focusing on relative expression patterns which are more consistent across experiments despite these issues.

Method: The authors propose STRank, a novel loss function that models relative expression patterns rather than absolute values. It assumes that relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute values are affected by noise and batch effects.

Result: Experiments using both synthetic and real datasets demonstrate the effectiveness of STRank in learning relative expression patterns that are robust to noise and batch effects.

Conclusion: STRank provides a more robust approach to gene expression estimation from pathology images by focusing on relative expression patterns rather than absolute values, addressing the challenges of noise and batch effects in RNA sequencing data.

Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [148] [Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach](https://arxiv.org/abs/2512.06613)
*Yueying Ke*

Main category: cs.CV

TL;DR: Hierarchical CNN with cascaded heads for diatom classification outperforms flat models by embedding taxonomic hierarchy, improving accuracy at upper levels and keeping errors taxonomically local.


<details>
  <summary>Details</summary>
Motivation: Conventional diatom identification relies heavily on expert taxonomists, and while deep learning helps automation, most approaches treat classification as flat, predicting only one taxonomic rank without leveraging hierarchical relationships.

Method: Hierarchical convolutional network with five cascaded heads predicting class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference.

Result: Hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, 92.5% of misclassified species are correctly predicted at genus level (vs. 67.2% for flat). Reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).

Conclusion: Hierarchical embedding improves multi-level taxonomic classification through bidirectional mechanisms: top-down constraint masks restrict prediction space, while bottom-up gradient propagation refines features. Produces more robust, interpretable, and biologically aligned predictions.

Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.

</details>


### [149] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

TL;DR: MAE pretraining on simulated strong-lensing images improves classification of dark matter models and super-resolution reconstruction compared to training from scratch.


<details>
  <summary>Details</summary>
Motivation: Strong gravitational lensing can reveal dark-matter substructure, but analyzing noisy, low-resolution images is challenging. Need generalizable representations for multiple analysis tasks.

Method: Use masked autoencoder (MAE) pretraining on simulated strong-lensing images from DeepLense benchmark. Pretrain Vision Transformer encoder with masked image modeling, then fine-tune separately for classification (dark matter models) and super-resolution (16x16 to 64x64).

Result: At 90% mask ratio: classifier achieves AUC 0.968 (vs 0.957 baseline) and accuracy 88.65% (vs 82.46%). Super-resolution achieves PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. Higher mask ratios improve classification but slightly degrade reconstruction.

Conclusion: MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks, outperforming training from scratch.

Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [150] [TextMamba: Scene Text Detector with Mamba](https://arxiv.org/abs/2512.06657)
*Qiyan Zhao,Yue Yan,Da-Han Wang*

Main category: cs.CV

TL;DR: A novel scene text detector using Mamba's selection mechanism with attention layers, enhanced by Top_k algorithm for key information selection, dual-scale feed-forward network, and embedding pyramid module, achieving SOTA performance on text detection benchmarks.


<details>
  <summary>Details</summary>
Motivation: Transformer-based methods for scene text detection have limitations in cross-domain performance and long-range dependency modeling, often forgetting important information or focusing on irrelevant representations. Mamba's linear complexity selection mechanism offers better long-range dependency modeling.

Method: Proposes a Mamba-based scene text detector that integrates selection mechanism with attention layers. Uses Top_k algorithm to explicitly select key information and reduce irrelevant information interference. Includes dual-scale feed-forward network for high-dimensional hidden state interactions and embedding pyramid enhancement module for multi-scale feature fusion.

Result: Achieves state-of-the-art or competitive performance: 89.7% F-measure on CTW1500, 89.2% on TotalText, and 78.5% on ICDAR19ArT benchmarks.

Conclusion: The proposed Mamba-based approach effectively addresses Transformer limitations in scene text detection by combining selection mechanisms with attention, achieving superior performance on challenging benchmarks through improved long-range dependency modeling and information selection.

Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.

</details>


### [151] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: DEPER learns personalized subject embeddings combining linguistic style and viewing behavior via attention prediction, enabling few-shot personalization for image description generation.


<details>
  <summary>Details</summary>
Motivation: Existing personalized image description models only focus on linguistic style, ignoring individual viewing patterns. People view images differently - focusing on different regions, objects, and details in varying orders - which significantly affects description variability.

Method: DEPER learns subject embeddings capturing both linguistic style and viewing behavior through an auxiliary attention-prediction task. Uses lightweight adapter to align embeddings with frozen vision-language model for few-shot personalization without retraining.

Result: Achieves 24% average improvement across four datasets spanning diverse viewing tasks and description types (short and detailed). Shows modeling personalized attention produces more human-aligned and high-quality descriptions.

Conclusion: Understanding how people see helps predict what they say; modeling human diversity in perception improves both performance and human alignment in multimodal systems.

Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [152] [CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks](https://arxiv.org/abs/2512.06663)
*Yu Qi,Yumeng Zhang,Chenting Gong,Xiao Tan,Weiming Zhang,Wei Zhang,Jingdong Wang*

Main category: cs.CV

TL;DR: CoT4Det reformulates perception tasks into classification, counting, and grounding steps to improve LVLM performance on object detection without compromising general vision-language capabilities.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models (LVLMs) perform poorly on perception-centric tasks like object detection compared to task-specific expert models, struggling with dense scenes and small object recall.

Method: Chain-of-Thought for Detection (CoT4Det) reformulates perception tasks into three interpretable steps: classification, counting, and grounding, aligning better with LVLM reasoning capabilities.

Result: CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val with Qwen2.5-VL-7B-Instruct, outperforms baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

Conclusion: The proposed CoT4Det strategy significantly improves perception performance of LVLMs while maintaining their general vision-language capabilities, demonstrating effective task reformulation.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

</details>


### [153] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

TL;DR: DEViL is a detector-empowered video LLM that improves spatio-temporal grounding by connecting a video LLM with an open-vocabulary detector via reference-semantic tokens, avoiding autoregressive spatial decoding issues and enabling end-to-end learning of referential understanding and spatial localization.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs for spatio-temporal grounding use autoregressive spatial decoding with bounding boxes as text tokens, which causes long output sequences, spatial error accumulation over time, and progressive localization drift across videos.

Method: DEViL couples a Video LLM with an open-vocabulary detector using reference-semantic tokens (RST) that distill user queries into rich semantic representations. The RST serves as both control signal and replacement for the detector's text embedding. Also includes tube-mined temporal regularization (TTReg) to ensure temporally-consistent object queries.

Result: DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG (Spatio-Temporal Video Grounding) and GroundedVQA.

Conclusion: The proposed DEViL framework effectively addresses limitations of autoregressive spatial decoding in video understanding by integrating detector capabilities with LLMs through semantic tokens, enabling better spatio-temporal grounding and reasoning.

Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [154] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

TL;DR: RunawayEvil is the first multimodal jailbreak framework for Image-to-Video models that uses a self-evolving "Strategy-Tactic-Action" architecture to generate coordinated text and image attacks, achieving state-of-the-art attack success rates on commercial I2V systems.


<details>
  <summary>Details</summary>
Motivation: The security of multimodal Image-to-Video generation systems is critically underexplored, particularly their vulnerability to jailbreak attacks. Current research lacks comprehensive frameworks to test and understand these vulnerabilities in I2V models.

Method: A three-component "Strategy-Tactic-Action" framework: (1) Strategy-Aware Command Unit uses reinforcement learning and LLMs for self-evolving attack strategies; (2) Multimodal Tactical Planning Unit generates coordinated text jailbreak instructions and image tampering guidelines; (3) Tactical Action Unit executes and evaluates multimodal coordinated attacks.

Result: RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models like Open-Sora 2.0 and CogVideoX, outperforming existing methods by 58.5 to 79 percent on COCO2017 benchmark.

Conclusion: The work provides a critical tool for vulnerability analysis of I2V models, laying a foundation for developing more robust and secure video generation systems by exposing current security weaknesses.

Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [155] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

TL;DR: EMGauss: A Gaussian splatting framework for 3D reconstruction from 2D slices that treats slice progression as temporal evolution, avoiding isotropy assumptions and enabling continuous slice synthesis.


<details>
  <summary>Details</summary>
Motivation: Volume electron microscopy (vEM) produces anisotropic volumes with limited axial resolution due to acquisition trade-offs. Existing deep learning methods fail for morphologically anisotropic structures because they rely on isotropy assumptions that don't hold for such biological structures.

Method: Reframes slice-to-3D reconstruction as 3D dynamic scene rendering using Gaussian splatting, modeling axial slice progression as temporal evolution of 2D Gaussian point clouds. Incorporates Teacher-Student bootstrapping to use high-confidence predictions on unobserved slices as pseudo-supervisory signals for data-sparse regimes.

Result: Substantially improves interpolation quality compared to diffusion- and GAN-based methods, enables continuous slice synthesis, and eliminates need for large-scale pretraining. Provides generalizable slice-to-3D solution applicable beyond vEM to diverse imaging domains.

Conclusion: EMGauss circumvents limitations of isotropy-based approaches for vEM reconstruction, offering a general framework for 3D reconstruction from planar scanned 2D slices that handles anisotropic structures effectively and has broad applicability across imaging domains.

Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [156] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: UniVoiceLite is a lightweight unsupervised audio-visual framework that unifies speech enhancement and speech separation using lip motion and facial identity cues, eliminating the need for paired noisy-clean data.


<details>
  <summary>Details</summary>
Motivation: Real-world audio often contains both background noise and overlapping speakers, requiring a unified solution for speech enhancement and separation. Existing multi-stage approaches are complex, parameter-heavy, and rely on supervised training, limiting scalability and generalization.

Method: UniVoiceLite uses lip motion and facial identity cues to guide speech extraction, employs Wasserstein distance regularization to stabilize the latent space, and operates in an unsupervised manner without requiring paired noisy-clean data.

Result: Experimental results show UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization.

Conclusion: UniVoiceLite provides a lightweight, unsupervised solution that effectively unifies speech enhancement and speech separation, addressing real-world audio challenges with improved scalability and generalization.

Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [157] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: ECVGPO is an entropy control algorithm for visual grounding tasks that balances exploration-exploitation trade-off, achieving broad improvements across benchmarks and models.


<details>
  <summary>Details</summary>
Motivation: While entropy control techniques have advanced MLLM fine-tuning via RL, their role in perception-oriented tasks like visual grounding remains unexplored. The paper aims to understand entropy characteristics in visual grounding vs. reasoning tasks and develop effective control strategies.

Method: ECVGPO (Entropy Control Visual Grounding Policy Optimization) - an interpretable algorithm for effective entropy regulation in visual grounding. Analyzes entropy role/characteristics in visual grounding compared to reasoning tasks, then designs entropy control to better balance exploration-exploitation trade-off.

Result: Experiments show ECVGPO achieves broad improvements across various benchmarks and models, demonstrating the effectiveness of entropy control for visual grounding tasks.

Conclusion: Entropy control is crucial for visual grounding tasks, and ECVGPO provides an effective interpretable algorithm that balances exploration-exploitation, leading to significant performance improvements across diverse benchmarks and models.

Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [158] [Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data](https://arxiv.org/abs/2512.06736)
*Jiaxing Fan,Jiaojiao Liu,Wenkong Wang,Yang Zhang,Xin Ma,Jichen Zhang*

Main category: cs.CV

TL;DR: GCN-LSTM-ATT network using skeleton data achieves 85.8% accuracy in detecting compensatory movements in stroke patients, outperforming traditional ML methods.


<details>
  <summary>Details</summary>
Motivation: Stroke patients often develop compensatory movements during rehabilitation that hinder long-term recovery, making detection of these movements crucial for effective rehabilitation.

Method: Proposed Graph Convolutional LSTM Attention Network (GCN-LSTM-ATT) using skeleton data from Kinect camera; compared with SVM, KNN, and Random Forest on data from 16 stroke patients performing rehabilitation movements.

Result: GCN-LSTM-ATT achieved 0.8580 detection accuracy, significantly outperforming traditional ML algorithms; ablation studies confirmed each component contributes to performance improvement.

Conclusion: The GCN-LSTM-ATT model provides a precise tool for detecting compensatory movements in stroke rehabilitation, potentially optimizing training strategies and improving patient outcomes.

Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.

</details>


### [159] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

TL;DR: FedSCAl is an FL framework for Federated source-Free Domain Adaptation that addresses client-drift in heterogeneous domains by aligning server-client predictions to improve pseudo-labeling accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the Federated source-Free Domain Adaptation (FFreeDA) problem where clients have unlabeled data with significant inter-client domain gaps, and access to source dataset is restricted during training. Existing SFDA methods adapted to FL struggle with client-drift due to extreme data heterogeneity, leading to unreliable pseudo-labels.

Method: FedSCAl framework with Server-Client Alignment (SCAl) mechanism that regularizes client updates by aligning the predictions of client and server models. This alignment helps mitigate client-drift and improves pseudo-labeling accuracy.

Result: FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks, as demonstrated through extensive experiments on benchmark vision datasets.

Conclusion: The proposed FedSCAl framework effectively addresses the challenges of FFreeDA by using server-client alignment to reduce client-drift and improve pseudo-labeling accuracy in heterogeneous federated learning scenarios.

Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [160] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: VLMs underperform in AI-generated image detection due to task-model misalignment. The paper proposes AlignGemini, a two-branch detector with separate semantic and pixel-artifact experts, achieving +9.5% average accuracy gain.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models (VLMs) are increasingly used for AI-generated image detection but require substantial resources for fine-tuning and still exhibit severe hallucinations. The core issue is task-model misalignment: semantics-oriented VLMs lack sensitivity to fine-grained pixel artifacts, while conventional pixel-artifact detectors lack semantic awareness.

Method: Formalizes AIGI detection as two complementary tasks: semantic consistency checking and pixel-artifact detection. Introduces Task-Model Alignment principle and instantiates it as AlignGemini - a two-branch detector with: 1) VLM fine-tuned exclusively with pure semantic supervision, and 2) pixel-artifact expert trained exclusively with pure pixel-artifact supervision. Uses orthogonal supervision on simplified datasets.

Result: On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy compared to existing methods, demonstrating that task-model alignment is an effective approach for generalizable AIGI detection.

Conclusion: Neglecting either semantic consistency checking or pixel-artifact detection induces systematic blind spots in AIGI detection. The Task-Model Alignment principle, implemented through complementary semantic and pixel-artifact branches, provides a more effective path to generalizable AI-generated image detection.

Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [161] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

TL;DR: UARE is the first unified vision-language model for image quality assessment, restoration, and enhancement that uses IQA to guide restoration through multi-task co-training.


<details>
  <summary>Details</summary>
Motivation: Image quality assessment (IQA) and restoration are conceptually connected but typically treated separately. Recent unified multimodal models show that stronger understanding can improve generation, motivating a single model that unifies IQA and restoration to study how quality assessment can guide restoration.

Method: Built on pretrained unified understanding-generation models with a two-stage training framework: 1) Progressive easy-to-hard schedule from single-type distortions to mixed degradations, 2) Unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives through multi-task co-training.

Result: Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE, showing improved performance by leveraging IQA to boost restoration and enhancement.

Conclusion: UARE successfully unifies image quality assessment, restoration, and enhancement in a single model, demonstrating that explicit integration of IQA can effectively guide and improve restoration performance, addressing a previously underexplored but valuable research direction.

Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [162] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

TL;DR: VisChainBench is a new benchmark for evaluating Large Vision-Language Models' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on static comparisons and rely heavily on language cues, overlooking progressive context-dependent reasoning and visual-to-visual inference challenges in multi-image, multi-turn scenarios.

Method: Created VisChainBench with 1,457 tasks spanning over 20,000 images across three diverse domains (daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes using a multi-agent generation pipeline for high visual diversity and controlled language bias.

Result: A large-scale benchmark with 1,457 tasks and over 20,000 images across three domains, available for public use with all data and code accessible via Hugging Face.

Conclusion: VisChainBench addresses a critical gap in evaluating LVLMs' multi-step visual reasoning capabilities and provides a comprehensive benchmark for assessing progressive, context-dependent visual reasoning with minimal language reliance.

Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [163] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: MMDuet2: A proactive Video MLLM that autonomously decides when to respond during video playback using RL-based training without precise timing annotations.


<details>
  <summary>Details</summary>
Motivation: Existing Video MLLMs operate in turn-based manner, but real-time applications require proactive interaction where models decide when to reply during video streaming. Previous methods face challenges with manual threshold tuning and precise reply time annotations.

Method: Text-to-text approach where model decides to respond or remain silent at each turn based on dialogue history and visual context. Uses multi-turn RL-based training to encourage timely responses without requiring precise response time annotations. Trained on 52k videos with two dialogue types via SFT and RL.

Result: MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

Conclusion: The proposed RL-based training approach enables effective proactive interaction in Video MLLMs without precise timing annotations, advancing real-time video understanding applications.

Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [164] [JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms](https://arxiv.org/abs/2512.06763)
*Chengyang Yan,Mitch Bryson,Donald G. Dansereau*

Main category: cs.CV

TL;DR: Joint optimization of camera hardware and adaptive control algorithms with vision tasks using hybrid gradient-based/derivative-free methods improves perception performance.


<details>
  <summary>Details</summary>
Motivation: Most prior camera-perception co-design approaches focus on optimizing fixed manufacturing parameters, but many parameters like exposure require adaptive runtime control. There's a need to jointly optimize both hardware and adaptive control algorithms.

Method: Proposes a unified optimization framework combining gradient-based and derivative-free methods (DF-Grad) to handle continuous/discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms.

Result: Method outperforms baselines that optimize static and dynamic parameters separately, especially under challenging conditions like low light and fast motion.

Conclusion: Joint optimization of hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.

Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.

</details>


### [165] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: DyToK is a training-free method that uses VLLMs' attention mechanisms to dynamically compress video tokens, achieving 4.3x faster inference while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: VLLMs face efficiency bottlenecks due to quadratic computational growth with long video token sequences. Existing keyframe sampling methods introduce additional computational costs before feature encoding and use suboptimal binary frame selection.

Method: DyToK (Dynamic Token compression via LLM-guided Keyframe prior) leverages VLLMs' inherent attention mechanisms to dynamically adjust per-frame token retention ratios. It uses query-conditioned keyframe priors encoded in attention layers to prioritize semantically rich frames while suppressing redundancies.

Result: DyToK achieves state-of-the-art efficiency-accuracy tradeoffs, showing 4.3x faster inference while preserving accuracy across multiple VLLMs (LLaVA-OneVision, Qwen2.5-VL). It demonstrates plug-and-play compatibility with existing compression methods like VisionZip and FastV.

Conclusion: DyToK provides an effective training-free paradigm for dynamic token compression in VLLMs, addressing efficiency bottlenecks without sacrificing accuracy, and is compatible with existing compression techniques.

Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [166] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

TL;DR: A plug-and-play method called Stitch and Tell (SiTe) addresses spatial hallucinations in vision-language models by creating stitched image-text pairs with structured spatial supervision, improving spatial understanding without harming general capabilities.


<details>
  <summary>Details</summary>
Motivation: Vision-language models suffer from spatial hallucinations (incorrect descriptions of object positions) due to asymmetric properties between images and text. There's a need to enrich spatial understanding without costly annotations or advanced models.

Method: SiTe constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or QA pairs based on the layout. It's annotation-free, plug-and-play, and doesn't rely on costly models or human involvement.

Result: SiTe improves spatial understanding tasks: MME_Position (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Evaluated across three architectures and eight benchmarks.

Conclusion: Explicitly injecting spatially-aware structure into training data effectively mitigates spatial hallucinations and improves spatial understanding while preserving general vision-language capabilities.

Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [167] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: First multimodal benchmark (NeuroABench) for evaluating anatomical understanding in neurosurgical videos, revealing MLLMs lag significantly behind human performance with only 40.87% accuracy vs. human average of 46.5%.


<details>
  <summary>Details</summary>
Motivation: Existing surgical video research focuses on procedures/workflows but neglects anatomical comprehension, which is critical for surgeons to interpret, review, and learn from surgical videos. There's a gap in evaluating anatomical understanding in the neurosurgical domain.

Method: Created NeuroABench with 9 hours of annotated neurosurgical videos covering 89 procedures using novel multimodal annotation pipeline with multiple review cycles. Evaluates identification of 68 clinical anatomical structures. Tested over 10 state-of-the-art MLLMs and compared with neurosurgical trainees.

Result: Best MLLM achieved only 40.87% accuracy in anatomical identification, while neurosurgical trainees averaged 46.5% (range: 28%-56%). MLLMs perform comparably to lowest-scoring student but lag significantly behind human average performance.

Conclusion: NeuroABench fills critical gap in evaluating anatomical understanding in surgical AI. While MLLMs show progress, substantial gap remains to achieve human-level anatomical comprehension in neurosurgical contexts, highlighting need for further research.

Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [168] [RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06774)
*Longjie Zhao,Ziming Hong,Zhenyang Ren,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: RDSplat introduces a robust watermarking method for 3D Gaussian Splatting that resists diffusion-based editing attacks by embedding watermarks in low-frequency Gaussians and using adversarial training.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS watermarking methods are vulnerable to diffusion-based editing attacks that can erase embedded watermarks, creating an urgent need for robust watermarking techniques that can withstand such editing while maintaining copyright protection.

Method: RDSplat uses a multi-domain framework that embeds watermarks into low-frequency Gaussians (which diffusion editing preserves) via coordinated covariance regularization and 2D filtering. It employs adversarial training using Gaussian blur as a surrogate for diffusion editing to enhance robustness.

Result: Comprehensive evaluations on three benchmark datasets show RDSplat maintains superior robustness against diffusion-based editing while preserving watermark invisibility, achieving state-of-the-art performance.

Conclusion: RDSplat provides an effective solution for robust watermarking of 3D Gaussian Splatting assets that can withstand diffusion-based editing attacks, addressing a critical need for copyright protection in digital asset creation.

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.

</details>


### [169] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

TL;DR: Real-time post-processing algorithm that enhances BlazePose 3D pose estimation by incorporating anatomical constraints and biomechanical models, reducing 3D joint position errors by 10.2% and angle errors by 16.6%.


<details>
  <summary>Details</summary>
Motivation: Current real-time pose estimation models like BlazePose lack anatomical constraints, limiting their accuracy for applications like physical therapy and sports coaching where biomechanical correctness is crucial.

Method: Weighted optimization algorithm that fuses BlazePose 3D and 2D estimations, penalizing deviations from expected bone lengths and biomechanical models. Uses Kalman filter with adaptive measurement trust to refine bone length estimations to individual anatomy.

Result: 10.2% reduction in 3D MPJPE (Mean Per Joint Position Error) and 16.6% decrease in errors of angles between body segments compared to BlazePose 3D estimation, evaluated on Physio2.2M dataset.

Conclusion: The method provides robust, anatomically consistent pose estimation suitable for automated physiotherapy, healthcare, and sports coaching on consumer devices, with backend processing using anonymized data only.

Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [170] [Generalized Geometry Encoding Volume for Real-time Stereo Matching](https://arxiv.org/abs/2512.06793)
*Jiaxin Liu,Gangwei Xu,Xianqi Wang,Chengliang Zhang,Xin Yang*

Main category: cs.CV

TL;DR: GGEV is a real-time stereo matching network that achieves strong generalization by extracting depth-aware features and using a Depth-aware Dynamic Cost Aggregation module, outperforming existing real-time methods in zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Real-time stereo matching methods focus on in-domain performance but lack generalization for real-world applications, while stereo foundation models have good generalization but suffer from high inference latency. There's a need for real-time methods with strong generalization capability.

Method: Proposes Generalized Geometry Encoding Volume (GGEV) with two key components: 1) Extraction of depth-aware features encoding domain-invariant structural priors, and 2) Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis to enhance fragile matching relationships in unseen scenes.

Result: GGEV surpasses all existing real-time methods in zero-shot generalization capability and achieves state-of-the-art performance on KITTI 2012, KITTI 2015, and ETH3D benchmarks.

Conclusion: GGEV successfully addresses the trade-off between real-time performance and generalization in stereo matching, providing a lightweight yet effective solution for real-world applications requiring both speed and strong generalization to unseen scenes.

Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.

</details>


### [171] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

TL;DR: VDOT is an efficient unified video creation model that uses distribution matching distillation with optimal transport to achieve high-quality video generation in just 4 steps, outperforming models requiring 100 steps.


<details>
  <summary>Details</summary>
Motivation: Existing video creation models are either limited to specific conditions or too slow for practical use due to complex inference processes. There's a need for an efficient, unified model that can handle multiple video creation tasks with fast generation times.

Method: VDOT uses distribution matching distillation (DMD) with computational optimal transport (OT) instead of KL minimization to optimize discrepancy between real and fake score distributions. It includes a discriminator for quality enhancement and employs an automated pipeline for video data annotation and filtering across multiple tasks.

Result: The 4-step VDOT model outperforms or matches other baselines that require 100 denoising steps, demonstrating superior efficiency and quality in video generation.

Conclusion: VDOT provides an efficient, unified solution for video creation that addresses the limitations of existing models, achieving state-of-the-art performance with significantly reduced computational requirements through optimal transport-based distillation.

Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [172] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

TL;DR: TRR is a three-stage training framework that enhances LVLM safety through policy-guided self-reflection, improving safe response rates from 42.8% to 87.7% on Qwen2.5-VL-7B while maintaining general performance.


<details>
  <summary>Details</summary>
Motivation: Single-pass think-then-answer safety reasoning in LVLMs remains vulnerable to jailbreak attacks and may overlook harmful content in its own output. The key insight is to exploit wasted signals through reflection for genuine self-correction.

Method: Three-stage framework: 1) Build ReSafe dataset with 5,000 think-reflect-revise examples, 2) Fine-tune target model on ReSafe to initialize reflective behavior, 3) Reinforce policy-guided reflection through reinforcement learning.

Result: TRR substantially improves safety performance across safety-awareness benchmarks and jailbreak attacks, increasing safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B while preserving stable performance on general benchmarks like MMMU and MMStar.

Conclusion: The Think-Reflect-Revise framework effectively enhances LVLM safety alignment through self-reflection, addressing vulnerabilities in single-pass reasoning while maintaining general capabilities.

Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [173] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

TL;DR: RMAdapter: A dual-branch adapter for VLMs that balances task-specific adaptation with general knowledge preservation through reconstruction, outperforming SOTA in few-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Current adapter-based approaches for Vision-Language Models in few-shot scenarios are underexplored and show performance gaps. There's a need to balance task-specific adaptation with generalization while keeping models lightweight.

Method: Dual-branch architecture with: 1) adaptation branch for task-specific knowledge injection via parameter-efficient fine-tuning, and 2) reconstruction branch that preserves general knowledge by reconstructing latent features back to original space. Uses local reconstruction loss, shared projection modules, and consistency constraints.

Result: RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics on three tasks: generalization to new categories, generalization to new target datasets, and domain generalization, without data augmentation or duplicate prompt designs.

Conclusion: RMAdapter effectively balances discriminability and generalization in few-shot VLM fine-tuning through its dual-branch design, achieving superior performance while maintaining computational efficiency.

Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [174] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: MeshSplatting bridges neural rendering and interactive 3D graphics by creating mesh-based representations from differentiable rendering that work with AR/VR/game engines, outperforming state-of-the-art mesh methods while being faster and more memory efficient.


<details>
  <summary>Details</summary>
Motivation: Current primitive-based splatting methods like 3D Gaussian Splatting provide real-time novel view synthesis but use point-based representations that are incompatible with mesh-based pipelines used in AR/VR and game engines, creating a gap between neural rendering and interactive 3D graphics.

Method: MeshSplatting jointly optimizes geometry and appearance through differentiable rendering, enforces connectivity via restricted Delaunay triangulation, and refines surface consistency to create smooth, high-quality meshes that render efficiently in real-time 3D engines.

Result: On Mip-NeRF360 dataset, MeshSplatting achieves +0.69 dB PSNR improvement over state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory.

Conclusion: MeshSplatting successfully bridges neural rendering and interactive 3D graphics by creating mesh-based representations that are compatible with AR/VR and game engine pipelines while maintaining high visual quality and real-time rendering efficiency.

Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [175] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

TL;DR: A framework for generating storytelling images with rich logical connections using LLMs and T2I models, with specialized evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Storytelling images with complex semantic connections are valuable for various applications but challenging to create manually, creating a need for AI-assisted generation.

Method: Two-stage pipeline (StorytellingPainter) combining LLMs for creative reasoning and T2I models for visual synthesis, plus specialized evaluation framework with three evaluators.

Result: Demonstrated feasibility and effectiveness of the approach, with development of lightweight Mini-Storyteller models to bridge performance gaps between open-source and proprietary LLMs.

Conclusion: The proposed framework successfully addresses the challenge of generating semantically rich storytelling images through AI collaboration, with practical implementation and evaluation tools.

Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [176] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

TL;DR: SparseCoop is a fully sparse cooperative perception framework for 3D detection and tracking that eliminates BEV representations, using kinematic-grounded instance queries for precise alignment, coarse-to-fine aggregation for robust fusion, and cooperative instance denoising for stable training.


<details>
  <summary>Details</summary>
Motivation: Current cooperative perception methods face limitations: BEV-based approaches have quadratically-scaling communication costs and lack flexibility for precise alignment across asynchronous/disparate viewpoints, while sparse query-based methods suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability.

Method: Three key innovations: 1) Kinematic-grounded instance query using explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; 2) Coarse-to-fine aggregation module for robust fusion; 3) Cooperative instance denoising task to accelerate and stabilize training. The framework completely discards intermediate BEV representations.

Result: Achieves state-of-the-art performance on V2X-Seq and Griffin datasets with superior computational efficiency, low transmission cost, and strong robustness to communication latency.

Conclusion: SparseCoop demonstrates that a fully sparse cooperative perception framework can overcome limitations of both BEV-based and existing sparse query-based approaches, offering efficient, robust, and high-performance 3D detection and tracking for autonomous driving.

Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [177] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

TL;DR: CADE is the first approach combining continual learning with weakly-supervised video anomaly detection to handle domain shifts across multiple scenes while preventing forgetting of previous anomaly patterns.


<details>
  <summary>Details</summary>
Motivation: Existing weakly-supervised VAD methods focus on static datasets and neglect domain shifts across different scenes. Without continual learning, training on new data causes forgetting of previous anomaly patterns, leading to performance degradation.

Method: CADE uses Dual-Generator to address data imbalance and label uncertainty in WVAD, and ensembles Multi-Discriminator to capture missed anomalies from past scenes due to forgetting, using multiple models to maintain detection capability across domains.

Result: Extensive experiments show CADE significantly outperforms existing VAD methods on multi-scene datasets like ShanghaiTech and Charlotte Anomaly datasets.

Conclusion: CADE successfully addresses domain-shift in video anomaly detection through continual learning, preventing forgetting and maintaining robust anomaly detection across multiple scenes with different characteristics.

Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [178] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

TL;DR: PA-VAD: Video anomaly detection trained only on synthesized pseudo-abnormal videos and real normal videos, achieving SOTA results without real abnormal footage.


<details>
  <summary>Details</summary>
Motivation: Real abnormal videos are scarce and expensive to collect, hindering practical deployment of video anomaly detection systems.

Method: 1) Synthesize pseudo-abnormal videos using CLIP for image selection, vision-language models for prompt refinement, and video diffusion models for generation. 2) Train detector with domain-aligned regularization to mitigate excessive spatiotemporal magnitude in synthesized anomalies.

Result: Achieves 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming UVAD SOTA on UCF-Crime by +1.9%.

Conclusion: High-accuracy anomaly detection can be achieved without collecting real anomalies, providing a practical path toward scalable deployment.

Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [179] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik M枚ller*

Main category: cs.CV

TL;DR: A weakly supervised method for vertebral metastasis segmentation in CT using only vertebra-level labels (healthy/malignant) without lesion masks, combining diffusion autoencoder editing with hide-and-seek attribution to generate accurate lesion segmentations.


<details>
  <summary>Details</summary>
Motivation: Vertebral metastasis segmentation in CT is clinically important but difficult to scale due to scarce voxel-level annotations and the similarity between malignant lesions (lytic/blastic) and benign degenerative changes.

Method: Combines a Diffusion Autoencoder (DAE) that produces classifier-guided healthy edits of vertebrae with pixel-wise difference maps for candidate lesion regions. Uses Hide-and-Seek Attribution: each candidate region is revealed while others are hidden, the edited image is projected back to the data manifold by DAE, and a latent-space classifier quantifies the isolated malignant contribution to identify true lesions.

Result: Achieved strong performance on held-out radiologist annotations: blastic (F1: 0.91, Dice: 0.87) and lytic (F1: 0.85, Dice: 0.78), significantly exceeding baseline methods (F1: 0.79/0.67; Dice: 0.74/0.55).

Conclusion: Vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT for vertebral metastasis detection.

Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [180] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: Proposes Omni-Referring Image Segmentation (OmniRIS) - a novel task for highly generalized image segmentation that accepts both text instructions and visual reference images (with masks, boxes, or scribbles) as omni-prompts, enabling granular attribute referring and uncommon object grounding.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation tasks are unimodally conditioned (text-only or visual-only), limiting their ability to exploit complementary strengths of both modalities. There's a need for a more flexible, generalized segmentation approach that can handle diverse input modalities and segmentation settings.

Method: 1) Introduces OmniRIS task supporting text instructions + visual reference images (with masks/boxes/scribbles) as omni-prompts. 2) Creates OmniRef dataset with 186,939 omni-prompts for 30,956 images. 3) Proposes OmniSegNet baseline to handle omni-prompt encoding challenges. 4) Establishes comprehensive evaluation system.

Result: Extensive experiments validate OmniSegNet's capability to follow omni-modal instructions and demonstrate OmniRIS's superiority for highly generalized image segmentation. The method can handle various segmentation settings (one vs. many, many vs. many).

Conclusion: OmniRIS represents a significant advancement toward highly generalized image segmentation by unifying text and visual modalities, with practical applications facilitated by its flexible segmentation settings and comprehensive dataset/benchmark.

Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [181] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

TL;DR: Training-free self-correction framework reduces VLM hallucinations through uncertainty-guided visual re-attention without model updates.


<details>
  <summary>Details</summary>
Motivation: Vision-language models often generate plausible but incorrect hallucinated content about images, requiring methods to improve reliability without costly retraining.

Method: Combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operates entirely with frozen pretrained VLMs, requiring no gradient updates.

Result: Reduces hallucination rates by 9.8 percentage points compared to baseline, improves object existence accuracy by 4.7 points on adversarial splits. Validated on POPE and MMHAL BENCH benchmarks using Qwen2.5-VL-7B.

Conclusion: Uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. Framework enables trustworthy multimodal systems without model updates, with plans to extend validation across diverse architectures.

Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [182] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS is an unsupervised Video Instance Segmentation framework that uses quality-guided self-training to bridge the synthetic-to-real domain gap without human annotations.


<details>
  <summary>Details</summary>
Motivation: Video Instance Segmentation requires pixel-level masks and temporal consistency labels, which are challenging to annotate. Existing unsupervised methods like VideoCutLER rely on synthetic data but suffer from the synthetic-to-real domain gap.

Method: AutoQ-VIS establishes a closed-loop system between pseudo-label generation and automatic quality assessment. It uses quality-guided self-training to progressively adapt from synthetic to real videos without optical flow dependencies.

Result: Achieves state-of-the-art performance with 52.6 AP50 on YouTubeVIS-2019 val set, surpassing previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations.

Conclusion: Demonstrates the viability of quality-aware self-training for unsupervised Video Instance Segmentation, effectively bridging the synthetic-to-real domain gap through a closed-loop quality assessment system.

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [183] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper proposes using offline-retrieved geographic images (e.g., from Google Maps) as additional input to enhance autonomous driving systems, addressing limitations of onboard sensors in poor visibility conditions.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems rely on onboard sensors which have limitations: restricted perception horizon, limited view scope, occlusion issues, and poor performance in extreme conditions like darkness and rain. Human drivers can recall road structure even under poor visibility, so the authors want to endow models with similar "recall" ability.

Method: Proposes a spatial retrieval paradigm using offline retrieved geographic images as additional input. These images are obtained from offline caches like Google Maps or stored autonomous driving datasets. The authors extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align them with ego-vehicle trajectories. They establish baselines across five core AD tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling.

Result: Extensive experiments show that the extended modality (geographic images) could enhance the performance of certain autonomous driving tasks. The authors will open-source dataset curation code, data, and benchmarks for further study.

Conclusion: The spatial retrieval paradigm using offline geographic images is a promising plug-and-play extension for existing autonomous driving systems that doesn't require additional sensors and can potentially overcome limitations of onboard sensor-based perception in challenging conditions.

Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [184] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: ECOCSeg introduces error-correcting output codes for semantic segmentation to handle noisy pseudo-labels in UDA and SSL, improving stability and generalization through bit-level error correction.


<details>
  <summary>Details</summary>
Motivation: Pseudo-label learning in segmentation suffers from error amplification due to one-hot encoding, especially in label-scarce scenarios like UDA and SSL. Erroneous pseudo-labels degrade model performance.

Method: ECOCSeg uses error-correcting output codes (ECOC) to create fine-grained class encodings. It introduces an ECOC-based classifier that disentangles classes into attributes and handles partial inaccurate bits, plus a bit-level label denoising mechanism for higher-quality pseudo-labels.

Result: ECOCSeg consistently shows significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures, and can be easily integrated with existing methods.

Conclusion: ECOCSeg provides an effective solution to pseudo-label noise in segmentation by leveraging ECOC encoding, offering better stability and generalization for UDA and SSL tasks.

Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [185] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

TL;DR: Proposes a lightweight convolutional mixer architecture for remote sensing scene classification that balances accuracy and efficiency through multi-scale spatial mixing and channel mixing operations.


<details>
  <summary>Details</summary>
Motivation: Remote sensing scene classification is crucial for Earth observation but remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions that reduce generalization of existing CNN and ViT models.

Method: Lightweight architecture based on convolutional mixer paradigm that alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information.

Result: Achieved 74.7% overall accuracy, 74.57% average accuracy, and 73.79 Kappa on AID dataset; 93.90% overall accuracy, 93.93% average accuracy, and 93.22 Kappa on EuroSAT dataset.

Conclusion: The proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models for remote sensing scene classification.

Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [186] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

TL;DR: Hierarchical image-guided 3D segmentation framework that uses 2D foundation models (SAM + YOLO-World) to progressively refine segmentation from instance to part level, addressing occlusion and scale issues in industrial scenes.


<details>
  <summary>Details</summary>
Motivation: Industrial environments have dense layouts with heavy occlusion that weakens geometric boundaries and large scale differences that cause end-to-end models to fail at capturing both coarse and fine details. Existing methods either require costly 3D annotations or suffer from semantic inconsistencies across views.

Method: Two-stage hierarchical approach: 1) Instance segmentation via top-view image rendering with SAM masks prompted by YOLO-World, back-projected to 3D point cloud; 2) Part-level segmentation by rendering multi-view images of each instance, applying same 2D segmentation and back-projection at each view, followed by Bayesian updating fusion for cross-view consistency.

Result: Experiments on real-world factory data show effective handling of occlusion and structural complexity with consistently high per-class mIoU scores. Additional evaluations on public dataset confirm generalization ability, robustness, annotation efficiency, and adaptability to diverse 3D environments.

Conclusion: The proposed hierarchical image-guided framework successfully addresses challenges in industrial 3D segmentation by leveraging 2D foundation models in a progressive refinement approach, achieving reliable segmentation without costly 3D annotations while maintaining semantic consistency across views.

Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [187] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

TL;DR: JoPano is a unified DiT-based approach for panorama generation that addresses both text-to-panorama and view-to-panorama tasks using a Joint-Face Adapter and condition switching mechanism, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing panorama generation methods face two major challenges: U-Net-based architectures limit visual quality, and treating text-to-panorama and view-to-panorama tasks independently leads to modeling redundancy and inefficiency.

Method: Proposes a joint-face panorama (JoPano) generation approach that unifies both tasks within a DiT-based model. Uses a Joint-Face Adapter on cubemap representation to transfer pretrained DiT capabilities to panorama domain. Implements Poisson Blending to reduce seam inconsistencies and introduces Seam-SSIM and Seam-Sobel metrics for evaluation. Includes a condition switching mechanism to unify both tasks in a single model.

Result: Comprehensive experiments show JoPano generates high-quality panoramas for both tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

Conclusion: JoPano successfully addresses the limitations of existing panorama generation methods by unifying text-to-panorama and view-to-panorama tasks within a DiT-based framework, improving both visual quality and modeling efficiency while achieving superior quantitative results.

Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [188] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: BLDA is a novel unsupervised domain adaptation method for semantic segmentation that addresses class imbalance by analyzing predicted logits distributions and using anchor distributions to balance learning across classes.


<details>
  <summary>Details</summary>
Motivation: Self-training techniques in UDA struggle with class imbalance due to distribution shifts between source and target domains, leading to biased learning where some classes are over-predicted while others are under-predicted.

Method: 1) Identify over/under-predicted classes by analyzing predicted logits distributions; 2) Use post-hoc approach with shared anchor distributions to align logits across classes; 3) Incorporate online logits correction into loss function during self-training; 4) Leverage cumulative density as domain-shared structural knowledge.

Result: Extensive experiments on standard UDA semantic segmentation benchmarks show BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods.

Conclusion: BLDA effectively addresses class imbalance in UDA semantic segmentation without requiring prior knowledge about distribution shifts, providing a practical solution that can be integrated into existing methods to improve balanced learning across classes.

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


### [189] [Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation](https://arxiv.org/abs/2512.06888)
*Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan*

Main category: cs.CV

TL;DR: Researchers created AIR-400, the largest public infant respiration video dataset, and developed the first reproducible infant respiration estimation pipelines using infant-specific ROI detection and spatiotemporal neural networks with optical flow.


<details>
  <summary>Details</summary>
Motivation: Contactless respiration monitoring for infants could enable early detection of breathing irregularities linked to neurodevelopmental impairments and SIDS, but there's a lack of public video datasets and reproducible algorithms specifically for infants.

Method: Developed infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs to create reproducible pipelines for infant respiration estimation.

Result: Created AIR-400 dataset with 400 videos (275 new annotated videos from 10 subjects), established first reproducible benchmarks for vision-based infant respiration estimation, and made all resources publicly available.

Conclusion: This work addresses critical gaps in infant respiration monitoring by providing the largest public dataset, reproducible algorithms, and benchmarks to advance research in early detection of breathing irregularities in infants.

Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.

</details>


### [190] [Scaling Zero-Shot Reference-to-Video Generation](https://arxiv.org/abs/2512.06905)
*Zijian Zhou,Shikun Liu,Haozhe Liu,Haonan Qiu,Zhaochong An,Weiming Ren,Zhiheng Liu,Xiaoke Huang,Kam Woh Ng,Tian Xie,Xiao Han,Yuren Cong,Hang Li,Chuyan Zhu,Aditya Patel,Tao Xiang,Sen He*

Main category: cs.CV

TL;DR: Saber is a scalable zero-shot framework for reference-to-video generation that eliminates the need for expensive explicit reference image-video-text triplets by training only on video-text pairs.


<details>
  <summary>Details</summary>
Motivation: Current R2V methods rely on expensive explicit reference image-video-text triplets that are difficult and costly to scale, creating a bottleneck for practical applications.

Method: Uses masked training strategy and tailored attention-based model design to learn identity-consistent and reference-aware representations; integrates mask augmentation techniques to mitigate copy-paste artifacts.

Result: Achieves superior performance on OpenS2V-Eval benchmark compared to methods trained with R2V data, demonstrates remarkable generalization across varying numbers of references, and eliminates need for explicit R2V data.

Conclusion: Saber provides a scalable zero-shot solution for reference-to-video generation that bypasses data bottleneck while maintaining high performance and generalization capabilities.

Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.

</details>


### [191] [Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology](https://arxiv.org/abs/2512.06949)
*Shravan Venkatraman,Muthu Subash Kavitha,Joe Dhanith P R,V Manikandarajan,Jia Wu*

Main category: cs.CV

TL;DR: NTRM introduces a tissue-level graph neural network to model spatial and functional relationships between tissues in histopathology segmentation, outperforming CNN-based methods by 4.9-31.25% in Dice score.


<details>
  <summary>Details</summary>
Motivation: Current CNN-based histopathology segmentation methods focus on visual texture but fail to capture biological context and inter-tissue relationships, especially in regions with overlapping or morphologically similar tissues, limiting structural coherence in boundary-dense zones.

Method: NTRM augments CNNs with a tissue-level graph neural network that constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection to explicitly encode inter-tissue dependencies.

Result: On the Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient 4.9% to 31.25% higher than the best-performing models among evaluated approaches.

Conclusion: Relational modeling provides a principled approach for more context-aware and interpretable histological segmentation compared to local receptive-field architectures that lack tissue-level structural awareness, enabling structurally coherent predictions in challenging boundary-dense zones.

Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.

</details>


### [192] [Selective Masking based Self-Supervised Learning for Image Semantic Segmentation](https://arxiv.org/abs/2512.06981)
*Yuemin Wang,Ian Stavness*

Main category: cs.CV

TL;DR: Selective masking image reconstruction for self-supervised semantic segmentation pretraining outperforms random masking and supervised ImageNet pretraining by 2.9% on general datasets and 2.5% on weed segmentation datasets.


<details>
  <summary>Details</summary>
Motivation: To improve self-supervised learning for semantic segmentation by replacing random masking with selective masking that leverages trained model knowledge, especially useful for scenarios with limited model capacity and computational resources.

Method: Proposes selective masking image reconstruction that iteratively masks patches with highest reconstruction loss, using the trained model's knowledge to guide masking rather than random masking.

Result: Outperforms random masking and supervised ImageNet pretraining by 2.9% on Pascal VOC/Cityscapes and 2.5% on weed segmentation datasets; significantly improves accuracy for lowest-performing classes; best results when using same dataset for pretraining and downstream tasks.

Conclusion: Selective Masking Image Reconstruction provides effective solution for improving end-to-end semantic segmentation workflows, particularly beneficial for resource-constrained scenarios requiring limited model capacity.

Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.

</details>


### [193] [Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues](https://arxiv.org/abs/2512.07034)
*Tuan-Anh Vu,Hai Nguyen-Truong,Ziqiang Zheng,Binh-Son Hua,Qing Guo,Ivor Tsang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: TransCues is a transformer-based framework that improves glass object segmentation by jointly enhancing boundary and reflection features, achieving state-of-the-art performance across multiple benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation methods struggle with transparent glass objects due to their transparency and reflective properties. While human perception uses both boundary and reflective-object features, current methods don't sufficiently capture both properties for transparent object segmentation.

Method: Proposes TransCues, a pyramidal transformer encoder-decoder architecture with two key modules: Boundary Feature Enhancement and Reflection Feature Enhancement. These modules work together in a mutually beneficial way to capture both boundary and reflection cues for better glass object segmentation.

Result: Significant performance improvements across multiple datasets: +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D. Outperforms state-of-the-art by large margins.

Conclusion: Jointly enhancing boundary and reflection features in a transformer architecture effectively addresses the challenges of glass object segmentation, demonstrating superior performance across diverse benchmark datasets for transparent and reflective object segmentation.

Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.

</details>


### [194] [Evaluating and Preserving High-level Fidelity in Super-Resolution](https://arxiv.org/abs/2512.07037)
*Josep M. Rocafort,Shaolin Su,Javier Vazquez-Corral,Alexandra Gomez-Villa*

Main category: cs.CV

TL;DR: Paper proposes measuring high-level semantic fidelity in image super-resolution to complement traditional quality metrics, creates annotated dataset, analyzes SOTA models, and shows foundation models can better address this task.


<details>
  <summary>Details</summary>
Motivation: Current SR models can hallucinate content changes while achieving high visual quality, but existing low-level metrics don't capture these high-level semantic changes that humans easily identify.

Method: Construct first annotated dataset with fidelity scores from different SR models, evaluate SOTA models' fidelity preservation, analyze correlation with existing metrics, leverage foundation models for better fidelity measurement, and fine-tune SR models using fidelity feedback.

Result: Shows SOTA SR models have fidelity issues despite high visual quality, foundation models better address high-level fidelity measurement, and fine-tuning with fidelity feedback improves both semantic fidelity and perceptual quality.

Conclusion: High-level fidelity measurement is crucial for evaluating SR model reliability, foundation models offer better solutions, and incorporating fidelity feedback can optimize both semantic preservation and perceptual quality in SR models.

Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.

</details>


### [195] [DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.07051)
*Adnan Munir,Shujaat Khan*

Main category: cs.CV

TL;DR: DAUNet: Lightweight UNet variant with Deformable V2 Convolutions and SimAM attention for medical image segmentation, achieving state-of-the-art performance with high parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation is crucial for automated diagnostic systems, but existing models often lack spatial adaptability and context-aware feature fusion without increasing model complexity. There's a need for lightweight yet effective segmentation models suitable for real-time clinical deployment.

Method: DAUNet integrates Deformable V2 Convolutions in the bottleneck for handling geometric variations and Parameter-Free Attention (SimAM) modules in decoder and skip pathways for saliency-aware refinement. This combination improves spatial adaptability and context-aware feature fusion while maintaining low parameter count.

Result: Outperforms state-of-the-art models on FH-PS-AoP (fetal head ultrasound) and FUMPE (CT pulmonary embolism) datasets in Dice score, HD95, and ASD metrics. Maintains superior parameter efficiency. Ablation studies confirm contributions of both deformable convolutions and SimAM attention.

Conclusion: DAUNet is a robust, lightweight segmentation model suitable for real-time clinical deployment, particularly effective in handling missing context and low-contrast regions while maintaining computational efficiency.

Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.

</details>


### [196] [RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting](https://arxiv.org/abs/2512.07052)
*Hoang-Nhat Tran,Francesco Di Sario,Gabriele Spadaro,Giuseppe Valenzise,Enzo Tartaglione*

Main category: cs.CV

TL;DR: Proposes a flexible compression scheme for 3D Gaussian Splatting that supports interpolation at any rate between bounds, enabling dynamic rate control without retraining.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting enables real-time photorealistic rendering but suffers from large memory requirements and costly training. Existing compression approaches operate at fixed rates, limiting adaptability to varying bandwidth and device constraints.

Method: A flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. The method is computationally lightweight and requires no retraining for any rate.

Result: Achieves efficient, high-quality compression while offering dynamic rate control. Preserves rendering quality across a broad range of operating points.

Conclusion: The approach makes 3DGS compression suitable for practical deployment in immersive applications by providing flexible rate control without quality degradation.

Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.

</details>


### [197] [$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction](https://arxiv.org/abs/2512.07062)
*Changliang Xia,Chengyou Jia,Minnan Luo,Zhuohang Dang,Xin Shen,Bowen Ping*

Main category: cs.CV

TL;DR: D鲁-Predictor is a deterministic framework that removes stochastic noise from diffusion models for dense prediction tasks, achieving SOTA performance with less training data and single-step inference.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models with strong visual priors have a core limitation: the stochastic noise in diffusion sampling is inherently misaligned with dense prediction tasks that require deterministic image-to-geometry mapping. This noise corrupts fine-grained spatial cues and pushes models toward timestep-specific noise objectives, destroying meaningful geometric structure mappings.

Method: D鲁-Predictor reformulates pretrained diffusion models without stochastic noise by viewing them as ensembles of timestep-dependent visual experts. It self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior, then uses task-specific supervision to adapt this noise-free prior to dense prediction tasks.

Result: Extensive experiments on various dense prediction tasks show D鲁-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. It requires less than half the training data previously used and efficiently performs inference in a single step.

Conclusion: The proposed deterministic framework successfully addresses the misalignment between stochastic diffusion sampling and deterministic dense prediction, enabling more efficient and effective use of diffusion priors for geometric understanding tasks.

Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.

</details>


### [198] [Persistent Homology-Guided Frequency Filtering for Image Compression](https://arxiv.org/abs/2512.07065)
*Anil Chintapalli,Peter Tenholder,Henry Chen,Arjun Rao*

Main category: cs.CV

TL;DR: Using discrete Fourier transform with persistent homology to extract frequency features for image compression, achieving JPEG-comparable compression with potential for improved binary classification in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Feature extraction in noisy image datasets presents challenges for model reliability, requiring methods that can differentiate meaningful data from noise while maintaining compression efficiency.

Method: Combines discrete Fourier transform with persistent homology analysis to extract specific frequencies corresponding to topological image features, enabling compression while preserving meaningful data.

Result: Achieves compression comparable to JPEG across six different metrics, with potential to improve binary classification performance when augmenting Convolutional Neural Networks compared to traditional methods.

Conclusion: Persistent homology-guided frequency filtration enhances image compression reliability under noisy conditions and shows promise for improving classification tasks.

Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.

</details>


### [199] [Context-measure: Contextualizing Metric for Camouflage](https://arxiv.org/abs/2512.07076)
*Chen-Yang Wang,Gepeng Ji,Song Shao,Ming-Ming Cheng,Deng-Ping Fan*

Main category: cs.CV

TL;DR: Proposes Context-measure, a new evaluation metric for camouflaged object segmentation that incorporates spatial context dependencies, addressing limitations of existing context-independent metrics.


<details>
  <summary>Details</summary>
Motivation: Current camouflage evaluation metrics overlook the critical context-dependent nature of camouflage, as they were originally designed for general/salient objects with assumptions of uncorrelated spatial context.

Method: Develops Context-measure based on a probabilistic pixel-aware correlation framework that incorporates spatial dependencies and pixel-wise camouflage quantification to better align with human perception.

Result: Extensive experiments across three challenging camouflaged object segmentation datasets show Context-measure delivers more reliability than existing context-independent metrics.

Conclusion: Context-measure provides a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns in agricultural, industrial, and medical scenarios.

Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.

</details>


### [200] [DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection](https://arxiv.org/abs/2512.07078)
*Bo Gao,Jingcheng Tong,Xingsheng Chen,Han Yu,Zichen Li*

Main category: cs.CV

TL;DR: DFIR-DETR: A transformer-based detector using dynamic feature aggregation and frequency-domain processing for small object detection in UAV images and industrial inspection, achieving SOTA results with lightweight design.


<details>
  <summary>Details</summary>
Motivation: Current transformer detectors struggle with small object detection due to feature degradation from downsampling, inability of spatial convolutions to capture long-range dependencies, and inefficient upsampling that inflates feature maps. Applications like UAV remote sensing and industrial defect detection face sparse/weak features, cluttered backgrounds, and dramatic scale variations.

Method: Three novel components: 1) DCFA module with dynamic K-sparse attention (O(NK) complexity) and spatial gated linear units; 2) DFPN module with amplitude-normalized upsampling to prevent feature inflation and dual-path shuffle convolution; 3) FIRC3 module operating in frequency domain for global receptive fields efficiently.

Result: Achieved mAP50 scores of 92.9% on NEU-DET and 51.6% on VisDrone datasets (both state-of-the-art). Model is lightweight with 11.7M parameters and 41.2 GFLOPs.

Conclusion: DFIR-DETR effectively addresses small object detection challenges, generalizes well across different domains (UAV and industrial inspection), and works efficiently in resource-limited settings for cross-scene applications.

Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.

</details>


### [201] [COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision](https://arxiv.org/abs/2512.07107)
*Jaeyoon Lee,Hojoon Jung,Sungtae Hwang,Jihyong Oh,Jongwon Choi*

Main category: cs.CV

TL;DR: COREA is a unified framework that jointly learns relightable 3D Gaussians and SDF for accurate geometry reconstruction and faithful relighting, addressing limitations of existing 3DGS methods.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting methods learn geometry from 2D renderings, resulting in coarse surfaces and unreliable BRDF-lighting decomposition. There's a need for more accurate geometry reconstruction and stable physically-based rendering.

Method: Introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy using depth for coarse alignment, depth gradients and normals for fine-scale refinement. Includes density-control mechanism for stable Gaussian growth and memory efficiency.

Result: Achieves superior performance on standard benchmarks for novel-view synthesis, mesh reconstruction, and physically-based rendering within a unified framework.

Conclusion: COREA successfully addresses limitations of existing 3DGS methods by enabling direct 3D geometric learning, resulting in more accurate geometry and stable BRDF-lighting decomposition for relightable 3D reconstruction.

Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.

</details>


### [202] [MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection](https://arxiv.org/abs/2512.07110)
*Liangwei Jiang,Jinluo Xie,Yecheng Huang,Hua Zhang,Hongyu Yang,Di Huang*

Main category: cs.CV

TL;DR: MSN is a two-stream CNN model for copy-move forgery detection that improves representation through multi-directional encoding and localization via 2-D similarity matrices, achieving state-of-the-art results on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Copy-move forgery detection is increasingly challenging due to complex transformations and fine-tuned operations, especially with deep generative networks. Existing methods have limitations in representation and localization capabilities.

Method: Proposes Multi-directional Similarity Network (MSN) with: 1) Hierarchical multi-directional CNN encoding for better patch similarity measurement across scales/rotations, 2) 2-D similarity matrix decoder that utilizes full spatial information instead of 1-D vectors.

Result: Achieves state-of-the-art performance on CASIA CMFD, CoMoFoD benchmarks and a new deep-synthesized forgery database. Demonstrates effectiveness in detecting both traditional and deep network-generated copy-move forgeries.

Conclusion: MSN effectively addresses representation and localization limitations in copy-move detection through its two-stream architecture with multi-directional encoding and 2-D similarity decoding, outperforming existing methods across diverse forgery types.

Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.

</details>


### [203] [Training-free Clothing Region of Interest Self-correction for Virtual Try-On](https://arxiv.org/abs/2512.07126)
*Shengjie Lu,Zhibin Wan,Jiejie Liu,Quan Zhang,Mingjie Sun*

Main category: cs.CV

TL;DR: CSC-VTON improves virtual try-on by using an energy function to constrain attention maps during generation, focusing on clothing details, and introduces a new evaluation metric VTID for better alignment assessment.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods have discrepancies between generated clothing results and target clothing in patterns, textures, and boundaries. Current evaluation metrics only focus on image realism and ignore alignment with target elements.

Method: Proposes using an energy function to impose constraints on attention maps extracted during the generation process, making attention focus more on clothing regions of interest. Also introduces Virtual Try-on Inception Distance (VTID) as a new evaluation metric.

Result: Outperforms previous SOTA methods by 1.4% (LPIPS), 2.3% (FID), 12.3% (KID), and 5.8% (VTID) on VITON-HD and DressCode datasets. Generated data improves downstream CC-Reid methods by 2.5%, 1.1%, and 1.6% on LTCC, PRCC, VC-Clothes datasets in Rank-1 metrics.

Conclusion: The energy function approach effectively improves clothing detail preservation in VTON, and the new VTID metric provides more comprehensive evaluation. The method demonstrates practical value for downstream applications like clothing-change re-identification.

Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

</details>


### [204] [MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP](https://arxiv.org/abs/2512.07128)
*Chau Truong,Hieu Ta Quang,Dung D. Le*

Main category: cs.CV

TL;DR: MulCLIP improves CLIP's ability to handle long, detailed text descriptions through multi-level alignment without needing region proposals.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with lengthy, detailed descriptions because it's trained on short captions. Existing region-proposal solutions are effective but add deployment costs.

Method: End-to-end multi-level alignment framework: 1) Global contrastive alignment with extended positional embeddings, 2) Token reconstruction alignment for word-patch connections, 3) Subcaption-aggregated patch alignment for context-rich patch extraction.

Result: Consistent performance improvements across diverse benchmarks, with multi-scale alignment driving better fine-grained capability than region-proposal approaches.

Conclusion: MulCLIP's multi-level alignment effectively bridges natural long-text structures with image components, making it suitable for diverse real-world applications without the deployment costs of region-proposal methods.

Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.

</details>


### [205] [TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning](https://arxiv.org/abs/2512.07135)
*Zebin Xing,Pengxuan Yang,Linbo Wang,Yichen Zhang,Yiming Hu,Yupeng Zheng,Junli Wang,Yinfeng Gao,Guang Li,Kun Ma,Long Chen,Zhongpu Xia,Qichao Zhang,Hangjun Ye,Dongbin Zhao*

Main category: cs.CV

TL;DR: The paper proposes a two-stage autonomous driving planning system that uses Mixture of Experts for scenario-specific trajectory priors and Reinforcement Learning for policy-driven trajectory scoring refinement.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end autonomous driving systems with trajectory priors have limitations: 1) they use fixed trajectory priors that don't adapt to different driving scenarios, and 2) their trajectory evaluation lacks policy-driven refinement due to one-stage supervised training constraints.

Method: 1) Use Mixture of Experts (MoE) to apply different trajectory priors tailored to specific driving scenarios. 2) Employ Reinforcement Learning to fine-tune the trajectory scoring mechanism for policy-driven refinement. 3) Integrate models with different perception backbones to enhance perceptual features.

Result: The integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place in the competition.

Conclusion: The proposed approach successfully addresses the limitations of previous trajectory prior methods by introducing scenario-adaptive priors via MoE and policy-driven refinement via RL, resulting in competitive performance on benchmark evaluation.

Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.

</details>


### [206] [A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning](https://arxiv.org/abs/2512.07136)
*Siyang Jiang,Mu Yuan,Xiang Ji,Bufang Yang,Zeyu Liu,Lilin Xu,Yang Li,Yuting He,Liran Dong,Wenrui Lu,Zhenyu Yan,Xiaofan Jiang,Wei Gao,Hongkai Chen,Guoliang Xing*

Main category: cs.CV

TL;DR: CUHK-X is a large-scale multimodal dataset for human action recognition, understanding, and reasoning, addressing limitations of existing datasets by providing both data labels and logically consistent captions across diverse sensor modalities.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal HAR datasets lack large-scale data-caption resources for non-RGB modalities (depth, IMU, mmWave), and provide only coarse data-label annotations insufficient for fine-grained action understanding and reasoning tasks enabled by recent LLM advances.

Method: Created CUHK-X dataset with 58,445 samples covering 40 actions by 30 participants across two indoor environments. Used prompt-based scene creation method leveraging LLMs to generate logically connected activity sequences with human validation, ensuring caption consistency.

Result: Achieved average accuracies of 76.52% for HAR, 40.76% for HAU, and 70.25% for HARn. Provides three benchmarks with six evaluation tasks for comprehensive multimodal human activity analysis.

Conclusion: CUHK-X enables the community to apply and develop data-intensive learning methods for robust multimodal human activity analysis, bridging the gap between traditional action recognition and advanced understanding/reasoning capabilities.

Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

</details>


### [207] [CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics](https://arxiv.org/abs/2512.07155)
*Dahyeon Kye,Jeahun Sung,MinKyu Jeon,Jihyong Oh*

Main category: cs.CV

TL;DR: CHIMERA is a zero-shot diffusion-based framework for smooth image morphing using cached inversion-guided denoising with adaptive feature injection and semantic prompting.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based image morphing methods often produce abrupt transitions or over-saturated appearances due to lack of adaptive structural and semantic alignments between dissimilar images.

Method: CHIMERA formulates morphing as cached inversion-guided denoising with two key components: 1) Adaptive Cache Injection (ACI) that caches and adaptively re-injects features from both input images during denoising, and 2) Semantic Anchor Prompting (SAP) that uses a vision-language model to generate a shared semantic anchor prompt. Also introduces Global-Local Consistency Score (GLCS) for evaluation.

Result: Extensive experiments and user studies show CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing new state-of-the-art in image morphing.

Conclusion: CHIMERA successfully addresses the challenge of smooth image morphing with large semantic disparities through adaptive feature injection and semantic anchoring, with code and project page to be publicly released.

Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

</details>


### [208] [MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation](https://arxiv.org/abs/2512.07165)
*Muyu Xu,Fangneng Zhan,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: MuSASplat is a lightweight framework for sparse-view 3D Gaussian splatting that reduces computational costs while maintaining rendering quality through efficient fine-tuning and feature fusion.


<details>
  <summary>Details</summary>
Motivation: Existing pose-free feed-forward methods for sparse-view 3D Gaussian splatting require full fine-tuning of large Vision Transformer backbones, which incurs substantial GPU costs and computational burden.

Method: Introduces two key components: 1) Multi-Scale Adapter for efficient fine-tuning of ViT-based architectures with minimal parameters, and 2) Feature Fusion Aggregator that integrates features across input views effectively while reducing memory usage and computational costs compared to memory banks.

Result: Achieves state-of-the-art rendering quality with significantly reduced parameters and training resource requirements across diverse datasets, maintaining high fidelity in novel view synthesis even with very sparse input views.

Conclusion: MuSASplat dramatically reduces computational burden for training pose-free feed-forward 3D Gaussian splatting models while preserving rendering quality, making it a more efficient alternative to existing methods.

Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.

</details>


### [209] [When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing](https://arxiv.org/abs/2512.07166)
*Siyuan Xu,Yibing Liu,Peilin Chen,Yung-Hui Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: This paper addresses privacy leakage in MLLMs by focusing on restoring surrogate-driven protected data, introducing the SPPE dataset and a unified approach for reliable privacy reconstruction while preserving MLLM editing quality.


<details>
  <summary>Details</summary>
Motivation: Existing privacy protection methods for MLLMs effectively obscure private information but overlook evaluating the authenticity and recovery quality of user privacy. The paper aims to address the critical challenge of restoring surrogate-driven protected data in diverse MLLM scenarios.

Method: 1) Creates the SPPE dataset with various privacy categories and user instructions simulating real MLLM applications, providing protected surrogates and their MLLM-edited versions. 2) Formulates privacy recovery as a guided generation task conditioned on complementary multimodal signals. 3) Introduces a unified approach that reliably reconstructs private content while preserving MLLM-generated edit fidelity.

Result: Experiments on both SPPE and InstructPix2Pix datasets show the approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.

Conclusion: The work provides a novel perspective on MLLM privacy by focusing on privacy recovery quality assessment, offering both a benchmark dataset and a practical solution that maintains the balance between privacy protection and model utility.

Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.

</details>


### [210] [Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach](https://arxiv.org/abs/2512.07170)
*Jiayang Li,Chengjie Jiang,Junjun Jiang,Pengwei Liang,Jiayi Ma,Liqiang Nie*

Main category: cs.CV

TL;DR: DiTFuse is an instruction-driven Diffusion-Transformer framework that performs end-to-end, semantics-aware image fusion across multiple modalities using natural language instructions.


<details>
  <summary>Details</summary>
Motivation: Existing fusion approaches lack robustness, adaptability, and controllability. They are task-specific, cannot incorporate user intent, and struggle with complex scenarios like low-light degradation. The absence of ground-truth fused images and small datasets make end-to-end training difficult for simultaneous high-level semantics understanding and fine-grained multimodal alignment.

Method: DiTFuse jointly encodes two images and natural-language instructions in a shared latent space using a Diffusion-Transformer framework. It employs multi-degradation masked-image modeling for training, learning cross-modal alignment, modality-invariant restoration, and task-aware feature selection without ground truth. A curated multi-granularity instruction dataset enables interactive fusion capabilities.

Result: Superior quantitative and qualitative performance on IVIF, MFF, and MEF benchmarks with sharper textures and better semantic retention. The model unifies infrared-visible, multi-focus, and multi-exposure fusion, supports text-controlled refinement, downstream tasks, multi-level user control, and zero-shot generalization to other fusion scenarios including instruction-conditioned segmentation.

Conclusion: DiTFuse overcomes limitations of existing fusion methods by providing a flexible, controllable framework that integrates high-level semantics with fine-grained multimodal alignment through instruction-driven fusion, enabling unified handling of diverse fusion tasks within a single architecture.

Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.

</details>


### [211] [TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration](https://arxiv.org/abs/2512.07171)
*Shravan Venkatraman,Rakesh Raj Madavan,Pavan Kumar S,Muthu Subash Kavitha*

Main category: cs.CV

TL;DR: TIDE is a two-stage underwater image restoration framework that decomposes degradations into four factors (color distortion, haze, detail loss, noise), generates specialized restoration hypotheses for each, and adaptively fuses them based on local degradation patterns.


<details>
  <summary>Details</summary>
Motivation: Existing underwater image restoration methods apply uniform strategies across entire images, struggling with spatially varying co-occurring degradations that change with water conditions. There's a need for targeted restoration that addresses multiple degradation factors simultaneously.

Method: Two-stage framework: 1) Decomposes underwater degradations into four key factors and designs specialized restoration experts for each, generating multiple restoration hypotheses. 2) Adaptively fuses these hypotheses based on local degradation patterns, followed by progressive refinement to correct residual artifacts.

Result: TIDE achieves competitive performance on reference-based fidelity metrics and outperforms state-of-the-art methods on non-reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement across both standard benchmarks and challenging turbid water conditions.

Conclusion: TIDE effectively addresses the complex, spatially varying nature of underwater degradations through specialized prior decomposition and adaptive fusion, providing a robust solution for underwater image restoration that balances competing degradation factors and produces natural results even in highly degraded regions.

Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.

</details>


### [212] [START: Spatial and Textual Learning for Chart Understanding](https://arxiv.org/abs/2512.07186)
*Zhuoming Liu,Xiaofeng Gao,Feiyang Niu,Qiaozi Gao,Liu Liu,Robinson Piramuthu*

Main category: cs.CV

TL;DR: START is a multimodal LLM approach for chart understanding that combines spatial layout learning and textual data learning through chart-element grounding and chart-to-code generation, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Charts require understanding both visual spatial layout and underlying textual data representation, which is crucial for real-world applications like analyzing scientific papers and technical reports. Existing methods fail to adequately handle both aspects.

Method: Proposes START with two key components: (1) chart-element grounding to understand visual layout, and (2) chart-to-code generation to recover underlying data. Uses a novel data generation pipeline where MLLMs translate real chart images to executable code, then LLMs evolve the code to capture element positions. Also introduces CS-Bench benchmark for spatial understanding evaluation.

Result: START delivers consistent gains across model sizes and benchmarks over base models, surpassing prior state-of-the-art by a clear margin.

Conclusion: Combining spatial and textual learning is essential for comprehensive chart understanding, and the proposed START framework effectively addresses this dual requirement, filling a critical gap in chart reasoning capabilities.

Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.

</details>


### [213] [Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification](https://arxiv.org/abs/2512.07190)
*Pengfei Gu,Huimin Li,Haoteng Tang,Dongkuan,Xu,Erik Enriquez,DongChul Kim,Bin Fu,Danny Z. Chen*

Main category: cs.CV

TL;DR: A new medical image classification framework integrates multi-scale and multi-filtration persistent topological features with vision backbones, improving recognition of complex anatomical structures.


<details>
  <summary>Details</summary>
Motivation: Current deep neural networks either focus on pixel-intensity features instead of fundamental anatomical structures, or capture only simple topological features via single-parameter persistence, limiting their ability to recognize complex anatomical patterns important for medical diagnosis.

Method: 1) Compute cubical persistence diagrams across multiple image resolutions/scales; 2) Develop a "vineyard" algorithm to consolidate these diagrams into a single stable diagram capturing signatures from global anatomy to local irregularities; 3) Design a cross-attention-based neural network to process consolidated persistence diagrams; 4) Fuse topological embeddings with feature maps from CNNs or Transformers in an end-to-end architecture.

Result: Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating enhanced model capacity to recognize complex anatomical structures.

Conclusion: The comprehensive topological perspective provides robust and interpretable medical image classification by integrating multi-scale and multi-filtration topological features into vision backbones, offering value for recognizing anatomical structures and early-stage disease indicators.

Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.

</details>


### [214] [RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction](https://arxiv.org/abs/2512.07191)
*Wenqi Zhao,Jiacheng Sang,Fenghua Cheng,Yonglu Shu,Dong Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: RefLSM: A novel variational level set model that integrates Retinex-inspired reflectance decomposition for robust medical image segmentation under non-uniform illumination conditions.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation faces challenges from intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods struggle with severe non-uniform imaging conditions due to their reliance on approximate bias field estimations.

Method: Proposes Reflectance-based Level Set Model (RefLSM) that decomposes images into reflectance and bias field components, then segments the illumination-invariant reflectance. Includes two innovations: 1) linear structural prior that guides smoothed reflectance gradients toward data-driven references, and 2) relaxed binary level-set enforced via convex relaxation and sign projection. Uses ADMM-based optimization.

Result: Extensive experiments on multiple medical imaging datasets show RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.

Conclusion: RefLSM effectively addresses limitations of traditional level set methods by explicitly integrating reflectance decomposition, providing reliable segmentation under challenging non-uniform imaging conditions while maintaining computational efficiency.

Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.

</details>


### [215] [HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression](https://arxiv.org/abs/2512.07192)
*Niu Yi,Xu Tianyi,Ma Mingming,Wang Xinkun*

Main category: cs.CV

TL;DR: HVQ-CGIC introduces a controllable generative image compression framework with VQ hyperprior that adapts entropy estimation to image content, achieving 61.3% bitrate reduction compared to SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Current VQ-based generative compression methods use static global probability distributions for entropy estimation, which fails to adapt to specific image content, leading to untapped bitrate potential and challenges in flexible rate control.

Method: Introduces HVQ-CGIC framework with mathematical foundation for VQ indices hyperprior, novel loss design for RD balance and control, and lightweight hyper-prior estimation network to adapt entropy estimation to image content.

Result: Achieves significant RD performance advantage over SOTA generative compression methods - same LPIPS quality as Control-GIC, CDC and HiFiC with 61.3% fewer bits on Kodak dataset.

Conclusion: HVQ-CGIC has potential to become foundational component for VQGAN-based image compression, analogous to HyperPrior framework's role in neural image compression.

Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.

</details>


### [216] [SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting](https://arxiv.org/abs/2512.07197)
*Seokhyun Youn,Soohyun Lee,Geonho Kim,Weeyoung Kwon,Sung-Ho Bae,Jihyong Oh*

Main category: cs.CV

TL;DR: Survey paper on efficient 3D/4D Gaussian Splatting techniques, categorizing methods into Parameter Compression and Restructuring Compression to address memory and computational demands while preserving quality.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting enables real-time, high-fidelity 3D reconstruction but faces massive memory and computational demands, especially in 4D dynamic scenes. This survey addresses the need to reduce redundancy while maintaining quality for practical applications.

Method: Provides first unified overview of efficient 3D/4D Gaussian Splatting techniques. Systematically categorizes existing methods into two major directions: Parameter Compression and Restructuring Compression. Covers datasets, evaluation metrics, and benchmark comparisons.

Result: Comprehensive survey that organizes the rapidly evolving field of efficient Gaussian splatting, identifies methodological trends, and provides systematic categorization of existing techniques for both static and dynamic scene representation.

Conclusion: The survey establishes a foundation for understanding efficient Gaussian splatting techniques, discusses current limitations, and outlines promising research directions toward scalable, compact, and real-time 3D/4D scene representation.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.

</details>


### [217] [Understanding Diffusion Models via Code Execution](https://arxiv.org/abs/2512.07201)
*Cheng Yu*

Main category: cs.CV

TL;DR: A concise ~300-line implementation tutorial that bridges theory and practice for diffusion models, focusing on code execution rather than just mathematical derivations.


<details>
  <summary>Details</summary>
Motivation: To address the gap between complex theoretical formulations of diffusion models and their practical implementations, providing a clear implementation-first understanding.

Method: Develop a minimal implementation (~300 lines) that includes essential components: forward diffusion, reverse sampling, noise-prediction network, and training loop, while removing unnecessary engineering details.

Result: A concise codebase that demonstrates how diffusion models actually operate in practice, with available pre-trained models and code at GitHub repository.

Conclusion: This technical report provides researchers with a practical, implementation-focused understanding of diffusion models, bridging the gap between theory and code execution.

Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.

</details>


### [218] [MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning](https://arxiv.org/abs/2512.07203)
*Xuhui Zheng,Kang An,Ziliang Wang,Yuhang Wang,Faqiang Qian,Yichao Wu*

Main category: cs.CV

TL;DR: MMRPT is a masked multimodal reinforcement pre-training framework that uses reinforcement learning to strengthen visual reasoning in multimodal LLMs by rewarding visual grounding instead of caption imitation.


<details>
  <summary>Details</summary>
Motivation: Current multimodal pre-training suffers from descriptive bias in image-caption pairs, causing models to rely too much on surface linguistic cues rather than grounded visual understanding.

Method: Incorporates reinforcement learning directly into pre-training, constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens, masks highly vision-dependent segments, and reconstructs spans through vision-grounded reasoning guided by semantic-visual rewards.

Result: Shows consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning.

Conclusion: Reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models by strengthening visual grounding capabilities.

Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

</details>


### [219] [AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT](https://arxiv.org/abs/2512.07206)
*Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong*

Main category: cs.CV

TL;DR: AutoLugano is a fully automated deep learning system that performs end-to-end lymphoma classification from FDG-PET/CT scans, including lesion segmentation, anatomical localization, and automated Lugano staging.


<details>
  <summary>Details</summary>
Motivation: To develop an automated system for lymphoma classification that can assist in initial staging, treatment stratification, and clinical decision-making by translating baseline FDG-PET/CT scans into complete Lugano stages without manual intervention.

Method: Three sequential modules: (1) Anatomy-Informed Lesion Segmentation using 3D nnU-Net trained on multi-channel inputs, (2) Atlas-based Anatomical Localization using TotalSegmentator toolkit to map lesions to 21 lymph node regions, and (3) Automated Lugano Staging that translates spatial distribution into Lugano stages and therapeutic groups (Limited vs. Advanced Stage). Trained on autoPET dataset (n=1,007) and validated on independent cohort of 67 patients.

Result: External validation showed overall accuracy of 88.31%, sensitivity of 74.47%, specificity of 94.21%, and F1-score of 80.80% for regional involvement detection. For therapeutic stratification (Limited vs. Advanced Stage), achieved 85.07% accuracy with 90.48% specificity and 82.61% sensitivity, outperforming baseline models.

Conclusion: AutoLugano is the first fully automated, end-to-end pipeline that translates single baseline FDG-PET/CT scans into complete Lugano stages, demonstrating strong potential to assist in initial staging, treatment stratification, and clinical decision-making.

Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.

</details>


### [220] [Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds](https://arxiv.org/abs/2512.07211)
*Frederik Hagelskj忙r,Dimitrios Arapis,Steffen Madsen,Thorbj酶rn Mosekj忙r Iversen*

Main category: cs.CV

TL;DR: First deep learning method for object pose uncertainty estimation using only 3D colorless data, validated in real-world bin picking with geometrically ambiguous objects.


<details>
  <summary>Details</summary>
Motivation: Single pose estimates can't capture uncertainty from visual ambiguity, leading to unreliable robotic behavior. Existing methods rely heavily on color information, which is often unavailable in industrial settings.

Method: Novel neural network-based approach that estimates object pose uncertainty using only 3D colorless data (no RGB input). First deep learning method for pose distribution estimation without color information.

Result: Method validated in real-world bin picking scenario with objects of varying geometric ambiguity. Current implementation handles symmetries in reflection and revolution, with framework extendable to full SE(3) pose distribution estimation.

Conclusion: Proposed approach enables reliable pose uncertainty estimation in industrial settings where color information is unavailable, addressing a critical gap in robotic perception for ambiguous objects.

Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io

</details>


### [221] [VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation](https://arxiv.org/abs/2512.07215)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: Comparison of CLIP vs DINOv2 for 3D hand-object pose estimation, showing CLIP excels in semantic understanding while DINOv2 provides better geometric features.


<details>
  <summary>Details</summary>
Motivation: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision with rich semantic and geometric representations, but there's a need to understand their comparative strengths for 3D pose estimation in robotic grasping scenarios.

Method: Comprehensive visual comparison between CLIP-based and DINOv2-based approaches for 3D pose estimation in hand-object grasping. Evaluation on 6D object pose estimation task using benchmark datasets to analyze complementary strengths.

Result: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. CLIP-based methods achieve better semantic consistency, while DINOv2-based approaches show competitive performance with enhanced geometric precision.

Conclusion: The analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping applications based on whether semantic understanding or geometric precision is prioritized.

Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

</details>


### [222] [Towards Robust Protective Perturbation against DeepFake Face Swapping](https://arxiv.org/abs/2512.07228)
*Hengyang Yao,Lin Li,Ke Sun,Jianing Qiu,Huiping Chen*

Main category: cs.CV

TL;DR: EOLT framework learns optimal transformation distributions for DeepFake protection, achieving 26% higher robustness than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current DeepFake protection methods using invisible perturbations are fragile against basic transformations like compression/resizing, and standard EOT with uniform sampling is suboptimal due to sensitivity to transformation choices.

Method: Proposes Expectation Over Learned distribution of Transformation (EOLT) framework with a policy network that learns to prioritize critical transformations and generate instance-specific perturbations via reinforcement learning.

Result: Achieves 26% higher average robustness and up to 30% gains on challenging transformation categories compared to state-of-the-art approaches.

Conclusion: Treating transformation distribution as a learnable component rather than fixed design choice significantly improves DeepFake protection robustness against various transformations.

Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.

</details>


### [223] [ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery](https://arxiv.org/abs/2512.07229)
*Fang Zhou,Zhiqiang Chen,Martin Pavlovski,Yizhong Zhang*

Main category: cs.CV

TL;DR: ReLKD is an end-to-end framework for Generalized Category Discovery that leverages implicit inter-class relations to improve novel class classification, especially with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: GCD faces challenges in categorizing unlabeled data with known and novel classes when only known class labels are available. Previous approaches treat classes independently, ignoring important inter-class relationships that are difficult to obtain directly in real-world scenarios.

Method: ReLKD uses three modules: 1) target-grained module for discriminative representations, 2) coarse-grained module for capturing hierarchical class relations, and 3) distillation module for transferring knowledge from coarse-grained to target-grained module to refine representation learning.

Result: Extensive experiments on four datasets demonstrate ReLKD's effectiveness, particularly in scenarios with limited labeled data.

Conclusion: ReLKD successfully addresses GCD challenges by exploiting implicit inter-class relations through a multi-module framework, showing strong performance especially with limited labeled data.

Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.

</details>


### [224] [STRinGS: Selective Text Refinement in Gaussian Splatting](https://arxiv.org/abs/2512.07230)
*Abhinav Raundhal,Gaurav Behera,P J Narayanan,Ravi Kiran Sarvadevabhatla,Makarand Tapaswi*

Main category: cs.CV

TL;DR: STRinGS is a text-aware refinement framework for 3D Gaussian Splatting that treats text and non-text regions separately to preserve fine-grained text details in 3D reconstructions.


<details>
  <summary>Details</summary>
Motivation: Text elements in real-world scenes convey important contextual information, but current 3D representations like 3D Gaussian Splatting struggle to preserve fine-grained text details. Small errors in text reconstruction can lead to significant semantic loss, creating a need for text-aware 3D reconstruction methods.

Method: STRinGS uses a selective refinement framework that treats text and non-text regions separately. It refines text regions first, then merges them with non-text regions for full-scene optimization. The approach introduces a text readability measure (OCR Character Error Rate) and a curated dataset STRinGS-360 with diverse text scenarios.

Result: STRinGS achieves a 63.6% relative improvement over standard 3DGS at just 7K iterations, producing sharp, readable text even in challenging configurations. The method effectively preserves text details in 3D reconstructions.

Conclusion: STRinGS and the accompanying STRinGS-360 dataset push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods that can better preserve semantic information from textual elements.

Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.

</details>


### [225] [Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models](https://arxiv.org/abs/2512.07234)
*Biao Chen,Lin Zuo,Mengmeng Jing,Kunbin He,Yuchen Wang*

Main category: cs.CV

TL;DR: Dropout Prompt Learning applies dropout to vision-language models by evaluating token significance based on intra-modal context and inter-modal alignment, with residual entropy regularization to maintain semantic alignment while encouraging diverse representations.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness of vision-language models by applying dropout regularization in a more sophisticated way than vanilla dropout, addressing challenges like low-shot learning, long-tail classification, and out-of-distribution generalization.

Method: Proposes Dropout Prompt Learning that applies dropout on textual and visual tokens with flexible probabilities based on token significance considering both intra-modal context and inter-modal alignment. Also introduces residual entropy regularization to maintain semantic alignment while encouraging diverse representations from dropout.

Result: Experiments on 15 benchmarks show effectiveness in challenging scenarios. Notably surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

Conclusion: Dropout Prompt Learning effectively improves vision-language model robustness through context-aware dropout and residual entropy regularization, demonstrating strong performance across various challenging generalization tasks.

Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

</details>


### [226] [Unified Camera Positional Encoding for Controlled Video Generation](https://arxiv.org/abs/2512.07237)
*Cheng Zhang,Boying Li,Meng Wei,Yan-Pei Cao,Camilo Cruz Gambardella,Dinh Phung,Jianfei Cai*

Main category: cs.CV

TL;DR: UCPE introduces a unified camera positional encoding that handles complete camera information (poses, intrinsics, distortions) for better camera-controllable video generation with transformers.


<details>
  <summary>Details</summary>
Motivation: Existing camera encoding methods rely on simplified pinhole assumptions, limiting generalization across diverse real-world camera intrinsics and lens distortions needed for 3D perception and video generation tasks.

Method: Proposes Relative Ray Encoding for geometry-consistent camera representation, Absolute Orientation Encoding for pitch/roll control, and integrates into pretrained video Diffusion Transformers via lightweight spatial attention adapter (<1% added parameters).

Result: Achieves state-of-the-art camera controllability and visual fidelity in camera-controlled text-to-video generation, validated on a large dataset covering diverse camera motions and lens types.

Conclusion: UCPE demonstrates effectiveness as a general camera representation for transformers with potential applications across multi-view, video, and 3D tasks.

Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

</details>


### [227] [Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture](https://arxiv.org/abs/2512.07241)
*Md. Srabon Chowdhury,Syeda Fahmida Tanzim,Sheekar Banerjee,Ishtiak Al Mamoon,AKM Muzahidul Islam*

Main category: cs.CV

TL;DR: Hybrid deep learning model combining SqueezeNet v1 (lightweight) and EfficientNet-B0 (high-performing) enhanced with handcrafted radiomic features achieves 98.93-99.08% accuracy for brain tumor classification on MRI, with low computational requirements.


<details>
  <summary>Details</summary>
Motivation: Brain tumor diagnosis requires timely and accurate MRI interpretation, but manual tumor delineation is difficult, time-consuming, and prone to inter-observer error. Current methods need improvement in both accuracy and computational efficiency.

Method: Proposed hybrid model combines SqueezeNet v1 (for lightweight computation) and EfficientNet-B0 (for high performance), enhanced with handcrafted radiomic features including HOG, LBP, Gabor filters, and Wavelet transforms. Trained on Nickparvar Brain Tumor MRI dataset with 7,023 contrast-enhanced T1-weighted axial MRI slices across four classes (glioma, meningioma, pituitary tumor, no tumor).

Result: Achieved 98.93% testing accuracy, improved to 99.08% with Test Time Augmentation (TTA). Model requires fewer than 2.1 million parameters and less than 1.2 GFLOPs, offering excellent computational efficiency while maintaining high diagnostic accuracy.

Conclusion: The hybrid approach provides optimal balance between computational efficiency and diagnostic accuracy, with near-clinical reliability for automated MRI-based tumor classification. Handcrafted features improve texture sensitivity while EfficientNet-B0 captures hierarchical features, making the model suitable for clinical decision-support systems.

Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.

</details>


### [228] [Zero-Shot Textual Explanations via Translating Decision-Critical Features](https://arxiv.org/abs/2512.07245)
*Toshinori Yamauchi,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: TEXTER generates textual explanations for image classifier decisions by isolating decision-critical features before alignment with language, producing more faithful explanations than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot explanation methods for image classifiers produce generic descriptions of visible content rather than capturing the specific features that drive the model's prediction. Large vision-language models generate general captions but lack classifier-specific reasoning.

Method: TEXTER identifies neurons contributing to predictions, emphasizes decision-critical features encoded in those neurons, maps them to CLIP feature space for textual retrieval, and uses a sparse autoencoder for improved interpretability in Transformer architectures.

Result: Extensive experiments show TEXTER generates more faithful and interpretable explanations than existing methods, better reflecting the model's actual reasoning process.

Conclusion: By isolating decision-critical features before language alignment, TEXTER overcomes limitations of existing explanation methods and produces textual explanations that transparently reveal classifier-specific reasoning.

Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.

</details>


### [229] [AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing](https://arxiv.org/abs/2512.07247)
*Ziming Hong,Tianyu Huang,Runnan Chen,Shanshan Ye,Mingming Gong,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: AdLift is the first editing safeguard for 3D Gaussian Splatting that prevents unauthorized instruction-driven editing by lifting 2D adversarial perturbations into 3D Gaussian representations.


<details>
  <summary>Details</summary>
Motivation: While diffusion-based editing pipelines enable 3DGS manipulation, they also expose 3D assets to unauthorized editing and malicious tampering. Existing 2D adversarial perturbation methods don't work well for 3DGS due to view-generalization challenges and difficulty balancing invisibility with protection.

Method: AdLift lifts strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguards. It uses a tailored Lifted PGD that alternates between gradient truncation during back-propagation from editing models and image-to-Gaussian fitting to optimize safeguard Gaussians across training views while constraining perturbations.

Result: AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing methods, providing consistent adversarial-based protection across different viewpoints that generalizes to novel views.

Conclusion: AdLift successfully addresses the challenges of view-generalizable protection and balancing invisibility with protection capability for 3DGS assets, providing the first effective safeguard against unauthorized instruction-driven editing of 3D Gaussian Splatting content.

Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.

</details>


### [230] [See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement](https://arxiv.org/abs/2512.07251)
*Junqi Liu,Zejun Wu,Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Ibrahim E. Hamamci,Sezgin Er,Tianyu Lin,Yi Luo,Szymon Potka,Bjoern Menze,Daguang Xu,Kai Ding,Kang Wang,Yang Yang,Yucheng Tang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: SMILE is an anatomy-aware diffusion model for medical image enhancement that improves image quality while preserving anatomical accuracy, outperforming existing methods across multiple metrics and datasets.


<details>
  <summary>Details</summary>
Motivation: Current medical image enhancement models often over-edit images, distorting organs, creating false findings, and missing small tumors because they lack understanding of anatomy and contrast dynamics. This can negatively impact clinical decision-making.

Method: SMILE uses three key innovations: (1) structure-aware supervision following true organ boundaries and contrast patterns, (2) registration-free learning working directly with unaligned multi-phase CT scans, and (3) unified inference providing fast, consistent enhancement across all contrast phases.

Result: Across six external datasets, SMILE outperforms existing methods with 14.2% higher SSIM, 20.6% higher PSNR, 50% better FID, and improves cancer detection from non-contrast CT by raising F1 score up to 10%.

Conclusion: SMILE successfully enhances only clinically relevant regions while preserving anatomical accuracy, producing diagnostically meaningful images that support better clinical decision-making in medical imaging.

Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.

</details>


### [231] [DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement](https://arxiv.org/abs/2512.07253)
*Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu*

Main category: cs.CV

TL;DR: A degradation-aware framework for real-time endoscopic video enhancement using contrastive learning to extract degradation representations and propagate them across frames for efficient surgical image improvement.


<details>
  <summary>Details</summary>
Motivation: Endoscopic surgery relies on intraoperative video quality, but videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Existing deep learning methods are too computationally demanding for real-time surgical use.

Method: Proposes a degradation-aware framework that extracts degradation representations using contrastive learning, then fuses these representations with image features to guide a single-frame enhancement model. Uses cycle-consistency constraint between degraded and restored images for robustness and generalization, enabling real-time enhancement through propagation of degradation representations across frames.

Result: The framework achieves superior balance between performance and efficiency compared with several state-of-the-art methods, demonstrating effectiveness of degradation-aware modeling for real-time endoscopic video enhancement.

Conclusion: Implicitly learning and propagating degradation representation offers a practical pathway for clinical application of real-time endoscopic video enhancement, addressing the computational demands of existing methods while maintaining high-quality results.

Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

</details>


### [232] [A graph generation pipeline for critical infrastructures based on heuristics, images and depth data](https://arxiv.org/abs/2512.07269)
*Mike Diessner,Yannick Tarant*

Main category: cs.CV

TL;DR: A graph generation pipeline using photogrammetry and deep learning to create virtual models of critical infrastructure from RGB images and stereo camera depth data, avoiding expensive laser scanning.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D point clouds from laser scanners for critical infrastructure modeling are expensive and require specialist knowledge. There's a need for more cost-effective and accessible methods to create virtual representations for simulations and digital twins.

Method: A photogrammetry-based pipeline using RGB images and depth data from stereo cameras. Employs deep learning for object detection and instance segmentation, then uses user-defined heuristics/rules to infer object relations and generate graphs.

Result: The method successfully generates graphs close to ground truth for two hydraulic systems. The approach is flexible for specific applications and transparent enough for high-stakes decision-making in critical infrastructure contexts.

Conclusion: The proposed photogrammetry-based graph generation pipeline offers a cost-effective, flexible, and transparent alternative to expensive laser scanning for creating virtual models of critical infrastructure, suitable for digital twin applications.

Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.

</details>


### [233] [RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2512.07273)
*Zhi Rao,Yucheng Zhou,Benjia Zhou,Yiqing Huang,Sergio Escalera,Jun Wan*

Main category: cs.CV

TL;DR: Proposes RVLF framework for gloss-free sign language translation using vision-language model with reinforcement learning to address inadequate sign representation and semantic misalignment.


<details>
  <summary>Details</summary>
Motivation: Gloss-free SLT faces two key challenges: 1) inadequate sign representation that fails to capture nuanced visual cues, and 2) sentence-level semantic misalignment in current LLM-based methods that limits translation quality.

Method: Three-stage RVLF framework: 1) Build LVLM for sign language with semantic representation learning fusing skeleton motion cues and DINOv2 visual features, 2) Instruction tuning to obtain SLT-SFT baseline, 3) GRPO-based RL optimization with reward combining BLEU (fidelity) and ROUGE (completeness) to get SLT-GRPO.

Result: Substantial gains without external pre-training: BLEU-4 improvements of +5.1 (CSL-Daily), +1.11 (PHOENIX-2014T), +1.4 (How2Sign), +1.61 (OpenASL). First work to incorporate GRPO into SLT.

Conclusion: RVLF effectively addresses sign representation and semantic alignment challenges in gloss-free SLT through vision-language fusion and RL optimization, achieving state-of-the-art performance across multiple datasets.

Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.

</details>


### [234] [Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation](https://arxiv.org/abs/2512.07275)
*Siyu Wang,Hua Wang,Huiyu Li,Fan Zhang*

Main category: cs.CV

TL;DR: Proposes a novel encoder-decoder network with multi-scale residual structures for skin lesion segmentation, introducing MRCF, CMAM, and EAB modules to address irregular shapes and low contrast challenges.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods fail to effectively handle irregular lesion shapes and low contrast in skin lesion segmentation, which is crucial for early detection and accurate diagnosis of skin diseases.

Method: Innovative encoder-decoder network with multi-scale residual structures, featuring: 1) Multi-Resolution Multi-Channel Fusion (MRCF) module for cross-scale feature capture, 2) Cross-Mix Attention Module (CMAM) for dynamic weight calculation across multiple contexts, and 3) External Attention Bridge (EAB) to compensate for information loss in skip connections.

Result: Extensive experiments on multiple skin lesion segmentation datasets show the proposed model significantly outperforms existing transformer and CNN-based models, demonstrating exceptional segmentation accuracy and robustness.

Conclusion: The proposed architecture effectively addresses challenges in skin lesion segmentation through innovative modules that enhance feature extraction, attention mechanisms, and information flow, leading to superior performance compared to state-of-the-art methods.

Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.

</details>


### [235] [Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery](https://arxiv.org/abs/2512.07276)
*Mai Tsujimoto,Junjue Wang,Weihao Xuan,Naoto Yokoya*

Main category: cs.CV

TL;DR: Geo3DVQA is a benchmark for evaluating vision-language models on 3D geospatial reasoning using only RGB remote sensing imagery, revealing current VLMs struggle with RGB-to-3D tasks but domain adaptation helps.


<details>
  <summary>Details</summary>
Motivation: Current 3D geospatial analysis relies on expensive sensors (LiDAR, multispectral) limiting global accessibility, and existing methods struggle with integrating multiple 3D cues, diverse queries, and interpretable reasoning.

Method: Created Geo3DVQA benchmark with 110k curated question-answer pairs across 16 task categories and three complexity levels (single-feature inference, multi-feature reasoning, application-level spatial analysis) using RGB-only remote sensing imagery.

Result: Top VLMs performed poorly: GPT-4o (28.6%), Gemini-2.5-Flash (33.0%), while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points improvement), showing limitations of current VLMs but effectiveness of domain adaptation.

Conclusion: Geo3DVQA introduces new challenges for scalable, accessible 3D geospatial analysis, demonstrating both the difficulty of RGB-to-3D reasoning and the value of domain-specific fine-tuning for vision-language models.

Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

</details>


### [236] [Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts](https://arxiv.org/abs/2512.07302)
*Mingning Guo,Mengwei Wu,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: AerialVP is an agent framework that enhances task prompts for UAV image perception by extracting multi-dimensional auxiliary information, addressing limitations of traditional VLM-based methods that struggle with UAV imagery challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional VLM-based image perception methods face limitations with UAV imagery due to challenges like target confusion, scale variations, and complex backgrounds. These issues arise because VLMs rely on semantic alignment between visual and textual tokens, which becomes difficult when task prompts are simplistic and image content is complex.

Method: AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts through a three-stage process: (1) analyzing task prompts to identify task type and enhancement needs, (2) selecting appropriate tools from a repository, and (3) generating enhanced task prompts based on analysis and selected tools.

Result: Experimental results show AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. The framework is evaluated using AerialSense, a comprehensive benchmark for UAV image perception.

Conclusion: AerialVP addresses the limitations of traditional VLM-based approaches for UAV image perception by enhancing task prompts with auxiliary information, resulting in improved performance across diverse UAV imagery challenges and conditions.

Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.

</details>


### [237] [Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset](https://arxiv.org/abs/2512.07305)
*Tobias Abraham Haider*

Main category: cs.CV

TL;DR: This study replicates Carl et al.'s work on using Google Inception-ResNet-v2 for automated detection of European wild mammal species in camera trap images, achieving similar results (62% accuracy vs original 71%) with a different dataset, confirming the approach's viability but highlighting limitations in generalization.


<details>
  <summary>Details</summary>
Motivation: To assess the reproducibility and generalizability of Carl et al.'s approach for automated wildlife species detection using pretrained CNN models, and to validate whether their findings hold with different datasets and implementations.

Method: Reimplemented the original experiment from scratch using openly available resources, applied minimal preprocessing, and tested on a different dataset of 900 images spanning 90 species using the Google Inception-ResNet-v2 model.

Result: Achieved 62% overall classification accuracy (close to original 71%), with substantial per-class performance variation (macro F1 score of 0.28), confirming similar performance despite dataset differences.

Conclusion: Pretrained CNNs provide a practical baseline for wildlife species identification, but species-specific adaptation or transfer learning is needed for consistent, high-quality predictions due to limitations in generalization when labels don't align with ImageNet classes.

Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.

</details>


### [238] [ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.07328)
*Ziyang Mai,Yu-Wing Tai*

Main category: cs.CV

TL;DR: ContextAnyone is a context-aware diffusion framework for character-consistent video generation from text and a single reference image, addressing identity drift by preserving broader contextual cues like hairstyle, outfit, and body shape.


<details>
  <summary>Details</summary>
Motivation: Existing T2V personalization methods focus mainly on facial identity but fail to preserve broader contextual cues (hairstyle, outfit, body shape) that are critical for visual coherence and character consistency across scenes.

Method: Proposes a context-aware diffusion framework with: 1) Joint reconstruction of reference image and generation of new video frames, 2) Emphasize-Attention module to selectively reinforce reference-aware features, 3) Dual-guidance loss combining diffusion and reference reconstruction objectives, 4) Gap-RoPE positional embedding to separate reference and video tokens for stable temporal modeling.

Result: Outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes.

Conclusion: ContextAnyone effectively addresses character identity consistency in T2V generation by preserving comprehensive contextual cues beyond just facial features, enabling high-quality character-consistent video generation from minimal reference.

Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

</details>


### [239] [The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers](https://arxiv.org/abs/2512.07331)
*Kanishk Awadhiya*

Main category: cs.CV

TL;DR: ViTs spontaneously develop U-shaped entropy profiles with information compression in middle layers, which is not architectural but data-dependent adaptation that correlates with task complexity.


<details>
  <summary>Details</summary>
Motivation: To understand why Vision Transformers (ViTs) exhibit "U-shaped" entropy profiles despite lacking hierarchical inductive biases, and to determine whether this is an architectural artifact or a data-driven adaptation.

Method: Analyzed layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100) to study information flow patterns.

Result: The depth of the bottleneck correlates strongly with semantic abstraction required by the task. Texture-heavy datasets preserve high-rank representations throughout, while object-centric datasets drive networks to dampen high-frequency information in middle layers, effectively learning a bottleneck to isolate semantic features.

Conclusion: The "Inductive Bottleneck" in ViTs is not an architectural artifact but a data-dependent adaptation where networks learn to compress information in middle layers based on the semantic complexity of the task.

Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.

</details>


### [240] [Generalized Referring Expression Segmentation on Aerial Photos](https://arxiv.org/abs/2512.07338)
*Lu铆s Marnoto,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: Aerial-D is a new large-scale referring expression segmentation dataset for aerial imagery with 37,288 images and 1.5M+ expressions covering 259K+ annotated targets across 21 classes, created via an automated pipeline with LLM enhancement and historical filters.


<details>
  <summary>Details</summary>
Motivation: Referring expression segmentation in aerial imagery faces unique challenges: varying spatial resolution, inconsistent color usage, tiny targets (few pixels), high object density, and partial occlusions. Existing datasets don't adequately address these aerial-specific complexities.

Method: Created Aerial-D dataset through fully automatic pipeline combining systematic rule-based expression generation with LLM enhancement for linguistic variety and visual detail focus. Applied filters to simulate historic imaging conditions. Used RSRefSeg architecture for training on combined datasets.

Result: Models trained on Aerial-D with prior aerial datasets achieve competitive performance on contemporary benchmarks while maintaining strong accuracy under monochrome, sepia, and grainy degradations typical of archival aerial photography.

Conclusion: Aerial-D enables unified instance and semantic segmentation from text for both modern and historical aerial images, addressing aerial-specific challenges. The dataset, models, and pipeline are publicly available to advance research in aerial referring expression segmentation.

Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .

</details>


### [241] [Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting](https://arxiv.org/abs/2512.07345)
*Shilong Jin,Haoran Duan,Litao Hua,Wentao Huang,Yuan Zhou*

Main category: cs.CV

TL;DR: TD-Attn is a novel framework that addresses multi-view inconsistency in 3D tasks by mitigating prior view bias in Text-to-Image diffusion models through 3D-aware attention guidance and hierarchical attention modulation.


<details>
  <summary>Details</summary>
Motivation: Text-to-Image diffusion models used for 3D tasks suffer from prior view bias, causing conflicting appearances between different views of an object due to subject-words preferentially activating prior view features regardless of target view conditions.

Method: TD-Attn uses two key components: 1) 3D-Aware Attention Guidance Module constructs view-consistent 3D attention Gaussians for subject-words to enforce spatial consistency; 2) Hierarchical Attention Modulation Module uses Semantic Guidance Tree and Semantic Response Profiler to localize and modulate cross-attention layers responsive to view conditions.

Result: Extensive experiments show TD-Attn significantly enhances multi-view consistency across 3D tasks and has potential to serve as a universal plugin for various 3D applications.

Conclusion: TD-Attn effectively addresses the prior view bias limitation in T2I models for 3D tasks, providing a framework for achieving multi-view consistency and enabling controllable, precise 3D editing through semantic-specific interventions.

Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

</details>


### [242] [MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition](https://arxiv.org/abs/2512.07348)
*Xinyu Wei,Kangrui Cen,Hongyang Wei,Zhen Guo,Bairui Li,Zeqing Wang,Jinrui Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: MICo-150K: A large-scale dataset for Multi-Image Composition with 150K synthetic composite images across 7 tasks, plus a Decomposition-and-Recomposition subset, enabling models to generate coherent images from multiple reference inputs.


<details>
  <summary>Details</summary>
Motivation: Multi-Image Composition (MICo) - generating coherent images from multiple reference inputs - is challenging due to lack of high-quality training data. Existing datasets are insufficient for comprehensive MICo research.

Method: 1) Systematically categorize MICo into 7 tasks; 2) Curate source images and construct diverse prompts; 3) Use proprietary models to synthesize composite images; 4) Human-in-the-loop filtering/refinement; 5) Create Decomposition-and-Recomposition subset from real images; 6) Build MICo-Bench benchmark with new evaluation metric.

Result: Created MICo-150K dataset with identity consistency, De&Re subset with 11K real-world decompositions, and MICo-Bench evaluation benchmark. Fine-tuned models show MICo-150K effectively equips models without MICo capability and enhances existing skills. Qwen-MICo baseline matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs.

Conclusion: MICo-150K dataset, benchmark, and baseline provide valuable resources for Multi-Image Composition research, addressing the data gap and enabling comprehensive evaluation of MICo capabilities in image generation models.

Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.

</details>


### [243] [DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351)
*Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Sami Azam*

Main category: cs.CV

TL;DR: DeepAgent: A multi-agent framework combining visual and audio analysis with Random Forest fusion for robust deepfake detection across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods that integrate audio and visual cues within single models are vulnerable to modality mismatches, noise, and manipulation. There's a need for more robust approaches that can handle diverse types of manipulations.

Method: DeepAgent uses two complementary agents: Agent-1 (AlexNet-based CNN for visual manipulation detection) and Agent-2 (audio-visual inconsistency detection using acoustic features, Whisper transcriptions, and EasyOCR frame analysis). Decisions are fused through a Random Forest meta-classifier.

Result: Agent-1 achieved 94.35% accuracy on Celeb-DF+FakeAVCeleb. Agent-2 and meta-classifier achieved 93.69% and 81.56% on FakeAVCeleb. Cross-dataset validation on DeepFakeTIMIT showed 97.49% accuracy, demonstrating strong generalization.

Conclusion: Hierarchy-based fusion enhances robustness by mitigating individual modality weaknesses. Multi-agent collaboration effectively addresses diverse deepfake manipulations, showing strong cross-dataset performance.

Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

</details>


### [244] [Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.07360)
*Qiming Huang,Hao Ai,Jianbo Jiao*

Main category: cs.CV

TL;DR: Proposes a structure-aware feature rectification method using region adjacency graphs to refine CLIP features for better open-vocabulary semantic segmentation by enhancing local discrimination and reducing noise.


<details>
  <summary>Details</summary>
Motivation: CLIP's pre-training on image-text pairs focuses on global semantic alignment, leading to suboptimal performance for fine-grained visual region-text association, resulting in noisy and inconsistent predictions in local areas due to dispersed bias from contrastive training.

Method: Proposes structure-aware feature rectification using instance-specific priors from images. Constructs region adjacency graph (RAG) based on low-level features (color, texture) to capture local structural relationships, then uses this graph to refine CLIP features by enhancing local discrimination.

Result: Extensive experiments show the method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

Conclusion: The proposed structure-aware feature rectification approach successfully addresses CLIP's limitations in fine-grained region-text association for open-vocabulary semantic segmentation by incorporating local structural priors to refine features and improve segmentation quality.

Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

</details>


### [245] [Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency](https://arxiv.org/abs/2512.07379)
*Mahila Moghadami,Mohammad Ali Keyvanrad,Melika Sabaghian*

Main category: cs.CV

TL;DR: Enhanced SW-YOLO model with architectural improvements achieves 61.2 mAP on VisDrone2019, significantly outperforming baseline YOLOv5L (35.5 mAP) and CZDet (58.36 mAP).


<details>
  <summary>Details</summary>
Motivation: Small object detection in large-scale aerial images is crucial for critical applications, but current methods using image cropping and architectural modifications need improvement for better accuracy and robustness.

Method: Modified SW-YOLO approach with refined sliding window cropping dimensions and overlap, plus architectural enhancements: advanced feature extraction modules in the neck, CBAM integration in backbone for spatial/channel information preservation, and new head design for improved small object detection.

Result: Achieved 61.2 mAP .5.5 on VisDrone2019 dataset, significantly outperforming baseline YOLOv5L (35.5 mAP) and CZDet (58.36 mAP), representing substantial accuracy improvement.

Conclusion: The proposed enhanced SW-YOLO model demonstrates significant improvements in small object detection accuracy for aerial imagery, offering a robust framework that outperforms existing methods including SAHI and CZDet.

Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.

</details>


### [246] [Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects](https://arxiv.org/abs/2512.07381)
*Shuohan Tao,Boyao Zhou,Hanzhang Tu,Yuwang Wang,Yebin Liu*

Main category: cs.CV

TL;DR: Tessellation GS improves 3D Gaussian Splatting by anchoring 2D Gaussians to mesh faces with hierarchical neural features, enabling better dynamic scene reconstruction from single cameras.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting struggles with viewpoint extrapolation, overfitting, and poor generalization in sparse-view and dynamic scene reconstruction, especially from single static cameras.

Method: Anchors 2D Gaussians to mesh faces, uses hierarchical neural features for attribute inference, employs adaptive face subdivision guided by detail-aware loss, and leverages reconstruction foundation model priors for Gaussian deformation initialization.

Result: Outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

Conclusion: Tessellation GS enables robust reconstruction of general dynamic objects from single static cameras, solving previously challenging problems for optimization-based methods.

Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

</details>


### [247] [LogicCBMs: Logic-Enhanced Concept-Based Learning](https://arxiv.org/abs/2512.07383)
*Deepika SN Vemuri,Gautham Bellamkonda,Aditya Pola,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: LogicCBM enhances concept bottleneck models by replacing linear concept combinations with differentiable logic operations, improving accuracy, interpretability, and intervention capabilities.


<details>
  <summary>Details</summary>
Motivation: Current Concept Bottleneck Models (CBMs) are limited by using only linear combinations of concepts for predictions, which restricts their expressiveness and ability to capture complex inter-concept relationships.

Method: Proposes LogicCBM with a logic module that connects learned concepts through differentiable logic operations (AND, OR, NOT, etc.), enabling end-to-end learning while going beyond simple weighted combinations.

Result: Empirical studies on benchmarks and synthetic datasets show LogicCBM achieves better accuracy, performs effective interventions, and maintains high interpretability compared to standard CBMs.

Conclusion: Enhancing CBMs with propositional logic operations improves model expressivity, captures inter-concept relations better, and yields superior performance while preserving interpretability benefits.

Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.

</details>


### [248] [How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline](https://arxiv.org/abs/2512.07385)
*Chunhui Zhang,Li Liu,Zhipeng Zhang,Yong Wang,Hao Wen,Xi Zhou,Shiming Ge,Yanfeng Wang*

Main category: cs.CV

TL;DR: Proposes UAV-Anti-UAV tracking task where a pursuer UAV tracks a target adversarial UAV, introduces million-scale dataset, and develops MambaSTS baseline method with integrated spatial-temporal-semantic learning.


<details>
  <summary>Details</summary>
Motivation: Current Anti-UAV research focuses on fixed ground cameras, but there's a gap in tracking target UAVs from another moving UAV platform. The UAV-Anti-UAV task addresses this real-world scenario with dual-dynamic disturbances from both pursuer and target motion.

Method: Proposes MambaSTS, a Mamba-based baseline method that enables integrated spatial-temporal-semantic learning. Uses Mamba for global semantic features, Transformer for spatial features, and leverages state space model's long-sequence modeling via temporal token propagation mechanism.

Result: Constructed a million-scale dataset with 1,810 videos, each annotated with bounding boxes, language prompts, and 15 tracking attributes. Experimental evaluation of 50 modern deep tracking algorithms shows significant room for improvement in UAV-Anti-UAV domain.

Conclusion: UAV-Anti-UAV is a challenging new tracking task that addresses real-world scenarios with dual-dynamic disturbances. The proposed dataset and MambaSTS baseline method provide foundation for future research, with current methods showing substantial room for improvement.

Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

</details>


### [249] [GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring](https://arxiv.org/abs/2512.07391)
*ore Nedeljkovi*

Main category: cs.CV

TL;DR: GlimmerNet is an ultra-lightweight CNN that achieves strong global perception without expensive self-attention, using grouped dilated convolutions and efficient feature fusion to set new accuracy-efficiency frontier for UAV emergency monitoring.


<details>
  <summary>Details</summary>
Motivation: While CNNs are efficient for edge/mobile vision, recent attempts to add global context via Vision Transformers introduce significant computational overhead. There's a need to retain strong global perception without expensive components for resource-constrained platforms like UAVs.

Method: GlimmerNet separates receptive field diversity from feature recombination. It uses Grouped Dilated Depthwise Convolutions (GDBlocks) that partition channels into groups with different dilation rates for multi-scale feature extraction at no extra parameters. An Aggregator module fuses features efficiently using grouped pointwise convolution to reduce parameter overhead.

Result: With only 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset.

Conclusion: GlimmerNet establishes a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms, demonstrating that strong global perception can be achieved without computationally expensive components.

Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.

</details>


### [250] [Reconstructing Objects along Hand Interaction Timelines in Egocentric Video](https://arxiv.org/abs/2512.07394)
*Zhifan Zhu,Siddhant Bansal,Shashank Tripathi,Dima Damen*

Main category: cs.CV

TL;DR: ROHIT introduces reconstructing objects along hand interaction timelines using pose constraints and propagation for better 3D reconstruction from videos without 3D ground truth.


<details>
  <summary>Details</summary>
Motivation: Current object reconstruction methods struggle with hand-object interactions, especially during dynamic manipulation phases. There's a need to model object pose changes along hand interaction timelines for better reconstruction in egocentric videos.

Method: Defines Hand Interaction Timeline (HIT) with phases: static  contact  stable grasp  release  static. Proposes Constrained Optimisation and Propagation (COP) framework to propagate object pose along HIT using pose constraints during stable grasps.

Result: Evaluated on HOT3D (1.2K clips) and EPIC-Kitchens (2.4K clips, 390 objects, 9 categories). COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% using 2D projection error metrics.

Conclusion: ROHIT enables effective object reconstruction along hand interaction timelines without 3D ground truth. The COP framework with pose propagation significantly improves reconstruction quality for hand-object interactions in egocentric videos.

Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

</details>


### [251] [InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs](https://arxiv.org/abs/2512.07410)
*Bin Li,Ruichi Zhang,Han Liang,Jingyan Zhang,Juze Zhang,Xin Chen,Lan Xu,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: InterAgent: First end-to-end framework for text-driven physics-based multi-agent humanoid control using autoregressive diffusion transformer with multi-stream blocks and interaction graph exteroception.


<details>
  <summary>Details</summary>
Motivation: Existing methods are confined to single-agent scenarios and overlook physically plausible interplay essential for multi-agent interactions. Need to bridge gap for text-driven multi-agent humanoid control.

Method: Proposes InterAgent with: 1) Autoregressive diffusion transformer with multi-stream blocks decoupling proprioception, exteroception, and action; 2) Interaction graph exteroception representation capturing joint-to-joint spatial dependencies; 3) Sparse edge-based attention mechanism pruning redundant connections and emphasizing critical inter-agent relations.

Result: Extensive experiments show InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. Enables coherent, physically plausible, and semantically faithful multi-agent behaviors from text prompts.

Conclusion: InterAgent is the first end-to-end framework for text-driven physics-based multi-agent humanoid control, demonstrating superior performance and enabling realistic multi-agent interactions from text descriptions.

Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.

</details>


### [252] [Data-driven Exploration of Mobility Interaction Patterns](https://arxiv.org/abs/2512.07415)
*Gabriele Galatolo,Mirco Nanni*

Main category: cs.CV

TL;DR: A data mining approach to discover mobility interaction patterns from real data, rather than using preconceived behavioral models, applied to car and pedestrian case studies.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for modeling human dynamics rely on preconceived behavioral models, but there's a need to understand actual movement behaviors and interactions between individuals from real data to improve applications like crowd simulation and emergency management.

Method: Proposes a data mining approach that searches mobility events in data for evidence of mutual interactions between individuals, then identifies complex, persistent patterns and time-evolving configurations of events from real-world data.

Result: The methodology is instantiated on two real case studies (cars and pedestrians) with full experimental evaluation covering performance, parameter sensitivity, and interpretation of sample results.

Conclusion: Studying discovered patterns can provide new insights into mobility interaction mechanics, potentially helping to improve existing simulation models for human dynamics applications.

Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.

</details>


### [253] [When normalization hallucinates: unseen risks in AI-powered whole slide image processing](https://arxiv.org/abs/2512.07426)
*Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif*

Main category: cs.CV

TL;DR: Deep learning-based WSI normalization methods can introduce realistic-looking hallucinations that mask important diagnostic features, with current evaluation practices failing to detect these artifacts.


<details>
  <summary>Details</summary>
Motivation: Current WSI normalization methods using deep learning tend to produce average-looking outputs that may obscure diagnostically important features and can introduce undetectable hallucinated content, posing serious risks for clinical applications.

Method: Proposed a novel image comparison measure to automatically detect hallucinations in normalized outputs, then systematically evaluated several well-cited normalization methods retrained on real-world clinical data.

Result: Found concerning frequency of hallucinations in real-world clinical data that were not captured by conventional metrics, revealing significant inconsistencies and failures in existing normalization methods.

Conclusion: Highlights the need for more robust, interpretable normalization techniques and stricter validation protocols for clinical deployment due to the real but underappreciated risk of hallucinations in WSI normalization.

Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

</details>


### [254] [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469)
*Xiangpeng Yang,Ji Xie,Yiyuan Yang,Yan Huang,Min Xu,Qiang Wu*

Main category: cs.CV

TL;DR: VideoCoF introduces a Chain-of-Frames approach for mask-free video editing that predicts edit-region latents before generating target videos, achieving precise instruction-to-region alignment without user-provided masks.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods face a trade-off: expert models need task-specific masks, while unified temporal in-context learning models lack explicit spatial cues, resulting in weak instruction-to-region mapping and imprecise localization.

Method: VideoCoF enforces a "see, reason, then edit" procedure where the video diffusion model first predicts reasoning tokens (edit-region latents) before generating target video tokens. Also introduces RoPE alignment strategy for motion alignment and length extrapolation.

Result: Achieves state-of-the-art performance on VideoCoF-Bench with minimal data cost of only 50k video pairs, demonstrating efficiency and effectiveness.

Conclusion: VideoCoF resolves the conflict between precision and unification in video editing by providing mask-free operation while maintaining precise instruction-to-region alignment through explicit reasoning steps.

Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

</details>


### [255] [Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance](https://arxiv.org/abs/2512.07480)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Zihan Zheng,Yuan Zhang,Yan Lu*

Main category: cs.CV

TL;DR: S2VC is a single-step diffusion-based video codec that achieves state-of-the-art perceptual quality at low bitrates with 52.73% bitrate savings over prior methods, using efficient single-step diffusion generation instead of complex sampling.


<details>
  <summary>Details</summary>
Motivation: Traditional and neural video codecs struggle with perceptual quality at low bitrates - some suffer from artifacts due to limited generation capacity, while others use heavy diffusion sampling that's computationally expensive. There's a need for efficient, high-quality video compression.

Method: S2VC integrates conditional coding with single-step diffusion generator. Key innovations: 1) Contextual Semantic Guidance extracts frame-adaptive semantics from buffered features instead of text captions, 2) Temporal Consistency Guidance in diffusion U-Net enforces temporal coherence across frames.

Result: Extensive experiments show S2VC delivers state-of-the-art perceptual quality with average 52.73% bitrate saving over prior perceptual methods, enabling realistic reconstruction at low bitrates with reduced sampling cost.

Conclusion: S2VC demonstrates the promise of single-step diffusion for efficient, high-quality video compression, overcoming limitations of both traditional codecs and complex diffusion-based approaches through innovative semantic conditioning and temporal consistency mechanisms.

Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.

</details>


### [256] [Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior](https://arxiv.org/abs/2512.07498)
*Chih-Chung Hsu,Shao-Ning Chen,Chia-Ming Lee,Yi-Fang Wang,Yi-Shiuan Chou*

Main category: cs.CV

TL;DR: LR-GCN: A Laplacian-regularized graph convolutional network that detects DeepFakes from noisy or unordered face sequences using order-free temporal graph embeddings and spectral priors.


<details>
  <summary>Details</summary>
Motivation: Existing DeepFake detectors assume clean, temporally consistent facial sequences, but real-world scenarios involve compression artifacts, occlusions, adversarial attacks, and face detection failures that destabilize detection.

Method: Proposes LR-GCN with Order-Free Temporal Graph Embedding (OF-TGE) that organizes CNN features into adaptive sparse graphs based on semantic affinities. Uses dual-level sparsity on graph structure and node features, and introduces Graph Laplacian Spectral Prior as high-pass operator to highlight forgery artifacts, followed by low-pass GCN aggregation.

Result: Achieves state-of-the-art performance on FF++, Celeb-DFv2, and DFDC datasets, with significantly improved robustness under severe disruptions including missing faces, occlusions, and adversarial perturbations.

Conclusion: LR-GCN effectively handles noisy, unordered face sequences through spectral band-pass mechanism that suppresses noise while preserving manipulation cues, demonstrating superior robustness in real-world DeepFake detection scenarios.

Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.

</details>


### [257] [MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer](https://arxiv.org/abs/2512.07500)
*Penghui Liu,Jiangshan Wang,Yutong Shen,Shanhui Mo,Chenyang Qi,Yue Ma*

Main category: cs.CV

TL;DR: MultiMotion is a novel framework for multi-object video motion transfer using Diffusion Transformers, featuring mask-aware attention motion flow and efficient sampling methods.


<details>
  <summary>Details</summary>
Motivation: Current Diffusion Transformer architectures struggle with multi-object video motion transfer due to motion entanglement and lack of object-level control, limiting precise manipulation of multiple objects in videos.

Method: Introduces Mask-aware Attention Motion Flow (AMF) using SAM2 masks to disentangle and control motion features for multiple objects within DiT, plus RectPC (high-order predictor-corrector solver) for efficient sampling.

Result: Achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects while maintaining DiT's high quality and scalability. Also creates the first benchmark dataset for DiT-based multi-object motion transfer.

Conclusion: MultiMotion successfully overcomes limitations of existing DiT architectures for multi-object motion transfer through explicit motion disentanglement and efficient sampling, enabling better control over multiple objects in video generation.

Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.

</details>


### [258] [SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation](https://arxiv.org/abs/2512.07503)
*Yao Teng,Zhihuan Jiang,Han Shi,Xian Liu,Xuefei Ning,Guohao Dai,Yu Wang,Zhenguo Li,Xihui Liu*

Main category: cs.CV

TL;DR: SJD++ accelerates autoregressive text-to-image generation by 2-7 through parallel multi-token prediction with speculative verification, without retraining.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models produce high-quality images but are slow due to sequential token-by-token generation requiring hundreds to thousands of forward passes.

Method: SJD++ combines Jacobi decoding's iterative multi-token prediction with speculative sampling's probabilistic drafting-and-verification, plus reuses high-confidence draft tokens after verification instead of resampling all.

Result: Achieves 2-3 inference latency reduction and 2-7 step compression across multiple autoregressive text-to-image models, with no observable visual quality degradation.

Conclusion: SJD++ provides a training-free solution to significantly accelerate autoregressive image generation while maintaining output quality, making these models more practical for real-world applications.

Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

</details>


### [259] [ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points](https://arxiv.org/abs/2512.07504)
*Ryota Okumura,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: ControlVP is a user-guided framework that corrects vanishing point inconsistencies in text-to-image generated scenes, improving geometric realism while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models like Stable Diffusion often produce geometrically inconsistent scenes with vanishing point errors, undermining structural realism, especially in architectural images.

Method: Extends pre-trained diffusion models by incorporating structural guidance from building contours and adding geometric constraints that align image edges with perspective cues.

Result: The method enhances global geometric consistency while maintaining visual fidelity comparable to baseline models, particularly valuable for applications requiring accurate spatial structure.

Conclusion: ControlVP successfully addresses vanishing point inconsistencies in generated images, improving structural realism for applications like image-to-3D reconstruction.

Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .

</details>


### [260] [MeshRipple: Structured Autoregressive Generation of Artist-Meshes](https://arxiv.org/abs/2512.07514)
*Junkai Lin,Hang Long,Huipeng Guo,Jielei Zhang,JiaYi Yang,Tianle Guo,Yang Yang,Jianwen Li,Wenxiao Zhang,Matthias Niener,Wei Yang*

Main category: cs.CV

TL;DR: MeshRipple: A novel mesh generation method that expands meshes from a frontier like ripples on water, addressing long-range dependency issues in autoregressive mesh generation.


<details>
  <summary>Details</summary>
Motivation: Autoregressive mesh generators face critical limitations due to memory constraints requiring truncated training segments and sliding-window inference, which breaks long-range geometric dependencies and causes holes/fragmented components in generated meshes.

Method: Three key innovations: 1) Frontier-aware BFS tokenization aligning generation order with surface topology, 2) Expansive prediction strategy for coherent connected surface growth, 3) Sparse-attention global memory providing effectively unbounded receptive field for long-range topological dependencies.

Result: MeshRipple generates meshes with high surface fidelity and topological completeness, outperforming strong recent baselines in mesh generation.

Conclusion: The integrated design of MeshRipple successfully addresses the critical limitation of long-range dependency breakdown in autoregressive mesh generation, enabling generation of complete, high-fidelity meshes through frontier-based expansion.

Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.

</details>


### [261] [From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images](https://arxiv.org/abs/2512.07527)
*Fei Yu,Yu Liu,Luyang Tang,Mingchao Sun,Zengye Ge,Rui Bu,Yuchao Jin,Haisen Zhao,He Sun,Yangyan Li,Mu Xu,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: City-scale 3D reconstruction from sparse satellite images using 2.5D height maps and texture restoration for extreme viewpoint extrapolation.


<details>
  <summary>Details</summary>
Motivation: City-scale 3D reconstruction from satellite imagery faces extreme viewpoint extrapolation challenges (nearly 90掳 viewpoint gaps) where current methods like NeRF and 3DGS fail due to sparse orbital images with minimal parallax, foreshortened facades, and flawed textures.

Method: 1) Model city geometry as 2.5D height map using Z-monotonic signed distance field (SDF) for stable optimization under sparse satellite views. 2) Paint mesh appearance via differentiable rendering and train generative texture restoration network to enhance degraded satellite inputs with high-frequency details.

Result: Successfully reconstructed 4km虏 real-world region from few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. Produces watertight meshes with crisp roofs and clean vertical facades.

Conclusion: The method provides scalable, robust urban reconstruction creating high-fidelity, application-ready assets for downstream tasks like urban planning and simulation, overcoming limitations of existing reconstruction engines for extreme viewpoint extrapolation.

Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.

</details>


### [262] [Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation](https://arxiv.org/abs/2512.07568)
*Xuecheng Li,Weikuan Jia,Alisher Kurbonaliev,Qurbonaliev Alisher,Khudzhamkulov Rustam,Ismoilov Shuhratjon,Eshmatov Javhariddin,Yuanjie Zheng*

Main category: cs.CV

TL;DR: DSRSD-Net disentangles modality-specific and shared information via residual decomposition and semantic decorrelation to address modality dominance and redundancy in multimodal learning.


<details>
  <summary>Details</summary>
Motivation: Multimodal representations suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. High-variance modalities overshadow weaker but semantically important signals, and naive fusion strategies entangle modality-shared and modality-specific factors uncontrollably.

Method: Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net) with three components: 1) dual-stream representation learning module separating intra-modal (private) and inter-modal (shared) latent factors via residual projection; 2) residual semantic alignment head mapping shared factors into common space using contrastive and regression objectives; 3) decorrelation and orthogonality loss regularizing covariance structure and enforcing orthogonality between shared and private streams.

Result: Experimental results on two large-scale educational benchmarks demonstrate consistent improvements in next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.

Conclusion: DSRSD-Net effectively addresses modality dominance and redundancy issues in multimodal learning through explicit disentanglement of shared and private factors, leading to better generalization and interpretability in educational prediction tasks.

Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na茂ve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.

</details>


### [263] [All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs](https://arxiv.org/abs/2512.07580)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Longzhen Yang,Yihang Liu,Chengmei Yang,Ying Wen,Xianfeng Tang,Hui Liu,Yuyin Zhou,Lianghua He*

Main category: cs.CV

TL;DR: Vision LLMs waste computation on redundant visual tokens in deep layers. The paper identifies "information horizon" where visual tokens become uniform and proposes random pruning in deep layers to maintain performance while cutting 50% of tokens.


<details>
  <summary>Details</summary>
Motivation: Vision LLMs use hundreds of visual tokens, causing high computational costs. Existing token pruning methods fail in deep layers, performing no better than random pruning beyond certain depths, indicating wasted computation on redundant tokens.

Method: Proposes measuring token information content by output probability change upon removal. Identifies "information horizon" where visual tokens become uniform. Uses random pruning in deep layers and integrates it with existing methods like DivPrune.

Result: Achieves state-of-the-art pruning: maintains 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. Random pruning in deep layers works effectively and enhances existing methods.

Conclusion: Visual tokens lose salience in deep layers, becoming redundant beyond an "information horizon." Simple random pruning in these layers efficiently balances performance and efficiency, with horizon depth varying by task and model capacity.

Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

</details>


### [264] [LongCat-Image Technical Report](https://arxiv.org/abs/2512.07584)
*Meituan LongCat Team,Hanghang Ma,Haoxian Tan,Jiale Huang,Junqiang Wu,Jun-Yan He,Lishuai Gao,Songlin Xiao,Xiaoming Wei,Xiaoqi Ma,Xunliang Cai,Yayong Guan,Jie Hu*

Main category: cs.CV

TL;DR: LongCat-Image is a bilingual Chinese-English foundation model for image generation that achieves SOTA in text rendering, photorealism, and efficiency with only 6B parameters, while providing comprehensive open-source ecosystem.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility in current image generation models, particularly for Chinese character rendering.

Method: Rigorous data curation across pre-training, mid-training, and SFT stages, complemented by curated reward models during RL phase. Compact 6B parameter diffusion model design for efficiency.

Result: Achieves SOTA in text rendering capabilities and photorealism, sets new industry standard for Chinese character rendering (including complex/rare characters), and achieves superior editing consistency. Remarkably efficient with minimal VRAM usage and rapid inference.

Conclusion: LongCat-Image establishes a comprehensive open-source ecosystem with multiple model versions and training toolchain, providing robust support for developers and researchers to advance visual content creation.

Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.

</details>


### [265] [Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation](https://arxiv.org/abs/2512.07590)
*Kaili Qi,Zhongyi Huang,Wenli Yang*

Main category: cs.CV

TL;DR: Robust VM_TUNet integrates variational PDEs with deep learning for noisy image segmentation, combining physical priors with neural networks for better boundary handling.


<details>
  <summary>Details</summary>
Motivation: To address challenges in segmenting noisy images with blurred or fragmented boundaries by combining the interpretability and boundary-smoothing advantages of variational PDEs with the strong representational ability of deep neural networks.

Method: Proposes a robust VM_TUNet framework that incorporates physical priors, edge detector, and mean curvature term into a modified Cahn-Hilliard equation. Uses two collaborative modules: F module for frequency domain preprocessing to alleviate poor local minima, and T module for accurate stable local computations with stability estimate.

Result: Achieves balanced trade-off between performance and computational efficiency, yielding competitive quantitative results and improved visual quality compared to pure CNN-based models, while achieving performance close to transformer-based methods with reasonable computational expense.

Conclusion: The hybrid variational-deep learning approach effectively addresses noisy image segmentation challenges by combining physical priors with neural networks, offering improved boundary handling and computational efficiency.

Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.

</details>


### [266] [More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery](https://arxiv.org/abs/2512.07596)
*Wenzhen Dong,Jieming Yu,Yiming Huang,Hongqiu Wang,Lei Zhu,Albert C. S. Chung,Hongliang Ren,Long Bai*

Main category: cs.CV

TL;DR: SAM 3 shows improved zero-shot segmentation with point/box prompts and introduces language-based segmentation, but language prompts underperform in surgery. It demonstrates strong 3D reconstruction from 2D surgical images but has limitations in complex dynamic scenes.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of the newly released SAM 3 model in robot-assisted surgery, assessing its zero-shot segmentation capabilities (including new language prompts) and 3D reconstruction abilities in surgical contexts.

Method: Empirical evaluation of SAM 3 on surgical benchmarks including MICCAI EndoVis 2017/2018 for segmentation, and SCARED, StereoMIS, EndoNeRF for 3D reconstruction. Testing includes point/box prompts, language prompts, and dynamic video tracking.

Result: SAM 3 outperforms SAM and SAM 2 in image/video segmentation with spatial prompts. Language prompts show potential but underperform in surgery. Strong monocular depth estimation and 3D instrument reconstruction, but limitations in complex dynamic surgical scenes.

Conclusion: SAM 3 represents significant advancement with improved segmentation and 3D capabilities, but domain-specific training is needed for language prompts, and challenges remain in highly dynamic surgical environments.

Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.

</details>


### [267] [Online Segment Any 3D Thing as Instance Tracking](https://arxiv.org/abs/2512.07599)
*Hanshi Wang,Zijian Cai,Jin Gao,Yiwei Zhang,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: AutoSeg3D rethinks online 3D segmentation as instance tracking, using object queries for temporal propagation to enhance embodied agents' environmental perception.


<details>
  <summary>Details</summary>
Motivation: Current query-based 3D segmentation methods lack temporal understanding, which is crucial for embodied agents operating in dynamic environments. Viewpoint variations in robotics often cause partial object visibility across frames, requiring holistic object understanding beyond instantaneous views.

Method: Reconceptualizes 3D segmentation as instance tracking using object queries for temporal propagation. Long-term instance association maintains feature/identity coherence, while short-term instance update enriches instant observations. Introduces spatial consistency learning to mitigate VFM fragmentation and enhance temporal learning.

Result: Achieves new SOTA, surpassing ESAM by 2.8 AP on ScanNet200, with consistent gains on ScanNet, SceneNN, and 3RScan datasets. Temporal propagation enhances spatial comprehension while avoiding computational burden of dense temporal point cloud interactions.

Conclusion: AutoSeg3D successfully integrates temporal understanding into 3D segmentation through instance tracking paradigm, enabling embodied agents to develop holistic object understanding despite partial visibility across frames, while maintaining computational efficiency.

Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

</details>


### [268] [Decomposition Sampling for Efficient Region Annotations in Active Learning](https://arxiv.org/abs/2512.07606)
*Jingna Qiu,Frauke Wilm,Mathias ttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: DECOMP is a new active learning strategy for dense prediction tasks that decomposes images into class-specific components using pseudo-labels and samples regions from each class, improving annotation efficiency and minority-class performance.


<details>
  <summary>Details</summary>
Motivation: Dense prediction tasks in medical imaging require costly region-level annotations. Existing active learning methods for representative region selection have limitations: high computational/memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling.

Method: DECOMP decomposes images into class-specific components using pseudo-labels, then samples regions from each class. Class-wise predictive confidence guides the sampling process to ensure difficult classes receive additional annotations.

Result: DECOMP consistently surpasses baseline methods across ROI classification, 2-D segmentation, and 3-D segmentation tasks by better sampling minority-class regions and boosting performance on challenging classes.

Conclusion: DECOMP addresses limitations of existing active learning methods for dense prediction by enhancing annotation diversity through class decomposition and confidence-guided sampling, improving efficiency and minority-class performance.

Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.

</details>


### [269] [MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation](https://arxiv.org/abs/2512.07628)
*Zhiqi Li,Wenhuan Li,Tengfei Wang,Zhenwei Wang,Junta Wu,Haoyuan Wang,Yunhan Yang,Zehuan Huang,Yang Li,Peidong Liu,Chunchao Guo*

Main category: cs.CV

TL;DR: MoCA introduces an efficient compositional 3D generative model that uses importance-based component routing and compression to enable scalable part-aware generation without quadratic attention costs.


<details>
  <summary>Details</summary>
Motivation: Existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components, limiting their practical application for fine-grained compositional asset creation.

Method: Two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserves contextual priors of unselected components while reducing computational complexity.

Result: Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks, enabling efficient, fine-grained compositional 3D asset creation with scalable number of components.

Conclusion: MoCA presents an effective solution to the scalability problem in compositional 3D generation, making part-aware generation practical for complex 3D assets with many components.

Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA

</details>


### [270] [Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method](https://arxiv.org/abs/2512.07651)
*Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang*

Main category: cs.CV

TL;DR: LiQA dataset for liver fibrosis staging with MRI scans, featuring segmentation and staging tasks under real-world challenges like domain shifts and missing data.


<details>
  <summary>Details</summary>
Motivation: Liver fibrosis is a major global health issue requiring accurate staging for clinical management. Current methods need to handle complex real-world conditions including domain shifts, missing modalities, and spatial misalignment in multi-center MRI data.

Method: Created LiQA dataset with 440 patients' multi-phase, multi-center MRI scans for benchmarking Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS). Top-performing approach uses semi-supervised learning with external data for segmentation, and multi-view consensus with CAM-based regularization for staging.

Result: Dataset established as part of CARE 2024 challenge. Baseline evaluation shows that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

Conclusion: The LiQA dataset provides a valuable benchmark for liver fibrosis algorithms, and the proposed methodology demonstrates that multi-source data integration with anatomical constraints improves clinical applicability under challenging real-world conditions.

Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

</details>


### [271] [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652)
*Hamad Almazrouei,Mariam Al Nasseri,Maha Alzaabi*

Main category: cs.CV

TL;DR: AI-powered AUV system combines YOLOv12 Nano for object detection, ResNet50 CNN for feature extraction, PCA for dimensionality reduction, K-Means++ clustering for grouping, and GPT-4o Mini LLM for report generation to automate underwater exploration and analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional sea exploration faces challenges from extreme conditions, limited visibility, high costs, and human risks, leaving vast ocean regions unexplored. There's a need for automated systems to overcome these limitations and enhance underwater data collection and analysis.

Method: Integrated AI system with YOLOv12 Nano for real-time object detection, ResNet50 CNN for feature extraction, PCA for dimensionality reduction (preserving 98% variance), K-Means++ clustering for grouping marine objects, and GPT-4o Mini LLM for generating structured reports. Trained on combined dataset of 55,000+ images from DeepFish and OzFish datasets.

Result: System achieved mAP@0.5 of 0.512, precision of 0.535, and recall of 0.438 for marine object detection. PCA effectively reduced feature dimensionality while preserving variance. K-Means clustering successfully grouped objects based on visual similarities. LLM generated insightful summaries of detections and clusters with location data.

Conclusion: The integrated AI system significantly reduces human diving risks, increases mission efficiency, and enhances underwater data analysis speed and depth. This approach enables more effective scientific research and discovery in challenging marine environments.

Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.

</details>


### [272] [Optimization-Guided Diffusion for Interactive Scene Generation](https://arxiv.org/abs/2512.07661)
*Shiaho Li,Naisheng Ye,Tianyu Li,Kashyap Chitta,Tuo An,Peng Su,Boyang Wang,Haiou Liu,Chen Lv,Hongyang Li*

Main category: cs.CV

TL;DR: OMEGA is an optimization-guided framework that improves diffusion-based multi-agent scene generation by enforcing physical and social constraints during sampling, enabling realistic safety-critical scenario generation.


<details>
  <summary>Details</summary>
Motivation: Safety-critical events are rare in real driving datasets but essential for autonomous vehicle evaluation. Existing data-driven scene generation models lack controllability and often produce physically/socially implausible scenes.

Method: OMEGA uses constrained optimization to re-anchor each reverse diffusion step, steering generation toward physically plausible and behaviorally coherent trajectories. It formulates ego-attacker interactions as game-theoretic optimization in distribution space to approximate Nash equilibria for adversarial scenarios.

Result: OMEGA improves generation realism, consistency, and controllability: increases valid scenes from 32.35% to 72.27% for free exploration, and from 11% to 80% for controllability-focused generation. Generates 5 more near-collision frames with TTC < 3s while maintaining scene realism.

Conclusion: OMEGA provides an effective optimization-guided framework for generating realistic, safety-critical multi-agent driving scenes with improved physical/social consistency and controllability compared to existing methods.

Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.

</details>


### [273] [EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset](https://arxiv.org/abs/2512.07668)
*Ronan John,Aditya Kesari,Vincenzo DiMatteo,Kristin Dana*

Main category: cs.CV

TL;DR: EgoCampus dataset and EgoCampusNet model for predicting pedestrian eye gaze during outdoor navigation on a university campus.


<details>
  <summary>Details</summary>
Motivation: To address the gap in predicting human visual attention during real-world outdoor navigation, as prior egocentric datasets focus on indoor tasks or lack eye gaze information.

Method: Collected EgoCampus dataset using Meta's Project Aria glasses with eye tracking, RGB cameras, inertial sensors, and GPS from 80+ pedestrians across 25 outdoor campus paths (6 km total). Developed EgoCampusNet model to predict eye gaze of navigating pedestrians.

Result: Created a diverse gaze-annotated video dataset of outdoor pedestrian navigation and developed a novel gaze prediction method for outdoor environments.

Conclusion: Provides a new resource for studying real-world attention and a foundation for future gaze prediction models in navigation contexts, with dataset and code to be publicly released.

Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

</details>


### [274] [DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations](https://arxiv.org/abs/2512.07674)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: DIST-CLIP is a novel MRI harmonization framework that disentangles anatomical content from image contrast using CLIP guidance, enabling flexible style transfer with either target images or DICOM metadata to address real-world clinical data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Deep learning for medical imaging faces clinical generalization challenges due to data heterogeneity, especially in MRI where scanner differences, acquisition protocols, and sequence parameters cause domain shifts that obscure biological signals. Existing harmonization methods are insufficient - image-based approaches need target images, while text-guided methods use simplistic labels or work only on limited datasets.

Method: DIST-CLIP disentangles anatomical content from image contrast, extracting contrast representations using pre-trained CLIP encoders. These embeddings are integrated into anatomical content via a novel Adaptive Style Transfer module. The framework flexibly uses either target images or DICOM metadata for guidance.

Result: The method was trained and evaluated on diverse real-world clinical datasets, showing significant improvements over state-of-the-art methods in both style translation fidelity and anatomical preservation.

Conclusion: DIST-CLIP offers a flexible solution for MRI style transfer and standardization, addressing real-world clinical heterogeneity by enabling guidance from either images or metadata, with publicly available code and weights.

Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.

</details>


### [275] [sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only](https://arxiv.org/abs/2512.07698)
*Arslan Artykov,Corentin Sautier,Vincent Lepetit*

Main category: cs.CV

TL;DR: First data-driven method to jointly predict part segmentation and joint parameters from monocular video using synthetic training only, with strong real-world generalization.


<details>
  <summary>Details</summary>
Motivation: Previous work on articulated object understanding has focused on constrained setups like multi-view systems, object scanning, or static cameras, lacking practical solutions for freely moving monocular video capture.

Method: Data-driven approach trained solely on synthetic data that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera.

Result: Demonstrates strong generalization to real-world objects despite being trained only on synthetic data, offering scalable and practical solution for articulated object understanding.

Conclusion: The method enables articulated object understanding from casually recorded video, making it suitable for real-time applications in dynamic environments for robotics and digital twin creation.

Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/

</details>


### [276] [Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment](https://arxiv.org/abs/2512.07702)
*Sangha Park,Eunji Kim,Yeongtak Oh,Jooyoung Choi,Sungroh Yoon*

Main category: cs.CV

TL;DR: NPC is an automated pipeline that improves text-to-image alignment by identifying and applying negative prompts to suppress unintended content in generated images.


<details>
  <summary>Details</summary>
Motivation: Despite progress in text-to-image generation, achieving precise alignment remains challenging for prompts with rich compositional structure or imaginative elements, requiring better methods to suppress unintended content.

Method: NPC analyzes cross-attention patterns to understand why both targeted and untargeted negative prompts work, then uses a verifier-captioner-proposer framework to generate candidate negative prompts and ranks them with a salient text-space score without requiring additional image synthesis.

Result: NPC outperforms strong baselines on GenEval++ (0.571 vs. 0.371) and achieves best overall performance on Imagine-Bench, demonstrating improved text-image alignment.

Conclusion: By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models.

Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.

</details>


### [277] [PVeRA: Probabilistic Vector-Based Random Matrix Adaptation](https://arxiv.org/abs/2512.07703)
*Leo Fillioux,Enzo Ferrante,Paul-Henry Courn猫de,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: PVeRA is a probabilistic version of the VeRA adapter that modifies low-rank matrices probabilistically to handle input ambiguities and enable different sampling configurations during training/testing, outperforming other adapters on VTAB-1k benchmark.


<details>
  <summary>Details</summary>
Motivation: Large foundation models require vast datasets and computational resources for training/finetuning, which are often scarce and costly. Adaptation methods provide computationally efficient solutions by allowing models to be finetuned with minimal data and computing power.

Method: PVeRA modifies the VeRA adapter's low-rank matrices in a probabilistic manner. VeRA uses frozen random low-rank matrices shared across all layers for parameter-efficient adaptation. PVeRA introduces probabilistic modifications to these matrices to handle input ambiguities and enable different sampling configurations during training and testing.

Result: Comprehensive evaluation on VTAB-1k benchmark with seven adapters shows PVeRA outperforms VeRA and other adapters.

Conclusion: PVeRA provides an effective probabilistic adaptation method that handles input ambiguities and offers flexible sampling configurations, achieving superior performance on benchmark tasks while maintaining parameter efficiency.

Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.

</details>


### [278] [UnCageNet: Tracking and Pose Estimation of Caged Animal](https://arxiv.org/abs/2512.07712)
*Sayak Dutta,Harish Katti,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: A three-stage preprocessing pipeline (cage segmentation, inpainting, evaluation) improves animal tracking/pose estimation by removing cage occlusions that cause performance drops in systems like STEP and ViTPose.


<details>
  <summary>Details</summary>
Motivation: Animal tracking and pose estimation systems suffer substantial performance degradation when processing images/videos with cage structures and systematic occlusions, which interfere with accurate detection and tracking.

Method: Three-stage pipeline: (1) Cage segmentation using Gabor-enhanced ResNet-UNet with 72 directional kernels for orientation-aware feature detection, (2) Cage inpainting using CRFill for content-aware reconstruction of occluded regions, (3) Evaluation of pose estimation and tracking on the processed uncaged frames.

Result: Removing cage occlusions enables pose estimation and tracking performance comparable to environments without occlusions, with significant improvements in keypoint detection accuracy and trajectory consistency.

Conclusion: The proposed preprocessing pipeline effectively addresses cage occlusion problems in animal tracking systems, restoring performance to levels similar to occlusion-free environments through specialized segmentation and inpainting techniques.

Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.

</details>


### [279] [ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation](https://arxiv.org/abs/2512.07720)
*Fan Yang,Heyuan Li,Peihao Li,Weihao Yuan,Lingteng Qiu,Chaoyue Song,Cheng Chen,Yisheng He,Shifeng Zhang,Xiaoguang Han,Steven Hoi,Guosheng Lin*

Main category: cs.CV

TL;DR: A novel framework that combines 3D reconstruction models with video diffusion models to generate high-fidelity upper-body 3D avatars from single images, achieving photorealistic details and fluid motion while maintaining structural stability.


<details>
  <summary>Details</summary>
Motivation: Current 3D avatar generation methods produce blurry textures and stiff motion, while video models suffer from structural errors and identity drift. There's a need to combine the strengths of both approaches for high-fidelity, dynamic avatars.

Method: Uses a 3D reconstruction model to provide structural and appearance priors, which guides a real-time autoregressive video diffusion model for rendering. This combines geometric stability with generative capabilities for photorealistic details and fluid dynamics.

Result: Significantly reduces artifacts, achieves substantial improvements in visual quality over leading methods, and enables real-time synthesis of high-frequency details and fluid motion while preventing structural inconsistencies.

Conclusion: The proposed method successfully unites 3D reconstruction stability with video generation quality, producing high-fidelity digital avatars with realistic appearance and dynamic motion for real-time applications like gaming and VR.

Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

</details>


### [280] [Improving action classification with brain-inspired deep networks](https://arxiv.org/abs/2512.07729)
*Aidas Aglinskas,Stefano Anzellotti*

Main category: cs.CV

TL;DR: Deep neural networks rely heavily on background context for action recognition while ignoring body information, unlike humans who use both. A brain-inspired dual-stream architecture improves performance and matches human patterns.


<details>
  <summary>Details</summary>
Motivation: To understand how DNNs use body vs. background information for action recognition compared to humans, and to test whether brain-inspired architectures with domain-specific streams can achieve more human-like performance.

Method: 1) Tested DNNs on HAA500 dataset with three stimulus versions (full, body-only, background-only). 2) Compared with human participants (N=28) on same stimuli. 3) Implemented novel brain-inspired architecture with separate body and background processing streams.

Result: DNNs performed well on full and background-only stimuli but failed on body-only stimuli (chance-level). Humans performed well on all three versions, better on body-only than background-only. The brain-inspired dual-stream architecture improved action recognition and matched human accuracy patterns across stimulus types.

Conclusion: DNNs over-rely on background context while ignoring body information, unlike humans who effectively use both. Brain-inspired domain-specific architectures can improve performance and achieve more human-like action recognition patterns.

Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.

</details>


### [281] [SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination](https://arxiv.org/abs/2512.07730)
*Sangha Park,Seungryong Yoo,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: SAVE is a training-free framework that uses Sparse Autoencoder features to reduce object hallucination in MLLMs by identifying and steering along visual understanding features.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models (MLLMs) suffer from object hallucination due to language priors and visual information loss, which undermines their reliability and grounded understanding.

Method: SAVE uses a binary object-presence QA probe to identify SAE features most indicative of visual information processing (visual understanding features), then steers the model along these features to reinforce grounded visual understanding.

Result: SAVE outperforms state-of-the-art training-free methods, achieving 10%p improvement in CHAIR_S and consistent gains on POPE and MMHal-Bench. It suppresses uncertain object tokens and increases attention to image tokens.

Conclusion: Steering along SAE-derived visual understanding features effectively mitigates hallucination in MLLMs, demonstrating robustness and generalizability across multiple models and layers.

Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.

</details>


### [282] [SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery](https://arxiv.org/abs/2512.07733)
*Meng Cao,Xingyu Li,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: SpatialDreamer is a reinforcement learning framework that enables MLLMs to perform complex spatial reasoning through active mental simulation, using a world model for visual imagination and geometric policy optimization for reward supervision.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs have limited performance on complex spatial reasoning tasks requiring mental simulation, as they rely on passive observation rather than active mental imagery processes.

Method: Proposes SpatialDreamer: a reinforcement learning framework with closed-loop active exploration, visual imagination via a world model, and evidence-grounded reasoning. Introduces Geometric Policy Optimization (GeoPO) with tree-structured sampling and step-level reward estimation with geometric consistency constraints.

Result: Extensive experiments show SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, representing a critical advancement in human-like active spatial mental simulation for MLLMs.

Conclusion: SpatialDreamer successfully bridges the gap in MLLMs' spatial reasoning capabilities by enabling active mental simulation through reinforcement learning and geometric policy optimization.

Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.

</details>


### [283] [HLTCOE Evaluation Team at TREC 2025: VQA Track](https://arxiv.org/abs/2512.07738)
*Dengjia Zhang,Charles Weng,Katherine Guerrerio,Yi Lu,Kenton Murray,Alexander Martin,Reno Kriz,Benjamin Van Durme*

Main category: cs.CV

TL;DR: The paper presents a listwise learning framework for video question answering that reranks candidate answers using a novel Masked Pointer Cross-Entropy Loss with Rank Weights to improve semantic precision and ranking consistency.


<details>
  <summary>Details</summary>
Motivation: To improve semantic precision and ranking consistency in video question answering, particularly for questions requiring temporal reasoning and semantic disambiguation, by bridging generative modeling with discriminative ranking.

Method: A two-stage approach: 1) A base multimodal model generates multiple candidate answers for video-question pairs, 2) A reranking model trained with novel Masked Pointer Cross-Entropy Loss with Rank Weights that integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction.

Result: Experiments show consistent gains in accuracy and ranking stability, especially for temporal reasoning and semantic disambiguation questions, demonstrating improved semantic precision and ranking consistency.

Conclusion: The proposed listwise learning framework successfully bridges generative and discriminative approaches, producing coherent, fine-grained answer lists with improved accuracy and ranking stability for video question answering.

Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.

</details>


### [284] [DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07745)
*Jialv Zou,Shaoyu Chen,Bencheng Liao,Zhiyu Zheng,Yuehao Song,Lefei Zhang,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DiffusionDriveV2 improves autonomous driving trajectory generation by combining reinforcement learning with diffusion models to overcome mode collapse and achieve better diversity-quality trade-off.


<details>
  <summary>Details</summary>
Motivation: Existing generative diffusion models for autonomous driving suffer from mode collapse, producing conservative and homogeneous behaviors. While DiffusionDrive uses predefined anchors for diversity, its imitation learning approach lacks constraints, creating a dilemma between diversity and consistent high quality.

Method: Uses reinforcement learning to constrain low-quality modes and explore superior trajectories while preserving multimodality. Features: 1) scale-adaptive multiplicative noise for broad exploration in trajectory planning, 2) intra-anchor GRPO for advantage estimation within single anchors, and 3) inter-anchor truncated GRPO for global perspective across different anchors to prevent improper advantage comparisons between distinct intentions.

Result: Achieves 91.2 PDMS on NAVSIM v1 and 85.5 EPDMS on NAVSIM v2 datasets in closed-loop evaluation with ResNet-34 backbone, setting new records. Resolves diversity-quality dilemma for truncated diffusion models and achieves best trade-off.

Conclusion: DiffusionDriveV2 successfully enhances autonomous driving trajectory generation quality while preserving diversity through reinforcement learning constraints and exploration techniques, overcoming the limitations of previous diffusion-based approaches.

Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2

</details>


### [285] [Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747)
*Shihao Zhao,Yitong Chen,Zeyinzi Jiang,Bojia Zi,Shaozhe Hao,Yu Liu,Chaojie Mao,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: Unison is a low-cost multimodal AI that automatically identifies tasks and extracts parameters for unified understanding and generation across text, image, and video tasks.


<details>
  <summary>Details</summary>
Motivation: Current approaches for unified multimodal understanding and generation either require massive resources (auto-regressive transformers) or suffer from limited task coverage and poor quality (two-stage methods). Both lack automatic parameter parsing and require manual configuration.

Method: Unison adopts a two-stage scheme that preserves pre-trained model capabilities while adding automatic intention parsing. It can identify task types and extract required meta-information (resolution, duration, etc.) without human intervention.

Result: With only 500k training samples and 50 GPU hours, Unison achieves superior performance across diverse multimodal tasks including text/image/video understanding, content generation, editing, controllable generation, and IP-based reference generation.

Conclusion: Unison demonstrates that low-cost training can enable comprehensive multimodal AI with automatic task identification and parameter extraction, making unified understanding and generation accessible to ordinary researchers.

Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

</details>


### [286] [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](https://arxiv.org/abs/2512.07756)
*Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig*

Main category: cs.CV

TL;DR: UltrasODM is a dual-stream framework that assists sonographers during ultrasound acquisition by providing calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts to reduce reconstruction errors.


<details>
  <summary>Details</summary>
Motivation: Clinical ultrasound acquisition is highly operator-dependent, with rapid probe motion and brightness fluctuations causing reconstruction errors that reduce trust and clinical utility.

Method: UltrasODM integrates: (1) contrastive ranking module for grouping frames by motion similarity, (2) optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (3) Human-in-the-Loop layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting low-confidence regions.

Result: Evaluated on clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs.

Conclusion: By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows.

Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.

</details>


### [287] [Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.07760)
*Menglin Wang,Xiaojin Gong,Jiachen Li,Genlin Ji*

Main category: cs.CV

TL;DR: Proposes a novel unsupervised visible-infrared person re-identification method using modality-aware Jaccard distance for bias-mitigated global association and split-and-contrast strategy for modality-invariant representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing USVI-ReID methods using optimal transport for cross-modality association are prone to propagating local cluster errors and overlook global instance-level relations. The significant modality gap between visible and infrared makes reliable cross-modality association challenging.

Method: 1) Modality-aware Jaccard distance to mitigate distance bias caused by modality discrepancy, enabling reliable cross-modality associations through global clustering. 2) Split-and-contrast strategy to obtain modality-specific global prototypes and align them under global association guidance for modality-invariant representation learning.

Result: Achieves state-of-the-art performance on benchmark VI-ReID datasets, outperforming existing methods by a significant margin.

Conclusion: The proposed approach effectively addresses cross-modality learning through bias-mitigated global association and modality-invariant representation learning, demonstrating superior performance despite conceptual simplicity.

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.

</details>


### [288] [GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring](https://arxiv.org/abs/2512.07776)
*Maximilian Schall,Felix Leonard Kn枚fel,Noah Elias K枚nig,Jan Jonas Kubeler,Maximilian von Klinski,Joan Wilhelm Linnemann,Xiaoshi Liu,Iven Jelle Schlegelmilch,Ole Woyciniuk,Alexandra Schild,Dante Wasmuht,Magdalena Bermejo Espinet,German Illera Basas,Gerard de Melo*

Main category: cs.CV

TL;DR: This paper introduces GorillaWatch, an end-to-end pipeline for automated re-identification of critically endangered western lowland gorillas using camera trap footage, addressing the lack of large-scale wild datasets through three novel benchmarks and innovative methods.


<details>
  <summary>Details</summary>
Motivation: Current monitoring of critically endangered western lowland gorillas requires immense manual effort to re-identify individuals from vast camera trap archives. The primary obstacle to automation has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models.

Method: The authors introduce three novel datasets: Gorilla-SPAC-Wild (largest wild primate re-ID dataset), Gorilla-Berlin-Zoo (for cross-domain generalization), and Gorilla-SPAC-MoT (for multi-object tracking). They present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. Key innovations include: 1) multi-frame self-supervised pretraining using tracklet consistency, 2) differentiable AttnLRP adaptation for scientific validation, 3) feature aggregation from large-scale image backbones, and 4) unsupervised population counting with spatiotemporal constraints.

Result: Extensive benchmarking demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. The self-supervised pretraining strategy effectively learns domain-specific features without manual labels. The AttnLRP verification confirms the model relies on discriminative biometric traits rather than background correlations. The unsupervised population counting approach mitigates over-segmentation through spatiotemporal constraints.

Conclusion: GorillaWatch provides a scalable, non-invasive monitoring solution for endangered species by addressing key challenges in automated re-identification. The publicly released code and datasets facilitate broader adoption and further research in wildlife conservation technology, potentially transforming how critically endangered species are monitored and protected.

Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species

</details>


### [289] [Distribution Matching Variational AutoEncoder](https://arxiv.org/abs/2512.07778)
*Sen Ye,Jianning Pei,Mengde Xu,Shuyang Gu,Chunyu Wang,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: DMVAE introduces a distribution-matching constraint to align encoder latents with arbitrary reference distributions, enabling systematic exploration of optimal latent distributions for image generation.


<details>
  <summary>Details</summary>
Motivation: Existing visual generative models (VAEs, foundation model encoders) implicitly constrain latent spaces without explicitly shaping their distributions, making it unclear which distributions are optimal for modeling.

Method: DMVAE explicitly aligns the encoder's latent distribution with arbitrary reference distributions via a distribution matching constraint, generalizing beyond Gaussian priors to distributions from self-supervised features, diffusion noise, or other priors.

Result: SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, achieving gFID=3.2 on ImageNet with only 64 training epochs.

Conclusion: Choosing suitable latent distribution structures through distribution-level alignment, rather than fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis.

Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.

</details>


### [290] [OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory](https://arxiv.org/abs/2512.07802)
*Zhaochong An,Menglin Jia,Haonan Qiu,Zijian Zhou,Xiaoke Huang,Zhiheng Liu,Weiming Ren,Kumara Kahatapitiya,Ding Liu,Sen He,Chenyang Zhang,Tao Xiang,Fanny Yang,Serge Belongie,Tian Xie*

Main category: cs.CV

TL;DR: OneStory enables consistent multi-shot video generation by reformulating it as next-shot generation with global cross-shot context modeling, achieving state-of-the-art narrative coherence.


<details>
  <summary>Details</summary>
Motivation: Existing multi-shot video generation methods struggle with long-range cross-shot context modeling, relying on limited temporal windows or single keyframe conditioning, which degrades performance under complex narratives.

Method: Reformulates MSV as next-shot generation task with autoregressive shot synthesis using pretrained I2V models. Introduces Frame Selection module for semantically-relevant global memory and Adaptive Conditioner for importance-guided patchification to generate compact context.

Result: Achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

Conclusion: OneStory provides an effective solution for consistent multi-shot video generation through global cross-shot context modeling, addressing limitations of existing methods and enabling better narrative storytelling in videos.

Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

</details>


### [291] [Multi-view Pyramid Transformer: Look Coarser to See Broader](https://arxiv.org/abs/2512.07806)
*Gyeongjin Kang,Seungkwon Yang,Seungtae Nam,Younggeun Lee,Jungwoo Kim,Eunbyung Park*

Main category: cs.CV

TL;DR: MVP is a scalable multi-view transformer that reconstructs large 3D scenes from tens to hundreds of images in one forward pass using dual hierarchies for efficiency and detail.


<details>
  <summary>Details</summary>
Motivation: To enable fast, efficient reconstruction of large 3D scenes from many images while maintaining both computational efficiency and rich representation of details.

Method: Multi-view Pyramid Transformer with two core principles: 1) local-to-global inter-view hierarchy (local views  groups  full scene), and 2) fine-to-coarse intra-view hierarchy (detailed spatial representations  compact tokens). Combined with 3D Gaussian Splatting as 3D representation.

Result: Achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across diverse view configurations on various datasets.

Conclusion: MVP's dual hierarchy approach enables fast, scalable reconstruction of large complex scenes with both computational efficiency and representational richness.

Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.

</details>


### [292] [Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes](https://arxiv.org/abs/2512.07807)
*Shai Krakovsky,Gal Fiebelman,Sagie Benaim,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: A method for embedding language fields in 3D Gaussian representations using low-dimensional semantic bottleneck features and multi-resolution hash encoding, with regularization techniques to address semantic misalignment, achieving state-of-the-art performance on HolyScenes dataset.


<details>
  <summary>Details</summary>
Motivation: Language fields in 3D representations enable richer semantic understanding of spatial environments, linking geometry with descriptive meaning for intuitive human-computer interaction, scene querying/editing, and improved tasks like scene retrieval and navigation. However, existing feature distillation approaches struggle with massive Internet data due to semantic feature misalignment and memory/runtime inefficiency.

Method: 1) Introduces extremely low-dimensional semantic bottleneck features within 3D Gaussian representation, processed through multi-resolution feature-based hash encoder for efficiency. 2) Proposes Attenuated Downsampler module and several regularization techniques to address semantic misalignment of ground truth 2D features.

Result: The method surpasses existing approaches in both performance and efficiency when evaluated on the in-the-wild HolyScenes dataset.

Conclusion: The proposed approach effectively addresses challenges of semantic feature misalignment and computational inefficiency in learning language fields from massive Internet data, enabling practical deployment for large-scale scene understanding and interaction.

Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.

</details>


### [293] [WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling](https://arxiv.org/abs/2512.07821)
*Shaoheng Fang,Hanwen Jiang,Yunpeng Bai,Niloy J. Mitra,Qixing Huang*

Main category: cs.CV

TL;DR: WorldReel is a 4D video generator that produces spatio-temporally consistent videos with explicit 4D scene representations (pointmaps, camera trajectory, dense flow), trained on synthetic+real data for geometric fidelity and visual realism.


<details>
  <summary>Details</summary>
Motivation: Current video generators achieve photorealism but lack 3D consistency, creating fundamentally inconsistent scenes that don't maintain coherent geometry and appearance over time and viewpoints.

Method: WorldReel jointly generates RGB frames with explicit 4D scene representations including pointmaps, camera trajectory, and dense flow mapping. It's trained on a blend of synthetic data (providing precise 4D supervision for geometry, motion, and camera) and real videos (contributing visual diversity and realism).

Result: WorldReel sets new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving geometric consistency, motion coherence, and reducing view-time artifacts compared to competing methods.

Conclusion: WorldReel advances video generation toward 4D-consistent world modeling, enabling agents to render, interact, and reason about scenes through a single stable spatiotemporal representation.

Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.

</details>


### [294] [OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing](https://arxiv.org/abs/2512.07826)
*Haoyang He,Jie Wang,Jiangning Zhang,Zhucun Xue,Xingyuan Bu,Qiangpeng Yang,Shilei Wen,Lei Xie*

Main category: cs.CV

TL;DR: OpenVE-3M is a large-scale, high-quality instruction-based video editing dataset with 8 edit types, plus OpenVE-Bench benchmark and OpenVE-Edit model achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Large-scale, high-quality datasets for instruction-based video editing are scarce compared to image editing, creating a gap in the field that needs to be addressed.

Method: Created OpenVE-3M dataset with 8 edit types (6 spatially-aligned, 2 non-spatially-aligned) using a meticulously designed data pipeline with quality filtering. Built OpenVE-Bench benchmark with 431 video-edit pairs and 3 human-aligned metrics. Trained OpenVE-Edit, a 5B parameter model on the dataset.

Result: OpenVE-3M surpasses existing datasets in scale, diversity, instruction length, and quality. OpenVE-Edit model sets new SOTA on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline.

Conclusion: The work addresses the video editing dataset scarcity by providing comprehensive resources (dataset, benchmark, model) that advance instruction-based video editing research and establish new performance standards.

Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

</details>


### [295] [One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation](https://arxiv.org/abs/2512.07829)
*Yuan Gao,Chen Chen,Tianrong Chen,Jiatao Gu*

Main category: cs.CV

TL;DR: FAE (Feature Auto-Encoder) is a simple framework that adapts pre-trained visual representations into low-dimensional latents suitable for generative models using minimal architecture (single attention layer), achieving state-of-the-art image generation quality.


<details>
  <summary>Details</summary>
Motivation: There's a fundamental mismatch between understanding-oriented visual representations (which need high-dimensional features for diverse hypotheses) and generation-friendly latent spaces (which need low-dimensional, noise-preserving latents). Current approaches require complex objectives and architectures to bridge this gap.

Method: FAE uses two separate deep decoders: one trained to reconstruct the original feature space from compressed latents, and a second that takes these reconstructed features as input for image generation. This allows adaptation of pre-trained encoders (like DINO, SigLIP) into low-dimensional latents with minimal architecture (as little as single attention layer).

Result: FAE achieves strong performance across class-conditional and text-to-image benchmarks. On ImageNet 256x256, diffusion model with CFG attains FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, reaches state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

Conclusion: FAE provides a simple yet effective solution to adapt pre-trained visual representations for generative models, bridging the gap between understanding and generation with minimal architectural complexity while achieving state-of-the-art performance.

Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

</details>


### [296] [UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation](https://arxiv.org/abs/2512.07831)
*Jiehui Huang,Yuechen Zhang,Xu He,Yuan Gao,Zhi Cen,Bin Xia,Yan Zhou,Xin Tao,Pengfei Wan,Jiaya Jia*

Main category: cs.CV

TL;DR: UnityVideo: A unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation, skeletons, DensePose, optical flow, depth) to enhance cross-modal understanding and physical world constraints.


<details>
  <summary>Details</summary>
Motivation: Current video generation models are limited by single-modality conditioning, which restricts holistic world understanding due to insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation.

Method: Introduces two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with in-context learner for unified processing via modular parameters and contextual learning. Creates a large-scale unified dataset with 1.3M samples.

Result: UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. It achieves superior video quality, consistency, and improved alignment with physical world constraints compared to existing approaches.

Conclusion: UnityVideo provides a unified framework that addresses limitations of single-modality video generation by enabling joint learning across multiple modalities, leading to better world understanding and physical constraint alignment in synthesized videos.

Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

</details>


### [297] [Relational Visual Similarity](https://arxiv.org/abs/2512.07833)
*Thao Nguyen,Sicheng Mo,Krishna Kumar Singh,Yilin Wang,Jing Shi,Nicholas Kolkin,Eli Shechtman,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: A new vision-language model is developed to measure relational similarity between images, going beyond surface attributes to capture underlying relational structures that humans perceive but current models miss.


<details>
  <summary>Details</summary>
Motivation: Current visual similarity metrics (LPIPS, CLIP, DINO) only capture perceptual attribute similarity but fail to capture relational similarity - the ability to see how different objects share similar internal relationships or functions, which is a key aspect of human cognition.

Method: 1) Formulate relational image similarity as a measurable problem; 2) Curate a 114k image-caption dataset with anonymized captions describing relational logic rather than surface content; 3) Finetune a Vision-Language model on this dataset to measure relational similarity between images.

Result: Developed the first model that can measure relational similarity between images, connecting images by their underlying relational structure rather than visible appearance. Shows that existing image similarity models fail to capture relational similarity, revealing a critical gap in visual computing.

Conclusion: Relational similarity has important real-world applications and represents a fundamental aspect of human visual understanding that current AI models lack. The proposed approach provides a first step toward bridging this gap by developing models that can perceive relational structures in images.

Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.

</details>


### [298] [Voxify3D: Pixel Art Meets Volumetric Rendering](https://arxiv.org/abs/2512.07834)
*Yi-Chuan Huang,Jiewen Chan,Hao-Jen Chien,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Voxify3D is a differentiable two-stage framework that generates voxel art from 3D meshes by integrating orthographic pixel art supervision, patch-based CLIP alignment, and palette-constrained Gumbel-Softmax quantization.


<details>
  <summary>Details</summary>
Motivation: Automated voxel art generation from 3D meshes is challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve pixel-precise, palette-constrained aesthetics.

Method: A differentiable two-stage framework with three key components: (1) orthographic pixel art supervision for precise voxel-pixel alignment, (2) patch-based CLIP alignment for semantic preservation across discretization levels, and (3) palette-constrained Gumbel-Softmax quantization for differentiable optimization over discrete color spaces with controllable palette strategies.

Result: Superior performance with 37.12 CLIP-IQA score and 77.90% user preference across diverse characters, with controllable abstraction (2-8 colors, 20x-50x resolutions).

Conclusion: Voxify3D successfully addresses fundamental challenges in voxel art generation by enabling semantic preservation under extreme discretization, achieving pixel-art aesthetics through volumetric rendering, and providing end-to-end discrete optimization.

Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [299] [Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules](https://arxiv.org/abs/2512.06575)
*Fariza Dahes*

Main category: eess.IV

TL;DR: Validation of ConvNeXt Tiny with GAGM, SEVector, and FSL framework on mammography classification shows GAGM and SEVector improve feature discriminability and reduce false negatives, but FSL doesn't help in mammography. Extended evaluation with multi-metrics, Grad-CAM, and interactive dashboard.


<details>
  <summary>Details</summary>
Motivation: To validate and extend a recent medical image classification framework (ConvNeXt Tiny with GAGM, SEVector, and FSL) from Alzheimer MRI to mammography classification, and investigate its transposability across different medical imaging domains.

Method: Used Kaggle dataset combining INbreast, MIAS, and DDSM mammography collections. Compared baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enhanced with GAGM and SEVector modules. Conducted multi-metric evaluation (macro F1, recall variance, ROC/AUC), feature interpretability analysis with Grad-CAM, and developed interactive clinical dashboard.

Result: GAGM and SEVector modules effectively enhance feature discriminability and reduce false negatives for malignant cases. However, Feature Smoothing Loss (FSL) did not yield measurable improvements in mammography classification. The framework shows domain-specific limitations.

Conclusion: The original framework is partially transposable to mammography classification, with GAGM and SEVector being effective but FSL not. Future work should explore alternative approaches to improve intra-class compactness and inter-class separability, especially for malignant vs. benign distinction in mammography.

Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.

</details>


### [300] [Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation](https://arxiv.org/abs/2512.05992)
*Azeez Idris,Abdurahman Ali Mohammed,Samuel Fanijo*

Main category: eess.IV

TL;DR: Self-supervised contrastive learning benefits from strong data augmentation, but existing augmentations don't always improve medical image segmentation performance. The paper evaluates and identifies better augmentations.


<details>
  <summary>Details</summary>
Motivation: Strong data augmentation is crucial for self-supervised contrastive learning's success in downstream tasks like semantic segmentation, but existing augmentation techniques may not be optimal for medical images.

Method: The paper evaluates various data augmentation techniques for self-supervised contrastive learning, experimenting with different augmentation compositions to find ones that improve medical image segmentation performance.

Result: Surprisingly, existing data augmentations don't always improve performance for medical image segmentation. The authors identify alternative augmentations that provide better results.

Conclusion: Not all data augmentations are equally effective for medical image segmentation in self-supervised contrastive learning. Careful selection and testing of augmentation techniques is necessary for optimal performance in medical imaging applications.

Abstract: Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.

</details>


### [301] [Semantic Temporal Single-photon LiDAR](https://arxiv.org/abs/2512.06008)
*Fang Li,Tonglin Mu,Shuling Li,Junran Guo,Keyuan Li,Jianing Li,Ziyang Luo,Xiaodong Fan,Ye Chen,Yunfeng Liu,Hong Cai,Lip Ket Chin,Jinbei Zhang,Shihai Sun*

Main category: eess.IV

TL;DR: Semantic TSP-LiDAR with self-updating knowledge base enables adaptive target recognition in open-set scenarios, outperforming conventional methods under low SNR and short acquisition times.


<details>
  <summary>Details</summary>
Motivation: Existing TSP-LiDAR systems struggle with open-set scenarios (unknown targets) and perform poorly under low signal-to-noise ratio and short acquisition times with few photons.

Method: Propose semantic TSP-LiDAR based on self-updating semantic knowledge base (SKB), formulating target recognition as semantic communication. The SKB dynamically updates semantic features of newly encountered targets without extensive neural network retraining.

Result: Approach surpasses conventional methods under low SNR and limited acquisition time. Self-updating SKB achieves 89% accuracy on nine unknown target types vs 66% without updating mechanism in real-world experiments.

Conclusion: The framework enables adaptive and robust target recognition in complex dynamic environments, highlighting potential for practical TSP-LiDAR applications.

Abstract: Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.

</details>


### [302] [Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics](https://arxiv.org/abs/2512.07224)
*Tianyi Ren,Daniel Low,Pittra Jaengprajak,Juampablo Heras Rivera,Jacob Ruzevick,Mehmet Kurt*

Main category: eess.IV

TL;DR: The paper proposes using contrast-level Shapley values to explain medical image segmentation models, showing that higher-performing models align better with clinical rankings and have lower uncertainty.


<details>
  <summary>Details</summary>
Motivation: Despite deep learning's success in medical image segmentation, there's a critical need for explainability to ensure clinical acceptance. Current gradient-based techniques focus on influential regions, but a broader approach is needed to explain how model performance attributes importance to different imaging contrasts.

Method: The authors use contrast-level Shapley values to systematically perturb model inputs and assess feature importance. They apply this to four MRI contrasts across four model architectures using the BraTS 2024 dataset. They propose two metrics: agreement between model and clinical imaging rankings, and uncertainty quantified through Shapley ranking variance across cross-validation folds.

Result: Higher-performing cases (Dice > 0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: r = -0.581). The metrics provide clinically interpretable proxies for model reliability.

Conclusion: Contrast-level Shapley values offer a clinically aligned approach to explain segmentation models, providing interpretable metrics that help clinicians understand model reliability and performance attribution across different imaging contrasts.

Abstract: Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.

</details>


### [303] [Affine Subspace Models and Clustering for Patch-Based Image Denoising](https://arxiv.org/abs/2512.07259)
*Tharindu Wickremasinghe,Marco F. Duarte*

Main category: eess.IV

TL;DR: The paper proposes using affine subspace models instead of linear subspaces for image tile clustering in denoising applications, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: Linear subspace models are not well-suited for image patches because images are non-negative and thus not distributed around the origin in the tile vector space. Affine subspaces better match the geometric structure of image tile vector space.

Method: The paper studies affine subspace models for tile clustering and presents a simple denoising algorithm using least squares projection onto these affine subspaces. Several algorithmic approaches to solve the affine subspace clustering problem are reviewed.

Result: Experimental results show performance improvements in both clustering and denoising when using affine subspace models compared to linear subspace approaches.

Conclusion: Affine subspace models are more appropriate than linear subspaces for image tile clustering in denoising applications, leading to better geometric matching and improved performance.

Abstract: Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.

</details>


### [304] [Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework](https://arxiv.org/abs/2512.07574)
*Xuecheng Li,Weikuan Jia,Komildzhon Sharipov,Alimov Ruslan,Lutfuloev Mazbutdzhon,Ismoilov Shuhratjon,Yuanjie Zheng*

Main category: eess.IV

TL;DR: Hybrid framework combining attention-enhanced cascaded U-Net, radiomics feature selection, and 3D CNN refinement for joint liver and liver-tumor segmentation in contrast-enhanced CT.


<details>
  <summary>Details</summary>
Motivation: Manual liver tumor contouring is slow, observer-dependent, and difficult to standardize. Automatic segmentation is challenged by low lesion-parenchyma contrast, blurred boundaries, heterogeneous enhancement patterns, and confounding structures like vessels and adjacent organs.

Method: Three-stage hybrid framework: 1) 2.5D two-stage network with densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates for initial probability maps, 2) Radiomics feature selection (728 descriptors reduced to 20) with random forest classifier to reject false positives, 3) Compact 3D patch-based CNN for voxel-level relabelling and contour smoothing in boundary regions.

Result: The method produces accurate 3D delineation of liver tumors with improved handling of thin/tiny lesions, noise suppression, and false-positive reduction through the integrated approach.

Conclusion: The proposed hybrid framework effectively addresses challenges in liver tumor segmentation by combining deep learning, radiomics, and 3D refinement, offering a robust solution for treatment planning, navigation, and response assessment.

Abstract: Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.

</details>


### [305] [R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation](https://arxiv.org/abs/2512.07576)
*Xuecheng Li,Weikuan Jia,Komildzhon Sharipov,Sharipov Hotam Beknazarovich,Farzona S. Ataeva,Qurbonaliev Alisher,Yuanjie Zheng*

Main category: eess.IV

TL;DR: R2MF-Net: A recurrent residual multi-path encoder-decoder network for automatic segmentation of multi-directional spine X-ray images to enable quantitative scoliosis assessment.


<details>
  <summary>Details</summary>
Motivation: Current spinal structure segmentation in X-ray images is manual, time-consuming, and non-reproducible, especially in low-contrast images with rib shadows or overlapping tissues. This hinders quantitative scoliosis assessment including Cobb angle measurement, vertebral translation estimation, and curvature classification.

Method: Proposes R2MF-Net with a two-stage cascade: coarse segmentation network followed by fine segmentation network. Features include: improved Inception-style multi-branch feature extractor, recurrent residual jump connection (R2-Jump) module for semantic alignment, multi-scale cross-stage skip (MC-Skip) mechanism for hierarchical representation reuse, and lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) for feature emphasis.

Result: Evaluated on a clinical multi-view radiograph dataset containing 228 sets of coronal, left-bending, and right-bending spine X-ray images with expert annotations. (Performance metrics not specified in abstract).

Conclusion: R2MF-Net addresses limitations of manual segmentation by providing an automated solution for multi-directional spine X-ray image segmentation, enabling more efficient and reproducible quantitative scoliosis assessment.

Abstract: Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [306] [AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study](https://arxiv.org/abs/2512.05983)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: AI models for finding compromise proposals in democratic text editing using NLP/LLMs to create semantic metric spaces and suggest compromise points.


<details>
  <summary>Details</summary>
Motivation: The challenge of finding compromise proposals that can unite agent coalitions in democratic processes like collaborative text editing (e.g., constitution drafting) remains an open question, with traditional tools being limited for large-scale applications.

Method: Formalized holistic model encompassing agent bounded rationality and uncertainty, applied NLP techniques and LLMs to create semantic metric space for text, and developed algorithms to suggest suitable compromise points.

Result: Simulated various coalition formation processes and demonstrated the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution.

Conclusion: AI models using NLP/LLMs can effectively generate compromise proposals for democratic text editing processes where traditional tools are limited, addressing the open question of finding compromise points in coalition formation.

Abstract: The challenge of finding compromises between agent proposals is fundamental to AI sub-fields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. The crucial step in this iterative process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals, however, remains an open question. We address this gap by formalizing a holistic model that encompasses agent bounded rationality and uncertainty and developing AI models to generate such compromise proposals. We focus on the domain of collaboratively writing text documents -- e.g., to enable the democratic creation of a community constitution. We apply NLP (Natural Language Processing) techniques and utilize LLMs (Large Language Models) to create a semantic metric space for text and develop algorithms to suggest suitable compromise points. To evaluate the effectiveness of our algorithms, we simulate various coalition formation processes and demonstrate the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution, an area where traditional tools are limited.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [307] [KidSpeak: A General Multi-purpose LLM for Kids' Speech Recognition and Screening](https://arxiv.org/abs/2512.05994)
*Rohan Sharma,Dancheng Liu,Jingchen Sun,Shijie Zhou,Jiayu Qin,Jinjun Xiong,Changyou Chen*

Main category: eess.AS

TL;DR: KidSpeak is a multi-task speech foundation model for children's speech, paired with FASA alignment tool to create high-quality datasets, achieving 87% accuracy across tasks and 13.6x data quality improvement.


<details>
  <summary>Details</summary>
Motivation: Current AI models fail on children's speech due to reliance on adult speech datasets, especially problematic for early developmental stages and speech/language pathologies. There's a need for specialized models that can handle unique children's speech patterns.

Method: Two-stage training process incorporating phonetic knowledge into speech encoder, plus development of Flexible and Automatic Speech Aligner (FASA) for constructing high-quality datasets from noisy children's speech data.

Result: Achieved 87% average accuracy across four tasks, and FASA improved data quality by 13.6x compared to human annotations on CHILDES dataset, enabling better alignment of noisy children's speech.

Conclusion: KidSpeak and FASA represent the first comprehensive solution for children's speech and language therapy, offering both a multi-purpose speech LLM and robust alignment tool to address current limitations in AI for children's speech.

Abstract: With the rapid advancement of conversational and diffusion-based AI, there is a growing adoption of AI in educational services, ranging from grading and assessment tools to personalized learning systems that provide targeted support for students. However, this adaptability has yet to fully extend to the domain of children's speech, where existing models often fail due to their reliance on datasets designed for clear, articulate adult speech. Children, particularly those in early developmental stages or with speech and language pathologies, present unique challenges that current AI models and datasets are ill-equipped to handle. To address this, we introduce KidSpeak, a multi-task speech-enhanced Foundation Model capable of both generative and discriminative tasks specifically tailored to children's speech patterns. Our framework employs a two-stage training process that incorporates phonetic knowledge into the speech encoder, achieving an average accuracy of 87% across four separate tasks. Furthermore, recognizing the limitations of scalable human annotation and existing speech alignment tools, we propose the Flexible and Automatic Speech Aligner (FASA) and leverage the method to construct high quality datasets for training and evaluation. This novel alignment tool significantly improves the quality of aligned children's speech from noisy data, enhancing data quality by 13.6x compared to human annotations, as demonstrated on the CHILDES dataset. To the best of our knowledge, KidSpeak and FASA represent the first comprehensive solution designed for speech and language therapy in children, offering both a multi-purpose speech LLM and a robust alignment tool.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [308] [The Road of Adaptive AI for Precision in Cybersecurity](https://arxiv.org/abs/2512.06048)
*Sahil Garg*

Main category: cs.CR

TL;DR: Practical insights on building production GenAI pipelines for cybersecurity, focusing on continual adaptation to evolving threats and knowledge bases.


<details>
  <summary>Details</summary>
Motivation: Cybersecurity's evolving complexity presents unique challenges for AI, requiring continual adaptation to shifting knowledge bases, tooling, and threats. The paper aims to provide actionable guidance for AI practitioners navigating GenAI for cybersecurity.

Method: Based on real-world deployments, the paper shares lessons from designing, building, and operating production-grade GenAI pipelines. It focuses on how different adaptation mechanisms (retrieval-level and model-level) complement each other in end-to-end systems.

Result: The paper provides practical guidance and best practices for leveraging retrieval- and model-level adaptation in cybersecurity GenAI systems, derived from actual deployment experiences.

Conclusion: The paper highlights open research directions for making GenAI more robust, precise, and auditable in cyber defense, emphasizing the need for continual adaptation mechanisms that work together in production systems.

Abstract: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.

</details>


### [309] [Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization](https://arxiv.org/abs/2512.06713)
*Donghang Duan,Xu Zheng*

Main category: cs.CR

TL;DR: RLAA is a fully localized text anonymization framework that uses an Attacker-Arbitrator-Anonymizer architecture to prevent utility collapse in small models by enforcing rational adversarial strategies.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based anonymization creates a "privacy paradox" - users must disclose data to third parties for privacy. Local small models fail catastrophically due to irrational greedy adversarial strategies in existing methods.

Method: Proposes Rational Localized Adversarial Anonymization (RLAA) with Attacker-Arbitrator-Anonymizer architecture. Models anonymization as trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC). The arbitrator acts as rationality gatekeeper to filter out feedback with negligible privacy benefits, enforcing rational early-stopping.

Result: RLAA achieves best privacy-utility trade-off across datasets, sometimes outperforming state-of-the-art on Pareto principle. Prevents utility collapse in local small models while maintaining privacy.

Conclusion: The failure of local small models in anonymization stems from irrational greedy strategies, not just capability deficits. RLAA's rational adversarial approach enables effective fully-localized anonymization without privacy paradox.

Abstract: Current LLM-based text anonymization frameworks usually rely on remote API services from powerful LLMs, which creates an inherent "privacy paradox": users must somehow disclose data to untrusted third parties for superior privacy preservation. Moreover, directly migrating these frameworks to local small-scale models (LSMs) offers a suboptimal solution with catastrophic collapse in utility based on our core findings. Our work argues that this failure stems not merely from the capability deficits of LSMs, but from the inherent irrationality of the greedy adversarial strategies employed by current state-of-the-art (SoTA) methods. We model the anonymization process as a trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC), and demonstrate that greedy strategies inevitably drift into an irrational state. To address this, we propose Rational Localized Adversarial Anonymization (RLAA), a fully localized and training-free framework featuring an Attacker-Arbitrator-Anonymizer (A-A-A) architecture. RLAA introduces an arbitrator that acts as a rationality gatekeeper, validating the attacker's inference to filter out feedback providing negligible benefits on privacy preservation. This mechanism enforces a rational early-stopping criterion, and systematically prevents utility collapse. Extensive experiments on different datasets demonstrate that RLAA achieves the best privacy-utility trade-off, and in some cases even outperforms SoTA on the Pareto principle. Our code and datasets will be released upon acceptance.

</details>


### [310] [OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation](https://arxiv.org/abs/2512.06589)
*Xiaojun Jia,Jie Liao,Qi Guo,Teng Ma,Simeng Qin,Ranjie Duan,Tianlin Li,Yihao Huang,Zhitao Zeng,Dongxian Wu,Yiming Li,Wenqi Ren,Xiaochun Cao,Yang Liu*

Main category: cs.CR

TL;DR: OmniSafeBench-MM is a comprehensive toolbox for evaluating multi-modal jailbreak attacks and defenses in large language models, addressing limitations of existing benchmarks through standardized evaluation protocols.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal jailbreak benchmarks have limitations: they focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified reproducible toolbox. There's a need for a comprehensive evaluation framework to better understand and improve MLLM safety.

Method: Introduces OmniSafeBench-MM with 13 attack methods, 15 defense strategies, and diverse dataset spanning 9 risk domains and 50 categories across consultative, imperative, and declarative inquiry types. Establishes three-dimensional evaluation protocol measuring harmfulness (multi-level scale), intent alignment, and response detail level.

Result: Extensive experiments on 10 open-source and 8 closed-source MLLMs reveal their vulnerability to multi-modal jailbreak attacks. The toolbox provides standardized foundation for future research with open-source code available.

Conclusion: OmniSafeBench-MM addresses critical gaps in multi-modal safety evaluation by unifying data, methodology, and evaluation into a comprehensive, reproducible platform that enables nuanced safety-utility analysis and standardized benchmarking.

Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [311] [MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions](https://arxiv.org/abs/2512.06933)
*Zifan Peng*

Main category: cs.CE

TL;DR: matex is a cognitive multi-agent framework that generates faithful, step-wise explanations for complex Ethereum transactions by combining on-chain evidence with protocol semantics.


<details>
  <summary>Details</summary>
Motivation: Understanding complex Ethereum transactions is challenging due to multi-hop token flows, nested contract calls, and opaque execution paths, leading users to blind signing. Interviews with users, developers, and auditors revealed the need for faithful explanations grounded in both on-chain evidence and real-world protocol semantics.

Method: Introduces matex, a cognitive multi-agent framework that models transaction understanding as collaborative investigation. It combines rapid hypothesis generation, dynamic off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation to produce faithful explanations.

Result: The framework produces step-wise explanations that help users understand complicated Ethereum transactions by integrating on-chain evidence with protocol semantics, addressing the blind signing problem.

Conclusion: matex provides a systematic approach to transaction understanding through multi-agent collaboration, offering faithful explanations that bridge the gap between on-chain evidence and real-world protocol knowledge.

Abstract: Understanding a complicated Ethereum transaction remains challenging: multi-hop token flows, nested contract calls, and opaque execution paths routinely lead users to blind signing. Based on interviews with everyday users, developers, and auditors, we identify the need for faithful, step-wise explanations grounded in both on-chain evidence and real-world protocol semantics. To meet this need, we introduce (matex, a cognitive multi-agent framework that models transaction understanding as a collaborative investigation-combining rapid hypothesis generation, dynamic off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation to produce faithful explanations.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [312] [Human Geometry Distribution for 3D Animation Generation](https://arxiv.org/abs/2512.07459)
*Xiangjun Tang,Biao Zhang,Peter Wonka*

Main category: cs.GR

TL;DR: Two-stage framework for generating realistic human geometry animations with detailed clothing dynamics using a compact latent representation and generative animation model.


<details>
  <summary>Details</summary>
Motivation: Generating realistic human geometry animations is challenging due to the need for modeling natural clothing dynamics with fine-grained details under limited data availability.

Method: Two-stage framework: 1) Compact distribution-based latent representation for efficient high-quality geometry generation with improved SMPL-to-avatar mapping, 2) Generative animation model exploiting limited motion data diversity through short-term transitions and long-term consistency via identity-conditioned design.

Result: Latent space produces 90% lower Chamfer Distance than previous methods; animation model synthesizes diverse animations with natural dynamics (2.2 higher user study score), achieving best results across all evaluation metrics.

Conclusion: The proposed two-stage framework successfully addresses challenges in human geometry animation generation, producing high-fidelity results with detailed dynamics while effectively utilizing limited data.

Abstract: Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \times$ higher user study score), achieving the best results across all evaluation metrics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [313] [Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels](https://arxiv.org/abs/2512.07474)
*Yifei Huang,Tianyu Yan,Sitong Gong,Xiwei Gao,Caixin Kang,Ruicong Liu,Huchuan Lu,Bo Zheng*

Main category: cs.HC

TL;DR: Living Novel system transforms literary works into immersive multi-character conversations using a two-stage training pipeline to solve persona drift and narrative incoherence problems in LLM-driven characters.


<details>
  <summary>Details</summary>
Motivation: Generic LLMs suffer from persona drift (failing to stay in character) and exhibit abilities beyond story constraints, causing narrative incoherence (spoiler leakage) and robustness failures (frame-breaking) in character-driven experiences.

Method: Two-stage training pipeline: 1) Deep Persona Alignment (DPA) uses data-free reinforcement finetuning for character fidelity; 2) Coherence and Robustness Enhancing (CRE) uses story-time-aware knowledge graph and retrieval-grounded training to enforce narrative constraints.

Result: DPA pipeline outperforms GPT-4o on persona-specific metrics; CRE stage achieves near-perfect performance in coherence and robustness. System validated through multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea with lab study and 5-day diary study.

Conclusion: Character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences in AI-driven narrative systems.

Abstract: We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [314] [When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models](https://arxiv.org/abs/2512.06343)
*Tong Xie,Andrew Bai,Yuanhao Ban,Yunqi Hong,Haoyu Li,Cho-jui Hsieh*

Main category: cs.LG

TL;DR: The paper identifies a limitation in the Bradley-Terry loss for reward modeling where gradient magnitude depends on representation distance, causing small-distance pairs to receive weak updates. They propose NormBT, a normalization scheme that balances representation effects and improves performance, especially on fine-grained distinctions.


<details>
  <summary>Details</summary>
Motivation: The standard Bradley-Terry loss used in reward modeling for LLM alignment suffers from a gradient scaling issue where update magnitude depends on representation distance between response pairs, not just prediction error. This causes small-distance pairs (where fine-grained distinctions matter) to receive vanishingly weak updates while large-distance pairs dominate training.

Method: Analyzed per-sample gradient of BT-loss, identified two scaling components: (1) reward prediction error difference, and (2) representation distance in final layer output space. Proposed NormBT - an adaptive pairwise normalization scheme that balances representation-driven effects and focuses learning on prediction error. NormBT is a lightweight, drop-in replacement for BT loss.

Result: NormBT consistently improves reward model performance across various LLM backbones and datasets. Achieved notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs requiring fine-grained distinctions.

Conclusion: The work reveals a key limitation in the widely used BT objective where representation distance impacts gradient magnitude, and provides a simple, effective correction through NormBT that better focuses learning on prediction error and improves performance on fine-grained distinctions.

Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.

</details>


### [315] [LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing](https://arxiv.org/abs/2512.06351)
*Zhiying Yang,Fang Liu,Wei Zhang,Xin Lou,Malcolm Yoke Hean Low,Boon Ping Gan*

Main category: cs.LG

TL;DR: LUCA is an LLM-enhanced graph reinforcement learning framework for carbon-aware flexible job shop scheduling that combines GNN and LLM embeddings with DRL to optimize both makespan and carbon emissions.


<details>
  <summary>Details</summary>
Motivation: Address challenges of dynamic and sustainable scheduling in smart manufacturing systems, specifically tackling the dual objectives of minimizing both makespan and carbon emissions in flexible job shop scheduling problems.

Method: Integrates graph neural network and large language model with in-house prompting strategy to create fused embeddings capturing structural characteristics and contextual semantics of scheduling states, then processes these embeddings through deep reinforcement learning policy network for real-time scheduling decisions.

Result: Outperforms comparison algorithms on both synthetic and public datasets, achieving average 4.1% and up to 12.2% lower makespan on synthetic data while maintaining same emission levels, with additional gains on public datasets for both objectives.

Conclusion: LUCA is an effective and practical framework for carbon-aware scheduling in smart manufacturing, successfully integrating LLM-enhanced graph RL to optimize both scheduling efficiency and environmental sustainability.

Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.

</details>


### [316] [A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs](https://arxiv.org/abs/2512.06607)
*Humzah Merchant,Bradford Levy*

Main category: cs.LG

TL;DR: A method to remove look-ahead bias from LLMs in finance by adjusting logits at inference using specialized smaller models, enabling proper backtesting without expensive retraining.


<details>
  <summary>Details</summary>
Motivation: Applying LLMs to financial prediction is problematic due to look-ahead bias from training on long time-series data, making proper backtesting impossible without prohibitively expensive retraining of frontier models from scratch with specific knowledge cutoffs.

Method: Adjusts generation at inference time by modifying the logits of a large base model using a pair of smaller specialized models: one fine-tuned on information to be forgotten and another on information to be retained.

Result: The method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior approaches to knowledge removal.

Conclusion: Provides a fast, effective, and low-cost alternative to expensive retraining for removing look-ahead bias in LLMs applied to financial prediction tasks, enabling proper backtesting.

Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.

</details>


### [317] [Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics](https://arxiv.org/abs/2512.06737)
*Nikhil Verma,Joonas Linnosmaa,Espinosa-Leal Leonardo,Napat Vajragupta*

Main category: cs.LG

TL;DR: ArcGD optimizer outperforms Adam on non-convex benchmarks and achieves highest accuracy on CIFAR-10 across 8 MLP architectures, showing better generalization without overfitting.


<details>
  <summary>Details</summary>
Motivation: To develop a new optimizer (ArcGD) that can handle challenging non-convex optimization problems and demonstrate superior performance on real-world ML tasks compared to state-of-the-art optimizers like Adam, AdamW, Lion, and SGD.

Method: Formulated and implemented ArcGD optimizer, evaluated on two fronts: 1) non-convex Rosenbrock function from 2D to 50,000D with learning-rate bias elimination, 2) CIFAR-10 image classification across 8 diverse MLP architectures (1-5 hidden layers) compared against Adam, AdamW, Lion, and SGD.

Result: ArcGD consistently outperformed Adam on Rosenbrock function under fair learning-rate settings. On CIFAR-10, ArcGD achieved highest average test accuracy (50.7%) at 20,000 iterations, beating all competitors and winning/tying on 6 of 8 architectures. ArcGD showed continued improvement while Adam/AdamW regressed with extended training.

Conclusion: ArcGD demonstrates superior performance on both geometric stress tests and deep learning benchmarks, showing better generalization and resistance to overfitting without requiring early stopping tuning. The optimizer has broad applicability and connections to existing methods like Lion.

Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.

</details>


### [318] [Flash Multi-Head Feed-Forward Network](https://arxiv.org/abs/2512.06989)
*Minshen Zhang,Xiang Hu,Jianguo Li,Wei Wu,Kewei Tu*

Main category: cs.LG

TL;DR: FlashMHF replaces standard FFNs in Transformers with multi-head FFNs using fused kernels and dynamic weighting to improve performance while reducing memory usage.


<details>
  <summary>Details</summary>
Motivation: Multi-head mechanisms enhance expressivity in attention, but applying them to FFNs faces memory scaling issues and imbalanced dimension ratios that degrade scalability and expressive power.

Method: Proposes Flash Multi-Head FFN (FlashMHF) with two innovations: 1) I/O-aware fused kernel computing outputs online in SRAM (like FlashAttention), and 2) dynamically weighted parallel sub-networks to maintain balanced intermediate-to-head dimension ratios.

Result: Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x.

Conclusion: Establishes multi-head design as superior architectural principle for FFNs, presenting FlashMHF as powerful, efficient, and scalable alternative to FFNs in Transformers.

Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.

</details>


### [319] [Block Sparse Flash Attention](https://arxiv.org/abs/2512.07011)
*Daniel Ohayon,Itay Lamprecht,Itay Hubara,Israel Cohen,Daniel Soudry,Noam Elata*

Main category: cs.LG

TL;DR: BSFA is a training-free sparse attention method that accelerates long-context inference by computing exact query-key similarities, selecting top-k value blocks, and skipping ~50% of computation via calibrated thresholds, achieving up to 1.24x speedup while maintaining >99% accuracy.


<details>
  <summary>Details</summary>
Motivation: Modern LLMs need long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates severe computational bottlenecks. Existing sparse attention methods often predict importance before computing scores, which can be inaccurate.

Method: BSFA computes exact query-key similarities to select top-k most important value blocks per query. It uses per-block maximum scores compared against calibrated thresholds to skip ~50% of computation. Requires only one-time threshold calibration on small dataset to learn per-layer/head attention score distributions.

Result: On Llama-3.1-8B: up to 1.10x speedup on real-world reasoning benchmarks, up to 1.24x for needle-in-a-haystack retrieval tasks, while maintaining above 99% baseline accuracy. Some configurations even improve accuracy by focusing on most relevant content.

Conclusion: BSFA provides an effective training-free drop-in replacement for FlashAttention that accelerates long-context inference while preserving model quality, substantially outperforming existing sparse attention methods.

Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention

</details>


### [320] [Pay Less Attention to Function Words for Free Robustness of Vision-Language Models](https://arxiv.org/abs/2512.07222)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Chao Shen*

Main category: cs.LG

TL;DR: FDA (Function-word De-Attention) improves VLM robustness by reducing vulnerability from function words in cross-modal attacks, achieving significant attack success rate reductions with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: There's a trade-off between robustness and performance in robust Vision-Language Models (VLMs). The paper observes that function words create vulnerabilities against cross-modal adversarial attacks, motivating the need to mitigate their impact.

Method: Proposes Function-word De-Attention (FDA) that calculates both original and function-word cross-attention within attention heads, then differentially subtracts the function-word attention from the original attention (similar to differential amplifiers).

Result: FDA achieves average 18/13/53% Attack Success Rate (ASR) drop on 3 models for retrieval with only 0.2/0.3/0.6% performance drop, and 90% ASR drop with 0.3% performance gain on visual grounding. Shows scalability, generalization, and zero-shot performance.

Conclusion: FDA effectively addresses the robustness-performance trade-off in VLMs by mitigating function-word vulnerabilities, demonstrating strong empirical results across multiple models, tasks, and attack scenarios.

Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.

</details>


### [321] [Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning](https://arxiv.org/abs/2512.07374)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: R2F enables efficient unlearning in LLMs by reconstructing full-model gradients from LoRA adapter updates using a gradient decoder, avoiding full-model fine-tuning and original data access.


<details>
  <summary>Details</summary>
Motivation: Unlearning in foundation models is crucial for dynamic knowledge updates, data deletion rights, and behavior correction, but existing methods require full-model fine-tuning or original data access, limiting scalability and practicality.

Method: Compute gradients with respect to LoRA parameters using paraphrased prompts, train a gradient decoder to approximate full-model gradients, and transfer decoder from proxy model to target models for black-box applicability.

Result: R2F achieves effective unlearning while preserving general model performance, offering a scalable and lightweight alternative without requiring full retraining or access to internal parameters.

Conclusion: R2F provides a practical framework for efficient unlearning in LLMs through gradient reconstruction from LoRA updates, enabling knowledge updates without the computational burden of full-model retraining.

Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.

</details>


### [322] [LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples](https://arxiv.org/abs/2512.07375)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: LUNE is a lightweight LLM unlearning framework using LoRA adapters for efficient knowledge removal without full model retraining.


<details>
  <summary>Details</summary>
Motivation: LLMs can't selectively remove information for privacy, bias mitigation, and knowledge correction. Traditional unlearning methods require expensive fine-tuning or weight editing, making them impractical for real-world deployment.

Method: LoRA-based Unlearning with Negative Examples (LUNE) uses negative-only unlearning by updating only low-rank adapters while freezing the backbone model. It targets intermediate representations to suppress or replace requested knowledge using Low-Rank Adaptation (LoRA).

Result: LUNE achieves effectiveness comparable to full fine-tuning and memory-editing methods while reducing computational cost by about an order of magnitude across multiple factual unlearning tasks.

Conclusion: LUNE provides a practical, efficient solution for LLM unlearning that localizes edits to avoid disruptive global changes, making knowledge removal feasible for real-world applications.

Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.

</details>


### [323] [Group Representational Position Encoding](https://arxiv.org/abs/2512.07805)
*Yifan Zhang,Zixiang Chen,Yifeng Liu,Zhen Qin,Huizhuo Yuan,Kangping Xu,Yang Yuan,Quanquan Gu,Andrew Chi-Chih Yao*

Main category: cs.LG

TL;DR: GRAPE is a unified framework for positional encoding using group actions, with two main variants: Multiplicative GRAPE (rotations in SO(d)) and Additive GRAPE (additive logits from unipotent actions in GL).


<details>
  <summary>Details</summary>
Motivation: To create a principled design space for positional geometry in long-context models that unifies existing approaches like RoPE and ALiBi under a single theoretical framework based on group theory.

Method: Uses group actions for positional encoding: (1) Multiplicative GRAPE with rotations in SO(d) using matrix exponentials of skew generators, (2) Additive GRAPE with unipotent actions in GL producing additive logits. Both preserve relative positional relationships and streaming cacheability.

Result: GRAPE recovers RoPE exactly when using canonical coordinate pairs with log-uniform spectrum, and recovers ALiBi and Forgetting Transformer as exact special cases. It extends these with learned commuting subspaces and non-commuting mixtures for better feature coupling.

Conclusion: GRAPE provides a unified theoretical framework for positional encoding that subsumes existing methods (RoPE, ALiBi) while offering principled extensions for improved long-context modeling with efficient computational costs.

Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.

</details>


### [324] [Vector Quantization using Gaussian Variational Autoencoder](https://arxiv.org/abs/2512.06609)
*Tongda Xu,Wendi Zheng,Jiajun He,Jose Miguel Hernandez-Lobato,Yan Wang,Ya-Qin Zhang,Jie Tang*

Main category: cs.LG

TL;DR: Gaussian Quant (GQ) converts Gaussian VAEs into VQ-VAEs without training by using random Gaussian noise as codebook, achieving better performance than existing VQ-VAE methods.


<details>
  <summary>Details</summary>
Motivation: VQ-VAEs are difficult to train due to discretization challenges. The paper aims to create a simpler, more effective approach by leveraging Gaussian VAEs instead of training VQ-VAEs from scratch.

Method: Proposes Gaussian Quant (GQ) technique that converts Gaussian VAEs into VQ-VAEs without training by generating random Gaussian noise as codebook and finding closest noise to posterior mean. Also introduces Target Divergence Constraint (TDC) heuristic to train Gaussian VAEs for effective GQ.

Result: GQ outperforms previous VQ-VAEs (VQGAN, FSQ, LFQ, BSQ) on both UNet and ViT architectures. TDC also improves upon previous Gaussian VAE discretization methods like TokenBridge.

Conclusion: Gaussian Quant provides a simple yet effective way to create high-performing VQ-VAEs from Gaussian VAEs without training, with theoretical guarantees and practical improvements over existing methods.

Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.

</details>


### [325] [Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network](https://arxiv.org/abs/2512.06648)
*Xiao Li*

Main category: cs.LG

TL;DR: A CNN-based financial fraud detection framework for Chinese A-share companies that transforms panel data into image-like representations for early fraud prediction, outperforming traditional methods while providing interpretability through local explanations.


<details>
  <summary>Details</summary>
Motivation: Financial fraud detection is challenging due to covert tactics, high audit costs, and limitations of existing methods. Traditional statistical models lack ability to capture nonlinear feature interactions, while machine learning models are often opaque. Most methods only detect current-year fraud, limiting timeliness.

Method: Proposes a CNN-based framework that transforms firm-year panel data into image-like representations to capture cross-sectional and temporal patterns. Uses feature engineering to enable advance fraud prediction and applies local explanation techniques for interpretability across entity, feature, and time dimensions.

Result: CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance. Proper threshold tuning is crucial in high-risk settings. Analysis reveals solvency, ratio structure, governance, and internal control as general fraud predictors, with environmental indicators mattering mainly in high-pollution industries.

Conclusion: The CNN framework effectively detects financial fraud with superior performance and interpretability. Fraud firms show heterogeneous patterns in short time windows, while non-fraud firms exhibit stable patterns. The approach enables timely detection and provides actionable insights for regulators and investors.

Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.

</details>


### [326] [Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning](https://arxiv.org/abs/2512.06649)
*Camellia Zakaria,Aryan Sadeghi,Weaam Jaafar,Junshi Xu,Alex Mariakakis,Marianne Hatzopoulou*

Main category: cs.LG

TL;DR: A machine learning system uses traffic video and weather data to estimate black carbon emissions at street level, achieving R虏=0.72 to address pollution monitoring gaps in urban areas.


<details>
  <summary>Details</summary>
Motivation: Black carbon emissions from traffic disproportionately affect marginalized communities near major roads, but monitoring is expensive and scarce. Traffic monitoring systems are widely deployed while environmental impact data is lacking, creating an information gap for local policy interventions.

Method: Machine learning system extracts visual information from traffic video to capture vehicle behaviors and conditions, then combines these features with weather data to estimate black carbon concentrations at street level.

Result: The model achieves an R-squared value of 0.72 and RMSE of 129.42 ng/m鲁, demonstrating effective estimation of black carbon concentrations using traffic video and weather data.

Conclusion: The approach leverages existing urban infrastructure (traffic cameras) to generate actionable BC data for pollution reduction, urban planning, public health, and environmental justice at local municipal levels.

Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.

</details>


### [327] [Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods](https://arxiv.org/abs/2512.06665)
*Panagiota Kiourti,Anu Singh,Preeti Duraipandian,Weichao Zhou,Wenchao Li*

Main category: cs.LG

TL;DR: The paper proposes a new framework for evaluating the robustness of feature attribution methods in deep neural networks, challenging existing notions and introducing new definitions, metrics, and adversarial generation methods.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of attribution method robustness largely ignores differences in model outputs and fails to objectively reveal weaknesses specific to attribution methods rather than the neural network itself.

Method: Proposes: 1) new definition of similar inputs, 2) new robustness metric, 3) novel GAN-based method to generate these inputs, and 4) comprehensive evaluation with existing metrics and state-of-the-art attribution methods.

Result: The proposed framework provides a more objective evaluation that reveals weaknesses of attribution methods specifically, rather than the neural network, highlighting the need for improved robustness metrics.

Conclusion: Current attribution robustness metrics are inadequate; the proposed approach offers a more accurate evaluation that distinguishes attribution method weaknesses from model weaknesses, advancing reliable feature attribution analysis.

Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.

</details>


### [328] [Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data](https://arxiv.org/abs/2512.06730)
*Lin Yang,Xiang Li,Xin Ma,Xinxin Zhao*

Main category: cs.LG

TL;DR: AR-SSVEP system using HoloLens 2 and enhanced CNN-BiLSTM with multi-head attention for motor intention recognition in rehabilitation.


<details>
  <summary>Details</summary>
Motivation: Patients with motor dysfunction show low engagement in rehabilitation, and traditional SSVEP-based BCIs rely on external visual equipment, limiting practicality in real-world settings.

Method: 1) Designed four HoloLens 2-based EEG classes and collected data from 7 healthy subjects; 2) Enhanced CNN-BiLSTM with multi-head attention (MACNN-BiLSTM); 3) Extracted 10 temporal-spectral EEG features for CNN learning; 4) Used BiLSTM for sequential dependencies; 5) Applied multi-head attention to highlight motor-intention patterns; 6) Used SHAP for model interpretability.

Result: The proposed system enhances real-time motor intention recognition and supports recovery in patients with motor impairments.

Conclusion: The AR-SSVEP system with MACNN-BiLSTM and SHAP interpretability addresses patient engagement issues and therapist workload, improving practicality for real-world rehabilitation settings.

Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.

</details>


### [329] [Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis](https://arxiv.org/abs/2512.07040)
*Sakib Mostafa,Lei Xing,Md. Tauhidul Islam*

Main category: cs.LG

TL;DR: Graph2Image transforms large biological networks into 2D images for CNN analysis, improving scalability, accuracy, and interpretability over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional biological network analysis methods (including deep learning) face challenges with scalability, long-range dependency capture, multimodal integration, expressivity bounds, and poor interpretability when dealing with large, complex biological networks.

Method: Graph2Image transforms large biological networks into sets of 2D images by spatially arranging representative network nodes on a 2D grid, decoupling nodes as images to enable CNN analysis with global receptive fields and multi-scale pyramids.

Result: Graph2Image improved classification accuracy by up to 67.2% over existing methods, enabled analysis of networks with over 1 billion nodes on personal computers, and provided interpretable visualizations revealing biologically coherent patterns.

Conclusion: Graph2Image provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and complex biological systems study.

Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.

</details>


### [330] [Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search](https://arxiv.org/abs/2512.07142)
*Tanay Arora,Christof Teuscher*

Main category: cs.LG

TL;DR: CTS is a new pruning-at-initialization method that uses combinatorial optimization and gradient balancing to find high-performing sparse subnetworks much faster than existing methods like LTR, especially at high sparsity levels.


<details>
  <summary>Details</summary>
Motivation: Current methods for finding winning tickets are either too computationally expensive (LTR) or have poor accuracy-sparsity trade-offs (saliency-based PaI). The authors argue that PaI's reliance on first-order saliency metrics that ignore inter-weight dependencies causes this performance gap.

Method: CTS frames subnetwork discovery as combinatorial optimization using Concrete relaxation of discrete search space and GRADBALANCE for sparsity control. It also introduces CTS-KL, which minimizes reverse KL divergence between sparse and dense network outputs.

Result: CTS produces subnetworks that pass sanity checks and achieve accuracy comparable to or exceeding LTR with much less computation. On ResNet-20/CIFAR10: 99.3% sparsity with 74.0% accuracy in 7.9 minutes vs LTR's 68.3% accuracy in 95.2 minutes.

Conclusion: CTS efficiently identifies high-performing subnetworks near initialization without sensitive hyperparameter tuning, outperforming saliency-based methods across all sparsities and showing particular advantage over LTR in highly sparse regimes.

Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.

</details>


### [331] [FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers](https://arxiv.org/abs/2512.07150)
*Jonghyun Park,Jong Chul Ye*

Main category: cs.LG

TL;DR: FlowLPS: A training-free framework using Langevin Proximal Sampling to solve inverse problems with pretrained flow models, addressing convergence and manifold deviation issues.


<details>
  <summary>Details</summary>
Motivation: Existing training-free methods for solving inverse problems with latent flow models often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces, limiting their effectiveness.

Method: FlowLPS integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, creating a training-free framework that solves inverse problems using pretrained flow models.

Result: The method achieves superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K datasets, outperforming state-of-the-art inverse solvers.

Conclusion: FlowLPS provides an effective training-free solution for inverse problems with flow models, addressing convergence and manifold deviation issues through the combination of Langevin dynamics and proximal optimization.

Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.

</details>


### [332] [Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood](https://arxiv.org/abs/2512.07390)
*Gilhyun Nam,Taewon Kim,Joonhyun Jeong,Eunho Yang*

Main category: cs.LG

TL;DR: SICL is a plug-and-play calibration framework that uses style-invariance to estimate correctness likelihood without backpropagation, reducing calibration error by 13% on average across various TTA methods.


<details>
  <summary>Details</summary>
Motivation: Test-time adaptation methods often produce poorly calibrated predictive uncertainty, which is critical in high-stakes domains like autonomous driving, finance, and healthcare. Existing calibration methods fail under dynamic test conditions because they assume fixed models or static distributions.

Method: SICL leverages style-invariance by measuring prediction consistency across style-altered variants to estimate instance-wise correctness likelihood. It requires only forward passes, making it backpropagation-free and compatible with any TTA method as a plug-and-play module.

Result: Comprehensive evaluations across 4 baselines, 5 TTA methods, and 2 realistic scenarios with 3 model architectures show SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.

Conclusion: SICL provides an effective, efficient solution for uncertainty calibration in test-time adaptation scenarios, addressing the critical need for reliable uncertainty estimation in dynamic real-world applications.

Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.

</details>


### [333] [Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models](https://arxiv.org/abs/2512.07419)
*Haidong Kang,Jun Du,Lihong Lin*

Main category: cs.LG

TL;DR: TAP is a novel LLM-driven training-free automatic proxy discovery framework for mixed-precision quantization that eliminates human expert involvement and achieves SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing MPQ methods either use costly differentiable optimization (inefficient/inflexible) or rely on human-designed proxies (labor-intensive, requires expert knowledge). The paper aims to design an automatic proxy discovery method without human experts or training.

Method: Proposes TAP framework that uses LLMs to automatically discover superior proxies for MPQ. Introduces Direct Policy Optimization (DPO)-based reinforcement learning to optimize prompts and enhance LLM reasoning, creating a positive feedback loop between LLMs and MPQ tasks.

Result: Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance in mixed-precision quantization.

Conclusion: TAP significantly contributes to the MPQ community by providing a new LLM-driven design paradigm that eliminates human expert involvement and training requirements while achieving superior performance.

Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.

</details>


### [334] [KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models](https://arxiv.org/abs/2512.07437)
*Chenwei Shi,Xueyu Luan*

Main category: cs.LG

TL;DR: KAN-Dreamer integrates Kolmogorov-Arnold Networks into DreamerV3, showing FastKAN can replace MLPs in reward/continue predictors without performance loss.


<details>
  <summary>Details</summary>
Motivation: To combine the sample efficiency of DreamerV3 with the parameter efficiency and interpretability of KANs, while addressing KANs' computational overhead through FastKAN variants.

Method: Replace specific MLP and convolutional components in DreamerV3 with KAN and FastKAN layers, implement vectorized JAX version with simplified grid management, and evaluate three subsystems: Visual Perception, Latent Prediction, and Behavior Learning.

Result: FastKAN as drop-in replacement for Reward and Continue predictors achieves performance parity with original MLP-based architecture in both sample efficiency and training speed on DeepMind Control Suite (walker_walk).

Conclusion: KAN-based architectures can be successfully integrated into model-based RL frameworks, with FastKAN maintaining performance while offering potential parameter efficiency benefits, serving as preliminary study for future KAN-based world models.

Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.

</details>


### [335] [Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces](https://arxiv.org/abs/2512.07509)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: The paper explores using predefined vector systems (like An root systems) as targets for latent space configurations to train neural networks without classification layers, enabling efficient training on datasets with massive numbers of classes.


<details>
  <summary>Details</summary>
Motivation: Training neural networks on datasets with extremely large numbers of classes is challenging due to classification layer complexity. Using predefined vector systems as latent space targets allows training without classification layers, addressing scalability issues.

Method: The paper provides a general overview of vector systems for NN training, their properties, and construction methods. These systems configure latent spaces of encoders and visual transformers, using minimum dimensions for specific class counts to optimize convergence.

Result: The approach significantly speeds up training on ImageNet-1K and datasets with 50k-600k classes. Using minimum latent space dimensions for specific class counts results in faster convergence and potential reduction in vector database size for storing embeddings.

Conclusion: Predefined vector systems offer an effective method for configuring neural network latent spaces, enabling efficient training without classification layers, faster convergence with optimized dimensions, and reduced storage requirements for embeddings in large-scale classification tasks.

Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.

</details>


### [336] [ReLaX: Reasoning with Latent Exploration for Large Reasoning Models](https://arxiv.org/abs/2512.07558)
*Shimin Zhang,Xianwei Chen,Yufan Shen,Ziyuan Ye,Jibin Wu*

Main category: cs.LG

TL;DR: ReLaX introduces latent dynamics analysis using Koopman operator theory to prevent entropy collapse in RLVR, achieving SOTA performance on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: RLVR enhances reasoning in LRMs but suffers from entropy collapse leading to premature policy convergence and performance saturation. Token-level entropy manipulation helps but latent dynamics contain richer computational structure for better exploration-exploitation tradeoff.

Method: Use Koopman operator theory to linearize hidden-state dynamics of LRMs, introduce Dynamic Spectral Dispersion (DSD) metric to quantify latent dynamics heterogeneity, and propose ReLaX paradigm that explicitly incorporates latent dynamics to regulate exploration-exploitation during policy optimization.

Result: Comprehensive experiments across multimodal and text-only reasoning benchmarks show ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.

Conclusion: Latent dynamics analysis provides effective mechanism for regulating exploration-exploitation tradeoff in RLVR, preventing entropy collapse and improving reasoning performance in LRMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [337] [Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast](https://arxiv.org/abs/2512.06350)
*Nghi Truong,Phanish Puranam,zgecan Ko莽ak*

Main category: cs.CY

TL;DR: The paper analyzes AI risk debates between "doomer" and "boomer" perspectives, finding disagreements stem from causal premises about system design vs emergence and applicability of past theories, not moral values.


<details>
  <summary>Details</summary>
Motivation: To understand why deep societal divisions persist in AI debates despite shared interests in benefiting humanity and avoiding catastrophes, by analyzing the reasoning chains behind different perspectives on AI risks.

Method: Parsing differences between "doomer" and "boomer" perspectives into definitional, factual, causal, and moral premises; using an ensemble of LLMs to analyze textual data and identify key points of contention at scale.

Result: Found that existential risk (X-risk) disagreements stem from causal premises about design vs emergence in complex systems, while employment risk (E-risk) disagreements involve causal premises about applicability of past theories (evolution) vs their inapplicability (revolution). Both share no significant moral value disagreements and relate to differing views on bounded human rationality.

Conclusion: The analytical approach using LLM ensembles can identify key contention points in public risk debates across domains, revealing that AI risk disagreements are fundamentally about causal reasoning rather than moral values.

Abstract: The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the "doomer" and "boomer" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk ("X-risk") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks ("E-risks") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [338] [An Index-based Approach for Efficient and Effective Web Content Extraction](https://arxiv.org/abs/2512.06641)
*Yihan Chen,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.IR

TL;DR: IWCE reframes web content extraction from slow generative token-by-token processing into efficient discriminative index prediction, enabling rapid query-relevant extraction by predicting positional indices of relevant content in structured HTML segments.


<details>
  <summary>Details</summary>
Motivation: Web agents consume massive web pages with low signal density, creating a context management challenge. Existing extraction methods are inadequate: generative models are slow, rule-based heuristics lack adaptability, and chunk-and-rerank methods ignore webpage structure.

Method: Index-based Web Content Extraction (IWCE) partitions HTML into structure-aware, addressable segments and extracts only the positional indices of content relevant to a given query, reframing extraction from token-by-token generation to efficient discriminative index prediction.

Result: The method improves QA accuracy in RAG systems and outperforms existing works in both accuracy and speed for main content extraction and query-relevant extraction scenarios, effectively bridging LLMs with webpages.

Conclusion: IWCE provides an effective and efficient solution for web content extraction that decouples extraction latency from content length, enabling rapid query-relevant extraction for web agents and RAG pipelines.

Abstract: As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [339] [XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association](https://arxiv.org/abs/2512.06757)
*Zhihua Fang,Shumei Tao,Junxu Wang,Liang He*

Main category: cs.SD

TL;DR: XM-ALIGN is a unified cross-modal embedding alignment framework that combines explicit and implicit alignment mechanisms with MSE loss and data augmentation to improve face-voice verification performance across heard and unheard languages.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of cross-modal verification between face and voice modalities, particularly for both "heard" and "unheard" languages, requiring robust alignment between different modalities.

Method: Extracts feature embeddings from face and voice encoders, jointly optimizes them using a shared classifier, employs mean squared error (MSE) as embedding alignment loss, and applies data augmentation strategies during training.

Result: Demonstrates superior performance on the MAV-Celeb dataset for cross-modal verification tasks.

Conclusion: The proposed XM-ALIGN framework effectively improves cross-modal verification performance through combined explicit and implicit alignment mechanisms, with code to be released publicly.

Abstract: This paper introduces our solution, XM-ALIGN (Unified Cross-Modal Embedding Alignment Framework), proposed for the FAME challenge at ICASSP 2026. Our framework combines explicit and implicit alignment mechanisms, significantly improving cross-modal verification performance in both "heard" and "unheard" languages. By extracting feature embeddings from both face and voice encoders and jointly optimizing them using a shared classifier, we employ mean squared error (MSE) as the embedding alignment loss to ensure tight alignment between modalities. Additionally, data augmentation strategies are applied during model training to enhance generalization. Experimental results show that our approach demonstrates superior performance on the MAV-Celeb dataset. The code will be released at https://github.com/PunkMale/XM-ALIGN.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [340] [ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment](https://arxiv.org/abs/2512.06196)
*Charlie Masters,Marta Grzekiewicz,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: ARCANE is a framework that uses natural-language rubrics (weighted sets of verifiable criteria) for interpretable, test-time adaptive alignment of LLM agents in long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: As LLM agents are deployed to long-horizon tasks, maintaining alignment with stakeholder preferences becomes critical. Current approaches lack interpretability (making it hard for stakeholders to understand/audit objectives) and cannot adapt to preference shifts at interaction time without retraining.

Method: Frames alignment as a multi-agent collaboration problem, representing stakeholder preferences as natural-language rubrics generated on-the-fly from task context. Uses a regularized Group-Sequence Policy Optimization (GSPO) procedure for rubric learning, formulated as a reconstruction problem inspired by utility theory.

Result: Evaluated on 219 labeled rubrics from GDPVal benchmark on challenging multi-step reasoning and tool use tasks. Learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining.

Conclusion: Rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems, balancing interpretability, faithfulness, and computational efficiency.

Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.

</details>


### [341] [On measuring grounding and generalizing grounding problems](https://arxiv.org/abs/2512.06205)
*Daniel Quigley,Eric Maynard*

Main category: cs.AI

TL;DR: The paper reframes the symbol grounding problem from a binary judgment into a multi-dimensional audit framework with six desiderata, applies it to analyze four grounding modes and three case studies, and provides a common technical framework for interdisciplinary investigation of meaning.


<details>
  <summary>Details</summary>
Motivation: To move beyond the simplistic binary view of symbol grounding (grounded vs ungrounded) and provide a more nuanced, operational framework that allows systematic evaluation of different approaches to meaning representation across multiple dimensions.

Method: Proposes a multi-dimensional audit framework with six desiderata evaluated through tuples (context, meaning type, threat model, reference distribution): authenticity, preservation, faithfulness (correlational & etiological), robustness, and compositionality. Applies this framework to analyze four grounding modes (symbolic, referential, vectorial, relational) and three case studies.

Result: Analysis shows: model-theoretic semantics achieves exact composition but lacks etiological warrant; LLMs show correlational fit and local robustness for linguistic tasks but lack selection-for-success on world tasks without grounded interaction; human language meets all desiderata under strong authenticity through evolutionary/developmental acquisition.

Conclusion: By operationalizing philosophical inquiry about representation, the framework provides a common language and technical toolkit for philosophers, computer scientists, linguists, and mathematicians to systematically investigate grounding and meaning across different approaches and domains.

Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.

</details>


### [342] [Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression](https://arxiv.org/abs/2512.06393)
*Qiming Bao,Xiaoxuan Fu*

Main category: cs.AI

TL;DR: LLMs show perfect accuracy on base logical tasks and strong invariance to semantic-preserving transformations, but fail dramatically when essential rules are removed or contradictions are introduced, revealing fundamental brittleness in logical reasoning.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLMs' generalization capabilities to structural perturbations in logical contexts, moving beyond standard task performance to understand reasoning reliability under stress conditions.

Method: Developed a controlled evaluation framework with four stress tests: rule deletion (redundant vs essential), contradictory evidence injection, logic-preserving rewrites using equivalence laws, and multi-law equivalence stacking. Tested on BERT, Qwen2, and LLaMA-like models.

Result: All models achieved perfect accuracy on base tasks and showed full generalization to redundant rule deletion and all equivalence-based rewrites. However, they failed sharply under essential rule deletion (dropping to 25% accuracy) and collapsed completely with contradictions (0% accuracy).

Conclusion: LLMs possess stable invariance to semantic-preserving logical transformations but remain fundamentally brittle to missing or conflicting evidence. The framework provides a diagnostic tool for isolating reasoning failure modes and highlights persistent gaps in logical generalization abilities.

Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.

</details>


### [343] [Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents](https://arxiv.org/abs/2512.06716)
*Zhibo Liang,Tianze Hu,Zaiye Chen,Mingjie Tang*

Main category: cs.AI

TL;DR: Proposes Cognitive Control Architecture (CCA) - a holistic defense framework against Indirect Prompt Injection attacks on LLM agents, achieving uncompromised security with efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents are vulnerable to Indirect Prompt Injection attacks that hijack agent behavior by polluting external information sources. Existing defenses are fragmented and force unacceptable trade-offs between security, functionality, and efficiency.

Method: Cognitive Control Architecture (CCA) with two synergistic pillars: (1) proactive control-flow and data-flow integrity enforcement via pre-generated "Intent Graph", and (2) "Tiered Adjudicator" that initiates deep reasoning with multi-dimensional scoring upon deviation detection.

Result: Experiments on AgentDojo benchmark show CCA effectively withstands sophisticated attacks that challenge other advanced defense methods, achieving uncompromised security with notable efficiency and robustness.

Conclusion: CCA reconciles the multi-dimensional trade-off between security, functionality, and efficiency in LLM agent defenses against Indirect Prompt Injection attacks, providing full-lifecycle cognitive supervision.

Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.

</details>


### [344] [ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems](https://arxiv.org/abs/2512.06721)
*Bufang Yang,Lilin Xu,Liekang Zeng,Yunqi Guo,Siyang Jiang,Wenrui Lu,Kaiwei Liu,Hancheng Xiang,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: ProAgent is an end-to-end proactive LLM agent system that uses sensory contexts and LLM reasoning to provide proactive assistance without explicit user instructions, achieving significant improvements over reactive baselines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents follow a reactive paradigm requiring explicit user instructions, which increases physical and cognitive workload. There's a need for proactive agents that can anticipate user needs without explicit prompting.

Method: ProAgent uses proactive-oriented context extraction with tiered perception to continuously sense environments and derive hierarchical contexts (sensory + persona cues), then employs a context-aware proactive reasoner to map contexts to user needs and tool calls.

Result: ProAgent achieves 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable user satisfaction improvements over state-of-the-art baselines in real-world testbed, public dataset, and user study evaluations.

Conclusion: ProAgent represents a significant step toward proactive assistants by demonstrating superior performance in anticipating user needs and providing timely assistance without explicit instructions, implemented on AR glasses with edge server architecture.

Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.

</details>


### [345] [A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy](https://arxiv.org/abs/2512.07109)
*Miguel Ingram,Arthur Joseph Merritt*

Main category: cs.AI

TL;DR: Researchers developed a 9-category taxonomy for 400 ARC tasks with 97.5% accuracy, used it to analyze Transformer limitations, revealing a Compositional Gap where local pattern recognition succeeds but global synthesis fails, showing architectural suitability bounds performance more than training data.


<details>
  <summary>Details</summary>
Motivation: To formally define task relatedness in ARC (Abstraction and Reasoning Corpus) and address Hodel et al.'s call for better understanding of neural architectures' limitations on compositional reasoning tasks.

Method: Created a 9-category taxonomy of 400 ARC tasks validated via rule-based code analysis (97.5% accuracy). Trained CNN on raw grid pixels to validate visual coherence. Fine-tuned 1.7M-parameter Transformer on 302 tasks to analyze performance patterns. Applied taxonomy to analyze ARC-AGI-2 test set and Li et al.'s ViTARC study.

Result: Revealed Compositional Gap: 69.5% of tasks achieve >80% cell accuracy but <10% grid accuracy. Found Neural Affinity Ceiling Effect where performance is bounded by architectural suitability. Low-affinity tasks achieve 51.9% vs 77.7% for high-affinity tasks. 35.3% of tasks show low neural affinity for Transformers.

Conclusion: Progress requires hybrid architectures with affinity-aligned modules rather than just more data or training. The taxonomy enables precise diagnosis of architectural limitations and provides evidence that current neural architectures have fundamental gaps in compositional reasoning.

Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,

</details>


### [346] [PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations](https://arxiv.org/abs/2512.07179)
*Wonbeen Lee,Channyoung Lee,Junho Sohn,Hansam Cho*

Main category: cs.AI

TL;DR: PICKT model addresses limitations in Knowledge Tracing by using a knowledge map to handle multiple input data types and solve cold start problems for new students and questions, demonstrating practical applicability in real-world ITS environments.


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Tracing models have limitations including restricted input formats, cold start problems with new students/questions, and insufficient stability for real-world deployment in Intelligent Tutoring Systems.

Method: Proposes PICKT model with a knowledge map that structures relationships among concepts using question and concept text information, enabling effective knowledge tracing even in cold start situations.

Result: Experiments show excellent performance and practicality, with significant improvements over existing models for new student enrollment and new question addition cold start challenges.

Conclusion: PICKT provides crucial theoretical and technical foundation for practical implementation of next-generation ITS by addressing key limitations and validating stability in real-world environments.

Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.

</details>


### [347] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

TL;DR: ReasonBENCH is the first benchmark to quantify instability in LLM reasoning, showing that most reasoning strategies have high variance in performance and cost, compromising reproducibility.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation practices focus on single-run accuracy while ignoring the intrinsic uncertainty from stochastic decoding, creating blind spots about whether reported performance is stable, reproducible, or cost-consistent.

Method: Created ReasonBENCH with: (i) modular evaluation library standardizing reasoning frameworks, models, and tasks; (ii) multi-run protocol reporting statistically reliable metrics for quality and cost; (iii) public leaderboard for variance-aware reporting.

Result: Most reasoning strategies and models exhibit high instability - strategies with similar average performance can have confidence intervals up to 4x wider, and top-performing methods often have higher and less stable costs.

Conclusion: Reproducibility is a critical dimension for reliable LLM reasoning; ReasonBENCH provides foundation for future reasoning methods and uncertainty quantification techniques.

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


### [348] [Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients](https://arxiv.org/abs/2512.06990)
*Krishna Arun,Moinak Bhattachrya,Paras Goel*

Main category: cs.AI

TL;DR: AI system for Glioblastoma diagnosis and treatment planning using sequential classification models for diagnosis and RL-based generative models for treatment optimization, achieving computational efficiency and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the lack of AI support for treating Glioblastoma Multiforme (GBM), the deadliest human cancer with only 5.1% five-year survival rate, by developing an end-to-end solution for both diagnosis and treatment planning.

Method: Diagnosis: Sequential decision-making framework with 4 classification models (CNNs and SVM) that progressively classify brain scans into specific categories. Treatment: RL system with 3 generative models - diffusion model for resection prediction, Spatio-Temporal Vision Transformer for radiotherapy progression, diffusion model for chemotherapy effects, plus survival rate calculator CNN and feedback loop using proximal policy optimization.

Result: 22.28x reduction in computing costs, 113-hour decrease in tumor progression inference time, 2.9% improvement in DICE scores, projected 0.9% increase in survival rates potentially saving 2,250 lives.

Conclusion: The developed AI system provides an effective end-to-end solution for GBM management, demonstrating significant improvements in computational efficiency, inference speed, and accuracy, with potential to save thousands of lives through modest survival rate improvements.

Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.

</details>


### [349] [A Geometric Unification of Concept Learning with Concept Cones](https://arxiv.org/abs/2512.07355)
*Alexandre Rocchi--Henry,Thomas Fel,Gianni Franchi*

Main category: cs.AI

TL;DR: The paper unifies supervised Concept Bottleneck Models (CBMs) and unsupervised Sparse Autoencoders (SAEs) through a shared geometric framework where both learn concept cones in activation space, differing only in how they select these cones.


<details>
  <summary>Details</summary>
Motivation: Two separate traditions of interpretability (CBMs and SAEs) have evolved independently but rarely communicate. CBMs use human supervision to define concepts, while SAEs discover emergent concepts through sparse coding. The authors aim to bridge these paradigms by showing they share the same underlying geometric structure.

Method: The paper demonstrates that both CBMs and SAEs instantiate linear directions in activation space whose nonnegative combinations form a concept cone. They propose a containment framework where CBMs provide reference geometries and SAEs are evaluated by how well their learned cones approximate or contain CBM cones. This yields quantitative metrics linking SAE inductive biases to concept emergence.

Result: The authors uncover a "sweet spot" in both sparsity and expansion factor that maximizes geometric and semantic alignment with CBM concepts. They provide principled metrics to measure SAE progress and assess alignment with human-interpretable concepts.

Conclusion: The work unifies supervised and unsupervised concept discovery through a shared geometric framework, creating an operational bridge between CBMs and SAEs. This provides tools to quantitatively evaluate how well discovered concepts align with plausible human concepts.

Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [350] [Small Language Models Reshape Higher Education: Courses, Textbooks, and Teaching](https://arxiv.org/abs/2512.06001)
*Jian Zhang,Jia Shao*

Main category: physics.ed-ph

TL;DR: MiniLMs offer better educational integration than LLMs for higher education due to accuracy and efficiency; applied to Atmospheric Physics with specialized corpus and image database to create dynamic, interdisciplinary course redesign.


<details>
  <summary>Details</summary>
Motivation: LLMs have limitations for higher education adoption due to inaccuracies and high computational costs, while MiniLMs offer lightweight, precise retrieval capabilities better suited for professional education needs requiring accurate, reliable knowledge.

Method: Built specialized corpus of 550,000+ PDFs from 130+ Earth/environmental science journals, extracting 100M+ sentence-level corpus and 3M+ academic images; organized into high-dimensional vector library using MiniLMs for precise retrieval; redesigned Atmospheric Physics course as interdisciplinary system with dynamic digital resource library and question-based learning pathways.

Result: Created comprehensive educational framework transforming Atmospheric Physics from static text to dynamic digital resource library, breaking traditional disciplinary boundaries and shifting from passive knowledge transfer to active cognitive development.

Conclusion: MiniLM-driven approach demonstrates specific avenue for "AI for education" by enabling precise, efficient educational content retrieval and interdisciplinary course redesign that better serves higher education's need for accuracy and reliability.

Abstract: While large language models (LLMs) have introduced novel paradigms in science and education, their adoption in higher education is constrained by inherent limitations. These include a tendency to produce inaccuracies and high computational requirements, which compromise the strict demands for accurate and reliable knowledge essential in higher education. Small language models (MiniLMs), by contrast, offer distinct advantages in professional education due to their lightweight nature and precise retrieval capabilities. This research takes "Atmospheric Physics" as an example. We established a specialized corpus and image repository by gathering over 550,000 full-text PDFs from over 130 international well-respected journals in Earth and environmental science. From this collection, we extracted over 100 million high-quality sentence-level corpus and more than 3 million high-resolution academic images. Using MiniLMs, these resources were organized into a high-dimensional vector library for precise retrieval and efficient utilization of extensive educational content. Consequently, we systematically redesigned the courses, textbooks, and teaching strategies for "Atmospheric Physics" based on MiniLMs. The course is designed as a "interdisciplinary-frontier" system, breaking down traditional boundaries between atmospheric science, space science, hydrology, and remote sensing. Teaching materials are transformed from static, lagging text formats into a dynamic digital resource library powered by MiniLM. For teaching methods, we have designed a question-based learning pathway. This paradigm promotes a shift from passive knowledge transfer to active cognitive development. Consequently, this MiniLM-driven "Atmospheric Physics" course demonstrates a specific avenue for "AI for education".

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [351] [GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers](https://arxiv.org/abs/2512.06147)
*Hochul Hwang,Soowan Yang,Jahir Sadik Monon,Nicholas A Giudice,Sunghoon Ivan Lee,Joydeep Biswas,Donghyun Kim*

Main category: cs.RO

TL;DR: GuideNav: A vision-only teach-and-repeat navigation system for blind/low-vision users, inspired by guide dog training methods, that achieves kilometer-scale outdoor route following without expensive sensors.


<details>
  <summary>Details</summary>
Motivation: There's a gap in robot navigation design specifically informed by the needs of blind and low-vision (BLV) individuals. Most existing research is user-centric but lacks direct references for navigation system design.

Method: 1) Comprehensive human study with 26 guide dog handlers, 4 cane users, 9 trainers, and 15+ hours of observation; 2) Development of GuideNav - a vision-only system that learns routes from sighted demonstrations, builds topological representations, uses visual place recognition with temporal filtering, and employs relative pose estimation for navigation.

Result: GuideNav successfully achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite scene variations. User study with 3 guide dog handlers and 1 trainer confirmed system feasibility as the first quadruped system to retrieve paths comparable to guide dogs.

Conclusion: GuideNav demonstrates that vision-only teach-and-repeat navigation can effectively assist BLV individuals, bridging the gap between user-centered research and practical robot navigation design while avoiding costly sensors like LiDAR.

Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.

</details>


### [352] [MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment](https://arxiv.org/abs/2512.06628)
*Ruicheng Zhang,Mingyang Zhang,Jun Zhou,Zhangrui Guo,Xiaofan Liu,Zunnan Xu,Zhizhou Zhong,Puxin Yan,Haocheng Luo,Xiu Li*

Main category: cs.RO

TL;DR: MIND-V is a hierarchical framework that generates physically plausible, long-horizon robotic manipulation videos using cognitive-inspired reasoning and reinforcement learning with physical foresight rewards.


<details>
  <summary>Details</summary>
Motivation: Current video generation models for robotic manipulation are limited to short clips of simple actions and rely on manually defined trajectories, lacking diverse, long-horizon data for embodied imitation learning.

Method: Three-component hierarchical framework: 1) Semantic Reasoning Hub (SRH) for task planning using vision-language models, 2) Behavioral Semantic Bridge (BSB) translating instructions to domain-invariant representations, 3) Motor Video Generator (MVG) for conditional rendering. Uses Staged Visual Future Rollouts for test-time optimization and GRPO reinforcement learning with Physical Foresight Coherence (PFC) reward based on V-JEPA world model.

Result: MIND-V achieves state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.

Conclusion: The framework successfully bridges high-level reasoning with pixel-level synthesis to generate physically plausible and logically coherent long-horizon manipulation videos, addressing data scarcity in embodied imitation learning.

Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.

</details>


### [353] [Dynamic Visual SLAM using a General 3D Prior](https://arxiv.org/abs/2512.06868)
*Xingguang Zhong,Liren Jin,Marija Popovi,Jens Behley,Cyrill Stachniss*

Main category: cs.RO

TL;DR: A monocular visual SLAM system that robustly estimates camera poses in dynamic natural environments by combining geometric patch-based bundle adjustment with feed-forward reconstruction models to filter dynamic regions and handle scale ambiguities.


<details>
  <summary>Details</summary>
Motivation: Camera pose estimation and 3D reconstruction are crucial for robotics, visualization, and AR applications, but are challenging in dynamic natural environments where scene dynamics degrade pose estimation accuracy.

Method: Proposes a novel monocular visual SLAM system that leverages complementary strengths of geometric patch-based online bundle adjustment and feed-forward reconstruction models. Uses a feed-forward reconstruction model to filter out dynamic regions precisely, while utilizing its depth prediction to enhance robustness of patch-based visual SLAM. Aligns depth prediction with estimated patches from bundle adjustment to handle inherent scale ambiguities of batch-wise application of the feed-forward reconstruction model.

Result: The system can robustly estimate camera poses in dynamic scenes by effectively filtering dynamic regions and handling scale ambiguities through the alignment of depth predictions with bundle adjustment patches.

Conclusion: The proposed approach successfully addresses the challenge of camera pose estimation in dynamic natural environments by combining geometric and learning-based methods, enabling reliable SLAM performance in scenes with significant dynamics.

Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.

</details>


### [354] [Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.06951)
*Ilia Larchenko,Gleb Zarin,Akash Karnatak*

Main category: cs.RO

TL;DR: A vision-action policy won 1st place in BEHAVIOR Challenge 2025 by achieving 26% q-score across 50 diverse household tasks using innovations in flow matching and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of the BEHAVIOR Challenge - a large-scale benchmark requiring bimanual manipulation, navigation, and context-aware decision making across 50 diverse long-horizon household tasks in photo-realistic simulation.

Method: Builds on Pi0.5 architecture with innovations: correlated noise for flow matching (improves training efficiency, enables correlation-aware inpainting), learnable mixed-layer attention, System 2 stage tracking for ambiguity resolution, multi-sample flow matching for reduced variance, and inference with action compression and challenge-specific correction rules.

Result: Achieved 1st place in BEHAVIOR Challenge 2025 with 26% q-score across all 50 tasks on both public and private leaderboards.

Conclusion: The proposed vision-action policy with correlated noise flow matching and attention mechanisms successfully addresses complex household tasks, demonstrating state-of-the-art performance in the challenging BEHAVIOR benchmark.

Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.

</details>


### [355] [VideoVLA: Video Generators Can Be Generalizable Robot Manipulators](https://arxiv.org/abs/2512.06963)
*Yichao Shen,Fangyun Wei,Zhiying Du,Yaobo Liang,Yan Lu,Jiaolong Yang,Nanning Zheng,Baining Guo*

Main category: cs.RO

TL;DR: VideoVLA transforms video generation models into robotic manipulators that jointly predict action sequences and future visual outcomes, enabling strong generalization in novel tasks and environments.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models have limited generalization to novel tasks, objects, and settings. There's a need for robotic manipulation systems that can better adapt to open-world environments and advance toward artificial general intelligence.

Method: VideoVLA uses a multi-modal Diffusion Transformer built on pre-trained video generative models. It takes language instructions and images as input, then jointly models video, language, and action modalities to predict both action sequences and future visual outcomes.

Result: Experiments show that high-quality imagined futures correlate with reliable action predictions and task success. VideoVLA demonstrates strong generalization capabilities, including imitating other embodiments' skills and handling novel objects.

Conclusion: The dual-prediction strategy of forecasting both actions and their visual consequences represents a paradigm shift in robot learning, unlocking new generalization capabilities in manipulation systems through visual imagination.

Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.

</details>


### [356] [Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07130)
*Zebin Xing,Yupeng Zheng,Qichao Zhang,Zhixing Ding,Pengxuan Yang,Songen Gu,Zhongpu Xia,Dongbin Zhao*

Main category: cs.RO

TL;DR: Mimir is a hierarchical dual-system framework for end-to-end autonomous driving that improves robustness by estimating goal point uncertainty with Laplace distribution and speeds up inference with multi-rate guidance.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end autonomous driving systems are constrained by inaccurate high-level guidance signals and computational overhead from complex guidance modules, limiting their performance and efficiency.

Method: Proposes Mimir with two key innovations: (1) estimates goal point uncertainty using Laplace distribution instead of deterministic modeling for enhanced robustness, (2) introduces multi-rate guidance mechanism that predicts extended goal points in advance to overcome slow inference speed.

Result: Achieves 20% improvement in driving score EPDMS on Navhard and Navtest benchmarks, with 1.6x improvement in high-level module inference speed without compromising accuracy.

Conclusion: Mimir demonstrates superior performance and efficiency in autonomous driving by addressing uncertainty estimation and computational bottlenecks through its hierarchical dual-system framework with Laplace distribution modeling and multi-rate guidance.

Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving

</details>

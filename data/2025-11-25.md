<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.CV](#cs.CV) [Total: 320]
- [eess.IV](#eess.IV) [Total: 5]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.LG](#cs.LG) [Total: 38]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering](https://arxiv.org/abs/2511.17559)
*Gyubok Lee,Woosog Chay,Edward Choi*

Main category: cs.CL

TL;DR: SCARE is a benchmark for evaluating post-hoc safety layers in EHR question answering systems, focusing on classifying question answerability and verifying/correcting SQL queries before execution to ensure safe clinical deployment.


<details>
  <summary>Details</summary>
Motivation: Current text-to-SQL models for EHRs lack proper safety verification mechanisms, and incorrect SQL queries can jeopardize patient care in clinical environments. There's no unified benchmark for evaluating post-hoc verification methods.

Method: Created SCARE benchmark with 4,200 triples of questions, candidate SQL queries, and expected outputs from MIMIC-III, MIMIC-IV, and eICU databases. Used SQL queries generated by 7 different text-to-SQL models to ensure realistic evaluation.

Result: Benchmarked various approaches including two-stage methods and agentic frameworks. Revealed a critical trade-off between question classification accuracy and SQL error correction performance.

Conclusion: SCARE fills an important gap in EHR QA safety evaluation and highlights key challenges that need to be addressed for safe deployment of text-to-SQL systems in clinical settings.

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.

</details>


### [2] [$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving](https://arxiv.org/abs/2511.17560)
*Yuechi Zhou,Yi Su,Jianxin Zhang,Juntao Li,Qingrong Xia,Zhefeng Wang,Xinyu Duan,Baoxing Huai*

Main category: cs.CL

TL;DR: A³ is a novel KV cache fusion algorithm that precomputes and selectively fuses KV cache of text chunks based on their relevance to questions, achieving better performance while reducing decoding latency.


<details>
  <summary>Details</summary>
Motivation: Current KV cache reuse methods suffer from performance degradation because recomputed tokens often don't align with the most relevant context segments, hindering proper updates to critical contextual representations.

Method: Proposed A³ algorithm precomputes and selectively fuses KV cache of text chunks based on their relevance to the question, enabling accurate integration with minimal computational overhead.

Result: Extensive experiments show A³ achieves the best task performance compared to four baselines while reducing time-to-first-token (TTFT) by 2x.

Conclusion: A³ effectively addresses KV cache reuse limitations by attention-aware selective fusion, improving both performance and efficiency in long-context LLM processing.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\textbf{A}$ttention-$\textbf{A}$ware $\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\times$.

</details>


### [3] [LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models](https://arxiv.org/abs/2511.17561)
*Huimin Ren,Yan Liang,Baiqiao Su,Chaobo Sun,Hengtong Lu,Kaike Zhang,Chen Wei*

Main category: cs.CL

TL;DR: LexInstructEval is a new benchmark and evaluation framework for assessing LLMs' ability to follow fine-grained lexical instructions, addressing limitations of current evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating LLM instruction following rely on subjective human evaluation or biased automated systems, while existing programmatic benchmarks lack expressiveness for testing intricate compositional constraints.

Method: Built on a formal rule-based grammar that deconstructs instructions into <Procedure, Relation, Value> triplets, with systematic dataset generation through a multi-stage human-in-the-loop pipeline and objective verification via transparent programmatic engine.

Result: The framework enables systematic evaluation of LLMs' lexical instruction following capabilities with objective verification, addressing biases and unreliability in current methods.

Conclusion: LexInstructEval provides an improved evaluation framework for LLM controllability and reliability, with released dataset and open-source tools to facilitate further research.

Abstract: The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical <Procedure, Relation, Value> triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.

</details>


### [4] [ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector](https://arxiv.org/abs/2511.17562)
*Wei Tian,YuhaoZhou*

Main category: cs.CL

TL;DR: ChineseErrorCorrector3-4B is a unified model based on Qwen3-4B that achieves state-of-the-art performance in both Chinese spelling correction (CSC) and grammatical error correction (CGC) across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To develop a unified model that can handle both spelling and grammatical error correction in Chinese text, addressing the need for comprehensive text correction capabilities.

Method: Built upon Qwen3-4B foundation model, creating a unified architecture for Chinese error correction that handles both spelling and grammatical errors simultaneously.

Result: Achieved state-of-the-art performance with significantly higher F1 and F0.5 scores on authoritative benchmarks including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC, ranking first in both spelling and grammatical error correction tasks.

Conclusion: ChineseErrorCorrector3-4B demonstrates outstanding performance as a unified model for Chinese text correction, surpassing existing publicly available models and establishing new state-of-the-art results in both spelling and grammatical error correction.

Abstract: This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.

</details>


### [5] [Generative Caching for Structurally Similar Prompts and Responses](https://arxiv.org/abs/2511.17565)
*Sarthak Chakraborty,Suman Nath,Xuchao Zhang,Chetan Bansal,Indranil Gupta*

Main category: cs.CL

TL;DR: Generative cache for LLMs that produces variation-aware responses for structurally similar prompts by identifying reusable patterns and synthesizing customized outputs.


<details>
  <summary>Details</summary>
Motivation: Address limitations of exact prompt matching (fails on structurally similar prompts) and semantic caching (may produce incorrect responses by ignoring critical differences) in repeatable workflows and agentic settings.

Method: Identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests.

Result: Achieves 83% cache hit rate with minimal incorrect hits, improves cache hit rate by ~20% and reduces end-to-end execution latency by ~34% in agentic workflows compared to standard prompt matching.

Conclusion: The proposed generative cache method effectively handles structurally similar prompts in LLM workflows, significantly improving performance while maintaining accuracy.

Abstract: Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \ourmethod{} achieves 83\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\sim$20\% and reduces end-to-end execution latency by $\sim$34\% compared to standard prompt matching.

</details>


### [6] [Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs](https://arxiv.org/abs/2511.17572)
*Patrick Gerard,Aiden Chang,Svitlana Volkova*

Main category: cs.CL

TL;DR: LLMs aligned to specific communities maintain stable behavioral patterns even after event knowledge deletion, showing alignment encodes generalizable behaviors beyond surface mimicry.


<details>
  <summary>Details</summary>
Motivation: To determine if aligned LLMs exhibit generalizable community-specific behavioral patterns or simply recall training data patterns when handling uncertainty.

Method: A framework using targeted deletion of event knowledge with multiple validation probes, tested on Russian-Ukrainian military discourse and U.S. partisan Twitter data.

Result: Even after aggressive fact removal, aligned LLMs maintained stable, community-specific behavioral patterns for handling uncertainty.

Conclusion: Alignment encodes structured, generalizable behaviors beyond surface mimicry, providing a systematic way to detect persistent behavioral biases under ignorance for safer LLM deployments.

Abstract: When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.

</details>


### [7] [Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models](https://arxiv.org/abs/2511.17575)
*Vladimir Berman*

Main category: cs.CL

TL;DR: A simple non-linguistic model of text as random sequences of letters and spaces generates geometric word length distribution, closed-form vocabulary growth, critical word length, and Zipf's law purely from combinatorics.


<details>
  <summary>Details</summary>
Motivation: To provide a structurally grounded null model for natural language statistics and LLM token patterns, showing that Zipf-like distributions can arise from basic combinatorics without linguistic organization.

Method: Model text as independent draws from finite alphabet plus space symbol, define words as maximal non-space blocks, use coupon-collector arguments and combinatorial analysis.

Result: Word lengths follow geometric distribution; closed-form expressions for word counts and vocabulary growth; critical length k* where word types transition; Zipf's law with explicit exponent from alphabet size and space probability.

Conclusion: Zipf-like patterns can emerge purely from random text structure without linguistic optimization, providing a baseline to identify phenomena requiring deeper explanation beyond combinatorial effects.

Abstract: We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.
  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.

</details>


### [8] [Computational frame analysis revisited: On LLMs for studying news coverage](https://arxiv.org/abs/2511.17746)
*Sharaj Kunjar,Alyssa Hasegawa Smith,Tyler R Mckenzie,Rushali Mohbe,Samuel V Scarpino,Brooke Foucault Welles*

Main category: cs.CL

TL;DR: Generative LLMs like GPT and Claude are less effective for media frame analysis than manual coding and sometimes even smaller language models, requiring human validation and a pluralistic approach.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of generative LLMs for media frame analysis compared to traditional computational methods and manual coding procedures.

Method: Systematic evaluation using a novel gold standard dataset developed through studying six months of US Mpox epidemic news coverage, comparing generative LLMs against bag-of-words models, encoder-only transformers, and manual coding.

Result: Generative LLMs were consistently outperformed by manual coders and sometimes by smaller language models. Human validation was always necessary for appropriate model selection, and approach suitability depended on specific frame analysis tasks.

Conclusion: Endorses a methodologically pluralistic approach and provides a roadmap for computational frame analysis, suggesting researchers leverage the complementarity of different approaches used in tandem.

Abstract: Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.

</details>


### [9] [PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://arxiv.org/abs/2511.17808)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

TL;DR: PoETa v2 is the most extensive evaluation of LLMs for Portuguese, using a comprehensive benchmark with 40+ tasks to assess 20+ models, revealing performance gaps compared to English.


<details>
  <summary>Details</summary>
Motivation: LLMs show significant performance variations across linguistic and cultural contexts, highlighting the need for systematic evaluation in diverse languages like Portuguese.

Method: Introduced PoETa v2 benchmark with over 40 Portuguese tasks to evaluate more than 20 LLMs across different training scales and computational resources.

Result: Revealed how computational investment and language-specific adaptation impact Portuguese performance, while identifying performance gaps compared to equivalent English tasks.

Conclusion: PoETa v2 establishes foundational groundwork for future Portuguese language modeling research and evaluation, with the benchmark publicly available.

Abstract: Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.

</details>


### [10] [Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation](https://arxiv.org/abs/2511.17813)
*Scott Merrill,Shashank Srivastava*

Main category: cs.CL

TL;DR: A pipeline to transform Zoom recordings into speaker-attributed transcripts with metadata, enabling realistic simulation of multi-party deliberation using LLMs.


<details>
  <summary>Details</summary>
Motivation: Current ASR transcripts use anonymous speaker labels, preventing models from capturing consistent human behavior in multi-party deliberation simulations.

Method: Developed a reproducible pipeline to process public Zoom recordings, creating speaker-attributed transcripts with persona profiles and pragmatic action tags. Fine-tuned LLMs on this "action-aware" data to model specific participants.

Result: 67% reduction in perplexity and nearly doubled classifier-based performance metrics for speaker fidelity and realism. Human evaluations show simulations are often indistinguishable from real deliberations.

Conclusion: Provides a practical and scalable method for complex realistic civic simulations by enabling speaker-attributed modeling of deliberation behavior.

Abstract: Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.

</details>


### [11] [A superpersuasive autonomous policy debating system](https://arxiv.org/abs/2511.17854)
*Allen Roush,Devin Gonier,John Hines,Judah Goldfeder,Philippe Martin Wyder,Sanjay Basu,Ravid Shwartz Ziv*

Main category: cs.CL

TL;DR: DeepDebater is an AI system that can participate in and win full competitive policy debates using multi-agent workflows with LLMs, iterative retrieval from a large evidence corpus, and produces complete speech transcripts, cross-examinations, and rebuttals.


<details>
  <summary>Details</summary>
Motivation: Previous AI debate systems like IBM Project Debater were limited to simplified debate formats for lay audiences. The goal is to create a system capable of handling complex, evidence-based persuasion in unmodified competitive policy debates.

Method: Hierarchical architecture with specialized multi-agent workflows where LLM-powered agents collaborate and critique each other. Uses iterative retrieval, synthesis, and self-correction from OpenDebateEvidence corpus. Features live presentation pipeline with AI speech synthesis and talking-head videos.

Result: In preliminary evaluations, DeepDebater produces qualitatively superior argumentative components, consistently wins simulated rounds against human-authored cases, and is preferred by expert human debate coaches for arguments, evidence, and case construction.

Conclusion: DeepDebater demonstrates advanced autonomous debate capabilities, supports both AI-AI and human-AI hybrid operation, and represents significant progress in AI persuasion systems for complex policy debates.

Abstract: The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main

</details>


### [12] [Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction](https://arxiv.org/abs/2511.17908)
*Debashish Chakraborty,Eugene Yang,Daniel Khashabi,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: Conformal prediction enables coverage-controlled context filtering in RAG systems, reducing retained context by 2-3x while maintaining factual accuracy by preserving relevant evidence.


<details>
  <summary>Details</summary>
Motivation: LLM accuracy declines with long or noisy contexts that exceed effective attention span, and existing pre-generation filters lack statistical control over retained evidence.

Method: Use conformal prediction framework with embedding- and LLM-based scoring functions to filter irrelevant content while preserving recall of supporting evidence, tested on NeuCLIR and RAGTIME collections.

Result: Conformal filtering consistently meets target coverage, reduces retained context by 2-3x, and improves downstream factual accuracy (ARGUE F1) under strict filtering while remaining stable at moderate coverage.

Conclusion: Conformal prediction provides reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.

Abstract: Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.

</details>


### [13] [L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention](https://arxiv.org/abs/2511.17910)
*Yuliang Zhan,Xinyu Tang,Han Wan,Jian Li,Ji-Rong Wen,Hao Sun*

Main category: cs.CL

TL;DR: L2V-CoT is a training-free method that transfers Chain-of-Thought reasoning from LLMs to VLMs using latent intervention in the frequency domain, achieving superior performance without architectural alignment or training costs.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with multi-step reasoning tasks due to limited multimodal reasoning data, while existing CoT transfer methods require high training costs or architectural alignment.

Method: Uses Linear Artificial Tomography to show LLMs and VLMs share similar low-frequency CoT latent representations, then extracts and resamples low-frequency CoT representations from LLMs in frequency domain for dimension matching and latent injection into VLMs during inference.

Result: Extensive experiments show L2V-CoT consistently outperforms training-free baselines and even surpasses supervised methods.

Conclusion: The approach successfully transfers CoT reasoning from LLMs to VLMs without training or architectural changes, demonstrating the shared latent representations across different model architectures.

Abstract: Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.

</details>


### [14] [Towards Efficient LLM-aware Heterogeneous Graph Learning](https://arxiv.org/abs/2511.17923)
*Wenda Li,Tongya Zheng,Shunyu Liu,Yu Wang,Kaixuan Chen,Hanyang Yuan,Bingde Hu,Zujie Ren,Mingli Song,Gang Chen*

Main category: cs.CL

TL;DR: ELLA is an efficient LLM-aware framework for heterogeneous graphs that uses LLMs to encode complex relation semantics while reducing computational complexity from exponential to linear through hop-level relation graph transformers.


<details>
  <summary>Details</summary>
Motivation: Existing methods for modeling complex relation semantics in heterogeneous graphs are limited by predefined semantic dependencies and scarce supervised signals, while LLM-based approaches face computational complexity issues.

Method: Proposes LLM-aware Relation Tokenizer to encode multi-hop, multi-type relations, Hop-level Relation Graph Transformer to reduce complexity, and task-aware Chain-of-Thought prompts to bridge semantic gaps between pre-training and fine-tuning.

Result: Outperforms state-of-the-art methods on four heterogeneous graphs, scales to 13b-parameter LLMs, and achieves up to 4x speedup compared to existing LLM-based methods.

Conclusion: ELLA effectively addresses semantic and computational challenges in heterogeneous graph modeling by leveraging LLMs' reasoning capabilities while maintaining efficiency through innovative architectural design.

Abstract: Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.

</details>


### [15] [SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization](https://arxiv.org/abs/2511.17938)
*Jianghao Wu,Yasmeen George,Jin Ye,Yicheng Wu,Daniel F. Schmidt,Jianfei Cai*

Main category: cs.CL

TL;DR: SPINE is a token-selective test-time reinforcement learning framework that updates only high-entropy forking tokens in reasoning chains, preventing response collapse and improving performance across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current test-time reinforcement learning methods for LLMs/MLLMs suffer from distribution shift, lack of verifiable supervision, and collapse issues where majority-vote rewards dominate, responses shorten, and performance declines.

Method: Proposes SPINE framework that: (i) identifies and updates only high-entropy forking tokens (branch points) using forward-pass statistics, (ii) applies entropy-band regularization to sustain exploration when entropy is too low and suppress noisy supervision when it's too high, working with GRPO-style objectives.

Result: Across ten benchmarks spanning multimodal VQA, general/expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and providing more stable training dynamics on both LLM and MLLM backbones.

Conclusion: Aligning updates with chain-of-thought branch points is a simple, label-free mechanism for stable and effective test-time adaptation in reasoning models.

Abstract: Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.

</details>


### [16] [Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://arxiv.org/abs/2511.17946)
*Shuo Zhang,Fabrizio Gotti,Fengran Mo,Jian-Yun Nie*

Main category: cs.CL

TL;DR: The paper investigates whether lexical training-data coverage of questions and generated answers can serve as a complementary signal for detecting hallucinations in LLMs, finding modest gains when combined with log-probabilities.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs is a major challenge, and while prior work focused on model-internal signals, the connection between pretraining data exposure and hallucination remains underexplored. The authors aim to examine if data coverage itself can provide detection signals.

Method: Constructed scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve n-gram statistics for both prompts and model generations, then evaluated their effectiveness for hallucination detection across three QA benchmarks.

Result: Occurrence-based features are weak predictors alone but yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty.

Conclusion: Lexical coverage features provide a complementary signal for hallucination detection, suggesting that data exposure patterns can enhance existing detection methods.

Abstract: Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.

</details>


### [17] [MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok](https://arxiv.org/abs/2511.17955)
*Dat Thanh Nguyen,Nguyen Hung Lam,Anh Hoang-Thi Nguyen,Trong-Hop Do*

Main category: cs.CL

TL;DR: MTikGuard is a real-time multimodal system for detecting harmful content on TikTok, achieving 89.37% accuracy through dataset expansion and multimodal feature fusion.


<details>
  <summary>Details</summary>
Motivation: The rapid rise of TikTok among children and teenagers creates a need for effective moderation of harmful content, which traditional methods struggle with due to the platform's massive volume and real-time nature.

Method: Extended TikHarm dataset to 4,723 labeled videos, developed multimodal classification framework integrating visual, audio, and textual features, and built scalable streaming architecture using Apache Kafka and Apache Spark.

Result: Achieved state-of-the-art performance with 89.37% accuracy and 89.45% F1-score in harmful content detection.

Conclusion: The system demonstrates effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation.

Abstract: With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.

</details>


### [18] [Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets](https://arxiv.org/abs/2511.18054)
*Gowtham,Sai Rupesh,Sanjay Kumar,Saravanan,Venkata Chaithanya*

Main category: cs.CL

TL;DR: Blu-WERP is a novel data preprocessing pipeline that significantly improves LLM training data quality from Common Crawl WARC files, outperforming existing methods like DCLM and Fineweb across multiple model scales and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing preprocessing pipelines struggle to effectively remove noise and unstructured content from web-scale corpora, which is fundamental to LLM performance. High-quality training data is crucial for optimal model capabilities.

Method: Blu-WERP processes CC WARC dumps using advanced filtering and quality assessment mechanisms. The pipeline was evaluated using models with 150M to 1B parameters across nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning.

Result: Blu-WERP consistently achieved superior performance across all model scales. At 1B parameters, it showed 4.0% improvement over DCLM and 9.5% over Fineweb, with categorical improvements of 2.4% in World Knowledge, 6.2% in Language Understanding, and 4.2% in Commonsense Reasoning.

Conclusion: Blu-WERP establishes itself as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream performance while reducing computational cost, representing a practical advancement in data-centric AI.

Abstract: High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.

</details>


### [19] [GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set](https://arxiv.org/abs/2511.18146)
*Yomal De Mel,Nisansa de Silva*

Main category: cs.CL

TL;DR: GeeSanBhava is a high-quality Sinhala song comment dataset manually annotated with emotion labels using Russell's Valence-Arousal model, achieving strong inter-annotator agreement and good MLP model performance.


<details>
  <summary>Details</summary>
Motivation: To create a valuable annotated dataset for Sinhala NLP and address the challenges of emotion mapping from user-generated content, particularly for music emotion recognition in Sinhala language.

Method: Manual extraction and annotation of Sinhala YouTube song comments using Russell's Valence-Arousal model by three independent annotators, with machine learning models pre-trained on Sinhala news comments and optimized MLP architecture.

Result: High inter-annotator agreement (Fleiss kappa = 84.96%), distinct emotional profiles for different songs, and an optimized MLP model achieving ROC-AUC score of 0.887 with 256-128-64 neuron configuration.

Conclusion: The research provides a valuable annotated dataset and insights for future Sinhala NLP and music emotion recognition work, demonstrating effective emotion mapping from user comments.

Abstract: This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.

</details>


### [20] [Vector Arithmetic in Concept and Token Subspaces](https://arxiv.org/abs/2511.18162)
*Sheridan Feucht,Byron Wallace,David Bau*

Main category: cs.CL

TL;DR: LLMs use concept and token induction heads to create semantic and surface-level subspaces that enable accurate word analogy operations like parallelogram arithmetic.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs represent semantic and surface-level information about words through attention mechanisms, and leverage these representations for improved word analogy tasks.

Method: Transform hidden states using attention weights from concept induction heads (for semantic information) and token induction heads (for surface-level information), then perform parallelogram arithmetic operations on the resulting subspaces.

Result: Concept head transformations achieved 80% nearest-neighbor accuracy on word analogies vs 47% with raw hidden states. Token head transformations enabled accurate surface-level operations like "coding" - "code" + "dance" = "dancing".

Conclusion: Attention heads in LLMs create structured subspaces that disentangle semantic and surface-level word information, enabling precise analogy operations that outperform raw hidden state representations.

Abstract: In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that "Athens" - "Greece" + "China" = "Beijing". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like "coding" - "code" + "dance" = "dancing".

</details>


### [21] [Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models](https://arxiv.org/abs/2511.18177)
*Elias Lumer,Matt Melich,Olivia Zino,Elena Kim,Sara Dieter,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah,James A. Burke,Roberto Hernandez*

Main category: cs.CL

TL;DR: Systematic comparison shows vector-based agentic RAG with cross-encoder reranking and small-to-big chunk retrieval outperforms hierarchical node-based systems for financial document Q&A, achieving 68% win rate with minimal latency impact.


<details>
  <summary>Details</summary>
Motivation: Existing work lacks systematic comparison of vector vs non-vector RAG architectures for financial documents, and empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remains unclear.

Method: First systematic evaluation comparing vector-based agentic RAG (hybrid search + metadata filtering) against hierarchical node-based systems. Evaluated cross-encoder reranking for retrieval precision and small-to-big chunk retrieval for context completeness on 1,200 SEC filings using 150-question benchmark.

Result: Vector-based agentic RAG achieved 68% win rate over hierarchical systems with comparable latency (5.2s vs 5.98s). Cross-encoder reranking achieved 59% absolute improvement in MRR@5. Small-to-big retrieval achieved 65% win rate over baseline with only 0.2s additional latency.

Conclusion: Advanced RAG techniques significantly improve retrieval accuracy and answer quality for financial Q&A systems, with cost-performance tradeoffs to consider in production deployments.

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.

</details>


### [22] [Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems](https://arxiv.org/abs/2511.18194)
*Faheem Nizar,Elias Lumer,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: Agent-as-a-Graph retrieval improves agent selection in multi-agent systems by representing tools and agents as knowledge graphs, achieving significant performance gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing agent retrieval methods match queries against single agent descriptions, obscuring fine-grained tool capabilities and leading to suboptimal agent selection in multi-agent systems.

Method: Knowledge graph retrieval augmented generation approach representing tools and agents as nodes/edges, using vector search, type-specific weighted reciprocal rank fusion (wRRF) for reranking, and knowledge graph traversal.

Result: Achieved 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers on LiveMCPBenchmark, plus 2.4% improvements from wRRF optimizations.

Conclusion: Agent-as-a-Graph retrieval effectively addresses fine-grained tool capability matching and significantly improves agent selection performance in multi-agent systems.

Abstract: Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.

</details>


### [23] [From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation](https://arxiv.org/abs/2511.18259)
*Xiaochen Zheng,Alvaro Serra,Ilya Schneider Chernov,Maddalena Marchesi,Eunice Musvasva,Tatyana Y. Doktorova*

Main category: cs.CL

TL;DR: DiscoVerse is a multi-agent co-scientist system that enables reverse translation by semantically retrieving, linking, and synthesizing knowledge from large pharmaceutical archives, achieving high recall and moderate precision on real-world drug development data.


<details>
  <summary>Details</summary>
Motivation: Pharmaceutical R&D has accumulated vast archives of data from discontinued programs, but reusing this knowledge for reverse translation is often infeasible in practice despite its potential value.

Method: Developed DiscoVerse - a multi-agent co-scientist with semantic retrieval, cross-document linking, and auditable synthesis capabilities. Tested on 180 molecules from Roche archives covering 0.87B tokens and 40+ years of research, using blinded expert evaluation.

Result: Achieved near-perfect recall (≥0.99) with moderate precision (0.71-0.91) across 7 benchmark queries. Qualitative assessments showed faithful, source-linked synthesis of discontinuation rationale and organ-specific toxicity across preclinical and clinical evidence.

Conclusion: DiscoVerse is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, demonstrating promising answer accuracy and decision-making insights through expert evaluation.

Abstract: Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.

</details>


### [24] ["AGI" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa](https://arxiv.org/abs/2511.18301)
*Harsh Rathva,Pruthwik Mishra,Shrikant Malviya*

Main category: cs.CL

TL;DR: This paper presents a data-centric approach for detecting hallucinations in multilingual scientific text generated by LLMs, achieving competitive results across 9 languages by unifying and balancing existing datasets to create a comprehensive training corpus.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of detecting hallucinations in multilingual scientific text from LLMs, particularly the scarcity and imbalance of training data for reliable AI systems.

Method: Unified and balanced five existing datasets to create a training corpus of 124,821 samples (50% correct, 50% hallucinated), then fine-tuned XLM-RoBERTa-Large with 560M parameters on this enhanced dataset.

Result: Achieved competitive performance across all 9 languages, including 2nd place in Gujarati (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages.

Conclusion: Systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.

Abstract: The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.

</details>


### [25] [Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning](https://arxiv.org/abs/2511.18306)
*Mohammad Aqib,Mohd Hamza,Ying Hei Chui,Qipei Mei*

Main category: cs.CL

TL;DR: This paper compares two methods for extracting information from tabular data in building codes using Vision Language Models (VLMs): direct image input vs. indirect LaTeX conversion, finding direct input performs better, with fine-tuned models achieving over 100% accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Building codes contain critical regulatory information, but tabular data presents challenges due to complex layouts, merged cells, and semantic relationships that traditional NLP and VLMs struggle to capture effectively.

Method: Two approaches were compared: 1) Direct input method feeding page images directly to VLMs, and 2) Indirect input method converting page images to LaTeX code first. Models were then fine-tuned using Low Rank Adaptation (LoRA) on domain-specific tabular data.

Result: Direct input method generally achieved higher accuracy than indirect input. Fine-tuned models showed substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%.

Conclusion: Parameter-efficient fine-tuning methods like LoRA can effectively adapt powerful VLMs for understanding complex structured data in specialized domains such as building code interpretation and regulatory compliance.

Abstract: Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.

</details>


### [26] [Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search](https://arxiv.org/abs/2511.18313)
*Joseph Oladokun*

Main category: cs.CL

TL;DR: PCR is a retrieval method that combines graph structural constraints with semantic search to ensure retrieved information maintains logical relationships, improving reasoning coherence in LLM agents.


<details>
  <summary>Details</summary>
Motivation: LLM agents often retrieve context from knowledge bases lacking structural consistency with current reasoning state, leading to incoherent reasoning chains.

Method: Path-Constrained Retrieval (PCR) restricts search space to nodes reachable from an anchor node in a knowledge graph, combining structural graph constraints with semantic search.

Result: PCR achieves full structural consistency vs 24-32% in baselines, maintains strong relevance scores, reduces average graph distance by 78%, and obtains full relevance at rank 10 with full structural consistency in technology domain.

Conclusion: Path-constrained retrieval is an effective approach for improving reliability and coherence of LLM agent reasoning systems.

Abstract: Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.

</details>


### [27] [Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection](https://arxiv.org/abs/2511.18324)
*Syed Mohaiminul Hoque,Naimur Rahman,Md Sakhawat Hossain*

Main category: cs.CL

TL;DR: Ensemble-based fine-tuning approach called "Gradient Masters" for Bangla hate speech identification, achieving 6th place in hate-type classification (73.23% F1) and 3rd place in target group classification (73.28% F1) in BLP-2025 shared task.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of hate speech detection in low-resource Bangla language scenarios, particularly for YouTube comments, through robust ensemble methods.

Method: Hybrid ensemble-based fine-tuning approach on Bangla Language Model, comparing various LM variants and conducting extensive experiments to evaluate robustness and generalization.

Result: Outperformed baseline models and secured competitive positions: 6th in subtask 1A (73.23% micro F1) and 3rd in subtask 1B (73.28% micro F1).

Conclusion: The proposed ensemble approach effectively handles Bangla hate speech detection in low-resource settings, with detailed analysis providing insights into misclassification patterns.

Abstract: This paper introduces the approach of "Gradient Masters" for BLP-2025 Task 1: "Bangla Multitask Hate Speech Identification Shared Task". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.

</details>


### [28] [OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas](https://arxiv.org/abs/2511.18335)
*James Y. Huang,Wenxuan Zhou,Nan Xu,Fei Wang,Qin Liu,Sheng Zhang,Hoifung Poon,Muhao Chen*

Main category: cs.CL

TL;DR: The paper introduces OmniStruct, a benchmark for evaluating LLMs on text-to-structure tasks, and shows that smaller models fine-tuned on synthetic data can match GPT-4o's performance on structured generation tasks.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel at generating unstructured natural language, their performance on structured output tasks like information extraction, table generation, and function calling remains unclear, creating a need for comprehensive evaluation.

Method: Created OmniStruct benchmark by adapting existing datasets for structured formats, generated synthetic training data, and fine-tuned smaller models on this synthetic data without supervised training on OmniStruct tasks.

Result: Smaller models fine-tuned on synthetic data achieved performance comparable to GPT-4o on diverse text-to-structure tasks without using supervised data.

Conclusion: It's possible to develop efficient universal structured generation models by fine-tuning smaller models on synthetic data, achieving GPT-4o level performance on structured output tasks.

Abstract: The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.

</details>


### [29] [Tu crois que c'est vrai ? Diversite des regimes d'enonciation face aux fake news et mecanismes d'autoregulation conversationnelle](https://arxiv.org/abs/2511.18369)
*Manon Berriche*

Main category: cs.CL

TL;DR: Fake news sharing is concentrated among a small group of highly politicized users who help set political agendas, while exposed users show critical distance through discursive caution or interventions, but this rarely leads to genuine debate.


<details>
  <summary>Details</summary>
Motivation: To resolve two paradoxes: why fake news represents only a small share of social media content despite lack of editorial control, and how political polarization intensifies despite users not being especially receptive to fake news.

Method: Mixed-methods design combining quantitative analysis of digital traces with online observation and interviews on Twitter and Facebook, examining user practices across different interactional situations while recording socio-demographic traits.

Result: 1) Fake news sharing concentrated among limited group of politicized, institution-critical users; 2) Exposed users show critical distance through discursive caution or interventions; 3) These interactions rarely produce genuine deliberative debates, instead creating dialogues of the deaf.

Conclusion: Fake news impact comes from concentrated sharing by small but highly active politicized groups, while critical responses exist but fail to generate meaningful deliberation, explaining polarization despite limited fake news reach.

Abstract: This thesis addresses two paradoxes: (1) why empirical studies find that fake news represent only a small share of the information consulted and shared on social media despite the absence of editorial control or journalistic norms, and (2) how political polarization has intensified even though users do not appear especially receptive to fake news. To investigate these issues, two complementary studies were carried out on Twitter and Facebook, combining quantitative analyses of digital traces with online observation and interviews. This mixed-methods design avoids reducing users to single reactions to identified fake items and instead examines the variety of practices across different interactional situations, online and offline, while recording socio-demographic traits. The first study mapped users who shared at least one item labeled fake by fact-checkers in the French Twittersphere. The second used a corpus of items flagged by Facebook users to study reactions to statements whose epistemic status is uncertain. Three main findings emerge. First, sharing fake news is concentrated among a limited group of users who are not less educated or cognitively disadvantaged but are more politicized and critical of institutions; owing to their high activity and prolific sharing, they can help set the agenda for their political camp. Second, exposed users can deploy varying forms of critical distance depending on their social position and the interactional norms of the situations they inhabit: either discursive caution (prudence énonciative) or interventions ('points d'arrêt') that express disagreement or corrections. Third, these forms of critical distance seldom yield genuine deliberative debates or agonistic pluralism; rather, they often produce dialogues of the deaf among a small, particularly active minority.

</details>


### [30] [Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models](https://arxiv.org/abs/2511.18393)
*Heejoon Koo*

Main category: cs.CL

TL;DR: This paper studies how text corruption in clinical data affects LLM performance in diagnosis prediction, proposing methods to improve robustness and reduce demographic bias.


<details>
  <summary>Details</summary>
Motivation: Clinical texts often contain errors from human or automated sources, raising concerns about AI reliability and fairness in healthcare decision-making, but the impact of such degradations is under-investigated.

Method: Systematic study of LLMs under text corruption scenarios, introducing clinically grounded label-reduction and hierarchical chain-of-thought strategy that mimics clinician reasoning.

Result: The proposed approach improves model robustness and reduces subgroup instability when dealing with degraded clinical inputs.

Conclusion: The methods advance reliable use of LLMs in clinical decision support systems by addressing noise-induced uncertainty and demographic fairness issues.

Abstract: A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.

</details>


### [31] [Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models](https://arxiv.org/abs/2511.18409)
*Dana Arad,Yonatan Belinkov,Hanjie Chen,Najoung Kim,Hosein Mohebbi,Aaron Mueller,Gabriele Sarti,Martin Tutek*

Main category: cs.CL

TL;DR: The BlackboxNLP 2025 Shared Task extends the Mechanistic Interpretability Benchmark (MIB) to provide standardized evaluation of mechanistic interpretability techniques through circuit and causal variable localization tracks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of measuring progress in mechanistic interpretability research by creating a community-wide reproducible comparison framework.

Method: Extends the MIB framework with two tracks: circuit localization (identifying causally influential components) and causal variable localization (mapping activations to interpretable features).

Result: Participants achieved notable gains using ensemble/regularization strategies for circuit discovery and low-dimensional/non-linear projections for causal variable localization.

Conclusion: The MIB leaderboard remains open to encourage continued standardized evaluation of mechanistic interpretability research progress.

Abstract: Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.

</details>


### [32] [SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data](https://arxiv.org/abs/2511.18411)
*Sultan Alrashed,Chadi Helwe,Francesco Orabona*

Main category: cs.CL

TL;DR: SmolKalam is a high-quality Arabic dataset created by translating Smoltalk2 using a multi-model ensemble pipeline with quality filtering, addressing the lack of multi-turn Arabic datasets with reasoning and tool calling.


<details>
  <summary>Details</summary>
Motivation: There is a lack of large-scale, multi-turn Arabic datasets that include reasoning and tool calling capabilities, and naive translation approaches are insufficient for post-training needs which demand higher quality data.

Method: Used a multi-model ensemble translation pipeline with quality filtering and conducted ablations to examine effective translation techniques for traditional decoder-only models.

Result: Successfully created SmolKalam, a translated version of Smoltalk2 that provides high-quality Arabic data suitable for post-training applications.

Conclusion: The multi-model ensemble translation pipeline with quality filtering is an effective approach for creating high-quality Arabic datasets that can support reasoning and tool calling capabilities in language models.

Abstract: Although the community has tackled the acquisition of high-quality Arabic pretraining data, we still lack large-scale, multi-turn Arabic datasets that include reasoning and tool calling. Naive translation can work at the pretraining scale, but post-training demands much higher quality, which requires a stricter approach to dataset curation. In this work, we introduce SmolKalam, a translation of Smoltalk2 that uses a multi-model ensemble translation pipeline, applies quality filtering, and examines effective translation techniques for traditional decoder-only models through ablations.

</details>


### [33] [Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations](https://arxiv.org/abs/2511.18413)
*Yu Xia,Sungchul Kim,Tong Yu,Ryan A. Rossi,Julian McAuely*

Main category: cs.CL

TL;DR: Proposes MACF framework for agentic recommendations using LLM agents to simulate collaborative filtering through dynamic multi-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: Existing agentic recommender systems underuse collaborative signals from user-item interaction history, leading to poor recommendation quality.

Method: Instantiate similar users and relevant items as LLM agents with unique profiles, using a central orchestrator to manage dynamic agent recruitment and personalized collaboration.

Result: Experimental results on three domain datasets show MACF outperforms strong agentic recommendation baselines.

Conclusion: MACF framework effectively bridges traditional collaborative filtering with LLM-based multi-agent systems for improved agentic recommendations.

Abstract: Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.

</details>


### [34] [General Agentic Memory Via Deep Research](https://arxiv.org/abs/2511.18423)
*B. Y. Yan,Chaofan Li,Hongjin Qian,Shuqi Lu,Zheng Liu*

Main category: cs.CL

TL;DR: GAM is a novel just-in-time memory framework that creates optimized contexts at runtime using a duo-design of Memorizer and Researcher components, achieving significant improvements over existing memory systems.


<details>
  <summary>Details</summary>
Motivation: Static memory systems in AI agents suffer from severe information loss when creating pre-available memory, limiting their effectiveness in memory-grounded tasks.

Method: GAM employs a duo-design: 1) Memorizer highlights key historical information with lightweight memory while maintaining complete history in a universal page-store; 2) Researcher retrieves and integrates useful information from the page-store at runtime guided by pre-constructed memory.

Result: GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems, effectively leveraging LLM capabilities and enabling end-to-end optimization.

Conclusion: The just-in-time compilation principle in GAM provides an effective solution to information loss in static memory systems, demonstrating superior performance through runtime context optimization.

Abstract: Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \textbf{general agentic memory (GAM)}. GAM follows the principle of "\textbf{just-in time (JIT) compilation}" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.

</details>


### [35] [MindEval: Benchmarking Language Models on Multi-turn Mental Health Support](https://arxiv.org/abs/2511.18491)
*José Pombal,Maya D'Eon,Nuno M. Guerreiro,Pedro Henrique Martins,António Farinhas,Ricardo Rei*

Main category: cs.CL

TL;DR: MindEval is a framework for automatically evaluating language models in realistic multi-turn mental health therapy conversations, developed with clinical psychologists. It shows current LLMs struggle significantly in therapeutic contexts, with scores below 4/6 on average.


<details>
  <summary>Details</summary>
Motivation: Current AI mental health chatbots have limitations like sycophancy and reinforcement of maladaptive beliefs, and existing benchmarks don't capture the complexity of real therapeutic interactions.

Method: Developed MindEval framework with clinical psychologists using patient simulation and automatic LLM evaluation. Validated realism of simulated patients and correlation with human expert judgments.

Result: 12 state-of-the-art LLMs scored below 4/6 on average, with weaknesses in problematic AI-specific communication patterns. Reasoning capabilities and model scale don't guarantee better performance, and systems deteriorate with longer interactions or severe symptoms.

Conclusion: Current LLMs struggle significantly in mental health therapy contexts, highlighting the need for better benchmarks and improved models for therapeutic applications.

Abstract: Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.

</details>


### [36] [For Those Who May Find Themselves on the Red Team](https://arxiv.org/abs/2511.18499)
*Tyler Shoemaker*

Main category: cs.CL

TL;DR: Literary scholars should engage with LLM interpretability research despite ideological challenges, as current instrumental approaches are insufficient for measuring interpretation.


<details>
  <summary>Details</summary>
Motivation: Current interpretability approaches for LLMs are too instrumental and cannot be the sole standard for measuring interpretation, requiring literary scholars' involvement.

Method: Proposes engagement through red teaming as a site for ideological struggle and critical engagement with LLM interpretability.

Result: Identifies the need for literary scholars to participate in LLM interpretability research despite potential complicity.

Conclusion: Literary scholars must engage with LLM interpretability research to challenge purely instrumental approaches and bring critical perspectives to the field.

Abstract: This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.

</details>


### [37] [Dealing with the Hard Facts of Low-Resource African NLP](https://arxiv.org/abs/2511.18557)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Panga Azazia Kamaté,Madani Amadou Tall,Emmanuel Élisé Koné,Aymane Dembélé,Michael Leventhal*

Main category: cs.CL

TL;DR: Field collection of 612 hours of Bambara speech, semi-automated transcription, creation of monolingual speech models, and comprehensive evaluation including human assessment.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of creating speech resources for low-resource languages like Bambara, which lack established frameworks and experience.

Method: Field collection of spontaneous speech, semi-automated annotation with transcriptions, creation of ultra-compact and small monolingual models, and both automatic and human evaluation.

Result: Successfully collected 612 hours of Bambara speech, created multiple speech models, and established evaluation frameworks with publicly available datasets, models, and code.

Conclusion: Provides practical guidelines for low-resource language speech processing, emphasizes importance of human evaluation, and makes comprehensive resources publicly available to support future work.

Abstract: Creating speech datasets, models, and evaluation frameworks for low-resource languages remains challenging given the lack of a broad base of pertinent experience to draw from. This paper reports on the field collection of 612 hours of spontaneous speech in Bambara, a low-resource West African language; the semi-automated annotation of that dataset with transcriptions; the creation of several monolingual ultra-compact and small models using the dataset; and the automatic and human evaluation of their output. We offer practical suggestions for data collection protocols, annotation, and model design, as well as evidence for the importance of performing human evaluation. In addition to the main dataset, multiple evaluation datasets, models, and code are made publicly available.

</details>


### [38] [Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks](https://arxiv.org/abs/2511.18597)
*H. M. Shadman Tabib,Jaber Ahmed Deedar*

Main category: cs.CL

TL;DR: GPT-4o performs poorly (37.75% accuracy) at predicting competitive programming problem difficulty compared to LightGBM (86% accuracy), showing bias toward simpler categories and inability to leverage numeric constraints effectively.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLMs' capability in predicting competitive programming problem difficulty, as they are increasingly deployed as automatic judges but their behavior on structured tasks remains under-explored.

Method: Compared GPT-4o as natural-language difficulty assessor against interpretable LightGBM ensemble trained on explicit numeric and textual features, using 1,825 LeetCode problems labeled Easy/Medium/Hard. Also conducted synthetic Hard-problem generation protocol.

Result: LightGBM achieved 86% accuracy while GPT-4o only reached 37.75%. GPT-4o showed strong bias toward simpler categories, overlooked crucial numeric constraints, and labeled most of its own synthetic Hard problems as Medium.

Conclusion: LLM-based judges have concrete failure modes and cannot be considered trustworthy for competitive programming, educational platforms, or reinforcement-learning pipelines without addressing these limitations.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language and code generation, and are increasingly deployed as automatic judges of model outputs and learning activities. Yet, their behavior on structured tasks such as predicting the difficulty of competitive programming problems remains under-explored. We conduct a systematic comparison of GPT-4o, used purely as a natural-language difficulty assessor, against an interpretable Light-GBM ensemble trained on explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, LightGBM attains 86% accuracy, whereas GPT-4o reaches only 37.75%. Detailed analyses, including confusion matrices and SHAP-based interpretability, show that numeric constraints -- such as input size limits and acceptance rates -- play a crucial role in separating Hard problems from easier ones. By contrast, GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories. We further probe GPT-4o through a synthetic Hard-problem generation protocol. Surprisingly, GPT-4o labels almost all of its own synthetic Hard problems as Medium, contradicting its tendency to downgrade real Hard problems to Easy. Our findings connect to recent work on LLMs-as-judges and automatic difficulty estimation in programming and education, and highlight concrete failure modes that must be addressed before LLM-based judges can be considered trustworthy in competitive programming, educational platforms, or reinforcement-learning pipelines.

</details>


### [39] [A Benchmark for Zero-Shot Belief Inference in Large Language Models](https://arxiv.org/abs/2511.18616)
*Joseph Malone,Rachith Aiyappa,Byunghwee Lee,Haewoon Kwak,Jisun An,Yong-Yeol Ahn*

Main category: cs.CL

TL;DR: A systematic benchmark evaluates LLMs' zero-shot ability to predict individuals' stances across diverse belief domains using online debate data, revealing that more background information improves accuracy but performance varies by domain.


<details>
  <summary>Details</summary>
Motivation: To understand how well LLMs generalize across diverse belief domains beyond narrow sociopolitical contexts, as beliefs are central to human reasoning and social connections.

Method: Created a reproducible benchmark using online debate platform data with multiple informational conditions to isolate demographic context and prior beliefs' contributions to predictive success in zero-shot settings.

Result: Providing more background information about individuals improves predictive accuracy, but performance varies substantially across different belief domains.

Conclusion: LLMs show both capacity and limitations in emulating human reasoning about beliefs, offering a scalable framework for modeling belief systems beyond sociopolitical contexts.

Abstract: Beliefs are central to how humans reason, communicate, and form social connections, yet most computational approaches to studying them remain confined to narrow sociopolitical contexts and rely on fine-tuning for optimal performance. Despite the growing use of large language models (LLMs) across disciplines, how well these systems generalize across diverse belief domains remains unclear. We introduce a systematic, reproducible benchmark that evaluates the ability of LLMs to predict individuals' stances on a wide range of topics in a zero-shot setting using data from an online debate platform. The benchmark includes multiple informational conditions that isolate the contribution of demographic context and known prior beliefs to predictive success. Across several small- to medium-sized models, we find that providing more background information about an individual improves predictive accuracy, but performance varies substantially across belief domains. These findings reveal both the capacity and limitations of current LLMs to emulate human reasoning, advancing the study of machine behavior and offering a scalable framework for modeling belief systems beyond the sociopolitical sphere.

</details>


### [40] [A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News](https://arxiv.org/abs/2511.18618)
*Mirza Raquib,Munazer Montasir Akash,Tawhid Ahmed,Saydul Akbar Murad,Farida Siddiqi Prity,Mohammad Amzad Hossain,Asif Pervez Polok,Nick Rahimi*

Main category: cs.CL

TL;DR: This paper presents a hybrid BERT-CNN-BiLSTM model for simultaneous Bangla news headline classification and sentiment analysis, achieving state-of-the-art results on the BAN-ABSA dataset.


<details>
  <summary>Details</summary>
Motivation: Newspapers are essential information sources but navigating vast news content is challenging. Sentiment analysis helps understand emotional tone of news quickly, especially important for Bangla language which is considered low-resource.

Method: Used hybrid transfer learning model BERT-CNN-BiLSTM on BAN-ABSA dataset of 9014 Bangla news headlines. Applied two experimental strategies: technique-1 (undersampling/oversampling before splitting) and technique-2 (undersampling/oversampling after splitting).

Result: Technique-1 with oversampling achieved 78.57% headline and 73.43% sentiment accuracy. Technique-2 on original imbalanced dataset achieved 81.37% headline and 64.46% sentiment accuracy. The proposed model significantly outperformed all baselines.

Conclusion: The BERT-CNN-BiLSTM model achieves new state-of-the-art results for Bangla news classification and sentiment analysis, demonstrating the importance of leveraging both headline and sentiment datasets, providing a strong baseline for Bangla text classification in low-resource scenarios.

Abstract: In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\% and 73.43\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\% and 64.46\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.

</details>


### [41] [Prompt Optimization as a State-Space Search Problem](https://arxiv.org/abs/2511.18619)
*Maanas Taneja*

Main category: cs.CL

TL;DR: Treating prompt optimization as a state-space search problem using beam search and random walk algorithms improves LLM performance across multiple NLP tasks, though test set gains are more modest than development set improvements.


<details>
  <summary>Details</summary>
Motivation: Language Models are highly sensitive to small prompt changes, and existing approaches like DSpy use demonstration-based optimization. This paper proposes an alternative by framing prompt optimization as a classical search problem.

Method: Model prompt space as a graph with nodes as prompt states and edges as transformations (shortening, adding examples, reordering). Use beam search and random walk algorithms to explore this space, evaluating candidates on development sets and pruning unpromising branches.

Result: Shallow search configurations (beam width=2, depth=2) improved development set performance across five NLP tasks. Beam search achieved development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements were more modest (0.20 to 0.50). Concise prompt transformations were most successful.

Conclusion: Prompt optimization can be effectively treated as a search problem. With more computational resources and better evaluation metrics, deeper exploration could yield more robust prompts that generalize better beyond development sets.

Abstract: Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].

</details>


### [42] [OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph](https://arxiv.org/abs/2511.18622)
*Michael J. Bommarito*

Main category: cs.CL

TL;DR: OpenGloss is a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates definitions, encyclopedic content, etymology, and semantic relationships, generated using LLMs at low cost and high speed.


<details>
  <summary>Details</summary>
Motivation: To address gaps in pedagogical applications and create comprehensive lexical resources that integrate multiple types of content (definitions, examples, collocations, encyclopedias, etymology) to support both vocabulary learning and NLP tasks, while demonstrating the feasibility of automated resource generation.

Method: Multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, producing the entire resource in under one week for under $1,000.

Result: Created a resource with 537K senses across 150K lexemes (comparable to WordNet 3.1), containing 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content - more than four times as many sense definitions as comparable resources.

Conclusion: Structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve, though the resource reflects both capabilities and limitations of current foundation models.

Abstract: We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.
  Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.
  As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.

</details>


### [43] [No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases](https://arxiv.org/abs/2511.18635)
*Shireen Chand,Faith Baca,Emilio Ferrara*

Main category: cs.CL

TL;DR: Targeted bias mitigation in LLMs often reduces bias in intended dimensions but frequently causes unintended negative consequences in other dimensions, increasing bias elsewhere and decreasing model coherence.


<details>
  <summary>Details</summary>
Motivation: LLMs inherit societal biases from training data, and current bias mitigation techniques are typically evaluated only on targeted bias dimensions without considering cross-category consequences.

Method: Applied four bias mitigation techniques across ten models from seven model families, studying racial, religious, profession- and gender-related biases using the StereoSet benchmark to measure impact on model coherence and stereotypical preference.

Result: Targeted mitigation sometimes reduces bias in intended dimensions but frequently leads to unintended negative consequences in other dimensions, including increased bias and decreased general coherence.

Conclusion: Robust, multi-dimensional evaluation tools are critically needed when developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.

Abstract: Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.

</details>


### [44] [Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting](https://arxiv.org/abs/2511.18649)
*Goun Pyeon,Inbum Heo,Jeesu Jung,Taewook Hwang,Hyuk Namgoong,Hyein Seo,Yerim Han,Eunbin Kim,Hyeonseok Kang,Sangkeun Jung*

Main category: cs.CL

TL;DR: This study evaluates LLMs' mathematical reasoning using the 2026 Korean CSAT math section in a contamination-free environment, finding GPT-5 Codex achieves perfect score while revealing geometry as the weakest domain and highlighting cost-efficiency trade-offs in reasoning enhancement.


<details>
  <summary>Details</summary>
Motivation: To address data leakage issues in existing benchmarks and systematically evaluate LLMs' mathematical reasoning capabilities using a completely unexposed, real-exam environment that eliminates contamination from training data.

Method: Digitized all 46 questions from the 2026 Korean CSAT Mathematics section within 2 hours of exam release, then evaluated 24 state-of-the-art LLMs across different input modalities (text, image, text+figure) and prompt languages (Korean, English).

Result: GPT-5 Codex achieved perfect score (100 points), while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Geometry was the weakest domain (77.7% average), text input outperformed image input, and increased reasoning intensity improved performance but drastically reduced efficiency.

Conclusion: The study establishes a contamination-free evaluation framework, demonstrates that models with minimal reasoning may be more practical due to cost-efficiency considerations, and provides a comprehensive assessment integrating performance, cost, and time factors.

Abstract: This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (text, image, text+figure) and prompt languages (Korean, English).
  GPT-5 Codex achieved the only perfect score (100 points) with text input and Korean prompts, while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed geometry as the weakest domain (77.7% average) with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale.
  In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (from 82.6 to 100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a real-exam-based LLM assessment framework, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard (https://isoft.cnu.ac.kr/csat2026/).

</details>


### [45] [CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning](https://arxiv.org/abs/2511.18659)
*Jie He,Richard He Bai,Sinead Williamson,Jeff Z. Pan,Navdeep Jaitly,Yizhe Zhang*

Main category: cs.CL

TL;DR: CLaRa is a unified framework that performs embedding-based compression and joint optimization in continuous space to improve retrieval-augmented generation, achieving state-of-the-art performance on QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented generation (RAG) suffers from long contexts and disjoint retrieval-generation optimization, limiting its effectiveness.

Method: Proposes CLaRa framework with SCP data synthesis for semantic compression, and trains reranker and generator end-to-end via single language modeling loss using differentiable top-k estimator.

Result: Achieves state-of-the-art compression and reranking performance across multiple QA benchmarks, often surpassing text-based fine-tuned baselines.

Conclusion: CLaRa's unified optimization in continuous space effectively aligns retrieval relevance with answer quality, demonstrating superior performance over disjoint approaches.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.

</details>


### [46] [Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models](https://arxiv.org/abs/2511.18696)
*Wangjiaxuan Xin*

Main category: cs.CL

TL;DR: ECN is a multi-stage prompting framework that enhances LLM empathy through four stages, achieving top EQ scores while maintaining performance on other metrics.


<details>
  <summary>Details</summary>
Motivation: To improve empathetic and inclusive capabilities of large language models in conversational AI applications.

Method: Four-stage prompting: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis.

Result: Achieved highest Empathy Quotient scores across GPT-3.5-turbo and GPT-4, with competitive Regard and Perplexity metrics.

Conclusion: ECN shows strong potential for applications requiring empathy and inclusivity in conversational AI systems.

Abstract: This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.

</details>


### [47] [RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context](https://arxiv.org/abs/2511.18743)
*Yu Lei,Shuzheng Si,Wei Wang,Yifei Wu,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: RhinoInsight is a deep research framework that adds verifiable checklists and evidence audit mechanisms to enhance robustness and reduce hallucinations in LLM-based research systems.


<details>
  <summary>Details</summary>
Motivation: Current linear pipeline approaches for LLM research agents suffer from error accumulation and context rot due to lack of explicit control over model behavior and context management.

Method: Two control mechanisms: 1) Verifiable Checklist module transforms requirements into traceable sub-goals with hierarchical outlines, 2) Evidence Audit module structures search content, updates outlines, and binds high-quality evidence to drafted content.

Result: RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.

Conclusion: The framework enhances robustness, traceability, and quality in LLM research systems without requiring parameter updates.

Abstract: Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.

</details>


### [48] [Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search](https://arxiv.org/abs/2511.18749)
*Matthew R. DeVerna,Kai-Cheng Yang,Harry Yaojun Yan,Filippo Menczer*

Main category: cs.CL

TL;DR: LLMs perform poorly at automated fact-checking despite reasoning and web search capabilities, but curated RAG systems with high-quality context show significant improvements.


<details>
  <summary>Details</summary>
Motivation: Evaluate LLMs' fact-checking capabilities as millions of users rely on chatbots for verification, requiring rigorous assessment of reasoning and web search tools.

Method: Tested 15 LLMs from major providers on 6,000 PolitiFact claims, comparing standard models with reasoning and web search variants against a curated RAG system using PolitiFact summaries.

Result: Standard models performed poorly, reasoning offered minimal benefits, web search provided only moderate gains despite available fact-checks online. Curated RAG system improved macro F1 by 233% on average.

Conclusion: Access to curated high-quality context is crucial for effective automated fact-checking, rather than relying on models' inherent reasoning or web search capabilities.

Abstract: Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.

</details>


### [49] [Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion](https://arxiv.org/abs/2511.18751)
*Daiqing Wu,Dongbao Yang,Can Ma,Yu Zhou*

Main category: cs.CL

TL;DR: Proposes DRF method for robust multimodal sentiment analysis using distribution-based feature recovery and fusion to handle low-quality and missing modalities in image-text pairs.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal sentiment analysis methods lack consideration for low-quality and missing modalities, which frequently occur in real-world social media applications, creating need for robust models.

Method: Maintains feature queues for each modality to approximate distributions, estimates modality qualities to reduce low-quality contributions, and builds inter-modal mapping relationships to recover missing modalities from available ones.

Result: Demonstrates universal improvements over SOTA methods on three public datasets using disruption strategies that corrupt and discard modalities, validating effectiveness in robust sentiment analysis.

Conclusion: DRF provides a unified framework that effectively handles both low-quality and missing modalities in multimodal sentiment analysis, showing superior robustness compared to existing methods.

Abstract: As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.

</details>


### [50] [Context-Aware Whisper for Arabic ASR Under Linguistic Varieties](https://arxiv.org/abs/2511.18774)
*Bashar Talafha,Amin Abu Alhassan,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: Context-aware prompting strategies adapt Whisper for Arabic ASR without retraining, reducing WER by up to 22.3% on MSA and 9.2% on dialects.


<details>
  <summary>Details</summary>
Motivation: Address low-resource ASR challenges for Arabic with wide dialectal variation and limited labeled data.

Method: Decoder prompting with first-pass transcriptions/retrieved utterances, encoder prefixing using synthesized speech, prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval.

Result: Reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, mitigating hallucinations and speaker mismatch.

Conclusion: Context-aware prompting effectively adapts Whisper for Arabic ASR in zero-shot settings without retraining.

Abstract: Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.

</details>


### [51] [HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations](https://arxiv.org/abs/2511.18808)
*Cao Linxiao,Wang Ruitao,Li Jindong,Zhou Zhipeng,Yang Menglin*

Main category: cs.CL

TL;DR: HyperbolicRAG improves graph-based retrieval-augmented generation by using hyperbolic geometry to better capture hierarchical relationships in knowledge graphs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard graph-based RAG methods use Euclidean embeddings that capture semantic similarity but fail to represent hierarchical depth and abstraction relationships in complex knowledge graphs.

Method: Proposes HyperbolicRAG with three components: depth-aware representation learning in Poincare manifold, unsupervised contrastive regularization for geometric consistency, and mutual-ranking fusion combining Euclidean and hyperbolic retrieval signals.

Result: Extensive experiments on multiple QA benchmarks show HyperbolicRAG outperforms competitive baselines including standard RAG and graph-augmented methods.

Conclusion: Hyperbolic geometry effectively enhances graph-based RAG by capturing both semantic similarity and hierarchical relationships, leading to improved performance in knowledge-intensive tasks.

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.

</details>


### [52] [Concept than Document: Context Compression via AMR-based Conceptual Entropy](https://arxiv.org/abs/2511.18832)
*Kaize Shi,Xueyao Sun,Xiaohui Tao,Lin Li,Qika Lin,Guandong Xu*

Main category: cs.CL

TL;DR: Unsupervised context compression framework using AMR graphs and conceptual entropy to filter redundant information in RAG systems, improving accuracy while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: LLMs face information overload with long contexts in RAG systems, where extensive supporting documents introduce redundant content that weakens reasoning accuracy and increases computational costs.

Method: Construct AMR graphs from raw contexts, compute conceptual entropy of each node to estimate importance, and retain only significant informative nodes to create condensed, semantically focused contexts.

Result: Experiments on PopQA and EntityQuestions datasets show superior performance over vanilla and other baselines, achieving higher accuracy with substantially reduced context length.

Conclusion: First work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering for improving LLM performance.

Abstract: Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.

</details>


### [53] [A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis](https://arxiv.org/abs/2511.18843)
*Heger Arfaoui,Mohammed Iheb Hergli,Beya Benzina,Slimane BenMiled*

Main category: cs.CL

TL;DR: A computational framework using BERTopic for analyzing focus group transcripts, addressing hyperparameter sensitivity, model stability, and interpretability validation through systematic evaluation and human expert assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional manual coding of focus group discussions is labor-intensive and limits scalability and reproducibility, necessitating a rigorous computational approach.

Method: Applied BERTopic to 10 focus groups (1,076 utterances) on HPV vaccine perceptions in Tunisia, with systematic hyperparameter evaluation (27 configurations), bootstrap resampling for stability (30 replicates), and human validation by three domain experts.

Result: Found substantial hyperparameter sensitivity, achieved topic coherence of 0.558 with hierarchical merging strategy, and confirmed topic quality with high inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578).

Conclusion: The framework provides practical guidelines for qualitative research, effectively balancing stability and coherence tradeoffs, with all code and protocols publicly available for reproduction.

Abstract: Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.

</details>


### [54] [Large Language Models for the Summarization of Czech Documents: From History to the Present](https://arxiv.org/abs/2511.18848)
*Václav Tran,Jakub Šmíd,Ladislav Lenc,Jean-Pierre Salmon,Pavel Král*

Main category: cs.CL

TL;DR: This paper advances Czech text summarization using LLMs, achieving SOTA on modern Czech texts and introducing a new historical Czech dataset with initial baselines.


<details>
  <summary>Details</summary>
Motivation: Czech summarization, especially for historical documents, is underexplored due to linguistic complexity and lack of annotated datasets.

Method: Used multilingual LLMs (Mistral, mT5) and proposed a translation-based approach: Czech→English→summarize→Czech.

Result: Achieved new SOTA on SumeCzech dataset and introduced Posel od Čerchova dataset for historical Czech text summarization.

Conclusion: The work establishes foundations for Czech summarization progress and provides valuable resources for low-resource language processing.

Abstract: Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.
  In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od Čerchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.
  By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.

</details>


### [55] [Cognitive Alpha Mining via LLM-Driven Code-Based Evolution](https://arxiv.org/abs/2511.18850)
*Fengyuan Liu,Huang Yi,Sichun Luo,Yuqi Wang,Yazheng Yang,Xinye Li,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: CogAlpha framework combines LLM-driven reasoning with evolutionary search to discover financial alphas, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing alpha discovery methods explore only narrow regions of search space, producing opaque patterns or ungeneralizable formulas, lacking human-like structured exploration.

Method: Combines code-level alpha representation with LLM-driven reasoning and evolutionary search, treating LLMs as cognitive agents that iteratively refine, mutate, and recombine alpha candidates through multi-stage prompts and financial feedback.

Result: Experiments on A-share equities show CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods.

Conclusion: Aligning evolutionary optimization with LLM-based reasoning enables automated and explainable alpha discovery, greatly expanding the effective search space.

Abstract: Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.

</details>


### [56] [FanarGuard: A Culturally-Aware Moderation Filter for Arabic Language Models](https://arxiv.org/abs/2511.18852)
*Masoomali Fatehkia,Enes Altinisik,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: FanarGuard is a bilingual moderation filter for Arabic and English that evaluates both safety and cultural alignment, outperforming existing filters on cultural awareness while matching safety performance.


<details>
  <summary>Details</summary>
Motivation: Existing content moderation filters focus narrowly on general safety and overlook cultural context, creating alignment failures in language models for non-English cultures.

Method: Developed a dataset of 468K prompt-response pairs scored by LLM judges on harmlessness and cultural awareness, trained two filter variants, and created the first Arabic cultural alignment benchmark with 1k norm-sensitive prompts annotated by human raters.

Result: FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability and matches state-of-the-art filter performance on safety benchmarks.

Conclusion: Cultural awareness is crucial for effective content moderation, and FanarGuard represents a practical step toward more context-sensitive safeguards for multilingual AI systems.

Abstract: Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce FanarGuard, a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising over 1k norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish FanarGuard as a practical step toward more context-sensitive safeguards.

</details>


### [57] [Generating Reading Comprehension Exercises with Large Language Models for Educational Applications](https://arxiv.org/abs/2511.18860)
*Xingyu Huang,Fei Jiang,Jianli Xiao*

Main category: cs.CL

TL;DR: RCEG is a framework that automatically generates high-quality English reading comprehension exercises using fine-tuned LLMs and a discriminator for content selection.


<details>
  <summary>Details</summary>
Motivation: LLMs show great potential in education for automatic text generation, enabling creation of intelligent and adaptive learning content, particularly for reading comprehension exercises.

Method: Uses fine-tuned LLMs to generate content candidates, then employs a discriminator to select the best candidate, significantly improving content quality.

Result: Experimental results show RCEG significantly improves relevance and cognitive appropriateness of generated exercises across metrics including content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment.

Conclusion: The proposed RCEG framework effectively generates high-quality personalized English reading comprehension exercises, demonstrating the practical application of LLMs in educational content creation.

Abstract: With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.

</details>


### [58] [Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models](https://arxiv.org/abs/2511.18864)
*Yang Xiang,Yixin Ji,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: First empirical study on pruning Large Reasoning Models (LRMs) showing existing methods fail, proposing a selective self-generated reasoning data strategy that improves pruned LRMs' reasoning by 10-13%.


<details>
  <summary>Details</summary>
Motivation: LRMs have high inference overhead due to long chain-of-thought reasoning, but pruning research has focused only on LLMs, leaving LRMs unexplored.

Method: Proposed Selective Self-Generated Reasoning (SSGR) data construction strategy using challenging and moderately long self-generated reasoning data as calibration for pruning.

Result: SSGR strategy improves reasoning ability of pruned LRMs by 10-13% compared to general pruning methods on DeepSeek-R1-Distill model series.

Conclusion: Self-generated reasoning data, particularly challenging and moderately long sequences, serve as ideal calibration data for effectively pruning LRMs.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning benchmarks. However, their long chain-of-thought reasoning processes incur significant inference overhead. Pruning has emerged as a promising approach to reducing computational costs. However, existing efforts have primarily focused on large language models (LLMs), while pruning LRMs remains unexplored. In this work, we conduct the first empirical study on pruning LRMs and show that directly applying existing pruning techniques fails to yield satisfactory results. Our findings indicate that using self-generated reasoning data for calibration can substantially improve pruning performance. We further investigate how the difficulty and length of reasoning data affect pruning outcomes. Our analysis reveals that challenging and moderately long self-generated reasoning data serve as ideal calibration data. Based on these insights, we propose a Selective Self-Generated Reasoning (SSGR) data construction strategy to provide effective calibration data for pruning LRMs. Experimental results on the DeepSeek-R1-Distill model series validate that our strategy improves the reasoning ability of pruned LRMs by 10%-13% compared to general pruning methods.

</details>


### [59] [CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation](https://arxiv.org/abs/2511.18889)
*Jingqian Zhao,Bingbing Wang,Geng Tu,Yice Zhang,Qianlong Wang,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: CoreEval is a contamination-resilient evaluation strategy that automatically updates datasets with real-world knowledge from GDELT database to mitigate LLM performance overestimation caused by data contamination.


<details>
  <summary>Details</summary>
Motivation: Data contamination unfairly inflates LLM evaluation results by exposing models to test data during training. Existing methods fail to fully eliminate pre-existing knowledge or preserve semantic complexity.

Method: Extracts entity relationships from original data, retrieves up-to-date knowledge from GDELT database, recontextualizes and integrates knowledge, refines data structure, and uses iterative verification to ensure label consistency.

Result: Extensive experiments show CoreEval effectively mitigates performance overestimation caused by data contamination, validating its robustness on updated datasets.

Conclusion: CoreEval provides an effective contamination-resilient evaluation approach that maintains semantic coherence while incorporating real-world knowledge updates.

Abstract: Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \textbf{CoreEval}, a \textbf{Co}ntamination-\textbf{re}silient \textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.

</details>


### [60] [Reproducibility Study of Large Language Model Bayesian Optimization](https://arxiv.org/abs/2511.18891)
*Adam Rychert,Gasper Spagnolo,Evgenii Posashkov*

Main category: cs.CL

TL;DR: LLAMBO framework using language models for Bayesian optimization remains effective when replacing GPT-3.5 with Llama 3.1 70B, confirming contextual warm starting improves early performance and language model priors benefit cross-task learning.


<details>
  <summary>Details</summary>
Motivation: To reproduce and validate the LLAMBO framework's effectiveness with different language model backbones, specifically testing if the architecture is robust when using open-weight models like Llama 3.1 70B instead of GPT-3.5.

Method: Replicated original Bayesmark and HPOBench experiments using Llama 3.1 70B for all text encoding components, conducted ablations to test importance of textual context, and experimented with smaller backbones (Gemma 27B, Llama 3.1 8B).

Result: LLAMBO's discriminative surrogate benefits from cross-task semantic priors, contextual warm starting improves early regret and reduces variance, and the candidate sampler generates higher quality proposals than alternatives. Smaller models produced unstable predictions.

Conclusion: The LLAMBO architecture is robust to language model backbone changes and remains effective with Llama 3.1 70B, though sufficient model capacity is required for reliable surrogate behavior.

Abstract: In this reproducibility study, we revisit the LLAMBO framework of Daxberger et al. (2024), a prompting-based Bayesian optimization (BO) method that uses large language models as discriminative surrogates and acquisition optimizers via text-only interactions. We replicate the core Bayesmark and HPOBench experiments under the original evaluation protocol, but replace GPT-3.5 with the open-weight Llama 3.1 70B model used for all text encoding components.
  Our results broadly confirm the main claims of LLAMBO. Contextual warm starting via textual problem and hyperparameter descriptions substantially improves early regret behaviour and reduces variance across runs. LLAMBO's discriminative surrogate is weaker than GP or SMAC as a pure single task regressor, yet benefits from cross task semantic priors induced by the language model. Ablations that remove textual context markedly degrade predictive accuracy and calibration, while the LLAMBO candidate sampler consistently generates higher quality and more diverse proposals than TPE or random sampling. Experiments with smaller backbones (Gemma 27B, Llama 3.1 8B) yield unstable or invalid predictions, suggesting insufficient capacity for reliable surrogate behaviour.
  Overall, our study shows that the LLAMBO architecture is robust to changing the language model backbone and remains effective when instantiated with Llama 3.1 70B.

</details>


### [61] [Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs](https://arxiv.org/abs/2511.18931)
*Sahil Kale*

Main category: cs.CL

TL;DR: Benchmark evaluation shows LLMs with web search improve factual accuracy but have poor confidence calibration, overconfidence issues, and weak query formulation that limits effectiveness.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs are efficiently calibrated to use web search when actually needed, and assess both necessity and effectiveness of web access across commercial models.

Method: Created benchmark with static split (783 pre-cutoff questions) to test search invocation based on internal confidence, and dynamic split (288 post-cutoff queries) to test search requirement recognition and updated information retrieval.

Result: Web access improves static accuracy for GPT-5-mini and Claude Haiku 4.5 but worsens confidence calibration. On dynamic queries, models frequently invoke search but remain below 70% accuracy due to weak query formulation. Costs per accuracy-improving call are low but returns diminish after initial retrieval fails.

Conclusion: Built-in web search meaningfully improves factual accuracy and can be invoked selectively, but models remain overconfident, skip retrieval when essential, and falter when initial search queries underperform. Works better as verification layer than analytical tool.

Abstract: Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.

</details>


### [62] [Skeletons Matter: Dynamic Data Augmentation for Text-to-Query](https://arxiv.org/abs/2511.18934)
*Yuchen Ji,Bo Xu,Jie Shi,Jiaqing Liang,Deqing Yang,Yu Mao,Hai Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: The paper proposes a unified Text-to-Query task paradigm and a dynamic data augmentation framework that diagnoses model weaknesses in handling query skeletons to synthesize targeted training data, achieving SOTA performance with minimal data.


<details>
  <summary>Details</summary>
Motivation: Existing semantic parsing studies focus on single query languages, resulting in limited generalizability across different languages. The authors aim to create a unified approach for Text-to-Query tasks.

Method: Proposed a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling query skeletons to synthesize targeted training data.

Result: Experiments on four Text-to-Query benchmarks show state-of-the-art performance using only a small amount of synthesized data.

Conclusion: The method demonstrates efficiency and generality, laying a solid foundation for unified research on Text-to-Query tasks.

Abstract: The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.

</details>


### [63] [Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials](https://arxiv.org/abs/2511.18937)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: A graphical, knowledge-based method that enhances MedDRA with a semantic layer (Safeterm) for automated adverse event clustering and signal detection in clinical trials.


<details>
  <summary>Details</summary>
Motivation: To improve clarity, efficiency, and accuracy in interpreting treatment-emergent adverse events in clinical trials by addressing limitations of standard MedDRA coding.

Method: Augments MedDRA with Safeterm - a hidden medical knowledge layer that captures semantic relationships in a 2-D map, enabling automatic AE clustering and using shrinkage incidence ratios for disproportionality metrics.

Result: Successfully recovered all expected safety signals in three legacy trials, with visual outputs supporting interpretation through semantic maps and disproportionality plots.

Conclusion: Adding a medical knowledge layer to MedDRA significantly improves AE interpretation for clinical trials by enabling automated clustering and enhanced signal detection.

Abstract: We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.

</details>


### [64] [Logic of Montage](https://arxiv.org/abs/2511.19063)
*Hayami Takahashi,Kensuke Takahashi*

Main category: cs.CL

TL;DR: Proposes a theoretical framework for emotional expression using 'Effect of Contradictory Structure' and montage operations to create 'Effect of Structure', with intensity as a key element.


<details>
  <summary>Details</summary>
Motivation: To develop an alternative form of emotional expression that complements natural language and serves as a proxy for emotional states.

Method: Establishes 'Effect of Contradictory Structure' as dynamic emotional expressions, uses montage operations to overlap structures, and incorporates 'intensity' from Deleuze/Guattari's philosophy as a model element.

Result: Creates a general theoretical framework called 'Word Import Between Systems (Models)' that demonstrates the 'Effect of Structure' process through educational progression examples.

Conclusion: The framework provides a systematic approach to modeling emotional expression through contradictory structures and montage operations, validated through philosophical justification of intensity import.

Abstract: In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form "Effect of Contradictory Structure." "Effect of Contradictory Structure" is not static but dynamic. Effect in "Effect of Contradictory Structure" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, "Effect of Contradictory Structure" can be overlapped with each other. This overlapping operation is called "montage." A broader "Structure" that includes related "Effect of Contradictory Structure" and "Effect of Structure" are set up. Montage produces "Effect of Structure". In montage, it is necessary to set something like "strength," so we adopted Deleuze and Deleuze/Guattari's word "intensity" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of "intensity" through Austin's use of the word "force." "Effect of Structure" process is demonstrated using the example of proceeding to the next level of education.

</details>


### [65] [GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning](https://arxiv.org/abs/2511.19078)
*Yutong Li,Yitian Zhou,Xudong Wang,GuoChen,Caiyan Qin*

Main category: cs.CL

TL;DR: GraphMind is a dynamic graph-based framework that combines GNNs with LLMs for multi-step reasoning, modeling reasoning as an evolving graph to enable context-aware theorem selection and iterative conclusion generation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM approaches lack explicit mechanisms to structurally represent and evolve intermediate reasoning states, limiting their ability for context-aware theorem selection and iterative conclusion generation in multi-step reasoning tasks.

Method: Models reasoning as a heterogeneous evolving graph with nodes representing conditions, theorems, and conclusions, and edges capturing logical dependencies. Uses GNN to encode reasoning states and semantic matching for theorem selection in a closed-loop framework.

Result: Experiments on various QA datasets show consistent performance improvements and significant outperformance over existing baselines in multi-step reasoning tasks.

Conclusion: GraphMind provides an effective and generalizable approach for structured, interpretable, and context-aware reasoning by integrating graph-based representations with LLMs.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.

</details>


### [66] [A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis](https://arxiv.org/abs/2511.19083)
*Wenxuan Mu,Jinzhong Ning,Di Zhao,Yijia Zhang*

Main category: cs.CL

TL;DR: KDR-Agent is a multi-agent framework for low-resource NER that uses knowledge retrieval, disambiguation, and reflective analysis to overcome limitations of existing ICL methods.


<details>
  <summary>Details</summary>
Motivation: Existing ICL-based NER methods have three key limitations: dependency on dynamic retrieval of annotated examples, poor generalization to unseen domains, and inability to incorporate external knowledge or resolve entity ambiguities.

Method: Proposes KDR-Agent framework with specialized agents for knowledge retrieval from Wikipedia, entity disambiguation via contextual reasoning, and reflective self-assessment to correct predictions. Uses natural-language type definitions and static entity-level contrastive demonstrations.

Result: Significantly outperforms existing zero-shot and few-shot ICL baselines across ten datasets from five domains using multiple LLM backbones.

Conclusion: KDR-Agent effectively addresses key limitations of ICL-based NER by integrating knowledge retrieval, disambiguation, and reflection, demonstrating strong performance in low-resource multi-domain scenarios.

Abstract: In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.

</details>


### [67] [DeCoRL: Decoupling Reasoning Chains via Parallel Sub-Step Generation and Cascaded Reinforcement for Interpretable and Scalable RLHF](https://arxiv.org/abs/2511.19097)
*Ziyuan Gao,Di Liang,Xianjie Wu,Philippe Morel,Minlong Peng*

Main category: cs.CL

TL;DR: DeCoRL is a novel RL framework that transforms sequential Chain-of-Thought reasoning into parallel modular orchestration, achieving faster inference, better interpretability, and higher efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for Chain-of-Thought reasoning suffer from undifferentiated reward signals that obscure individual step contributions and sequential decoding with O(n) time complexity, making real-time deployment impractical.

Method: Trains lightweight specialized models to generate reasoning sub-steps concurrently, uses modular reward functions to score each sub-step independently, and applies cascaded DRPO optimization to coordinate rewards while preserving inter-step dependencies.

Result: State-of-the-art results across multiple benchmarks, 3.8x faster inference, 22.7% improvement in interpretability, 72.4% reduction in energy consumption, and 68% increase in throughput.

Conclusion: DeCoRL makes real-time deployment of complex reasoning systems practical by eliminating sequential bottlenecks and enabling precise error attribution through parallel modular processing.

Abstract: Existing reinforcement learning methods for Chain-of-Thought reasoning suffer from two critical limitations. First, they operate as monolithic black boxes that provide undifferentiated reward signals, obscuring individual step contributions and hindering error diagnosis. Second, sequential decoding has O(n) time complexity. This makes real-time deployment impractical for complex reasoning tasks. We present DeCoRL (Decoupled Reasoning Chains via Coordinated Reinforcement Learning), a novel framework that transforms reasoning from sequential processing into collaborative modular orchestration. DeCoRL trains lightweight specialized models to generate reasoning sub-steps concurrently, eliminating sequential bottlenecks through parallel processing. To enable precise error attribution, the framework designs modular reward functions that score each sub-step independently. Cascaded DRPO optimization then coordinates these rewards while preserving inter-step dependencies. Comprehensive evaluation demonstrates state-of-the-art results across RM-Bench, RMB, and RewardBench, outperforming existing methods including large-scale models. DeCoRL delivers 3.8 times faster inference while maintaining superior solution quality and offers a 22.7\% improvement in interpretability through explicit reward attribution. These advancements, combined with a 72.4\% reduction in energy consumption and a 68\% increase in throughput, make real-time deployment of complex reasoning systems a reality.

</details>


### [68] [A symbolic Perl algorithm for the unification of Nahuatl word spellings](https://arxiv.org/abs/2511.19118)
*Juan-José Guzmán-Landa,Jesús Vázquez-Osorio,Juan-Manuel Torres-Moreno,Ligia Quintana Torres,Miguel Figueroa-Saavedra,Martha-Lorena Avendaño-Garrido,Graham Ranger,Patricia Velázquez-Morales,Gerardo Eugenio Sierra Martínez*

Main category: cs.CL

TL;DR: Automatic orthographic unification of Nawatl text documents using symbolic regular expressions and linguistic rules, evaluated through semantic tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a model for automatically unifying Nawatl text documents written in different orthographies, addressing the challenge of orthographic variation in the language.

Method: Symbolic model based on algorithms for Nawatl sentence analysis, using the $π$-yalli corpus with multiple orthographies. Implementation of linguistic rules via symbolic regular expressions for automatic unification, with manual evaluation protocol for quality assessment.

Result: Encouraging results from evaluators for most desired features of the artificially unified sentences, showing effectiveness in the semantic task evaluation.

Conclusion: The proposed symbolic model successfully performs automatic orthographic unification of Nawatl texts, with promising evaluation outcomes supporting its utility.

Abstract: In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $π$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences

</details>


### [69] [On the Optimality of Discrete Object Naming: a Kinship Case Study](https://arxiv.org/abs/2511.19120)
*Phong Le,Mees Lindeman,Raquel G. Alhama*

Main category: cs.CL

TL;DR: The paper introduces an information-theoretic framework for naming systems that addresses limitations of prior work by considering non-optimal listeners and varying communicative needs across languages.


<details>
  <summary>Details</summary>
Motivation: To overcome simplifications in prior work that assumed optimal listeners and universal communicative needs, and to develop a more realistic framework for analyzing naming systems in natural languages.

Method: Developed an information-theoretic framework for discrete object naming systems, used referential game setup from emergent communication, and focused on kinship semantic domain to test theoretical predictions.

Result: Proved that optimal trade-off between informativeness and complexity is achievable only when listener's decoder matches Bayesian decoder of speaker, and showed this optimality emerges empirically in learned communication systems.

Conclusion: The proposed framework successfully addresses limitations of prior work and demonstrates that theoretically predicted optimal naming systems can emerge in practice through learning processes.

Abstract: The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.

</details>


### [70] [Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2511.19122)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: The paper introduces an emotion-enhanced multi-task framework for aspect category sentiment analysis that jointly learns sentiment polarity and category-specific emotions using Ekman's six basic emotions, with a VAD-based refinement mechanism to ensure emotion accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing ACSA approaches primarily focus on sentiment polarity while ignoring the underlying emotional dimensions that shape sentiment expressions, limiting the model's ability to capture fine-grained affective signals toward specific aspect categories.

Method: A multi-task framework that leverages LLMs to generate emotional descriptions for aspect categories, with an emotion refinement mechanism using the Valence-Arousal-Dominance (VAD) framework to project emotions and re-annotate inconsistent ones through structured LLM-based refinement.

Result: Experimental results show the approach significantly outperforms strong baselines on all benchmark datasets, demonstrating the effectiveness of integrating affective dimensions into ACSA.

Conclusion: Integrating emotional dimensions with sentiment polarity through the proposed emotion-enhanced multi-task framework significantly improves aspect category sentiment analysis performance.

Abstract: Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.

</details>


### [71] [Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization](https://arxiv.org/abs/2511.19131)
*Zijian Wang,Yanxiang Ma,Chang Xu*

Main category: cs.CL

TL;DR: A novel probabilistic approach for eliciting Chain-of-Thought reasoning from base LLMs through hidden state manipulation that outperforms existing steering methods.


<details>
  <summary>Details</summary>
Motivation: Base LLMs struggle with reasoning due to lack of specialized training, and existing hidden state manipulation methods cause distribution shifts and degraded text quality.

Method: Reformulates the challenge as an optimization problem with balanced likelihood and prior regularization framework to guide hidden states toward reasoning-oriented trajectories while preserving linguistic coherence.

Result: Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks show consistent outperformance over existing steering methods.

Conclusion: Provides a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs through probabilistic conditional generation.

Abstract: Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.

</details>


### [72] [Representational Stability of Truth in Large Language Models](https://arxiv.org/abs/2511.19166)
*Samantha Dies,Courtney Maynard,Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: LLMs show varying stability in truth representations - unfamiliar fictional statements cause large boundary shifts (up to 40% flipped judgments) while familiar fictional statements remain stable (≤8.2% changes), suggesting stability comes from epistemic familiarity rather than linguistic form.


<details>
  <summary>Details</summary>
Motivation: To understand how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations, and assess representational stability under perturbations in truth definitions.

Method: Train linear probes on LLM activations to separate true from not-true statements, measure decision boundary shifts under controlled label changes using 16 open-source models across 3 factual domains, comparing unfamiliar vs familiar fictional statements.

Result: Unfamiliar statements (about entities absent from training data) induce largest boundary shifts (up to 40% flipped truth judgments), while familiar fictional statements remain coherently clustered with smaller changes (≤8.2%).

Conclusion: Representational stability stems more from epistemic familiarity than linguistic form, providing a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty.

Abstract: Large language models (LLMs) are widely used for factual tasks such as "What treats asthma?" or "What is the capital of Latvia?". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\leq 8.2\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.

</details>


### [73] [In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations](https://arxiv.org/abs/2511.19232)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: The study explores how transformer models detect semantic violations in sentences, revealing that detection accuracy improves in middle layers and that violation encoding undergoes an exploratory phase followed by consolidation.


<details>
  <summary>Details</summary>
Motivation: To understand how and where transformer models detect semantic anomalies in sentences, and to compare this with human psycholinguistic processing.

Method: Used causal language model (phi-2) with plausible/implausible sentence endings, analyzed hidden states across layers using linear probes and examined the dimensionality of violation encoding.

Result: Linear decoder struggled in lower layers but accuracy sharply increased in middle layers; violation encoding showed initial widening of representational subspace followed by collapse after mid-stack bottleneck.

Conclusion: Transformer semantic violation detection aligns with human psycholinguistic findings, where semantic anomalies are detected after syntactic resolution in later processing stages.

Abstract: How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.

</details>


### [74] [MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset](https://arxiv.org/abs/2511.19317)
*Md. Tanzim Ferdous,Naeem Ahsan Chowdhury,Prithwiraj Bhattacharjee*

Main category: cs.CL

TL;DR: Created a new Bangla abstractive summarization dataset with 54,000+ articles from diverse sources to address the limitation of existing news-only datasets and enable more adaptable summarization systems.


<details>
  <summary>Details</summary>
Motivation: Existing Bangla summarization studies focus mainly on news articles with fixed writing styles, failing to handle the diverse real-world Bangla content from blogs, newspapers, and social media. There's a pressing need to reduce information overload and help readers understand content quickly.

Method: Developed a dataset of over 54,000 Bangla articles and summaries from multiple sources including blogs (Cinegolpo) and newspapers (Samakal, The Business Standard). Trained and evaluated using deep learning models like LSTM, BanglaT5-small, and MTS-small.

Result: The multi-domain dataset spans various domains and writing styles, offering greater adaptability and practical relevance. Strong baselines were established, highlighting its potential as a benchmark for future Bangla NLP research.

Conclusion: This dataset provides a solid foundation for building robust Bangla summarization systems and helps expand NLP resources for low-resource languages, addressing the gap in diverse text processing capabilities.

Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.

</details>


### [75] [Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces](https://arxiv.org/abs/2511.19333)
*Shaltiel Shmidman,Asher Fredman,Oleg Sudakov,Meriem Bendris*

Main category: cs.CL

TL;DR: Comparison of medium-sized LLMs' performance on math problems after post-training with reasoning traces from DeepSeek-R1 vs gpt-oss models, focusing on accuracy and inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Test-time scaling enables LLMs to reason through complex problems, and their reasoning traces can serve as high-quality supervised data for teaching reasoning capabilities to smaller models without expensive human curation.

Method: Post-training medium-sized LLMs on reasoning traces generated by two different frontier models (DeepSeek-R1 and gpt-oss), then evaluating their performance on math problems.

Result: The paper compares the impact of reasoning traces from DeepSeek-R1 and gpt-oss LLMs on accuracy and inference efficiency of medium-sized models.

Conclusion: Analysis of which reasoning trace source (DeepSeek-R1 vs gpt-oss) provides better performance for teaching reasoning capabilities to medium-sized LLMs on mathematical problems.

Abstract: Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.

</details>


### [76] [DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research](https://arxiv.org/abs/2511.19399)
*Rulin Shao,Akari Asai,Shannon Zejiang Shen,Hamish Ivison,Varsha Kishore,Jingming Zhuo,Xinran Zhao,Molly Park,Samuel G. Finlayson,David Sontag,Tyler Murray,Sewon Min,Pradeep Dasigi,Luca Soldaini,Faeze Brahman,Wen-tau Yih,Tongshuang Wu,Luke Zettlemoyer,Yoon Kim,Hannaneh Hajishirzi,Pang Wei Koh*

Main category: cs.CL

TL;DR: RLER enables training open models for long-form deep research by evolving rubrics with the policy model, resulting in DR Tulu-8B that matches proprietary systems while being smaller and cheaper.


<details>
  <summary>Details</summary>
Motivation: Existing open deep research models are trained on short-form QA tasks with verifiable rewards, which doesn't extend to realistic long-form research tasks.

Method: Reinforcement Learning with Evolving Rubrics (RLER) constructs rubrics that co-evolve with the policy model during training, incorporating newly explored information and providing discriminative feedback.

Result: DR Tulu-8B substantially outperforms existing open deep research models and matches/exceeds proprietary systems across four benchmarks in science, healthcare and general domains.

Conclusion: RLER successfully enables training of open models for long-form deep research, with DR Tulu-8B achieving state-of-the-art performance while being more efficient than proprietary alternatives.

Abstract: Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.

</details>


### [77] [Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration](https://arxiv.org/abs/2511.19417)
*James Y. Huang,Sheng Zhang,Qianchu Liu,Guanghui Qin,Tinghui Zhu,Tristan Naumann,Muhao Chen,Hoifung Poon*

Main category: cs.CL

TL;DR: BeMyEyes is a modular multi-agent framework that enables LLMs to perform multimodal reasoning by orchestrating collaboration between efficient VLMs as perceivers and powerful LLMs as reasoners, avoiding the need for training large-scale multimodal models.


<details>
  <summary>Details</summary>
Motivation: To extend LLMs' capabilities to multimodal reasoning without costly development of large-scale vision language models, while preserving LLMs' broad knowledge and reasoning capabilities that smaller VLMs often lack.

Method: Proposes a modular multi-agent framework with perceiver agents (efficient VLMs) and reasoner agents (powerful LLMs) collaborating through conversations, plus a data synthesis and supervised fine-tuning pipeline to train the perceiver agent.

Result: Enables text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver to outperform large-scale proprietary VLMs like GPT-4o on knowledge-intensive multimodal tasks, demonstrating effective multimodal reasoning capabilities.

Conclusion: The framework provides an effective, modular, and scalable approach for building multimodal reasoning systems by combining complementary strengths of perception and reasoning agents without training large multimodal models.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks](https://arxiv.org/abs/2511.17576)
*Rayan Aldajani*

Main category: cs.CV

TL;DR: AI models using frontal body images and anthropometric data can estimate body fat percentage with RMSE of 4.44% and R² of 0.807, offering low-cost alternatives to expensive DEXA scans.


<details>
  <summary>Details</summary>
Motivation: Gold-standard body fat measurement methods like DEXA scans are expensive and inaccessible for most people, creating a need for affordable alternatives.

Method: Developed two approaches: ResNet-based image models and regression models using anthropometric measurements (weight, height, neck, ankle, wrist), with a multimodal fusion framework proposed for future use.

Result: Image-based model achieved RMSE of 4.44% and R² of 0.807, demonstrating good predictive performance for body fat estimation.

Conclusion: AI-assisted models provide accessible and low-cost body fat estimates that can support future consumer applications in health and fitness.

Abstract: Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.

</details>


### [79] [Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding](https://arxiv.org/abs/2511.17596)
*Yassir Benhammou,Suman Kalyan,Sujay Kumar*

Main category: cs.CV

TL;DR: Proposes a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data for automated metadata extraction and semantic clustering in broadcast content.


<details>
  <summary>Details</summary>
Motivation: Existing AI systems operate on single modalities, limiting understanding of cross-modal relationships in broadcast material, while current methods require large paired datasets.

Method: Uses a Multimodal Autoencoder trained on LUMA dataset with joint reconstruction losses across modalities to discover modality-invariant semantic structures without large paired datasets.

Result: Significant improvements in clustering metrics (Silhouette, ARI, NMI) compared to linear baselines, demonstrating effective cross-modal representation learning.

Conclusion: Reconstruction-based multimodal embeddings can serve as foundation for scalable metadata generation and cross-modal retrieval, enhancing automation and content management in broadcast workflows.

Abstract: Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.

</details>


### [80] [BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction](https://arxiv.org/abs/2511.17597)
*Zhengsen Xu,Sibo Cheng,Hongjie He,Lanying Wang,Wentao Sun,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: A 25-year daily wildfire dataset covering 240M hectares with 38 covariates is introduced, enabling evaluation of various time-series forecasting models and analysis of fire-driving factors.


<details>
  <summary>Details</summary>
Motivation: Wildfire risk prediction is challenging due to complex interactions among multiple factors, and there's a scarcity of public benchmark datasets supporting long-term temporal modeling, large-scale spatial coverage, and multimodal drivers.

Method: Created a comprehensive 25-year daily-resolution dataset covering 240M hectares with 38 covariates including fire detections, weather, fuel conditions, terrain, and anthropogenic factors. Evaluated CNN-based, linear-based, Transformer-based, and Mamba-based time-series forecasting models.

Result: The dataset enables benchmarking of various forecasting architectures and investigation of position embedding effectiveness and relative importance of different fire-driving factors.

Conclusion: The presented dataset addresses the gap in wildfire prediction benchmarks and provides a foundation for evaluating diverse forecasting models while analyzing key fire-driving factors.

Abstract: Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire

</details>


### [81] [Robustness of Structured Data Extraction from Perspectively Distorted Documents](https://arxiv.org/abs/2511.17607)
*Hyakka Nakada,Yoshiyasu Tanaka*

Main category: cs.CV

TL;DR: This study investigates how perspective distortions and rotations affect data extraction accuracy in multi-modal LLMs (specifically Gemini-1.5-pro) for OCR tasks, finding that structure recognition accuracy degrades significantly but can be improved with simple rotational correction.


<details>
  <summary>Details</summary>
Motivation: Real-world document images often contain both in-plane rotations and perspective distortions, which impact OCR accuracy in multi-modal LLMs, but existing research has mainly focused on rotations while neglecting perspective distortions.

Method: The researchers observed typical document distortions and found they follow isosceles-trapezoidal transformations, reducing parameters from eight to two (rotation angle and distortion ratio). They extracted entities from synthetically generated documents while varying these parameters and evaluated both character-recognition and structure-recognition accuracy.

Result: Structure-recognition accuracy was significantly degraded by document distortion, while character-recognition accuracy was less affected. Simple rotational correction was found to improve the structure-recognition accuracy.

Conclusion: Document distortions significantly impact structure recognition in multi-modal LLMs for OCR, but simple rotational corrections can mitigate these effects, contributing to practical applications of these models in real-world OCR tasks.

Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.

</details>


### [82] [3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF](https://arxiv.org/abs/2511.17609)
*Linh Van Ma,Unse Fatima,Tepy Sokun Chriv,Haroon Imran,Moongu Jeon*

Main category: cs.CV

TL;DR: A method using Unscented Kalman Filter to fuse 2D bounding boxes or pose keypoints from multiple calibrated cameras into accurate 3D ground truth, handling occlusion and providing full 3D shape information.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D ground truth estimation is critical for autonomous navigation, surveillance, and robotics applications. Existing approaches often provide only ground-plane information without full 3D shape.

Method: Multi-camera single-object tracking algorithm that transforms 2D image coordinates into 3D world coordinates through homography-based projection and UKF-based fusion, using human-annotated 2D ground truth from multiple calibrated cameras.

Result: High accuracy in 3D localization demonstrated on CMC, Wildtrack, and Panoptic datasets compared to available 3D ground truth. The method outputs full 3D shape of each object, unlike existing approaches.

Conclusion: The algorithm provides a scalable and fully automatic solution for multi-camera systems using only 2D image annotations, effectively handling challenges like occlusion while delivering accurate 3D ground truth with complete shape information.

Abstract: Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.

</details>


### [83] [Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression](https://arxiv.org/abs/2511.17612)
*Siddiqua Namrah*

Main category: cs.CV

TL;DR: Unsupervised multi-stage deep learning framework for enhancing low-light traffic images through illumination adaptation, reflectance restoration, and over-exposure compensation without paired ground-truth data.


<details>
  <summary>Details</summary>
Motivation: Low-light traffic images suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare, which hinder autonomous driving and surveillance tasks.

Method: Multi-stage framework decomposing images into illumination/reflectance components with three modules: Illumination Adaptation, Reflectance Restoration with spatial-channel attention, and Over-Exposure Compensation, trained using self-supervised losses.

Result: Superior performance over state-of-the-art methods on both general and traffic-specific datasets in quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality.

Conclusion: The approach effectively enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios without requiring paired training data.

Abstract: Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.

</details>


### [84] [HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2511.17614)
*Danyang Sun,Fadi Dornaika,Nagore Barrena*

Main category: cs.CV

TL;DR: HSMix is a novel data augmentation method for medical image segmentation that combines hard mixing of superpixels from two images with soft brightness adjustment using saliency coefficients, preserving local semantics while increasing diversity.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation faces data scarcity issues due to high annotation costs and rare diseases. While self-supervised and semi-supervised learning help, they are complex. Data augmentation offers a simpler solution, but local image editing techniques for segmentation are under-explored.

Method: HSMix creates hard-augmented images by mixing homogeneous regions (superpixels) from two source images, then applies soft brightness mixing using locally aggregated pixel-wise saliency coefficients. Ground-truth masks undergo the same mixing operations.

Result: Extensive experiments demonstrate HSMix's effectiveness across various medical segmentation tasks. The method preserves local semantic information while enriching augmentation diversity.

Conclusion: HSMix is a plug-and-play, model-agnostic solution that fully exploits contour and saliency information to address data scarcity in medical image segmentation through effective local image editing augmentation.

Abstract: Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.

</details>


### [85] [Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2511.17615)
*Young-Beom Woo*

Main category: cs.CV

TL;DR: PnP-MIX is a tuning-free method for multi-concept personalization in text-to-image generation that uses guided appearance attention, mask-guided noise mixing, and background dilution++ to prevent concept leakage and preserve compositional fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-concept personalization often fail in complex scenes, causing unintended alterations in both personalized and non-personalized regions, disrupting semantic consistency and prompt structure.

Method: The approach combines guided appearance attention for faithful concept representation, mask-guided noise mixing to preserve non-personalized regions, and background dilution++ to prevent concept leakage.

Result: Extensive experiments show PnP-MIX consistently outperforms existing methods in both single- and multi-concept personalization scenarios without requiring additional model tuning.

Conclusion: PnP-MIX provides a robust, tuning-free solution for high-fidelity multi-concept personalization that maintains compositional integrity and prevents semantic inconsistencies.

Abstract: Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.

</details>


### [86] [Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach](https://arxiv.org/abs/2511.17618)
*Ju-Young Oh*

Main category: cs.CV

TL;DR: FIQ enhances video question answering by generating foundational Q&A pairs from video content to improve scene understanding and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing VQA datasets are event-centric and lack fundamental scene information like object categories and spatial configurations, limiting model generalization and reasoning.

Method: Generates Q&A pairs from descriptive video information and uses VQ-CAlign module to align question embeddings with visual features for better contextual preservation.

Result: Achieves state-of-the-art performance on SUTD-TrafficQA dataset, surpassing existing baseline approaches.

Conclusion: FIQ framework improves VQA reasoning by enhancing foundational scene comprehension through generated Q&A pairs and visual-question alignment.

Abstract: Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.

</details>


### [87] [Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds](https://arxiv.org/abs/2511.17619)
*Qinghao Meng,Junbo Yin,Jianbing Shen,Yunde Jia*

Main category: cs.CV

TL;DR: Proposes corner-aligned regression for LiDAR 3D object detection to replace unstable center-based methods, using geometrically informative corners in dense regions and enabling weakly supervised learning with only corner annotations.


<details>
  <summary>Details</summary>
Motivation: Center-aligned regression suffers from instability because object centers often fall in sparse or empty BEV regions due to LiDAR's front-surface-biased point clouds, leading to noisy and inaccurate bounding box predictions.

Method: Shifts prediction target from centers to corners in dense regions, leverages geometric constraints between corners and 2D boxes to recover partial 3D parameters, and designs a corner-aware detection head that can be integrated into existing detectors.

Result: Improves performance by 3.5% AP over center-based baseline on KITTI dataset, and achieves 83% of fully supervised accuracy using only BEV corner clicks.

Conclusion: Corner-aligned regression strategy effectively addresses the limitations of center-based methods and enables efficient weakly supervised learning for 3D object detection.

Abstract: Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.

</details>


### [88] [BD-Net: Has Depth-Wise Convolution Ever Been Applied in Binary Neural Networks?](https://arxiv.org/abs/2511.17633)
*DoYoung Kim,Jin-Seop Lee,Noo-ri Kim,SungJoon Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: Proposes 1.58-bit convolution and pre-BN residual connection to enable successful binarization of depth-wise convolutions in Binary Neural Networks, achieving state-of-the-art performance with 33M OPs on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Extreme quantization in Binary Neural Networks limits representational capacity and destabilizes training, especially for lightweight architectures with depth-wise convolutions.

Method: Introduces 1.58-bit convolution for enhanced expressiveness and pre-BN residual connection to stabilize optimization by improving Hessian condition number.

Result: Achieves 33M OPs on ImageNet with MobileNet V1, outperforming prior methods with comparable OPs and showing up to 9.3 percentage points accuracy improvement across multiple datasets.

Conclusion: Successfully enables binarization of depth-wise convolutions in BNNs, establishing new state-of-the-art performance while maintaining extreme efficiency.

Abstract: Recent advances in model compression have highlighted the potential of low-bit precision techniques, with Binary Neural Networks (BNNs) attracting attention for their extreme efficiency. However, extreme quantization in BNNs limits representational capacity and destabilizes training, posing significant challenges for lightweight architectures with depth-wise convolutions. To address this, we propose a 1.58-bit convolution to enhance expressiveness and a pre-BN residual connection to stabilize optimization by improving the Hessian condition number. These innovations enable, to the best of our knowledge, the first successful binarization of depth-wise convolutions in BNNs. Our method achieves 33M OPs on ImageNet with MobileNet V1, establishing a new state-of-the-art in BNNs by outperforming prior methods with comparable OPs. Moreover, it consistently outperforms existing methods across various datasets, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements of up to 9.3 percentage points.

</details>


### [89] [Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection](https://arxiv.org/abs/2511.17634)
*Kaikwan Lau,Andrew S. Na,Justin W. L. Wan*

Main category: cs.CV

TL;DR: A novel framework accelerates score-based diffusion models by converting them to Fokker-Planck formulation and using cross-matrix Krylov projection to solve large linear systems efficiently.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models require solving large linear systems for each image, leading to high computational costs when training with many images.

Method: Convert stable diffusion to Fokker-Planck formulation, then use cross-matrix Krylov projection that exploits mathematical similarities between matrices using shared subspaces from seed matrices to solve target matrices rapidly.

Result: Achieves 15.8% to 43.7% time reduction over standard sparse solvers, up to 115× speedup over DDPM baselines in denoising tasks, and produces high-quality images under fixed computational budget where DDPM fails.

Conclusion: The approach provides a practical method for efficient generation in resource-limited settings by significantly accelerating diffusion models while maintaining image quality.

Abstract: This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed" matrices to rapidly solve for subsequent ``target" matrices. Our experiments show that this technique achieves a 15.8\% to 43.7\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.

</details>


### [90] [Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification](https://arxiv.org/abs/2511.17635)
*Max A. Nelson,Elif Keles,Eminenur Sen Tasci,Merve Yazol,Halil Ertugrul Aktas,Ziliang Hong,Andrea Mia Bejar,Gorkem Durak,Oznur Leman Boyunaga,Ulas Bagci*

Main category: cs.CV

TL;DR: UPMI is a lightweight augmentation method that generates synthetic meta-features in low-dimensional space to improve pediatric pancreatitis diagnosis from multimodal MRI data, achieving ~5% AUC improvement over baseline.


<details>
  <summary>Details</summary>
Motivation: Pediatric pancreatitis diagnosis is challenging due to limited sample availability and complex multimodal imaging, which also poses difficulties for machine learning methods.

Method: UPMI uses modality-specific logistic regressions on T1W/T2W MRI radiomics to create 7D meta-features, then fits class-conditional GMMs to sample synthetic meta-features that train a Random Forest meta-classifier.

Result: On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieved mean AUC of 0.908 ± 0.072, a ~5% relative gain over real-only baseline (AUC 0.864 ± 0.061).

Conclusion: UPMI effectively addresses data scarcity in pediatric pancreatitis diagnosis by operating in meta-feature space rather than image space, demonstrating significant performance improvements.

Abstract: Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).

</details>


### [91] [TSRE: Channel-Aware Typical Set Refinement for Out-of-Distribution Detection](https://arxiv.org/abs/2511.17636)
*Weijun Gao,Rundong He,Jinyang Dong,Yongshun Gong*

Main category: cs.CV

TL;DR: Proposes a channel-aware typical set refinement method for OOD detection that considers channel discriminability and activity, along with skewness-based refinement to mitigate distributional bias, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing activation-based OOD detection methods overlook channel characteristics and distributional skewness, leading to inaccurate typical set estimation and improper inclusion of anomalous activations.

Method: Typical set refinement based on discriminability and activity to create channel-aware typical sets, skewness-based refinement to mitigate distributional bias, and energy score computation using rectified activations.

Result: Achieves state-of-the-art performance on ImageNet-1K and CIFAR-100 benchmarks, with effective generalization across different backbones and score functions.

Conclusion: The proposed channel-aware and skewness-based refinement approach significantly improves OOD detection by addressing limitations in existing activation rectification methods.

Abstract: Out-of-Distribution (OOD) detection is a critical capability for ensuring the safe deployment of machine learning models in open-world environments, where unexpected or anomalous inputs can compromise model reliability and performance. Activation-based methods play a fundamental role in OOD detection by mitigating anomalous activations and enhancing the separation between in-distribution (ID) and OOD data. However, existing methods apply activation rectification while often overlooking channel's intrinsic characteristics and distributional skewness, which results in inaccurate typical set estimation. This discrepancy can lead to the improper inclusion of anomalous activations across channels. To address this limitation, we propose a typical set refinement method based on discriminability and activity, which rectifies activations into a channel-aware typical set. Furthermore, we introduce a skewness-based refinement to mitigate distributional bias in typical set estimation. Finally, we leverage the rectified activations to compute the energy score for OOD detection. Experiments on the ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method achieves state-of-the-art performance and generalizes effectively across backbones and score functions.

</details>


### [92] [SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios](https://arxiv.org/abs/2511.17649)
*Jieru Lin,Zhiwei Yu,Börje F. Karlsson*

Main category: cs.CV

TL;DR: SWITCH is an embodied benchmark for evaluating AI agents' ability to interact with real-world control interfaces through egocentric video, testing perception, reasoning, and action verification.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks lack testing for real-world interaction with tangible control interfaces, which requires commonsense reasoning, physics understanding, and outcome verification in situated environments.

Method: Created SWITCH-Basic benchmark with 351 tasks across 98 real devices, evaluating five abilities: task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification using egocentric RGB video input.

Result: Commercial and open LMMs show inconsistent performance on single-step interactions, often over-relying on text cues and under-using visual evidence, with high aggregate scores masking specific failures.

Conclusion: SWITCH provides reproducible evaluation framework to address gaps in embodied AI interaction capabilities and enables community contributions for more challenging future iterations.

Abstract: Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.

</details>


### [93] [Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment](https://arxiv.org/abs/2511.17655)
*Md. Mohaiminul Islam,Md. Mofazzal Hossen,Maher Ali Rusho,Nahiyan Nazah Ridita,Zarin Tasnia Shanta,Md. Simanto Haider,Ahmed Faizul Haque Dhrubo,Md. Khurshid Jahan,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: A comprehensive deep learning system for brain tumor classification from MRI images using six CNN architectures, achieving state-of-the-art accuracy (99.53%) with Inception-ResNet V2 and developing a compact 1.31M parameter model for edge deployment.


<details>
  <summary>Details</summary>
Motivation: To create a standardized, interpretable, and deployable AI system for brain tumor classification that addresses the black-box problem and works in both advanced and low-resource healthcare settings.

Method: Used six CNN architectures (five ImageNet-pre-trained models and one custom compact CNN) with standardized preprocessing, AdamW optimizer, CosineAnnealingLR, and early stopping. Applied Grad-CAM and GradientShap for interpretability and evaluated with multiple metrics including IoU, Hausdorff distance, and precision-recall curves.

Result: Inception-ResNet V2 achieved 99.53% test accuracy with precision, recall, and F1-score ≥99.50%. The custom compact CNN achieved 96.49% accuracy with 100x smaller size than Inception-ResNet V2 and real-time inference (375ms) on edge devices.

Conclusion: The study provides an end-to-end solution that balances accuracy, interpretability, and deployability, enabling clinical screening and triage in both advanced and low-resource healthcare systems.

Abstract: Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.

</details>


### [94] [MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation](https://arxiv.org/abs/2511.17668)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: MedPEFT-CL is a parameter-efficient continual learning framework for medical vision-language segmentation that prevents catastrophic forgetting when adapting to new anatomical structures using dual-phase architecture with semantic-driven adapter allocation and bidirectional Fisher-memory coordination.


<details>
  <summary>Details</summary>
Motivation: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits clinical deployment. Continual learning approaches specifically designed for medical vision-language tasks remain underexplored.

Method: Dual-phase architecture based on CLIPSeg: (1) adaptive learning phase with semantic similarity-based adapter allocation and parameter-efficient fine-tuning through prompt similarity analysis, (2) knowledge consolidation phase with bi-directional Fisher-memory coordination. Uses bi-modal LoRA adaptation to reduce trainable parameters.

Result: Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead.

Conclusion: The framework is effective for continual learning in medical vision-language scenarios, enabling efficient learning of new tasks while preserving previous knowledge without catastrophic forgetting.

Abstract: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.

</details>


### [95] [Person Recognition in Aerial Surveillance: A Decade Survey](https://arxiv.org/abs/2511.17674)
*Kien Nguyen,Feng Liu,Clinton Fookes,Sridha Sridharan,Xiaoming Liu,Arun Ross*

Main category: cs.CV

TL;DR: A comprehensive survey of 150+ papers on human-centric aerial surveillance using drones/UAVs, covering detection, identification, and re-identification tasks from computer vision and machine learning perspectives.


<details>
  <summary>Details</summary>
Motivation: The rapid emergence of airborne platforms and imaging sensors enables new forms of aerial surveillance with unprecedented advantages in scale, mobility, deployment, and covert observation capabilities.

Method: Systematic review and technical analysis of current aerial surveillance research, identifying unique challenges compared to ground-based settings, compiling available datasets, and analyzing approaches that address aerial-specific challenges.

Result: Comprehensive overview of human-centric aerial surveillance tasks over the last 10 years, with deep analysis of how current methods address aerial challenges and techniques for improvement.

Conclusion: Identifies gaps and open research questions to inform future research avenues in aerial surveillance.

Abstract: The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.

</details>


### [96] [Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models](https://arxiv.org/abs/2511.17681)
*Weiyi Lv,Ning Zhang,Hanyang Sun,Haoran Jiang,Kai Zhao,Jing Xiao,Dan Zeng*

Main category: cs.CV

TL;DR: VMRMOT is a novel RMOT framework that integrates motion modality using MLLMs to address the limitation of static language references in tracking dynamic object motion.


<details>
  <summary>Details</summary>
Motivation: Current RMOT benchmarks use static language references that fail to capture dynamic motion changes, causing temporal discrepancies and limiting multi-modal tracking performance.

Method: Proposes VMRMOT framework with motion-aware descriptions from object dynamics, VMRA module for hierarchical cross-modal alignment, and MGPH for motion-guided prediction enhancement using MLLMs.

Result: Extensive experiments on multiple RMOT benchmarks show VMRMOT outperforms existing state-of-the-art methods.

Conclusion: VMRMOT successfully addresses the static reference limitation in RMOT by integrating motion modality through MLLMs, achieving superior tracking performance.

Abstract: Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.

</details>


### [97] [Understanding Counting Mechanisms in Large Language and Vision-Language Models](https://arxiv.org/abs/2511.17699)
*Hosein Hasani,Amirmohammad Izadi,Fatemeh Askari,Mobin Bagherian,Sadegh Mohammadian,Mohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: This paper investigates how LLMs and LVLMs handle numerical counting tasks through mechanistic interpretability, revealing layerwise emergence of numerical representations and internal counter mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand how large language and vision-language models represent and compute numerical information in counting tasks, particularly the underlying mechanisms and representations.

Method: Used controlled experiments with repeated items, causal mediation analysis, activation patching, and developed CountScope tool for mechanistic interpretability of numerical content.

Result: Found that tokens/visual features encode latent positional count information, numerical representations emerge progressively across layers, and models use internal counter mechanisms and structural cues like separators.

Conclusion: Counting emerges as a structured, layerwise process in LLMs and follows similar patterns in LVLMs, shaped by vision encoder properties, with identifiable counter mechanisms and transferable representations.

Abstract: This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.

</details>


### [98] [Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions](https://arxiv.org/abs/2511.17722)
*Saurav Sengupta,Nazanin Moradinasab,Jiebei Liu,Donald E. Brown*

Main category: cs.CV

TL;DR: VLMs struggle with counting tasks due to inherent biases and attention allocation issues, especially under complex visual/linguistic conditions. Attention interventions can modestly improve performance.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models often rely on training biases when answering visual property queries, particularly in counting tasks that require specific focus on image areas.

Method: Developed synthetic benchmark dataset and evaluation framework to analyze counting performance variations. Used open-source VLMs to study attention allocation changes with input parameters and implemented attention-based interventions to modulate visual token focus.

Result: VLM counting performance remains challenging under high visual/linguistic complexity, but certain attention interventions can lead to modest performance gains.

Conclusion: While VLMs face significant challenges in counting tasks, targeted attention interventions show promise for improving performance across varying visual conditions.

Abstract: Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.

</details>


### [99] [AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography](https://arxiv.org/abs/2511.17724)
*Mohammad Atwany,Mojtaba Lashgari,Robin P. Choudhury,Vicente Grau,Abhirup Banerjee*

Main category: cs.CV

TL;DR: AngioDG is a novel domain generalization method for coronary vessel segmentation in X-ray angiography that uses channel regularization to enhance domain-invariant features and suppress domain-specific ones, achieving superior out-of-distribution performance.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular diseases are the leading cause of death globally, and vessel segmentation from XCA can help with quantitative assessments like stenosis measurement. However, domain shifts due to imaging protocol variations and lack of annotated data make single-source domain generalization necessary.

Method: Proposes AngioDG with channel regularization strategy that identifies early feature channel contributions to task-specific metrics, then reweights channels to amplify domain-invariant features while attenuating domain-specific ones.

Result: Evaluated on 6 X-ray angiography datasets for coronary vessel segmentation, achieving best out-of-distribution performance among compared methods while maintaining consistent in-domain test performance.

Conclusion: The proposed AngioDG method effectively addresses domain generalization challenges in coronary vessel segmentation through interpretable channel regularization, demonstrating superior generalization capability across different datasets.

Abstract: Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.

</details>


### [100] [The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation](https://arxiv.org/abs/2511.17727)
*Victor Li,Naveenraj Kamalakannan,Avinash Parnandi,Heidi Schambra,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: VLMs show potential but current limitations for stroke rehabilitation video analysis - can classify high-level activities but lack fine-grained motion understanding for precise dose quantification and impairment scoring.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of vision-language models (VLMs) for automatic quantification of rehabilitation dose and impairment from videos in stroke rehabilitation, addressing fundamental challenges in data-driven healthcare.

Method: Formulated stroke rehabilitation analysis as motion-identification tasks using VLMs, evaluated on cohort of 29 healthy controls and 51 stroke survivors with optimized prompting and post-processing, without task-specific training.

Result: Current VLMs lack fine-grained motion understanding for precise quantification - dose estimates comparable to baseline without visual information, impairment scores unreliable. However, can classify high-level activities from few frames, detect motion/grasp with moderate accuracy, approximate dose counts within 25% of ground truth for mildly impaired/healthy participants.

Conclusion: VLMs show both current limitations and emerging opportunities for data-driven stroke rehabilitation and clinical video analysis - promising for high-level classification but need improvement for fine-grained motion understanding.

Abstract: Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.

</details>


### [101] [VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.17731)
*Lingxiao Li,Yifan Wang,Xinyan Gao,Chen Tang,Xiangyu Yue,Chenyu You*

Main category: cs.CV

TL;DR: VisReason is a large-scale dataset for visual Chain-of-Thought reasoning that enables MLLMs to perform systematic, human-like stepwise reasoning across visual domains, with VisReason-Pro providing expert-level annotations.


<details>
  <summary>Details</summary>
Motivation: Current visual-CoT resources are limited in scale, domain coverage, and lack human-like stepwise structure needed for compositional visual reasoning in MLLMs.

Method: Created VisReason (489K examples) and VisReason-Pro (165K subset) with multi-round human-like rationales, detailed reasoning traces, and 3D spatial grounding via depth-informed annotations.

Result: Fine-tuning Qwen2.5-VL on VisReason datasets yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization.

Conclusion: VisReason equips MLLMs with systematic and generalizable reasoning capabilities, serving as a cornerstone for developing human-like visual reasoning in multimodal intelligence.

Abstract: Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.

</details>


### [102] [Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders](https://arxiv.org/abs/2511.17735)
*Samuel Stevens,Jacob Beattie,Tanya Berger-Wolf,Yu Su*

Main category: cs.CV

TL;DR: Sparse autoencoders enable open-ended feature discovery from foundation model representations, allowing systematic exploration of unknown patterns in scientific data beyond pre-specified targets.


<details>
  <summary>Details</summary>
Motivation: Scientific archives contain massive datasets that could reveal undiscovered patterns, but current methods only extract structure for pre-specified targets and don't support open-ended discovery of unknown patterns.

Method: Use sparse autoencoders (SAEs) to decompose foundation model representations and discover features without supervision, evaluated through controlled rediscovery studies and concept-alignment metrics.

Result: SAEs successfully surface fine-grained anatomical structure in ecological imagery without segmentation or part labels, outperforming label-free alternatives on concept-alignment metrics.

Conclusion: Sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, enabling the transition from confirmation to genuine discovery across various scientific domains.

Abstract: Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.

</details>


### [103] [AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations](https://arxiv.org/abs/2511.17747)
*Dawid Wolkiewicz,Anastasiya Pechko,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

TL;DR: AEGIS is the first privacy-preserving identity masking framework for 3D Gaussian Avatars that conceals identity-related facial features while maintaining perceptual realism and functional integrity.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of photorealistic 3D facial avatars introduces risks of online identity theft, especially in biometric authentication systems, with existing methods only addressing 2D images and lacking robust, viewpoint-consistent protection for dynamic 3D avatars.

Method: AEGIS applies adversarial perturbations to the Gaussian color coefficients guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry.

Result: AEGIS achieves complete de-identification (0% face retrieval and verification accuracy) while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB) and preserving key facial attributes like age, race, gender, and emotion.

Conclusion: AEGIS demonstrates strong privacy protection for 3D Gaussian Avatars with minimal visual distortion, effectively addressing the gap in viewpoint-consistent identity protection for dynamic 3D avatars.

Abstract: The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.

</details>


### [104] [SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration](https://arxiv.org/abs/2511.17750)
*Zhimin Shao,Abhay Yadav,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: SPIDER is a universal feature matching framework that combines 2D and 3D correspondence estimation to handle challenging cross-domain image matching with large viewpoint changes.


<details>
  <summary>Details</summary>
Motivation: Traditional 2D feature matching struggles with large appearance, scale, and viewpoint variations across domains. While 3D foundation models provide spatial coherence, they focus on dominant planar regions and miss fine-grained geometric details.

Method: Integrates shared feature extraction with specialized network heads for both 2D-based and 3D-based correspondences, using a coarse-to-fine approach. Also introduces a new evaluation benchmark for unconstrained scenarios with large baselines.

Result: SPIDER significantly outperforms state-of-the-art methods on the proposed benchmark, demonstrating superior performance as a universal image-matching method.

Conclusion: The hybrid approach combining 2D and 3D correspondence estimation effectively addresses limitations of both methods, providing robust feature matching across challenging domain variations and large viewpoint changes.

Abstract: Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses. However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint. Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry. While powerful, we observe that these spatially coherent matches often concentrate on dominant planar regions, e.g., walls or ground surfaces, while being less sensitive to fine-grained geometric details, particularly under large viewpoint changes. To better understand these trade-offs, we first perform linear probe experiments to evaluate the performance of various vision foundation models for image matching. Building on these insights, we introduce SPIDER, a universal feature matching framework that integrates a shared feature extraction backbone with two specialized network heads for estimating both 2D-based and 3D-based correspondences from coarse to fine. Finally, we introduce an image-matching evaluation benchmark that focuses on unconstrained scenarios with large baselines. SPIDER significantly outperforms SoTA methods, demonstrating its strong ability as a universal image-matching method.

</details>


### [105] [CORA: Consistency-Guided Semi-Supervised Framework for Reasoning Segmentation](https://arxiv.org/abs/2511.17755)
*Prantik Howlader,Hoang Nguyen-Canh,Srijan Das,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: CORA is a semi-supervised reasoning segmentation framework that achieves robust pixel-accurate segmentation with minimal labeled data by leveraging unlabeled images and multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: Current reasoning segmentation methods struggle with generalization due to the high cost of curating diverse pixel annotations with rich linguistic supervision, leading to brittle performance under distribution shift.

Method: CORA introduces three components: 1) conditional visual instructions encoding spatial/contextual relationships, 2) noisy pseudo-label filtering based on multimodal LLM consistency across equivalent queries, and 3) token-level contrastive alignment between labeled and pseudo-labeled samples.

Result: CORA achieves state-of-the-art results with minimal supervision: +2.3% improvement with only 100 labeled images on Cityscapes, and +2.4% improvement with 180 labeled images on PanNuke.

Conclusion: CORA enables robust reasoning segmentation with minimal supervision through its semi-supervised framework, outperforming existing baselines in constrained annotation settings across different domains.

Abstract: Reasoning segmentation seeks pixel-accurate masks for targets referenced by complex, often implicit instructions, requiring context-dependent reasoning over the scene. Recent multimodal language models have advanced instruction following segmentation, yet generalization remains limited. The key bottleneck is the high cost of curating diverse, high-quality pixel annotations paired with rich linguistic supervision leading to brittle performance under distribution shift. Therefore, we present CORA, a semi-supervised reasoning segmentation framework that jointly learns from limited labeled data and a large corpus of unlabeled images. CORA introduces three main components: 1) conditional visual instructions that encode spatial and contextual relationships between objects; 2) a noisy pseudo-label filter based on the consistency of Multimodal LLM's outputs across semantically equivalent queries; and 3) a token-level contrastive alignment between labeled and pseudo-labeled samples to enhance feature consistency. These components enable CORA to perform robust reasoning segmentation with minimal supervision, outperforming existing baselines under constrained annotation settings. CORA achieves state-of-the-art results, requiring as few as 100 labeled images on Cityscapes, a benchmark dataset for urban scene understanding, surpassing the baseline by $+2.3\%$. Similarly, CORA improves performance by $+2.4\%$ with only 180 labeled images on PanNuke, a histopathology dataset.

</details>


### [106] [Latent Dirichlet Transformer VAE for Hyperspectral Unmixing with Bundled Endmembers](https://arxiv.org/abs/2511.17757)
*Giancarlo Giannetti,Faisal Z. Qureshi*

Main category: cs.CV

TL;DR: LDVAE-T is a transformer-based variational autoencoder with Dirichlet prior for hyperspectral unmixing that treats materials as bundled endmembers and outperforms state-of-the-art methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Spectral mixing in hyperspectral images obscures pure material signatures, making accurate material identification challenging. Existing methods often rely on fixed ground truth spectra, limiting their ability to capture intrinsic material variability.

Method: Combines transformer architecture for global context modeling with Dirichlet prior in latent space to enforce sum-to-one and non-negativity constraints. Uses bundled endmembers approach where decoder predicts mean spectrum and structured covariance for each endmember per patch. Mixes learned bundles with Dirichlet-distributed abundances from transformer encoder.

Result: LDVAE-T consistently outperforms state-of-the-art models on Samson, Jasper Ridge, and HYDICE Urban datasets in both abundance estimation (measured by RMSE) and endmember extraction (measured by spectral angle distance).

Conclusion: The proposed LDVAE-T model effectively addresses hyperspectral unmixing by combining transformer capabilities with physically meaningful constraints, representing material variability while maintaining interpretability and achieving superior performance over existing methods.

Abstract: Hyperspectral images capture rich spectral information that enables per-pixel material identification; however, spectral mixing often obscures pure material signatures. To address this challenge, we propose the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T) for hyperspectral unmixing. Our model combines the global context modeling capabilities of transformer architectures with physically meaningful constraints imposed by a Dirichlet prior in the latent space. This prior naturally enforces the sum-to-one and non-negativity conditions essential for abundance estimation, thereby improving the quality of predicted mixing ratios. A key contribution of LDVAE-T is its treatment of materials as bundled endmembers, rather than relying on fixed ground truth spectra. In the proposed method our decoder predicts, for each endmember and each patch, a mean spectrum together with a structured (segmentwise) covariance that captures correlated spectral variability. Reconstructions are formed by mixing these learned bundles with Dirichlet-distributed abundances garnered from a transformer encoder, allowing the model to represent intrinsic material variability while preserving physical interpretability. We evaluate our approach on three benchmark datasets, Samson, Jasper Ridge, and HYDICE Urban and show that LDVAE-T consistently outperforms state-of-the-art models in abundance estimation and endmember extraction, as measured by root mean squared error and spectral angle distance, respectively.

</details>


### [107] [Deepfake Geography: Detecting AI-Generated Satellite Images](https://arxiv.org/abs/2511.17766)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: Vision Transformers significantly outperform CNNs in detecting AI-generated satellite images, achieving 95.11% accuracy vs 87.02%, due to better modeling of long-range dependencies and global structures.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of generative models like StyleGAN2 and Stable Diffusion threatens satellite imagery authenticity, which is crucial for scientific and security applications. Current deepfake detection methods are mainly focused on facial contexts, leaving satellite imagery detection under-explored.

Method: Comprehensive comparison between CNNs and Vision Transformers using a curated dataset of 130,000+ labeled RGB images from DM-AER and FSI datasets. Used architecture-specific interpretability methods (Grad-CAM for CNNs, Chefer's attention attribution for ViTs) to enhance transparency.

Result: ViTs significantly outperformed CNNs with 95.11% accuracy vs 87.02%, demonstrating superior robustness in detecting structural inconsistencies and repetitive textural patterns in synthetic satellite imagery.

Conclusion: ViTs are more effective than CNNs for satellite imagery deepfake detection due to their ability to model global semantic structures. Future work will extend to multispectral/SAR modalities and frequency-domain analysis to further enhance detection capabilities.

Abstract: The rapid advancement of generative models such as StyleGAN2 and Stable Diffusion poses a growing threat to the authenticity of satellite imagery, which is increasingly vital for reliable analysis and decision-making across scientific and security domains. While deepfake detection has been extensively studied in facial contexts, satellite imagery presents distinct challenges, including terrain-level inconsistencies and structural artifacts. In this study, we conduct a comprehensive comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for detecting AI-generated satellite images. Using a curated dataset of over 130,000 labeled RGB images from the DM-AER and FSI datasets, we show that ViTs significantly outperform CNNs in both accuracy (95.11 percent vs. 87.02 percent) and overall robustness, owing to their ability to model long-range dependencies and global semantic structures. We further enhance model transparency using architecture-specific interpretability methods, including Grad-CAM for CNNs and Chefer's attention attribution for ViTs, revealing distinct detection behaviors and validating model trustworthiness. Our results highlight the ViT's superior performance in detecting structural inconsistencies and repetitive textural patterns characteristic of synthetic imagery. Future work will extend this research to multispectral and SAR modalities and integrate frequency-domain analysis to further strengthen detection capabilities and safeguard satellite imagery integrity in high-stakes applications.

</details>


### [108] [Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?](https://arxiv.org/abs/2511.17792)
*Dingrui Wang,Hongyuan Ye,Zhihao Liang,Zhexiao Sun,Zhaowei Lu,Yuchen Zhang,Yuyu Zhao,Yuan Gao,Marvin Seegert,Finn Schäfer,Haotong Qin,Wei Li,Luigi Palmieri,Felix Jahncke,Mattia Piccinini,Johannes Betz*

Main category: cs.CV

TL;DR: Target-Bench is the first benchmark for evaluating world models on mapless path planning toward semantic targets, revealing significant limitations in current models and showing that fine-tuning on a small dataset can dramatically improve performance.


<details>
  <summary>Details</summary>
Motivation: While world models can generate realistic videos, their capability for robot path planning remains unclear and unquantified. There is a need for specialized evaluation benchmarks for robotic planning tasks.

Method: Created Target-Bench with 450 robot-collected video sequences across 45 semantic categories with SLAM-based ground truth. Developed evaluation pipeline that recovers camera motion from generated videos and uses five complementary metrics to measure planning performance.

Result: Best off-the-shelf model (Wan2.2-Flash) achieved only 0.299 overall score. Fine-tuning a 5B-parameter model on 325 scenarios improved performance to 0.345 overall score - 400% improvement over base version and 15% higher than best off-the-shelf model.

Conclusion: Current world models have significant limitations for robotic planning tasks, but targeted fine-tuning on small datasets can substantially improve their path planning capabilities.

Abstract: While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.

</details>


### [109] [Attention Guided Alignment in Efficient Vision-Language Models](https://arxiv.org/abs/2511.17793)
*Shweta Mahajan,Hoang Le,Hyojin Park,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: AGE-VLM framework reduces object hallucination in efficient VLMs by using interleaved cross-attention layers and spatial knowledge from SAM to improve visual grounding.


<details>
  <summary>Details</summary>
Motivation: Current concatenation-based VLMs fail to distinguish between matching and non-matching image-text pairs, leading to object hallucination problems.

Method: Introduces interleaved cross-attention layers to instill vision capabilities in pretrained small language models, leveraging spatial knowledge from Segment Anything Model (SAM) to guide attention to correct image regions.

Result: Significantly reduces hallucination and achieves comparable or better performance than prior work on efficient VLMs across vision-centric benchmarks.

Conclusion: The approach provides valuable insights for enhancing visual and linguistic understanding in VLMs through improved attention mechanisms and visual grounding.

Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability "look" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.

</details>


### [110] [Pillar-0: A New Frontier for Radiology Foundation Models](https://arxiv.org/abs/2511.17803)
*Kumar Krishna Agrawal,Longchao Liu,Long Lian,Michael Nercessian,Natalia Harguindeguy,Yufu Wu,Peter Mikhael,Gigin Lin,Lecia V. Sequist,Florian Fintelmann,Trevor Darrell,Yutong Bai,Maggie Chung,Adam Yala*

Main category: cs.CV

TL;DR: Pillar-0 is a radiology foundation model that processes full 3D CT/MRI scans with grayscale contrast, trained on 155,292 medical images, and outperforms existing models across multiple clinical tasks with significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing medical AI models that process volumetric scans as 2D slices, discard grayscale information, and lack clinically relevant evaluation frameworks, while tackling the challenge of rising imaging volumes outpacing workforce growth.

Method: Pretrained on 155,292 medical images (CTs and MRIs) from a large academic center, using RATE framework for automated labeling of 366 radiologic findings with LLMs, processing full 3D volumes with grayscale contrast.

Result: Achieved mean AUROCs of 86.4-90.1 across different body regions, outperforming all baselines by 7.8-15.8 AUROC points, ranking best in 87.2% of tasks, and showing superior performance in external validation and specialized tasks like lung cancer risk prediction.

Conclusion: Pillar-0 and RATE provide an open, clinically rigorous foundation for high-performance radiology systems, enabling previously infeasible applications and establishing a new performance frontier in medical imaging AI.

Abstract: Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.

</details>


### [111] [A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking](https://arxiv.org/abs/2511.17805)
*Chengan Che,Chao Wang,Xinyue Chen,Sophia Tsoka,Luis C. Garcia-Peraza-Herrera*

Main category: cs.CV

TL;DR: PL-Stitch is a self-supervised learning framework that addresses the lack of procedural awareness in current methods by using temporal order as supervisory signal through Plackett-Luce model objectives.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised learning methods fail to capture the procedural nature of activities like cooking and surgery, as shown by models producing similar features for forward and time-reversed sequences.

Method: Integrates two probabilistic objectives: primary PL objective for chronological frame sorting to learn global workflow progression, and secondary spatio-temporal jigsaw loss for fine-grained cross-frame object correlations.

Result: Achieves superior performance across five surgical and cooking benchmarks, with significant gains in surgical phase recognition (+11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (+5.7 pp linear probing accuracy on Breakfast).

Conclusion: PL-Stitch effectively learns procedural video representations by leveraging temporal order as supervisory signal, demonstrating strong performance on procedural activity understanding tasks.

Abstract: Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.

</details>


### [112] [REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion](https://arxiv.org/abs/2511.17806)
*Ryoma Yataka,Pu Perry Wang,Petros Boufounos,Ryuhei Takahashi*

Main category: cs.CV

TL;DR: REXO is a multi-view radar object detection method that uses 3D bounding box diffusion with explicit cross-view feature association, outperforming state-of-the-art methods on indoor radar datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on implicit cross-view radar feature association, leading to ambiguous feature matches and degraded detection in complex indoor scenes.

Method: Lifts 2D bounding box diffusion into 3D radar space, uses noisy 3D boxes to guide explicit cross-view feature association, and incorporates prior knowledge that people are in contact with the ground to reduce diffusion parameters.

Result: Surpasses state-of-the-art methods by +4.22 AP on HIBER dataset and +11.02 AP on MMVR dataset.

Conclusion: REXO's explicit cross-view feature association and 3D bounding box diffusion approach effectively addresses limitations of existing methods in multi-view indoor radar perception.

Abstract: Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.

</details>


### [113] [Importance-Weighted Non-IID Sampling for Flow Matching Models](https://arxiv.org/abs/2511.17812)
*Xinshuang Liu,Runfa Blark Li,Shaoxiu Wei,Truong Nguyen*

Main category: cs.CV

TL;DR: A novel importance-weighted non-IID sampling framework for flow-matching models that jointly draws multiple samples to cover diverse regions while maintaining unbiased estimation through learned importance weights.


<details>
  <summary>Details</summary>
Motivation: Flow-matching models can represent complex distributions, but estimating expectations of functions under limited sampling budgets is challenging due to high variance from independent sampling, especially when rare high-impact outcomes dominate expectations.

Method: Proposed importance-weighted non-IID sampling with score-based regularization for diversity, using gradient of log probability to push samples apart within high-density regions. Developed approach for importance weighting by learning residual velocity field that reproduces marginal distribution of non-IID samples.

Result: Empirically produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing reliable characterization of flow-matching model outputs.

Conclusion: The method enables more reliable estimation of expectations from flow-matching models under limited sampling budgets by combining non-IID sampling with proper importance weighting.

Abstract: Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.

</details>


### [114] [QAL: A Loss for Recall Precision Balance in 3D Reconstruction](https://arxiv.org/abs/2511.17824)
*Pranay Meshram,Yash Turkar,Kartikeya Singh,Praveen Raj Masilamani,Charuvahan Adhivarahan,Karthik Dantu*

Main category: cs.CV

TL;DR: QAL is a new loss function that replaces Chamfer Distance and Earth Mover's Distance in 3D vision tasks, explicitly balancing recall and precision through tunable components for better coverage of thin structures and underrepresented regions.


<details>
  <summary>Details</summary>
Motivation: Current volumetric learning objectives (CD/EMD) fail to balance recall and precision, leading to poor coverage of thin structures and underrepresented regions in 3D vision tasks like completion and reconstruction.

Method: Proposes Quality-Aware Loss (QAL) with two components: coverage-weighted nearest-neighbor term and uncovered-ground-truth attraction term, explicitly decoupling recall and precision as tunable parameters.

Result: QAL achieves consistent coverage gains (+4.3 pts over CD, +2.8 pts over alternatives), recovers thin structures, shows stable performance across hyperparameters/resolutions, and improves robotic grasp scores by 1.5-2.0 pts.

Conclusion: QAL provides a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines, with improved coverage translating directly to more reliable robotic manipulation.

Abstract: Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.
  Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.
  QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines

</details>


### [115] [Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations](https://arxiv.org/abs/2511.17828)
*Guilherme J. Cavalcante,José Gabriel A. Moreira,Gabriel A. B. do Nascimento,Vincent Dong,Alex Nguyen,Thaís G. do Rêgo,Yuri Malheiros,Telmo M. Silva Filho,Carla R. Zeballos Torrez,James C. Gee,Anne Marie McCarthy,Andrew D. A. Maidment,Bruno Barufaldi*

Main category: cs.CV

TL;DR: BiomedCLIP foundation model adapted for BI-RADS breast density classification using multi-modality mammographic data, achieving strong performance and generalization across different imaging modalities.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of foundation models for specialized medical imaging tasks, particularly in breast imaging where model generalization remains challenging.

Method: Adapted BiomedCLIP for automated BI-RADS breast density classification using 96,995 multi-modality mammographic images (synthesized 2D, digital mammography, digital breast tomosynthesis), employing weighted contrastive learning to address class imbalance.

Result: Both single-modality (s2D only) and multi-modality approaches achieved similar accuracy (0.73-0.74), with multi-modality model showing broader applicability and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on RSNA and EMBED datasets showed strong generalization (AUC: 0.80-0.93).

Conclusion: Foundation models like BiomedCLIP show significant potential for breast imaging applications, demonstrating robust performance, interpretability, and strong generalization capabilities across different imaging modalities.

Abstract: Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.

</details>


### [116] [Show Me: Unifying Instructional Image and Video Generation with Diffusion Models](https://arxiv.org/abs/2511.17839)
*Yujiang Pu,Zhanbo Huang,Vishnu Boddeti,Yu Kong*

Main category: cs.CV

TL;DR: ShowMe is a unified framework that combines image manipulation and video prediction tasks using video diffusion models, achieving better performance than specialized models in both domains.


<details>
  <summary>Details</summary>
Motivation: Prior works treat text-guided image manipulation and video prediction as separate tasks, leading to limitations: image manipulation ignores temporal dynamics, while video prediction overlooks intended outcomes.

Method: Proposes ShowMe framework that selectively activates spatial and temporal components of video diffusion models, with structure and motion consistency rewards for improved fidelity and coherence.

Result: Outperforms expert models in both instructional image and video generation across diverse benchmarks, demonstrating dual benefits from unification.

Conclusion: Video diffusion models serve as effective unified action-object state transformers, with spatial knowledge from video pretraining enhancing image edits and instruction-guided manipulation improving video prediction.

Abstract: Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.

</details>


### [117] [JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception](https://arxiv.org/abs/2511.17843)
*Chenyi Wang,Zhaowei Li,Ming F. Li,Wujie Wen*

Main category: cs.CV

TL;DR: JigsawComm is a communication-efficient multi-agent cooperative perception framework that maximizes perception accuracy under limited bandwidth by transmitting semantically essential and non-redundant features.


<details>
  <summary>Details</summary>
Motivation: Multi-agent cooperative perception faces severe bandwidth constraints, and existing approaches don't adequately address semantic relevance or cross-agent redundancy in sensory data transmission.

Method: Uses a regularized encoder to extract sparse semantic features, a Feature Utility Estimator to predict feature contributions, and exchanges meta utility maps to compute optimal transmission policies that select highest-utility features.

Result: Reduces total data volume by >500× while achieving matching or superior accuracy on OPV2V and DAIR-V2X benchmarks, with scalable O(1) communication cost as agents increase.

Conclusion: JigsawComm provides a practical solution for bandwidth-constrained cooperative perception by maximizing the contribution of every transmitted bit through semantic-aware feature selection.

Abstract: Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.

</details>


### [118] [Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation](https://arxiv.org/abs/2511.17844)
*Shihan Cheng,Nilesh Kulkarni,David Hyde,Dmitriy Smirnov*

Main category: cs.CV

TL;DR: Fine-tuning text-to-video diffusion models with sparse, low-quality synthetic data enables better camera parameter controls than using photorealistic real data.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large-scale text-to-video models for camera parameter controls typically requires vast, high-fidelity datasets that are difficult to acquire.

Method: Propose a data-efficient fine-tuning strategy that learns camera controls from sparse, low-quality synthetic data rather than photorealistic real data.

Result: Fine-tuning on simple synthetic data not only enables desired camera controls but yields superior results compared to models fine-tuned on photorealistic real data.

Conclusion: Provides a framework justifying why sparse, low-quality synthetic data outperforms photorealistic data for learning camera parameter controls in diffusion models.

Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.

</details>


### [119] [MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use](https://arxiv.org/abs/2511.17881)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: MGA-VQA is a multi-modal framework for Document Visual Question Answering that addresses limitations in spatial relationship modeling, efficiency with high-resolution documents, multi-hop reasoning, and interpretability through token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression.


<details>
  <summary>Details</summary>
Motivation: Current DocVQA methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability, creating a need for more transparent and effective approaches.

Method: Proposes MGA-VQA framework integrating token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression with interpretable graph-based decision pathways and structured memory access.

Result: Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, RICO) demonstrates superior accuracy and efficiency with consistent improvements in both answer prediction and spatial localization.

Conclusion: MGA-VQA provides an effective solution for Document Visual Question Answering with enhanced reasoning transparency and performance across multiple document understanding tasks.

Abstract: Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.

</details>


### [120] [ArticFlow: Generative Simulation of Articulated Mechanisms](https://arxiv.org/abs/2511.17883)
*Jiong Lin,Jinchen Ruan,Hod Lipson*

Main category: cs.CV

TL;DR: ArticFlow is a two-stage flow matching framework for generating articulated 3D shapes under explicit action control, achieving high kinematic accuracy and shape quality while generalizing across diverse categories.


<details>
  <summary>Details</summary>
Motivation: Articulated 3D generation remains challenging due to action-dependent deformations and limited datasets, while recent generative models have focused mainly on static 3D shapes.

Method: Two-stage flow matching framework with (i) latent flow transporting noise to shape-prior code and (ii) point flow transporting points conditioned on action and shape prior, enabling single model to represent diverse articulated categories.

Result: Achieves higher kinematic accuracy and better shape quality compared to object-specific simulators and action-conditioned static point-cloud generators; functions as both generative model and neural simulator.

Conclusion: Action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.

Abstract: Recent advances in generative models have produced strong results for static 3D shapes, whereas articulated 3D generation remains challenging due to action-dependent deformations and limited datasets. We introduce ArticFlow, a two-stage flow matching framework that learns a controllable velocity field from noise to target point sets under explicit action control. ArticFlow couples (i) a latent flow that transports noise to a shape-prior code and (ii) a point flow that transports points conditioned on the action and the shape prior, enabling a single model to represent diverse articulated categories and generalize across actions. On MuJoCo Menagerie, ArticFlow functions both as a generative model and as a neural simulator: it predicts action-conditioned kinematics from a compact prior and synthesizes novel morphologies via latent interpolation. Compared with object-specific simulators and an action-conditioned variant of static point-cloud generators, ArticFlow achieves higher kinematic accuracy and better shape quality. Results show that action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.

</details>


### [121] [FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning](https://arxiv.org/abs/2511.17885)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Wei Chen,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CV

TL;DR: FastMMoE is a training-free acceleration framework for MoE-based MLLMs that reduces redundant visual tokens through expert activation reduction and routing-aware token pruning, achieving up to 55% FLOPs reduction while maintaining ~95.5% performance.


<details>
  <summary>Details</summary>
Motivation: High-resolution visual inputs in MLLMs create long sequences of visual tokens, causing substantial inference latency and computational/memory burdens, especially problematic for resource-constrained or latency-sensitive deployment scenarios.

Method: Two complementary strategies: (1) expert activation reduction for visual tokens to minimize unnecessary expert computation, and (2) routing-aware token pruning that uses similarity in routing probability distributions to identify and remove highly redundant visual tokens.

Result: Experiments on DeepSeek-VL2 and InternVL3.5 show FastMMoE reduces FLOPs by up to 55.0% while retaining approximately 95.5% of original performance, consistently outperforming dense-model pruning baselines like FastV and SparseVLM.

Conclusion: FastMMoE effectively accelerates MoE-based MLLMs through training-free token reduction strategies, enabling efficient deployment in resource-constrained environments while preserving model performance.

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.

</details>


### [122] [When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA](https://arxiv.org/abs/2511.17886)
*Pume Tuchinda,Parinthapat Pengpun,Romrawin Chumpu,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CV

TL;DR: Systematic study of knowledge distillation for CLIP-style vision-language models reveals that stronger teachers don't always yield better students, challenging conventional KD assumptions in multimodal settings.


<details>
  <summary>Details</summary>
Motivation: Vision-language models have high computational demands, and while knowledge distillation works well in language and vision domains, its application to CLIP-style VLMs remains limited with constraints to small-scale teachers and narrow evaluation tasks.

Method: Conducted systematic study of distillation across various CLIP-style teacher models, from standard baselines to large-scale state-of-the-art models, evaluating performance on downstream multimodal tasks.

Result: Contrary to NLP and vision trends, stronger teachers do not consistently yield better students; existing distillation frameworks often fail to scale and lead to degraded performance in tasks like visual question answering.

Conclusion: Findings challenge prevailing KD assumptions and point toward new directions for designing parameter-efficient multimodal models.

Abstract: Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.

</details>


### [123] [MINDiff: Mask-Integrated Negative Attention for Controlling Overfitting in Text-to-Image Personalization](https://arxiv.org/abs/2511.17888)
*Seulgi Jeong,Jaeil Kim*

Main category: cs.CV

TL;DR: MINDiff introduces negative attention during inference to suppress subject influence in irrelevant regions, enabling better semantic control and text alignment without retraining DreamBooth models.


<details>
  <summary>Details</summary>
Motivation: To address overfitting in personalized text-to-image models like DreamBooth, which use class-specific prior-preservation loss that increases computational cost and limits user control during inference.

Method: Modifies cross-attention mechanism during inference by introducing negative attention to suppress subject's influence in masked irrelevant regions, with adjustable scale parameter lambda for balance.

Result: Qualitative and quantitative experiments show MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss while maintaining subject fidelity.

Conclusion: MINDiff provides an inference-time solution that improves semantic control and text alignment without model architecture changes, directly applicable to existing DreamBooth models.

Abstract: In the personalization process of large-scale text-to-image models, overfitting often occurs when learning specific subject from a limited number of images. Existing methods, such as DreamBooth, mitigate this issue through a class-specific prior-preservation loss, which requires increased computational cost during training and limits user control during inference time. To address these limitations, we propose Mask-Integrated Negative Attention Diffusion (MINDiff). MINDiff introduces a novel concept, negative attention, which suppresses the subject's influence in masked irrelevant regions. We achieve this by modifying the cross-attention mechanism during inference. This enables semantic control and improves text alignment by reducing subject dominance in irrelevant regions. Additionally, during the inference time, users can adjust a scale parameter lambda to balance subject fidelity and text alignment. Our qualitative and quantitative experiments on DreamBooth models demonstrate that MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss. As our method operates entirely at inference time and does not alter the model architecture, it can be directly applied to existing DreamBooth models without re-training. Our code is available at https://github.com/seuleepy/MINDiff.

</details>


### [124] [Decoupled Audio-Visual Dataset Distillation](https://arxiv.org/abs/2511.17890)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: DAVDD is a pretraining-based decoupled audio-visual distillation framework that addresses cross-modal alignment challenges in dataset distillation by disentangling common and private representations.


<details>
  <summary>Details</summary>
Motivation: Conventional Distribution Matching methods struggle with cross-modal alignment, and existing approaches face issues with inconsistent modality mapping spaces and damage to modality-specific information during direct cross-modal interactions.

Method: Uses a diverse pretrained bank for stable modality features, lightweight decoupler bank to disentangle common/private representations, Common Intermodal Matching, and Sample-Distribution Joint Alignment strategy.

Result: Achieves state-of-the-art results across multiple benchmarks under all IPC settings, demonstrating effectiveness of decoupled representation learning.

Conclusion: DAVDD effectively preserves cross-modal structure while safeguarding modality-specific information, enabling high-quality audio-visual dataset distillation.

Abstract: Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.

</details>


### [125] [CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation](https://arxiv.org/abs/2511.17904)
*Yuhang Ming,Chenxin Fang,Xingyuan Yu,Fan Zhang,Weichen Dai,Wanzeng Kong,Guofeng Zhang*

Main category: cs.CV

TL;DR: CUS-GS bridges the gap between semantics-oriented and structure-oriented 3D scene representations by connecting multimodal semantic features with structured 3D geometry using a compact unified Gaussian Splatting framework.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing approaches where semantics-oriented methods lack explicit 3D geometry modeling and structure-oriented methods provide limited semantic abstraction, creating a unified representation that combines both aspects.

Method: Uses a voxelized anchor structure as spatial scaffold, extracts multimodal semantic features from foundation models (CLIP, DINOv2, SEEM), employs multimodal latent feature allocation to unify appearance/geometry/semantics, and implements feature-aware significance evaluation for dynamic anchor management.

Result: Achieves competitive performance with state-of-the-art methods using only 6M parameters - an order of magnitude smaller than the closest competitor at 35M, demonstrating excellent performance-efficiency trade-off.

Conclusion: CUS-GS successfully bridges the gap between semantic and structural 3D scene representations, providing a compact and efficient unified framework that maintains semantic integrity while significantly reducing model size.

Abstract: Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.

</details>


### [126] [Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation](https://arxiv.org/abs/2511.17914)
*Chenyang Jiang,Hang Zhao,Xinyu Zhang,Zhengcen Li,Qiben Shan,Shaocong Wu,Jingyong Su*

Main category: cs.CV

TL;DR: ADSA addresses soft-label bias in long-tailed dataset distillation through adaptive soft-label alignment, improving tail-class accuracy by up to 11.8% on ImageNet-1k-LT.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods focus on balanced datasets and struggle with real-world long-tailed distributions, leading to performance degradation in tail classes.

Method: Proposes ADSA (Adaptive Soft-label Alignment) module that identifies and calibrates two sources of soft-label bias from distillation models and distilled images through systematic perturbation of data imbalance levels.

Result: On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. The method works across various distillation techniques and limited label budgets.

Conclusion: ADSA provides a robust and generalizable solution for long-tailed dataset distillation by addressing soft-label bias, with consistent performance improvements across different settings.

Abstract: Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.

</details>


### [127] [Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization](https://arxiv.org/abs/2511.17918)
*Youngsik Yun,Dongjun Gu,Youngjung Uh*

Main category: cs.CV

TL;DR: FASR improves 3D Gaussian Splatting's generalization to novel viewpoints in few-shot scenarios by adaptively regularizing based on local image frequencies, preventing overfitting while preserving high-frequency details.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting lacks generalization across novel viewpoints in few-shot scenarios due to overfitting to sparse observations. The paper frames novel view synthesis as a generalization problem to unseen viewpoints.

Method: Proposes Frequency-Adaptive Sharpness Regularization (FASR) that adapts regularization strength based on local image frequencies, preventing excessive smoothing while reducing sharpness of the loss landscape.

Result: Consistently improves various baselines across datasets, preventing floater artifacts in novel viewpoints and reconstructing fine details that standard SAM tends to oversmooth.

Conclusion: FASR effectively addresses the generalization problem in 3DGS by frequency-adaptive regularization, achieving better novel view synthesis performance in few-shot scenarios.

Abstract: Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.

</details>


### [128] [PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning](https://arxiv.org/abs/2511.17927)
*Yingjie Ma,Xun Lin,Yong Xu,Weicheng Xie,Zitong Yu*

Main category: cs.CV

TL;DR: PA-FAS enhances multimodal face anti-spoofing by constructing extended reasoning sequences and using answer-shuffling to improve reasoning paths and prevent shortcut learning, achieving better accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Current RL approaches for multimodal FAS face limitations in reasoning paths and suffer from shortcut learning where models bypass intended multimodal reasoning by mapping images directly to answers.

Method: Proposes PA-FAS with two key components: (1) constructing high-quality extended reasoning sequences from limited annotations to enrich reasoning paths, and (2) answer-shuffling mechanism during supervised fine-tuning to force comprehensive multimodal analysis and prevent shortcut learning.

Result: PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization while better unifying multimodal fusion, generalization, and interpretability for trustworthy FAS.

Conclusion: The proposed PA-FAS framework effectively addresses limitations in multimodal FAS by enhancing reasoning paths and preventing shortcut learning, leading to improved performance across multiple dimensions including accuracy, generalization, and interpretability.

Abstract: Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.

</details>


### [129] [MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection](https://arxiv.org/abs/2511.17929)
*Hui Lu,Yi Yu,Shijian Lu,Deepu Rajan,Boon Poh Ng,Alex C. Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: MambaTAD is a new state-space model for Temporal Action Detection that introduces diagonal-masked bidirectional state-space modeling and global feature fusion to address challenges in detecting long-span actions with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional TAD methods struggle with long-span action detection due to lack of global awareness and inefficient detection heads. State-space models face decay of temporal context and self-element conflict during global visual context modeling.

Method: Proposes MambaTAD with two key designs: 1) Diagonal-Masked Bidirectional State-Space (DMBSS) module for global feature fusion, 2) Global feature fusion head with multi-granularity features. Uses end-to-end one-stage approach with state-space temporal adapter (SSTA) for linear complexity.

Result: Extensive experiments show MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.

Conclusion: MambaTAD effectively addresses challenges in temporal action detection, particularly for long-span actions, through its novel state-space architecture and global feature modeling capabilities.

Abstract: Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.

</details>


### [130] [UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection](https://arxiv.org/abs/2511.17930)
*Yuan Qu,Zhipeng Zhang,Chaojun Xu,Qiao Wan,Mengying Xie,Yuzeng Chen,Zhenqi Liu,Yanfei Zhong*

Main category: cs.CV

TL;DR: UniRSCD is a unified change detection framework that handles multiple tasks (BCD, SCD, BDA) using a state space model backbone with frequency change prompts, eliminating the need for specialized decoders.


<details>
  <summary>Details</summary>
Motivation: Existing change detection methods require expert-designed specialized decoders for different tasks, limiting universality and introducing uncertainty in model selection for abrupt change scenarios.

Method: Uses state space model backbone with frequency change prompt generator as unified encoder, integrating high/low-frequency information. Unified decoder establishes shared representation space through hierarchical feature interaction and task-adaptive output mapping.

Result: Achieves leading performance on five datasets including LEVIR-CD (binary change), SECOND (semantic change), and xBD (building damage assessment).

Conclusion: The proposed unified framework successfully adapts to multiple change detection tasks with different output granularities, demonstrating superior performance and eliminating the need for task-specific decoder designs.

Abstract: In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.

</details>


### [131] [Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion](https://arxiv.org/abs/2511.17932)
*Yan Xu,Yixing Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: Zero-shot sparse-input novel view synthesis using video diffusion models to hallucinate plausible in-between views, combined with 3D Gaussian Splatting for scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of synthesizing novel views from extremely sparse inputs by treating it as test-time natural video completion rather than just filling spatial gaps.

Method: Uses pretrained video diffusion models to generate pseudo views at novel camera poses with uncertainty-aware mechanism, then employs 3D Gaussian Splatting for scene reconstruction with iterative feedback between 2D view synthesis and 3D geometry.

Result: Produces coherent, high-fidelity renderings from sparse inputs without scene-specific training, significantly outperforming strong 3D-GS baselines on multiple datasets under extreme sparsity.

Conclusion: The framework successfully combines generation-guided view synthesis with 3D reconstruction to achieve robust novel view synthesis from very limited input views.

Abstract: Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \emph{completing a natural video} unfolding through space.
  We recast the task as \emph{test-time natural video completion}, using powerful priors from \emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.
  The result is coherent, high-fidelity renderings from sparse inputs \emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.

</details>


### [132] [V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction](https://arxiv.org/abs/2511.17941)
*Xiangyan Kong,Xuecheng Wu,Xiongwei Zhao,Xiaodong Li,Yunyun Shi,Gang Wang,Dingkang Yang,Yang Liu,Hong Chen,Yulong Gao*

Main category: cs.CV

TL;DR: V2X-RECT is a trajectory prediction framework for dense traffic that improves data association consistency, reduces redundant interactions, and reuses historical information for more efficient and accurate V2X prediction.


<details>
  <summary>Details</summary>
Motivation: Address challenges in dense traffic scenarios where frequent identity switching hinders cross-view association, multi-source information creates redundant interactions, and traditional vehicle-centric encoding leads to repetitive historical trajectory feature encoding, degrading real-time performance.

Method: Multi-source identity matching and correction module for stable target association; traffic signal-guided interaction module to filter key vehicles and capture signal change impacts; local spatiotemporal coordinate encoding for reusable historical features and parallel decoding.

Result: Significant improvements over SOTA methods on V2X-Seq and V2X-Traj datasets, with enhanced robustness and inference efficiency across diverse traffic densities.

Conclusion: V2X-RECT effectively addresses key challenges in dense V2X prediction through improved association consistency, reduced redundancy, and efficient feature reuse, achieving superior performance and efficiency.

Abstract: V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.

</details>


### [133] [SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System](https://arxiv.org/abs/2511.17943)
*Zhiyu Xu,Weilong Yan,Yufei Shi,Xin Meng,Tao He,Huiping Zhuang,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: SciEducator is a self-evolving multi-agent system for scientific video understanding and education that outperforms leading MLLMs and video agents on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models struggle with scientific video understanding due to the need for professional knowledge integration and step-wise reasoning in this domain.

Method: Proposes SciEducator, an iterative self-evolving multi-agent system based on the Deming Cycle (Plan-Do-Study-Act) that generates multimodal educational content including text, visuals, audio, and interactive references.

Result: Outperforms leading closed-source MLLMs (Gemini, GPT-4o) and state-of-the-art video agents on SciVBench, a new benchmark of 500 expert-verified science QA pairs across 5 categories.

Conclusion: Establishes a new paradigm for scientific video comprehension and education through self-evolving reasoning mechanisms and multimodal content generation.

Abstract: Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.

</details>


### [134] [Test-Time Temporal Sampling for Efficient MLLM Video Understanding](https://arxiv.org/abs/2511.17945)
*Kaibin Wang,Mingbao Lin*

Main category: cs.CV

TL;DR: T3S is a training-free inference wrapper that enables MLLMs to process long videos efficiently by generating multiple short subsequences, packing them in one forward pass, and aggregating predictions, reducing computational cost while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for processing long videos with MLLMs face computational challenges due to quadratic self-attention scaling, and existing solutions compromise accuracy, require additional training, or reduce inference speed.

Method: T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions.

Result: T3S improves accuracy by up to 3.1% and reduces first token delay by 2.04× on long video understanding benchmarks, with minimal integration effort and no model modifications.

Conclusion: T3S turns video redundancy into a computational advantage, offering a scalable, plug-and-play solution for long-video understanding that requires no training or model changes.

Abstract: Processing long videos with multimodal large language models (MLLMs) poses a significant computational challenge, as the model's self-attention mechanism scales quadratically with the number of video tokens, resulting in high computational demand and slow inference speed. Current solutions, such as rule-based sub-sampling, learned frame selector, or memory-based summarization, often introduce their own trade-offs: they compromise accuracy, necessitate additional training, or decrease inference speed. In this paper, we propose Test-Time Temporal Sampling (T3S), a training-free, plug-and-play inference wrapper that enables MLLMs to process long videos both efficiently and effectively. T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions. This multi-subsequence formulation broadens visual coverage while reducing the computational cost of self-attention from $O(L^2)$ to $O(\sum_{i=1}^m α_i^2L^2)$, where $\sum_{i=1}^m α_i^2 < 1$. Extensive experiments on long video understanding benchmarks demonstrate that T3S improves accuracy by up to 3.1% and reduces first token delay by $2.04\times$, all with minimal integration effort. Our approach operates entirely at inference time, requires no model modifications or fine-tuning, and is compatible with a wide range of pretrained MLLMs. T3S turns video redundancy into a computational advantage, offering a scalable solution for long-video understanding. The code is available at https://github.com/kaibinwang3/T3S.

</details>


### [135] [Multi-speaker Attention Alignment for Multimodal Social Interaction](https://arxiv.org/abs/2511.17952)
*Liangyang Ouyang,Yifei Huang,Mingfang Zhang,Caixin Kang,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: The paper proposes a multimodal multi-speaker attention alignment method that improves MLLMs' ability to understand social interactions in videos by aligning visual and textual tokens with speakers.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs show inconsistent performance on social tasks because visual and textual tokens lack speaker-consistent alignment in multi-speaker scenes, with weaker cross-modal attention than in object-centric images.

Method: A multimodal multi-speaker attention alignment method with dynamic cross-modal head selection to identify grounding heads, and adaptive social-aware attention bias computed from existing attention patterns and speaker locations, injected into attention mechanism without trainable parameters.

Result: The method integrated into three MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, InternVL3) achieves state-of-the-art results on three benchmarks (TVQA+, MMSI, OnlineMMSI) across four social tasks, with attention visualizations confirming improved focus on speaker-relevant regions.

Conclusion: The proposed approach successfully enables more robust multi-party social reasoning in MLLMs by reinforcing alignment between speakers' visual representations and their utterances.

Abstract: Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.

</details>


### [136] [HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation](https://arxiv.org/abs/2511.17958)
*Yulong Shi,Jiapeng Li,Lin Qi*

Main category: cs.CV

TL;DR: HEAL is a novel SFUDA framework that addresses domain shift without accessing source data or target labels, using hierarchical denoising, edge-guided selection, size-aware fusion, and learning-free characteristics.


<details>
  <summary>Details</summary>
Motivation: Growing demands for clinical data privacy and storage constraints require adapting models to unseen target domains without accessing source data or target labels.

Method: HEAL integrates hierarchical denoising, edge-guided selection, size-aware fusion, and learning-free characteristics to handle domain shift in SFUDA settings.

Result: Large-scale cross-modality experiments show HEAL outperforms existing SFUDA approaches, achieving state-of-the-art performance.

Conclusion: HEAL provides an effective solution for SFUDA challenges, with publicly available source code for further research and application.

Abstract: Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.

</details>


### [137] [VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment](https://arxiv.org/abs/2511.17962)
*Ziheng Jia,Linhan Cao,Jinliang Han,Zicheng Zhang,Jiaying Qian,Jiarui Wang,Zijian Chen,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: Proposed VITAL-Series LMMs for visual quality assessment using vision-encoder-centered generative pre-training with multi-task training and efficient model extension.


<details>
  <summary>Details</summary>
Motivation: Existing VQualA LMMs focus on single tasks and use full-parameter fine-tuning, leading to overfitting and limited generalization across modalities and tasks.

Method: Vision-encoder-centered generative pre-training pipeline with machine-executed annotation-scrutiny paradigm (4.5M VL pairs), multi-task training for scoring precision and quality interpretation across image/video modalities, and efficient model zoo extension with minimal data warm-up.

Result: Created largest VQualA training dataset, model zoo exhibits strong zero-shot performance, and paired decoders achieve comparable performance to fully trained models using less than 1/1000 of pre-training data.

Conclusion: The work lays a cornerstone for advancing toward the foundation LMM for visual quality assessment, achieving versatility, powerfulness, and transferability.

Abstract: Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.
  However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.

</details>


### [138] [X-ReID: Multi-granularity Information Interaction for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.17964)
*Chenyang Yu,Xuehu Liu,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: X-ReID is a novel cross-modality framework for Video-based Visible-Infrared Person Re-Identification that addresses modality gaps and leverages spatiotemporal information through Cross-modality Prototype Collaboration and Multi-granularity Information Interaction.


<details>
  <summary>Details</summary>
Motivation: Large-scale vision-language models like CLIP show promise for retrieval tasks but their potential for VVI-ReID remains unexplored. The main challenges are narrowing the modality gap between visible and infrared domains and effectively utilizing spatiotemporal information in video sequences.

Method: Proposes Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, and Multi-granularity Information Interaction (MII) that incorporates short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment.

Result: Extensive experiments on two large-scale VVI-ReID benchmarks (HITSZ-VCM and BUPTCampus) demonstrate superiority over state-of-the-art methods, achieving robust sequence-level representations.

Conclusion: The proposed X-ReID framework effectively addresses modality gaps and leverages spatiotemporal information in VVI-ReID, showing superior performance on benchmark datasets compared to existing methods.

Abstract: Large-scale vision-language models (e.g., CLIP) have recently achieved remarkable performance in retrieval tasks, yet their potential for Video-based Visible-Infrared Person Re-Identification (VVI-ReID) remains largely unexplored. The primary challenges are narrowing the modality gap and leveraging spatiotemporal information in video sequences. To address the above issues, in this paper, we propose a novel cross-modality feature learning framework named X-ReID for VVI-ReID. Specifically, we first propose a Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, guiding the network to reduce the modality discrepancy. Then, a Multi-granularity Information Interaction (MII) is designed, incorporating short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment to enhance temporal modeling and further reduce modality gaps. Finally, by integrating multi-granularity information, a robust sequence-level representation is achieved. Extensive experiments on two large-scale VVI-ReID benchmarks (i.e., HITSZ-VCM and BUPTCampus) demonstrate the superiority of our method over state-of-the-art methods. The source code is released at https://github.com/AsuradaYuci/X-ReID.

</details>


### [139] [Signal: Selective Interaction and Global-local Alignment for Multi-Modal Object Re-Identification](https://arxiv.org/abs/2511.17965)
*Yangyang Liu,Yuhao Wang,Pingping Zhang*

Main category: cs.CV

TL;DR: Proposes Signal framework with Selective Interaction Module and Global-Local Alignment for multi-modal object ReID to address background interference and multi-modal consistency issues.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal ReID methods focus on feature fusion but neglect background interference and suffer from multi-modal consistency alignment problems.

Method: Uses Selective Interaction Module to select important patch tokens, Global Alignment Module to align multi-modal features via 3D polyhedra volume minimization, and Local Alignment Module for shift-aware local feature alignment.

Result: Extensive experiments on RGBNT201, RGBNT100, and MSVR310 benchmarks validate the method's effectiveness.

Conclusion: The proposed Signal framework extracts more discriminative features for multi-modal object ReID by addressing background interference and improving multi-modal consistency alignment.

Abstract: Multi-modal object Re-IDentification (ReID) is devoted to retrieving specific objects through the exploitation of complementary multi-modal image information. Existing methods mainly concentrate on the fusion of multi-modal features, yet neglecting the background interference. Besides, current multi-modal fusion methods often focus on aligning modality pairs but suffer from multi-modal consistency alignment. To address these issues, we propose a novel selective interaction and global-local alignment framework called Signal for multi-modal object ReID. Specifically, we first propose a Selective Interaction Module (SIM) to select important patch tokens with intra-modal and inter-modal information. These important patch tokens engage in the interaction with class tokens, thereby yielding more discriminative features. Then, we propose a Global Alignment Module (GAM) to simultaneously align multi-modal features by minimizing the volume of 3D polyhedra in the gramian space. Meanwhile, we propose a Local Alignment Module (LAM) to align local features in a shift-aware manner. With these modules, our proposed framework could extract more discriminative features for object ReID. Extensive experiments on three multi-modal object ReID benchmarks (i.e., RGBNT201, RGBNT100, MSVR310) validate the effectiveness of our method. The source code is available at https://github.com/010129/Signal.

</details>


### [140] [CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking](https://arxiv.org/abs/2511.17967)
*Hao Li,Yuhao Wang,Xiantao Hu,Wenning Hao,Pingping Zhang,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: CADTrack is a novel RGB-Thermal tracking framework that uses Mamba-based feature interaction, contextual aggregation, and deformable alignment to address modality discrepancies and improve tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing RGBT trackers struggle with modality discrepancies between visible and thermal infrared data, which hinders effective cross-modal information fusion and reduces tracking robustness.

Method: Proposes three key components: 1) Mamba-based Feature Interaction for efficient cross-modal interaction with linear complexity, 2) Contextual Aggregation Module using Mixture-of-Experts for dynamic layer activation, and 3) Deformable Alignment Module for spatial alignment and temporal propagation.

Result: Extensive experiments on five RGBT tracking benchmarks demonstrate the effectiveness of CADTrack, achieving robust and accurate tracking in complex scenarios.

Conclusion: CADTrack successfully addresses modality discrepancies in RGBT tracking through its novel framework, providing improved feature representation and cross-modal fusion for all-weather object tracking.

Abstract: RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.

</details>


### [141] [Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning](https://arxiv.org/abs/2511.17973)
*Hiroto Honda*

Main category: cs.CV

TL;DR: APR uses adversarial attacks on new task images to create pseudo-replay samples for knowledge distillation, addressing catastrophic forgetting in exemplar-free class-incremental learning without storing old data.


<details>
  <summary>Details</summary>
Motivation: Address the plasticity-stability dilemma in exemplar-free class-incremental learning where old images cannot be stored due to storage constraints or privacy concerns, preventing catastrophic forgetting of old knowledge.

Method: Adversarial pseudo-replay (APR) perturbs new task images using adversarial attacks with old class mean prototypes as targets, then uses resulting images for knowledge distillation. Also calibrates covariance matrices using transfer matrix learning on pseudo-replay samples.

Result: Achieves state-of-the-art performance on challenging cold-start settings of standard EFCIL benchmarks, effectively reconciling stability and plasticity.

Conclusion: APR successfully addresses catastrophic forgetting in exemplar-free class-incremental learning by generating pseudo-replay samples online without storing actual replay data, demonstrating effective balance between learning new tasks and retaining old knowledge.

Abstract: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.

</details>


### [142] [FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning](https://arxiv.org/abs/2511.17979)
*Bo Yin,Xiaobin Hu,Xingyu Zhou,Peng-Tao Jiang,Yue Liao,Junwei Zhu,Jiangning Zhang,Ying Tai,Chengjie Wang,Shuicheng Yan*

Main category: cs.CV

TL;DR: FeRA is a frequency-driven fine-tuning framework that aligns parameter updates with diffusion models' intrinsic frequency energy progression for effective adaptation to new tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting large pretrained diffusion models to new tasks by understanding and leveraging the underlying frequency energy mechanism during denoising.

Method: Three components: frequency energy indicator to characterize latent bandwise energy distribution, soft frequency router that fuses multiple frequency-specific adapter experts, and frequency energy consistency regularization for stable optimization.

Result: FeRA provides a simple, stable, and compatible paradigm that integrates seamlessly with adapter-based tuning schemes and generalizes well across diffusion backbones and resolutions.

Conclusion: By aligning adaptation with the frequency energy mechanism, FeRA enables effective and robust diffusion model adaptation through frequency-driven fine-tuning.

Abstract: Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.

</details>


### [143] [Plan-X: Instruct Video Generation via Semantic Planning](https://arxiv.org/abs/2511.17986)
*Lun Huang,You Xie,Hongyi Xu,Tianpei Gu,Chenxu Zhang,Guoxian Song,Zenan Li,Xiaochen Zhao,Linjie Luo,Guillermo Sapiro*

Main category: cs.CV

TL;DR: Plan-X is a framework that uses a Semantic Planner to generate structured semantic tokens for video diffusion models, reducing hallucinations and improving instruction alignment in video generation.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers struggle with high-level semantic reasoning and long-horizon planning, leading to visual hallucinations and misalignments with user instructions in complex scenarios.

Method: Uses a learnable multimodal language model (Semantic Planner) to reason over user intent and generate text-grounded spatio-temporal semantic tokens that guide video diffusion models.

Result: Substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.

Conclusion: Plan-X effectively integrates language models' reasoning capabilities with diffusion models' synthesis strengths for improved video generation.

Abstract: Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.

</details>


### [144] [HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.17988)
*Haodong Chen,Xianfei Han,Qwen*

Main category: cs.CV

TL;DR: HyM-UNet: A hybrid CNN-Mamba architecture for medical image segmentation that combines local feature extraction with global modeling, achieving superior performance on skin lesion segmentation with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: CNNs struggle to capture complex global anatomical structures due to limited receptive fields, while medical segmentation tasks require understanding both local details and global context for accurate organ and lesion segmentation.

Method: Proposes HyM-UNet with: 1) Hierarchical Encoder using CNNs in shallow stages for texture details and Visual Mamba in deep stages for global dependencies; 2) Mamba-Guided Fusion Skip Connection that uses deep semantic features to suppress background noise in shallow features.

Result: Outperforms state-of-the-art methods on ISIC 2018 dataset in Dice coefficient and IoU metrics, while maintaining lower parameter counts and inference latency.

Conclusion: HyM-UNet effectively handles medical segmentation tasks with complex shapes and scale variations, validating the synergy between CNN's local feature extraction and Mamba's global modeling capabilities.

Abstract: Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.

</details>


### [145] [SD-PSFNet: Sequential and Dynamic Point Spread Function Network for Image Deraining](https://arxiv.org/abs/2511.17993)
*Jiayu Wang,Haoyu Bian,Haoran Sun,Shaoning Zeng*

Main category: cs.CV

TL;DR: SD-PSFNet is a multi-stage image deraining method that uses Point Spread Function mechanisms to model rain degradation, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Image deraining faces challenges from complex multi-scale rain physics and its coupling with scenes, requiring better physical modeling of the degradation process.

Method: Uses a three-stage sequential restoration architecture with learned PSF mechanisms to dynamically simulate rain streak optics, combined with adaptive gated fusion for cross-stage feature integration.

Result: Achieves SOTA PSNR/SSIM: Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), RealRain-1k-H (41.08dB/0.9838).

Conclusion: SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall, providing a new physics-aware approach to image deraining.

Abstract: Image deraining is crucial for vision applications but is challenged by the complex multi-scale physics of rain and its coupling with scenes. To address this challenge, a novel approach inspired by multi-stage image restoration is proposed, incorporating Point Spread Function (PSF) mechanisms to reveal the image degradation process while combining dynamic physical modeling with sequential feature fusion transfer, named SD-PSFNet. Specifically, SD-PSFNet employs a sequential restoration architecture with three cascaded stages, allowing multiple dynamic evaluations and refinements of the degradation process estimation. The network utilizes components with learned PSF mechanisms to dynamically simulate rain streak optics, enabling effective rain-background separation while progressively enhancing outputs through novel PSF components at each stage. Additionally, SD-PSFNet incorporates adaptive gated fusion for optimal cross-stage feature integration, enabling sequential refinement from coarse rain removal to fine detail restoration. Our model achieves state-of-the-art PSNR/SSIM metrics on Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), and RealRain-1k-H (41.08dB/0.9838). In summary, SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall conditions, providing a new physics-aware approach to image deraining.

</details>


### [146] [RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale](https://arxiv.org/abs/2511.18005)
*Shengyuan Wang,Zhiheng Zheng,Yu Shang,Lixuan He,Yangcheng Yu,Fan Hangyu,Jie Feng,Qingmin Liao,Yong Li*

Main category: cs.CV

TL;DR: RAISECity is a reality-aligned intelligent synthesis engine for city-scale 3D generation that uses an agentic framework with multimodal tools to create high-quality 3D worlds.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges in quality, fidelity, and scalability for city-scale 3D generation, which is important for embodied intelligence and world models.

Method: An agentic framework leveraging diverse multimodal foundation tools with dynamic data processing, iterative self-reflection and refinement, and invocation of advanced multimodal tools.

Result: Achieves over 90% win-rate against existing baselines for overall perceptual quality, with superior performance in real-world alignment, shape precision, texture fidelity, and aesthetics.

Conclusion: RAISECity provides a promising foundation for applications in immersive media, embodied intelligence, and world models due to its combination of 3D quality, reality alignment, scalability, and compatibility with computer graphics pipelines.

Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \textbf{R}eality-\textbf{A}ligned \textbf{I}ntelligent \textbf{S}ynthesis \textbf{E}ngine that creates detailed, \textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.

</details>


### [147] [Is Complete Labeling Necessary? Understanding Active Learning in Longitudinal Medical Imaging](https://arxiv.org/abs/2511.18007)
*Siteng Ma,Honghui Du,Prateek Mathur,Brendan S. Kelly,Ronan P. Killeen,Aonghus Lawlor,Ruihai Dong*

Main category: cs.CV

TL;DR: Proposes LMI-AL, a deep active learning framework for longitudinal medical imaging change detection that achieves comparable performance to fully supervised models using less than 8% labeled data.


<details>
  <summary>Details</summary>
Motivation: Labeling longitudinal medical images is costly and time-consuming due to the need to identify subtle changes across multiple time points, and existing deep active learning methods focus on static tasks rather than change detection.

Method: Pairs and differences all 2D slices from baseline and follow-up 3D images, then iteratively selects the most informative pairs for labeling using deep active learning to train models with minimal manual annotation.

Result: With less than 8% of data labeled, LMI-AL achieves performance comparable to models trained on fully labeled datasets, demonstrating significant reduction in labeling costs.

Conclusion: LMI-AL provides an effective framework for longitudinal medical imaging change detection with minimal annotation requirements, and the code is publicly available for future research.

Abstract: Detecting changes in longitudinal medical imaging using deep learning requires a substantial amount of accurately labeled data. However, labeling these images is notably more costly and time-consuming than labeling other image types, as it requires labeling across various time points, where new lesions can be minor, and subtle changes are easily missed. Deep Active Learning (DAL) has shown promise in minimizing labeling costs by selectively querying the most informative samples, but existing studies have primarily focused on static tasks like classification and segmentation. Consequently, the conventional DAL approach cannot be directly applied to change detection tasks, which involve identifying subtle differences across multiple images. In this study, we propose a novel DAL framework, named Longitudinal Medical Imaging Active Learning (LMI-AL), tailored specifically for longitudinal medical imaging. By pairing and differencing all 2D slices from baseline and follow-up 3D images, LMI-AL iteratively selects the most informative pairs for labeling using DAL, training a deep learning model with minimal manual annotation. Experimental results demonstrate that, with less than 8% of the data labeled, LMI-AL can achieve performance comparable to models trained on fully labeled datasets. We also provide a detailed analysis of the method's performance, as guidance for future research. The code is publicly available at https://github.com/HelenMa9998/Longitudinal_AL.

</details>


### [148] [RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios](https://arxiv.org/abs/2511.18011)
*Jun Zhang,Jie Feng,Long Chen,Junhui Wang,Zhicheng Liu,Depeng Jin,Yong Li*

Main category: cs.CV

TL;DR: RoadBench is a comprehensive benchmark for evaluating multimodal LLMs' fine-grained spatial understanding in urban scenarios, focusing on road markings with 9,121 test cases across 6 tasks.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack evaluation for fine-grained spatial understanding in complex urban scenarios, particularly for road markings which form essential traffic networks.

Method: Proposed RoadBench with 6 tasks using BEV and FPV images to systematically evaluate recognition, joint understanding, reasoning, and domain knowledge integration capabilities.

Result: Evaluation of 14 mainstream MLLMs revealed significant shortcomings in fine-grained spatial understanding, with performance sometimes worse than simple baselines.

Conclusion: RoadBench is challenging for current MLLMs and will help advance their spatial understanding capabilities in urban environments.

Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.

</details>


### [149] [State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection](https://arxiv.org/abs/2511.18012)
*Jiaying Zhou,Qingchao Chen*

Main category: cs.CV

TL;DR: The paper introduces two prototype enhancement strategies for Weakly Supervised Open-Vocabulary Object Detection: State-Enhanced Semantic Prototypes (SESP) to capture intra-class variations and Scene-Augmented Pseudo Prototypes (SAPP) to address semantic mismatch between visual regions and text embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing semantic prototypes in WS-OVOD are static and limited, failing to capture rich intra-class visual variations from different object states. Additionally, standard pseudo-box generation creates semantic mismatch between visual region proposals (containing context) and object-centric text embeddings.

Method: Proposes two complementary strategies: 1) SESP generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, and 2) SAPP incorporates contextual semantics (e.g., "cat lying on sofa") with soft alignment mechanism for contextually consistent visual-textual representations.

Result: The integrated approach effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements in WS-OVOD performance.

Conclusion: By combining SESP and SAPP, the method successfully addresses key challenges in WS-OVOD by capturing intra-class variations and resolving semantic mismatches, leading to enhanced object detection performance.

Abstract: Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.

</details>


### [150] [Modeling Retinal Ganglion Cells with Neural Differential Equations](https://arxiv.org/abs/2511.18014)
*Kacper Dobek,Daniel Jankowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: LTC and CfC networks outperform CNNs and LSTMs in modeling retinal cell activity with better accuracy, speed, and efficiency, making them ideal for edge vision prosthetics.


<details>
  <summary>Details</summary>
Motivation: To develop efficient neural network models for retinal ganglion cell activity that work well with limited data and frequent retraining requirements in vision prosthetic applications.

Method: Used Liquid Time-Constant Networks (LTCs) and Closed-form Continuous-time Networks (CfCs) to model retinal ganglion cell activity in tiger salamanders across three datasets, comparing against convolutional baselines and LSTMs.

Result: Both LTCs and CfCs achieved lower MAE, faster convergence, smaller model sizes, and favorable query times compared to baselines, though with slightly lower Pearson correlation.

Conclusion: LTC and CfC networks are efficient and adaptable for scenarios with limited data and frequent retraining, making them well-suited for edge deployments in vision prosthetics.

Abstract: This work explores Liquid Time-Constant Networks (LTCs) and Closed-form Continuous-time Networks (CfCs) for modeling retinal ganglion cell activity in tiger salamanders across three datasets. Compared to a convolutional baseline and an LSTM, both architectures achieved lower MAE, faster convergence, smaller model sizes, and favorable query times, though with slightly lower Pearson correlation. Their efficiency and adaptability make them well suited for scenarios with limited data and frequent retraining, such as edge deployments in vision prosthetics.

</details>


### [151] [MambaX: Image Super-Resolution with State Predictive Control](https://arxiv.org/abs/2511.18028)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Naoto Yokoya,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: MambaX is a nonlinear state predictive control model for image super-resolution that addresses limitations of existing approaches by dynamically learning nonlinear state parameters and introducing state cross-control for multimodal fusion.


<details>
  <summary>Details</summary>
Motivation: Existing super-resolution methods focus on final resolution enhancement but neglect error propagation control in intermediate stages. Mamba's fixed linear mapper has limited receptive field and flexibility for fine-grained images.

Method: Maps consecutive spectral bands into latent state space, uses dynamic state predictive control to approximate nonlinear differential coefficients, introduces state cross-control for multimodal fusion, and employs progressive transitional learning to mitigate domain/modality shifts.

Result: Demonstrates superior performance in both single-image SR and multimodal fusion-based SR tasks, showing substantial potential for spectrally generalized modeling across dimensions and modalities.

Conclusion: MambaX effectively addresses limitations of existing sequence models through dynamic nonlinear state control and multimodal fusion capabilities, advancing spectrally generalized modeling for super-resolution tasks.

Abstract: Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.

</details>


### [152] [Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation](https://arxiv.org/abs/2511.18037)
*Yunfan Lu,Nico Messikommer,Xiaogang Xu,Liming Chen,Yuhan Chen,Nikola Zubic,Davide Scaramuzza,Hui Xiong*

Main category: cs.CV

TL;DR: First unified noise model for event frame hybrid sensors that jointly models APS and EVS noise, with calibration pipeline and HESIM simulator for realistic noise generation.


<details>
  <summary>Details</summary>
Motivation: Event frame hybrid sensors combine APS and EVS advantages but introduce complex noise patterns that are poorly understood and unmodeled.

Method: Developed statistics-based imaging noise model incorporating photon shot noise, dark current noise, fixed-pattern noise, and quantization noise; created calibration pipeline to estimate noise parameters; built HESIM simulator for realistic noise generation.

Result: Validated model on two hybrid sensors across multiple imaging tasks (video frame interpolation, deblurring), demonstrating strong simulation-to-real data transfer.

Conclusion: The unified noise model and HESIM simulator provide effective tools for understanding and simulating noise in event frame hybrid sensors, with practical applications in various imaging tasks.

Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.

</details>


### [153] [UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios](https://arxiv.org/abs/2511.18050)
*Tian Ye,Song Fei,Lei Zhu*

Main category: cs.CV

TL;DR: UltraFlux is a 4K diffusion transformer that addresses coupled failure modes in positional encoding, VAE compression, and optimization through data-model co-design, achieving stable 4K generation across diverse aspect ratios.


<details>
  <summary>Details</summary>
Motivation: Extending diffusion transformers to native 4K resolution across diverse aspect ratios reveals tightly coupled failure modes that cannot be solved by addressing individual components in isolation.

Method: Combines Resonance 2D RoPE with YaRN for positional encoding, VAE post-training for reconstruction fidelity, SNR-Aware Huber Wavelet objective for gradient balancing, and Stage-wise Aesthetic Curriculum Learning for high-aesthetic supervision.

Result: Outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics on 4K benchmarks, and matches or surpasses proprietary Seedream 4.0 with LLM prompt refinement.

Conclusion: The data-model co-design approach with integrated components enables stable, detail-preserving 4K diffusion transformers that generalize across wide, square, and tall aspect ratios.

Abstract: Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.

</details>


### [154] [IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment](https://arxiv.org/abs/2511.18055)
*Bowen Qu,Shangkun Sun,Xiaoyu Liang,Wei Gao*

Main category: cs.CV

TL;DR: IE-Bench: A benchmark suite and IE-Critic-R1 metric for evaluating text-driven image editing quality that aligns with human perception using RLVR training.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for text-driven image editing focus only on text-image alignment and don't well capture human perception, failing to account for the dynamic relationship between source images and editing prompts.

Method: Created IE-Bench with diverse source images, editing prompts, and 4,000 human-rated samples; developed IE-Critic-R1 using Reinforcement Learning from Verifiable Rewards (RLVR) for comprehensive quality assessment.

Result: IE-Critic-R1 demonstrates superior alignment with human perception compared to previous metrics on text-driven image editing tasks.

Conclusion: IE-Bench provides a robust evaluation framework, and IE-Critic-R1 offers more comprehensive and explainable assessment that better matches human judgment for text-driven image editing.

Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.

</details>


### [155] [Hierarchical Semi-Supervised Active Learning for Remote Sensing](https://arxiv.org/abs/2511.18058)
*Wei Huang,Zhitong Xiong,Chenying Liu,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: HSSAL framework combines semi-supervised learning and hierarchical active learning to efficiently utilize unlabeled remote sensing data, achieving near-fully-supervised accuracy with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of costly and time-consuming labeled data collection in remote sensing while vast amounts of unlabeled imagery remain underutilized.

Method: Hierarchical Semi-Supervised Active Learning framework that iteratively refines models using SSL (weak-to-strong self-training) and conducts hierarchical active learning with progressive clustering for sample selection based on scalability, diversity, and uncertainty.

Result: Achieves over 95% of fully-supervised accuracy with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45 datasets respectively, outperforming SSL- or AL-only baselines.

Conclusion: HSSAL demonstrates superior label efficiency through effective exploitation of unlabeled data informativeness, providing an efficient solution for remote sensing applications with limited labeled data.

Abstract: The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.

</details>


### [156] [A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)](https://arxiv.org/abs/2511.18063)
*Gabriela Fernandes*

Main category: cs.CV

TL;DR: Deep learning model using EfficientNet-B3 achieves 73.23% accuracy in distinguishing cervical adenocarcinoma in situ from normal cervical gland histology, deployed as a virtual pathology assistant.


<details>
  <summary>Details</summary>
Motivation: Cervical adenocarcinoma in situ (AIS) is a challenging premalignant lesion to diagnose, and early detection is crucial to prevent progression to invasive cancer.

Method: Used EfficientNet-B3 CNN trained on 2240 H&E images with stain normalization, patch preprocessing, class-balanced sampling, and focal loss to handle dataset imbalance.

Result: Achieved 73.23% overall accuracy with F1-scores of 0.75 (Abnormal) and 0.71 (Normal), with Grad-CAM showing biologically interpretable activation patterns.

Conclusion: Demonstrates feasibility of lightweight, interpretable AI systems for cervical gland pathology with applications in screening, education, and low-resource settings.

Abstract: Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.

</details>


### [157] [VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection](https://arxiv.org/abs/2511.18075)
*Jianhang Yao,Yongbin Zheng,Siqi Lu,Wanying Xu,Peng Sun*

Main category: cs.CV

TL;DR: VK-Det is a visual knowledge-guided open-vocabulary object detection framework that eliminates text dependence by leveraging vision encoder's inherent region perception and prototype-aware pseudo-labeling, achieving state-of-the-art performance without extra supervision.


<details>
  <summary>Details</summary>
Motivation: Existing open-vocabulary aerial object detection methods rely on text supervision, which causes semantic bias and restricts expansion to text-specified concepts, limiting their ability to identify objects beyond predefined categories.

Method: 1) Leverages vision encoder's inherent informative region perception for fine-grained localization and adaptive distillation. 2) Introduces prototype-aware pseudo-labeling that models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching.

Result: Achieves state-of-the-art performance with 30.1 mAP^N on DIOR and 23.3 mAP^N on DOTA, outperforming even methods with extra supervision.

Conclusion: VK-Det successfully eliminates text dependence in open-vocabulary object detection by leveraging visual knowledge, enabling better generalization to novel categories without requiring additional supervision.

Abstract: To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.

</details>


### [158] [ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models](https://arxiv.org/abs/2511.18082)
*Wencheng Ye,Tianshi Wang,Lei Zhu,Fengling Li,Guoli Yang*

Main category: cs.CV

TL;DR: ActDistill is a framework that transfers action prediction capabilities from large VLA models to lightweight counterparts using action-guided distillation, achieving over 50% computation reduction and up to 1.67x speedup while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current VLA models have heavy computational overhead and inference latency that limit their deployment in robotic manipulation, despite their impressive flexibility and generalization capabilities.

Method: Uses a graph-structured encapsulation strategy to model hierarchical action prediction evolution, with a teacher-student distillation framework and dynamic router that adaptively selects computation paths based on action prediction demands.

Result: Achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup on embodied benchmarks.

Conclusion: Establishes a general paradigm for efficient embodied intelligence by enabling lightweight VLA models with minimal computation and latency while maintaining high-precision action prediction.

Abstract: Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.

</details>


### [159] [Less Is More: An Explainable AI Framework for Lightweight Malaria Classification](https://arxiv.org/abs/2511.18083)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CV

TL;DR: Simple feature engineering with Logistic Regression achieves 94.80% accuracy for malaria cell classification, outperforming complex deep learning models in efficiency while maintaining clinical relevance.


<details>
  <summary>Details</summary>
Motivation: To determine if complex neural networks are necessary for simple binary classification tasks like malaria detection, and to create a transparent, reproducible, low-compute alternative suitable for real-world deployment.

Method: Used NIH Malaria Cell Images dataset, extracted two morphological features (non-background pixels and cell holes), compared Logistic Regression and Random Forest against deep learning models (ResNet18, DenseNet121, MobileNetV2, EfficientNet) on accuracy, model size, and CPU inference time.

Result: Single-variable Logistic Regression achieved 94.80% accuracy with 1.2 kB file size and 2.3 ms inference time. Ensemble model improved to 97.15% accuracy. Deep learning models required 13.6-44.7 MB storage and 68 ms inference time.

Conclusion: Compact feature engineering with interpretable models can provide clinically meaningful performance while offering superior transparency, reproducibility, speed, and deployment feasibility compared to deep learning approaches.

Abstract: Background and Objective: Deep learning models have high computational needs and lack interpretability but are often the first choice for medical image classification tasks. This study addresses whether complex neural networks are essential for the simple binary classification task of malaria. We introduce the Extracted Morphological Feature Engineered (EMFE) pipeline, a transparent, reproducible, and low compute machine learning approach tailored explicitly for simple cell morphology, designed to achieve deep learning performance levels on a simple CPU only setup with the practical aim of real world deployment.
  Methods: The study used the NIH Malaria Cell Images dataset, with two features extracted from each cell image: the number of non background pixels and the number of holes within the cell. Logistic Regression and Random Forest were compared against ResNet18, DenseNet121, MobileNetV2, and EfficientNet across accuracy, model size, and CPU inference time. An ensemble model was created by combining Logistic Regression and Random Forests to achieve higher accuracy while retaining efficiency.
  Results: The single variable Logistic Regression model achieved a test accuracy of 94.80 percent with a file size of 1.2 kB and negligible inference latency (2.3 ms). The two stage ensemble improved accuracy to 97.15 percent. In contrast, the deep learning methods require 13.6 MB to 44.7 MB of storage and show significantly higher inference times (68 ms).
  Conclusion: This study shows that a compact feature engineering approach can produce clinically meaningful classification performance while offering gains in transparency, reproducibility, speed, and deployment feasibility. The proposed pipeline demonstrates that simple interpretable features paired with lightweight models can serve as a practical diagnostic solution for environments with limited computational resources.

</details>


### [160] [Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective](https://arxiv.org/abs/2511.18089)
*Wenjing Liu,Qin Ren,Wen Zhang,Yuewei Lin,Chenyu You*

Main category: cs.CV

TL;DR: TTA introduces a min-max optimization framework for multi-modal survival analysis that balances cross-modal alignment with preservation of modality-specific characteristics to prevent representation collapse.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal survival analysis methods overemphasize cross-modal alignment through attention mechanisms, leading to representation collapse and reduced diversity by neglecting modality-specific characteristics.

Method: Together-Then-Apart (TTA) framework with dual optimization: Together stage minimizes semantic discrepancies via shared prototypes and unbalanced optimal transport; Apart stage maximizes diversity through modality anchors and contrastive regularization.

Result: Extensive experiments on five TCGA benchmarks show TTA consistently outperforms state-of-the-art methods in survival analysis.

Conclusion: TTA provides a theoretical perspective for jointly achieving alignment and distinctiveness, enabling robust, interpretable, and biologically meaningful multi-modal survival analysis.

Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.

</details>


### [161] [Versatile Recompression-Aware Perceptual Image Super-Resolution](https://arxiv.org/abs/2511.18090)
*Mingwei He,Tongda Xu,Xingtong Ge,Ming Sun,Chao Zhou,Yan Wang*

Main category: cs.CV

TL;DR: VRPSR is a method that makes perceptual super-resolution aware of recompression, using a diffusion-based codec simulator to optimize SR models for better compression performance.


<details>
  <summary>Details</summary>
Motivation: Current perceptual SR methods ignore recompression, leading to suboptimal results when outputs are compressed for storage/transmission, as codecs add additional artifacts.

Method: Formulates compression as conditional text-to-image generation using pre-trained diffusion model as codec simulator, with training techniques including perceptual optimization and using slightly compressed images as targets.

Result: Saves more than 10% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression.

Conclusion: VRPSR enables joint optimization of SR and post-processing after recompression, making perceptual SR compression-aware.

Abstract: Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.

</details>


### [162] [Spotlight: Identifying and Localizing Video Generation Errors Using VLMs](https://arxiv.org/abs/2511.18102)
*Aditya Chinchure,Sahithya Ravi,Pushkar Shukla,Vered Shwartz,Leonid Sigal*

Main category: cs.CV

TL;DR: Spotlight introduces a novel task for localizing and explaining errors in text-to-video generation, identifying six error types across 600 videos from three state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current T2V models still produce nuanced errors, but existing evaluation methods assess videos holistically without identifying when specific errors occur or describing their nature.

Method: Generated 600 videos using 200 diverse prompts and three video generators (Veo 3, Seedance, LTX-2), annotated over 1600 fine-grained errors across six types including motion, physics, and prompt adherence.

Result: Adherence and physics errors are predominant and persist longer, while appearance-disappearance and body pose errors occur in shorter segments. VLMs significantly lag behind humans in error identification and localization.

Conclusion: The task enables building fine-grained evaluation tools and more sophisticated reward models for video generators, with inference-time strategies improving VLM performance by nearly 2x.

Abstract: Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.

</details>


### [163] [Consolidating Diffusion-Generated Video Detection with Unified Multimodal Forgery Learning](https://arxiv.org/abs/2511.18104)
*Xiaohong Liu,Xiufeng Song,Huayu Zheng,Lei Bai,Xiaoming Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: MM-Det++ is a multimodal detection algorithm for identifying diffusion-generated videos, using spatio-temporal analysis and multimodal reasoning with MLLMs, achieving superior performance through unified multimodal learning.


<details>
  <summary>Details</summary>
Motivation: The proliferation of diffusion-generated videos raises security concerns, while existing methods focus mainly on image-level forgery detection, leaving video-level detection underexplored.

Method: Two-branch approach: ST branch uses Frame-Centric Vision Transformer for spatio-temporal analysis; MM branch uses Multimodal Large Language Models for semantic forgery reasoning; Unified Multimodal Learning module integrates representations.

Result: Extensive experiments demonstrate superiority of MM-Det++ and effectiveness of unified multimodal forgery learning in detecting diffusion-generated videos.

Conclusion: The proposed MM-Det++ with unified multimodal learning effectively addresses video forgery detection for diffusion-generated content, advancing video forensics research.

Abstract: The proliferation of videos generated by diffusion models has raised increasing concerns about information security, highlighting the urgent need for reliable detection of synthetic media. Existing methods primarily focus on image-level forgery detection, leaving generic video-level forgery detection largely underexplored. To advance video forensics, we propose a consolidated multimodal detection algorithm, named MM-Det++, specifically designed for detecting diffusion-generated videos. Our approach consists of two innovative branches and a Unified Multimodal Learning (UML) module. Specifically, the Spatio-Temporal (ST) branch employs a novel Frame-Centric Vision Transformer (FC-ViT) to aggregate spatio-temporal information for detecting diffusion-generated videos, where the FC-tokens enable the capture of holistic forgery traces from each video frame. In parallel, the Multimodal (MM) branch adopts a learnable reasoning paradigm to acquire Multimodal Forgery Representation (MFR) by harnessing the powerful comprehension and reasoning capabilities of Multimodal Large Language Models (MLLMs), which discerns the forgery traces from a flexible semantic perspective. To integrate multimodal representations into a coherent space, a UML module is introduced to consolidate the generalization ability of MM-Det++. In addition, we also establish a large-scale and comprehensive Diffusion Video Forensics (DVF) dataset to advance research in video forgery detection. Extensive experiments demonstrate the superiority of MM-Det++ and highlight the effectiveness of unified multimodal forgery learning in detecting diffusion-generated videos.

</details>


### [164] [AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens](https://arxiv.org/abs/2511.18105)
*Purvish Jajal,Nick John Eliopoulos,Benjamin Shiue-Hal Chou,George K. Thiruvathukal,Yung-Hsiang Lu,James C. Davis*

Main category: cs.CV

TL;DR: AdaPerceiver is the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model, enabling dynamic computation allocation to meet diverse hardware and latency constraints.


<details>
  <summary>Details</summary>
Motivation: Modern transformers are rigid in computation allocation at inference time, while real-world deployment requires adaptation to diverse hardware and latency constraints. Existing approaches focus on single axes like token reduction, lacking comprehensive adaptivity.

Method: Proposed AdaPerceiver architecture with unified adaptivity across depth, width, and tokens, coupled with an efficient joint training regime to maintain performance across various configurations.

Result: On image classification: achieves 85.4% accuracy with 36% higher throughput than FlexiViT-L. On dense prediction: matches ViT-H/14 performance with ~26x fewer encoder FLOPs on semantic segmentation and depth estimation. With policy: maintains ImageNet1K accuracy (±0.1%) while reducing FLOPs by 24-33%.

Conclusion: AdaPerceiver successfully expands the accuracy-throughput Pareto front and enables efficient dynamic computation allocation across multiple axes, making transformers more adaptable to real-world deployment constraints.

Abstract: Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.

</details>


### [165] [Muskie: Multi-view Masked Image Modeling for 3D Vision Pre-training](https://arxiv.org/abs/2511.18115)
*Wenyu Li,Sidun Liu,Peng Qiao,Yong Dou,Tongrui Hu*

Main category: cs.CV

TL;DR: Muskie is a native multi-view vision backbone that processes multiple views simultaneously to achieve better multi-view consistency and geometric understanding for 3D vision tasks.


<details>
  <summary>Details</summary>
Motivation: Existing models are frame-wise with limited multi-view consistency, so the authors aim to develop a model that can process multiple views simultaneously and introduce multi-view consistency during pre-training.

Method: Muskie is trained with an aggressive masking strategy to reconstruct heavily masked content in one view by finding geometric correspondences from other views, learning view-invariant features without 3D supervision.

Result: Muskie achieves higher multi-view correspondence accuracy than state-of-the-art frame-wise backbones like DINO, and consistently enhances performance on downstream 3D tasks including camera pose estimation and pointmap reconstruction.

Conclusion: Muskie demonstrates that native multi-view processing with geometric correspondence learning during pre-training leads to better multi-view consistency and improved performance on 3D vision tasks.

Abstract: We present Muskie, a native multi-view vision backbone designed for 3D vision tasks. Unlike existing models, which are frame-wise and exhibit limited multi-view consistency, Muskie is designed to process multiple views simultaneously and introduce multi-view consistency in pre-training stage. Muskie is trained to reconstruct heavily masked content in one view by finding and utilizing geometric correspondences from other views. Through this pretext task and our proposed aggressive masking strategy, the model implicitly to learn view-invariant features and develop strong geometric understanding without any 3D supervision. Compared with state-of-the-art frame-wise backbones such as DINO, Muskie achieves higher multi-view correspondence accuracy. Furthermore, we demonstrate that using Muskie as a backbone consistently enhances performance on downstream 3D tasks, including camera pose estimation and pointmap reconstruction. Codes are publicly available at https://leo-frank.github.io/Muskie/

</details>


### [166] [PromptMoE: Generalizable Zero-Shot Anomaly Detection via Visually-Guided Prompt Mixtures](https://arxiv.org/abs/2511.18116)
*Yuheng Shao,Lizhang Wang,Changhao Li,Peixian Chen,Qinyuan Liu*

Main category: cs.CV

TL;DR: PromptMoE is a zero-shot anomaly detection method that uses a Mixture-of-Experts approach to dynamically combine multiple expert prompts instead of using single fixed prompts, achieving state-of-the-art performance across 15 datasets.


<details>
  <summary>Details</summary>
Motivation: Current vision-language model based methods for zero-shot anomaly detection suffer from representational bottlenecks and overfitting due to limited prompt engineering strategies, failing to handle the complexity and diversity of unseen anomalies.

Method: Proposes PromptMoE with a Visually-Guided Mixture of Prompt (VGMoP) mechanism that learns a pool of expert prompts as semantic primitives and uses image-gated sparse MoE to dynamically combine them for each instance.

Result: Extensive experiments across 15 industrial and medical datasets demonstrate state-of-the-art performance and effectiveness of the proposed method.

Conclusion: PromptMoE provides a compositional approach to prompt learning that overcomes limitations of existing methods and achieves robust zero-shot anomaly detection through dynamic expert prompt combination.

Abstract: Zero-Shot Anomaly Detection (ZSAD) aims to identify and localize anomalous regions in images of unseen object classes. While recent methods based on vision-language models like CLIP show promise, their performance is constrained by existing prompt engineering strategies. Current approaches, whether relying on single fixed, learnable, or dense dynamic prompts, suffer from a representational bottleneck and are prone to overfitting on auxiliary data, failing to generalize to the complexity and diversity of unseen anomalies. To overcome these limitations, we propose $\mathtt{PromptMoE}$. Our core insight is that robust ZSAD requires a compositional approach to prompt learning. Instead of learning monolithic prompts, $\mathtt{PromptMoE}$ learns a pool of expert prompts, which serve as a basis set of composable semantic primitives, and a visually-guided Mixture-of-Experts (MoE) mechanism to dynamically combine them for each instance. Our framework materializes this concept through a Visually-Guided Mixture of Prompt (VGMoP) that employs an image-gated sparse MoE to aggregate diverse normal and abnormal expert state prompts, generating semantically rich textual representations with strong generalization. Extensive experiments across 15 datasets in industrial and medical domains demonstrate the effectiveness and state-of-the-art performance of $\mathtt{PromptMoE}$.

</details>


### [167] [MVS-TTA: Test-Time Adaptation for Multi-View Stereo via Meta-Auxiliary Learning](https://arxiv.org/abs/2511.18120)
*Hannuo Zhang,Zhixiang Chi,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: MVS-TTA is a test-time adaptation framework that enhances learning-based multi-view stereo methods by using self-supervised cross-view consistency loss and meta-auxiliary learning for scene-specific adaptation.


<details>
  <summary>Details</summary>
Motivation: Learning-based MVS methods have limited generalization due to fixed parameters trained on limited data, while optimization-based methods lack scalability and require costly per-scene optimization.

Method: Uses self-supervised cross-view consistency loss as auxiliary task for test-time adaptation, with meta-auxiliary learning strategy to train models to benefit from auxiliary-task-based updates.

Result: Consistently improves performance on standard datasets (DTU, BlendedMVS) and challenging cross-dataset settings, even when applied to state-of-the-art MVS models.

Conclusion: First integration of optimization-based test-time adaptation into learning-based MVS using meta-learning, providing model-agnostic framework with minimal architectural changes.

Abstract: Recent learning-based multi-view stereo (MVS) methods are data-driven and have achieved remarkable progress due to large-scale training data and advanced architectures. However, their generalization remains sub-optimal due to fixed model parameters trained on limited training data distributions. In contrast, optimization-based methods enable scene-specific adaptation but lack scalability and require costly per-scene optimization. In this paper, we propose MVS-TTA, an efficient test-time adaptation (TTA) framework that enhances the adaptability of learning-based MVS methods by bridging these two paradigms. Specifically, MVS-TTA employs a self-supervised, cross-view consistency loss as an auxiliary task to guide inference-time adaptation. We introduce a meta-auxiliary learning strategy to train the model to benefit from auxiliary-task-based updates explicitly. Our framework is model-agnostic and can be applied to a wide range of MVS methods with minimal architectural changes. Extensive experiments on standard datasets (DTU, BlendedMVS) and a challenging cross-dataset generalization setting demonstrate that MVS-TTA consistently improves performance, even when applied to state-of-the-art MVS models. To our knowledge, this is the first attempt to integrate optimization-based test-time adaptation into learning-based MVS using meta-learning. The code will be available at https://github.com/mart87987-svg/MVS-TTA.

</details>


### [168] [VCU-Bridge: Hierarchical Visual Connotation Understanding via Semantic Bridging](https://arxiv.org/abs/2511.18121)
*Ming Zhong,Yuanlei Wang,Liuzhou Zhang,Arctanx An,Renrui Zhang,Hao Liang,Ming Lu,Ying Shen,Wentao Zhang*

Main category: cs.CV

TL;DR: VCU-Bridge is a framework that mimics human visual understanding hierarchy, progressing from perception to semantic bridging to abstract connotation, with explicit evidence tracing. HVCU-Bench evaluates this hierarchy, showing performance declines at higher reasoning levels. MCTS-guided instruction tuning improves both hierarchical and general benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs process visual information differently from humans, treating details and concepts in isolation rather than integrating them. Existing evaluations decouple perception from reasoning, missing semantic dependencies and hiding performance bottlenecks.

Method: Developed VCU-Bridge framework with hierarchical visual connotation understanding (perception → semantic bridging → abstract connotation) and explicit evidence tracing. Created HVCU-Bench benchmark for level-wise diagnostics. Used Monte Carlo Tree Search for instruction tuning data generation.

Result: Performance consistently declines as reasoning progresses to higher levels. MCTS-guided instruction tuning improves HVCU-Bench performance and general benchmarks (+2.53% average, +7.26% on MMStar), showing hierarchical thinking enhances MLLM capabilities.

Conclusion: The hierarchical thinking pattern is significant for MLLM capabilities. Strengthening low-level visual understanding yields measurable gains at higher reasoning levels, demonstrating the framework's effectiveness in bridging human-like visual integration.

Abstract: While Multimodal Large Language Models (MLLMs) excel on benchmarks, their processing paradigm differs from the human ability to integrate visual information. Unlike humans who naturally bridge details and high-level concepts, models tend to treat these elements in isolation. Prevailing evaluation protocols often decouple low-level perception from high-level reasoning, overlooking their semantic and causal dependencies, which yields non-diagnostic results and obscures performance bottlenecks. We present VCU-Bridge, a framework that operationalizes a human-like hierarchy of visual connotation understanding: multi-level reasoning that advances from foundational perception through semantic bridging to abstract connotation, with an explicit evidence-to-inference trace from concrete cues to abstract conclusions. Building on this framework, we construct HVCU-Bench, a benchmark for hierarchical visual connotation understanding with explicit, level-wise diagnostics. Comprehensive experiments demonstrate a consistent decline in performance as reasoning progresses to higher levels. We further develop a data generation pipeline for instruction tuning guided by Monte Carlo Tree Search (MCTS) and show that strengthening low-level capabilities yields measurable gains at higher levels. Interestingly, it not only improves on HVCU-Bench but also brings benefits on general benchmarks (average +2.53%), especially with substantial gains on MMStar (+7.26%), demonstrating the significance of the hierarchical thinking pattern and its effectiveness in enhancing MLLM capabilities. The project page is at https://vcu-bridge.github.io .

</details>


### [169] [Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models](https://arxiv.org/abs/2511.18123)
*Dachuan Zhao,Weiyue Li,Zhenda Shen,Yushu Qiu,Bowen Xu,Haoyu Chen,Yongchao Chen*

Main category: cs.CV

TL;DR: SPD is a subspace projection debiasing method that removes entire bias subspaces from vision-language models while preserving semantic fidelity, achieving better fairness than coordinate-wise approaches.


<details>
  <summary>Details</summary>
Motivation: Current coordinate-wise debiasing methods fail due to feature entanglement, poor generalization, and incomplete bias removal, as bias is distributed across linear subspaces rather than isolated coordinates.

Method: Proposes Subspace Projection Debiasing (SPD) that identifies and removes entire subspaces of linearly decodable bias while reinserting neutral mean components to maintain semantic fidelity.

Result: SPD achieves 18.5% average improvement across four fairness metrics while maintaining minimal task performance loss in zero-shot classification, text-to-image retrieval, and image generation tasks.

Conclusion: SPD provides a geometrically principled framework for robust debiasing that outperforms coordinate-wise approaches by addressing bias at the subspace level rather than individual coordinates.

Abstract: Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\textbf{S}$ubspace $\textbf{P}$rojection $\textbf{D}$ebiasing ($\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.

</details>


### [170] [SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation](https://arxiv.org/abs/2511.18127)
*Ruicong Liu,Yifei Huang,Liangyang Ouyang,Caixin Kang,Yoichi Sato*

Main category: cs.CV

TL;DR: SFHand is the first streaming framework for real-time 3D hand forecasting using video and language instructions, achieving state-of-the-art performance and improving downstream task success rates.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D hand forecasting require offline video sequences and cannot incorporate language guidance, making them unsuitable for real-time applications like AR and robotics where task intent matters.

Method: SFHand uses an autoregressive streaming architecture with ROI-enhanced memory layer to predict future 3D hand states (type, bounding box, pose, trajectory) from continuous video and language streams.

Result: SFHand outperforms prior work by up to 35.8% in 3D hand forecasting and improves downstream manipulation task success rates by up to 13.4% on multiple benchmarks.

Conclusion: The framework enables real-time, language-guided 3D hand forecasting and demonstrates practical utility through transfer learning to embodied manipulation tasks.

Abstract: Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.

</details>


### [171] [Video4Edit: Viewing Image Editing as a Degenerate Temporal Process](https://arxiv.org/abs/2511.18131)
*Xiaofan Li,Yanpeng Sun,Chenming Wu,Fan Duan,YuAn Wang,Weihao Bo,Yumeng Zhang,Dingkang Liang*

Main category: cs.CV

TL;DR: This paper proposes a data-efficient image editing method that transfers temporal modeling priors from video pre-training, achieving comparable performance to state-of-the-art methods using only 1% of the supervision data.


<details>
  <summary>Details</summary>
Motivation: Current instruction-driven image editing methods require massive high-quality training triplets and are sensitive to precise semantic references in instructions, making them costly and challenging to scale.

Method: The approach treats image editing as a degenerate temporal process and transfers single-frame evolution priors learned from video pre-training to enable highly data-efficient fine-tuning.

Result: The method matches the performance of leading open-source baselines while using only about 1% of the supervision data required by mainstream editing models.

Conclusion: Temporal modeling from video pre-training provides an effective framework for data-efficient instruction-driven image editing, significantly reducing the need for large-scale curated training data.

Abstract: We observe that recent advances in multimodal foundation models have propelled instruction-driven image generation and editing into a genuinely cross-modal, cooperative regime. Nevertheless, state-of-the-art editing pipelines remain costly: beyond training large diffusion/flow models, they require curating massive high-quality triplets of \{instruction, source image, edited image\} to cover diverse user intents. Moreover, the fidelity of visual replacements hinges on how precisely the instruction references the target semantics. We revisit this challenge through the lens of temporal modeling: if video can be regarded as a full temporal process, then image editing can be seen as a degenerate temporal process. This perspective allows us to transfer single-frame evolution priors from video pre-training, enabling a highly data-efficient fine-tuning regime. Empirically, our approach matches the performance of leading open-source baselines while using only about one percent of the supervision demanded by mainstream editing models.

</details>


### [172] [SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation](https://arxiv.org/abs/2511.18136)
*Chunming He,Rihan Zhang,Longxiang Tang,Ziyun Yang,Kai Li,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: SCALER is a collaborative framework that jointly optimizes a mean-teacher segmenter and SAM through reciprocal supervision for label-deficient concealed object segmentation, achieving performance gains across eight semi- and weakly-supervised tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LDCOS methods have limited performance due to target concealment and annotation scarcity. This study investigates whether consistency constraints and SAM-based supervision can be jointly integrated, and whether the segmenter can guide SAM through reciprocal supervision for mutual improvement.

Method: SCALER operates in two alternating phases: Phase I optimizes the segmenter under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting. Phase II updates SAM via augmentation invariance and noise resistance losses.

Result: Experiments demonstrate consistent performance gains across eight semi- and weakly-supervised COS tasks. SCALER serves as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions.

Conclusion: SCALER successfully integrates consistency constraints and SAM-based supervision through reciprocal learning, enabling mutual improvement between the segmenter and SAM, providing an effective solution for label-deficient concealed object segmentation.

Abstract: Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.

</details>


### [173] [Compact neural networks for astronomy with optimal transport bias correction](https://arxiv.org/abs/2511.18139)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li*

Main category: cs.CV

TL;DR: WaveletMamba is a theory-driven framework that combines wavelet decomposition with state-space modeling to overcome the efficiency-resolution tradeoff in astronomical imaging, achieving high classification accuracy with minimal parameters and computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the efficiency-resolution tradeoff in astronomical imaging that limits large-scale morphological classification and redshift prediction.

Method: Integrates wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction using HK distance (distribution-level optimal transport) and Color-Aware Weighting (sample-level fine-tuning).

Result: Achieves 81.72% classification accuracy at 64x64 resolution with only 3.54M parameters, delivers 80.93% accuracy at 244x244 resolution with 9.7x computational efficiency gains, and shows Resolution Multistability across different input scales.

Conclusion: Mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery.

Abstract: Astronomical imaging confronts an efficiency-resolution tradeoff that limits large-scale morphological classification and redshift prediction. We introduce WaveletMamba, a theory-driven framework integrating wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction. WaveletMamba achieves 81.72% +/- 0.53% classification accuracy at 64x64 resolution with only 3.54M parameters, delivering high-resolution performance (80.93% +/- 0.27% at 244x244) at low-resolution inputs with 9.7x computational efficiency gains. The framework exhibits Resolution Multistability, where models trained on low-resolution data achieve consistent accuracy across different input scales despite divergent internal representations. The framework's multi-level bias correction synergizes HK distance (distribution-level optimal transport) with Color-Aware Weighting (sample-level fine-tuning), achieving 22.96% Log-MSE improvement and 26.10% outlier reduction without explicit selection function modeling. Here, we show that mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery.

</details>


### [174] [UnfoldLDM: Deep Unfolding-based Blind Image Restoration with Latent Diffusion Priors](https://arxiv.org/abs/2511.18152)
*Chunming He,Rihan Zhang,Zheng Chen,Bowen Yang,CHengyu Fang,Yunlong Lin,Fengyang Xiao,Sina Farsiu*

Main category: cs.CV

TL;DR: UnfoldLDM integrates deep unfolding networks with latent diffusion models for blind image restoration, addressing degradation-specific dependency and over-smoothing bias through multi-granularity degradation-aware modules and degradation-resistant priors.


<details>
  <summary>Details</summary>
Motivation: Existing deep unfolding networks suffer from degradation-specific dependency (being tied to known degradation models) and over-smoothing bias (suppressing fine textures), making them unsuitable for blind image restoration tasks.

Method: Proposes UnfoldLDM with: (1) multi-granularity degradation-aware module for gradient descent step to estimate unknown degradations, (2) degradation-resistant latent diffusion model for proximal step to extract degradation-invariant priors, and (3) over-smoothing correction transformer to recover high-frequency components and enhance textures.

Result: UnfoldLDM achieves leading performance on various blind image restoration tasks and benefits downstream tasks. The design is compatible with existing DUN-based methods as a plug-and-play framework.

Conclusion: The integration of deep unfolding networks with latent diffusion models effectively addresses limitations in blind image restoration, providing degradation-free and visually rich results while maintaining compatibility with existing methods.

Abstract: Deep unfolding networks (DUNs) combine the interpretability of model-based methods with the learning ability of deep networks, yet remain limited for blind image restoration (BIR). Existing DUNs suffer from: (1) \textbf{Degradation-specific dependency}, as their optimization frameworks are tied to a known degradation model, making them unsuitable for BIR tasks; and (2) \textbf{Over-smoothing bias}, resulting from the direct feeding of gradient descent outputs, dominated by low-frequency content, into the proximal term, suppressing fine textures. To overcome these issues, we propose UnfoldLDM to integrate DUNs with latent diffusion model (LDM) for BIR. In each stage, UnfoldLDM employs a multi-granularity degradation-aware (MGDA) module as the gradient descent step. MGDA models BIR as an unknown degradation estimation problem and estimates both the holistic degradation matrix and its decomposed forms, enabling robust degradation removal. For the proximal step, we design a degradation-resistant LDM (DR-LDM) to extract compact degradation-invariant priors from the MGDA output. Guided by this prior, an over-smoothing correction transformer (OCFormer) explicitly recovers high-frequency components and enhances texture details. This unique combination ensures the final result is degradation-free and visually rich. Experiments show that our UnfoldLDM achieves a leading place on various BIR tasks and benefits downstream tasks. Moreover, our design is compatible with existing DUN-based methods, serving as a plug-and-play framework. Code will be released.

</details>


### [175] [Assessing the alignment between infants' visual and linguistic experience using multimodal language models](https://arxiv.org/abs/2511.18824)
*Alvin Wei Ming Tan,Jane Yang,Tarun Sepuri,Khai Loong Aw,Robert Z. Sparks,Zi Yin,Virginia A. Marchman,Michael C. Frank,Bria Long*

Main category: cs.CV

TL;DR: CLIP models can automatically detect vision-language alignment in infant videos, revealing that ideal learning moments are rare in everyday experiences compared to ML datasets.


<details>
  <summary>Details</summary>
Motivation: To understand how aligned children's visual and linguistic experiences are during everyday learning, overcoming limitations of manual annotation methods.

Method: Used contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos from infant perspective in home environments, validated with human judgments.

Result: Ideal aligned learning moments (e.g., "look at the ball" with ball in view) are relatively rare in children's everyday experiences compared to ML datasets, with variability both within and across children.

Conclusion: Infrequent alignment is a constraint for word learning models, and CLIP offers a new method for investigating children's multimodal environment.

Abstract: Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., "look at the ball" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.

</details>


### [176] [Matching-Based Few-Shot Semantic Segmentation Models Are Interpretable by Design](https://arxiv.org/abs/2511.18163)
*Pasquale De Marinis,Uzay Kaymak,Rogier Brussee,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: First dedicated interpretability method for few-shot semantic segmentation that explains model decisions by identifying which support image pixels contribute most to query predictions.


<details>
  <summary>Details</summary>
Motivation: Few-shot semantic segmentation models lack interpretability despite being critical for understanding model behavior and guiding support set selection in data-scarce scenarios.

Method: Affinity Explainer approach extracts attribution maps using matching scores between support and query features at multiple feature levels to highlight influential support pixels.

Result: Significantly outperforms adapted standard attribution methods on FSS benchmarks, provides structured coherent attention patterns aligned with model architectures, and enables effective model diagnosis.

Conclusion: Establishes foundation for interpretable FSS research, enabling better model understanding and more reliable few-shot segmentation systems.

Abstract: Few-Shot Semantic Segmentation (FSS) models achieve strong performance in segmenting novel classes with minimal labeled examples, yet their decision-making processes remain largely opaque. While explainable AI has advanced significantly in standard computer vision tasks, interpretability in FSS remains virtually unexplored despite its critical importance for understanding model behavior and guiding support set selection in data-scarce scenarios. This paper introduces the first dedicated method for interpreting matching-based FSS models by leveraging their inherent structural properties. Our Affinity Explainer approach extracts attribution maps that highlight which pixels in support images contribute most to query segmentation predictions, using matching scores computed between support and query features at multiple feature levels. We extend standard interpretability evaluation metrics to the FSS domain and propose additional metrics to better capture the practical utility of explanations in few-shot scenarios. Comprehensive experiments on FSS benchmark datasets, using different models, demonstrate that our Affinity Explainer significantly outperforms adapted standard attribution methods. Qualitative analysis reveals that our explanations provide structured, coherent attention patterns that align with model architectures and and enable effective model diagnosis. This work establishes the foundation for interpretable FSS research, enabling better model understanding and diagnostic for more reliable few-shot segmentation systems. The source code is publicly available at https://github.com/pasqualedem/AffinityExplainer.

</details>


### [177] [Nested Unfolding Network for Real-World Concealed Object Segmentation](https://arxiv.org/abs/2511.18164)
*Chunming He,Rihan Zhang,Dingming Zhang,Fengyang Xiao,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: NUN is a nested unfolding network that decouples image restoration from segmentation for real-world concealed object segmentation, using a DUN-in-DUN design with vision-language guidance to handle unknown degradations.


<details>
  <summary>Details</summary>
Motivation: Existing DUN-based methods couple background estimation with image restoration, creating conflicting objectives and requiring pre-defined degradation types that don't match real-world scenarios.

Method: Proposes NUN with DUN-in-DUN design: DeRUN (degradation-resistant unfolding network) embedded within SODUN (segmentation-oriented unfolding network). DeRUN uses VLM to infer degradation semantics and restore images, while SODUN performs reversible foreground-background estimation.

Result: Extensive experiments show NUN achieves leading performance on both clean and degraded benchmarks for concealed object segmentation.

Conclusion: NUN provides a unified framework for real-world COS that effectively handles unknown degradations through decoupled restoration and segmentation with mutual refinement.

Abstract: Deep unfolding networks (DUNs) have recently advanced concealed object segmentation (COS) by modeling segmentation as iterative foreground-background separation. However, existing DUN-based methods (RUN) inherently couple background estimation with image restoration, leading to conflicting objectives and requiring pre-defined degradation types, which are unrealistic in real-world scenarios. To address this, we propose the nested unfolding network (NUN), a unified framework for real-world COS. NUN adopts a DUN-in-DUN design, embedding a degradation-resistant unfolding network (DeRUN) within each stage of a segmentation-oriented unfolding network (SODUN). This design decouples restoration from segmentation while allowing mutual refinement. Guided by a vision-language model (VLM), DeRUN dynamically infers degradation semantics and restores high-quality images without explicit priors, whereas SODUN performs reversible estimation to refine foreground and background. Leveraging the multi-stage nature of unfolding, NUN employs image-quality assessment to select the best DeRUN outputs for subsequent stages, naturally introducing a self-consistency loss that enhances robustness. Extensive experiments show that NUN achieves a leading place on both clean and degraded benchmarks. Code will be released.

</details>


### [178] [EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses](https://arxiv.org/abs/2511.18173)
*Enrico Pallotta,Sina Mokhtarzadeh Azar,Lars Doorenbos,Serdar Ozsoy,Umar Iqbal,Juergen Gall*

Main category: cs.CV

TL;DR: EgoControl is a pose-controllable video diffusion model that generates egocentric videos conditioned on 3D body pose sequences, enabling fine-grained motion control for embodied AI agents.


<details>
  <summary>Details</summary>
Motivation: To enable embodied AI agents that can simulate, predict, and plan actions through fine-grained control of body motion in egocentric video generation.

Method: Proposes EgoControl - a video diffusion model trained on egocentric data that conditions future frame generation on explicit 3D body pose sequences. Introduces a novel pose representation capturing global camera dynamics and articulated body movements, integrated through a dedicated control mechanism in the diffusion process.

Result: Generates temporally coherent and visually realistic future frames that align with provided pose control, producing high-quality, pose-consistent egocentric videos.

Conclusion: EgoControl paves the way toward controllable embodied video simulation and understanding by achieving precise motion control in egocentric video generation.

Abstract: Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.

</details>


### [179] [Unified Spherical Frontend: Learning Rotation-Equivariant Representations of Spherical Images from Any Camera](https://arxiv.org/abs/2511.18174)
*Mukai Yu,Mosam Dabhi,Liuyue Xie,Sebastian Scherer,László A. Jeni*

Main category: cs.CV

TL;DR: USF is a lens-agnostic framework that transforms images from any calibrated camera into a unit-sphere representation and performs spherical operations directly in the spatial domain, avoiding costly spherical harmonic transforms while maintaining rotation-equivariance.


<details>
  <summary>Details</summary>
Motivation: Current perception pipelines use planar CNNs designed for pinhole imagery on wide FoV cameras, causing image-space neighborhoods to misrepresent physical adjacency and making models sensitive to global rotations. Frequency-domain spherical CNNs partially address this but have resolution and efficiency constraints.

Method: USF transforms images via ray-direction correspondences into unit-sphere representation, performs spherical resampling, convolution, and pooling directly in spatial domain with distance-only spherical kernels that offer configurable rotation-equivariance. The framework is modular with decoupled projection, sampling, interpolation, and resolution control.

Result: USF processes high-resolution spherical imagery efficiently, maintains <1% performance drop under random test-time rotations without rotational augmentation, and enables zero-shot generalization from one lens type to unseen wide-FoV lenses with minimal performance degradation.

Conclusion: USF provides an efficient spherical frontend that overcomes limitations of both planar CNNs and frequency-domain spherical CNNs, offering robust performance across various vision tasks while being lens-agnostic and rotation-equivariant.

Abstract: Modern perception increasingly relies on fisheye, panoramic, and other wide field-of-view (FoV) cameras, yet most pipelines still apply planar CNNs designed for pinhole imagery on 2D grids, where image-space neighborhoods misrepresent physical adjacency and models are sensitive to global rotations. Frequency-domain spherical CNNs partially address this mismatch but require costly spherical harmonic transforms that constrain resolution and efficiency. We introduce the Unified Spherical Frontend (USF), a lens-agnostic framework that transforms images from any calibrated camera into a unit-sphere representation via ray-direction correspondences, and performs spherical resampling, convolution, and pooling directly in the spatial domain. USF is modular: projection, location sampling, interpolation, and resolution control are fully decoupled. Its distance-only spherical kernels offer configurable rotation-equivariance (mirroring translation-equivariance in planar CNNs) while avoiding harmonic transforms entirely. We compare standard planar backbones with their spherical counterparts across classification, detection, and segmentation tasks on synthetic (Spherical MNIST) and real-world datasets (PANDORA, Stanford 2D-3D-S), and stress-test robustness to extreme lens distortions, varying FoV, and arbitrary rotations. USF processes high-resolution spherical imagery efficiently and maintains less than 1% performance drop under random test-time rotations, even without rotational augmentation, and even enables zero-shot generalization from one lens type to unseen wide-FoV lenses with minimal performance degradation.

</details>


### [180] [Early Lung Cancer Diagnosis from Virtual Follow-up LDCT Generation via Correlational Autoencoder and Latent Flow Matching](https://arxiv.org/abs/2511.18185)
*Yutong Wu,Yifan Wang,Qining Zhang,Chuan Zhou,Lei Ying*

Main category: cs.CV

TL;DR: Proposes CorrFlowNet, a generative AI method that creates virtual one-year follow-up CT scans from baseline scans to enable early lung cancer diagnosis without waiting for clinical follow-ups.


<details>
  <summary>Details</summary>
Motivation: Early lung cancer diagnosis is critical but challenging due to difficulty distinguishing subtle early malignancy signals from benign conditions. Current AI methods focus on single CT scans, while patients often need multiple follow-ups for definitive diagnosis.

Method: Uses correlational autoencoder to encode baseline and follow-up CT images into latent space capturing nodule progression dynamics, followed by flow matching with neural ODE. Includes auxiliary classifier for enhanced diagnostic accuracy.

Result: Significantly improves downstream lung nodule risk assessment compared to baseline models. Diagnostic accuracy comparable to real clinical CT follow-ups.

Conclusion: CorrFlowNet shows potential to improve cancer diagnosis by enabling early detection through virtual follow-ups, reducing need for waiting for clinical follow-up examinations.

Abstract: Lung cancer is one of the most commonly diagnosed cancers, and early diagnosis is critical because the survival rate declines sharply once the disease progresses to advanced stages. However, achieving an early diagnosis remains challenging, particularly in distinguishing subtle early signals of malignancy from those of benign conditions. In clinical practice, a patient with a high risk may need to undergo an initial baseline and several annual follow-up examinations (e.g., CT scans) before receiving a definitive diagnosis, which can result in missing the optimal treatment. Recently, Artificial Intelligence (AI) methods have been increasingly used for early diagnosis of lung cancer, but most existing algorithms focus on radiomic features extraction from single early-stage CT scans. Inspired by recent advances in diffusion models for image generation, this paper proposes a generative method, named CorrFlowNet, which creates a virtual, one-year follow-up CT scan after the initial baseline scan. This virtual follow-up would allow for an early detection of malignant/benign nodules, reducing the need to wait for clinical follow-ups. During training, our approach employs a correlational autoencoder to encode both early baseline and follow-up CT images into a latent space that captures the dynamics of nodule progression as well as the correlations between them, followed by a flow matching algorithm on the latent space with a neural ordinary differential equation. An auxiliary classifier is used to further enhance the diagnostic accuracy. Evaluations on a real clinical dataset show our method can significantly improve downstream lung nodule risk assessment compared with existing baseline models. Moreover, its diagnostic accuracy is comparable with real clinical CT follow-ups, highlighting its potential to improve cancer diagnosis.

</details>


### [181] [From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation](https://arxiv.org/abs/2511.19149)
*Moazzam Umer Gondal,Hamad Ul Qudous,Daniya Siddiqui,Asma Ahmad Farhan*

Main category: cs.CV

TL;DR: A retrieval-augmented framework combining multi-garment detection, attribute reasoning, and LLM prompting for generating fashion captions and hashtags with better factual grounding than end-to-end approaches.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization in fashion imagery, aiming to produce visually grounded, descriptive, and stylistically interesting text.

Method: Pipeline with YOLO-based detector for multi-garment localization, k-means clustering for color extraction, CLIP-FAISS retrieval for fabric/gender attributes, and LLM prompting using factual evidence pack to generate captions and hashtags.

Result: YOLO detector achieved mAP@0.5 of 0.71 for nine garment categories. RAG-LLM pipeline achieved mean attribute coverage of 0.80 with full coverage at 50% threshold in hashtag generation, outperforming BLIP baseline in factual grounding with less hallucination.

Conclusion: Retrieval-augmented generation is an effective and interpretable paradigm for automated and visually grounded fashion content generation with great potential for scalable deployment across clothing domains.

Abstract: This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.

</details>


### [182] [ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization](https://arxiv.org/abs/2511.18192)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: ARIAL is a modular framework using LLM-based agentic reasoning to achieve both accurate answer extraction and reliable spatial grounding in Document VQA, outperforming previous methods across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Document VQA systems either achieve strong textual accuracy with unreliable spatial grounding or sacrifice performance for interpretability, creating a need for solutions that provide both precise answers and reliable localization for high-stakes applications.

Method: A modular framework that orchestrates specialized tools through an LLM-based planning agent, decomposing Document VQA into structured subtasks: OCR-based text extraction, retrieval-augmented context selection, answer generation via fine-tuned Gemma 3-27B, and explicit bounding-box localization through text-to-region alignment.

Result: State-of-the-art performance across four benchmarks: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA.

Conclusion: Agentic orchestration of specialized tools can simultaneously improve performance and interpretability in Document VQA, providing transparent reasoning traces and enabling trustworthy, explainable document AI systems.

Abstract: Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.

</details>


### [183] [InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity](https://arxiv.org/abs/2511.18200)
*Haoming Wang,Qiyao Xue,Wei Gao*

Main category: cs.CV

TL;DR: InfiniBench is a fully automated benchmark generator that creates diverse 3D scenes for evaluating vision-language models' spatial reasoning abilities, with customizable scene complexity and physical plausibility.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating VLMs' spatial reasoning lack customizability, scalability, and ability to isolate specific failure modes under different spatial conditions.

Method: Uses LLM-based agentic framework for scene constraint refinement, cluster-based layout optimizer for dense scenes, and task-aware camera trajectory optimization for video generation.

Result: Outperforms state-of-the-art methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios, and successfully generates benchmarks for spatial reasoning tasks.

Conclusion: InfiniBench provides a scalable, customizable solution for comprehensively evaluating VLMs' spatial reasoning capabilities across diverse scene complexities.

Abstract: Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.

</details>


### [184] [Generating Synthetic Human Blastocyst Images for In-Vitro Fertilization Blastocyst Grading](https://arxiv.org/abs/2511.18204)
*Pavan Narahari,Suraj Rajendran,Lorena Bori,Jonas E. Malmsten,Qiansheng Zhan,Zev Rosenwaks,Nikica Zaninovic,Iman Hajirasouliha*

Main category: cs.CV

TL;DR: DIA framework uses latent diffusion models to generate high-quality synthetic day 5 blastocyst images with granular control over morphological categories and focal depth, addressing data scarcity and class imbalance in IVF embryo assessment.


<details>
  <summary>Details</summary>
Motivation: Current IVF blastocyst assessment is subjective and inconsistent, while AI models face challenges due to data scarcity, class imbalance, and privacy constraints. Existing generative models have limitations in image quality, training data size, and clinical relevance.

Method: Developed Diffusion Based Imaging Model for Artificial Blastocysts (DIA) - latent diffusion models conditioned on Gardner-based morphological categories and z-axis focal depth. Evaluated using FID, memorization metrics, embryologist Turing test, and downstream classification tasks.

Result: DIA generates realistic images indistinguishable from real ones by embryologists. Synthetic data augmentation significantly improved classification accuracy on imbalanced datasets and provided performance gains even on balanced datasets. Synthetic data could replace up to 40% of real data without significant accuracy loss.

Conclusion: DIA provides a robust solution for mitigating data scarcity and class imbalance in embryo datasets, enabling improved performance, fairness, and standardization of AI embryo assessment tools through novel, high-fidelity synthetic image generation.

Abstract: The success of in vitro fertilization (IVF) at many clinics relies on the accurate morphological assessment of day 5 blastocysts, a process that is often subjective and inconsistent. While artificial intelligence can help standardize this evaluation, models require large, diverse, and balanced datasets, which are often unavailable due to data scarcity, natural class imbalance, and privacy constraints. Existing generative embryo models can mitigate these issues but face several limitations, such as poor image quality, small training datasets, non-robust evaluation, and lack of clinically relevant image generation for effective data augmentation. Here, we present the Diffusion Based Imaging Model for Artificial Blastocysts (DIA) framework, a set of latent diffusion models trained to generate high-fidelity, novel day 5 blastocyst images. Our models provide granular control by conditioning on Gardner-based morphological categories and z-axis focal depth. We rigorously evaluated the models using FID, a memorization metric, an embryologist Turing test, and three downstream classification tasks. Our results show that DIA models generate realistic images that embryologists could not reliably distinguish from real images. Most importantly, we demonstrated clear clinical value. Augmenting an imbalanced dataset with synthetic images significantly improved classification accuracy (p < 0.05). Also, adding synthetic images to an already large, balanced dataset yielded statistically significant performance gains, and synthetic data could replace up to 40% of real data in some cases without a statistically significant loss in accuracy. DIA provides a robust solution for mitigating data scarcity and class imbalance in embryo datasets. By generating novel, high-fidelity, and controllable synthetic images, our models can improve the performance, fairness, and standardization of AI embryo assessment tools.

</details>


### [185] [Large-Scale Pre-training Enables Multimodal AI Differentiation of Radiation Necrosis from Brain Metastasis Progression on Routine MRI](https://arxiv.org/abs/2511.18208)
*Ahmed Gomaa,Annette Schwarz,Ludwig Singer,Arnd Dörfler,Matthias Stefan May,Pluvio Stephan,Ishita Sheth,Juliane Szkitsak,Katharina Breininger,Yixing Huang,Benjamin Frey,Oliver Schnell,Daniel Delev,Roland Coras,Daniel Höfler,Philipp Schubert,Jenny Stritzelberger,Sabine Semrau,Andreas Maier,Dieter H Heiland,Udo S. Gaipl,Andrea Wittig,Rainer Fietkau,Christoph Bert,Stefanie Corradini,Florian Putz*

Main category: cs.CV

TL;DR: Self-supervised learning on large unlabeled brain MRI datasets enables accurate differentiation of radiation necrosis from tumor progression, outperforming supervised methods and radiomics.


<details>
  <summary>Details</summary>
Motivation: Differentiating radiation necrosis from tumor progression after stereotactic radiosurgery is clinically critical but challenging due to limited biopsy-confirmed training data and invasiveness of gold standard histopathology.

Method: Two-phase deep learning: Vision Transformer pre-trained via self-supervised learning on 10,167 unlabeled T1CE MRI sub-volumes, then fine-tuned for classification using multimodal input (T1CE MRI + segmentation masks) on MOLAB dataset.

Result: Self-supervised model achieved AUC 0.916 (same-center) and 0.764 (external), significantly outperforming supervised ViT (AUC 0.624/0.496) and radiomics (AUC 0.807/0.691). Multimodal integration further improved performance (AUC 0.947/0.821).

Conclusion: Large-scale pre-training on unlabeled brain metastases datasets substantially improves AI performance, providing an interpretable, clinically accessible solution using only routine MRI and clinical data.

Abstract: Background: Differentiating radiation necrosis (RN) from tumor progression after stereotactic radiosurgery (SRS) remains a critical challenge in brain metastases. While histopathology represents the gold standard, its invasiveness limits feasibility. Conventional supervised deep learning approaches are constrained by scarce biopsy-confirmed training data. Self-supervised learning (SSL) overcomes this by leveraging the growing availability of large-scale unlabeled brain metastases imaging datasets. Methods: In a two-phase deep learning strategy inspired by the foundation model paradigm, a Vision Transformer (ViT) was pre-trained via SSL on 10,167 unlabeled multi-source T1CE MRI sub-volumes. The pre-trained ViT was then fine-tuned for RN classification using a two-channel input (T1CE MRI and segmentation masks) on the public MOLAB dataset (n=109) using 20% of datasets as same-center held-out test set. External validation was performed on a second-center test cohort (n=28). Results: The self-supervised model achieved an AUC of 0.916 on the same-center test set and 0.764 on the second center test set, surpassing the fully supervised ViT (AUC 0.624/0.496; p=0.001/0.008) and radiomics (AUC 0.807/0.691; p=0.005/0.014). Multimodal integration further improved performance (AUC 0.947/0.821; p=0.073/0.001). Attention map visualizations enabled interpretability showing the model focused on clinically relevant lesion subregions. Conclusion: Large-scale pre-training on increasingly available unlabeled brain metastases datasets substantially improves AI model performance. A two-phase multimodal deep learning strategy achieved high accuracy in differentiating radiation necrosis from tumor progression using only routine T1CE MRI and standard clinical data, providing an interpretable, clinically accessible solution that warrants further validation.

</details>


### [186] [Using MLIR Transform to Design Sliced Convolution Algorithm](https://arxiv.org/abs/2511.18222)
*Victor Ferrari,Marcio Pereira,Lucas Alvarenga,Gustavo Leite,Guido Araujo*

Main category: cs.CV

TL;DR: SConvTransform is an MLIR extension that optimizes 2D convolutions through declarative transformations, achieving up to 60-67% of peak performance on different architectures.


<details>
  <summary>Details</summary>
Motivation: To provide a structured, declarative approach for optimizing 2D convolutions within MLIR's transform dialect, enabling reusable and analyzable transformations.

Method: Uses SConvOp to lower Linalg convolutions into tiled and packed generic operations via Convolution Slicing Analysis, handling edge cases through region splitting and affine map adjustments.

Result: Achieves up to 60% of peak performance on ARM SME and 67% on Intel AVX512 for standard convolution configurations.

Conclusion: The approach successfully combines static shape analysis with structured tiling and packing strategies, with modular design enabling future extensions and optimizations.

Abstract: This paper proposes SConvTransform, a Transform dialect extension that provides operations for optimizing 2D convolutions in MLIR. Its main operation, SConvOp, lowers Linalg convolutions into tiled and packed generic operations through a fully declarative transformation pipeline. The process is guided by a Convolution Slicing Analysis that determines tile sizes and data layout strategies based on input and filter shapes, as well as target architecture parameters. SConvOp handles edge cases by splitting irregular regions and adjusting affine maps where needed. All packing and tiling operations are derived from a parametric set of affine equations, enabling reusable and analyzable transformations. Although functional correctness was the primary goal of this work, the experimental evaluation demonstrates the effectiveness of SConvTransform, achieving good enough performance across different target architectures. Future work will focus on optimizing performance and porting to other target devices. When applied to standard convolution configurations, the generated code achieves up to 60% of peak performance on ARM SME and 67% on Intel AVX512. These results validate the benefit of combining static shape analysis with structured tiling and packing strategies within the MLIR Transform dialect. Furthermore, the modular design of SConvTransform facilitates integration with future extensions, enabling continued optimization of convolution workloads through MLIR's extensible compilation infrastructure.

</details>


### [187] [Parallel qMRI Reconstruction from 4x Accelerated Acquisitions](https://arxiv.org/abs/2511.18232)
*Mingi Kang*

Main category: cs.CV

TL;DR: End-to-end deep learning framework for accelerated MRI reconstruction that jointly estimates coil sensitivity maps and reconstructs images from undersampled k-space data at 4x acceleration, achieving visually smooth results comparable to conventional SENSE.


<details>
  <summary>Details</summary>
Motivation: MRI acquisitions require long scan times, limiting patient throughput and increasing motion artifacts. Accelerated parallel MRI reduces acquisition time but needs robust reconstruction methods. Traditional approaches like SENSE require both undersampled data and pre-computed coil sensitivity maps.

Method: Two-module architecture: Coil Sensitivity Map (CSM) estimation module and U-Net-based MRI reconstruction module. Jointly estimates coil sensitivity maps and reconstructs images from only undersampled k-space measurements at 4x acceleration.

Result: Produces visually smoother reconstructions compared to conventional SENSE output, achieving comparable visual quality despite lower PSNR/SSIM metrics. Evaluated on multi-coil brain MRI data from 10 subjects with 8 echoes each.

Conclusion: Identifies key challenges including spatial misalignment between different acceleration factors and proposes future directions for improved reconstruction quality. The framework shows promise for accelerated MRI without requiring pre-computed coil sensitivity maps.

Abstract: Magnetic Resonance Imaging (MRI) acquisitions require extensive scan times, limiting patient throughput and increasing susceptibility to motion artifacts. Accelerated parallel MRI techniques reduce acquisition time by undersampling k-space data, but require robust reconstruction methods to recover high-quality images. Traditional approaches like SENSE require both undersampled k-space data and pre-computed coil sensitivity maps. We propose an end-to-end deep learning framework that jointly estimates coil sensitivity maps and reconstructs images from only undersampled k-space measurements at 4x acceleration. Our two-module architecture consists of a Coil Sensitivity Map (CSM) estimation module and a U-Net-based MRI reconstruction module. We evaluate our method on multi-coil brain MRI data from 10 subjects with 8 echoes each, using 2x SENSE reconstructions as ground truth. Our approach produces visually smoother reconstructions compared to conventional SENSE output, achieving comparable visual quality despite lower PSNR/SSIM metrics. We identify key challenges including spatial misalignment between different acceleration factors and propose future directions for improved reconstruction quality.

</details>


### [188] [EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning](https://arxiv.org/abs/2511.18242)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: EgoVITA is a reinforcement learning framework that enables multimodal LLMs to reason about intentions and actions from first-person (egocentric) videos through structured planning and verification.


<details>
  <summary>Details</summary>
Motivation: Reasoning about intentions and actions from egocentric videos is challenging due to partial observability, limited field of view, and self-referenced motion, unlike third-person videos.

Method: Uses Group Relative Policy Optimization (GRPO) with alternating stages: egocentric planning (first-person step-by-step action prediction) and exocentric verification (third-person visual/logical consistency checking).

Result: Achieves significant gains: +7.7 on EgoBlind and +4.4 on EgoOrient compared to Qwen2.5-VL-7B baseline, while maintaining strong generalization on exocentric tasks.

Conclusion: EgoVITA enables more coherent and visually grounded reasoning by learning to make plans that are causally predictive of upcoming visual observations.

Abstract: Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion. We introduce $\textbf{EgoVITA}$, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an $\textbf{egocentric planning phase}$, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an $\textbf{exocentric verification phase}$, where it switches to a third-person perspective to check the visual and logical consistency of that plan. Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by $\mathbf{+7.7}$ on EgoBlind and $\mathbf{+4.4}$ on EgoOrient, while maintaining strong generalization on exocentric video tasks.

</details>


### [189] [UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization](https://arxiv.org/abs/2511.18254)
*Siyi Li,Qingwen Zhang,Ishan Khatri,Kyle Vedder,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: UniFlow achieves state-of-the-art LiDAR scene flow performance by cross-dataset training on multiple LiDAR sensors, improving accuracy by 5.1-35.2% on Waymo/nuScenes and 30.1% on unseen datasets.


<details>
  <summary>Details</summary>
Motivation: To learn general motion priors that transfer across diverse LiDAR sensors, challenging conventional wisdom that multi-dataset training hurts performance in LiDAR tasks.

Method: Proposes UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities.

Result: Establishes new SOTA on Waymo (5.1% improvement) and nuScenes (35.2% improvement), and achieves SOTA on unseen datasets like TruckScenes (30.1% improvement over dataset-specific models).

Conclusion: Motion estimation is less sensitive to sensor configuration than other LiDAR tasks, and cross-dataset training significantly benefits scene flow performance across diverse and unseen sensors.

Abstract: LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.

</details>


### [190] [Sequence-Adaptive Video Prediction in Continuous Streams using Diffusion Noise Optimization](https://arxiv.org/abs/2511.18255)
*Sina Mokhtarzadeh Azar,Emad Bahrami,Enrico Pallotta,Gianpiero Francesca,Radu Timofte,Juergen Gall*

Main category: cs.CV

TL;DR: SAVi-DNO adapts diffusion-based video prediction models to continuous video streams by optimizing diffusion noise during inference, improving prediction quality without fine-tuning model parameters.


<details>
  <summary>Details</summary>
Motivation: To leverage continuously arriving training samples in video streams to improve video prediction models, avoiding expensive fine-tuning of large diffusion models.

Method: Refine diffusion noise during inference while keeping model parameters frozen, allowing adaptive determination of suitable sampling noise for continuous video streams.

Result: Improved performance on FVD, SSIM, and PSNR metrics on long videos from Ego4D, OpenDV-YouTube, UCF-101, and SkyTimelapse datasets.

Conclusion: SAVi-DNO effectively adapts diffusion models to continuous video streams through noise optimization, demonstrating enhanced video prediction performance across multiple datasets.

Abstract: In this work, we investigate diffusion-based video prediction models, which forecast future video frames, for continuous video streams. In this context, the models observe continuously new training samples, and we aim to leverage this to improve their predictions. We thus propose an approach that continuously adapts a pre-trained diffusion model to a video stream. Since fine-tuning the parameters of a large diffusion model is too expensive, we refine the diffusion noise during inference while keeping the model parameters frozen, allowing the model to adaptively determine suitable sampling noise. We term the approach Sequence Adaptive Video Prediction with Diffusion Noise Optimization (SAVi-DNO). To validate our approach, we introduce a new evaluation setting on the Ego4D dataset, focusing on simultaneous adaptation and evaluation on long continuous videos. Empirical results demonstrate improved performance based on FVD, SSIM, and PSNR metrics on long videos of Ego4D and OpenDV-YouTube, as well as videos of UCF-101 and SkyTimelapse, showcasing SAVi-DNO's effectiveness.

</details>


### [191] [MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2511.18262)
*Tao Shen,Xin Wan,Taicai Chen,Rui Zhang,Junwen Pan,Dawei Lu,Fanding Lei,Zhilin Lu,Yunfei Yang,Chen Cheng,Qi She,Chang Liu,Zhenbang Sun*

Main category: cs.CV

TL;DR: Mammoth2 is a unified autoregressive-diffusion framework that combines semantic planning with high-fidelity image generation, achieving strong performance in both text-to-image generation and multimodal understanding tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between discrete semantic reasoning and high-fidelity visual synthesis in unified multimodal models, addressing the challenge of integrating understanding and generation within a single framework.

Method: Uses a serial AR-Diffusion design with autoregressive path for semantic modeling and diffusion transformer decoder for image synthesis. Features AR-Diffusion alignment module with multi-layer feature aggregation, unified condition encoding, and in-context conditioning. Trained end-to-end with joint Next-Token Prediction and Flow Matching objectives.

Result: Achieves 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit benchmarks for text-to-image and instruction-based editing, while remaining competitive with understanding-only models on multimodal understanding tasks.

Conclusion: A carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.

Abstract: Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.

</details>


### [192] [SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors](https://arxiv.org/abs/2511.18264)
*Ruijie Fan,Junyan Ye,Huan Chen,Zilong Huang,Xiaolei Wang,Weijia Li*

Main category: cs.CV

TL;DR: SatSAM2 is a zero-shot satellite video tracker that adapts SAM2 foundation model with motion constraints and state management to handle occlusion and improve generalization without scenario-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing satellite video trackers struggle with generalization, require scenario-specific training, and are prone to track loss during occlusion, limiting their practical deployment.

Method: Built on SAM2 foundation model, SatSAM2 introduces Kalman Filter-based Constrained Motion Module (KFCMM) for temporal motion cues and Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability.

Result: Extensive experiments show SatSAM2 outperforms traditional and foundation model-based trackers, achieving 5.84% AUC improvement on OOTB dataset over state-of-the-art methods.

Conclusion: SatSAM2 effectively adapts foundation models to remote sensing domain with motion constraints, demonstrating superior performance and generalization in satellite video tracking.

Abstract: Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.

</details>


### [193] [Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models](https://arxiv.org/abs/2511.18271)
*Tianyang Han,Junhao Su,Junjie Hu,Peizhen Yang,Hengyu Shi,Junfeng Luo,Jialin Gao*

Main category: cs.CV

TL;DR: PicWorld is a benchmark for evaluating text-to-image models' implicit world knowledge and physical causal reasoning through 1,100 prompts across three categories, using a multi-agent evaluator for evidence-based assessment.


<details>
  <summary>Details</summary>
Motivation: Current T2I models often fail on prompts requiring implicit world knowledge, and existing evaluation methods inadequately test knowledge grounding, multi-physics interactions, and evidence-based verification.

Method: Developed PicWorld benchmark with 1,100 prompts across three categories, and created PW-Agent - an evidence-grounded multi-agent evaluator that hierarchically assesses images by decomposing prompts into verifiable visual evidence.

Result: Evaluation of 17 mainstream T2I models revealed universal limitations in implicit world knowledge and physical causal reasoning capabilities across all tested models.

Conclusion: The findings demonstrate the need for reasoning-aware, knowledge-integrative architectures in future text-to-image systems to address current limitations.

Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.

</details>


### [194] [Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation](https://arxiv.org/abs/2511.18272)
*Richard J. Young*

Main category: cs.CV

TL;DR: Vision token masking in VLMs achieves 42.9% PHI reduction for medical OCR, effectively suppressing long-form identifiers but failing on short structured identifiers due to language model contextual inference.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns about protected health information (PHI) exposure when using large vision-language models for optical character recognition in healthcare settings.

Method: Systematic evaluation of seven vision token masking strategies targeting different architectural layers in DeepSeek-OCR, tested on 100 synthetic medical billing statements with perfect ground-truth annotations.

Result: All masking strategies converged to 42.9% PHI reduction - 100% effective for long-form identifiers (names, DOB, addresses) but 0% effective for short structured identifiers (record numbers, SSN, email, account numbers).

Conclusion: Vision-only privacy interventions have fundamental limits; future work should focus on decoder-level fine-tuning and hybrid architectures combining vision masking with NLP post-processing for HIPAA compliance.

Abstract: Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.

</details>


### [195] [Point-to-Point: Sparse Motion Guidance for Controllable Video Editing](https://arxiv.org/abs/2511.18277)
*Yeji Song,Jaehyun Lee,Mijin Koo,JunHoo Lee,Nojun Kwak*

Main category: cs.CV

TL;DR: Point-to-Point method uses anchor tokens as motion representation to preserve motion fidelity in video editing by capturing essential motion patterns through point trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods struggle with balancing edit fidelity and motion preservation, often relying on motion representations that are either overfitted or implicitly defined.

Method: Proposes anchor tokens - a novel motion representation that leverages video diffusion model priors to encode video dynamics through informative point trajectories, which can be flexibly relocated to align with new subjects.

Result: Extensive experiments show anchor tokens enable more controllable and semantically aligned video edits with superior performance in both edit and motion fidelity across diverse scenarios.

Conclusion: Anchor tokens provide an effective motion representation that generalizes well across different video editing scenarios while maintaining high motion and edit fidelity.

Abstract: Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations that are either overfitted to the layout or only implicitly defined. To overcome this limitation, we revisit point-based motion representation. However, identifying meaningful points remains challenging without human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, achieving superior performance in terms of edit and motion fidelity.

</details>


### [196] [Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation](https://arxiv.org/abs/2511.18281)
*Yara Bahram,Melodie Desbos,Mohammadhadi Shateri,Eric Granger*

Main category: cs.CV

TL;DR: Uni-DAD is a single-stage pipeline that unifies distillation and adaptation of diffusion models, enabling fast and high-quality generation for novel domains without the complexity of two-stage training pipelines.


<details>
  <summary>Details</summary>
Motivation: Current approaches for fast and high-quality generation in novel domains require two-stage training (Adapt-then-Distill or Distill-then-Adapt), which adds design complexity and suffers from degraded quality or diversity.

Method: Uni-DAD combines: (1) dual-domain distribution-matching distillation objective that guides the student toward source and target teacher distributions, and (2) multi-head GAN loss for target realism across multiple feature scales. Source domain distillation preserves diverse knowledge while multi-head GAN stabilizes training.

Result: Uni-DAD delivers higher quality than state-of-the-art adaptation methods with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity across few-shot image generation and subject-driven personalization tasks.

Conclusion: Uni-DAD provides an effective single-stage solution for unifying distillation and adaptation of diffusion models, achieving superior performance compared to complex two-stage approaches while maintaining efficiency.

Abstract: Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.

</details>


### [197] [RoadSceneVQA: Benchmarking Visual Question Answering in Roadside Perception Systems for Intelligent Transportation System](https://arxiv.org/abs/2511.18286)
*Runwei Guan,Rongsheng Hu,Shangshu Chen,Ningyuan Xiao,Xue Xia,Jiayang Liu,Beibei Chen,Ziren Tang,Ningwei Ouyang,Shaofeng Liang,Yuxuan Fan,Wanjie Sun,Yutao Yue*

Main category: cs.CV

TL;DR: RoadSceneVQA is a large-scale VQA dataset for roadside scenarios with 34,736 QA pairs, targeting object attributes, intent, legality, and interactions. The proposed RoadMind model with CogniAnchor Fusion and Assisted Decoupled Chain-of-Thought achieves state-of-the-art performance in traffic perception and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current roadside perception systems focus on instance-level perception but lack natural language interaction and contextual reasoning about traffic behaviors, creating a gap in comprehensive traffic scene understanding.

Method: Proposed RoadMind model with CogniAnchor Fusion (vision-language fusion inspired by human scene anchoring) and Assisted Decoupled Chain-of-Thought (enhancing reasoning via CoT prompting and multi-task learning).

Result: Experiments show the pipeline improves reasoning accuracy and computational efficiency, achieving state-of-the-art performance on RoadSceneVQA and CODA-LM benchmarks for structural traffic perception and reasoning tasks.

Conclusion: RoadSceneVQA dataset and RoadMind model successfully bridge the gap between instance-level perception and comprehensive traffic scene understanding, enabling natural language interaction and contextual reasoning in roadside scenarios.

Abstract: Current roadside perception systems mainly focus on instance-level perception, which fall short in enabling interaction via natural language and reasoning about traffic behaviors in context. To bridge this gap, we introduce RoadSceneVQA, a large-scale and richly annotated visual question answering (VQA) dataset specifically tailored for roadside scenarios. The dataset comprises 34,736 diverse QA pairs collected under varying weather, illumination, and traffic conditions, targeting not only object attributes but also the intent, legality, and interaction patterns of traffic participants. RoadSceneVQA challenges models to perform both explicit recognition and implicit commonsense reasoning, grounded in real-world traffic rules and contextual dependencies. To fully exploit the reasoning potential of Multi-modal Large Language Models (MLLMs), we further propose CogniAnchor Fusion (CAF), a vision-language fusion module inspired by human-like scene anchoring mechanisms. Moreover, we propose the Assisted Decoupled Chain-of-Thought (AD-CoT) to enhance the reasoned thinking via CoT prompting and multi-task learning. Based on the above, we propose the baseline model RoadMind. Experiments on RoadSceneVQA and CODA-LM benchmark show that the pipeline consistently improves both reasoning accuracy and computational efficiency, allowing the MLLM to achieve state-of-the-art performance in structural traffic perception and reasoning tasks.

</details>


### [198] [SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes](https://arxiv.org/abs/2511.18290)
*Jungho Lee,Minhyeok Lee,Sunghun Yang,Minseok Kang,Sangyoun Lee*

Main category: cs.CV

TL;DR: SwiftVGGT is a training-free method that achieves high-quality dense 3D reconstruction in large-scale scenes with significantly reduced inference time, requiring only 33% of the time compared to recent VGGT-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods face a trade-off between accuracy and computational efficiency - they either produce low-quality results quickly or achieve high-quality reconstruction slowly.

Method: SwiftVGGT performs loop closure without external VPR models to maintain global consistency, and uses a simple point sampling method with Sim(3)-based SVD alignment instead of IRLS optimization.

Result: The method achieves state-of-the-art reconstruction quality while reducing inference time to only 33% of recent VGGT-based approaches, enabling accurate reconstruction over kilometer-scale environments.

Conclusion: SwiftVGGT successfully addresses the accuracy-efficiency trade-off in large-scale 3D reconstruction through training-free optimization and efficient alignment techniques.

Abstract: 3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.

</details>


### [199] [DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition](https://arxiv.org/abs/2511.18305)
*Raja Kumar,Arka Sadhu,Ram Nevatia*

Main category: cs.CV

TL;DR: DiVE-k is a framework that uses top-k predictions as training signals to improve LVLMs' fine-grained image recognition through differential reasoning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with fine-grained image recognition and existing RL-based fine-tuning methods encourage memorization rather than differential reasoning needed for generalization.

Method: Creates multiple-choice questions from model's top-k predictions and uses RL to train the model to select correct answers, forcing fine-grained differential reasoning.

Result: Outperforms QWEN2.5-VL-7B by 10.04% and ViRFT by 6.16% on Harmonic Mean in base-to-novel generalization, with similar gains in mixed-domain and few-shot scenarios.

Conclusion: DiVE-k effectively improves LVLMs' fine-grained recognition by leveraging top-k predictions for differential reasoning training, enhancing generalization capabilities.

Abstract: Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\textbf{DiVE-k}$, $\textbf{Di}$fferential $\textbf{V}$isual r$\textbf{E}$asoning using top-$\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.

</details>


### [200] [ScriptViT: Vision Transformer-Based Personalized Handwriting Generation](https://arxiv.org/abs/2511.18307)
*Sajjan Acharya,Rajendra Baskota*

Main category: cs.CV

TL;DR: A unified framework for styled handwriting generation that captures global stylistic patterns using Vision Transformer-based style encoder and cross-attention mechanism, with interpretable stroke-level analysis.


<details>
  <summary>Details</summary>
Motivation: Current handwriting generation methods struggle to capture full writer-specific attributes, especially global stylistic patterns with long-range spatial dependencies like consistent slant, curvature, and stroke pressure.

Method: Vision Transformer-based style encoder learns global patterns from multiple references, integrated with target text via cross-attention mechanism, plus Salient Stroke Attention Analysis for interpretability.

Result: The framework produces handwriting that is more stylistically coherent and faithful to the intended style while maintaining text accuracy.

Conclusion: The proposed approach enables better capture of writer-specific traits and provides interpretable stroke-level analysis, leading to improved handwriting synthesis.

Abstract: Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.

</details>


### [201] [Stro-VIGRU: Defining the Vision Recurrent-Based Baseline Model for Brain Stroke Classification](https://arxiv.org/abs/2511.18316)
*Subhajeet Das,Pritam Paul,Rohit Bahadur,Sohan Das*

Main category: cs.CV

TL;DR: A Vision Transformer-based transfer learning framework with Bi-GRU achieves 94.06% accuracy for early brain stroke detection from CT scans.


<details>
  <summary>Details</summary>
Motivation: Stroke is a major cause of death and disability worldwide, and early recognition is crucial for successful treatment. Manual analysis of CT scans can be time-consuming and error-prone, necessitating automated solutions.

Method: Uses pre-trained Vision Transformer with some frozen encoder blocks and fine-tuned others to learn stroke-specific features. Extracted features are fed to a single-layer Bi-GRU for classification, with data augmentation to handle class imbalance.

Result: The model achieved 94.06% accuracy in classifying brain stroke from the Stroke Dataset.

Conclusion: The proposed Vision Transformer-based transfer learning framework with Bi-GRU is effective for automated brain stroke detection from CT scans, providing high accuracy for early diagnosis.

Abstract: Stroke majorly causes death and disability worldwide, and early recognition is one of the key elements of successful treatment of the same. It is common to diagnose strokes using CT scanning, which is fast and readily available, however, manual analysis may take time and may result in mistakes. In this work, a pre-trained Vision Transformer-based transfer learning framework is proposed for the early identification of brain stroke. A few of the encoder blocks of the ViT model are frozen, and the rest are allowed to be fine-tuned in order to learn brain stroke-specific features. The features that have been extracted are given as input to a single-layer Bi-GRU to perform classification. Class imbalance is handled by data augmentation. The model has achieved 94.06% accuracy in classifying brain stroke from the Stroke Dataset.

</details>


### [202] [Optimal Pose Guidance for Stereo Calibration in 3D Deformation Measurement](https://arxiv.org/abs/2511.18317)
*Dongcai Tan,Shunkun Liang,Bin Li,Banglei Guan,Ang Su,Yuan Lin,Dapeng Zhang,Minggang Wan,Zibin Liu,Chenglong Wang,Jiajian Zhu,Zhang Li,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: Proposes an interactive calibration framework with optimal pose guidance for stereo optical measurement systems, improving efficiency and accuracy in 3D deformation measurement.


<details>
  <summary>Details</summary>
Motivation: Current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements.

Method: Pose optimization method with joint optimization of relative and absolute extrinsic parameters, using minimization of covariance matrix trace as loss function. Integrated with user-friendly graphical interface.

Result: Superior efficiency (fewer images required) and accuracy (lower measurement errors) compared to random pose, with robustness across varying FOVs. High agreement with FEA simulations in thermal deformation tests.

Conclusion: The proposed pose guidance method demonstrates significant application potential for high-precision stereo calibration in 3D deformation measurement.

Abstract: Stereo optical measurement techniques, such as digital image correlation (DIC), are widely used in 3D deformation measurement as non-contact, full-field measurement methods, in which stereo calibration is a crucial step. However, current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements. The aim of this study is to develop an interactive calibration framework that automatically generates the next optimal pose, enabling high-accuracy stereo calibration for 3D deformation measurement. We propose a pose optimization method that introduces joint optimization of relative and absolute extrinsic parameters, with the minimization of the covariance matrix trace adopted as the loss function to solve for the next optimal pose. Integrated with this method is a user-friendly graphical interface, which guides even non-expert users to capture qualified calibration images. Our proposed method demonstrates superior efficiency (requiring fewer images) and accuracy (demonstrating lower measurement errors) compared to random pose, while maintaining robustness across varying FOVs. In the thermal deformation measurement tests on an S-shaped specimen, the results exhibit high agreement with finite element analysis (FEA) simulations in both deformation magnitude and evolutionary trends. We present a pose guidance method for high-precision stereo calibration in 3D deformation measurement. The simulation experiments, real-world experiments, and thermal deformation measurement applications all demonstrate the significant application potential of our proposed method in the field of 3D deformation measurement.
  Keywords: Stereo calibration, Optimal pose guidance, 3D deformation measurement, Digital image correlation

</details>


### [203] [General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification](https://arxiv.org/abs/2511.18326)
*Helia Abedini,Saba Rahimi,Reza Vaziri*

Main category: cs.CV

TL;DR: Modern general-purpose CNNs like ConvNeXt-Tiny outperform domain-specific pretrained models for brain tumor classification when only small datasets are available.


<details>
  <summary>Details</summary>
Motivation: To determine whether domain-specific pretraining (medical data) or general-purpose pretraining performs better for brain tumor detection from MRI scans when limited data is available.

Method: Systematic evaluation of three CNN architectures: RadImageNet DenseNet121 (medical-domain pretraining), EfficientNetV2S, and ConvNeXt-Tiny (general-purpose CNNs), all trained and fine-tuned under identical conditions using a limited brain MRI dataset.

Result: ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121 showed poor generalization with lower accuracy and higher loss despite domain-specific pretraining.

Conclusion: Domain-specific pretraining may not generalize well under small-data conditions, while modern general-purpose CNNs pretrained on large-scale datasets offer superior transfer learning performance for medical imaging tasks.

Abstract: Brain tumor detection from MRI scans plays a crucial role in early diagnosis and treatment planning. Deep convolutional neural networks (CNNs) have demonstrated strong performance in medical imaging tasks, particularly when pretrained on large datasets. However, it remains unclear which type of pretrained model performs better when only a small dataset is available: those trained on domain-specific medical data or those pretrained on large general datasets. In this study, we systematically evaluate three pretrained CNN architectures for brain tumor classification: RadImageNet DenseNet121 with medical-domain pretraining, EfficientNetV2S, and ConvNeXt-Tiny, which are modern general-purpose CNNs. All models were trained and fine-tuned under identical conditions using a limited-size brain MRI dataset to ensure a fair comparison. Our results reveal that ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121, despite being pretrained on domain-specific medical data, exhibited poor generalization with lower accuracy and higher loss. These findings suggest that domain-specific pretraining may not generalize well under small-data conditions. In contrast, modern, deeper general-purpose CNNs pretrained on large-scale datasets can offer superior transfer learning performance in specialized medical imaging tasks.

</details>


### [204] [SciPostLayoutTree: A Dataset for Structural Analysis of Scientific Posters](https://arxiv.org/abs/2511.18329)
*Shohei Tanaka,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: The paper introduces SciPostLayoutTree, a dataset of 8,000 annotated scientific posters for structural analysis, and proposes Layout Tree Decoder model to handle spatially challenging relations in poster layouts.


<details>
  <summary>Details</summary>
Motivation: Scientific posters are underexplored in structural analysis research despite their importance in academic communication, with existing research primarily focusing on papers rather than posters.

Method: Created SciPostLayoutTree dataset with reading order and parent-child relations, developed Layout Tree Decoder that incorporates visual features, bounding box features, and uses beam search to predict relations while capturing sequence-level plausibility.

Result: The model improves prediction accuracy for spatially challenging relations (upward, horizontal, long-distance) and establishes a solid baseline for poster structure analysis.

Conclusion: The work addresses the gap in poster structural analysis by providing a comprehensive dataset and effective model, with both dataset and code publicly available for further research.

Abstract: Scientific posters play a vital role in academic communication by presenting ideas through visual summaries. Analyzing reading order and parent-child relations of posters is essential for building structure-aware interfaces that facilitate clear and accurate understanding of research content. Despite their prevalence in academic communication, posters remain underexplored in structural analysis research, which has primarily focused on papers. To address this gap, we constructed SciPostLayoutTree, a dataset of approximately 8,000 posters annotated with reading order and parent-child relations. Compared to an existing structural analysis dataset, SciPostLayoutTree contains more instances of spatially challenging relations, including upward, horizontal, and long-distance relations. As a solution to these challenges, we develop Layout Tree Decoder, which incorporates visual features as well as bounding box features including position and category information. The model also uses beam search to predict relations while capturing sequence-level plausibility. Experimental results demonstrate that our model improves the prediction accuracy for spatially challenging relations and establishes a solid baseline for poster structure analysis. The dataset is publicly available at https://huggingface.co/datasets/omron-sinicx/scipostlayouttree. The code is also publicly available at https://github.com/omron-sinicx/scipostlayouttree.

</details>


### [205] [ConsistCompose: Unified Multimodal Layout Control for Image Composition](https://arxiv.org/abs/2511.18333)
*Xuanke Shi,Boxuan Li,Xiaoyang Han,Zhongang Cai,Lei Yang,Dahua Lin,Quan Wang*

Main category: cs.CV

TL;DR: ConsistCompose is a unified multimodal framework that enables layout-controlled multi-instance image generation by embedding layout coordinates directly into language prompts, achieving improved spatial accuracy while maintaining identity fidelity.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models focus on visual grounding but lack precise compositional control through linguistic-embedded layout-grounded generation (LELG), limiting layout-controllable multi-instance generation capabilities.

Method: The framework uses instance-coordinate binding prompts and coordinate-aware classifier-free guidance to translate linguistic layout cues into spatial control, trained on ConsistCompose3M dataset with 3.4M layout and identity annotations.

Result: Experiments show substantial improvements in spatial accuracy over layout-controlled baselines on COCO-Position and MS-Bench while preserving identity fidelity and competitive multimodal understanding.

Conclusion: ConsistCompose establishes a unified paradigm for layout-controllable multimodal image generation by directly embedding layout coordinates in language prompts within a single generative interface.

Abstract: Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.

</details>


### [206] [A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles](https://arxiv.org/abs/2511.18344)
*Tianyang Xu,Jinjie Gu,Xuefeng Zhu,XiaoJun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: MM-UAV is the first large-scale multi-modal benchmark for UAV tracking, integrating RGB, IR, and event signals with 1,321 synchronized sequences and 2.8M annotated frames. A novel tracking framework with adaptive alignment and fusion modules, plus event-enhanced association, outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Single visual modality tracking often fails in challenging UAV scenarios like low illumination and cluttered backgrounds. Multi-modal tracking is more resilient but has been hindered by the lack of dedicated public datasets.

Method: Created MM-UAV dataset with RGB, IR, and event signals. Developed a multi-modal tracking framework with offset-guided adaptive alignment for sensor mismatch resolution, adaptive dynamic fusion for modality balancing, and event-enhanced association using motion cues for identity maintenance.

Result: The proposed framework consistently outperforms state-of-the-art methods in comprehensive experiments. The dataset contains 1,321 synchronized multi-modal sequences spanning 30+ challenging scenarios with 2.8M annotated frames.

Conclusion: MM-UAV bridges the dataset gap in multi-modal UAV tracking and provides an effective baseline framework. The integration of event modality with motion cues significantly enhances tracking reliability, especially for identity maintenance in complex environments.

Abstract: With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.

</details>


### [207] [FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement](https://arxiv.org/abs/2511.18346)
*Wenshuo Gao,Junyi Fan,Jiangyue Zeng,Shuai Yang*

Main category: cs.CV

TL;DR: FlowPortal is a training-free flow-based video relighting framework that achieves superior temporal consistency, structural preservation, and lighting realism through residual-corrected flow and decoupled condition design.


<details>
  <summary>Details</summary>
Motivation: Existing video relighting methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness, which is critical for film production and creative media applications.

Method: Uses Residual-Corrected Flow mechanism to transform standard flow-based models into editing models, Decoupled Condition Design for precise lighting control, High-Frequency Transfer for detail preservation, and masking strategy to separate foreground relighting from background generation.

Result: Achieves superior performance in temporal coherence, structural preservation, and lighting realism while maintaining high efficiency compared to existing methods.

Conclusion: FlowPortal provides an effective training-free solution for video relighting with background replacement that addresses key challenges in temporal consistency and illumination naturalness.

Abstract: Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.

</details>


### [208] [MagicWand: A Universal Agent for Generation and Evaluation Aligned with User Preference](https://arxiv.org/abs/2511.18352)
*Zitong Xu,Dake Shen,Yaosong Du,Kexiang Hao,Jinghan Huang,Xiande Huang*

Main category: cs.CV

TL;DR: MagicWand is a universal AIGC agent that enhances prompt generation and content evaluation based on user preferences, trained on the UniPrefer-100K dataset and evaluated on UniPreferBench.


<details>
  <summary>Details</summary>
Motivation: Users struggle to obtain AIGC content that aligns with their preferences due to difficulty in crafting detailed prompts and lack of preference retention mechanisms.

Method: Constructed UniPrefer-100K dataset with images, videos, and preference text; developed MagicWand agent for preference-based prompt enhancement, content generation, and evaluation refinement.

Result: MagicWand consistently generates content and evaluations well-aligned with user preferences across diverse scenarios, as demonstrated on UniPreferBench with over 120K annotations.

Conclusion: The proposed approach effectively addresses user preference alignment in AIGC through dataset construction, preference-aware generation, and comprehensive evaluation.

Abstract: Recent advances in AIGC (Artificial Intelligence Generated Content) models have enabled significant progress in image and video generation. However, users still struggle to obtain content that aligns with their preferences due to the difficulty of crafting detailed prompts and the lack of mechanisms to retain their preferences. To address these challenges, we construct \textbf{UniPrefer-100K}, a large-scale dataset comprising images, videos, and associated text that describes the styles users tend to prefer. Based on UniPrefer-100K, we propose \textbf{MagicWand}, a universal generation and evaluation agent that enhances prompts based on user preferences, leverages advanced generation models for high-quality content, and applies preference-aligned evaluation and refinement. In addition, we introduce \textbf{UniPreferBench}, the first large-scale benchmark with over 120K annotations for assessing user preference alignment across diverse AIGC tasks. Experiments on UniPreferBench demonstrate that MagicWand consistently generates content and evaluations that are well aligned with user preferences across a wide range of scenarios.

</details>


### [209] [TRANSPORTER: Transferring Visual Semantics from VLM Manifolds](https://arxiv.org/abs/2511.18359)
*Alexandros Stergiou*

Main category: cs.CV

TL;DR: This paper introduces TRANSPORTER, a model-independent approach that generates videos from vision-language model logits to interpret their internal reasoning processes.


<details>
  <summary>Details</summary>
Motivation: To understand and control how vision-language models reason about complex video scenes, addressing the challenge of interpreting their internal processes despite their ability to handle diverse objects, actions, and scene dynamics.

Method: TRANSPORTER uses optimal transport coupling to map VLM's high-semantic embedding spaces to text-to-video models, where logit scores define embedding directions for conditional video generation.

Result: TRANSPORTER successfully generates videos that reflect caption changes across object attributes, action adverbs, and scene context, providing interpretable visualizations of VLM reasoning.

Conclusion: The logits-to-video (L2V) task offers a novel, fidelity-rich direction for model interpretability that enables visual understanding of how VLMs acquire their answers.

Abstract: How do video understanding models acquire their answers? Although current Vision Language Models (VLMs) reason over complex scenes with diverse objects, action performances, and scene dynamics, understanding and controlling their internal processes remains an open challenge. Motivated by recent advancements in text-to-video (T2V) generative models, this paper introduces a logits-to-video (L2V) task alongside a model-independent approach, TRANSPORTER, to generate videos that capture the underlying rules behind VLMs' predictions. Given the high-visual-fidelity produced by T2V models, TRANSPORTER learns an optimal transport coupling to VLM's high-semantic embedding spaces. In turn, logit scores define embedding directions for conditional video generation. TRANSPORTER generates videos that reflect caption changes over diverse object attributes, action adverbs, and scene context. Quantitative and qualitative evaluations across VLMs demonstrate that L2V can provide a fidelity-rich, novel direction for model interpretability that has not been previously explored.

</details>


### [210] [Alias-free 4D Gaussian Splatting](https://arxiv.org/abs/2511.18367)
*Zilong Chen,Huan-ang Gao,Delin Qu,Haohan Chi,Hao Tang,Kai Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: The paper addresses aliasing artifacts in 4D Gaussian Splatting when adjusting camera parameters, proposing a maximum sampling frequency formulation and scale-adaptive filter to eliminate high-frequency artifacts and reduce redundant Gaussians.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic scene reconstruction methods using Gaussian Splatting suffer from strong artifacts when changing camera focal length or distance due to frequency constraints of 4D Gaussians and Gaussian scale mismatch from 2D dilated filters.

Method: Derived a maximum sampling frequency formulation for 4D Gaussian Splatting and introduced a 4D scale-adaptive filter with scale loss to flexibly regulate sampling frequency.

Result: The approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction, validated through monocular and multi-view video experiments.

Conclusion: The proposed method successfully addresses aliasing issues in 4D Gaussian Splatting by adaptively controlling sampling frequency, improving rendering quality and efficiency in dynamic scene reconstruction.

Abstract: Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/

</details>


### [211] [MimiCAT: Mimic with Correspondence-Aware Cascade-Transformer for Category-Free 3D Pose Transfer](https://arxiv.org/abs/2511.18370)
*Zenghao Chai,Chen Tang,Yongkang Wong,Xulei Yang,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: MimiCAT enables category-free 3D pose transfer across different character types using a cascade-transformer model with soft correspondence learning, overcoming limitations of existing methods restricted to similar structures.


<details>
  <summary>Details</summary>
Motivation: Existing 3D pose transfer methods fail to generalize across different character categories (e.g., humanoid to quadruped) due to structural and transformation diversity, leading to mismatched regions and poor transfer quality.

Method: Proposes MimiCAT, a cascade-transformer model that uses semantic keypoint labels to learn soft correspondence for flexible many-to-many matching across characters, treating pose transfer as conditional generation with shape-conditioned refinement.

Result: Extensive experiments show MimiCAT successfully transfers plausible poses across different characters, significantly outperforming prior methods limited to narrow category transfer.

Conclusion: MimiCAT enables effective category-free 3D pose transfer through soft correspondence learning and cascade-transformer architecture, addressing the challenge of structural diversity across different character types.

Abstract: 3D pose transfer aims to transfer the pose-style of a source mesh to a target character while preserving both the target's geometry and the source's pose characteristic. Existing methods are largely restricted to characters with similar structures and fail to generalize to category-free settings (e.g., transferring a humanoid's pose to a quadruped). The key challenge lies in the structural and transformation diversity inherent in distinct character types, which often leads to mismatched regions and poor transfer quality. To address these issues, we first construct a million-scale pose dataset across hundreds of distinct characters. We further propose MimiCAT, a cascade-transformer model designed for category-free 3D pose transfer. Instead of relying on strict one-to-one correspondence mappings, MimiCAT leverages semantic keypoint labels to learn a novel soft correspondence that enables flexible many-to-many matching across characters. The pose transfer is then formulated as a conditional generation process, in which the source transformations are first projected onto the target through soft correspondence matching and subsequently refined using shape-conditioned representations. Extensive qualitative and quantitative experiments demonstrate that MimiCAT transfers plausible poses across different characters, significantly outperforming prior methods that are limited to narrow category transfer (e.g., humanoid-to-humanoid).

</details>


### [212] [MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models](https://arxiv.org/abs/2511.18373)
*Xiyang Wu,Zongxia Li,Jihui Jin,Guangyao Shi,Gouthaman KV,Vishnu Raj,Nilotpal Sinha,Jingxi Chen,Fan Du,Dinesh Manocha*

Main category: cs.CV

TL;DR: The paper introduces MASS-Bench, a physics-focused video QA benchmark, and MASS, a method to enhance VLMs' physics reasoning by injecting spatial-temporal signals through depth-based 3D encoding and motion tracking.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with physics-driven reasoning involving motion dynamics and spatial interactions, limiting their ability to interpret real/AIGC videos and generate physically consistent content.

Method: MASS method injects spatial-temporal signals into VLM language space via depth-based 3D encoding and visual grounding, coupled with motion tracker for object dynamics, plus reinforcement fine-tuning for cross-modal alignment.

Result: Refined VLMs outperform comparable/larger baselines and prior SoTA by 8.7% and 6.0%, achieving performance comparable to Gemini-2.5-Flash on physics reasoning.

Conclusion: The approach effectively addresses VLMs' physics reasoning limitations through interpretable physical-world context representations and spatial-temporal signal injection.

Abstract: Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.

</details>


### [213] [Synthetic Curriculum Reinforces Compositional Text-to-Image Generation](https://arxiv.org/abs/2511.18378)
*Shijian Wang,Runhao Fu,Siyi Zhao,Qingqin Zhan,Xingjian Wang,Jiarui Jin,Yuan Lu,Hanqian Wu,Cunjian Chen*

Main category: cs.CV

TL;DR: CompGen is a compositional curriculum reinforcement learning framework that improves text-to-image generation by using scene graphs to create difficulty-aware training curricula and progressive optimization through reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Text-to-image generation struggles with compositional synthesis of complex scenes containing multiple objects with diverse attributes and intricate relationships, requiring precise object placement and coherent interactions.

Method: Leverages scene graphs to establish difficulty criteria and develops adaptive Markov Chain Monte Carlo graph sampling for difficulty-aware curriculum data synthesis. Integrates curriculum learning with Group Relative Policy Optimization (GRPO) and investigates different curriculum scheduling strategies.

Result: CompGen exhibits distinct scaling curves with easy-to-hard and Gaussian sampling strategies performing best. Significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models.

Conclusion: The framework effectively improves compositional T2I generation systems by addressing compositional weaknesses through progressive curriculum learning and reinforcement optimization.

Abstract: Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.

</details>


### [214] [RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models](https://arxiv.org/abs/2511.18380)
*Timing Yang,Guoyizhe Wei,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: This paper analyzes Mamba's representational properties in vision tasks, showing it's a low-rank approximation of Softmax Attention, introduces a binary segmentation metric for activation evaluation, and demonstrates strong performance with 78.5% ImageNet accuracy.


<details>
  <summary>Details</summary>
Motivation: Mamba has shown effectiveness in vision tasks but its underlying mechanism remains poorly understood, creating a need to systematically investigate its representational properties and bridge the gap between different attention forms.

Method: Theoretical analysis of Mamba's relationship to Softmax and Linear Attention, introduction of a novel binary segmentation metric for activation map evaluation, and leveraging DINO for self-supervised pretraining to obtain clearer activation maps.

Result: Confirmed Mamba as a low-rank approximation of Softmax Attention, demonstrated its capacity to model long-range dependencies through quantitative metrics, and achieved 78.5% linear probing accuracy on ImageNet with clearer activation maps than supervised approaches.

Conclusion: This work provides valuable insights into Mamba's representational properties, bridges the gap between Softmax and Linear Attention forms, and highlights Mamba's potential for interpretability and strong performance in vision tasks.

Abstract: Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures.

</details>


### [215] [ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access](https://arxiv.org/abs/2511.18382)
*Timing Yang,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: ViMix-14M is a large-scale video-text dataset with 14M pairs that addresses data bottlenecks in text-to-video generation by providing crawl-free access and high-quality captions through multi-source merging, de-duplication, and ground-truth-guided re-captioning.


<details>
  <summary>Details</summary>
Motivation: Address the data bottleneck in open-source text-to-video generation by creating a large, high-quality, easily accessible video-text corpus without the limitations of manual YouTube crawling (link rot, access limits, licensing issues).

Method: Merge diverse open video sources, apply unified de-duplication and quality filtering, and implement a multi-granularity ground-truth-guided re-captioning pipeline to refine descriptions for better alignment with video actions, scenes, and temporal structure.

Result: ViMix-14M dataset shows consistent improvements over counterpart datasets in multimodal retrieval, text-to-video generation, and video question answering tasks.

Conclusion: ViMix-14M removes key barriers for training open-source video foundation models and provides insights for building high-quality, generalizable video-text datasets.

Abstract: Text-to-video generation has surged in interest since Sora, yet open-source models still face a data bottleneck: there is no large, high-quality, easily obtainable video-text corpus. Existing public datasets typically require manual YouTube crawling, which yields low usable volume due to link rot and access limits, and raises licensing uncertainty. This work addresses this challenge by introducing ViMix-14M, a curated multi-source video-text dataset of around 14 million pairs that provides crawl-free, download-ready access and long-form, high-quality captions tightly aligned to video. ViMix-14M is built by merging diverse open video sources, followed by unified de-duplication and quality filtering, and a multi-granularity, ground-truth-guided re-captioning pipeline that refines descriptions to better match actions, scenes, and temporal structure. We evaluate the dataset by multimodal retrieval, text-to-video generation, and video question answering tasks, observing consistent improvements over counterpart datasets. We hope this work can help removing the key barrier to training and fine-tuning open-source video foundation models, and provide insights of building high-quality and generalizable video-text datasets.

</details>


### [216] [Can a Second-View Image Be a Language? Geometric and Semantic Cross-Modal Reasoning for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.18385)
*Chuang Peng,Renshuai Tao,Zhongwei Ren,Xianglong Liu,Yunchao Wei*

Main category: cs.CV

TL;DR: Introduces DualXrayBench benchmark and GSR model for dual-view X-ray prohibited items detection, treating second-view images as language-like modality to improve cross-view reasoning.


<details>
  <summary>Details</summary>
Motivation: Traditional X-ray detection relies on single-view visual modality, but human inspectors use dual-view images in practice. The research explores whether the second view can provide constraints similar to language modality.

Method: Proposes Geometric-Semantic Reasoner (GSR) that jointly learns correspondences between cross-view geometry and cross-modal semantics. Uses GSXray dataset with structured Chain-of-Thought sequences: <top>, <side>, <conclusion>.

Result: GSR achieves significant improvements across all eight X-ray tasks in DualXrayBench benchmark, demonstrating the effectiveness of treating second-view images as language-like modality.

Conclusion: The approach offers a new perspective for real-world X-ray inspection by leveraging dual-view reasoning, with GSR showing superior performance across comprehensive evaluation tasks.

Abstract: Automatic X-ray prohibited items detection is vital for security inspection and has been widely studied. Traditional methods rely on visual modality, often struggling with complex threats. While recent studies incorporate language to guide single-view images, human inspectors typically use dual-view images in practice. This raises the question: can the second view provide constraints similar to a language modality? In this work, we introduce DualXrayBench, the first comprehensive benchmark for X-ray inspection that includes multiple views and modalities. It supports eight tasks designed to test cross-view reasoning. In DualXrayBench, we introduce a caption corpus consisting of 45,613 dual-view image pairs across 12 categories with corresponding captions. Building upon these data, we propose the Geometric (cross-view)-Semantic (cross-modality) Reasoner (GSR), a multimodal model that jointly learns correspondences between cross-view geometry and cross-modal semantics, treating the second-view images as a "language-like modality". To enable this, we construct the GSXray dataset, with structured Chain-of-Thought sequences: <top>, <side>, <conclusion>. Comprehensive evaluations on DualXrayBench demonstrate that GSR achieves significant improvements across all X-ray tasks, offering a new perspective for real-world X-ray inspection.

</details>


### [217] [SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation](https://arxiv.org/abs/2511.18386)
*Peter Siegel,Federico Tombari,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: SegSplat is a feed-forward framework that enables rapid 3D reconstruction with open-vocabulary semantic understanding by using 2D foundation model features and predicting semantic indices for 3D Gaussians.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between fast 3D reconstruction and rich semantic understanding, enabling practical generation of semantically aware 3D environments for robotics, AR, and intelligent systems.

Method: Constructs a compact semantic memory bank from multi-view 2D foundation model features and predicts discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass.

Result: Achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while enabling robust open-set semantic segmentation without per-scene optimization.

Conclusion: Represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments.

Abstract: We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.

</details>


### [218] [Exploring Weak-to-Strong Generalization for CLIP-based Classification](https://arxiv.org/abs/2511.18396)
*Jinhao Li,Sarah M. Erfani,Lei Feng,James Bailey,Feng Liu*

Main category: cs.CV

TL;DR: The paper proposes class prototype learning (CPL) to enhance CLIP-based classification using weak-to-strong generalization, achieving 3.67% improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Aligning large models with user intent is crucial but human supervision becomes impractical as models surpass human knowledge. Weak-to-strong generalization offers a scalable solution by using weaker models to supervise stronger ones.

Method: Proposes class prototype learning (CPL) which learns more representative prototypes for each category in CLIP-based classification, using a simple loss function under weak supervision.

Result: CPL yields robust improvements, particularly when pretraining is limited, achieving 3.67% improvement over strong baseline methods in extensive experiments.

Conclusion: Weak-to-strong generalization is effective for vision-language models, and CPL provides a practical approach to enhance CLIP classification capabilities under limited supervision.

Abstract: Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.

</details>


### [219] [ChineseVideoBench: Benchmarking Multi-modal Large Models for Chinese Video Question Answering](https://arxiv.org/abs/2511.18399)
*Yuxiang Nie,Han Wang,Yongjie Ye,Haiyang Yu,Weitao Jia,Tao Zeng,Hao Feng,Xiang Fei,Yang Li,Xiaohui Lv,Guozhi Tang,Jingqun Tang,Jinghui Lu,Zehui Dai,Jiacong Wang,Dingkang Yang,An-Lan Wang,Can Huang*

Main category: cs.CV

TL;DR: ChineseVideoBench is a new benchmark for evaluating Multimodal Large Language Models on Chinese Video Question Answering, featuring culturally-aware content and comprehensive evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Address the lack of comprehensive, culturally-aware evaluation frameworks for MLLMs in Chinese video analysis, given the growing demand for sophisticated video understanding capabilities.

Method: Developed a benchmark with 8 main classes and 12 sub-classes covering tasks requiring deep video understanding and Chinese linguistic/cultural awareness, with tailored evaluation metrics.

Result: The benchmark presents significant challenges to current MLLMs. Gemini 2.5 Pro achieved highest performance (77.9%), while InternVL-38B was the most competitive open-source model.

Conclusion: ChineseVideoBench successfully fills the gap in evaluation frameworks for Chinese video understanding and demonstrates the current limitations of MLLMs in handling culturally-aware video content.

Abstract: This paper introduces ChineseVideoBench, a pioneering benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in Chinese Video Question Answering. The growing demand for sophisticated video analysis capabilities highlights the critical need for comprehensive, culturally-aware evaluation frameworks. ChineseVideoBench addresses this gap by providing a robust dataset and tailored evaluation metrics, enabling rigorous assessment of state-of-the-art MLLMs on complex Chinese video content. Specifically, ChineseVideoBench comprises 8 main classes and 12 sub-classes, encompassing tasks that demand both deep video understanding and nuanced Chinese linguistic and cultural awareness. Our empirical evaluations reveal that ChineseVideoBench presents a significant challenge to current MLLMs. Among the models assessed, Gemini 2.5 Pro achieves the highest performance with an overall score of 77.9%, while InternVL-38B emerges as the most competitive open-source model.

</details>


### [220] [4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation](https://arxiv.org/abs/2511.18416)
*Haonan Wang,Hanyu Zhou,Haoyue Liu,Luxin Yan*

Main category: cs.CV

TL;DR: 4D-VGGT is a foundation model for dynamic scene geometry estimation that uses divide-and-conquer spatiotemporal representation to address mismatched features between spatial and temporal domains.


<details>
  <summary>Details</summary>
Motivation: Existing methods align spatial and temporal features into a unified latent space, but this suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features.

Method: Proposes 4D-VGGT with three key components: 1) Multi-setting input with adaptive visual grid for arbitrary views/time steps, 2) Multi-level representation with cross-view global fusion (spatial) and cross-time local fusion (temporal), 3) Multi-task prediction with task-specific heads for comprehensive geometry estimation.

Result: The model enhances feature discriminability and application universality for dynamic scenes, verified through extensive experiments on multiple dynamic scene geometry benchmarks.

Conclusion: 4D-VGGT provides an effective unified framework for dynamic scene geometry estimation by addressing the heterogeneous nature of spatial and temporal features through divide-and-conquer representation.

Abstract: We investigate a challenging task of dynamic scene geometry estimation, which requires representing both spatial and temporal features. Typically, existing methods align the two features into a unified latent space to model scene geometry. However, this unified paradigm suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features. In this work, we propose 4D-VGGT, a general foundation model with divide-and-conquer spatiotemporal representation for dynamic scene geometry. Our model is divided into three aspects: 1) Multi-setting input. We design an adaptive visual grid that supports input sequences with arbitrary numbers of views and time steps. 2) Multi-level representation. We propose a cross-view global fusion for spatial representation and a cross-time local fusion for temporal representation. 3) Multi-task prediction. We append multiple task-specific heads to spatiotemporal representations, enabling a comprehensive visual geometry estimation for dynamic scenes. Under this unified framework, these components enhance the feature discriminability and application universality of our model for dynamic scenes. In addition, we integrate multiple geometry datasets to train our model and conduct extensive experiments to verify the effectiveness of our method across various tasks on multiple dynamic scene geometry benchmarks.

</details>


### [221] [NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI](https://arxiv.org/abs/2511.18422)
*Mohammad Jafari Vayeghan,Niloufar Delfan,Mehdi Tale Masouleh,Mansour Parvaresh Rizi,Behzad Moshiri*

Main category: cs.CV

TL;DR: NeuroVascU-Net is a lightweight deep learning model that accurately segments cerebrovascular structures from T1CE MRI scans for neurosurgical planning, achieving high accuracy with significantly fewer parameters than transformer-based models.


<details>
  <summary>Details</summary>
Motivation: Manual 3D segmentation of cerebral vasculature is time-consuming and variable, while existing automated methods sacrifice accuracy for computational efficiency, limiting clinical adoption. There's a gap in methods specifically designed for T1CE MRI in neuro-oncology patients.

Method: Based on dilated U-Net with two specialized modules: Multi-Scale Contextual Feature Fusion (MSC²F) for capturing local/global information via multi-scale dilated convolutions, and Cross-Domain Adaptive Feature Fusion (CDA²F) for dynamic domain-specific feature integration.

Result: Achieved Dice score of 0.8609 and precision of 0.8841 on a dataset of 137 brain tumor biopsy patients, accurately segmenting both major and fine vascular structures with only 12.4M parameters (significantly fewer than transformer models).

Conclusion: NeuroVascU-Net provides an optimal balance of accuracy and efficiency, making it a practical solution for computer-assisted neurosurgical planning in clinical settings.

Abstract: Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.

</details>


### [222] [CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images](https://arxiv.org/abs/2511.18424)
*Avishka Perera,Kumal Hewagamage,Saeedha Nazar,Kavishka Abeywardana,Hasitha Gallella,Ranga Rodrigo,Mohamed Afham*

Main category: cs.CV

TL;DR: CrossJEPA is a cross-modal joint embedding predictive architecture that efficiently transfers knowledge from 2D image foundation models to 3D point clouds, achieving state-of-the-art performance with minimal computational requirements.


<details>
  <summary>Details</summary>
Motivation: Current image-to-point cross-modal learning methods are computationally expensive and slow to train, making them impractical for resource-constrained environments. The Joint-embedding Predictive Architecture (JEPA) shows promise but has been under-explored in cross-modal settings due to misconceptions about masking requirements.

Method: CrossJEPA trains a predictor to infer embeddings of rendered 2D views from 3D point clouds, using cross-domain projection information to purify supervision signals. It employs frozen teacher design with one-time target embedding caching for efficiency.

Result: Achieves state-of-the-art linear probing performance: 94.2% on ModelNet40 and 88.3% on ScanObjectNN, using only 14.1M pretraining parameters (8.5M in point encoder) and about 6 hours of pretraining on a single GPU.

Conclusion: CrossJEPA provides a performant, memory-efficient, and fast-to-train framework for 3D representation learning through knowledge distillation, demonstrating that JEPA-style pretraining can be effectively applied beyond masking in cross-modal settings.

Abstract: Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.

</details>


### [223] [LungX: A Hybrid EfficientNet-Vision Transformer Architecture with Multi-Scale Attention for Accurate Pneumonia Detection](https://arxiv.org/abs/2511.18425)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: LungX is a hybrid AI architecture combining EfficientNet, CBAM attention, and Vision Transformer that achieves state-of-the-art pneumonia detection with 86.5% accuracy and 0.943 AUC on chest X-rays.


<details>
  <summary>Details</summary>
Motivation: Pneumonia is a leading global cause of mortality where timely diagnosis is critical, requiring improved AI detection methods.

Method: Hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for pneumonia detection.

Result: Achieved state-of-the-art performance with 86.5% accuracy and 0.943 AUC on 20,000 chest X-rays from RSNA and CheXpert, representing 6.7% AUC improvement over EfficientNet-B0 baselines.

Conclusion: LungX demonstrates superior lesion localization and future directions include multi-center validation and architectural optimizations targeting 88% accuracy for clinical deployment.

Abstract: Pneumonia remains a leading global cause of mortality where timely diagnosis is critical. We introduce LungX, a novel hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for enhanced pneumonia detection. Evaluated on 20,000 curated chest X-rays from RSNA and CheXpert, LungX achieves state-of-the-art performance (86.5 percent accuracy, 0.943 AUC), representing a 6.7 percent AUC improvement over EfficientNet-B0 baselines. Visual analysis demonstrates superior lesion localization through interpretable attention maps. Future directions include multi-center validation and architectural optimizations targeting 88 percent accuracy for clinical deployment as an AI diagnostic aid.

</details>


### [224] [DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation](https://arxiv.org/abs/2511.18434)
*Yongkun Du,Pinxuan Chen,Xuye Ying,Zhineng Chen*

Main category: cs.CV

TL;DR: DocPTBench is a new benchmark for photographed document parsing and translation that reveals significant performance drops in existing models when dealing with real-world document capture conditions compared to pristine digital documents.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like OmniDocBench and DITrans focus on pristine scanned or digital-born documents, failing to address the challenges of real-world capture conditions such as geometric distortions and photometric variations that occur in photographed documents.

Method: The authors introduce DocPTBench, a comprehensive benchmark comprising over 1,300 high-resolution photographed documents from multiple domains, including eight translation scenarios with meticulously human-verified annotations for both parsing and translation.

Result: Experiments show substantial performance declines when transitioning from digital-born to photographed documents: MLLMs drop 18% in parsing accuracy and 12% in translation, while specialized document parsing models show 25% average decrease in performance.

Conclusion: The significant performance gap highlights the unique challenges of real-world document capture conditions and reveals the limited robustness of existing models, emphasizing the need for better handling of photographed documents.

Abstract: The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at https://github.com/Topdu/DocPTBench.

</details>


### [225] [When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection](https://arxiv.org/abs/2511.18436)
*Hao Shen,Jikang Cheng,Renye Yan,Zhongyuan Wang,Wei Peng,Baojin Huang*

Main category: cs.CV

TL;DR: Proposes Domain-Aware Relative Weighting (DARW) strategy to effectively use generative replay for incremental face forgery detection by distinguishing between domain-safe and domain-risky samples.


<details>
  <summary>Details</summary>
Motivation: Current sample replay methods for incremental forgery detection suffer from low diversity and privacy concerns, while generative replay's feasibility remains unclear due to potential domain boundary issues.

Method: DARW strategy that directly supervises domain-safe samples and applies Relative Separation Loss for domain-risky samples, with dynamic Domain Confusion Score to adjust supervision-confusion balance.

Result: Extensive experiments show DARW consistently improves incremental learning performance for forgery detection across different generative replay settings and reduces adverse effects of domain overlap.

Conclusion: The proposed DARW approach effectively leverages generative replay for incremental forgery detection by addressing domain boundary challenges through adaptive sample weighting.

Abstract: The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.

</details>


### [226] [Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning](https://arxiv.org/abs/2511.18437)
*Chi Zhang,Haibo Qiu,Qiming Zhang,Yufei Xu,Zhixiong Zeng,Siqi Yang,Peng Shi,Lin Ma,Jing Zhang*

Main category: cs.CV

TL;DR: PEARL addresses visual hallucinations in VLMs by anchoring reasoning to verified visual evidence through perceptual checklists and dual-branch reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Vanilla RLVR for VLMs only verifies final textual outputs, ignoring visual perception, which leads to visual hallucinations and reward hacking from reasoning based on flawed perception.

Method: PEARL uses dual-branch perception-reasoning synergy with perception checklists - sets of verifiable sub-questions about visual evidence. It computes perceptual rewards from auxiliary rollouts and uses them as fidelity gates for reasoning updates.

Result: PEARL achieves substantial gains on multimodal reasoning benchmarks, with +9.7% improvement over baseline and +6.6% over GRPO on MathVerse.

Conclusion: Explicitly anchoring multimodal reasoning to verified visual evidence through perceptual checklists effectively mitigates visual hallucinations and enhances reasoning reliability in VLMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.

</details>


### [227] [ReCoGS: Real-time ReColoring for Gaussian Splatting scenes](https://arxiv.org/abs/2511.18441)
*Lorenzo Rutayisire,Nicola Capodieci,Fabio Pellacini*

Main category: cs.CV

TL;DR: A user-friendly pipeline for precise region selection and recoloring in Gaussian Splatting scenes, with real-time interactive performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D editing using Gaussian Splatting often suffer from view inconsistencies, lack of fine-grained control, and high computational demands when using 2D diffusion models.

Method: Developed a pipeline that enables precise selection and recoloring of regions within pre-trained Gaussian Splatting scenes, accompanied by an interactive tool for real-time experimentation.

Result: The method provides precise recoloring capabilities with real-time performance, demonstrated through an interactive tool that allows practical experimentation.

Conclusion: The introduced pipeline successfully addresses the limitations of existing methods by offering fine-grained control and real-time performance for Gaussian Splatting scene recoloring tasks.

Abstract: Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.

</details>


### [228] [SineProject: Machine Unlearning for Stable Vision Language Alignment](https://arxiv.org/abs/2511.18444)
*Arpit Garg,Hemanth Saratchandran,Simon Lucey*

Main category: cs.CV

TL;DR: SineProject improves multimodal LLM unlearning by stabilizing vision-language alignment through sinusoidal parameter modulation, reducing benign query refusals while effectively forgetting targeted unsafe/private information.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods for MLLMs disrupt vision-language alignment, causing models to reject both harmful and benign queries due to ill-conditioned Jacobian in projector networks during unlearning.

Method: Augment frozen projector with sinusoidally modulated trainable parameters to improve Jacobian's spectral conditioning and stabilize cross-modal embeddings throughout unlearning.

Result: Across safety and privacy benchmarks using LLaVA v1.5 7B/13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, achieving state-of-the-art forget-retain trade-offs.

Conclusion: SineProject effectively stabilizes multimodal alignment during unlearning with negligible computational overhead, enabling selective forgetting without disrupting model performance on benign queries.

Abstract: Multimodal Large Language Models (MLLMs) increasingly need to forget specific knowledge such as unsafe or private information without requiring full retraining. However, existing unlearning methods often disrupt vision language alignment, causing models to reject both harmful and benign queries. We trace this failure to the projector network during unlearning, its Jacobian becomes severely illconditioned, leading to unstable optimization and drift in cross modal embeddings. We introduce SineProject, a simple method that augments the frozen projector with sinusoidally modulated trainable parameters, improving the Jacobian's spectral conditioning and stabilizing alignment throughout unlearning. Across standard safety and privacy unlearning benchmarks using LLaVA v1.5 7B and 13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, yielding state of the art forget retain trade offs with negligible computational overhead.

</details>


### [229] [EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs](https://arxiv.org/abs/2511.18448)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventBench is a comprehensive benchmark for evaluating multimodal large language models (MLLMs) in event-based vision, featuring eight diverse task metrics and a large-scale dataset with over one million event-text pairs.


<details>
  <summary>Details</summary>
Motivation: Despite significant advancements in event-based vision with MLLMs, there is a lack of unified benchmarks to comprehensively evaluate their capabilities across different tasks and dimensions.

Method: Developed EventBench with four key innovations: openness (releasing all raw event streams and task instructions), diversity (covering understanding, recognition, and spatial reasoning tasks), integration (pioneering 3D spatial reasoning tasks), and scale (large dataset supporting training and evaluation).

Result: Evaluation of state-of-the-art models (GPT-5, Gemini-2.5 Pro, Qwen2.5-VL, InternVL3, EventGPT) revealed that while event-based MLLMs perform well in event stream understanding, they struggle with fine-grained recognition and spatial reasoning tasks.

Conclusion: EventBench provides a comprehensive evaluation framework that highlights current limitations of event-based MLLMs in fine-grained recognition and spatial reasoning, while offering a standardized benchmark for future model development and comparison.

Abstract: Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.

</details>


### [230] [NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering](https://arxiv.org/abs/2511.18452)
*Loick Chambon,Paul Couairon,Eloi Zablocki,Alexandre Boulch,Nicolas Thome,Matthieu Cord*

Main category: cs.CV

TL;DR: NAF is a zero-shot feature upsampling method that bridges the gap between classical filters and modern learnable upsamplers, achieving state-of-the-art performance across multiple tasks without retraining for different Vision Foundation Models.


<details>
  <summary>Details</summary>
Motivation: Vision Foundation Models produce spatially downsampled representations that are challenging for pixel-level tasks. Existing upsampling methods face a trade-off between fast but fixed classical filters and accurate but VFM-specific learnable upsamplers that require retraining.

Method: Neighborhood Attention Filtering (NAF) learns adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings, guided solely by high-resolution input images. It operates zero-shot without retraining for different VFMs.

Result: NAF outperforms VFM-specific upsamplers and achieves state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Also shows strong performance on image restoration.

Conclusion: NAF is the first VFM-agnostic architecture that bridges the gap between classical filters and modern upsamplers, enabling zero-shot feature upsampling for any Vision Foundation Model while achieving superior performance and efficiency.

Abstract: Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.

</details>


### [231] [RegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading](https://arxiv.org/abs/2511.18454)
*Ming-Jhe Lee*

Main category: cs.CV

TL;DR: RegDeepLab: A dual-branch MTL framework combining semantic segmentation and multi-scale regression for automated embryo fragmentation grading in IVF, addressing explainability and accuracy challenges through decoupled training and range loss.


<details>
  <summary>Details</summary>
Motivation: Manual embryo fragmentation grading in IVF is time-consuming with high inter-observer variability. Existing deep learning solutions lack either visual explainability (regression models) or direct clinical grade translation (segmentation models).

Method: Proposes RegDeepLab with dual-branch MTL integrating DeepLabV3+ segmentation and multi-scale regression head. Uses two-stage decoupled training strategy to address gradient conflict and negative transfer, plus range loss for semi-supervised learning.

Result: End-to-end MTL achieves minimal grading error (MAE=0.046) but compromises segmentation boundaries. Decoupled strategy maintains SOTA segmentation accuracy (Dice=0.729) while providing robust grading predictions.

Conclusion: Presents a clinical auxiliary solution combining high accuracy with visual explainability for embryo fragmentation assessment in IVF.

Abstract: The degree of embryo fragmentation serves as a critical morphological indicator for assessing embryo developmental potential in In Vitro Fertilization (IVF) clinical decision-making. However, current manual grading processes are not only time-consuming but also limited by significant inter-observer variability and efficiency bottlenecks. Although deep learning has demonstrated potential in automated grading in recent years, existing solutions face a significant challenge: pure regression models lack the visual explainability required for clinical practice, while pure segmentation models struggle to directly translate pixel-level masks into precise clinical grades. This study proposes RegDeepLab, a dual-branch Multi-Task Learning (MTL) framework that integrates State-of-the-Art (SOTA) semantic segmentation (DeepLabV3+) with a multi-scale regression head. Addressing the common issues of "Gradient Conflict" and "Negative Transfer" in multi-task training, we propose a "Two-Stage Decoupled Training Strategy." Experimental results demonstrate that while standard end-to-end MTL training can minimize grading error (MAE=0.046) through our designed "Feature Injection" mechanism, it compromises the integrity of segmentation boundaries. In contrast, our decoupled strategy successfully provides robust and high-precision grading predictions while preserving SOTA-level segmentation accuracy (Dice=0.729). Furthermore, we introduce a "Range Loss" to effectively utilize large-scale discrete grading data for semi-supervised learning. This study ultimately presents a dual-module clinical auxiliary solution that combines high accuracy with visual explainability.

</details>


### [232] [Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding](https://arxiv.org/abs/2511.18463)
*Bowei Pu,Chuanbin Liu,Yifan Ge,Peichen Zhou,Yiwei Sun,Zhiyin Lu,Jiankang Wang,Hongtao Xie*

Main category: cs.CV

TL;DR: Video-PLR introduces a loop-based perception paradigm with anti-hallucination rewards to address perception shortcuts and hallucinations in video reasoning LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing Video Reasoning LLMs suffer from perception shortcuts and hallucinations due to flawed single-step perception paradigms that describe videos first then reason, risking insufficient evidence.

Method: Proposes Perception Loop Reasoning (PLR) paradigm with iterative video segment analysis and Factual-Aware Evaluator (FAE) with anti-hallucination rewards trained on AnetHallu-117K dataset.

Result: Achieves state-of-the-art performance in both 3B and 7B parameter scales with best data efficiency, with FAE performing comparably to GPT-4o.

Conclusion: The loop-based paradigm with anti-hallucination rewards effectively addresses perception insufficiency and hallucinations in video reasoning.

Abstract: Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.

</details>


### [233] [Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span](https://arxiv.org/abs/2511.18470)
*Heeseung Yun,Joonil Na,Jaeyeon Kim,Calvin Murdock,Gunhee Kim*

Main category: cs.CV

TL;DR: EgoSpanLift transforms egocentric visual span forecasting from 2D to 3D scenes, using SLAM keypoints and volumetric regions to predict where a person will focus visual perception in 3D space.


<details>
  <summary>Details</summary>
Motivation: Current egocentric research focuses on motion and contact-based interactions, but forecasting human visual perception itself remains underexplored despite its fundamental role in guiding actions and applications in AR/VR and assistive technologies.

Method: EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry, extracts volumetric visual span regions, and combines with 3D U-Net and unidirectional transformers for spatio-temporal fusion to predict future visual span in 3D grids.

Result: Outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization, achieving comparable results when projected back to 2D without additional training. Created benchmark with 364.6K samples.

Conclusion: The approach successfully transforms visual span forecasting to 3D space, demonstrating superior performance in predicting future visual perception focus while maintaining 2D compatibility.

Abstract: People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.

</details>


### [234] [Robust Posterior Diffusion-based Sampling via Adaptive Guidance Scale](https://arxiv.org/abs/2511.18471)
*Liav Hen,Tom Tirer,Raja Giryes,Shady Abu-Hussein*

Main category: cs.CV

TL;DR: AdaPS introduces an adaptive likelihood step-size strategy for diffusion-based inverse problem solving, improving reconstruction quality across imaging tasks without task-specific tuning.


<details>
  <summary>Details</summary>
Motivation: Balancing diffusion prior with data fidelity in inverse problems is challenging - aggressive updates cause artifacts while conservative ones slow convergence.

Method: Developed adaptive likelihood step-size based on agreement between two approximations of intermediate likelihood gradients, adapting to diffusion schedule and stochasticity.

Result: Consistently surpasses existing diffusion baselines in perceptual quality with minimal distortion loss on CelebA-HQ and ImageNet-256 across super-resolution, deblurring tasks.

Conclusion: AdaPS is a hyperparameter-free approach that robustly improves reconstruction quality across diverse imaging tasks without requiring task-specific tuning.

Abstract: Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions. In this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity. The resulting approach, Adaptive Posterior diffusion Sampling (AdaPS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks - including super-resolution, Gaussian deblurring, and motion deblurring - on CelebA-HQ and ImageNet-256 validation sets. AdaPS consistently surpasses existing diffusion-based baselines in perceptual quality with minimal or no loss in distortion, without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.

</details>


### [235] [Uncertainty Quantification in HSI Reconstruction using Physics-Aware Diffusion Priors and Optics-Encoded Measurements](https://arxiv.org/abs/2511.18473)
*Juan Romero,Qiang Fu,Matteo Ravasi,Wolfgang Heidrich*

Main category: cs.CV

TL;DR: HSDiff is a Bayesian framework using diffusion models for hyperspectral image reconstruction that addresses hallucination issues through metameric augmentation and provides uncertainty-aware results.


<details>
  <summary>Details</summary>
Motivation: Current data-driven methods for hyperspectral image reconstruction suffer from hallucination due to limited spectral diversity in datasets, especially when evaluating metamerism phenomena.

Method: Formulates HSI reconstruction as Bayesian inference using unconditionally trained pixel-level diffusion prior and posterior diffusion sampling. Proposes enhanced metameric augmentation with region-based metameric black and partition-of-union spectral upsampling.

Result: HSDiff generates diverse HSI samples consistent with measurements, provides calibrated informative uncertainty, and demonstrates the importance of effective spectral encoding in snapshot hyperspectral imaging.

Conclusion: HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction and highlights the significance of effective spectral encoding.

Abstract: Hyperspectral image reconstruction from a compressed measurement is a highly ill-posed inverse problem. Current data-driven methods suffer from hallucination due to the lack of spectral diversity in existing hyperspectral image datasets, particularly when they are evaluated for the metamerism phenomenon. In this work, we formulate hyperspectral image (HSI) reconstruction as a Bayesian inference problem and propose a framework, HSDiff, that utilizes an unconditionally trained, pixel-level diffusion prior and posterior diffusion sampling to generate diverse HSI samples consistent with the measurements of various hyperspectral image formation models. We propose an enhanced metameric augmentation technique using region-based metameric black and partition-of-union spectral upsampling to expand training with physically valid metameric spectra, strengthening the prior diversity and improving uncertainty calibration. We utilize HSDiff to investigate how the studied forward models shape the posterior distribution and demonstrate that guiding with effective spectral encoding provides calibrated informative uncertainty compared to non-encoded models. Through the lens of the Bayesian framework, HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction. Our results also reiterate the significance of effective spectral encoding in snapshot hyperspectral imaging.

</details>


### [236] [Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression](https://arxiv.org/abs/2511.18504)
*Md Tasnin Tanvir,Soumitra Das,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.CV

TL;DR: The paper proposes two adaptive compression techniques (STTF and ANC) for edge AI vision-language models that dynamically optimize computation based on scene complexity, achieving superior performance with significantly reduced parameters and FLOPs compared to larger models.


<details>
  <summary>Details</summary>
Motivation: Enable real-time vision-language AI on resource-constrained edge devices with limited power and memory by developing adaptive compression methods that go beyond static pruning or uniform scaling approaches.

Method: Two adaptive compression techniques: Sparse Temporal Token Fusion (STTF) that dynamically reuses visual tokens through event-driven change detection, and Adaptive Neural Compression (ANC) that conditionally activates encoder branches via a learned router for fine-grained adaptation to scene complexity.

Result: TinyGPT-STTF (3B parameters) achieves CIDEr 131.2, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer FLOPs. STTF reduces token count by 84% while preserving 95.6% accuracy, and ANC cuts FLOPs by up to 90% in low-motion scenes. Overall improvements of up to 4.4% accuracy and 13x latency reduction.

Conclusion: The proposed adaptive compression techniques enable efficient deployment of capable vision-language models on real-world edge devices by achieving superior performance with significantly reduced computational requirements.

Abstract: The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.

</details>


### [237] [Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives](https://arxiv.org/abs/2511.18507)
*Kai Jiang,Siqi Huang,Xiangyu Chen,Jiawei Shao,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: UNIFIER is a multimodal continual learning method that addresses catastrophic forgetting in MLLMs by decoupling visual information from different scenarios into separate branches and projecting them into the same feature space with consistency constraints.


<details>
  <summary>Details</summary>
Motivation: MLLMs deployed on devices need to continuously adapt to dynamic scenarios (background and perspective variations) in downstream tasks to perform complex visual tasks effectively, but suffer from catastrophic forgetting when facing scenario shifts in real-world data streams.

Method: UNIFIER decouples visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. It imposes consistency constraints on features of each branch to maintain stability of visual representations across scenarios.

Result: Extensive experiments on the MSVQA dataset (encompassing high altitude, underwater, low altitude, and indoor scenarios) show that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.

Conclusion: UNIFIER successfully addresses visual discrepancies while learning different scenarios in multimodal continual learning, demonstrating effective mitigation of catastrophic forgetting and enabling knowledge accumulation across diverse visual perspectives.

Abstract: Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.

</details>


### [238] [LRDUN: A Low-Rank Deep Unfolding Network for Efficient Spectral Compressive Imaging](https://arxiv.org/abs/2511.18513)
*He Huang,Yujun Guo,Wei He*

Main category: cs.CV

TL;DR: Proposes LRDUN, a low-rank deep unfolding network for spectral compressive imaging that integrates low-rank decomposition with sensing models to reduce ill-posedness and computational cost while achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing DUNs operate on full high-dimensional HSIs, leading to computational redundancy and suffering from ill-posed nature of mapping 2D residuals to 3D HSI space.

Method: Develops LRDUN with novel imaging models based on spectral basis and subspace images using low-rank decomposition, implemented via unfolded proximal gradient descent with Generalized Feature Unfolding Mechanism.

Result: Achieves state-of-the-art reconstruction quality with significantly reduced computational cost on both simulated and real datasets.

Conclusion: LRDUN effectively mitigates ill-posedness in spectral compressive imaging through low-rank decomposition and provides superior performance with reduced computational requirements.

Abstract: Deep unfolding networks (DUNs) have achieved remarkable success and become the mainstream paradigm for spectral compressive imaging (SCI) reconstruction. Existing DUNs are derived from full-HSI imaging models, where each stage operates directly on the high-dimensional HSI, refining the entire data cube based on the single 2D coded measurement. However, this paradigm leads to computational redundancy and suffers from the ill-posed nature of mapping 2D residuals back to 3D space of HSI. In this paper, we propose two novel imaging models corresponding to the spectral basis and subspace image by explicitly integrating low-rank (LR) decomposition with the sensing model. Compared to recovering the full HSI, estimating these compact low-dimensional components significantly mitigates the ill-posedness. Building upon these novel models, we develop the Low-Rank Deep Unfolding Network (LRDUN), which jointly solves the two subproblems within an unfolded proximal gradient descent (PGD) framework. Furthermore, we introduce a Generalized Feature Unfolding Mechanism (GFUM) that decouples the physical rank in the data-fidelity term from the feature dimensionality in the prior module, enhancing the representational capacity and flexibility of the network. Extensive experiments on simulated and real datasets demonstrate that the proposed LRDUN achieves state-of-the-art (SOTA) reconstruction quality with significantly reduced computational cost.

</details>


### [239] [Unified Deep Learning Platform for Dust and Fault Diagnosis in Solar Panels Using Thermal and Visual Imaging](https://arxiv.org/abs/2511.18514)
*Abishek Karthik,Sreya Mynampati,Pandiyaraju V*

Main category: cs.CV

TL;DR: A centralized platform for detecting dust and faults on solar panels using image analysis and thermal imaging with CNN and ResNet models, achieving better efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Solar panel output varies due to environmental factors like dust, debris, and faults, requiring efficient detection methods for maintenance across different geographical conditions and scales.

Method: Uses image preprocessing (gamma removal, Gaussian filtering, normalization) and thermal imaging with CNN, ResNet, and KerNet models with self-attention mechanisms for classification of dust and fault detection.

Result: The model demonstrates better efficiency and accuracy compared to existing models in detecting dust and various faults on solar panels.

Conclusion: The multi-application centralized platform proves efficient and optimized for solar panel maintenance across different scales and geographical conditions.

Abstract: Solar energy is one of the most abundant and tapped sources of renewable energies with enormous future potential. Solar panel output can vary widely with factors like intensity, temperature, dirt, debris and so on affecting it. We have implemented a model on detecting dust and fault on solar panels. These two applications are centralized as a single-platform and can be utilized for routine-maintenance and any other checks. These are checked against various parameters such as power output, sinusoidal wave (I-V component of solar cell), voltage across each solar cell and others. Firstly, we filter and preprocess the obtained images using gamma removal and Gaussian filtering methods alongside some predefined processes like normalization. The first application is to detect whether a solar cell is dusty or not based on various pre-determined metrics like shadowing, leaf, droppings, air pollution and from other human activities to extent of fine-granular solar modules. The other one is detecting faults and other such occurrences on solar panels like faults, cracks, cell malfunction using thermal imaging application. This centralized platform can be vital since solar panels have different efficiency across different geography (air and heat affect) and can also be utilized for small-scale house requirements to large-scale solar farm sustentation effectively. It incorporates CNN, ResNet models that with self-attention mechanisms-KerNet model which are used for classification and results in a fine-tuned system that detects dust or any fault occurring. Thus, this multi-application model proves to be efficient and optimized in detecting dust and faults on solar panels. We have performed various comparisons and findings that demonstrates that our model has better efficiency and accuracy results overall than existing models.

</details>


### [240] [Breaking Forgetting: Training-Free Few-Shot Class-Incremental Learning via Conditional Diffusion](https://arxiv.org/abs/2511.18516)
*Haidong Kang,Ketong Qian,Yi Lu*

Main category: cs.CV

TL;DR: Proposes CD-FSCIL, a training-free framework for Few-Shot Class-Incremental Learning that replaces gradient optimization with conditional diffusion processes and multimodal learning with LLM-generated descriptions.


<details>
  <summary>Details</summary>
Motivation: To overcome catastrophic forgetting and training cost explosion in FSCIL caused by gradient-based optimization under extreme data scarcity, and to enable training-free continual adaptation.

Method: Uses Conditional Diffusion process instead of gradient updates, integrates multimodal learning with LLM-generated language descriptions to enhance few-shot representations, and eliminates gradient optimization entirely.

Result: Achieves state-of-the-art performance on FSCIL benchmarks while drastically reducing computational and memory overhead compared to gradient-based methods.

Conclusion: Demonstrates a paradigm shift toward training-free continual adaptation in FSCIL, effectively mitigating catastrophic forgetting and computational costs through diffusion-based generative transitions and multimodal learning.

Abstract: Efforts to overcome catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) have primarily focused on developing more effective gradient-based optimization strategies. In contrast, little attention has been paid to the training cost explosion that inevitably arises as the number of novel classes increases, a consequence of relying on gradient learning even under extreme data scarcity. More critically, since FSCIL typically provides only a few samples for each new class, gradient-based updates not only induce severe catastrophic forgetting on base classes but also hinder adaptation to novel ones. This paper seeks to break this long-standing limitation by asking: Can we design a training-free FSCIL paradigm that entirely removes gradient optimization? We provide an affirmative answer by uncovering an intriguing connection between gradient-based optimization and the Conditional Diffusion process. Building on this observation, we propose a Conditional Diffusion-driven FSCIL (CD-FSCIL) framework that substitutes the conventional gradient update process with a diffusion-based generative transition, enabling training-free incremental adaptation while effectively mitigating forgetting. Furthermore, to enhance representation under few-shot constraints, we introduce a multimodal learning strategy that integrates visual features with natural language descriptions automatically generated by Large Language Models (LLMs). This synergy substantially alleviates the sample scarcity issue and improves generalization across novel classes. Extensive experiments on mainstream FSCIL benchmarks demonstrate that our method not only achieves state-of-the-art performance but also drastically reduces computational and memory overhead, marking a paradigm shift toward training-free continual adaptation.

</details>


### [241] [DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation](https://arxiv.org/abs/2511.18533)
*Md Mizanur Rahman Mustakim,Jianwu Li,Sumya Bhuiyan,Mohammad Mehedi Hasan,Bing Han*

Main category: cs.CV

TL;DR: DE-KAN: A dual encoder network using Kolmogorov Arnold Networks for tooth segmentation in panoramic radiographs, achieving state-of-the-art performance with up to 4.7% improvement in Dice coefficient.


<details>
  <summary>Details</summary>
Motivation: Accurate tooth segmentation is challenging due to anatomical variations, irregular shapes, and overlapping structures in panoramic radiographs, which limit conventional deep learning models.

Method: Proposes DE-KAN with dual encoders (ResNet-18 for augmented inputs and custom CNN for original inputs) to extract complementary global and local features, fused through KAN-based bottleneck layers with nonlinear learnable activation functions.

Result: Achieved mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36% on two benchmark datasets, outperforming state-of-the-art methods with up to +4.7% improvement in Dice.

Conclusion: DE-KAN effectively enhances feature representation and segmentation precision for dental X-rays through its dual encoder architecture and KAN-based fusion, demonstrating superior performance over existing segmentation models.

Abstract: Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.

</details>


### [242] [HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2511.18534)
*Pengcheng Fang,Hongli Chen,Guangzhen Yao,Jian Shi,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: cs.CV

TL;DR: HiFi-MambaV2 is a hierarchical MoE Mamba architecture for MRI reconstruction that combines frequency decomposition with adaptive computation, outperforming CNN, Transformer, and prior Mamba methods across multiple datasets and acceleration factors.


<details>
  <summary>Details</summary>
Motivation: To reconstruct high-fidelity MR images from undersampled k-space data by recovering high-frequency details while maintaining anatomical coherence, addressing limitations of existing methods.

Method: Uses hierarchical shared-routed MoE Mamba architecture with separable frequency-consistent Laplacian pyramid for alias-resistant frequency streams and per-pixel top-1 sparse dispatch to shared experts, fused with global context path and data-consistency regularization.

Result: Consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across fastMRI, CC359, ACDC, M4Raw, and Prostate158 datasets for single- and multi-coil settings at multiple acceleration factors.

Conclusion: HiFi-MambaV2 enables reliable and robust MRI reconstruction with improved high-frequency detail and structural fidelity, demonstrating the effectiveness of the proposed hierarchical MoE Mamba approach.

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction.

</details>


### [243] [Zero-Shot Video Deraining with Video Diffusion Models](https://arxiv.org/abs/2511.18537)
*Tuomas Varanka,Juan Luis Gonzalez,Hyeongwoo Kim,Pablo Garrido,Xu Yao*

Main category: cs.CV

TL;DR: First zero-shot video deraining method for dynamic scenes using pretrained text-to-video diffusion models without synthetic data or fine-tuning, leveraging negative prompting and attention switching.


<details>
  <summary>Details</summary>
Motivation: Existing methods require paired datasets (synthetic or static camera) which limit real-world generalization, and diffusion model fine-tuning weakens generative priors.

Method: Invert input video into diffusion model latent space, use negative prompting to push away from rain concepts, and employ attention switching mechanism to maintain background dynamics and structural consistency.

Result: Substantial improvements over prior methods on real-world rain datasets, demonstrating robust generalization without supervised training.

Conclusion: Proposed zero-shot approach effectively removes rain from dynamic scenes while preserving background motion and structural details, outperforming existing supervised methods.

Abstract: Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion. Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases. In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities. By inverting an input video into the latent space of diffusion models, its reconstruction process can be intervened and pushed away from the model's concept of rain using negative prompting. At the core of our approach is an attention switching mechanism that we found is crucial for maintaining dynamic backgrounds as well as structural consistency between the input and the derained video, mitigating artifacts introduced by naive negative prompting. Our approach is validated through extensive experiments on real-world rain datasets, demonstrating substantial improvements over prior methods and showcasing robust generalization without the need for supervised training.

</details>


### [244] [C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction](https://arxiv.org/abs/2511.18559)
*Kuan Wei Huang,Brandon Li,Bharath Hariharan,Noah Snavely*

Main category: cs.CV

TL;DR: This paper introduces C3, a new dataset for cross-modal geometric reasoning between ground-level photos and floor plans, addressing limitations in existing datasets and showing significant improvements in correspondence prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Current geometric models fail when inputs are from vastly different viewpoints or modalities than training data, particularly in challenging scenarios like predicting correspondences between ground-level photos and floor plans where existing datasets are limited.

Method: Created a new dataset (C3) by reconstructing scenes in 3D from Internet photos via structure-from-motion, then manually registering reconstructions to floor plans gathered online to derive correspondences between images and floor plans.

Result: C3 contains 90K paired floor plans and photos across 597 scenes with 153M pixel-level correspondences and 85K camera poses. Training on this data improved the best performing method by 34% in RMSE.

Conclusion: The paper identifies open challenges in cross-modal geometric reasoning and provides a valuable dataset to help address these challenges, showing that state-of-the-art correspondence models struggle with this task but can be significantly improved with appropriate training data.

Abstract: Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos. However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs. ground) or modalities (e.g., photos vs. abstract drawings) compared to what was observed during training. This paper addresses a challenging version of this problem: predicting correspondences between ground-level photos and floor plans. Current datasets for joint photo--floor plan reasoning are limited, either lacking in varying modalities (VIGOR) or lacking in correspondences (WAFFLE). To address these limitations, we introduce a new dataset, C3, created by first reconstructing a number of scenes in 3D from Internet photo collections via structure-from-motion, then manually registering the reconstructions to floor plans gathered from the Internet, from which we can derive correspondence between images and floor plans. C3 contains 90K paired floor plans and photos across 597 scenes with 153M pixel-level correspondences and 85K camera poses. We find that state-of-the-art correspondence models struggle on this task. By training on our new data, we can improve on the best performing method by 34% in RMSE. We also identify open challenges in cross-modal geometric reasoning that our dataset aims to help address.

</details>


### [245] [PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation](https://arxiv.org/abs/2511.18570)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Dinesh Manocha*

Main category: cs.CV

TL;DR: PhysGS extends 3D Gaussian Splatting with Bayesian inference to estimate dense physical properties like mass, hardness, and friction from visual cues, while modeling uncertainties.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods focus only on geometry and appearance, lacking the ability to infer essential physical properties needed for safe robot interaction with environments.

Method: Bayesian-inferred extension of 3D Gaussian Splatting that formulates property estimation as Bayesian inference over Gaussian splats, iteratively refining material and property beliefs with new observations while modeling aleatoric and epistemic uncertainties.

Result: Improves mass estimation accuracy by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines across object-scale, indoor, and outdoor datasets.

Conclusion: PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single spatially continuous framework for dense physical property estimation.

Abstract: Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.

</details>


### [246] [Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation](https://arxiv.org/abs/2511.18591)
*Wei Dong,Han Zhou,Junwei Lin,Jun Chen*

Main category: cs.CV

TL;DR: A generative framework using visual autoregressive modeling with VLM guidance for unsupervised dark image restoration, addressing illumination, noise, and blur through adaptive curve estimation and phase-domain modulation.


<details>
  <summary>Details</summary>
Motivation: Real-world dark images suffer from low visibility, complex noise, and blur, while existing methods rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization.

Method: Proposes VAR modeling guided by VLM priors, with adaptive curve estimation for illumination modulation, SF-RoPE for blur structure modeling, and recursive phase-domain modulation for artifact mitigation.

Result: The framework achieves state-of-the-art performance on benchmark datasets without requiring paired training data.

Conclusion: The proposed unsupervised generative framework effectively restores dark images by leveraging VLM guidance and specialized modules for illumination and blur challenges.

Abstract: Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.

</details>


### [247] [Stage-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI](https://arxiv.org/abs/2511.18595)
*Wenhao Guo,Golrokh Mirzaei*

Main category: cs.CV

TL;DR: Deep learning models for distinguishing true tumor progression from pseudoprogression in glioblastoma show comparable accuracy (~70-74%) across early follow-up stages, with performance improving at later time points. Mamba+CNN hybrids offer the best accuracy-efficiency trade-off.


<details>
  <summary>Details</summary>
Motivation: Differentiating true tumor progression from treatment-related pseudoprogression in glioblastoma is challenging, especially at early follow-up, requiring better diagnostic tools.

Method: Eleven deep learning model families (CNNs, LSTMs, hybrids, transformers, selective state-space models) were trained using a unified pipeline with patient-level cross-validation on the Burdenko GBM Progression cohort (n=180), analyzing different post-RT scans independently.

Result: Accuracies were comparable across stages (~0.70-0.74), but discrimination improved at second follow-up with increased F1 and AUC. Mamba+CNN hybrids performed best for accuracy-efficiency, transformers had competitive AUC but high computational cost, while lightweight CNNs were efficient but less reliable.

Conclusion: Results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts to improve discrimination of tumor progression vs pseudoprogression.

Abstract: Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.

</details>


### [248] [NeAR: Coupled Neural Asset-Renderer Stack](https://arxiv.org/abs/2511.18600)
*Hong Li,Chongjie Ye,Houyuan Chen,Weiqing Xiao,Ziyang Yan,Lixing Xiao,Zhaoxi Chen,Jianfeng Xiang,Shaocong Xu,Xuhui Liu,Yikai Wang,Baochang Zhang,Xiaoguang Han,Jiaolong Yang,Hao Zhao*

Main category: cs.CV

TL;DR: NeAR introduces a coupled neural asset-renderer stack that jointly designs asset representation and neural rendering, enabling end-to-end learnable graphics with improved fidelity, consistency, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current neural asset authoring and neural rendering are disjoint fields. The paper argues that coupling them can unlock benefits in fidelity, consistency, and efficiency for an end-to-end learnable graphics stack.

Method: Uses Trellis-style Structured 3D Latents to create lighting-homogenized neural assets from casually lit inputs via rectified-flow backbone, combined with a lighting-aware neural renderer that uses explicit view embeddings and HDR environment maps.

Result: NeAR outperforms state-of-the-art baselines in quantitative metrics and perceptual quality across four tasks: G-buffer-based forward rendering, random-lit single-image reconstruction, unknown-lit single-image relighting, and novel-view relighting.

Conclusion: The coupled asset-renderer perspective should inspire future graphics stacks to view neural assets and renderers as co-designed components rather than independent entities.

Abstract: Neural asset authoring and neural rendering have emerged as fundamentally disjoint threads: one generates digital assets using neural networks for traditional graphics pipelines, while the other develops neural renderers that map conventional assets to images. However, the potential of jointly designing the asset representation and renderer remains largely unexplored. We argue that coupling them can unlock an end-to-end learnable graphics stack with benefits in fidelity, consistency, and efficiency. In this paper, we explore this possibility with NeAR: a Coupled Neural Asset-Renderer Stack. On the asset side, we build on Trellis-style Structured 3D Latents and introduce a lighting-homogenized neural asset: from a casually lit input, a rectified-flow backbone predicts a Lighting-Homogenized SLAT that encodes geometry and intrinsic material cues in a compact, view-agnostic latent. On the renderer side, we design a lighting-aware neural renderer that uses this neural asset, along with explicit view embeddings and HDR environment maps, to achieve real-time, relightable rendering. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit single-image reconstruction, (3) unknown-lit single-image relighting, and (4) novel-view relighting. Our coupled stack surpasses state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.

</details>


### [249] [RigAnyFace: Scaling Neural Facial Mesh Auto-Rigging with Unlabeled Data](https://arxiv.org/abs/2511.18601)
*Wenchao Ma,Dario Kneubuehler,Maurice Chu,Ian Sachs,Haomiao Jiang,Sharon Xiaolei Huang*

Main category: cs.CV

TL;DR: RAF is a neural auto-rigging framework that creates expressive blendshape rigs from static facial meshes, supporting diverse topologies including multiple disconnected components like eyeballs, with enhanced generalization through 2D supervision.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of rigging facial meshes with diverse topologies, including those with multiple disconnected components, while overcoming the limitations of small manually-rigged datasets that constrain generalization.

Method: Uses a triangulation-agnostic surface learning network conditioned on FACS parameters, with tailored architecture for disconnected components, and employs 2D supervision strategy for unlabeled neutral meshes to enhance data diversity and model generalization.

Result: RAF outperforms previous works in accuracy and generalizability, successfully rigging diverse topologies including artist-crafted assets and in-the-wild samples, while supporting multiple disconnected components for detailed expression animation.

Conclusion: RAF provides a scalable solution for facial auto-rigging that handles diverse mesh topologies and disconnected components, demonstrating superior performance and generalization capabilities compared to existing methods.

Abstract: In this paper, we present RigAnyFace (RAF), a scalable neural auto-rigging framework for facial meshes of diverse topologies, including those with multiple disconnected components. RAF deforms a static neutral facial mesh into industry-standard FACS poses to form an expressive blendshape rig. Deformations are predicted by a triangulation-agnostic surface learning network augmented with our tailored architecture design to condition on FACS parameters and efficiently process disconnected components. For training, we curated a dataset of facial meshes, with a subset meticulously rigged by professional artists to serve as accurate 3D ground truth for deformation supervision. Due to the high cost of manual rigging, this subset is limited in size, constraining the generalization ability of models trained exclusively on it. To address this, we design a 2D supervision strategy for unlabeled neutral meshes without rigs. This strategy increases data diversity and allows for scaled training, thereby enhancing the generalization ability of models trained on this augmented data. Extensive experiments demonstrate that RAF is able to rig meshes of diverse topologies on not only our artist-crafted assets but also in-the-wild samples, outperforming previous works in accuracy and generalizability. Moreover, our method advances beyond prior work by supporting multiple disconnected components, such as eyeballs, for more detailed expression animation. Project page: https://wenchao-m.github.io/RigAnyFace.github.io

</details>


### [250] [Functional Localization Enforced Deep Anomaly Detection Using Fundus Images](https://arxiv.org/abs/2511.18627)
*Jan Benedikt Ruhland,Thorsten Papenbrock,Jan-Peter Sowa,Ali Canbay,Nicole Eter,Bernd Freisleben,Dominik Heider*

Main category: cs.CV

TL;DR: ViT classifier with geometric augmentation achieved state-of-the-art performance (AUC 0.91) for retinal disease detection across multiple datasets, with complementary GANomaly-based anomaly detection providing explainability.


<details>
  <summary>Details</summary>
Motivation: Address challenges in retinal disease detection including imaging quality variability, subtle early-stage manifestations, and domain shift across datasets.

Method: Systematically evaluated Vision Transformer (ViT) classifier with multiple augmentation/enhancement strategies across heterogeneous datasets, plus GANomaly-based anomaly detector with probabilistic calibration.

Result: ViT achieved accuracies 0.789-0.843 across datasets; geometric and color augmentations most effective; outperformed convolutional baselines (AUC 0.91 vs 0.87); GANomaly achieved AUC 0.76 with explainability.

Conclusion: Transformer architectures with multi-dataset training provide superior retinal disease detection, with geometric augmentation being most beneficial, while complementary anomaly detection offers clinical explainability.

Abstract: Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.
  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.

</details>


### [251] [Health system learning achieves generalist neuroimaging models](https://arxiv.org/abs/2511.18640)
*Akhil Kondepudi,Akshay Rao,Chenhui Zhao,Yiwei Lyu,Samir Harake,Soumyanil Banerjee,Rushikesh Joshi,Anna-Katharina Meissner,Renly Hou,Cheng Jiang,Asadur Chowdury,Ashok Srinivasan,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: NeuroVFM is a visual foundation model trained on 5.24M clinical MRI/CT scans that outperforms frontier AI models in neuroimaging tasks through health system learning from private clinical data.


<details>
  <summary>Details</summary>
Motivation: Frontier AI models underperform in neuroimaging due to lack of access to private clinical data, as public datasets are limited by identifiable facial features in medical scans.

Method: Health system learning paradigm using scalable volumetric joint-embedding predictive architecture on 5.24 million clinical MRI and CT volumes from routine care.

Result: State-of-the-art performance across clinical tasks including diagnosis and report generation, with emergent neuroanatomic understanding and reduced hallucinations compared to frontier models.

Conclusion: Health system learning enables high-performance medical AI, with NeuroVFM providing safer clinical decision support and establishing a scalable framework for clinical foundation models.

Abstract: Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.

</details>


### [252] [From Healthy Scans to Annotated Tumors: A Tumor Fabrication Framework for 3D Brain MRI Synthesis](https://arxiv.org/abs/2511.18654)
*Nayu Dong,Townim Chowdhury,Hieu Phan,Mark Jenkinson,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: TF is a two-stage framework for unpaired 3D brain tumor synthesis that uses healthy scans and limited annotated data to generate synthetic tumor data, improving segmentation performance in low-data settings.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of annotated MRI tumor data which hinders accurate automated tumor segmentation, overcoming limitations of manual modeling and deep generative models that require large training datasets.

Method: Two-stage framework: coarse tumor synthesis followed by refinement using generative models, leveraging healthy image scans and limited real annotated data to synthesize paired synthetic data.

Result: Synthetic image-label pairs significantly improve performance on downstream tumor segmentation tasks in low-data regimes.

Conclusion: TF offers a scalable and reliable solution for medical image enrichment, addressing critical challenges in data scarcity for clinical AI applications.

Abstract: The scarcity of annotated Magnetic Resonance Imaging (MRI) tumor data presents a major obstacle to accurate and automated tumor segmentation. While existing data synthesis methods offer promising solutions, they often suffer from key limitations: manual modeling is labor intensive and requires expert knowledge. Deep generative models may be used to augment data and annotation, but they typically demand large amounts of training pairs in the first place, which is impractical in data limited clinical settings. In this work, we propose Tumor Fabrication (TF), a novel two-stage framework for unpaired 3D brain tumor synthesis. The framework comprises a coarse tumor synthesis process followed by a refinement process powered by a generative model. TF is fully automated and leverages only healthy image scans along with a limited amount of real annotated data to synthesize large volumes of paired synthetic data for enriching downstream supervised segmentation training. We demonstrate that our synthetic image-label pairs used as data enrichment can significantly improve performance on downstream tumor segmentation tasks in low-data regimes, offering a scalable and reliable solution for medical image enrichment and addressing critical challenges in data scarcity for clinical AI applications.

</details>


### [253] [Robust Physical Adversarial Patches Using Dynamically Optimized Clusters](https://arxiv.org/abs/2511.18656)
*Harrison Bagley,Will Meakin,Simon Lucey,Yee Wei Law,Tat-Jun Chin*

Main category: cs.CV

TL;DR: Proposes a superpixel-based regularization method using SLIC clustering to create scale-resilient adversarial patches that maintain effectiveness when rescaled.


<details>
  <summary>Details</summary>
Motivation: Physical adversarial attacks are vulnerable to scale variability, where rescaling causes interpolation-induced color mixing that degrades adversarial signals by smoothing high-frequency patterns.

Method: Uses Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels during patch optimization, with Implicit Function Theorem for gradient backpropagation to update superpixel boundaries and colors.

Result: Method achieves better digital performance and preserves gains in physical realization, with improved resilience to scale changes and interpolation losses.

Conclusion: Superpixel-based regularization produces scale-resilient adversarial patches that maintain structural integrity across different scales, enhancing physical attack robustness.

Abstract: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.

</details>


### [254] [Data Augmentation Strategies for Robust Lane Marking Detection](https://arxiv.org/abs/2511.18668)
*Flora Lian,Dinh Quang Huynh,Hector Penades,J. Stephany Berrio Perez,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: A generative AI pipeline enhances lane detection robustness for side-mounted cameras by simulating deployment-specific viewpoints through geometric transformation, AI inpainting, and vehicle overlays.


<details>
  <summary>Details</summary>
Motivation: Models trained on public datasets like CULane fail to generalize across different camera viewpoints, particularly for side-mounted cameras used in lane-wheel monitoring.

Method: Combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity.

Result: Both SCNN and UFLDv2 models show improved robustness to different conditions including shadows, with gains in precision, recall, and F1 score compared to pre-trained models.

Conclusion: The method bridges the gap between available datasets and deployment scenarios, providing a scalable framework to improve lane detection reliability in pilot deployments.

Abstract: Robust lane detection is essential for advanced driver assistance and autonomous driving, yet models trained on public datasets such as CULane often fail to generalise across different camera viewpoints. This paper addresses the challenge of domain shift for side-mounted cameras used in lane-wheel monitoring by introducing a generative AI-based data enhancement pipeline. The approach combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity. We evaluated the effectiveness of the proposed augmentation in two state-of-the-art models, SCNN and UFLDv2. With the augmented data trained, both models show improved robustness to different conditions, including shadows. The experimental results demonstrate gains in precision, recall, and F1 score compared to the pre-trained model.
  By bridging the gap between widely available datasets and deployment-specific scenarios, our method provides a scalable and practical framework to improve the reliability of lane detection in a pilot deployment scenario.

</details>


### [255] [Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement](https://arxiv.org/abs/2511.18672)
*Yuchen Xia,Souvik Kundu,Mosharaf Chowdhury,Nishil Talati*

Main category: cs.CV

TL;DR: Sphinx is a training-free hybrid framework for Novel View Synthesis that combines regression-based initialization with diffusion models to achieve high-quality results with significantly reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based NVS produces high-quality results but is computationally expensive, while regression-based NVS is fast but produces suboptimal quality. There's a need for a framework that achieves diffusion-level fidelity with efficient inference.

Method: Uses regression-based fast initialization to guide and reduce denoising workload for diffusion models, integrates selective refinement with adaptive noise scheduling to allocate more compute to uncertain regions and frames.

Result: Achieves 1.8x speedup over diffusion model inference with less than 5% perceptual degradation, establishing new Pareto frontier between quality and latency.

Conclusion: Sphinx successfully bridges the gap between diffusion-based and regression-based NVS by providing flexible performance-quality trade-off navigation for dynamically changing inference scenarios.

Abstract: Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.

</details>


### [256] [Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers](https://arxiv.org/abs/2511.18673)
*Yiqing Shi,Yiren Song,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Edit2Perceive adapts image editing diffusion models for dense perception tasks like depth, normal, and matting, achieving state-of-the-art results with faster inference.


<details>
  <summary>Details</summary>
Motivation: Most dense perception methods rely on text-to-image generators designed for stochastic generation, but image editing diffusion models are inherently more image-to-image consistent, making them better suited for perception tasks.

Method: Built on FLUX.1 Kontext architecture with full-parameter fine-tuning and pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states, using single-step deterministic inference.

Result: Achieves comprehensive state-of-the-art results across depth, normal, and matting tasks with up to faster runtime while training on relatively small datasets.

Conclusion: Editing-oriented diffusion transformers show strong potential for geometry-aware perception tasks.

Abstract: Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.

</details>


### [257] [MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis](https://arxiv.org/abs/2511.18676)
*Yongcheng Yao,Yongshuo Zong,Raman Dutt,Yongxin Yang,Sotirios A Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: MedVision introduces a large-scale dataset and benchmark to evaluate and improve vision-language models on quantitative medical image analysis tasks like detection, size estimation, and angle/distance measurement.


<details>
  <summary>Details</summary>
Motivation: Current medical VLMs focus on categorical QA and qualitative tasks, but clinical decision-making requires quantitative assessments (e.g., tumor size measurement) which remain underexplored in existing models.

Method: Created MedVision dataset spanning 22 public datasets with 30.8M image-annotation pairs, focusing on three quantitative tasks: anatomical detection, tumor/lesion size estimation, and angle/distance measurement. Evaluated off-the-shelf VLMs and performed supervised fine-tuning.

Result: Off-the-shelf VLMs performed poorly on quantitative tasks, but supervised fine-tuning on MedVision significantly improved performance across all three tasks with reduced error rates and improved precision.

Conclusion: MedVision provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging, addressing a critical gap in current medical AI systems.

Abstract: Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., "Is this normal or abnormal?") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.

</details>


### [258] [A Theory-Inspired Framework for Few-Shot Cross-Modal Sketch Person Re-Identification](https://arxiv.org/abs/2511.18677)
*Yunpeng Gong,Yongjie Hou,Jiangming Shi,Kim Long Diep,Min Jiang*

Main category: cs.CV

TL;DR: KTCAA is a framework for few-shot sketch-based person re-identification that addresses modality gaps through alignment augmentation and knowledge transfer catalyst modules.


<details>
  <summary>Details</summary>
Motivation: Sketch-based person re-identification faces challenges due to significant modality gaps between hand-drawn sketches and RGB surveillance images, and limited annotated data availability.

Method: Proposes two components: Alignment Augmentation (AA) applies sketch-style transformations to simulate target distributions, and Knowledge Transfer Catalyst (KTC) enhances invariance through worst-case perturbations and consistency enforcement, optimized under meta-learning.

Result: KTCAA achieves state-of-the-art performance on multiple benchmarks, particularly excelling in data-scarce conditions.

Conclusion: The framework successfully addresses cross-modal generalization challenges in sketch-based re-identification through theoretically grounded alignment and invariance mechanisms.

Abstract: Sketch based person re-identification aims to match hand-drawn sketches with RGB surveillance images, but remains challenging due to significant modality gaps and limited annotated data. To address this, we introduce KTCAA, a theoretically grounded framework for few-shot cross-modal generalization. Motivated by generalization theory, we identify two key factors influencing target domain risk: (1) domain discrepancy, which quantifies the alignment difficulty between source and target distributions; and (2) perturbation invariance, which evaluates the model's robustness to modality shifts. Based on these insights, we propose two components: (1) Alignment Augmentation (AA), which applies localized sketch-style transformations to simulate target distributions and facilitate progressive alignment; and (2) Knowledge Transfer Catalyst (KTC), which enhances invariance by introducing worst-case perturbations and enforcing consistency. These modules are jointly optimized under a meta-learning paradigm that transfers alignment knowledge from data-rich RGB domains to sketch-based scenarios. Experiments on multiple benchmarks demonstrate that KTCAA achieves state-of-the-art performance, particularly in data-scarce conditions.

</details>


### [259] [Neural Geometry Image-Based Representations with Optimal Transport (OT)](https://arxiv.org/abs/2511.18679)
*Xiang Gao,Yuanpeng Liu,Xinmu Wang,Jiazhi Li,Minghao Guo,Yu Guo,Xiyun Song,Heather Yu,Zhiqiang Lao,Xianfeng David Gu*

Main category: cs.CV

TL;DR: A decoder-free neural representation for 3D meshes using geometry images with Optimal Transport for efficient storage and single-pass restoration.


<details>
  <summary>Details</summary>
Motivation: Existing neural mesh representations require computationally expensive successive decoding passes due to irregular mesh connectivity, while image-based methods offer efficient processing but are difficult to apply to meshes.

Method: Transform irregular meshes into regular geometry images using Optimal Transport to resolve sampling issues, enabling efficient image-based neural processing with mipmapping for continuous LoD.

Result: Achieves state-of-the-art storage efficiency and restoration accuracy with superior compression ratio, Chamfer distance, and Hausdorff distance metrics.

Conclusion: Geometry image-based representation with Optimal Transport enables decoder-free, storage-efficient neural processing of 3D meshes with single-pass restoration and continuous LoD.

Abstract: Neural representations for 3D meshes are emerging as an effective solution for compact storage and efficient processing. Existing methods often rely on neural overfitting, where a coarse mesh is stored and progressively refined through multiple decoder networks. While this can restore high-quality surfaces, it is computationally expensive due to successive decoding passes and the irregular structure of mesh data. In contrast, images have a regular structure that enables powerful super-resolution and restoration frameworks, but applying these advantages to meshes is difficult because their irregular connectivity demands complex encoder-decoder architectures. Our key insight is that a geometry image-based representation transforms irregular meshes into a regular image grid, making efficient image-based neural processing directly applicable. Building on this idea, we introduce our neural geometry image-based representation, which is decoder-free, storage-efficient, and naturally suited for neural processing. It stores a low-resolution geometry-image mipmap of the surface, from which high-quality meshes are restored in a single forward pass. To construct geometry images, we leverage Optimal Transport (OT), which resolves oversampling in flat regions and undersampling in feature-rich regions, and enables continuous levels of detail (LoD) through geometry-image mipmapping. Experimental results demonstrate state-of-the-art storage efficiency and restoration accuracy, measured by compression ratio (CR), Chamfer distance (CD), and Hausdorff distance (HD).

</details>


### [260] [Hierarchical GraphCut Phase Unwrapping based on Invariance of Diffeomorphisms Framework](https://arxiv.org/abs/2511.18682)
*Xiang Gao,Xinmu Wang,Zhou Zhao,Junqi Huang,Xianfeng David Gu*

Main category: cs.CV

TL;DR: Proposes a phase unwrapping framework using diffeomorphisms and GraphCut for real-time 3D scanning, achieving 45.5x speedup with lower error.


<details>
  <summary>Details</summary>
Motivation: Existing phase unwrapping methods trade speed for accuracy - fast approaches lack precision while accurate algorithms are too slow for real-time applications like 4D facial dynamics capture.

Method: Reformulates GraphCut-based unwrapping as pixel-labeling problem using diffeomorphisms applied via conformal and optimal transport maps. Uses odd number of precomputed diffeomorphisms with hierarchical GraphCut in each domain, then fuses results via majority voting.

Result: Experimental results show 45.5x speedup and lower L2 error in both real experiments and simulations compared to existing methods.

Conclusion: The framework enables real-time phase unwrapping with improved accuracy, making it suitable for applications requiring high-speed 3D scanning like VR/AR and digital human creation.

Abstract: Recent years have witnessed rapid advancements in 3D scanning technologies, with applications spanning VR/AR, digital human creation, and medical imaging. Structured-light scanning with phase-shifting techniques is preferred for its use of low-intensity visible light and high accuracy, making it well suited for capturing 4D facial dynamics. A key step is phase unwrapping, which recovers continuous phase values from measurements wrapped modulo 2pi. The goal is to estimate the unwrapped phase count k in the equation Phi = phi + 2pi k, where phi is the wrapped phase and Phi is the true phase. Noise, occlusions, and complex 3D geometry make recovering the true phase challenging because phase unwrapping is ill-posed: measurements only provide modulo 2pi values, and estimating k requires assumptions about surface continuity. Existing methods trade speed for accuracy: fast approaches lack precision, while accurate algorithms are too slow for real-time use. To overcome these limitations, this work proposes a phase unwrapping framework that reformulates GraphCut-based unwrapping as a pixel-labeling problem. This framework improves the estimation of the unwrapped phase count k through the invariance property of diffeomorphisms applied in image space via conformal and optimal transport (OT) maps. An odd number of diffeomorphisms are precomputed from the input phase data, and a hierarchical GraphCut algorithm is applied in each domain. The resulting label maps are fused via majority voting to robustly estimate k at each pixel. Experimental results demonstrate a 45.5x speedup and lower L2 error in real experiments and simulations, showing potential for real-time applications.

</details>


### [261] [Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation](https://arxiv.org/abs/2511.18684)
*Shristi Das Biswas,Arani Roy,Kaushik Roy*

Main category: cs.CV

TL;DR: ICE is a training-free, modality-agnostic approach for precise concept removal in text-to-image and text-to-video models using anisotropic energy-weighted scaling and spectral unlearning.


<details>
  <summary>Details</summary>
Motivation: Existing concept removal methods suffer from costly retraining, inference overhead, vulnerability to attacks, and collateral damage due to unmodeled semantic overlap between target concepts and surrounding content.

Method: Defines erase and preserve subspaces using anisotropic energy-weighted scaling, uses a closed-form overlap projector to regularize against their intersection, and applies a convex Spectral Unlearning Objective with analytical solution translated to model's text-conditioning layers.

Result: Achieves strong concept erasure with improved robustness to red-teaming while causing minimal degradation of original generative abilities in both T2I and T2V models.

Conclusion: ICE provides an efficient, permanent, runtime-free solution for precise concept removal across multiple modalities without retraining or inference overhead.

Abstract: Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.

</details>


### [262] [Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents](https://arxiv.org/abs/2511.18685)
*Dayong Liu,Chao Xu,Weihong Chen,Suyu Zhang,Juncheng Wang,Jiankang Deng,Baigui Sun,Yang Liu*

Main category: cs.CV

TL;DR: CFG-Bench is a new benchmark with 1,368 videos and 19,562 multimodal QA pairs to evaluate MLLMs' fine-grained action intelligence for embodied physical interaction, revealing current models' limitations in detailed action instruction and higher-order reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on high-level planning and spatial reasoning but neglect fine-grained action intelligence needed for embodied physical interaction in complex environments.

Method: Created CFG-Bench with curated videos and multimodal QA pairs targeting four cognitive abilities: Physical Interaction, Temporal-Causal Relation, Intentional Understanding, and Evaluative Judgment.

Result: Leading MLLMs struggle with detailed physical interaction instructions and show limitations in higher-order reasoning. SFT on CFG-Bench data significantly improves performance on established embodied benchmarks.

Conclusion: The benchmark highlights current limitations and provides insights for developing more capable embodied agents, showing that teaching fine-grained action articulation directly improves embodied task performance.

Abstract: Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.

</details>


### [263] [EVCC: Enhanced Vision Transformer-ConvNeXt-CoAtNet Fusion for Classification](https://arxiv.org/abs/2511.18691)
*Kazi Reyazul Hasan,Md Nafiu Rahman,Wasif Jalal,Sadif Ahmed,Shahriar Raj,Mubasshira Musarrat,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: EVCC is a hybrid vision architecture combining Vision Transformer, ConvNeXt, and CoAtNet with adaptive token pruning, gated cross-attention, auxiliary heads, and dynamic routing to achieve state-of-the-art accuracy with 25-35% FLOPs reduction.


<details>
  <summary>Details</summary>
Motivation: Hybrid vision architectures combining Transformers and CNNs advance image classification but at high computational cost. Need for efficient models that balance accuracy and efficiency trade-off.

Method: Multi-branch architecture integrating Vision Transformer, lightweight ConvNeXt, and CoAtNet with adaptive token pruning, gated bidirectional cross-attention, auxiliary classification heads, and dynamic router gate with context-aware weighting.

Result: Achieves state-of-the-art accuracy with up to 2 percentage points improvement over DeiT-Base, MaxViT-Base, and CrossViT-Base, while reducing FLOPs by 25-35% across CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets.

Conclusion: EVCC efficiently balances accuracy-efficiency trade-off by dynamically adjusting computational demands, combining global context, local details, and hierarchical features for real-world applications.

Abstract: Hybrid vision architectures combining Transformers and CNNs have significantly advanced image classification, but they usually do so at significant computational cost. We introduce EVCC (Enhanced Vision Transformer-ConvNeXt-CoAtNet), a novel multi-branch architecture integrating the Vision Transformer, lightweight ConvNeXt, and CoAtNet through key innovations: (1) adaptive token pruning with information preservation, (2) gated bidirectional cross-attention for enhanced feature refinement, (3) auxiliary classification heads for multi-task learning, and (4) a dynamic router gate employing context-aware confidence-driven weighting. Experiments across the CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets demonstrate EVCC's superiority over powerful models like DeiT-Base, MaxViT-Base, and CrossViT-Base by consistently achieving state-of-the-art accuracy with improvements of up to 2 percentage points, while reducing FLOPs by 25 to 35%. Our adaptive architecture adjusts computational demands to deployment needs by dynamically reducing token count, efficiently balancing the accuracy-efficiency trade-off while combining global context, local details, and hierarchical features for real-world applications. The source code of our implementation is available at https://anonymous.4open.science/r/EVCC.

</details>


### [264] [Exploring Surround-View Fisheye Camera 3D Object Detection](https://arxiv.org/abs/2511.18695)
*Changcai Li,Wenwei Lin,Zuoxun Hou,Gang Chen,Wei Zhang,Huihui Zhou,Weishi Zheng*

Main category: cs.CV

TL;DR: This paper explores 3D object detection using fisheye cameras, develops two fisheye-compatible methods (FisheyeBEVDet and FisheyePETR), and releases a new dataset called Fisheye3DOD.


<details>
  <summary>Details</summary>
Motivation: Classic pinhole-based 3D object detectors suffer performance degradation when applied to fisheye imagery due to different geometric properties.

Method: Developed two methods incorporating fisheye geometry: FisheyeBEVDet (BEV paradigm) and FisheyePETR (query-based paradigm), both using spherical spatial representations. Also created Fisheye3DOD dataset using CARLA simulation.

Result: The fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods on the Fisheye3DOD dataset.

Conclusion: Incorporating fisheye geometry into 3D object detection frameworks is technically feasible and significantly improves performance compared to standard pinhole-based approaches.

Abstract: In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.

</details>


### [265] [Dendritic Convolution for Noise Image Recognition](https://arxiv.org/abs/2511.18699)
*Jiarui Xue,Dongjian Yang,Ye Sun,Gang Liu*

Main category: cs.CV

TL;DR: Proposes a dendritic anti-noise convolution that mimics neuron dendrites to improve robustness against noise in image recognition tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for noisy image recognition have reached performance bottlenecks, and there's limited exploration of anti-interference solutions from a neuronal perspective.

Method: Develops dendritic convolution that integrates neighborhood interaction computation logic of dendrites, simulating XOR logic preprocessing through nonlinear feature interactions to reconstruct feature extraction mathematically.

Result: In image classification and object detection tasks, replacing traditional convolution with dendritic convolution improved EfficientNet-B0 accuracy by 11.23% on noisy datasets and increased YOLOv8 mAP by 19.80%.

Conclusion: The biologically-inspired dendritic convolution significantly outperforms traditional convolution in noisy environments due to its alignment with biological neuron computation principles.

Abstract: In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.

</details>


### [266] [ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction](https://arxiv.org/abs/2511.18701)
*Mustafa Munir,Harsh Goel,Xiwen Wei,Minkyu Choi,Sahil Shah,Kartikeya Bhardwaj,Paul Whatmough,Sandeep Chinchali,Radu Marculescu*

Main category: cs.CV

TL;DR: ObjectAlign is a framework that combines perceptual metrics with symbolic reasoning to detect and fix object inconsistencies in edited videos, using learnable thresholds and neuro-symbolic verification to ensure both low-level stability and high-level temporal correctness.


<details>
  <summary>Details</summary>
Motivation: Video editing often introduces object inconsistencies like frame flicker and identity drift that degrade quality, requiring a systematic approach to detect and correct these issues.

Method: Proposes learnable thresholds for object consistency metrics, a neuro-symbolic verifier with SMT-based identity checks and temporal fidelity verification, and neural network interpolation for frame repair.

Result: Shows up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to state-of-the-art baselines on DAVIS and Pexels datasets.

Conclusion: ObjectAlign effectively addresses object-level and temporal inconsistencies in video editing through a unified neuro-symbolic approach that combines perceptual metrics with formal verification.

Abstract: Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed "consistent" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.

</details>


### [267] [CoD: A Diffusion Foundation Model for Image Compression](https://arxiv.org/abs/2511.18706)
*Zhaoyang Jia,Zihan Zheng,Naifu Xue,Jiahao Li,Bin Li,Zongyu Guo,Xiaoyi Zhang,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: CoD is a compression-oriented diffusion foundation model that replaces text conditioning with image-only training, achieving SOTA compression efficiency at ultra-low bitrates while being 300x faster to train than Stable Diffusion.


<details>
  <summary>Details</summary>
Motivation: Text conditioning in existing diffusion codecs is suboptimal for compression, especially at ultra-low bitrates, limiting the potential of downstream diffusion-based compression methods.

Method: Developed CoD - a compression-oriented diffusion foundation model trained from scratch on open image-only datasets, enabling end-to-end optimization of both compression and generation without text conditioning.

Result: CoD achieves SOTA compression results (e.g., 0.0039 bpp), is 300x faster to train than Stable Diffusion, and provides new insights showing pixel-space diffusion can reach VTM-level PSNR with high perceptual quality while outperforming GAN-based codecs with fewer parameters.

Conclusion: CoD establishes a new foundation for diffusion-based codec research, offering superior compression efficiency, efficient training, and novel insights into diffusion-based compression capabilities.

Abstract: Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \textbf{CoD}, the first \textbf{Co}mpression-oriented \textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \textbf{Low-cost and reproducible training}, 300$\times$ faster training than Stable Diffusion ($\sim$ 20 vs. $\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.

</details>


### [268] [Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation](https://arxiv.org/abs/2511.18711)
*Yuyang Wanyan,Xiaoshan Yang,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: Proposes MC-LRD framework for Few-Shot Video Domain Adaptation that decomposes modality features into unique and shared components with different domain shift levels to improve multimodal collaboration and domain alignment.


<details>
  <summary>Details</summary>
Motivation: Videos' multimodal nature requires simultaneous domain alignment and modality collaboration in few-shot scenarios. Domain shift affects individual modalities and fused features due to coupled features with varying domain shifts, reducing multimodal integration effectiveness.

Method: Uses Modality-Collaborative LowRank Decomposers (MC-LRD) with multiple decomposers per modality and Multimodal Decomposition Routers (MDR) to extract modality-unique and modality-shared features. Applies orthogonal decorrelation constraints and cross-domain activation consistency loss for domain alignment.

Result: Extensive experiments on three public benchmarks show significant improvements over existing methods.

Conclusion: MC-LRD effectively addresses FSVDA challenges by decomposing multimodal features into domain-alignment-friendly components, achieving superior performance through collaborative modality processing and consistent cross-domain activation.

Abstract: In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.

</details>


### [269] [DriveFlow: Rectified Flow Adaptation for Robust 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2511.18713)
*Hongbin Lin,Yiming Yang,Chaoda Zheng,Yifan Zhang,Shuaicheng Niu,Zilu Guo,Yafeng Li,Gui Gui,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: DriveFlow is a Rectified Flow Adaptation method that enhances training data for autonomous driving vision models by adapting pre-trained Text-to-Image flow models with frequency decomposition strategies to maintain 3D object geometry while improving OOD robustness.


<details>
  <summary>Details</summary>
Motivation: Vision-centric 3D object detection in autonomous driving faces OOD issues due to high annotation costs and diverse outdoor scenes. Existing training-free image editing methods have limitations: inversion-based approaches are ineffective and inaccurate, while rectified-flow methods struggle to preserve accurate 3D geometry.

Method: DriveFlow adapts pre-trained Text-to-Image flow models using frequency decomposition. It introduces: 1) High-Frequency Foreground Preservation with alignment loss to maintain 3D object geometry, and 2) Dual-Frequency Background Optimization to balance editing flexibility and semantic consistency.

Result: Comprehensive experiments show DriveFlow achieves comprehensive performance improvements across all categories in OOD scenarios, validating its effectiveness and efficiency for training data enhancement.

Conclusion: DriveFlow successfully addresses the limitations of existing training-free image editing methods by preserving 3D object geometry while enhancing model robustness against OOD scenarios in autonomous driving applications.

Abstract: In autonomous driving, vision-centric 3D object detection recognizes and localizes 3D objects from RGB images. However, due to high annotation costs and diverse outdoor scenes, training data often fails to cover all possible test scenarios, known as the out-of-distribution (OOD) issue. Training-free image editing offers a promising solution for improving model robustness by training data enhancement without any modifications to pre-trained diffusion models. Nevertheless, inversion-based methods often suffer from limited effectiveness and inherent inaccuracies, while recent rectified-flow-based approaches struggle to preserve objects with accurate 3D geometry. In this paper, we propose DriveFlow, a Rectified Flow Adaptation method for training data enhancement in autonomous driving based on pre-trained Text-to-Image flow models. Based on frequency decomposition, DriveFlow introduces two strategies to adapt noise-free editing paths derived from text-conditioned velocities. 1) High-Frequency Foreground Preservation: DriveFlow incorporates a high-frequency alignment loss for foreground to maintain precise 3D object geometry. 2) Dual-Frequency Background Optimization: DriveFlow also conducts dual-frequency optimization for background, balancing editing flexibility and semantic consistency. Comprehensive experiments validate the effectiveness and efficiency of DriveFlow, demonstrating comprehensive performance improvements on all categories across OOD scenarios. Code is available at https://github.com/Hongbin98/DriveFlow.

</details>


### [270] [Seeing What Matters: Visual Preference Policy Optimization for Visual Generation](https://arxiv.org/abs/2511.18719)
*Ziqi Ni,Yuanzhi Liang,Rui Li,Yi Zhou,Haibing Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: ViPO enhances Group Relative Policy Optimization (GRPO) for visual generative models by converting scalar rewards into pixel-level advantage maps, enabling fine-grained alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO pipelines use single scalar rewards per sample, ignoring spatial and temporal structure of visual content, which limits correction of localized artifacts and modeling of fine-grained perceptual cues.

Method: ViPO employs a Perceptual Structuring Module using pretrained vision backbones to create spatially and temporally aware advantage maps, redistributing optimization pressure to perceptually important regions while maintaining GRPO stability.

Result: ViPO consistently outperforms vanilla GRPO across image and video benchmarks, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations.

Conclusion: ViPO provides a more expressive and informative learning signal for visual generation, is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines.

Abstract: Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.

</details>


### [271] [GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.18729)
*Lin Liu,Caiyan Jia,Guanyi Yu,Ziying Song,JunQiao Li,Feiyang Jia,Peiliang Wu,Xiaoshuai Hao,Yandan Luo*

Main category: cs.CV

TL;DR: GuideFlow is a novel autonomous driving planning framework that uses Constrained Flow Matching to address multimodal trajectory mode collapse in imitative planners and constraint handling issues in generative planners, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing E2E autonomous driving planners have limitations: Imitative planners suffer from multimodal trajectory mode collapse (lack diversity), while Generative planners struggle to incorporate safety and physical constraints directly into generation, requiring additional optimization.

Method: Uses Constrained Flow Matching that explicitly models flow matching process to mitigate mode collapse and allows flexible guidance. Enforces explicit constraints directly in flow matching generation rather than implicit encoding. Unifies flow matching training with Energy-Based Model (EBM) for autonomous constraint optimization. Parameterizes driving aggressiveness as control signal for trajectory style manipulation.

Result: Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim, ADV-NuScenes) validate effectiveness. Achieved SOTA on NavSim test hard split with EPDMS score of 43.0.

Conclusion: GuideFlow successfully addresses key limitations in existing E2E planners by combining constrained flow matching with EBM training, enabling diverse trajectory generation with explicit constraint enforcement and style control.

Abstract: Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \textit{\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \textit{\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \textit{\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \textit{\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \textit{\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \textit{\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be released.

</details>


### [272] [Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion](https://arxiv.org/abs/2511.18734)
*Keyang Lu,Sifan Zhou,Hongbin Xu,Gang Xu,Zhifei Yang,Yikai Wang,Zhen Xiao,Jieyi Long,Ming Li*

Main category: cs.CV

TL;DR: Yo'City is an agentic framework for personalized and infinitely expandable 3D city generation using large models, featuring hierarchical planning, iterative 3D synthesis, and relationship-guided expansion.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on single diffusion models, limiting personalized and boundless city-scale scene generation capabilities.

Method: Uses hierarchical 'City-District-Grid' planning with Global Planner and Local Designer, followed by 'produce-refine-evaluate' isometric image synthesis loop and image-to-3D generation, plus relationship-guided expansion with scene graph-based optimization.

Result: Extensive experiments show Yo'City consistently outperforms state-of-the-art methods across all evaluation aspects including semantics, geometry, texture, and layout.

Conclusion: Yo'City enables user-customized and infinitely expandable 3D city generation through agentic framework leveraging large models' reasoning capabilities.

Abstract: Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.

</details>


### [273] [Thinking Ahead: Foresight Intelligence in MLLMs and World Models](https://arxiv.org/abs/2511.18735)
*Zhantao Gong,Liaoyuan Fan,Qing Guo,Xun Xu,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: Introduces FSU-QA, a new VQA dataset for evaluating Foresight Intelligence - the ability to anticipate future events, and shows current VLMs struggle with foresight reasoning but can be improved through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Foresight Intelligence is essential for applications like autonomous driving but overlooked by existing research, creating a gap in AI capabilities for anticipating future events.

Method: Created FSU-QA dataset for foresight-oriented VQA tasks, evaluated state-of-the-art VLMs, and tested model enhancement through fine-tuning and world model integration.

Result: Current VLMs struggle with foresight reasoning, but fine-tuning small VLMs on FSU-QA enables them to outperform much larger advanced models by substantial margins.

Conclusion: FSU-QA provides a principled foundation for developing next-generation models capable of true future event anticipation and understanding.

Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.

</details>


### [274] [ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion](https://arxiv.org/abs/2511.18742)
*Zhenghan Fang,Jian Zheng,Qiaozi Gao,Xiaofeng Gao,Jeremias Sulam*

Main category: cs.CV

TL;DR: ProxT2I is a text-to-image diffusion model using backward discretization with learned proximal operators instead of score functions, optimized with RL for task-specific rewards, achieving efficient sampling and human-preference alignment with lower compute requirements.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion models use forward discretization and score functions, which are slow, unstable, and require many sampling steps. The authors aim to develop a more efficient and stable alternative.

Method: Developed ProxT2I using backward discretizations with learned conditional proximal operators instead of score functions, combined with reinforcement learning for policy optimization. Created LAION-Face-T2I-15M dataset with 15M high-quality human images for training.

Result: The approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, achieving state-of-the-art performance with lower compute and smaller model size.

Conclusion: ProxT2I offers a lightweight yet performant solution for human text-to-image generation, demonstrating the effectiveness of backward discretization and proximal operators in diffusion models.

Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.

</details>


### [275] [Any4D: Open-Prompt 4D Generation from Natural Language and Images](https://arxiv.org/abs/2511.18746)
*Hao Li,Qiao Sun*

Main category: cs.CV

TL;DR: PEWM addresses embodied world model limitations by restricting video generation to shorter horizons, enabling fine-grained language-action alignment, reducing complexity, improving data efficiency, and decreasing inference latency.


<details>
  <summary>Details</summary>
Motivation: Current video-generation-based embodied world models face bottlenecks due to reliance on large-scale embodied interaction data, which is scarce, difficult to collect, and high-dimensional, limiting language-action alignment and long-horizon video generation.

Method: Proposes Primitive Embodied World Models (PEWM) with fixed shorter video generation horizons, equipped with modular VLM planner and Start-Goal heatmap Guidance (SGG) for closed-loop control and compositional generalization of primitive-level policies.

Result: Enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, reduces learning complexity, improves data efficiency in embodied data collection, and decreases inference latency.

Conclusion: PEWM bridges the gap between fine-grained physical interaction and high-level reasoning by leveraging spatiotemporal vision priors and semantic awareness, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \textit{"GPT moment"} in the embodied domain. There is a naive observation: \textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \textit{2) reduces} learning complexity, \textit{3) improves} data efficiency in embodied data collection, and \textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

</details>


### [276] [From Features to Reference Points: Lightweight and Adaptive Fusion for Cooperative Autonomous Driving](https://arxiv.org/abs/2511.18757)
*Yongqi Zhu,Morui Zhu,Qi Chen,Deyuan Qu,Song Fu,Qing Yang*

Main category: cs.CV

TL;DR: RefPtsFusion is a lightweight cooperative autonomous driving framework that exchanges compact reference points instead of large feature maps, reducing communication bandwidth by 5 orders of magnitude while maintaining perception performance.


<details>
  <summary>Details</summary>
Motivation: Traditional cooperative driving methods share large feature maps or query embeddings, causing high communication overhead that limits scalability and real-time performance in heterogeneous vehicle environments.

Method: Vehicles exchange compact reference points (object positions, velocities, sizes) and use selective Top-K query fusion to add high-confidence queries from senders, creating a sensor- and model-independent interface.

Result: On M3CAD dataset, RefPtsFusion reduces communication overhead from hundreds of MB/s to a few KB/s at 5 FPS while maintaining stable perception performance, demonstrating strong robustness and consistent transmission behavior.

Conclusion: RefPtsFusion provides a scalable, real-time solution for cooperative driving systems by shifting focus from 'what is seen' to 'where to see', enabling efficient collaboration across heterogeneous vehicles with minimal bandwidth requirements.

Abstract: We present RefPtsFusion, a lightweight and interpretable framework for cooperative autonomous driving. Instead of sharing large feature maps or query embeddings, vehicles exchange compact reference points, e.g., objects' positions, velocities, and size information. This approach shifts the focus from "what is seen" to "where to see", creating a sensor- and model-independent interface that works well across vehicles with heterogeneous perception models while greatly reducing communication bandwidth. To enhance the richness of shared information, we further develop a selective Top-K query fusion that selectively adds high-confidence queries from the sender. It thus achieves a strong balance between accuracy and communication cost. Experiments on the M3CAD dataset show that RefPtsFusion maintains stable perception performance while reducing communication overhead by five orders of magnitude, dropping from hundreds of MB/s to only a few KB/s at 5 FPS (frame per second), compared to traditional feature-level fusion methods. Extensive experiments also demonstrate RefPtsFusion's strong robustness and consistent transmission behavior, highlighting its potential for scalable, real-time cooperative driving systems.

</details>


### [277] [VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement](https://arxiv.org/abs/2511.18763)
*Xuanzhao Dong,Wenhui Zhu,Yujian Xiong,Xiwen Chen,Hao Wang,Xin Li,Jiajun Cheng,Zhipeng Wang,Shao Tang,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: VAOT is a vessel-aware optimal transport framework for enhancing color fundus photos that preserves vascular structure using skeleton-based and endpoint-aware regularizers, outperforming GAN-based methods on vessel and lesion segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: CFP image quality suffers from acquisition variability, and existing GAN-based enhancement methods often distort clinically critical vasculature by altering vessel topology and endpoint integrity.

Method: Combines optimal-transport objective with two structure-preserving regularizers: skeleton-based loss for global vascular connectivity and endpoint-aware loss for local termini stability in unpaired enhancement setting.

Result: Superior performance on synthetic degradation benchmark and downstream vessel/lesion segmentation evaluations compared to state-of-the-art baselines.

Conclusion: VAOT effectively reduces noise while preserving vessel structure, addressing critical limitations of existing enhancement methods for retinal imaging.

Abstract: Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT

</details>


### [278] [NI-Tex: Non-isometric Image-based Garment Texture Generation](https://arxiv.org/abs/2511.18765)
*Hui Shan,Ming Li,Haitao Yang,Kai Zheng,Sizhe Zheng,Yanwei Fu,Xiangru Huang*

Main category: cs.CV

TL;DR: This paper presents a method for generating diverse PBR textures for 3D garment meshes from non-isometric images, overcoming limitations of existing approaches that require strict topological consistency or accurate mesh deformation.


<details>
  <summary>Details</summary>
Motivation: Existing industrial 3D garment meshes have limited texture diversity, and current generative methods for texture extraction require strict topological consistency between input images and 3D meshes, constraining generation quality and flexibility.

Method: Constructed 3D Garment Videos dataset with consistent geometry/material supervision across deformations; used Nano Banana for non-isometric image editing; proposed iterative baking method with uncertainty-guided view selection and reweighting for seamless PBR texture fusion.

Result: The feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design, as demonstrated through extensive experiments.

Conclusion: The proposed approach enables robust cross-pose texture learning and reliable cross-topology texture generation between non-isometric image-geometry pairs, producing production-ready PBR textures for industrial garment design.

Abstract: Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited. To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes. However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility. To address the challenging problem of non-isometric image-based garment texture generation, we construct 3D Garment Videos, a physically simulated, garment-centric dataset that provides consistent geometry and material supervision across diverse deformations, enabling robust cross-pose texture learning. We further employ Nano Banana for high-quality non-isometric image editing, achieving reliable cross-topology texture generation between non-isometric image-geometry pairs. Finally, we propose an iterative baking method via uncertainty-guided view selection and reweighting that fuses multi-view predictions into seamless, production-ready PBR textures. Through extensive experiments, we demonstrate that our feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design.

</details>


### [279] [Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment](https://arxiv.org/abs/2511.18766)
*Xintao Chen,Xiaohao Xu,Bozhong Zheng,Yun Liu,Yingna Wu*

Main category: cs.CV

TL;DR: VSAD is a novel framework for unsupervised visual anomaly detection from multi-view images that learns viewpoint-invariant representations by modeling geometric consistency across views, achieving state-of-the-art performance on challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat multiple views as disconnected images, leading to inconsistent feature representations and high false-positive rates when distinguishing genuine defects from benign appearance variations caused by viewpoint changes.

Method: VSAD uses a Multi-View Alignment Module (MVAM) with homography to project and align feature regions between views, integrated into a View-Align Latent Diffusion Model (VALDM) for progressive multi-stage alignment, plus a Fusion Refiner Module (FRM) for global consistency.

Result: Extensive experiments on RealIAD and MANTA datasets show VSAD sets new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly detection with robustness to large viewpoint shifts and complex textures.

Conclusion: VSAD effectively addresses the challenge of multi-view anomaly detection by learning viewpoint-invariant representations through geometric consistency modeling, demonstrating superior performance over existing approaches.

Abstract: Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.

</details>


### [280] [Rethinking Garment Conditioning in Diffusion-based Virtual Try-On](https://arxiv.org/abs/2511.18775)
*Kihyun Na,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: Re-CatVTON is an efficient single UNet model for Virtual Try-On that achieves high performance with reduced computational overhead compared to Dual UNet models, using improved conditioning strategies and direct garment latent injection.


<details>
  <summary>Details</summary>
Motivation: While diffusion-based VTON models with Dual UNet architecture show superior fidelity, they suffer from substantial computational and memory overhead due to their heavy structure.

Method: Developed Re-CatVTON based on three hypotheses about context feature learning, featuring a modified classifier-free guidance strategy for spatial concatenation conditioning and direct injection of ground-truth garment latent to prevent error accumulation.

Result: Significantly improved FID, KID, and LPIPS scores with only marginal decrease in SSIM, requiring less computation and memory than high-performance Dual UNet model Leffa.

Conclusion: Establishes a new efficiency-performance trade-off for single UNet VTON models, demonstrating that high performance can be achieved with reduced computational resources.

Abstract: Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.

</details>


### [281] [ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection](https://arxiv.org/abs/2511.18780)
*Ruize Ma,Minghong Cai,Yilei Jiang,Jiaming Han,Yi Feng,Yingshui Tan,Xiaoyong Zhu,Bo Zhang,Bo Zheng,Xiangyu Yue*

Main category: cs.CV

TL;DR: ConceptGuard is a unified safeguard framework for detecting and mitigating unsafe semantics in multimodal video generation, operating through contrastive detection and semantic suppression mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address safety risks in multimodal video generation where harmful content can emerge from individual modalities or their interactions, overcoming limitations of text-only methods and post-generation auditors.

Method: Two-stage approach: 1) Contrastive detection module projects fused image-text inputs into structured concept space to identify latent safety risks; 2) Semantic suppression mechanism intervenes in multimodal conditioning to steer generation away from unsafe concepts.

Result: Outperforms existing baselines on both ConceptRisk and T2VSafetyBench-TI2V benchmarks, achieving state-of-the-art results in risk detection and safe video generation.

Conclusion: ConceptGuard provides an effective proactive solution for multimodal video safety, demonstrating superior performance over existing methods through its unified detection and mitigation framework.

Abstract: Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.

</details>


### [282] [A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data](https://arxiv.org/abs/2511.18781)
*Haotian Yan,Bocheng Guo,Jianzhong He,Nir A. Sochen,Ofer Pasternak,Lauren J O'Donnell,Fan Zhang*

Main category: cs.CV

TL;DR: A dual-stream framework combining dMRI and fMRI data for improved white matter tract classification by enhancing functional coherence of tract parcellation.


<details>
  <summary>Details</summary>
Motivation: Current streamline classification methods rely only on geometric features and fail to distinguish functionally distinct fiber tracts with similar pathways.

Method: Novel dual-stream network with pretrained backbone for full streamline trajectories and auxiliary network processing fMRI signals from fiber endpoint regions.

Result: Successfully parcellated corticospinal tract (CST) into four somatotopic subdivisions with superior performance in ablation studies and comparisons with state-of-the-art methods.

Conclusion: The proposed framework effectively enhances tract parcellation by jointly analyzing dMRI and fMRI data, providing functionally coherent streamline classification.

Abstract: Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.

</details>


### [283] [STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution](https://arxiv.org/abs/2511.18786)
*Junyang Chen,Jiangxin Dong,Long Sun,Yixin Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: STCDiT is a video super-resolution framework that uses a pre-trained video diffusion model to restore structurally faithful and temporally stable videos from degraded inputs, even with complex camera motions.


<details>
  <summary>Details</summary>
Motivation: The main challenges in video super-resolution are maintaining temporal stability during reconstruction and preserving structural fidelity during generation, especially under complex camera motions.

Method: Developed motion-aware VAE reconstruction with segment-wise processing for uniform motion characteristics, and anchor-frame guidance that leverages structural information from first-frame latents to constrain generation and improve structural fidelity.

Result: Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.

Conclusion: The combination of motion-aware VAE reconstruction and anchor-frame guidance enables high-quality video super-resolution with improved structural fidelity and temporal stability.

Abstract: We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.

</details>


### [284] [Understanding Task Transfer in Vision-Language Models](https://arxiv.org/abs/2511.18787)
*Bhuvan Sachdeva,Karan Uppal,Abhinav Java,Vineeth N. Balasubramanian*

Main category: cs.CV

TL;DR: This paper studies how finetuning Vision-Language Models on one visual perception task affects their zero-shot performance on other tasks, introducing a metric called Perfection Gap Factor to quantify transfer effects and revealing task relationships through transfer graphs.


<details>
  <summary>Details</summary>
Motivation: VLMs perform well on multimodal benchmarks but lag in visual perception tasks, and task-specific finetuning can unpredictably affect performance on other tasks, making efficient training challenging.

Method: Systematically study task transferability by finetuning three open-weight VLMs on one perception task and evaluating zero-shot performance on 13 others, using the introduced Perfection Gap Factor metric to quantify transfer effects and construct task-transfer graphs.

Result: The analysis reveals patterns of positive and negative transfer, identifies task groups that mutually influence each other, organizes tasks into personas based on transfer behavior, and shows how PGF can guide data selection for more efficient training.

Conclusion: The findings highlight opportunities for positive transfer and risks of negative interference, providing actionable guidance for advancing VLMs through better understanding of task relationships and transfer effects.

Abstract: Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.

</details>


### [285] [StereoDETR: Stereo-based Transformer for 3D Object Detection](https://arxiv.org/abs/2511.18788)
*Shiyi Mu,Zichong Gu,Zhiqi Ai,Anqi Liu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: StereoDETR is an efficient stereo 3D object detection framework that combines monocular DETR with stereo disparity features to achieve real-time inference while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Stereo-based 3D detection methods offer higher accuracy than monocular approaches but suffer from high computational overhead and latency, with current state-of-the-art stereo methods being only half as fast as monocular approaches despite having twice the accuracy.

Method: StereoDETR consists of two branches: a monocular DETR branch with additional channels for predicting object scale, orientation, and sampling points, and a stereo branch that uses low-cost multi-scale disparity features to predict object-level depth maps. The branches are coupled through a differentiable depth sampling strategy with constrained supervision for handling occlusion.

Result: StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It achieves competitive accuracy on the KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets.

Conclusion: StereoDETR successfully bridges the speed gap between stereo and monocular 3D detection methods while maintaining high accuracy, demonstrating that efficient stereo 3D detection is achievable through careful architectural design.

Abstract: Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.

</details>


### [286] [Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing](https://arxiv.org/abs/2511.18792)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Chun Tung Chou,Wen Hu*

Main category: cs.CV

TL;DR: Foundation model approach using Masked Autoencoding pretraining on large-scale Wi-Fi CSI datasets improves cross-domain robustness for Wi-Fi sensing tasks.


<details>
  <summary>Details</summary>
Motivation: Wi-Fi sensing lacks robustness across domains due to domain shift problems, with models failing to generalize to new environments, hardware, or users.

Method: Applied Masked Autoencoding (MAE) style pretraining on 1.3+ million samples from 14 datasets across 4 devices, 2.4/5/6 GHz bands, and 20-160 MHz bandwidths.

Result: Log-linear improvements in unseen domain performance with increased pretraining data; larger models provide marginal gains; cross-domain accuracy improved by 2.2-15.7% compared to supervised baselines.

Conclusion: Data scale and diversity, not model capacity, are key bottlenecks for Wi-Fi sensing generalization, providing direction for robust real-world deployment.

Abstract: While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical "domain shift" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.

</details>


### [287] [PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion](https://arxiv.org/abs/2511.18801)
*Yichen Yang,Hong Li,Haodong Zhu,Linin Yang,Guojun Lei,Sheng Xu,Baochang Zhang*

Main category: cs.CV

TL;DR: PartDiffuser: A semi-autoregressive diffusion framework for point-cloud-to-mesh generation that balances global structure with local details using part-wise processing.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive methods struggle with balancing global structural consistency and high-fidelity local details, and are susceptible to error accumulation in mesh generation.

Method: Performs semantic segmentation, then uses autoregression between parts for global topology and parallel discrete diffusion within each part for local details. Based on DiT architecture with part-aware cross-attention using point clouds as geometric conditioning.

Result: Significantly outperforms state-of-the-art models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.

Conclusion: PartDiffuser effectively decouples global and local generation tasks, achieving superior mesh generation with both structural consistency and high-frequency geometric features.

Abstract: Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a "part-wise" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.

</details>


### [288] [TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging](https://arxiv.org/abs/2511.18806)
*Qinglei Cao,Ziyao Tang,Xiaoqin Tang*

Main category: cs.CV

TL;DR: A novel 3D CT reconstruction framework that uses target priors from projection data to enhance implicit neural representation learning, achieving superior efficiency and quality in ultra-sparse view scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing implicit 3D reconstruction methods neglect anatomical priors, limiting reconstruction precision and learning efficiency, especially in ultra-sparse view CT imaging.

Method: Integrates positional and structural encoding for voxel-wise implicit reconstruction, using target priors to guide voxel sampling and enrich structural encoding. Includes a CUDA-based algorithm for rapid 3D target prior estimation from sparse-view projections.

Result: Achieves 10x learning efficiency improvement over NAF model and PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections respectively compared to NeRP.

Conclusion: The proposed framework significantly enhances both learning efficiency and reconstruction quality in sparse-view CT reconstruction by effectively leveraging target priors for implicit neural representation learning.

Abstract: X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.

</details>


### [289] [Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache](https://arxiv.org/abs/2511.18811)
*Yuqiu Jiang,Xiaozhen Qiao,Tianyu Mei,Haojian Huang,Yifan Chen,Ye Zheng,Zhe Sun*

Main category: cs.CV

TL;DR: Proposes Adaptive Diversity Cache (ADC), a training-free plug-and-play module that mitigates long-tail bias in HOI detection by accumulating diverse feature representations and using frequency-aware adaptation for rare categories.


<details>
  <summary>Details</summary>
Motivation: Existing VLM-based HOI detection methods require additional training or prompt tuning, causing computational overhead and poor performance on rare interactions in long-tailed scenarios.

Method: ADC constructs class-specific caches that accumulate high-confidence diverse features during inference, with frequency-aware adaptation favoring rare categories for prediction calibration without training.

Result: Achieves up to +8.57% mAP gain on rare categories and +4.39% on full dataset in HICO-DET and V-COCO, improving existing HOI detectors without additional training.

Conclusion: ADC effectively mitigates long-tail bias in HOI detection while preserving overall performance, offering a scalable training-free solution.

Abstract: Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\% mAP gain on rare categories and +4.39\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.

</details>


### [290] [DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video](https://arxiv.org/abs/2511.18814)
*Jiawei Hou,Shenghao Zhang,Can Wang,Zheng Gu,Yonggen Ling,Taiping Zeng,Xiangyang Xue,Jingbo Zhang*

Main category: cs.CV

TL;DR: DetAny4D is an end-to-end open-set 4D object detection framework that uses multi-modal feature fusion and geometry-aware spatiotemporal decoding to achieve reliable 3D detection in streaming video with improved temporal stability.


<details>
  <summary>Details</summary>
Motivation: Existing 4D object detection methods lack temporal consistency modeling or use complex multi-stage pipelines prone to error propagation, and there's a shortage of large-scale datasets with continuous reliable 3D bounding box annotations.

Method: Proposes DetAny4D framework that fuses multi-modal features from pre-trained foundational models, uses geometry-aware spatiotemporal decoder to capture spatial and temporal dynamics, and employs multi-task learning with dedicated training strategy for global consistency across variable-length sequences.

Result: Extensive experiments show DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing jitter and inconsistency issues in 4D object detection.

Conclusion: DetAny4D provides an effective solution for reliable 4D object detection with improved temporal consistency, supported by the new DA4D dataset containing over 280k sequences with high-quality annotations.

Abstract: Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.

</details>


### [291] [SupLID: Geometrical Guidance for Out-of-Distribution Detection in Semantic Segmentation](https://arxiv.org/abs/2511.18816)
*Nimeshika Udayangani,Sarah Erfani,Christopher Leckie*

Main category: cs.CV

TL;DR: SupLID is a novel framework for pixel-level Out-of-Distribution detection in semantic segmentation that uses Linear Intrinsic Dimensionality to guide classifier-derived OOD scores, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Traditional image-level OOD methods adapted for pixel-level detection inherit limitations like vulnerability to overconfidence, and direct application of LID at pixel level is challenging.

Method: SupLID constructs a geometrical coreset capturing ID subspace structure, computes OOD scores at superpixel level for efficiency and spatial smoothness, and combines geometrical cues with classifier confidence.

Result: SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across AUR, FPR, and AUP metrics.

Conclusion: Geometrical cues from SupLID serve as complementary signals to traditional classifier confidence, enabling improved OOD detection as a post-hoc method that integrates seamlessly with any semantic segmentation classifier.

Abstract: Out-of-Distribution (OOD) detection in semantic segmentation aims to localize anomalous regions at the pixel level, advancing beyond traditional image-level OOD techniques to better suit real-world applications such as autonomous driving. Recent literature has successfully explored the adaptation of commonly used image-level OOD methods--primarily based on classifier-derived confidence scores (e.g., energy or entropy)--for this pixel-precise task. However, these methods inherit a set of limitations, including vulnerability to overconfidence. In this work, we introduce SupLID, a novel framework that effectively guides classifier-derived OOD scores by exploiting the geometrical structure of the underlying semantic space, particularly using Linear Intrinsic Dimensionality (LID). While LID effectively characterizes the local structure of high-dimensional data by analyzing distance distributions, its direct application at the pixel level remains challenging. To overcome this, SupLID constructs a geometrical coreset that captures the intrinsic structure of the in-distribution (ID) subspace. It then computes OOD scores at the superpixel level, enabling both efficient real-time inference and improved spatial smoothness. We demonstrate that geometrical cues derived from SupLID serve as a complementary signal to traditional classifier confidence, enhancing the model's ability to detect diverse OOD scenarios. Designed as a post-hoc scoring method, SupLID can be seamlessly integrated with any semantic segmentation classifier at deployment time. Our results demonstrate that SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across key evaluation metrics, including AUR, FPR, and AUP. Code is available at https://github.com/hdnugit/SupLID.

</details>


### [292] [Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring](https://arxiv.org/abs/2511.18817)
*Siyuan Wei,Chunjie Wang,Xiao Liu,Xiaosheng Yan,Zhishan Zhou,Rui Huang*

Main category: cs.CV

TL;DR: Automated pipeline generates large-scale 3D scene-dialogue dataset (Disc3D) to address data scarcity in 3D MLLMs, resolving viewpoint and object referring ambiguities through rule-based and MLLM/LLM integration.


<details>
  <summary>Details</summary>
Motivation: 3D MLLMs lag behind 2D counterparts due to lack of large-scale, high-quality 3D scene-dialogue datasets. Existing methods rely on expensive human annotation and fail to resolve viewpoint ambiguity and object referring ambiguity.

Method: Four-stage automated pipeline: (1) meta-annotation collection for object/frame/scene captions, (2) scene graph construction with relation correction, (3) discriminative object referring for exclusive descriptions, (4) multi-task data generation for diverse dialogues.

Result: Produced Disc3D dataset with over 2 million samples across 25K hybrid 3D scenes, covering captioning, visual grounding, and five object-centric QA tasks. Training with Disc3D yields consistent significant improvements on benchmarks.

Conclusion: The automated pipeline successfully generates high-quality 3D dialogue data at low cost, systematically mitigating dataset flaws and enabling scalable 3D MLLM training with demonstrated performance gains.

Abstract: 3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.

</details>


### [293] [DiP: Taming Diffusion Models in Pixel Space](https://arxiv.org/abs/2511.18822)
*Zhennan Chen,Junwei Zhu,Xu Chen,Jiangning Zhang,Xiaobin Hu,Hanzhen Zhao,Chengjie Wang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DiP is an efficient pixel space diffusion framework that decouples generation into global structure construction and local detail restoration, achieving LDM-level efficiency without VAE reliance.


<details>
  <summary>Details</summary>
Motivation: To resolve the fundamental trade-off between generation quality and computational efficiency in diffusion models, addressing limitations of both LDMs (information loss, non-end-to-end training) and pixel space models (computational prohibitive for high-resolution).

Method: Decouples generation into global and local stages: Diffusion Transformer backbone operates on large patches for efficient global structure, while lightweight Patch Detailer Head restores fine-grained local details using contextual features.

Result: Achieves computational efficiency comparable to LDMs without VAE, with up to 10x faster inference than previous methods, only 0.3% parameter increase, and 1.90 FID score on ImageNet 256x256.

Conclusion: DiP successfully resolves the quality-efficiency dilemma in diffusion models through synergistic global-local decoupling, enabling efficient high-resolution pixel space generation.

Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\times$256.

</details>


### [294] [VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models](https://arxiv.org/abs/2511.18823)
*Fufangchen Zhao,Liao Zhang,Daiqi Shi,Yuanjun Gao,Chen Ye,Yang Cai,Jian Gao,Danfeng Yan*

Main category: cs.CV

TL;DR: VideoPerceiver is a video multimodal large language model that enhances fine-grained perception in video understanding through a two-stage training framework with key-information-missing videos and relative rewards.


<details>
  <summary>Details</summary>
Motivation: Address VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos by improving fine-grained perception.

Method: Two-stage training: 1) SFT with key-information-missing videos via frame replacement and auxiliary contrastive loss, 2) RL with relative rewards comparing responses from complete vs degraded videos.

Result: Substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks while maintaining strong performance on standard tasks.

Conclusion: Prioritizing task-relevant visual features redefines video-language model training for fine-grained perception.

Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct "key-information-missing" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.

</details>


### [295] [Q-Save: Towards Scoring and Attribution for Generated Video Evaluation](https://arxiv.org/abs/2511.18825)
*Xiele Wu,Zicheng Zhang,Mingtao Chen,Yixian Liu,Yiming Liu,Shushi Wang,Zhichao Hu,Yuhong Liu,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: Q-Save is a benchmark dataset and model for holistic, explainable evaluation of AI-generated video quality, featuring 10K videos with multi-aspect annotations and a unified evaluation model using SlowFast architecture and multi-stage training.


<details>
  <summary>Details</summary>
Motivation: To enable accurate and interpretable quality assessment of AI-generated videos by providing fine-grained attribution labels across visual quality, dynamic quality, and text-video alignment dimensions.

Method: Proposes a unified evaluation model using SlowFast framework to distinguish fast/low-res and slow/high-res frames, trained with multi-stage strategy: SFT → GRPO → SFT using Chain-of-Thought formatted data.

Result: The model achieves state-of-the-art performance in video quality prediction while providing human-aligned, interpretable justifications for the scores.

Conclusion: Q-Save establishes a strong foundation for explainable evaluation in generative video research, contributing to multimodal generation and trustworthy AI development.

Abstract: We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.

</details>


### [296] [Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification](https://arxiv.org/abs/2511.18826)
*Aakash Gore,Anoushka Dey,Aryan Mishra*

Main category: cs.CV

TL;DR: Proposes an uncertainty-aware dual-student knowledge distillation framework using two heterogeneous student architectures (ResNet-18 and MobileNetV2) with peer-learning, achieving improved accuracy over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge distillation treats all teacher predictions equally, ignoring teacher confidence levels, which limits effectiveness.

Method: Uncertainty-aware dual-student framework with peer-learning mechanism where two heterogeneous students learn collaboratively from teacher and each other.

Result: On ImageNet-100: ResNet-18 achieved 83.84% top-1 accuracy (2.04% improvement), MobileNetV2 achieved 81.46% top-1 accuracy (0.92% improvement) over traditional single-student distillation.

Conclusion: The proposed uncertainty-aware dual-student knowledge distillation framework with peer-learning effectively improves model compression performance by leveraging teacher uncertainty and collaborative student learning.

Abstract: Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\% top-1 accuracy and MobileNetV2 achieving 81.46\% top-1 accuracy, representing improvements of 2.04\% and 0.92\% respectively over traditional single-student distillation approaches.

</details>


### [297] [Leveraging Metaheuristic Approaches to Improve Deep Learning Systems for Anxiety Disorder Detection](https://arxiv.org/abs/2511.18827)
*Mohammadreza Amiri,Monireh Hosseini*

Main category: cs.CV

TL;DR: This paper proposes a hybrid model combining deep learning with swarm intelligence optimization for automated anxiety detection using multimodal sensor data, achieving improved accuracy and generalization compared to deep learning alone.


<details>
  <summary>Details</summary>
Motivation: Traditional anxiety assessment methods are subjective, time-consuming, and evaluator-dependent. The study aims to leverage AI for more consistent and automated anxiety detection to overcome these limitations.

Method: Integration of deep learning architectures with swarm intelligence optimization (genetic algorithms and particle swarm optimization) using multimodal wearable-sensor datasets to analyze physiological, emotional, and behavioral signals.

Result: The hybrid model significantly enhances detection performance with notable improvements in accuracy and demonstrates stronger generalization across various individuals compared to using deep networks alone.

Conclusion: Combining metaheuristic optimization with deep learning shows strong potential for developing scalable, objective, and clinically meaningful solutions for anxiety disorder assessment.

Abstract: Despite being among the most common psychological disorders, anxiety-related conditions are still primarily identified through subjective assessments, such as clinical interviews and self-evaluation questionnaires. These conventional methods often require significant time and may vary depending on the evaluator. However, the emergence of advanced artificial intelligence techniques has created new opportunities for detecting anxiety in a more consistent and automated manner. To address the limitations of traditional approaches, this study introduces a comprehensive model that integrates deep learning architectures with optimization strategies inspired by swarm intelligence. Using multimodal and wearable-sensor datasets, the framework analyzes physiological, emotional, and behavioral signals. Swarm intelligence techniques including genetic algorithms and particle swarm optimization are incorporated to refine the feature space and optimize hyperparameters. Meanwhile, deep learning components are tasked with deriving layered and discriminative representations from sequential, multi-source inputs. Our evaluation shows that the fusion of these two computational paradigms significantly enhances detection performance compared with using deep networks alone. The hybrid model achieves notable improvements in accuracy and demonstrates stronger generalization across various individuals. Overall, the results highlight the potential of combining metaheuristic optimization with deep learning to develop scalable, objective, and clinically meaningful solutions for assessing anxiety disorders

</details>


### [298] [VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction](https://arxiv.org/abs/2511.18831)
*Shaobo Wang,Tianle Niu,Runkang Yang,Deshan Liu,Xu He,Zichen Wen,Conghui He,Xuming Hu,Linfeng Zhang*

Main category: cs.CV

TL;DR: VideoCompressa is a novel video data synthesis framework that addresses video dataset inefficiency through dynamic latent compression, achieving unprecedented data efficiency by identifying and compressing the most informative frames.


<details>
  <summary>Details</summary>
Motivation: Video understanding models face scalability limitations due to high storage and computational costs of large-scale video datasets. Traditional data synthesis methods struggle with video's temporal redundancy and complex spatiotemporal dynamics.

Method: Jointly optimizes a differentiable keyframe selector (lightweight ConvNet with Gumbel-Softmax) to identify informative frames and a pretrained frozen VAE to compress frames into compact latent codes. The compression network enables end-to-end backpropagation with co-optimized keyframe selection and synthetic latent codes.

Result: Achieves 2.34% improvement over full-data training on UCF101 using only 0.13% of original data with 5800x speedup. Matches full-data performance on HMDB51 fine-tuning Qwen2.5-7B-VL using just 0.41% of training data, outperforming zero-shot baseline by 10.61%.

Conclusion: VideoCompressa effectively addresses video dataset inefficiency by focusing on intra-sample frame-level redundancy rather than inter-sample redundancy, enabling highly efficient video data synthesis with significant performance gains and computational savings.

Abstract: The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\% points using only 0.13\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\% of the training data-outperforming zero-shot baseline by 10.61\%.

</details>


### [299] [FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories](https://arxiv.org/abs/2511.18834)
*Lei Ke,Hubery Yin,Gongye Liu,Zhengyao Lv,Jingcai Guo,Chen Li,Wenhan Luo,Yujiu Yang,Jing Lyu*

Main category: cs.CV

TL;DR: FlowSteer improves ReFlow-based distillation by addressing distribution mismatch and introducing adversarial training on teacher trajectories, achieving better sampling efficiency for flow models.


<details>
  <summary>Details</summary>
Motivation: ReFlow has theoretical consistency with flow matching but underperforms compared to other distillation methods in practice, limiting its adoption for efficient sampling.

Method: Proposes FlowSteer with Online Trajectory Alignment to fix distribution mismatch, adversarial distillation on ODE trajectories, and fixes a flaw in FlowMatchEulerDiscreteScheduler.

Result: Experiments on SD3 demonstrate the method's efficacy in improving ReFlow-based distillation performance.

Conclusion: FlowSteer successfully unlocks ReFlow's potential by guiding students along authentic teacher trajectories, making it competitive with other distillation approaches.

Abstract: With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.

</details>


### [300] [FVAR: Visual Autoregressive Modeling via Next Focus Prediction](https://arxiv.org/abs/2511.18838)
*Xiaofan Li,Chenming Wu,Yanpeng Sun,Jiaming Zhou,Delin Qu,Yansong Qu,Weihao Bo,Haibao Yu,Dingkang Liang*

Main category: cs.CV

TL;DR: FVAR introduces a next-focus prediction paradigm that replaces traditional next-scale prediction in visual autoregressive models, using progressive defocusing to eliminate aliasing artifacts and improve fine detail generation.


<details>
  <summary>Details</summary>
Motivation: Conventional visual autoregressive models use uniform scale downsampling which causes aliasing artifacts, compromising fine details and introducing jaggies and moiré patterns.

Method: Three key innovations: 1) Next-Focus Prediction Paradigm using progressive blur reduction; 2) Progressive Refocusing Pyramid Construction with physics-consistent defocus kernels; 3) High-Frequency Residual Learning with a specialized teacher network.

Result: FVAR substantially reduces aliasing artifacts, improves fine detail preservation, enhances text readability, and achieves superior performance while maintaining compatibility with existing VAR frameworks.

Conclusion: The next-focus prediction paradigm effectively eliminates aliasing at its source and enables high-quality visual generation with better detail preservation than traditional scale-based approaches.

Abstract: Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moiré patterns. To tackle this issue, we present \textbf{FVAR}, which reframes the paradigm from \emph{next-scale prediction} to \emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks.

</details>


### [301] [Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification](https://arxiv.org/abs/2511.18839)
*Yasiru Laksara,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: Deep Ensembles outperform Monte Carlo Dropout for uncertainty quantification in chest X-ray diagnosis, achieving superior calibration and enabling reliable uncertainty decomposition into aleatoric and epistemic components.


<details>
  <summary>Details</summary>
Motivation: Deep learning models like CheXNet lack reliable uncertainty measures, limiting their clinical utility. This project addresses the need for robust uncertainty quantification in medical diagnosis.

Method: Transitioned from Monte Carlo Dropout to a 9-member Deep Ensemble architecture after initial MCD approach failed to stabilize performance and calibration.

Result: Deep Ensemble achieved SOTA performance with AUROC of 0.8559, F1 score of 0.3857, excellent calibration (ECE 0.0728, NLL 0.1916), and reliable uncertainty decomposition with mean epistemic uncertainty of 0.0240.

Conclusion: Deep Ensembles provide a trustworthy and explainable platform for clinical decision support by transforming probabilistic models into reliable diagnostic systems with proper uncertainty quantification.

Abstract: The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.

</details>


### [302] [Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration](https://arxiv.org/abs/2511.18847)
*Ishmam Tashdeed,Md. Atiqur Rahman,Sabrina Islam,Md. Azam Hossain*

Main category: cs.CV

TL;DR: FedOAP is a personalized federated learning approach for organ-agnostic tumor segmentation that uses decoupled cross-attention to model inter-organ dependencies and perturbed boundary loss to improve segmentation precision.


<details>
  <summary>Details</summary>
Motivation: Existing PFL approaches overlook the benefits of leveraging shared features across clients with different organ segmentation data, failing to capture inter-organ dependencies that could improve tumor segmentation.

Method: FedOAP uses decoupled cross-attention (DCA) to retain local queries while attending to globally shared key-value pairs, and perturbed boundary loss (PBL) to focus on boundary inconsistencies for more precise localization.

Result: Extensive experiments show FedOAP consistently outperforms state-of-the-art federated and personalized segmentation methods across diverse tumor segmentation tasks spanning different organs.

Conclusion: The proposed FedOAP framework effectively leverages shared features across clients while maintaining data confidentiality, demonstrating superior performance in organ-agnostic tumor segmentation through cross-attention and boundary-aware optimization.

Abstract: Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.

</details>


### [303] [Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization](https://arxiv.org/abs/2511.18851)
*Yilin Wen,Kechuan Dong,Yusuke Sugano*

Main category: cs.CV

TL;DR: Proposes motion discretization and soft-reset mechanisms for online test-time adaptation in 3D human pose estimation to mitigate error accumulation from imperfect self-supervision.


<details>
  <summary>Details</summary>
Motivation: Online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time.

Method: Uses unsupervised clustering to derive anchor motions from latent representations, enabling efficient self-replay. Introduces soft-reset mechanism by reverting to exponential moving average during continuous adaptation.

Result: Outperforms previous online test-time adaptation methods and enables robust exploitation of personal shape and motion traits for enhanced accuracy.

Conclusion: The proposed solution effectively mitigates error accumulation in online test-time adaptation for 3D human pose estimation through motion discretization and soft-reset mechanisms.

Abstract: Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.

</details>


### [304] [Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos](https://arxiv.org/abs/2511.18856)
*Sana Alamgeer*

Main category: cs.CV

TL;DR: A hybrid saliency model for predicting regions of interest in 360° videos to optimize streaming efficiency and viewing experience.


<details>
  <summary>Details</summary>
Motivation: ROI prediction is crucial for 360° video streaming to reduce bandwidth usage, predict view-ports for head-mounted devices, and enable intelligent video cuts for improved streaming efficiency.

Method: Preprocess videos to obtain frames, develop a hybrid saliency model for ROI prediction, and post-process predictions to get ROI for each frame.

Result: Performance compared with subjective annotations from the 360RAT dataset.

Conclusion: The proposed hybrid saliency model effectively identifies regions of interest in 360° videos for streaming optimization.

Abstract: The main goal of the project is to design a new model that predicts regions of interest in 360$^{\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.

</details>


### [305] [Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling](https://arxiv.org/abs/2511.18858)
*Xiao Cui,Yulei Qin,Xinyue Li,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: This paper addresses the limitations of dataset distillation methods on long-tailed datasets by introducing a statistical alignment approach with three key components: enhanced expert models, recalibrated BN statistics, and multi-round synthetic image initialization.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods perform well on balanced datasets but struggle with long-tailed distributions where imbalanced class frequencies cause biased model representations and corrupt statistical estimates like Batch Normalization statistics.

Method: The approach uses statistical alignment with three components: (1) enhanced expert models for reliable statistics estimation and soft-label generation, (2) recalibrated BN statistics via full forward pass with dynamic momentum to reduce representation skew, (3) multi-round initialization of synthetic images using high-confidence diverse augmentations for coverage and diversity.

Result: Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods, with top-1 accuracy improvements of 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.

Conclusion: The proposed statistical alignment approach effectively mitigates model bias and restores fair supervision in long-tailed dataset distillation, achieving significant performance gains across various class imbalance scenarios.

Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.

</details>


### [306] [DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection](https://arxiv.org/abs/2511.18865)
*Yu Zhang,Haoan Ping,Yuchen Li,Zhenshan Bing,Fuchun Sun,Alois Knoll*

Main category: cs.CV

TL;DR: DualGazeNet is a biologically inspired Transformer framework that achieves state-of-the-art salient object detection with high efficiency by modeling human visual system principles, outperforming 25 existing methods while being 60% faster with 53.4% fewer FLOPs.


<details>
  <summary>Details</summary>
Motivation: Current SOD methods have become overly complex with multi-stage pipelines and specialized modules, introducing feature redundancy and performance bottlenecks. The authors question whether a biologically grounded yet simple architecture can achieve better results without engineering complexity.

Method: DualGazeNet models dual biological principles: robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation from human visual system, using a pure Transformer framework.

Result: Outperforms 25 state-of-the-art CNN- and Transformer-based methods on five RGB SOD benchmarks. Achieves 60% higher inference speed and 53.4% fewer FLOPs than similar-capacity Transformer baselines. Shows strong cross-domain generalization on camouflaged and underwater SOD.

Conclusion: A biologically inspired simple architecture can achieve superior performance and efficiency compared to complex engineered solutions, demonstrating that architectural simplicity grounded in biological principles is effective for salient object detection.

Abstract: Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\% higher inference speed and 53.4\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.

</details>


### [307] [HunyuanVideo 1.5 Technical Report](https://arxiv.org/abs/2511.18870)
*Bing Wu,Chang Zou,Changlin Li,Duojun Huang,Fang Yang,Hao Tan,Jack Peng,Jianbing Wu,Jiangfeng Xiong,Jie Jiang,Linus,Patrol,Peizhen Zhang,Peng Chen,Penghao Zhao,Qi Tian,Songtao Liu,Weijie Kong,Weiyan Wang,Xiao He,Xin Li,Xinchi Deng,Xuefei Zhe,Yang Li,Yanxin Long,Yuanbo Peng,Yue Wu,Yuhong Liu,Zhenyu Wang,Zuozhuo Dai,Bo Peng,Coopers Li,Gu Gong,Guojian Xiao,Jiahe Tian,Jiaxin Lin,Jie Liu,Jihong Zhang,Jiesong Lian,Kaihang Pan,Lei Wang,Lin Niu,Mingtao Chen,Mingyang Chen,Mingzhe Zheng,Miles Yang,Qiangqiang Hu,Qi Yang,Qiuyong Xiao,Runzhou Wu,Ryan Xu,Rui Yuan,Shanshan Sang,Shisheng Huang,Siruis Gong,Shuo Huang,Weiting Guo,Xiang Yuan,Xiaojia Chen,Xiawei Hu,Wenzhi Sun,Xiele Wu,Xianshun Ren,Xiaoyan Yuan,Xiaoyue Mi,Yepeng Zhang,Yifu Sun,Yiting Lu,Yitong Li,You Huang,Yu Tang,Yixuan Li,Yuhang Deng,Yuan Zhou,Zhichao Hu,Zhiguang Liu,Zhihe Yang,Zilin Yang,Zhenzhi Lu,Zixiang Zhou,Zhao Zhong*

Main category: cs.CV

TL;DR: HunyuanVideo 1.5 is a lightweight 8.3B-parameter video generation model that achieves SOTA quality and motion coherence while enabling efficient inference on consumer GPUs.


<details>
  <summary>Details</summary>
Motivation: To provide a high-performance, open-source video generation foundation that lowers barriers to video creation and research, making advanced video generation accessible to broader audiences.

Method: Key components include meticulous data curation, advanced DiT architecture with selective and sliding tile attention (SSTA), glyph-aware text encoding, progressive pre-training and post-training, and efficient video super-resolution network.

Result: Extensive experiments show the model establishes new SOTA among open-source video generation models with high visual quality and motion coherence.

Conclusion: The compact 8.3B-parameter model provides a high-performance foundation for text-to-video and image-to-video generation, with all code and weights publicly released to advance community research.

Abstract: We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.

</details>


### [308] [Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction](https://arxiv.org/abs/2511.18873)
*Yiming Wang,Shaofei Wang,Marko Mihajlovic,Siyu Tang*

Main category: cs.CV

TL;DR: Neural Texture Splatting (NTS) enhances 3D Gaussian Splatting by using a global neural field to predict local appearance and geometry for each primitive, achieving state-of-the-art performance across various reconstruction tasks.


<details>
  <summary>Details</summary>
Motivation: 3DGS has limited representational capacity due to using only 3D Gaussian kernels. While recent approaches added per-splat textures, they primarily target dense view synthesis and underperform in general reconstruction scenarios. The authors aim to achieve concrete performance improvements across a wide range of reconstruction tasks.

Method: Introduces Neural Texture Splatting (NTS) with a global neural field (hybrid of tri-plane and neural decoder) that predicts local appearance and geometric fields for each primitive. This shared representation reduces model size, enables efficient global information exchange, and models view- and time-dependent effects.

Result: Extensive experiments show NTS consistently improves models and achieves state-of-the-art results across multiple benchmarks for novel view synthesis, geometry and dynamic reconstruction under both sparse and dense input settings.

Conclusion: Neural Texture Splatting successfully enhances 3DGS by leveraging a global neural representation for local texture fields, demonstrating strong generalization across tasks and introducing expressive view- and time-dependent effects that existing methods lack.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.

</details>


### [309] [Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference](https://arxiv.org/abs/2511.18875)
*Wengyi Zhan,Mingbao Lin,Zhihang Lin,Rongrong Ji*

Main category: cs.CV

TL;DR: ParVTS is a training-free framework that partitions visual tokens into subject/non-subject groups, processes them in parallel, then discards non-subject tokens mid-inference to reduce computation in multimodal LLMs.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from high inference latency due to quadratic self-attention scaling with sequence length, particularly from thousands of visual tokens in high-resolution images. Naive token pruning risks losing essential contextual information.

Method: Partition visual tokens into subject and non-subject groups, process them in parallel to transfer semantics to question tokens, then discard non-subject path mid-inference without requiring additional training or modules.

Result: Prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction across multiple MLLM backbones.

Conclusion: ParVTS effectively reduces computational complexity in MLLMs while maintaining accuracy, is training-free, and compatible with diverse existing architectures.

Abstract: Multimodal large language models (MLLMs) deliver impressive vision-language reasoning but suffer steep inference latency because self-attention scales quadratically with sequence length and thousands of visual tokens contributed by high-resolution images. Naively pruning less-informative visual tokens reduces this burden, yet indiscriminate removal can strip away contextual cues essential for background or fine-grained questions, undermining accuracy. In this paper, we present ParVTS (Parallel Vision Token Scheduling), a training-free scheduling framework that partitions visual tokens into subject and non-subject groups, processes them in parallel to transfer their semantics into question tokens, and discards the non-subject path mid-inference to reduce computation. This scheduling reduces computational complexity, requires no heuristics or additional modules, and is compatible with diverse existing MLLM architectures. Experiments across multiple MLLM backbones show that ParVTS prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction.

</details>


### [310] [Facade Segmentation for Solar Photovoltaic Suitability](https://arxiv.org/abs/2511.18882)
*Ayca Duran,Christoph Waibel,Bernd Bickel,Iro Armeni,Arno Schlueter*

Main category: cs.CV

TL;DR: A pipeline that uses SegFormer-B5 fine-tuned on facade data to automatically identify suitable BIPV surfaces and estimate solar energy potential, showing installable potential is much lower than theoretical.


<details>
  <summary>Details</summary>
Motivation: BIPV facades offer urban decarbonization potential where roofs are insufficient, but automated approaches for facade PV planning remain scarce and oversimplified.

Method: Fine-tunes SegFormer-B5 on CMP Facades dataset, converts semantic predictions into PV suitability masks and panel layouts considering module sizes and clearances.

Result: Applied to 373 facades from 10 cities, installable BIPV potential is significantly lower than theoretical potential.

Conclusion: The pipeline can scale with growing facade imagery availability to support BIPV planning in cities worldwide.

Abstract: Building integrated photovoltaic (BIPV) facades represent a promising pathway towards urban decarbonization, especially where roof areas are insufficient and ground-mounted arrays are infeasible. Although machine learning-based approaches to support photovoltaic (PV) planning on rooftops are well researched, automated approaches for facades still remain scarce and oversimplified. This paper therefore presents a pipeline that integrates detailed information on the architectural composition of the facade to automatically identify suitable surfaces for PV application and estimate the solar energy potential. The pipeline fine-tunes SegFormer-B5 on the CMP Facades dataset and converts semantic predictions into facade-level PV suitability masks and PV panel layouts considering module sizes and clearances. Applied to a dataset of 373 facades with known dimensions from ten cities, the results show that installable BIPV potential is significantly lower than theoretical potential, thus providing valuable insights for reliable urban energy planning. With the growing availability of facade imagery, the proposed pipeline can be scaled to support BIPV planning in cities worldwide.

</details>


### [311] [MagicWorld: Interactive Geometry-driven Video World Exploration](https://arxiv.org/abs/2511.18886)
*Guangyuan Li,Siming Zheng,Shuolin Xu,Jinwei Chen,Bo Li,Xiaobin Hu,Lei Zhao,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: MagicWorld is an interactive video world model that addresses structural instability and historical forgetting in scene generation by integrating 3D geometric priors and historical retrieval mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing interactive video world models fail to fully exploit the correspondence between instruction-driven scene motion and underlying 3D geometry, leading to structural instability under viewpoint changes. They also easily forget historical information during multi-step interaction, causing error accumulation and progressive drift in scene semantics and structure.

Method: MagicWorld integrates 3D geometric priors through Action-Guided 3D Geometry Module (AG3D) that constructs point clouds from first frames and actions, providing explicit geometric constraints. It also uses History Cache Retrieval (HCR) mechanism to retrieve and inject relevant historical frames as conditioning signals during generation.

Result: Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations compared to existing methods.

Conclusion: MagicWorld successfully addresses key limitations in interactive video world modeling by leveraging 3D geometric constraints and historical retrieval, leading to more stable and continuous scene evolution under user instructions.

Abstract: Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.

</details>


### [312] [MFmamba: A Multi-function Network for Panchromatic Image Resolution Restoration Based on State-Space Model](https://arxiv.org/abs/2511.18888)
*Qian Jiang,Qianqian Wang,Xin Jin,Michal Wozniak,Shaowen Yao,Wei Zhou*

Main category: cs.CV

TL;DR: MFmamba is a multi-function model that performs super-resolution, spectral recovery, and joint SR+spectral recovery using only a single PAN image as input, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitation that existing methods require multiple inputs or cannot simultaneously improve both spatial and spectral resolution when only a single PAN image is available.

Method: Uses UNet++ backbone with Mamba Upsample Block, replaces skip connections with Dual Pool Attention, and employs Multi-scale Hybrid Cross Block for initial feature extraction.

Result: MFmamba achieves competitive performance in evaluation metrics and visual results across all three tasks using only PAN image input.

Conclusion: The proposed MFmamba model effectively solves the problem of obtaining high-resolution color images from single PAN inputs through integrated SR and spectral recovery capabilities.

Abstract: Remote sensing images are becoming increasingly widespread in military, earth resource exploration. Because of the limitation of a single sensor, we can obtain high spatial resolution grayscale panchromatic (PAN) images and low spatial resolution color multispectral (MS) images. Therefore, an important issue is to obtain a color image with high spatial resolution when there is only a PAN image at the input. The existing methods improve spatial resolution using super-resolution (SR) technology and spectral recovery using colorization technology. However, the SR technique cannot improve the spectral resolution, and the colorization technique cannot improve the spatial resolution. Moreover, the pansharpening method needs two registered inputs and can not achieve SR. As a result, an integrated approach is expected. To solve the above problems, we designed a novel multi-function model (MFmamba) to realize the tasks of SR, spectral recovery, joint SR and spectral recovery through three different inputs. Firstly, MFmamba utilizes UNet++ as the backbone, and a Mamba Upsample Block (MUB) is combined with UNet++. Secondly, a Dual Pool Attention (DPA) is designed to replace the skip connection in UNet++. Finally, a Multi-scale Hybrid Cross Block (MHCB) is proposed for initial feature extraction. Many experiments show that MFmamba is competitive in evaluation metrics and visual results and performs well in the three tasks when only the input PAN image is used.

</details>


### [313] [MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting](https://arxiv.org/abs/2511.18894)
*Chenyu Mu,Guihai Chen,Xun Yang,Erkun Yang,Cheng Deng*

Main category: cs.CV

TL;DR: MetaDCSeg is a robust medical image segmentation framework that dynamically learns pixel-wise weights to handle noisy annotations and ambiguous boundaries using a Dynamic Center Distance mechanism.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation faces challenges from noisy annotations and ambiguous anatomical boundaries, causing training instability. Existing methods fail to adequately address performance degradation in boundary regions.

Method: Proposes MetaDCSeg with Dynamic Center Distance (DCD) mechanism that learns optimal pixel-wise weights to suppress noisy labels while preserving reliable ones. Uses weighted feature distances for foreground, background, and boundary centers to focus on hard-to-segment boundary pixels.

Result: Extensive experiments across four benchmark datasets with varying noise levels show MetaDCSeg consistently outperforms state-of-the-art methods.

Conclusion: The proposed framework effectively handles annotation noise and boundary ambiguity, significantly improving segmentation performance in challenging medical imaging scenarios.

Abstract: Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.

</details>


### [314] [Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation](https://arxiv.org/abs/2511.18919)
*Ruiying Liu,Yuanzhi Liang,Haibin Huang,Tianshu Yu,Chi Zhang*

Main category: cs.CV

TL;DR: BPGO improves GRPO by modeling reward uncertainty with semantic priors, enabling adaptive optimization that emphasizes reliable feedback and reduces overfitting to ambiguous signals.


<details>
  <summary>Details</summary>
Motivation: Standard GRPO underperforms due to the many-to-many relationship between text prompts and visual outputs, causing reward models to generate uncertain signals that lead to suboptimal optimization.

Method: BPGO uses Bayesian prior-guided optimization with semantic anchors, featuring inter-group trust allocation that prioritizes consistent groups and intra-group renormalization that sharpens sample distinctions.

Result: BPGO achieves stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants across image and video generation tasks.

Conclusion: Explicitly modeling reward uncertainty through semantic priors significantly improves GRPO performance by enabling more reliable and discriminative optimization.

Abstract: Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.

</details>


### [315] [EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models](https://arxiv.org/abs/2511.18920)
*Wenhao Xu,Xin Dong,Yue Li,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: EventSTU is a training-free framework that uses event-based vision principles to efficiently reduce video tokens for large language models, achieving 3x FLOPs reduction and speedup while improving performance.


<details>
  <summary>Details</summary>
Motivation: Video large language models have high inference costs due to massive tokens in long videos, and event-based vision can help eliminate redundancy.

Method: Coarse-to-fine keyframe sampling using event change-triggered properties, adaptive token pruning with event visual saliency, and holistic spatio-temporal integration with question relevance.

Result: 3.01x FLOPs reduction and 3.10x prefilling speedup over strongest baseline while still improving performance.

Conclusion: EventSTU provides an effective training-free solution for efficient video understanding using event-based principles, with both physical and simulated event support.

Abstract: Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.

</details>


### [316] [BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models](https://arxiv.org/abs/2511.18921)
*Juncheng Li,Yige Li,Hanxun Huang,Yunhao Chen,Xin Wang,Yixu Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: BackdoorVLM is the first comprehensive benchmark for evaluating backdoor attacks on vision-language models across 5 threat categories, revealing VLMs' high sensitivity to textual instructions and effectiveness of attacks with low poisoning rates.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks have been well-studied in unimodal settings but remain largely underexplored in multimodal foundation models, particularly vision-language models, creating a need for systematic evaluation.

Method: Developed BackdoorVLM benchmark with 5 threat categories, evaluated using 12 attack methods with text/image/bimodal triggers on 2 VLMs and 3 datasets, focusing on image captioning and visual question answering tasks.

Result: VLMs show strong sensitivity to textual instructions; text triggers dominate image triggers in bimodal backdoors; attacks remain highly effective with only 1% poisoning rates achieving over 90% success; significant vulnerabilities discovered in current VLMs.

Conclusion: BackdoorVLM serves as a valuable benchmark for analyzing and mitigating multimodal backdoor threats, highlighting previously underexplored vulnerabilities in vision-language models that require attention.

Abstract: Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\% yielding over 90\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .

</details>


### [317] [One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control](https://arxiv.org/abs/2511.18922)
*Zhenxing Mi,Yuxin Wang,Dan Xu*

Main category: cs.CV

TL;DR: One4D is a unified framework for 4D generation and reconstruction that produces synchronized RGB frames and pointmaps, handling varying input sparsities through Unified Masked Conditioning and using Decoupled LoRA Control to maintain video model quality.


<details>
  <summary>Details</summary>
Motivation: To create a general framework that can seamlessly transition between 4D generation from single images and 4D reconstruction from videos, addressing the challenge of maintaining base video model quality when jointly generating RGB and pointmaps.

Method: Uses Unified Masked Conditioning (UMC) for handling varying input sparsities, adapts video generation models for joint RGB and pointmap generation, and introduces Decoupled LoRA Control (DLC) with modality-specific LoRA adapters connected by zero-initialized control links to learn mutual consistency.

Result: Produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks, trained on synthetic and real 4D datasets with modest computational budgets.

Conclusion: Represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models, demonstrating unified handling of 4D content generation and reconstruction.

Abstract: We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D

</details>


### [318] [AttenDence: Maximizing Attention Confidence for Test Time Adaptation](https://arxiv.org/abs/2511.18925)
*Yash Mali*

Main category: cs.CV

TL;DR: Proposes minimizing attention entropy from CLS token to image patches as a novel test-time adaptation objective for transformers, improving robustness to distribution shifts even with single test images.


<details>
  <summary>Details</summary>
Motivation: Transformers provide additional unsupervised learning signals through attention mechanisms beyond output entropy minimization, which can be leveraged for more effective test-time adaptation.

Method: Minimize entropy of attention distributions from the CLS token to image patches during test-time adaptation, encouraging more confident attention to relevant regions under distribution shift.

Result: Attention entropy minimization improves robustness across diverse corruption types while maintaining performance on clean data, effective even with single test image streams.

Conclusion: Attention entropy minimization is a novel and effective TTA objective that leverages transformer attention mechanisms to enhance model adaptation to distribution shifts at inference time.

Abstract: Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.

</details>


### [319] [FineXtrol: Controllable Motion Generation via Fine-Grained Text](https://arxiv.org/abs/2511.18927)
*Keming Shen,Bizhu Wu,Junliang Chen,Xiaoqin Wang,Linlin Shen*

Main category: cs.CV

TL;DR: FineXtrol is a novel control framework for text-driven motion generation that uses temporally-aware, fine-grained textual control signals to direct specific body part movements, addressing issues of misalignment and computational cost in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for controllable motion generation either use LLMs that introduce misaligned details and lack temporal cues, or use 3D coordinate sequences that are computationally expensive to convert to standard motion representations.

Method: Proposes FineXtrol framework with hierarchical contrastive learning module that encourages text encoder to produce discriminative embeddings for fine-grained control signals describing specific body part movements over time.

Result: Quantitative results show strong performance in controllable motion generation, and qualitative analysis demonstrates flexibility in directing specific body part movements.

Conclusion: FineXtrol provides an efficient solution for precise motion generation using user-friendly, fine-grained textual control signals with improved controllability.

Abstract: Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.

</details>


### [320] [Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search](https://arxiv.org/abs/2511.18929)
*Zijian Song,Xiaoxin Lin,Tao Pu,Zhenlong Yuan,Guangrun Wang,Liang Lin*

Main category: cs.CV

TL;DR: The paper introduces Human-centric Open-future Task Discovery (HOTD) to help LMMs identify tasks that reduce human effort in dynamic scenarios, proposes HOTD-Bench with 2K+ videos, and presents CMAST framework that outperforms existing LMMs.


<details>
  <summary>Details</summary>
Motivation: Current LMMs struggle to discover tasks that assist humans in open-future scenarios where human intentions are concurrent and dynamic, particularly tasks that reduce human effort across multiple plausible futures.

Method: Proposes Collaborative Multi-Agent Search Tree (CMAST) framework that uses multi-agent system for complex reasoning decomposition and scalable search tree for structured reasoning, plus HOTD-Bench with real-world videos and simulation-based evaluation.

Result: CMAST achieves best performance on HOTD-Bench, significantly surpassing existing LMMs, and integrates well with existing LMMs to consistently improve their performance.

Conclusion: The CMAST framework effectively addresses the HOTD problem, demonstrating superior task discovery capabilities and compatibility with existing LMM systems for human-centric assistance in open-future scenarios.

Abstract: Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.

</details>


### [321] [VeCoR - Velocity Contrastive Regularization for Flow Matching](https://arxiv.org/abs/2511.18942)
*Zong-Wei Hong,Jing-lun Li,Lin-Ze Li,Shen Zhang,Yao Tang*

Main category: cs.CV

TL;DR: VeCoR extends Flow Matching with contrastive regularization, adding both positive (attractive) and negative (repulsive) supervision to improve trajectory stability and image quality, especially in low-step and lightweight settings.


<details>
  <summary>Details</summary>
Motivation: Standard Flow Matching may accumulate errors and drive samples off the data manifold, leading to perceptual degradation in lightweight or low-step configurations.

Method: Proposes Velocity Contrastive Regularization (VeCoR) that augments standard FM with two-sided supervision: aligning with stable reference directions (positive) and pushing away from inconsistent off-manifold directions (negative).

Result: Achieves 22-35% relative FID reductions on ImageNet-1K and 32% relative FID gains on MS-COCO text-to-image generation, with consistent improvements in stability and convergence.

Conclusion: VeCoR transforms FM into a two-sided training scheme that significantly enhances perceptual fidelity across datasets and backbones, particularly effective in challenging low-step and lightweight settings.

Abstract: Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.
  To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both "where to go" and "where not to go." To be formal, we propose \textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.
  On ImageNet-1K 256$\times$256, VeCoR yields 22\% and 35\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/

</details>


### [322] [Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining](https://arxiv.org/abs/2511.18946)
*José Teixeira,Pascal Klöckner,Diana Montezuma,Melis Erdal Cesur,João Fraga,Hugo M. Horlings,Jaime S. Cardoso,Sara P. Oliveira*

Main category: cs.CV

TL;DR: This paper introduces CSSP2P GAN for virtual IHC staining from H&E images, addressing limitations in current methods by focusing on adversarial loss impact and proposing better evaluation approaches.


<details>
  <summary>Details</summary>
Motivation: Immunohistochemistry is costly and labor-intensive, making virtual staining a promising alternative. Current methods overlook adversarial loss impact and use inadequate evaluation metrics.

Method: Developed CSSP2P GAN model with focus on adversarial loss optimization and conducted blind pathological expert evaluation for validation.

Result: CSSP2P GAN achieves heightened pathological fidelity and demonstrates superior performance compared to reference works in the field.

Conclusion: Adversarial loss plays a crucial role in virtual staining quality, and current evaluation metrics (SSIM, PSNR) are insufficient for assessing virtually stained images.

Abstract: In addition to evaluating tumor morphology using H&E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.

</details>


### [323] [Eevee: Towards Close-up High-resolution Video-based Virtual Try-on](https://arxiv.org/abs/2511.18957)
*Jianhao Zeng,Yancheng Bai,Ruidong Chen,Xuanpu Zhang,Lei Sun,Dongyang Jin,Ryan Xu,Nannan Zhang,Dan Song,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Proposes a high-resolution video virtual try-on dataset with detailed garment images and close-up videos, plus a new VGID metric for evaluating garment consistency in texture and structure preservation.


<details>
  <summary>Details</summary>
Motivation: Current virtual try-on methods are limited by single garment images that don't capture realistic texture details, and they only generate full-shot videos while neglecting business needs for detailed close-ups.

Method: Created a new dataset with high-fidelity garment images including close-ups and textual descriptions, plus full-shot and close-up try-on videos. Also proposed VGID metric for evaluating garment consistency.

Result: Existing video generation models using the detailed images from this dataset can extract and incorporate texture features, significantly enhancing realism and detail fidelity. Benchmark identified texture and structural preservation problems in current methods.

Conclusion: The dataset and VGID metric address critical limitations in video virtual try-on, enabling better texture preservation and supporting both full-shot and close-up video generation for practical e-commerce applications.

Abstract: Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.

</details>


### [324] [CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery](https://arxiv.org/abs/2511.18968)
*Bhuvan Sachdeva,Sneha Kumari,Rudransh Agarwal,Shalaka Kumaraswamy,Niharika Singri Prasad,Simon Mueller,Raphael Lechtenboehmer,Maximilian W. M. Wintergerst,Thomas Schultz,Kaushik Murali,Mohit Jain*

Main category: cs.CV

TL;DR: CataractCompDetect is an AI framework that automatically detects intraoperative complications in cataract surgery videos using phase-aware localization, SAM 2 tracking, risk scoring, and vision-language reasoning.


<details>
  <summary>Details</summary>
Motivation: Cataract surgery complications like iris prolapse, PCR, and vitreous loss are major causes of adverse outcomes, and automated detection could enable early warning systems and objective training feedback.

Method: Combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification.

Result: Achieves 70.63% average F1 score on CataComp dataset, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss).

Conclusion: The framework demonstrates the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events.

Abstract: Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.

</details>


### [325] [Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs](https://arxiv.org/abs/2511.18976)
*Huaming Ling,Ying Wang,Si Chen,Junfeng Fan*

Main category: cs.CV

TL;DR: This paper presents methods to adapt deep CNNs for fully homomorphic encryption (FHE) inference by replacing non-linear activations with low-degree polynomials and overcoming ciphertext capacity limitations for high-resolution images.


<details>
  <summary>Details</summary>
Motivation: To enable efficient FHE-based inference with deep CNNs by addressing two key challenges: approximating non-linear activations with low-degree polynomials without significant accuracy loss, and overcoming ciphertext capacity barriers for processing high-resolution images.

Method: Proposes single-stage fine-tuning (SFT) to convert pre-trained CNNs to FHE-friendly forms using low-degree polynomials, and a generalized interleaved packing (GIP) scheme with specialized homomorphic operators to handle feature maps of arbitrary spatial resolutions.

Result: Achieved competitive accuracy on CIFAR-10, ImageNet, and MS COCO datasets comparable to baselines using ReLU or SiLU activations, and demonstrated the first FHE-based inference for YOLO object detection architectures.

Conclusion: The proposed methods enable efficient end-to-end FHE inference across diverse CNN architectures while maintaining competitive accuracy, representing significant advances in privacy-preserving deep learning.

Abstract: We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.

</details>


### [326] [Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models](https://arxiv.org/abs/2511.18978)
*Santiago Moreno,Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: ZEUS is a zero-shot visual-language segmentation pipeline for whole-slide images that uses frozen VLM encoders and class-specific textual prompt ensembles to automatically generate high-resolution tumor masks without pixel-level labels.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurate annotation in cutaneous neoplasm biopsies due to morphological variability and subtle distinctions between benign/malignant lesions, while overcoming limitations of existing VLM applications that struggle with fine-grained segmentation in gigapixel WSIs.

Method: Partition WSIs into overlapping patches, extract visual embeddings using frozen VLM encoders, compute cosine similarities against class-specific textual prompt ensembles, and generate final segmentation masks through zero-shot approach.

Result: Demonstrated competitive performance on two in-house datasets (primary spindle cell neoplasms and cutaneous metastases), showing influence of prompt design, domain shifts, and institutional variability in histopathology VLMs.

Conclusion: ZEUS significantly reduces annotation burden while providing scalable, explainable tumor delineation for downstream diagnostic workflows in histopathology.

Abstract: Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.

</details>


### [327] [UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection](https://arxiv.org/abs/2511.18983)
*Ching-Yi Lai,Chih-Yu Jian,Pei-Cheng Chuang,Chia-Ming Lee,Chih-Chung Hsu,Chiou-Ting Hsu,Chia-Wen Lin*

Main category: cs.CV

TL;DR: A novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate deepfake detection that transforms single visual modality into three complementary features and aligns them through contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Address challenges in deepfake detection caused by varying compression rates on social media platforms, where single-modal methods struggle with feature degradation and multimodal approaches face expensive data collection and inconsistent modal quality.

Method: UMCL framework transforms single visual modality into three features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings. Uses affinity-driven semantic alignment (ASA) and cross-quality similarity learning (CQSL) for feature alignment and robustness.

Result: Achieves superior performance across various compression rates and manipulation types, maintaining high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships.

Conclusion: Establishes a new benchmark for robust deepfake detection with explicit feature alignment and cross-compression-rate robustness.

Abstract: In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.

</details>


### [328] [Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning](https://arxiv.org/abs/2511.18989)
*Wassim Benabbas,Mohammed Brahimi,Samir Akhrouf,Bilal Fortas*

Main category: cs.CV

TL;DR: The study investigates whether attention-based architectures and zero-shot learning can bridge the gap between curated academic datasets and real-world plant disease classification, finding that CLIP models offer strong adaptability without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing models trained on clean datasets like PlantVillage fail to generalize to real-world field images, creating a significant gap between academic research and practical agricultural applications.

Method: Evaluated three model categories: CNNs, Vision Transformers, and CLIP-based zero-shot models on their ability to handle domain shift from curated to real-world agricultural conditions.

Result: CNNs showed limited robustness under domain shift, Vision Transformers demonstrated stronger generalization through global contextual features, and CLIP models successfully classified diseases from natural language descriptions without task-specific training.

Conclusion: Zero-shot learning, particularly CLIP models, offers a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments, providing strong adaptability and interpretability.

Abstract: Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.

</details>


### [329] [View-Consistent Diffusion Representations for 3D-Consistent Video Generation](https://arxiv.org/abs/2511.18991)
*Duolikun Danier,Ge Gao,Steven McDonagh,Changjian Li,Hakan Bilen,Oisin Mac Aodha*

Main category: cs.CV

TL;DR: ViCoDR improves 3D consistency in video generation by learning multi-view consistent diffusion representations, reducing visual artifacts from 3D inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Current video generation models suffer from 3D inconsistencies where objects deform under camera pose changes, undermining user experience and simulation fidelity.

Method: Proposes ViCoDR approach that learns multi-view consistent diffusion representations through representation alignment for video diffusion models.

Result: Demonstrates significant improvements in 3D consistency for camera-controlled image-to-video, text-to-video, and multi-view generation models.

Conclusion: Improving multi-view consistency of video diffusion representations yields more 3D-consistent video generation across various video generation tasks.

Abstract: Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.

</details>


### [330] [AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization](https://arxiv.org/abs/2511.18993)
*Christos Koutlis,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: AuViRe is a novel method for temporal localization of deepfakes using Audio-Visual Speech Representation Reconstruction, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: With the rapid advancement of sophisticated synthetic audio-visual content and malicious manipulations, ensuring digital media integrity has become crucial.

Method: Reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform), leveraging that cross-modal reconstruction is more challenging in manipulated segments, creating amplified discrepancies for detection.

Result: Outperforms state-of-the-art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on in-the-wild experiments.

Conclusion: AuViRe provides robust discriminative cues for precise temporal forgery localization through cross-modal reconstruction discrepancies.

Abstract: With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.

</details>


### [331] [A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation](https://arxiv.org/abs/2511.19004)
*Wentao Qu,Guofeng Mei,Yang Wu,Yongshun Gong,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: T2LDM is a Text-to-LiDAR diffusion model with Self-Conditioned Representation Guidance that generates detailed 3D scenes from text prompts while addressing issues of data scarcity and low-quality text descriptions.


<details>
  <summary>Details</summary>
Motivation: Text-to-LiDAR generation faces challenges due to scarce Text-LiDAR pairs causing overly smooth 3D scenes and low-quality text descriptions degrading generation quality and controllability.

Method: Proposes T2LDM with Self-Conditioned Representation Guidance (SCRG) that provides soft supervision during training, directional position prior to mitigate street distortion, and learns conditional encoder for multiple conditional tasks including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation.

Result: T2LDM outperforms existing methods in both unconditional and conditional generation, achieving state-of-the-art scene generation with improved geometric structure perception and scene fidelity.

Conclusion: The proposed T2LDM framework effectively addresses text-to-LiDAR generation challenges through SCRG guidance and multiple conditional capabilities, providing practical insights for text prompt design and achieving superior generation quality.

Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.

</details>


### [332] [Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting](https://arxiv.org/abs/2511.19021)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min*

Main category: cs.CV

TL;DR: Grc-ViT is a dynamic coarse-to-fine Vision Transformer that adaptively adjusts visual granularity based on image complexity to enhance fine-grained discrimination while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers struggle with fine-grained local details and existing multi-scale approaches use fixed patch sizes with redundant computation.

Method: Two-stage framework: (1) Coarse Granularity Evaluation using edge density, entropy, and frequency-domain cues to estimate patch/window sizes; (2) Fine-grained Refinement module that adapts attention computation. Uses learnable parameters α and β for global-local balance.

Result: Grc-ViT enhances fine-grained discrimination and achieves superior trade-off between accuracy and computational efficiency.

Conclusion: The proposed dynamic granularity adjustment framework effectively addresses ViTs' limitations in local detail representation while optimizing computational resources.

Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, α and \b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.

</details>


### [333] [Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling](https://arxiv.org/abs/2511.19024)
*Long Tang,Guoquan Zhen,Jie Hao,Jianbo Zhang,Huiyu Duan,Liang Yuan,Guangtao Zhai*

Main category: cs.CV

TL;DR: Life-IQA proposes a novel BIQA framework using GCN-enhanced layer interaction and MoE-based feature decoupling to improve quality prediction by addressing unequal feature contributions and exploring effective decoding architectures.


<details>
  <summary>Details</summary>
Motivation: Existing BIQA approaches fuse shallow and deep features without considering their unequal contributions to quality prediction, and effective quality decoding architectures remain underexplored despite various vision encoder backbones being widely adopted.

Method: Proposes Life-IQA with two key modules: 1) GCN-enhanced layer interaction using cross-attention between deepest-layer features (query) and penultimate-layer features (key/value), 2) MoE-based feature decoupling that uses different experts specialized for specific distortion types or quality dimensions to decouple fused representations.

Result: Extensive experiments show Life-IQA achieves more favorable balance between accuracy and cost than vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.

Conclusion: Life-IQA effectively addresses the limitations of existing BIQA approaches by properly handling feature contributions and exploring novel decoding architectures, demonstrating superior performance in blind image quality assessment.

Abstract: Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \underline{l}ayer\underline{i}nteraction and MoE-based \underline{f}eature d\underline{e}coupling, termed \textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\texttt{Life-IQA}}.

</details>


### [334] [Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric](https://arxiv.org/abs/2511.19032)
*Xiangjie Sui,Songyang Li,Hanwei Zhu,Baoliang Chen,Yuming Fang,Xin Sun*

Main category: cs.CV

TL;DR: Bench-C is a new benchmark for evaluating corruption robustness in large vision-language models, addressing limitations of existing datasets and metrics by focusing on discriminative samples and introducing RAS to measure prediction structure degradation.


<details>
  <summary>Details</summary>
Motivation: Current evaluation paradigms for LVLM robustness have two major limitations: dominance of low-discriminative samples that mask real robustness gaps, and accuracy-based metrics that fail to capture prediction structure degradation under corruption.

Method: Proposed Bench-C benchmark with a selection strategy that considers prediction inconsistency under corruption and semantic diversity, plus Robustness Alignment Score (RAS) metric that measures degradation in logit-level prediction structure through shifts in prediction uncertainty and calibration alignment.

Result: Experiments revealed: 1) distinct model behavior patterns under corruption (erroneous confidence, hesitation); 2) subtle corruption can slightly improve accuracy but still degrades overall prediction structure; 3) decomposing robustness into destructive/corrective components reveals distinct failure and recovery patterns across models.

Conclusion: Bench-C and RAS provide more comprehensive evaluation of LVLM corruption robustness, revealing important insights about model behavior patterns and prediction structure degradation that traditional metrics miss.

Abstract: Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.

</details>


### [335] [ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay](https://arxiv.org/abs/2511.19033)
*Gengyuan Zhang,Mingcong Ding,Jingpei Wu,Ruotong Liao,Volker Tresp*

Main category: cs.CV

TL;DR: ReEXplore is a training-free framework that improves embodied exploration by using retrospective experience replay and hierarchical frontier selection, achieving up to 3x better performance than MLLM baselines.


<details>
  <summary>Details</summary>
Motivation: Current MLLM-based embodied agents struggle with exploration due to reliance on stale pre-trained knowledge, expensive training requirements for long-horizon tasks, and difficulty handling large action spaces in frontier-based exploration.

Method: Uses retrospective experience replay to inject distilled abstract experience at inference time, and hierarchical frontier selection that decomposes frontier ranking into coarse-to-fine decisions.

Result: Achieves up to 3x higher performance in both success rate and navigation efficiency across multiple embodied exploration benchmarks compared to strong MLLM baselines.

Conclusion: ReEXplore enables robust, traceable, and efficient exploration without requiring expensive training, making it a practical solution for embodied AI exploration tasks.

Abstract: Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.

</details>


### [336] [CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones](https://arxiv.org/abs/2511.19035)
*Kai Zhenga,Zhenkai Wu,Fupeng Wei,Miaolan Zhou,Kai Lie,Haitao Guo,Lei Ding,Wei Zhang,Hang-Cheng Dong*

Main category: cs.CV

TL;DR: Proposes MC-DiSNet with DINOv3 backbone for change semantic detection (CSD) - a new task focusing only on semantic changes in damaged areas, eliminating need for full semantic annotations of bi-temporal images.


<details>
  <summary>Details</summary>
Motivation: Accurate and rapid damage assessment in conflict zones is crucial for humanitarian aid and regional stability. Challenges include limited data, annotation difficulties, high intra-class similarity, and ambiguous semantic changes in damaged areas.

Method: Uses pre-trained DINOv3 model with multi-scale cross-attention difference siamese network (MC-DiSNet) for robust feature extraction from bi-temporal remote sensing images. Introduces change semantic detection (CSD) task focusing only on changed regions.

Result: Evaluated on Gaza-Change and SECOND datasets. Method effectively addresses CSD task with outstanding performance, enabling practical applications for rapid damage assessment in conflict zones.

Conclusion: The proposed approach successfully handles the challenging CSD task and provides a viable solution for rapid damage assessment in conflict areas through focused semantic change detection.

Abstract: Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.

</details>


### [337] [MedSAM3: Delving into Segment Anything with Medical Concepts](https://arxiv.org/abs/2511.19046)
*Anglin Liu,Rundong Xue,Xu R. Cao,Yifan Shen,Yi Lu,Xiang Li,Qianqian Chen,Jintai Chen*

Main category: cs.CV

TL;DR: MedSAM-3 is a text-promptable medical segmentation model that enables precise anatomical structure targeting using open-vocabulary text descriptions, outperforming existing models across diverse medical imaging modalities.


<details>
  <summary>Details</summary>
Motivation: Existing medical segmentation methods lack generalizability and require extensive manual annotation for new clinical applications, creating barriers to widespread adoption.

Method: Fine-tuned Segment Anything Model 3 architecture on medical images with semantic conceptual labels, enabling medical Promptable Concept Segmentation (PCS) and integrating MLLMs for complex reasoning in an agent-in-the-loop workflow.

Result: Significantly outperforms existing specialist and foundation models across diverse medical imaging modalities including X-ray, MRI, Ultrasound, CT, and video.

Conclusion: MedSAM-3 provides a powerful text-promptable segmentation solution that enhances generalizability and reduces manual annotation burden in medical imaging applications.

Abstract: Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.

</details>


### [338] [Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation](https://arxiv.org/abs/2511.19049)
*Ruojun Xu,Yu Kai,Xuhua Ren,Jiaxiang Cheng,Bing Ma,Tianxiang Zheng,Qinhlin Lu*

Main category: cs.CV

TL;DR: PG-DPO addresses likelihood displacement in DPO for diffusion models by combining Adaptive Rejection Scaling and Implicit Preference Regularization, improving video generation quality.


<details>
  <summary>Details</summary>
Motivation: DPO suffers from likelihood displacement where chosen sample probabilities decrease during training, particularly impacting diffusion-based models and video generation tasks.

Method: Introduced Policy-Guided DPO (PG-DPO) with Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to mitigate optimization conflicts and suboptimal maximization identified through formal analysis.

Result: PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations for video generation tasks.

Conclusion: PG-DPO provides a robust solution for improving preference alignment in diffusion-based models, effectively addressing DPO's likelihood displacement issues.

Abstract: Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.

</details>


### [339] [LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space](https://arxiv.org/abs/2511.19057)
*Hai Wu,Shuai Tang,Jiale Wang,Longkun Zou,Mingyue Guo,Rongqin Liang,Ke Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: LAA3D is a large-scale dataset for 3D detection and tracking of low-altitude aircraft, containing 15K real images and 600K synthetic frames with 3D bounding box annotations, supporting multiple aerial vehicle categories.


<details>
  <summary>Details</summary>
Motivation: Current datasets for 3D perception of low-altitude aircraft (LAA) are scarce, limiting research in precise 3D object localization and behavior understanding for aerial vehicles.

Method: Created LAA3D dataset with diverse scenarios (urban/suburban), multiple aerial object categories (eVTOL, MAVs, helicopters), and established benchmark with unified evaluation protocols. Proposed MonoLAA baseline for monocular 3D detection from zoom cameras.

Result: Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. The dataset enables robust 3D localization from varying focal length cameras.

Conclusion: LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception, addressing the gap in specialized datasets for aerial vehicle detection and tracking.

Abstract: Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.

</details>


### [340] [Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation](https://arxiv.org/abs/2511.19062)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min,Yi Zhang*

Main category: cs.CV

TL;DR: Grc-SAM is a coarse-to-fine framework that uses granular computing to enable prompt-free image segmentation, addressing limitations of single-granularity approaches like SAM by improving localizability and scalability.


<details>
  <summary>Details</summary>
Motivation: To overcome two key limitations in existing prompt-free segmentation models: (1) lack of autonomous region localization mechanisms, and (2) limited fine-grained modeling at high resolution.

Method: A three-stage approach: coarse stage extracts high-response regions for foreground localization; fine stage uses finer patch partitioning with sparse local attention for detail modeling; refined masks are encoded as latent prompts for SAM decoder.

Result: Extensive experiments show Grc-SAM outperforms baseline methods in both accuracy and scalability, demonstrating superior performance in prompt-free segmentation tasks.

Conclusion: Grc-SAM successfully bridges granular computing with vision transformers, offering a unique computational perspective for automated prompt-free segmentation that eliminates reliance on handcrafted prompts.

Abstract: Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.

</details>


### [341] [Understanding, Accelerating, and Improving MeanFlow Training](https://arxiv.org/abs/2511.19065)
*Jin-Young Kim,Hyojun Go,Lea Bogensperger,Julius Erbach,Nikolai Kalischek,Federico Tombari,Konrad Schindler,Dominik Narnhofer*

Main category: cs.CV

TL;DR: Analysis of MeanFlow training dynamics reveals that instantaneous velocity learning must precede average velocity learning, and proposes an enhanced training scheme that accelerates convergence and improves few-step generation quality.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics of MeanFlow models and improve their few-step generation performance by analyzing the interaction between instantaneous and average velocity fields.

Method: Analyzed the interaction between instantaneous and average velocity fields, then designed an enhanced training scheme that first accelerates instantaneous velocity formation, then shifts emphasis from short- to long-interval average velocity learning.

Result: Enhanced MeanFlow training achieves FID of 2.87 on 1-NFE ImageNet 256x256 (vs 3.43 baseline), or matches baseline performance with 2.5x shorter training time or smaller DiT-L backbone.

Conclusion: The proposed training strategy based on understanding velocity field interactions significantly improves MeanFlow's convergence speed and few-step generation quality.

Abstract: MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.

</details>


### [342] [DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling](https://arxiv.org/abs/2511.19067)
*Timur Mamedov,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: DynaMix combines labeled multi-camera and pseudo-labeled single-camera data for generalizable person re-identification, using dynamic relabeling, efficient centroids, and smart data sampling to handle large-scale training effectively.


<details>
  <summary>Details</summary>
Motivation: Existing person Re-ID methods rely heavily on limited labeled multi-camera data, which restricts their generalizability across unseen cameras and environments.

Method: DynaMix uses three core components: Relabeling Module for refining pseudo-labels, Efficient Centroids Module for robust identity representations, and Data Sampling Module for balanced mini-batch composition.

Result: Extensive experiments show that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.

Conclusion: DynaMix enables effective large-scale training and achieves superior performance by dynamically adapting to data structure and noise through its specialized components.

Abstract: Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.

</details>


### [343] [DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation](https://arxiv.org/abs/2511.19071)
*Fangda Chen,Jintao Tang,Pancheng Wang,Ting Wang,Shasha Li,Ting Deng*

Main category: cs.CV

TL;DR: DEAP-3DSAM enhances SAM for 3D medical image segmentation by adding a Feature Enhanced Decoder to preserve spatial features and a Dual Attention Prompter for automatic prompting, achieving state-of-the-art performance on abdominal tumor datasets.


<details>
  <summary>Details</summary>
Motivation: SAM shows potential for medical image segmentation but has limitations in 3D applications due to spatial feature loss from pseudo 3D processing and reliance on manual prompts that require expert knowledge and are impractical for real-world use.

Method: Proposed DEAP-3DSAM with two key components: 1) Feature Enhanced Decoder that fuses original image features with spatial information, and 2) Dual Attention Prompter using Spatial Attention and Channel Attention to automatically generate prompts.

Result: Achieved state-of-the-art performance on four public abdominal tumor segmentation datasets, outperforming or matching existing manual prompt methods. Ablation studies confirmed effectiveness of both proposed modules.

Conclusion: DEAP-3DSAM successfully addresses SAM's limitations in 3D medical image segmentation by enhancing spatial features and enabling automatic prompting, making it more practical for real-world applications.

Abstract: The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.

</details>


### [344] [Graph-based 3D Human Pose Estimation using WiFi Signals](https://arxiv.org/abs/2511.19105)
*Jichao Chen,YangYang Qu,Ruibo Tang,Dirk Slock*

Main category: cs.CV

TL;DR: GraphPose-Fi is a graph-based framework for WiFi-based 3D human pose estimation that explicitly models skeletal topology using CNN encoders, attention modules, and graph-based regression with GCN layers and self-attention.


<details>
  <summary>Details</summary>
Motivation: Existing WiFi-based HPE methods use regression networks that directly map CSI to 3D joint coordinates, ignoring the inherent topological relationships among human joints, which limits performance.

Method: The framework includes: 1) CNN encoder shared across antennas for subcarrier-time feature extraction, 2) lightweight attention module for adaptive feature reweighting over time and across antennas, 3) graph-based regression head combining GCN layers with self-attention to capture local topology and global dependencies.

Result: Significantly outperforms existing methods on the MM-Fi dataset in various settings.

Conclusion: GraphPose-Fi effectively models skeletal topology for WiFi-based 3D human pose estimation, achieving superior performance by explicitly capturing joint relationships through graph-based approaches.

Abstract: WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.

</details>


### [345] [HABIT: Human Action Benchmark for Interactive Traffic in CARLA](https://arxiv.org/abs/2511.19109)
*Mohan Ramesh,Mark Azer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: HABIT is a high-fidelity simulation benchmark that integrates real-world human motion into CARLA to address limitations in current autonomous driving simulations by providing more realistic and diverse pedestrian behavior.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving simulations inadequately represent realistic human behavior, particularly complex pedestrian interactions and dynamic intentions, which is critical for ensuring safety and reliability in real-world deployment.

Method: HABIT integrates real-world human motion from mocap and videos into CARLA using a modular motion retargeting pipeline, curating 4,730 traffic-compatible pedestrian motions in SMPL format for physically consistent trajectories.

Result: Evaluation of three state-of-the-art AD agents (InterFuser, TransFuser, BEVDriver) revealed critical failures: up to 7.43 collisions/km (vs near-zero in CARLA Leaderboard), 12.94% AIS 3+ injury risk, and unnecessary braking in up to 33% of cases.

Conclusion: HABIT exposes critical weaknesses in AD agents that remain hidden in scripted simulations, demonstrating the need for more realistic pedestrian behavior modeling to ensure safety and reliability in autonomous driving systems.

Abstract: Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.

</details>


### [346] [DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection](https://arxiv.org/abs/2511.19111)
*Hai Ci,Ziheng Peng,Pei Yang,Yingxin Xuan,Mike Zheng Shou*

Main category: cs.CV

TL;DR: DiffSeg30k is a 30k-image dataset with pixel-level annotations for detecting and localizing diffusion-based image edits, shifting AIGC detection from binary classification to semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing AIGC detection benchmarks focus on classifying entire images but overlook localization of diffusion-based edits, which enables realistic modification of local image regions and makes AI-generated content harder to detect.

Method: Created DiffSeg30k dataset featuring: 1) In-the-wild images from COCO; 2) Diverse diffusion models (8 SOTA models); 3) Multi-turn editing (up to 3 sequential edits); 4) VLM-based pipeline for automatic region identification and context-aware prompts covering additions, removals, and attribute changes.

Result: Benchmarking revealed significant challenges in semantic segmentation tasks, particularly robustness to image distortions. Segmentation models emerged as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers and showing great potential in cross-generator generalization.

Conclusion: DiffSeg30k advances fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods, enabling simultaneous localization of edits and identification of editing models.

Abstract: Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k

</details>


### [347] [3M-TI: High-Quality Mobile Thermal Imaging via Calibration-free Multi-Camera Cross-Modal Diffusion](https://arxiv.org/abs/2511.19117)
*Minchong Chen,Xiaoyun Yuan,Junzhe Wan,Jianing Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 3M-TI is a calibration-free multi-camera cross-modality diffusion framework that enhances mobile thermal imaging resolution without requiring explicit camera calibration between thermal and RGB sensors.


<details>
  <summary>Details</summary>
Motivation: Miniaturized thermal sensors on mobile platforms suffer from limited spatial resolution and textural fidelity, producing blurry images. Existing thermal super-resolution methods either struggle with fine structure recovery or require laborious cross-camera calibration.

Method: Integrates a cross-modal self-attention module into diffusion UNet to adaptively align thermal and RGB features during denoising, leveraging generative priors to enhance resolution and texture without calibration.

Result: Achieves state-of-the-art performance on real-world mobile thermal cameras and public benchmarks, with substantial improvements in downstream tasks like object detection and segmentation.

Conclusion: 3M-TI provides a practical and robust solution for mobile thermal perception systems, eliminating calibration requirements while significantly enhancing thermal image quality and downstream task performance.

Abstract: The miniaturization of thermal sensors for mobile platforms inherently limits their spatial resolution and textural fidelity, leading to blurry and less informative images. Existing thermal super-resolution (SR) methods can be grouped into single-image and RGB-guided approaches: the former struggles to recover fine structures from limited information, while the latter relies on accurate and laborious cross-camera calibration, which hinders practical deployment and robustness. Here, we propose 3M-TI, a calibration-free Multi-camera cross-Modality diffusion framework for Mobile Thermal Imaging. At its core, 3M-TI integrates a cross-modal self-attention module (CSM) into the diffusion UNet, replacing the original self-attention layers to adaptively align thermal and RGB features throughout the denoising process, without requiring explicit camera calibration. This design enables the diffusion network to leverage its generative prior to enhance spatial resolution, structural fidelity, and texture detail in the super-resolved thermal images. Extensive evaluations on real-world mobile thermal cameras and public benchmarks validate our superior performance, achieving state-of-the-art results in both visual quality and quantitative metrics. More importantly, the thermal images enhanced by 3M-TI lead to substantial gains in critical downstream tasks like object detection and segmentation, underscoring its practical value for robust mobile thermal perception systems. More materials: https://github.com/work-submit/3MTI.

</details>


### [348] [MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images](https://arxiv.org/abs/2511.19119)
*Qirui Wang,Jingyi He,Yining Pan,Si Yong Yeo,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: MonoSR is a large-scale dataset for monocular spatial reasoning across indoor, outdoor, and object-centric scenarios, addressing limitations of existing multi-view approaches and enabling open-world applications.


<details>
  <summary>Details</summary>
Motivation: Existing spatial reasoning research focuses on indoor environments with multi-view observations, limiting generalizability to outdoor scenarios and monocular images - the most common real-world setting.

Method: Created MonoSR dataset spanning diverse scenarios (indoor, outdoor, object-centric) with multiple question types, evaluated advanced vision-language models, and analyzed the importance of auxiliary information.

Result: Established a foundation for monocular spatial reasoning, revealed limitations of current vision-language models on this task, and provided practical guidance for future model design.

Conclusion: MonoSR enables open-world monocular spatial reasoning in real-world environments, addressing a critical gap in current research and providing a benchmark for future development.

Abstract: Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.

</details>


### [349] [When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP](https://arxiv.org/abs/2511.19126)
*Beilin Chu,Weike You,Mengtao Li,Tingting Zheng,Kehan Zhao,Xuan Xu,Zhigao Lu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CV

TL;DR: SemAnti is a semantic-antagonistic fine-tuning method that freezes CLIP's semantic subspace and adapts artifact-sensitive layers under shuffled semantics, achieving state-of-the-art cross-domain generalization for AI-generated image detection.


<details>
  <summary>Details</summary>
Motivation: CLIP-based detectors for AI-generated images often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. The work aims to address semantic bias in detection.

Method: Proposes SemAnti paradigm that freezes semantic subspace and adapts only artifact-sensitive layers under shuffled semantics, which disrupts global semantic continuity while preserving local artifact cues.

Result: Achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage benchmarks, demonstrating robust performance across different distributions.

Conclusion: Regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection, and semantic-antagonistic fine-tuning provides an effective approach for this.

Abstract: The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.

</details>


### [350] [MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery](https://arxiv.org/abs/2511.19134)
*Shuyu Cao,Minxin Chen,Yucheng Song,Zhaozhong Chen,Xinyou Zhang*

Main category: cs.CV

TL;DR: MambaRefine-YOLO is a novel object detection framework for UAV imagery that uses dual-gated mamba fusion and hierarchical feature aggregation to achieve state-of-the-art performance with good computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Small object detection in UAV imagery is challenging due to low resolution and background clutter. Existing RGB-IR fusion methods struggle with balancing effective cross-modal interaction and computational efficiency.

Method: Proposes MambaRefine-YOLO with two key components: Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities using illumination-aware and difference-aware gating, and Hierarchical Feature Aggregation Neck (HFAN) that uses a "refine-then-fuse" strategy for multi-scale features.

Result: Achieves state-of-the-art mAP of 83.2% on DroneVehicle dataset (7.9% improvement over baseline). On VisDrone dataset, HFAN-only variant also shows significant gains, demonstrating general applicability.

Conclusion: The framework provides superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.

Abstract: Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.

</details>


### [351] [FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation](https://arxiv.org/abs/2511.19137)
*Zhifeng Xie,Keyi Zhang,Yiye Yan,Yuling Guo,Fan Yang,Jiting Zhou,Mengtian Li*

Main category: cs.CV

TL;DR: FilmSceneDesigner is an automated system that generates film scenes from natural language descriptions using agent-based parameter generation and procedural generation pipeline, achieving structurally sound scenes with cinematic fidelity.


<details>
  <summary>Details</summary>
Motivation: Traditional film set design is labor-intensive and time-consuming, requiring expert-driven manual modeling. The paper aims to automate this process to improve efficiency.

Method: Uses agent-based chaining framework to generate structured parameters from natural language input, followed by procedural generation pipeline for floorplan/structure generation, material assignment, door/window placement, and object retrieval/layout. Also created SetDepot-Pro dataset with 6,862 film-specific 3D assets.

Result: System produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks like virtual previs, construction drawing and mood board creation. Human evaluations confirm effectiveness.

Conclusion: FilmSceneDesigner successfully automates film set design workflow, addressing the limitations of traditional manual approaches while maintaining professional quality and cinematic realism.

Abstract: Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.

</details>


### [352] [ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation](https://arxiv.org/abs/2511.19145)
*Dongha Lee,Jinhee Park,Minjun Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: ABM-LoRA is a principled initialization method that accelerates LoRA convergence by aligning adapter activation boundaries with the pretrained model, reducing information loss and improving early training performance.


<details>
  <summary>Details</summary>
Motivation: LoRA's random initialization causes gradient updates in mismatched tangent spaces, leading to significant information loss and slow convergence during early training stages.

Method: Aligns the adapter's activation boundaries with those of the pretrained model before downstream training to maximize projection of full-parameter gradients into the adapter subspace.

Result: Achieves highest accuracy on VTAB-1K among all methods, with strong gains on structured reasoning tasks requiring geometric understanding. Shows effectiveness across language understanding, dialogue generation, and vision recognition tasks.

Conclusion: ABM-LoRA substantially accelerates LoRA convergence by reducing information loss at initialization through principled activation boundary alignment.

Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.

</details>


### [353] [Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation](https://arxiv.org/abs/2511.19147)
*Huisoo Lee,Jisu Han,Hyunsouk Cho,Wonjun Hwang*

Main category: cs.CV

TL;DR: CoMA proposes a collaborative multi-foundation adaptation framework that leverages two complementary foundation models (CLIP and BLIP) for source-free domain adaptation, using bidirectional adaptation and decomposed mutual information to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Single foundation models in SFDA often bias adaptation toward restricted semantic coverage and fail to capture diverse contextual cues under domain shift, limiting their effectiveness.

Method: Jointly leverages two complementary FMs (CLIP and BLIP) with bidirectional adaptation: aligns FMs with target model while maintaining semantic distinctiveness, and transfers complementary knowledge. Uses Decomposed Mutual Information for stable mini-batch training.

Result: Consistently outperforms existing state-of-the-art SFDA methods across Office-31, Office-Home, DomainNet-126, and VisDA benchmarks in closed-set setting, and achieves best results on partial-set and open-set variants.

Conclusion: Collaborative use of multiple foundation models with complementary properties effectively captures both global semantics and local contextual cues, enabling superior source-free domain adaptation performance.

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.

</details>


### [354] [Test-Time Preference Optimization for Image Restoration](https://arxiv.org/abs/2511.19169)
*Bingchen Li,Xin Li,Jiaqi Xu,Jiaming Guo,Wenbo Li,Renjing Pei,Zhibo Chen*

Main category: cs.CV

TL;DR: Proposes TTPO, a test-time preference optimization method that enhances perceptual quality in image restoration by generating preference data on-the-fly and optimizing restored images to align with human preferences without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing IR models often fail to align with human preferences, creating a need for methods that enhance restoration quality and adapt to various tasks without retraining or labor-intensive preference data collection.

Method: Three-stage training-free pipeline: (1) generate candidate preference images using diffusion inversion/denoising, (2) select preferred/dispreferred images using automated metrics or human feedback, (3) use preference images as reward signals to guide diffusion denoising optimization.

Result: Extensive experiments across various IR tasks and models demonstrate the method's effectiveness and flexibility in enhancing perceptual quality.

Conclusion: TTPO is the first test-time preference optimization paradigm for IR that successfully aligns restored images with human preferences while being compatible with any IR model backbone.

Abstract: Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.

</details>


### [355] [MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes](https://arxiv.org/abs/2511.19172)
*Kehua Chen,Tianlu Mao,Zhuxin Ma,Hao Jiang,Zehao Li,Zihan Liu,Shuqi Gao,Honglong Zhao,Feng Dai,Yucheng Zhang,Zhaoqi Wang*

Main category: cs.CV

TL;DR: MetroGS is a novel Gaussian Splatting framework for efficient and robust large-scale urban scene reconstruction, featuring distributed 2D Gaussian representation, structured dense enhancement, progressive geometric optimization, and depth-guided appearance modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving high-quality geometric fidelity efficiently and stably in large-scale scene reconstruction using 3D Gaussian Splatting methods, particularly in complex urban environments.

Method: Uses distributed 2D Gaussian Splatting as backbone; implements structured dense enhancement with SfM priors and pointmap model; employs progressive hybrid geometric optimization combining monocular and multi-view optimization; incorporates depth-guided appearance modeling for 3D-consistent spatial features.

Result: Experiments on large-scale urban datasets show superior geometric accuracy and rendering quality compared to existing methods.

Conclusion: MetroGS provides a unified solution for high-fidelity large-scale scene reconstruction with improved geometric fidelity, completeness, and stability in complex urban environments.

Abstract: Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.

</details>


### [356] [Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification](https://arxiv.org/abs/2511.19180)
*Mansur Ozaman*

Main category: cs.CV

TL;DR: Comparative analysis of three source camera identification methods: PRNU, JPEG compression artifacts, and CNNs, evaluating their device classification accuracy and discussing implementation needs for real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Identifying the source camera of an image is crucial for comprehensive image analysis in computer vision applications.

Method: Comparative evaluation of three techniques: Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs).

Result: The paper evaluates each method's device classification accuracy performance.

Conclusion: Discusses scientific developments needed for implementing these source camera identification methods in real-life scenarios.

Abstract: One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.

</details>


### [357] [nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation](https://arxiv.org/abs/2511.19183)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jaeger,Fabian Isensee,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: nnActive is an open-source Active Learning framework for 3D biomedical imaging that addresses four evaluation pitfalls and reveals that while AL methods outperform standard random sampling, none reliably beat improved foreground-aware random sampling.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation in biomedical imaging requires large annotated datasets, but manual labeling is costly and requires expertise. Active Learning aims to reduce annotation effort by selecting informative samples, but current evaluations have methodological flaws.

Method: Developed nnActive framework that: (1) conducts large-scale study across 4 biomedical datasets and 3 label regimes, (2) extends nnU-Net using partial annotations with 3D patch-based query selection, (3) proposes Foreground Aware Random sampling strategies, and (4) introduces foreground efficiency metric.

Result: (A) AL methods outperform standard random sampling but not improved foreground-aware random sampling; (B) AL benefits are task-specific; (C) Predictive Entropy is best performing but requires most annotation effort; (D) AL performance improves with compute-intensive choices.

Conclusion: nnActive serves as a catalyst for AL research in 3D biomedical imaging, providing a holistic framework that addresses current evaluation limitations and enables more reliable assessment of AL methods.

Abstract: Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive

</details>


### [358] [SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection](https://arxiv.org/abs/2511.19187)
*Nithira Jayarathne,Naveen Basnayake,Keshawa Jayasundara,Pasindu Dodampegama,Praveen Wijesinghe,Hirushika Pelagewatta,Kavishka Abeywardana,Sandushan Ranaweera,Chamira Edussooriya*

Main category: cs.CV

TL;DR: Lightweight EfficientNet-B6 model for deepfake detection using transformation techniques to handle class imbalance, achieving high accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: To combat misinformation by developing accessible deepfake detection tools for non-experts.

Method: Fine-tuned EfficientNet-B6 with transformation techniques, robust preprocessing, oversampling, and optimization strategies to address class imbalance.

Result: Achieved high accuracy, stability, and generalization in deepfake detection, though Fourier transform features showed minimal impact.

Conclusion: The proposed framework enables effective deepfake identification by non-experts, advancing accessible and reliable detection.

Abstract: Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.

</details>


### [359] [Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks](https://arxiv.org/abs/2511.19198)
*Ann-Sophia Müller,Moonkwang Jeong,Meng Zhang,Jiyuan Tian,Arkadiusz Miernik,Stefanie Speidel,Tian Qiu*

Main category: cs.CV

TL;DR: A novel workflow for automated 3D anatomical data generation using physical organ models and 3D GANs to overcome data scarcity in surgical planning and training.


<details>
  <summary>Details</summary>
Motivation: Overcoming the bottleneck of obtaining large amounts of 3D anatomical models from real patients due to legal, ethical, and technical challenges, especially for soft tissue organs like prostate with poor imaging contrast.

Method: Uses physical organ models made of biomimetic hydrogels with multi-zone imaging contrast, placed in customized ultrasound scanner. Neural network segmentation of ultrasound images, 3D mesh reconstruction, and 3D GAN for manifold generation.

Result: Neural network segmentation outperforms conventional computer vision techniques in IoU. Successfully generates 3D models and provides performance feedback for surgical simulation.

Conclusion: The workflow enables automated generation of 3D anatomical data for surgical planning and training, particularly valuable for soft tissue organs where real patient data is difficult to obtain.

Abstract: Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.

</details>


### [360] [CLASH: A Benchmark for Cross-Modal Contradiction Detection](https://arxiv.org/abs/2511.19199)
*Teodora Popordanoska,Jiameng Li,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: CLASH is a new benchmark for detecting contradictions between images and text, addressing the gap in existing multimodal benchmarks that assume input consistency. It uses COCO images with contradictory captions and evaluates models' ability to detect object-level and attribute-level conflicts.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal inputs often contain contradictions, but current benchmarks don't evaluate cross-modal contradiction detection, which is crucial for preventing hallucinations and ensuring reliable multimodal systems.

Method: Created CLASH benchmark with COCO images paired with contradictory captions containing object-level or attribute-level contradictions. Includes multiple-choice and open-ended questions, with automated quality filtering for training data and human-verified diagnostic sets.

Result: Analysis of state-of-the-art models shows significant limitations in recognizing cross-modal conflicts, revealing systematic modality biases and category-specific weaknesses. Fine-tuning on CLASH substantially improves conflict detection capabilities.

Conclusion: CLASH effectively exposes weaknesses in current multimodal models' contradiction detection abilities and demonstrates that targeted training can significantly enhance this crucial capability for reliable multimodal systems.

Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.

</details>


### [361] [Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?](https://arxiv.org/abs/2511.19200)
*Itay Cohen,Ethan Fetaya,Amir Rosenfeld*

Main category: cs.CV

TL;DR: The paper studies whether vision-language models like CLIP can distinguish between real objects and their lookalikes (toys, statues, etc.), finding that a learned direction in CLIP's embedding space improves discrimination and captioning.


<details>
  <summary>Details</summary>
Motivation: To address gaps between AI and human perception, specifically the ability to judge if an image looks like an object without being an instance of it.

Method: Curated a RoLA dataset of real and lookalike exemplars, evaluated prompt-based baselines, and learned a direction in CLIP's embedding space to shift representations between real and lookalike.

Result: The learned direction improved cross-modal retrieval on Conceptual12M and enhanced captions from a CLIP prefix captioner.

Conclusion: CLIP can capture the distinction between real objects and lookalikes through learned embedding directions, bridging a perceptual gap.

Abstract: Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired "real"/"lookalike" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.

</details>


### [362] [NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting](https://arxiv.org/abs/2511.19202)
*Brent Zoomers,Florian Hahlbohm,Joni Vanherck,Lode Jorissen,Marcus Magnor,Nick Michiels*

Main category: cs.CV

TL;DR: A novel method for occlusion culling in 3D Gaussian Splatting using a shared MLP to learn viewpoint-dependent visibility, enabling efficient discarding of occluded primitives during rendering.


<details>
  <summary>Details</summary>
Motivation: The semi-transparent nature of Gaussians prevents the application of occlusion culling, which is a highly effective technique for accelerating rendering of scenes with many primitives.

Method: Use a small, shared MLP across instances to learn viewpoint-dependent visibility function, query it for Gaussians within viewing frustum before rasterization, and integrate neural queries into an instanced software rasterizer leveraging Tensor Cores.

Result: Outperforms current state-of-the-art for composed scenes in VRAM usage and image quality, with complementary properties to existing LoD techniques.

Conclusion: The proposed occlusion culling method successfully addresses the limitation of semi-transparent Gaussians and enables efficient rendering through neural visibility queries.

Abstract: 3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.

</details>


### [363] [ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment](https://arxiv.org/abs/2511.19217)
*Wanjiang Weng,Xiaofeng Tan,Junbo Wang,Guo-Sen Xie,Pan Zhou,Hongsong Wang*

Main category: cs.CV

TL;DR: ReAlign improves text-to-motion generation by using a reward-guided sampling strategy to address misalignment between text and motion distributions in diffusion models.


<details>
  <summary>Details</summary>
Motivation: There exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions in text-to-motion generation.

Method: Proposes Reward-guided sampling Alignment (ReAlign) with a step-aware reward model to assess alignment quality during denoising sampling and a reward-guided strategy that directs diffusion toward optimally aligned distribution.

Result: Extensive experiments show the approach significantly improves text-motion alignment and motion quality compared to state-of-the-art methods in both motion generation and retrieval tasks.

Conclusion: ReAlign effectively addresses the text-motion misalignment problem in diffusion models and generates higher quality, more semantically consistent motions.

Abstract: Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.

</details>


### [364] [Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering](https://arxiv.org/abs/2511.19220)
*Federico Felizzi,Olivia Riccomi,Michele Ferramola,Francesco Andrea Causio,Manuel Del Medico,Vittorio De Vita,Lorenzo De Mori,Alessandra Piscitelli Pietro Eric Risuleo,Bianca Destro Castaniti,Antonio Cristiano Alessia Longo,Luigi De Angelis,Mariapia Vassalli,Marcello Di Pumpo*

Main category: cs.CV

TL;DR: Large VLMs show varying visual dependency on Italian medical questions, with GPT-4o showing strongest visual grounding while others rely more on textual shortcuts.


<details>
  <summary>Details</summary>
Motivation: To investigate whether frontier VLMs genuinely integrate visual information when answering medical questions, as their reliance on visual grounding remains unclear despite impressive benchmark performance.

Method: Tested four state-of-the-art VLMs on 60 Italian medical questions requiring image interpretation, substituting correct medical images with blank placeholders to assess visual dependency.

Result: GPT-4o showed strongest visual grounding with 27.9pp accuracy drop, while GPT-5-mini, Gemini, and Claude maintained high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. All models generated confident explanations for fabricated visual interpretations.

Conclusion: VLMs exhibit critical differences in visual grounding robustness, highlighting the need for rigorous evaluation before clinical deployment due to varying reliance on textual shortcuts versus genuine visual analysis.

Abstract: Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.

</details>


### [365] [Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving](https://arxiv.org/abs/2511.19221)
*Jianhua Han,Meng Tian,Jiangtong Zhu,Fan He,Huixin Zhang,Sitong Guo,Dechang Zhu,Hao Tang,Pei Xu,Yuze Guo,Minzhe Niu,Haojie Zhu,Qichao Dong,Xuechao Yan,Siyuan Dong,Lu Hou,Qingqiu Huang,Xiaosong Jia,Hang Xu*

Main category: cs.CV

TL;DR: Percept-WAM is a perception-enhanced World-Awareness-Action Model that integrates 2D/3D scene understanding in a single VLM, improving spatial grounding and stability in autonomous driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models have weak spatial grounding and understanding, leading to limited perception and localization abilities in autonomous driving, especially in long-tail scenarios and complex interactions.

Method: Unifies 2D/3D perception tasks into World-PV and World-BEV tokens with spatial coordinates and confidence, uses grid-conditioned prediction with IoU-aware scoring and parallel autoregressive decoding, and leverages pretrained VLM parameters.

Result: Achieves 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection, improves planning performance by surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM, and shows strong open-vocabulary and long-tail generalization.

Conclusion: Percept-WAM successfully integrates 2D/3D perception within a single VLM, matching or surpassing classical detectors while retaining general intelligence and improving autonomous driving performance.

Abstract: Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.

</details>


### [366] [Learning Plug-and-play Memory for Guiding Video Diffusion Models](https://arxiv.org/abs/2511.19229)
*Selena Song,Ziming Xu,Zijun Zhang,Kun Zhou,Jiaxian Guo,Lianhui Qin,Biwei Huang*

Main category: cs.CV

TL;DR: DiT-Mem is a plug-and-play memory module for Diffusion Transformer video generation models that injects world knowledge to improve physical rule following and video fidelity.


<details>
  <summary>Details</summary>
Motivation: Current DiT-based video generation models often violate basic physical laws and commonsense dynamics due to lack of explicit world knowledge.

Method: Proposes a learnable memory encoder (DiT-Mem) using stacked 3D CNNs, low/high-pass filters, and self-attention layers to map reference videos into memory tokens, which are concatenated in DiT self-attention layers. Only the memory encoder is trained while keeping the diffusion backbone frozen.

Result: Achieves efficient training with only 150M parameters and 10K data samples, demonstrating improved physical rule following and video fidelity in state-of-the-art models.

Conclusion: DiT-Mem effectively injects world knowledge into video generation models through plug-and-play memory, addressing physical law violations while maintaining training efficiency.

Abstract: Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.

</details>


### [367] [IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2511.19235)
*Carl Lindström,Mahan Rafidashti,Maryam Fatemi,Lars Hammarstrand,Martin R. Oswald,Lennart Svensson*

Main category: cs.CV

TL;DR: IDSplat is a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic driving scenes with explicit instance decomposition and learnable motion trajectories without human annotations, using language-grounded video tracking and coordinated-turn smoothing.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dynamic scene reconstruction either require costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation.

Method: Models dynamic objects as coherent instances undergoing rigid transformations, employs zero-shot language-grounded video tracking anchored to 3D using lidar, estimates consistent poses via feature correspondences, and uses coordinated-turn smoothing for temporally consistent motion trajectories followed by joint optimization.

Result: Achieves competitive reconstruction quality on Waymo Open Dataset while maintaining instance-level decomposition, and generalizes across diverse sequences and view densities without retraining.

Conclusion: IDSplat provides a practical solution for large-scale autonomous driving applications by enabling self-supervised dynamic scene reconstruction with explicit instance decomposition and physically consistent motion trajectories.

Abstract: Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.

</details>


### [368] [Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation](https://arxiv.org/abs/2511.19254)
*Mohamed Rissal Hedna,Sesugh Samuel Nder*

Main category: cs.CV

TL;DR: First study of adversarial patch attacks on cargo-occupancy classifiers using 3D simulation, showing high success rates especially for denial-of-service attacks.


<details>
  <summary>Details</summary>
Motivation: Computer vision systems in logistics are vulnerable to physical adversarial attacks via printable patches that can manipulate cargo occupancy estimation for planning, routing, and billing.

Method: Used Mitsuba 3 for differentiable rendering to optimize patch textures across variations in geometry, lighting, and viewpoint in fully simulated 3D environments, comparing with 2D compositing baseline.

Result: 3D-optimized patches achieved 84.94% success in denial-of-service (empty to full) and 30.32% in concealment attacks (full to empty), significantly outperforming 2D baseline.

Conclusion: Adversarial patches pose serious security risks to automated logistics pipelines, highlighting the need for strengthening physical robustness in computer vision systems.

Abstract: Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.

</details>


### [369] [LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models](https://arxiv.org/abs/2511.19261)
*Shuai Wang,Daoan Zhang,Tianyi Bai,Shitong Shao,Jiebo Luo,Jiaheng Wei*

Main category: cs.CV

TL;DR: LAST improves 3D spatial and long video understanding in vision-language models by enabling them to build visual thinking trajectories in space and time before answering.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with 3D space and long video understanding despite being powerful in typical vision-language tasks, requiring specialized architectures for each domain separately.

Method: LAST enables VLMs to think in space and time by building visual thinking trajectories using only 2D images as inputs, applied in zero-shot prompting of proprietary models and fine-tuning general VLMs.

Result: Substantial gains across 10 benchmarks: 15.8% improvement on EgoSchema with GPT-4o (zero-shot) and 8.3 gains on VSI-Bench compared to Qwen2.5-VL-7B.

Conclusion: LAST effectively improves 3D spatial and temporal understanding in VLMs by enabling visual thinking trajectories, achieving significant performance gains across multiple tasks.

Abstract: Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.

</details>


### [370] [BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment](https://arxiv.org/abs/2511.19268)
*Dewei Zhou,Mingwei Li,Zongxin Yang,Yu Lu,Yunqiu Xu,Zhizhong Wang,Zeyi Huang,Yi Yang*

Main category: cs.CV

TL;DR: BideDPO is a bidirectionally decoupled DPO framework that addresses conflicts in conditional image generation by creating disentangled preference pairs for text and condition signals, using adaptive loss balancing and automated data generation.


<details>
  <summary>Details</summary>
Motivation: Current conditional image generation methods struggle with input-level conflicts (conditioning image contradicts text) and model-bias conflicts (generative biases disrupt alignment), which standard supervised fine-tuning cannot adequately address.

Method: Proposes BideDPO framework with: 1) two disentangled preference pairs for condition and text to reduce gradient entanglement, 2) adaptive loss balancing strategy, 3) automated data pipeline for conflict-aware data generation, and 4) iterative optimization strategy.

Result: Significantly improves text success rates (+35%) and condition adherence on the DualAlign benchmark, with validation on COCO dataset showing effective conflict resolution between text and condition.

Conclusion: BideDPO effectively addresses conflict challenges in conditional image generation through bidirectional decoupling and adaptive optimization, demonstrating substantial improvements in text-condition alignment.

Abstract: Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.

</details>


### [371] [Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection](https://arxiv.org/abs/2511.19274)
*Mingyang Chen,Jiawei Du,Bo Huang,Yi Wang,Xiaobo Zhang,Wei Wang*

Main category: cs.CV

TL;DR: A novel core-set selection method using diffusion models to estimate data likelihood via reconstruction deviation, outperforming existing heuristics and achieving near-full-data performance with only 50% of data.


<details>
  <summary>Details</summary>
Motivation: Existing core-set selection methods rely on heuristic scoring signals without explicit modeling of data likelihood, potentially missing critical distributional structures needed for effective model training.

Method: Leverages diffusion models to estimate data likelihood through reconstruction deviation from partial reverse denoising, with a formal connection to ELBO and an information-theoretic method to identify optimal reconstruction timestep.

Result: Outperforms existing baselines across selection ratios on ImageNet, closely matching full-data training performance using only 50% of the data.

Conclusion: Reconstruction deviation provides an effective, likelihood-informed scoring criterion that reveals insights into data distribution characteristics and model learning preferences.

Abstract: Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.

</details>


### [372] [ReMatch: Boosting Representation through Matching for Multimodal Retrieval](https://arxiv.org/abs/2511.19278)
*Qianying Liu,Xiao Liang,Zhiqiang Zhang,Yibo Chen,Xu Tang,Zhongfei Qing,Fengfan Zhou,Yao Hu,Paul Henderson*

Main category: cs.CV

TL;DR: ReMatch is a framework that uses MLLMs for multimodal retrieval by training them end-to-end with a generative matching stage, achieving state-of-the-art results on MMEB with strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Previous approaches underutilized MLLMs' generative nature, compositional reasoning, and world knowledge by treating them as simple encoders rather than leveraging their full capabilities.

Method: Train embedding MLLM end-to-end with chat-style generative matching stage that autoregressively decides relevance from multi-view inputs; use multiple learnable tokens to generate fine-grained contextual embeddings; combine with contrastive loss for stronger supervision.

Result: Achieved new state-of-the-art on Massive Multimodal Embedding Benchmark (MMEB); showed particularly strong zero-shot generalization on five datasets; demonstrated robustness and transferability.

Conclusion: ReMatch effectively leverages MLLMs' generative capabilities for multimodal retrieval, outperforming previous methods and showing excellent generalization through its novel training approach.

Abstract: We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.

</details>


### [373] [DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting](https://arxiv.org/abs/2511.19294)
*Phurtivilai Patt,Leyang Huang,Yinqiang Zhang,Yang Lei*

Main category: cs.CV

TL;DR: A novel densify beforehand approach for 3D Gaussian Splatting that combines LiDAR and monocular depth to create dense point clouds, eliminating adaptive density control issues and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing 3DGS methods, particularly adaptive density control that causes floating artifacts and inefficient resource usage.

Method: Combine sparse LiDAR data with monocular depth estimation from RGB images using ROI-aware sampling to create dense point clouds before optimization.

Result: Achieves comparable results to state-of-the-art methods while significantly reducing resource consumption and training time, validated on four new datasets.

Conclusion: The densify beforehand approach effectively bypasses adaptive density control issues, improves visual quality, and enhances computational efficiency in 3D scene reconstruction.

Abstract: This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.

</details>


### [374] [IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection](https://arxiv.org/abs/2511.19301)
*Johannes Meier,Florian Günther,Riccardo Marin,Oussema Dhaouadi,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: IDEAL-M3D is an instance-level active learning method for monocular 3D detection that addresses limitations of previous approaches by using diverse ensembles and avoiding bias toward depth-ambiguous objects.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D detection requires expensive 3D annotations, and existing active learning methods are inefficient due to image-level selection and uncertainty bias toward distant objects.

Method: Proposes IDEAL-M3D with instance-level selection, diverse ensembles using heterogeneous backbones, task-agnostic features, loss weight perturbation, and time-dependent bagging.

Result: Achieves similar or better AP3D on KITTI with only 60% of annotations compared to full dataset training, demonstrating significant resource savings.

Conclusion: Instance-level active learning with diverse ensembles is effective for monocular 3D detection, overcoming previous limitations and reducing annotation costs.

Abstract: Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.
  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.

</details>


### [375] [Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection](https://arxiv.org/abs/2511.19306)
*Zixuan Wang,Haoran Sun,Jiaming Lu,Wenxuan Wang,Zhongling Huang,Dingwen Zhang,Xuelin Qian,Junwei Han*

Main category: cs.CV

TL;DR: DGSPNet is a language prompt-driven framework for infrared small target detection that uses dual-granularity semantic prompts and text-guided attention mechanisms to improve detection accuracy without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Current CLIP-inspired methods for infrared small target detection suffer from inaccurate text descriptions and reliance on manual annotations, leading to sub-optimal performance due to limited feature representation and background interference.

Method: Proposes DGSPNet with dual-granularity semantic prompts (coarse-grained textual priors and fine-grained personalized semantic descriptions via visual-to-textual mapping), text-guide channel attention (TGCA), and text-guide spatial attention (TGSA) mechanisms.

Result: Extensive experiments show significant improvement in detection accuracy and state-of-the-art performance on three benchmark datasets.

Conclusion: DGSPNet effectively leverages language prompts for infrared small target detection without annotation requirements, achieving superior performance through dual-granularity semantic prompts and text-guided attention mechanisms.

Abstract: Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.

</details>


### [376] [Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach](https://arxiv.org/abs/2511.19316)
*Xincheng Wang,Hanchi Sun,Wenjun Sun,Kejun Xue,Wangqiu Zhou,Jianbo Zhang,Wei Sun,Dandan Zhu,Xiongkuo Min,Jun Jia,Zhijun Fang*

Main category: cs.CV

TL;DR: This paper establishes a comprehensive evaluation framework for dataset watermarking in diffusion models, revealing vulnerabilities in current methods and proposing a practical watermark removal technique.


<details>
  <summary>Details</summary>
Motivation: Current fine-tuning techniques for diffusion models introduce copyright and security risks, and while dataset watermarking has been proposed for traceability, existing methods lack a unified evaluation framework to assess their effectiveness.

Method: The paper establishes a general threat model and introduces a comprehensive evaluation framework with three key metrics: Universality, Transmissibility, and Robustness. It also proposes a practical watermark removal method that eliminates dataset watermarks without affecting fine-tuning.

Result: Experiments show existing watermarking methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, but fall short under real-world threat scenarios. The proposed removal method successfully eliminates watermarks.

Conclusion: Current dataset watermarking methods have significant vulnerabilities in real-world scenarios, and the proposed removal method highlights a key challenge that needs to be addressed in future research for effective copyright protection.

Abstract: Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.

</details>


### [377] [SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis](https://arxiv.org/abs/2511.19319)
*Lingwei Dang,Zonghan Li,Juntong Li,Hongwen Zhang,Liang An,Yebin Liu,Qingyao Wu*

Main category: cs.CV

TL;DR: SyncMV4D is the first model that jointly generates synchronized multi-view hand-object interaction videos and 4D motions by unifying visual priors, motion dynamics, and multi-view geometry, overcoming limitations of single-view video methods and 3D approaches dependent on controlled lab data.


<details>
  <summary>Details</summary>
Motivation: Current HOI generation methods have limitations: single-view video methods cause geometric distortions and unrealistic motion, while 3D approaches depend on high-quality lab data and lack real-world generalization. There's a need for methods that combine visual realism with accurate 3D geometry.

Method: Two core innovations: (1) Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) Diffusion Points Aligner (DPA) that refines coarse motion into globally aligned 4D metric point tracks. Uses a closed-loop cycle where generated video conditions motion refinement and aligned 4D tracks guide next-step generation.

Result: The method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.

Conclusion: SyncMV4D successfully addresses limitations of existing HOI generation methods by jointly generating synchronized multi-view videos and 4D motions, achieving better visual quality and geometric accuracy through its unified approach.

Abstract: Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.

</details>


### [378] [SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation](https://arxiv.org/abs/2511.19320)
*Jiaming Zhang,Shengming Cao,Rui Li,Xiaotong Zhao,Yutao Cui,Xinglin Hou,Gangshan Wu,Haolan Chen,Yu Xu,Limin Wang,Kai Ma*

Main category: cs.CV

TL;DR: SteadyDancer is an I2V framework that preserves first-frame identity while ensuring precise motion control, addressing spatio-temporal misalignments in human image animation through condition reconciliation, synergistic pose modulation, and staged training.


<details>
  <summary>Details</summary>
Motivation: Existing R2V methods suffer from identity drift and visual artifacts due to spatio-temporal misalignments in real-world applications, failing to preserve first-frame identity while maintaining motion control.

Method: Uses Condition-Reconciliation Mechanism to harmonize conflicting conditions, Synergistic Pose Modulation Modules for adaptive pose representation, and Staged Decoupled-Objective Training Pipeline for hierarchical optimization.

Result: Achieves state-of-the-art performance in appearance fidelity and motion control while requiring significantly fewer training resources than comparable methods.

Conclusion: SteadyDancer successfully addresses fundamental challenges in human image animation by robustly preserving first-frame identity and ensuring precise motion control through its novel I2V paradigm.

Abstract: Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.

</details>


### [379] [MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation](https://arxiv.org/abs/2511.19326)
*Farnoosh Koleini,Hongfei Xue,Ahmed Helmy,Pu Wang*

Main category: cs.CV

TL;DR: MonoMSK is a hybrid framework that combines data-driven learning and physics-based simulation to reconstruct biomechanically realistic 3D human motion from monocular video, jointly recovering both kinematics and kinetics using an anatomically accurate musculoskeletal model.


<details>
  <summary>Details</summary>
Motivation: Current monocular methods use oversimplified, anatomically inaccurate models (like SMPL) and ignore physics, limiting biomechanical fidelity. Marker-based systems are lab-bound and slow, creating a need for accurate physics-aware motion reconstruction from monocular video.

Method: Hybrid framework integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation. Uses a physics-regulated inverse-forward loop and novel forward-inverse consistency loss to enforce biomechanical causality and physical plausibility.

Result: Significantly outperforms state-of-the-art methods in kinematic accuracy on BML-MoVi, BEDLAM, and OpenCap datasets, while enabling precise monocular kinetics estimation for the first time.

Conclusion: MonoMSK successfully bridges data-driven learning and physics-based simulation to achieve biomechanically realistic 3D human motion estimation from monocular video, addressing limitations of existing methods by incorporating anatomical accuracy and physical constraints.

Abstract: Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.

</details>


### [380] [POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse](https://arxiv.org/abs/2511.19339)
*Anjie Le,Can Peng,Yuyuan Liu,J. Alison Noble*

Main category: cs.CV

TL;DR: POUR introduces a provably optimal representation-level unlearning method using geometric projections based on Neural Collapse theory, achieving effective forgetting while preserving retained knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning approaches often only modify classifiers while leaving internal representations intact, resulting in incomplete forgetting of visual concepts.

Method: Developed POUR with two variants: POUR-P (closed-form geometric projection) and POUR-D (feature-level unlearning under distillation), based on the theoretical insight that orthogonal projection of simplex ETFs remains optimal in lower dimensions.

Result: POUR outperforms state-of-the-art unlearning methods on CIFAR-10/100 and PathMNIST datasets, achieving effective unlearning while preserving retained knowledge on both classification-level and representation-level metrics.

Conclusion: Representation-level unlearning is crucial for complete forgetting, and POUR provides a provably optimal solution that effectively balances forgetting efficacy, retention fidelity, and class separation.

Abstract: In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.

</details>


### [381] [Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning](https://arxiv.org/abs/2511.19343)
*Qihan Huang,Haofei Zhang,Rong Wei,Yi Wang,Rui Tang,Mingli Song,Jie Song*

Main category: cs.CV

TL;DR: Syn-GRPO improves MLLM perception by synthesizing high-quality, diverse training data through an online generator and diversity rewards, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for MLLMs suffer from low data quality where samples fail to elicit diverse responses, limiting exploration scope. Current entropy constraints don't address the root problem.

Method: Syn-GRPO uses an online data generator with two components: (1) data server that synthesizes new samples from existing ones using image generation models with decoupled asynchronous scheme, (2) GRPO workflow that provides image descriptions and uses diversity reward to supervise MLLM for diverse response generation.

Result: Experiments across three visual perception tasks show Syn-GRPO significantly improves data quality and achieves superior performance compared to existing MLLM perception methods.

Conclusion: Syn-GRPO presents promising potential for scaling long-term self-evolving reinforcement learning in multimodal language models.

Abstract: RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.

</details>


### [382] [CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting](https://arxiv.org/abs/2511.19351)
*Abdurahman Ali Mohammed,Catherine Fonder,Ying Wei,Wallapak Tavanapong,Donald S Sakaguchi,Qi Li,Surya K. Mallapragada*

Main category: cs.CV

TL;DR: Introduces a large-scale cell counting dataset with 3,023 images and benchmarks various methods, showing that a SAM-based approach (SAM-Counter) achieves state-of-the-art performance with MAE of 22.12.


<details>
  <summary>Details</summary>
Motivation: Manual cell counting is labor-intensive and error-prone, while existing datasets are too small (often <500 images) to train reliable deep learning models for automated cell counting in biomedical applications.

Method: Created a large annotated dataset of 3,023 immunocytochemistry images with over 430,000 cell locations. Benchmarked regression-based, crowd-counting, and cell-counting methods, and adapted Segment Anything Model (SAM) for cell counting using dot annotations.

Result: SAM-Counter (density-map-based adaptation of SAM) achieved MAE of 22.12, outperforming existing approaches (second-best MAE of 27.46). The dataset presents challenges including high cell density, overlapping cells, and long-tailed distribution.

Conclusion: The large-scale dataset and benchmarking framework provide a robust foundation for advancing automated cell counting, with SAM adaptation showing promising results for this challenging biomedical task.

Abstract: Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.

</details>


### [383] [Growing with the Generator: Self-paced GRPO for Video Generation](https://arxiv.org/abs/2511.19356)
*Rui Li,Yuanzhi Liang,Ziqi Ni,Haibing Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Self-Paced GRPO introduces a progressive reward mechanism that co-evolves with the generator, shifting from visual fidelity to temporal coherence and semantic alignment as quality improves, outperforming static-reward GRPO methods.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO pipelines use static reward models that introduce distributional bias, saturate quickly, and limit reinforcement learning effectiveness due to rigid rewards that don't adapt to generator improvements.

Method: Self-Paced GRPO framework with progressive reward mechanism that automatically shifts emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases.

Result: Experiments on VBench across multiple video generation backbones show consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards.

Conclusion: Self-Paced GRPO effectively addresses reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization compared to static-reward approaches.

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.

</details>


### [384] [DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation](https://arxiv.org/abs/2511.19365)
*Zehong Ma,Longhui Wei,Shuai Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: DeCo proposes a frequency-decoupled pixel diffusion framework that separates high-frequency detail generation from low-frequency semantic modeling using a lightweight pixel decoder and DiT, achieving state-of-the-art performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Existing pixel diffusion models suffer from slow training and inference because they model both high-frequency signals and low-frequency semantics within a single diffusion transformer, limiting efficiency.

Method: The framework decouples frequency components by using a lightweight pixel decoder for high-frequency details conditioned on semantic guidance from DiT, which specializes in low-frequency semantics. Also introduces a frequency-aware flow-matching loss.

Result: Achieves FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Text-to-image model achieves overall score of 0.86 on GenEval.

Conclusion: DeCo demonstrates superior performance among pixel diffusion models by efficiently decoupling frequency components, making pixel diffusion more competitive with latent diffusion approaches.

Abstract: Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.

</details>


### [385] [An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification](https://arxiv.org/abs/2511.19367)
*Saniah Kayenat Chowdhury,Rusab Sarmun,Muhammad E. H. Chowdhury,Sohaib Bassam Zoghoul,Israa Al-Hashimi,Adam Mushtak,Amith Khandakar*

Main category: cs.CV

TL;DR: A medically grounded hybrid pipeline for lung cancer tumor staging that combines segmentation networks with rule-based classification, achieving 91.36% accuracy by explicitly measuring tumor size and distances to anatomical structures.


<details>
  <summary>Details</summary>
Motivation: End-to-end deep learning approaches for lung cancer staging often overlook crucial spatial and anatomical information central to the tumor-node-metastasis system, and small variations in tumor properties can significantly alter staging outcomes.

Method: Uses specialized encoder-decoder networks to segment lung anatomy (lobes, tumor, mediastinum, diaphragm), extracts tumor size and distance properties from masks, then applies rule-based staging aligned with medical guidelines.

Result: Achieved 91.36% overall accuracy on Lung-PET-CT-Dx dataset with per-stage F1-scores: 0.93 (T1), 0.89 (T2), 0.96 (T3), 0.90 (T4), outperforming traditional deep learning models.

Conclusion: First study to embed explicit clinical context into tumor stage classification, providing both state-of-the-art performance and transparent decision support compared to black-box CNN approaches.

Abstract: Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable "black box" manner, our method offers both state-of-the-art performance and transparent decision support.

</details>


### [386] [UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval](https://arxiv.org/abs/2511.19380)
*Maroun Ayli,Youssef Bakouny,Tushar Sharma,Nader Jalloul,Hani Seifeddine,Rima Kilany*

Main category: cs.CV

TL;DR: A graph-based representation for UI screenshots that captures structural properties, combined with a contrastive graph autoencoder for embeddings, enabling multi-modal search with high accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: Enterprise software companies face challenges in maintaining design consistency and pattern discovery across thousands of UI screens, as existing methods lack explicit modeling of structural properties.

Method: Convert UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, then use a contrastive graph autoencoder to learn embeddings preserving multi-level similarity across visual, structural, and semantic properties.

Result: UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency on 20,396 financial software UIs, scaling to 20,000+ screens and enabling fine-grained UI distinction impossible with vision-only approaches.

Conclusion: The structural embeddings provide better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in UI representation expressiveness and enabling complex queries through a hybrid indexing architecture.

Abstract: Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.

</details>


### [387] [BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation](https://arxiv.org/abs/2511.19394)
*Rachit Saluja,Asli Cihangir,Ruining Deng,Johannes C. Paetzold,Fengbei Liu,Mert R. Sabuncu*

Main category: cs.CV

TL;DR: BackSplit improves small lesion segmentation by subdividing the background into fine-grained anatomical classes rather than treating it as a single class, leading to better performance without increasing inference costs.


<details>
  <summary>Details</summary>
Motivation: Traditional lesion segmentation treats all non-lesion pixels as a single background class, ignoring the rich anatomical context. The background is actually heterogeneous, composed of various tissues and organs that can be labeled.

Method: Proposes BackSplit - training with fine-grained labels that sub-divide the background class into anatomical structures. Uses auxiliary labels from pretrained segmentation models or interactive frameworks.

Result: Consistently boosts small-lesion segmentation performance across multiple datasets and architectures. Works even with automatically generated auxiliary labels.

Conclusion: BackSplit is a simple yet powerful paradigm that significantly improves small lesion segmentation by leveraging fine-grained background modeling, with proven theoretical benefits and broad applicability.

Abstract: Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single "background" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.
  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.

</details>


### [388] [In-Video Instructions: Visual Signals as Generative Control](https://arxiv.org/abs/2511.19401)
*Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CV

TL;DR: Video generative models can interpret visual instructions embedded in frames (In-Video Instruction) for controllable image-to-video generation, enabling explicit spatial control over multiple objects' actions.


<details>
  <summary>Details</summary>
Motivation: To enable more precise and spatial-aware control in video generation beyond coarse text prompts, by leveraging visual signals embedded directly in frames as instructions.

Method: In-Video Instruction paradigm that encodes user guidance through visual elements like overlaid text, arrows, or trajectories, allowing distinct instructions for different objects in the same scene.

Result: Extensive experiments on Veo 3.1, Kling 2.5, and Wan 2.2 show video models can reliably interpret and execute visually embedded instructions, especially in complex multi-object scenarios.

Conclusion: Video generative models can effectively interpret visual instructions embedded in frames, providing a more explicit and spatial-aware alternative to text-based control for multi-object video generation.

Abstract: Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.

</details>


### [389] [Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens](https://arxiv.org/abs/2511.19418)
*Yiming Qin,Bomin Wei,Jiaxin Ge,Konstantinos Kallidromitis,Stephanie Fu,Trevor Darrell,Xudong Wang*

Main category: cs.CV

TL;DR: Chain-of-Visual-Thought (COVT) enables VLMs to reason through continuous visual tokens, improving perceptual understanding and spatial reasoning by 3-16% across diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with dense visual perception tasks like spatial reasoning and geometric awareness due to limited mechanisms to capture dense visual information across spatial dimensions.

Method: COVT framework uses compact visual tokens (∼20 tokens) that encode rich perceptual cues from lightweight vision experts, including 2D appearance, 3D geometry, spatial layout, and edge structure. The VLM is trained to autoregressively predict these visual tokens to reconstruct dense supervision signals.

Result: Integration of COVT into strong VLMs (Qwen2.5-VL and LLaVA) consistently improves performance by 3% to 16% across more than ten diverse perception benchmarks including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench.

Conclusion: Compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence, preserving efficiency while optionally decoding dense predictions for interpretability.

Abstract: Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.

</details>


### [390] [SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation](https://arxiv.org/abs/2511.19425)
*Tianrun Chen,Runlong Cao,Xinda Yu,Lanyun Zhu,Chaotao Ding,Deyi Ji,Cheng Chen,Qi Zhu,Chunyan Xu,Papa Mao,Ying Zang*

Main category: cs.CV

TL;DR: SAM3-Adapter is the first adapter framework for Segment Anything 3 (SAM3) that enhances its segmentation capabilities for fine-grained tasks like medical imaging, camouflaged object detection, and shadow detection, achieving state-of-the-art performance with reduced computational overhead.


<details>
  <summary>Details</summary>
Motivation: Previous SAM generations struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, and shadow detection. The emergence of SAM3's improved architecture and training pipeline provides an opportunity to address these limitations more effectively.

Method: Proposes SAM3-Adapter, an adapter framework specifically designed for SAM3 that follows the modular and composable design philosophy of the original SAM-Adapter. It reduces computational overhead while enhancing segmentation capabilities.

Result: SAM3-Adapter consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks including medical imaging, camouflaged object segmentation, and shadow detection. It provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision.

Conclusion: SAM3-Adapter unlocks SAM3's full segmentation capability with superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations, serving as a foundation for future research and practical segmentation applications.

Abstract: The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.

</details>


### [391] [Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction](https://arxiv.org/abs/2511.19426)
*Yun Zhou,Yaoting Wang,Guangquan Jie,Jinyu Liu,Henghui Ding*

Main category: cs.CV

TL;DR: Ref-SAM3D extends SAM3D by adding text-guided 3D reconstruction from single RGB images using natural language descriptions as priors.


<details>
  <summary>Details</summary>
Motivation: SAM3D lacks the ability to reconstruct specific objects based on textual descriptions, which is essential for practical applications like 3D editing, game development, and virtual environments.

Method: Extends SAM3D by incorporating textual descriptions as high-level priors to enable text-guided 3D reconstruction from a single RGB image.

Result: Ref-SAM3D delivers competitive and high-fidelity zero-shot reconstruction performance using only natural language and a single 2D view.

Conclusion: Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction.

Abstract: SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.

</details>


### [392] [Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution](https://arxiv.org/abs/2511.19430)
*Dingkang Liang,Cheng Zhang,Xiaopeng Xu,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: ORS3D is a new embodied AI task that combines language understanding, 3D spatial grounding, and operations research optimization to minimize task completion time through parallel subtask scheduling.


<details>
  <summary>Details</summary>
Motivation: Existing embodied AI datasets oversimplify task planning by ignoring operations research knowledge and 3D spatial grounding, limiting real-world efficiency.

Method: Created ORS3D-60K dataset with 60K composite tasks across 4K real-world scenes, and proposed GRANT model with scheduling token mechanism for efficient task scheduling and grounded actions.

Result: Extensive experiments on ORS3D-60K validated GRANT's effectiveness across language understanding, 3D grounding, and scheduling efficiency metrics.

Conclusion: ORS3D enables more realistic embodied AI task scheduling by integrating OR knowledge and 3D grounding, with GRANT demonstrating strong performance on this challenging task.

Abstract: Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT

</details>


### [393] [Cloud4D](https://arxiv.org/abs/2511.19431)
*Jacob Lin,Edward Gryspeerdt,Ronald Clark*

Main category: cs.CV

TL;DR: Cloud4D is a learning-based framework that reconstructs 4D cloud states using ground-based cameras, achieving 25m spatial and 5s temporal resolution with single-digit error against radar measurements.


<details>
  <summary>Details</summary>
Motivation: Current global weather models operate at kilometer-scale resolution, making it difficult to model individual clouds and extreme weather phenomena. High-resolution observations are needed but current instruments struggle to provide them.

Method: Uses synchronized ground-based cameras with a homography-guided 2D-to-3D transformer to infer the full 3D distribution of liquid water content, and tracks these retrievals over time to estimate horizontal wind vectors.

Result: Achieves order-of-magnitude improvement in space-time resolution compared to state-of-the-art satellite measurements, with less than 10% relative error against collocated radar measurements across a two-month deployment with six cameras.

Conclusion: Cloud4D demonstrates the feasibility of using ground-based camera networks for high-resolution 4D cloud reconstruction, providing physically consistent cloud states at unprecedented resolution for weather and climate modeling.

Abstract: There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.

</details>


### [394] [Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts](https://arxiv.org/abs/2511.19434)
*Yasin Esfandiari,Stefan Bauer,Sebastian U. Stich,Andrea Dittadi*

Main category: cs.CV

TL;DR: A plug-and-play sampling method that combines two pretrained diffusion experts by switching between them during denoising to break the trade-off between perceptual quality and data likelihood.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face a trade-off between perceptual sample quality and data likelihood - quality-focused training yields realistic images but poor likelihoods, while likelihood-focused training harms visual fidelity.

Method: Combine two pretrained diffusion experts by switching between them along the denoising trajectory: use image-quality expert at high noise levels for global structure, then switch to likelihood expert at low noise levels for pixel refinement.

Result: On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms base components, improving or preserving both likelihood and sample quality relative to each expert alone.

Conclusion: Expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models without requiring retraining or fine-tuning.

Abstract: Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.

</details>


### [395] [Are Image-to-Video Models Good Zero-Shot Image Editors?](https://arxiv.org/abs/2511.19435)
*Zechuan Zhang,Zhenyuan Chen,Zongxin Yang,Yi Yang*

Main category: cs.CV

TL;DR: IF-Edit is a tuning-free framework that repurposes pretrained video diffusion models for instruction-driven image editing, addressing prompt misalignment, temporal redundancy, and blurry frames through prompt enhancement, latent compression, and post-refinement.


<details>
  <summary>Details</summary>
Motivation: Large-scale video diffusion models have strong world simulation capabilities but their potential as zero-shot image editors remains underexplored, creating an opportunity to leverage these models for instruction-based image editing without additional training.

Method: Three key components: (1) chain-of-thought prompt enhancement to transform static instructions into temporally grounded reasoning prompts, (2) temporal latent dropout to compress frame latents after expert-switch point for faster denoising, and (3) self-consistent post-refinement using short still-video trajectories to sharpen late-stage frames.

Result: Experiments on four public benchmarks show strong performance on reasoning-centric tasks (non-rigid editing, physical/temporal reasoning) while remaining competitive on general-purpose instruction edits.

Conclusion: The study provides a systematic approach for using video diffusion models as image editors and demonstrates a simple recipe for unified video-image generative reasoning.

Abstract: Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.

</details>


### [396] [VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection](https://arxiv.org/abs/2511.19436)
*Qiang Wang,Xinyuan Gao,SongLin Dong,Jizhou Han,Jiangyang Li,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: VDC-Agent is a self-evolving framework for video captioning that creates training data without human annotations or teacher models through a closed-loop process of caption generation, scoring, and refinement.


<details>
  <summary>Details</summary>
Motivation: To develop a video captioning system that can improve itself automatically without relying on expensive human annotations or larger teacher models, enabling scalable and cost-effective training.

Method: Uses a closed-loop system with caption generation, principle-guided scoring with textual suggestions, prompt refinement, and self-reflection. Converts generated trajectories into preference tuples for fine-tuning using curriculum direct preference optimization.

Result: VDC-Agent-7B achieves state-of-the-art performance on VDC benchmark with 49.08% average accuracy and 2.50 score, improving over base model by +5.13% accuracy and +0.27 score at similar inference cost.

Conclusion: The self-evolving framework successfully creates high-quality training data automatically and enables significant performance improvements in video captioning without human supervision or external models.

Abstract: We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.

</details>


### [397] [LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context](https://arxiv.org/abs/2511.19437)
*Jingzhi Bao,Hongze Chen,Lingting Zhu,Chenyu Liu,Runze Zhang,Keyang Luo,Zeyu Hu,Weikai Chen,Yingda Yin,Xin Wang,Zehong Lin,Jun Zhang,Xiaoguang Han*

Main category: cs.CV

TL;DR: LumiTex is an end-to-end framework for generating high-quality PBR textures that addresses material decomposition under limited illumination and ensures seamless, view-consistent texture completion through multi-branch generation, lighting-aware attention, and geometry-guided inpainting.


<details>
  <summary>Details</summary>
Motivation: Existing PBR texture generation methods fail to handle two key challenges: materials decomposition from image prompts with limited illumination cues, and achieving seamless, view-consistent texture completion.

Method: LumiTex uses a three-component approach: (1) multi-branch generation to disentangle albedo and metallic-roughness with shared illumination priors, (2) lighting-aware material attention for physically grounded map generation, and (3) geometry-guided inpainting for seamless UV completion.

Result: Extensive experiments show LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.

Conclusion: LumiTex provides an effective solution for high-quality PBR texture generation by addressing fundamental challenges in material decomposition and seamless texture completion through its integrated framework.

Abstract: Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [398] [Robust Detection of Retinal Neovascularization in Widefield Optical Coherence Tomography](https://arxiv.org/abs/2511.17744)
*Jinyi Hao,Jie Wang,Kotaro Tsuboi,Liqin Gao,Tristan T. Hormel,Yukun Guo,An-Lun Wu,Min Gao,Christina J. Flaxel,Steven T. Bailey,Thomas S. Hwang,Yali Jia*

Main category: eess.IV

TL;DR: A deep learning approach for automated detection and segmentation of retinal neovascularization in widefield OCTA images, achieving high diagnostic accuracy (AUC 0.96-0.99) and segmentation performance (IOU 0.76-0.88).


<details>
  <summary>Details</summary>
Motivation: Retinal neovascularization (RNV) in diabetic retinopathy causes preventable vision loss, but existing algorithms are optimized for narrow field OCTA views, not the new widefield technology that could improve early detection.

Method: Reframes RNV identification as direct binary localization task instead of multi-layer retinal segmentation. Fully automated deep learning model trained on 589 widefield OCT/OCTA scans from multiple devices and clinics.

Result: Achieved device-dependent AUC of 0.96-0.99 for RNV diagnosis and mean IOU of 0.76-0.88 for segmentation. Demonstrated ability to monitor lesion growth longitudinally.

Conclusion: Deep learning-based analysis of widefield OCTA images offers valuable means for improving RNV screening and management in clinical practice.

Abstract: Retinal neovascularization (RNV) is a vision threatening development in diabetic retinopathy (DR). Vision loss associated with RNV is preventable with timely intervention, making RNV clinical screening and monitoring a priority. Optical coherence tomography (OCT) angiography (OCTA) provides high-resolution imaging and high-sensitivity detection of RNV lesions. With recent commercial devices introducing widefield OCTA imaging to the clinic, the technology stands to improve early detection of RNV pathology. However, to meet clinical requirements these imaging capabilities must be combined with effective RNV detection and quantification, but existing algorithms for OCTA images are optimized for conventional, i.e. narrow, fields of view. Here, we present a novel approach for RNV diagnosis and staging on widefield OCT/OCTA. Unlike conventional methods dependent on multi-layer retinal segmentation, our model reframes RNV identification as a direct binary localization task. Our fully automated approach was trained and validated on 589 widefield scans (17x17-mm to 26x21-mm) collected from multiple devices at multiple clinics. Our method achieved a device-dependent area under curve (AUC) ranging from 0.96 to 0.99 for RNV diagnosis, and mean intersection over union (IOU) ranging from 0.76 to 0.88 for segmentation. We also demonstrate our method's ability to monitor lesion growth longitudinally. Our results indicate that deep learning-based analysis for widefield OCTA images could offer a valuable means for improving RNV screening and management.

</details>


### [399] [Spectral Super-Resolution Neural Operator with Atmospheric Radiative Transfer Prior](https://arxiv.org/abs/2511.17895)
*Ziye Zhang,Bin Pan,Zhenwei Shi*

Main category: eess.IV

TL;DR: SSRNO is a spectral super-resolution method that integrates atmospheric radiative transfer priors with neural operators to reconstruct hyperspectral images from multispectral data, achieving physically consistent results through upsampling, reconstruction, and refinement stages.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven spectral super-resolution methods often ignore physical principles, leading to unrealistic spectra especially in atmosphere-affected bands, which limits their practical applications in remote sensing.

Method: Three-stage framework: 1) Upsampling using prior information to generate initial hyperspectral estimate, 2) Neural operator for continuous spectral mapping, 3) Refinement with hard constraints to eliminate color distortion. Uses guidance matrix projection and U-shaped spectral-aware convolution layers.

Result: SSRNO achieves physically consistent spectral reconstruction, enables continuous spectral reconstruction and zero-shot extrapolation, and demonstrates effectiveness and generalization ability in various experiments.

Conclusion: Incorporating atmospheric radiative transfer priors into data-driven neural operators produces more physically realistic hyperspectral image reconstruction while maintaining the advantages of data-driven approaches.

Abstract: Spectral super-resolution (SSR) aims to reconstruct hyperspectral images (HSIs) from multispectral observations, with broad applications in remote sensing. Data-driven methods are widely used, but they often overlook physical principles, leading to unrealistic spectra, particularly in atmosphere-affected bands. To address this challenge, we propose the Spectral Super-Resolution Neural Operator (SSRNO), which incorporates atmospheric radiative transfer (ART) prior into the data-driven procedure, yielding more physically consistent predictions. The proposed SSRNO framework consists of three stages: upsampling, reconstruction, and refinement. In the upsampling stage, we leverage prior information to expand the input multispectral image, producing a physically plausible hyperspectral estimate. Subsequently, we utilize a neural operator in the reconstruction stage to learn a continuous mapping across the spectral domain. Finally, the refinement stage imposes a hard constraint on the output HSI to eliminate color distortion. The upsampling and refinement stages are implemented via the proposed guidance matrix projection (GMP) method, and the reconstruction neural operator adopts U-shaped spectral-aware convolution (SAC) layers to capture multi-scale features. Moreover, we theoretically demonstrate the optimality of the GMP method. With the neural operator and ART priors, SSRNO also achieves continuous spectral reconstruction and zero-shot extrapolation. Various experiments validate the effectiveness and generalization ability of the proposed approach.

</details>


### [400] [Linear Algebraic Approaches to Neuroimaging Data Compression: A Comparative Analysis of Matrix and Tensor Decomposition Methods for High-Dimensional Medical Images](https://arxiv.org/abs/2511.18197)
*Jaeho Kim,Daniel David,Ana Vizitiv*

Main category: eess.IV

TL;DR: Tucker decomposition outperforms SVD for neuroimaging data compression by better preserving multi-dimensional relationships and reconstruction fidelity.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare Tucker decomposition and SVD for compressing neuroimaging data, focusing on their ability to preserve structural and temporal relationships.

Method: Used Tucker decomposition and Singular Value Decomposition (SVD) methods to compress neuroimaging data and compared their performance.

Result: Tucker decomposition preserved multi-dimensional relationships better, achieving superior reconstruction fidelity and perceptual similarity. SVD performed well in extreme compression scenarios but sacrificed fidelity.

Conclusion: Tucker decomposition is more suitable for neuroimaging applications that require preservation of structural and temporal relationships in the data.

Abstract: This paper evaluates Tucker decomposition and Singular Value Decomposition (SVD) for compressing neuroimaging data. Tucker decomposition preserves multi-dimensional relationships, achieving superior reconstruction fidelity and perceptual similarity. SVD excels in extreme compression but sacrifices fidelity. The results highlight Tucker decomposition's suitability for applications requiring the preservation of structural and temporal relationships.

</details>


### [401] [Shape-Adapting Gated Experts: Dynamic Expert Routing for Colonoscopic Lesion Segmentation](https://arxiv.org/abs/2511.18493)
*Gia Huy Thai,Hoang-Nguyen Vu,Anh-Minh Phan,Quang-Thinh Ly,Tram Dinh,Thi-Ngoc-Truc Nguyen,Nhat Ho*

Main category: eess.IV

TL;DR: SAGE is an input-adaptive framework that enables dynamic expert routing in visual networks to handle cellular heterogeneity in cancer detection on Whole Slide Images, achieving state-of-the-art segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Address cellular heterogeneity in cancer detection on gigapixel WSIs where existing CNN-Transformer hybrids use static computation graphs causing redundant computation and limiting adaptability to input variability.

Method: SAGE framework with dual-path design: backbone stream preserves representation while expert path is selectively activated through hierarchical gating. Shape-Adapting Hub (SA-Hub) bridges CNN and Transformer modules. Uses two-level hierarchical selection between shared and specialized experts with Top-K activation.

Result: Achieves superior segmentation on three medical benchmarks: EBHI (95.57% Dice), DigestPath (95.16% Dice), and GlaS (94.17% Dice), with robust cross-domain generalization by adaptively balancing local refinement and global context.

Conclusion: SAGE provides a scalable foundation for dynamic expert routing, enabling flexible visual reasoning and effectively handling cellular heterogeneity in medical image analysis.

Abstract: The substantial diversity in cell scale and form remains a primary challenge in computer-aided cancer detection on gigapixel Whole Slide Images (WSIs), attributable to cellular heterogeneity. Existing CNN-Transformer hybrids rely on static computation graphs with fixed routing, which consequently causes redundant computation and limits their adaptability to input variability. We propose Shape-Adapting Gated Experts (SAGE), an input-adaptive framework that enables dynamic expert routing in heterogeneous visual networks. SAGE reconfigures static backbones into dynamically routed expert architectures. SAGE's dual-path design features a backbone stream that preserves representation and selectively activates an expert path through hierarchical gating. This gating mechanism operates at multiple hierarchical levels, performing a two-level, hierarchical selection between shared and specialized experts to modulate model logits for Top-K activation. Our Shape-Adapting Hub (SA-Hub) harmonizes structural and semantic representations across the CNN and the Transformer module, effectively bridging diverse modules. Embodied as SAGE-UNet, our model achieves superior segmentation on three medical benchmarks: EBHI, DigestPath, and GlaS, yielding state-of-the-art Dice Scores of 95.57%, 95.16%, and 94.17%, respectively, and robustly generalizes across domains by adaptively balancing local refinement and global context. SAGE provides a scalable foundation for dynamic expert routing, enabling flexible visual reasoning.

</details>


### [402] [Neural B-Frame Coding: Tackling Domain Shift Issues with Lightweight Online Motion Resolution Adaptation](https://arxiv.org/abs/2511.18724)
*Sang NguyenQuang,Xiem HoangVan,Wen-Hsiao Peng*

Main category: eess.IV

TL;DR: Lightweight classifiers predict optimal downsampling factors for B-frame video codecs to handle large motion efficiently, achieving comparable performance to exhaustive search with much lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Hierarchical B-frame codecs suffer from domain-shift issues due to mismatched GOP sizes between training and testing, causing inaccurate motion estimation for large motion. Current solutions require costly rate-distortion optimization to determine optimal downsampling factors.

Method: Proposes three lightweight classifier variants: (1) Bi-Class - binary classifier using Focal Loss for high/low resolution selection, (2) Mu-Class - multi-class classifier with soft labels based on rate-distortion costs, (3) Co-Class - combines multi-class prediction with binary selective search. All work with existing codecs without retraining.

Result: Experimental results show the classifiers achieve coding performance comparable to exhaustive search methods while significantly reducing computational complexity.

Conclusion: The proposed lightweight classifier approach effectively balances rate-distortion performance with computational efficiency for B-frame video coding, providing a practical solution to the domain-shift problem without requiring codec modifications.

Abstract: Learned B-frame codecs with hierarchical temporal prediction often encounter the domain-shift issue due to mismatches between the Group-of-Pictures (GOP) sizes for training and testing, leading to inaccurate motion estimates, particularly for large motion. A common solution is to turn large motion into small motion by downsampling video frames during motion estimation. However, determining the optimal downsampling factor typically requires costly rate-distortion optimization. This work introduces lightweight classifiers to predict downsampling factors. These classifiers leverage simple state signals from current and reference frames to balance rate-distortion performance with computational cost. Three variants are proposed: (1) a binary classifier (Bi-Class) trained with Focal Loss to choose between high and low resolutions, (2) a multi-class classifier (Mu-Class) trained with novel soft labels based on rate-distortion costs, and (3) a co-class approach (Co-Class) that combines the predictive capability of the multi-class classifier with the selective search of the binary classifier. All classifier methods can work seamlessly with existing B-frame codecs without requiring codec retraining. Experimental results show that they achieve coding performance comparable to exhaustive search methods while significantly reducing computational complexity. The code is available at: https://github.com/NYCU-MAPL/Fast-OMRA.git.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [403] [Classification EM-PCA for clustering and embedding](https://arxiv.org/abs/2511.18992)
*Zineddine Tighidet,Lazhar Labiod,Mohamed Nadif*

Main category: stat.ML

TL;DR: Proposes a simultaneous PCA-CEM algorithm for fast clustering and dimensionality reduction, addressing EM's slow convergence and high-dimensional challenges.


<details>
  <summary>Details</summary>
Motivation: Gaussian mixture models with EM are slow to converge and struggle with high-dimensional data. CEM offers faster convergence but dimensionality reduction remains problematic.

Method: Combines PCA for dimensionality reduction and CEM for clustering simultaneously and non-sequentially, rather than as separate sequential steps.

Result: Demonstrates improved clustering performance and effective data embedding, with connections established to other clustering approaches.

Conclusion: Simultaneous PCA-CEM integration provides an effective solution for fast clustering in high-dimensional spaces while maintaining good data embedding properties.

Abstract: The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [404] [Beyond the Rubric: Cultural Misalignment in LLM Benchmarks for Sexual and Reproductive Health](https://arxiv.org/abs/2511.17554)
*Sumon Kanti Dey,Manvi S,Zeel Mehta,Meet Shah,Unnati Agrawal,Suhani Jalota,Azra Ismail*

Main category: cs.CY

TL;DR: Current LLM health benchmarks show Western bias, failing to accurately evaluate culturally appropriate health responses for Global South communities.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Western-centric health benchmarks in evaluating LLMs for underserved communities in the Global South, particularly for sexual and reproductive health.

Method: Evaluated 330 single-turn SRH conversations using HealthBench benchmark, comparing automated grading with qualitative analysis by trained annotators and public health experts.

Result: Automated grading consistently rated responses low, but qualitative analysis revealed many were culturally appropriate and medically accurate, exposing Western biases in legal norms, diet assumptions, and cost frameworks.

Conclusion: Current benchmarks are inadequate for diverse cultural contexts; culturally adaptive evaluation frameworks are needed that maintain quality standards while recognizing population-specific needs.

Abstract: Large Language Models (LLMs) have been positioned as having the potential to expand access to health information in the Global South, yet their evaluation remains heavily dependent on benchmarks designed around Western norms. We present insights from a preliminary benchmarking exercise with a chatbot for sexual and reproductive health (SRH) for an underserved community in India. We evaluated using HealthBench, a benchmark for conversational health models by OpenAI. We extracted 637 SRH queries from the dataset and evaluated on the 330 single-turn conversations. Responses were evaluated using HealthBench's rubric-based automated grader, which rated responses consistently low. However, qualitative analysis by trained annotators and public health experts revealed that many responses were actually culturally appropriate and medically accurate. We highlight recurring issues, particularly a Western bias, such as for legal framing and norms (e.g., breastfeeding in public), diet assumptions (e.g., fish safe to eat during pregnancy), and costs (e.g., insurance models). Our findings demonstrate the limitations of current benchmarks in capturing the effectiveness of systems built for different cultural and healthcare contexts. We argue for the development of culturally adaptive evaluation frameworks that meet quality standards while recognizing needs of diverse populations.

</details>


### [405] [A Cross-Cultural Assessment of Human Ability to Detect LLM-Generated Fake News about South Africa](https://arxiv.org/abs/2511.17682)
*Tim Schlippe,Matthias Wölfel,Koena Ronny Mabokela*

Main category: cs.CY

TL;DR: South Africans were better at detecting true South African news but worse at identifying AI-generated fake news compared to other nationalities, showing cultural proximity creates asymmetric detection patterns.


<details>
  <summary>Details</summary>
Motivation: Understanding how cultural proximity affects human ability to detect AI-generated fake news across different cultural contexts, as LLMs enable sophisticated fake news creation.

Method: Survey with 89 participants (56 South Africans, 33 others) evaluating 10 true South African news articles and 10 AI-generated fake versions.

Result: South Africans performed better detecting true news (40% deviation from ideal) vs others (52%), but worse identifying fake news (62% vs 55%). South Africans used content knowledge while others focused on linguistic features.

Conclusion: Cultural familiarity aids verification of authentic information but may introduce bias when evaluating fabricated content, highlighting cross-cultural dimensions of misinformation detection.

Abstract: This study investigates how cultural proximity affects the ability to detect AI-generated fake news by comparing South African participants with those from other nationalities. As large language models increasingly enable the creation of sophisticated fake news, understanding human detection capabilities becomes crucial, particularly across different cultural contexts. We conducted a survey where 89 participants (56 South Africans, 33 from other nationalities) evaluated 10 true South African news articles and 10 AI-generated fake versions. Results reveal an asymmetric pattern: South Africans demonstrated superior performance in detecting true news about their country (40% deviation from ideal rating) compared to other participants (52%), but performed worse at identifying fake news (62% vs. 55%). This difference may reflect South Africans' higher overall trust in news sources. Our analysis further shows that South Africans relied more on content knowledge and contextual understanding when judging credibility, while participants from other countries emphasised formal linguistic features such as grammar and structure. Overall, the deviation from ideal rating was similar between groups (51% vs. 53%), suggesting that cultural familiarity appears to aid verification of authentic information but may also introduce bias when evaluating fabricated content. These insights contribute to understanding cross-cultural dimensions of misinformation detection and inform strategies for combating AI-generated fake news in increasingly globalised information ecosystems where content crosses cultural and geographical boundaries.

</details>


### [406] [Animated Territorial Data Extractor (ATDE): A Computer-Vision Method for Extracting Territorial Data from Animated Historical Maps](https://arxiv.org/abs/2511.17920)
*Hamza Alshamy,Isaiah Woram,Advay Mishra,Zihan Xia,Pascal Wallisch*

Main category: cs.CY

TL;DR: ATDE is a computer vision tool that extracts territorial data from animated historical map videos using color segmentation and filtering techniques, converting them into structured time-series data.


<details>
  <summary>Details</summary>
Motivation: To enable extraction of quantitative territorial data from animated historical maps without requiring pre-existing shapefiles, making historical territorial analysis more accessible.

Method: Uses HSV-based color segmentation, RGB channel filtering, and Direct-Neighbor Filtering to identify territorial control pixels, combined with preprocessing for temporal alignment and cross-video scaling.

Result: Successfully applied to ten Chinese dynasties (200 BCE - 1912 CE), producing year-by-year pixel counts that align with expected historical patterns.

Conclusion: ATDE is suitable for educational demonstrations, preliminary data exploration, and comparative analysis of territorial dynamics, though not a substitute for authoritative historical datasets.

Abstract: We present Animated Territorial Data Extractor (ATDE), a computer vision tool that extracts quantitative territorial data from animated historical map videos. ATDE employs HSV-based color segmentation, RGB channel filtering, and Direct-Neighbor Filtering to identify and count pixels representing territorial control. Combined with preprocessing for temporal alignment and cross-video scaling, the pipeline converts animated videos into structured time-series data. We demonstrate the tool on ten Chinese dynasties (200 BCE - 1912 CE), producing year-by-year pixel counts that align with expected historical patterns. While not a substitute for authoritative historical datasets, ATDE is well-suited for educational demonstrations, preliminary data exploration, and comparative analysis of territorial dynamics. The tool requires no pre-existing shapefiles and can be applied to any animated map video given seed colors and basic configuration. Code and examples are available on GitHub.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [407] [SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder](https://arxiv.org/abs/2511.17547)
*Jeyoung Lee,Hochul Kang*

Main category: eess.SP

TL;DR: SYNAPSE is a two-stage framework for EEG-to-image generation that learns semantically structured EEG representations and integrates them with Stable Diffusion, achieving state-of-the-art image quality with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To extend diffusion models to brain signals for understanding human perception, overcoming EEG challenges like noise, low resolution, and inter-subject variability that limit existing approaches.

Method: Two-stage framework: Stage1 uses CLIP-aligned EEG autoencoder for semantic latent representation; Stage2 freezes encoder and integrates with lightweight Stable Diffusion adaptation for efficient EEG conditioning.

Result: Achieves state-of-the-art perceptual fidelity on CVPR40 dataset, outperforming prior EEG-to-image models in reconstruction efficiency and image quality, with effective cross-subject generalization.

Conclusion: Reconstructing what the brain perceives, rather than what it classifies, is key to faithful EEG-based image generation, as demonstrated by SYNAPSE's semantic coherence and generalization capabilities.

Abstract: Recent progress in diffusion-based generative models has enabled high-quality image synthesis conditioned on diverse modalities. Extending such models to brain signals could deepen our understanding of human perception and mental representations. However,electroencephalography (EEG) presents major challenges for image generation due to high noise, low spatial resolution, and strong inter-subject variability. Existing approaches,such as DreamDiffusion, BrainVis, and GWIT, primarily adapt EEG features to pre-trained Stable Diffusion models using complex alignment or classification pipelines, often resulting in large parameter counts and limited interpretability. We introduce SYNAPSE, a two-stage framework that bridges EEG signal representation learning and high-fidelity image synthesis. In Stage1, a CLIP-aligned EEG autoencoder learns a semantically structured latent representation by combining signal reconstruction and cross-modal alignment objectives. In Stage2, the pretrained encoder is frozen and integrated with a lightweight adaptation of Stable Diffusion, enabling efficient conditioning on EEG features with minimal trainable parameters. Our method achieves a semantically coherent latent space and state-of-the-art perceptual fidelity on the CVPR40 dataset, outperforming prior EEG-to-image models in both reconstruction efficiency and image quality. Quantitative and qualitative analyses demonstrate that SYNAPSE generalizes effectively across subjects, preserving visual semantics even when class-level agreement is reduced. These results suggest that reconstructing what the brain perceives, rather than what it classifies, is key to faithful EEG-based image generation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [408] [Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation](https://arxiv.org/abs/2511.18415)
*Wei Yang,Yiran Zhu,Zilin Li,Xunjia Zhang,Hongtao Wang*

Main category: cs.MM

TL;DR: Vision-language models struggle with hierarchical reasoning tasks. The paper proposes Self-Elicited Knowledge Distillation (SEKD) where a VLM acts as its own teacher through stepwise reasoning, enabling efficient single-pass students to learn hierarchical understanding without human labels.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models have rich knowledge but fail at hierarchical understanding tasks where they need to maintain consistent coarse-to-fine taxonomy paths across all levels.

Method: Proposed Self-Elicited Knowledge Distillation (SEKD): the same VLM reasons step by step as a teacher, exposing hard labels, soft distributions, and decoder hidden states, while a single-pass student distills these signals without human labels or external tools.

Result: Student VLM approaches teacher accuracy while remaining efficient: improves in-domain path consistency by +29.50 percentage points, raises zero-shot HCA on unseen taxonomy from 4.15% to 42.26%, and yields gains on mathematical benchmarks.

Conclusion: SEKD provides a practical route to imbue compact VLMs with dependency-aware multi-step reasoning that scales to new taxonomies and datasets without annotation cost.

Abstract: Vision-language models (VLMs) possess rich knowledge but often fail on hierarchical understanding tasks, where the goal is to predict a coarse-to-fine taxonomy path that remains consistent across all levels. We compare three inference paradigms for hierarchical VQA and find that stepwise reasoning, when conditioned on prior answers, significantly outperforms single-pass prompting. Further analysis indicates that the main limitation of current VLMs is their inability to maintain cross-level state, rather than a lack of taxonomic knowledge. Motivated by this diagnosis, we propose Self-Elicited Knowledge Distillation (SEKD), which requires no human labels or external tools: the same VLM is prompted to reason step by step and act as a teacher by exposing its hard labels, soft distributions, and decoder hidden states, while a single-pass student distills these signals. The student VLM remains efficient while approaching the accuracy of its multi-step teacher. It improves in-domain path consistency (HCA) by up to +29.50 percentage points, raises zero-shot HCA on an unseen taxonomy from 4.15% to 42.26%, and yields gains on challenging mathematical benchmarks. Because all supervision is self-elicited, SEKD scales to new taxonomies and datasets without annotation cost, providing a practical route to imbue compact VLMs with dependency-aware multi-step reasoning.

</details>


### [409] [Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach](https://arxiv.org/abs/2511.19080)
*Fan Nie,Jiangqun Ni,Jian Zhang,Bin Zhang,Weizhe Zhang,Bin Li*

Main category: cs.MM

TL;DR: FoVB: A variational Bayesian framework for audio-visual deepfake detection that models cross-modal correlations as Gaussian latent variables to expose inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Address security concerns from AIGC-generated deepfakes by developing effective multi-modal detection that leverages audio-visual correlation inconsistencies as crucial detection clues.

Method: Uses variational Bayesian estimation to model audio-visual correlation as Gaussian latent variables, employs difference convolutions and high-pass filters for forgery trace extraction, and factorizes variables with orthogonality constraints.

Result: Extensive experiments show FoVB outperforms state-of-the-art methods across various benchmarks.

Conclusion: The variational Bayesian approach effectively captures audio-visual inconsistencies for robust deepfake detection, demonstrating superior performance over existing methods.

Abstract: The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [410] [Speech Recognition Model Improves Text-to-Speech Synthesis using Fine-Grained Reward](https://arxiv.org/abs/2511.17555)
*Guansu Wang,Peijie Sun*

Main category: eess.AS

TL;DR: W3AR uses ASR model attention to provide fine-grained word-level rewards for TTS optimization, improving quality and zero-shot robustness without explicit annotations.


<details>
  <summary>Details</summary>
Motivation: Current TTS evaluation methods like MOS are coarse-grained over entire utterances, while failures occur at word level. ASR models can detect word-level mismatches through cross-attention.

Method: W3AR leverages attention from pre-trained ASR models (e.g., Whisper) to provide word-level rewards for TTS optimization, enabling fine-grained alignment between speech and text without explicit annotations.

Result: W3AR improves quality of existing TTS systems and enhances zero-shot robustness on unseen speakers. Experiments demonstrate effectiveness of the approach.

Conclusion: Understanding models like ASR can serve as evaluators to provide fine-grained feedback for generative model optimization, suggesting a broader recipe for generative modeling.

Abstract: Recent advances in text-to-speech (TTS) have enabled models to clone arbitrary unseen speakers and synthesize high-quality, natural-sounding speech. However, evaluation methods lag behind: typical mean opinion score (MOS) estimators perform regression over entire utterances, while failures usually occur in a few problematic words. We observe that encoder-decoder ASR models (e.g., Whisper) surface word-level mismatches between speech and text via cross-attention, providing a fine-grained reward signal. Building on this, we introduce Word-level TTS Alignment by ASR-driven Attentive Reward (W3AR). Without explicit reward annotations, W3AR uses attention from a pre-trained ASR model to drive finer-grained alignment and optimization of sequences predicted by a TTS model. Experiments show that W3AR improves the quality of existing TTS systems and strengthens zero-shot robustness on unseen speakers. More broadly, our results suggest a simple recipe for generative modeling: understanding models can act as evaluators, delivering informative, fine-grained feedback for optimization.

</details>


### [411] [InstructAudio: Unified speech and music generation with natural language instruction](https://arxiv.org/abs/2511.18487)
*Chunyu Qiang,Kang Yin,Xiaopeng Wang,Yuzhe Liang,Jiahui Zhao,Ruibo Fu,Tianrui Wang,Cheng Gong,Chen Zhang,Longbiao Wang,Jianwu Dang*

Main category: eess.AS

TL;DR: InstructAudio is the first unified framework for instruction-based control of both speech and music generation, enabling natural language control over acoustic attributes like timbre, emotion, and musical properties.


<details>
  <summary>Details</summary>
Motivation: Current TTS and TTM systems have limited instruction-based control, depend on reference audio or expert annotations, and lack unified modeling despite sharing acoustic characteristics.

Method: Uses joint and single diffusion transformer layers with standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data for multi-task learning and cross-modal alignment.

Result: Achieves optimal results on most metrics compared to mainstream TTS and TTM models, supporting expressive speech, music, and dialogue generation in English and Chinese.

Conclusion: InstructAudio successfully demonstrates unified instruction-controlled framework for both speech and music generation, overcoming previous limitations in acoustic attribute control.

Abstract: Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation. TTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis. Despite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions. We introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese. The model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment. Fig. 1 visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics. To our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: https://qiangchunyu.github.io/InstructAudio/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [412] [Practical Machine Learning for Aphasic Discourse Analysis](https://arxiv.org/abs/2511.17553)
*Jason M. Pittman,Anton Phillips,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.LG

TL;DR: This study evaluates five machine learning models for automating Correct Information Unit (CIU) analysis in aphasia discourse assessment, finding high accuracy for word identification but variable performance for CIU detection.


<details>
  <summary>Details</summary>
Motivation: Manual CIU analysis by speech-language pathologists is labor-intensive, limiting clinical use. Machine learning could automate this process to augment clinical practice.

Method: Five supervised ML models were trained on human-coded transcripts from persons with aphasia performing picture description tasks, evaluating performance on word vs non-word and CIU vs non-CIU classification.

Result: All models achieved near-perfect accuracy (0.995) for word identification with high AUC scores (0.914-0.995). For CIU detection, k-NN performed best with 0.824 accuracy and 0.787 AUC, showing greater variability across models.

Conclusion: Supervised ML models can reliably distinguish words from non-words but face challenges in identifying CIUs, indicating the complexity of automated discourse analysis in aphasia.

Abstract: Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging.

</details>


### [413] [Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.17577)
*Fengming Yu,Qingyu Meng,Haiwei Pan,Kejia Zhang*

Main category: cs.LG

TL;DR: Lightweight optimization method combining dynamic attention head pruning and knowledge distillation to reduce LLM computational costs while maintaining mathematical reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Large language models have strong reasoning capabilities but face practical deployment challenges due to high computational and storage costs.

Method: Dynamic attention head pruning using weight norms and entropy to evaluate head importance, combined with knowledge distillation to transfer knowledge from original model to pruned student model.

Result: With 30% pruning ratio on Math23k: 18.7% parameter reduction, 27.5% inference speed improvement, 19.3% FLOPs reduction, with only 0.7% accuracy drop (84.4% to 83.7%).

Conclusion: The method achieves substantial efficiency gains while maintaining strong reasoning performance, providing practical solution for efficient LLM deployment in mathematical reasoning tasks.

Abstract: With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.

</details>


### [414] [Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection](https://arxiv.org/abs/2511.17589)
*Sören Dréano,Derek Molloy,Noel Murphy*

Main category: cs.LG

TL;DR: Llamazip is a lossless text compression algorithm using LLaMA3's predictive capabilities, storing only unpredicted tokens to achieve high compression ratios while maintaining data integrity.


<details>
  <summary>Details</summary>
Motivation: To leverage language model predictions for efficient text compression and address data provenance concerns in language model training.

Method: Uses LLaMA3 language model to predict text tokens and only stores tokens that the model fails to predict, analyzing factors like quantization and context window size.

Result: Achieves significant data reduction while maintaining lossless compression, and can identify whether documents were part of a language model's training dataset.

Conclusion: Llamazip demonstrates effective text compression using language model predictions and provides a method to address data provenance and transparency issues in language model training.

Abstract: This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.

</details>


### [415] [PocketLLM: Ultimate Compression of Large Language Models via Meta Networks](https://arxiv.org/abs/2511.17637)
*Ye Tian,Chengcheng Wang,Jing Han,Yehui Tang,Kai Han*

Main category: cs.LG

TL;DR: PocketLLM compresses LLMs using meta-networks to project weights into discrete latent vectors via a codebook, achieving 10x compression with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: As LLMs grow larger, storing and transmitting them on edge devices becomes challenging. Traditional compression methods struggle with extreme compression without sacrificing accuracy.

Method: Uses an encoder to project LLM weights into discrete latent vectors represented by a compact codebook, and a lightweight decoder to map codebook vectors back to original weight space.

Result: Achieves 10x compression of Llama 2-7B with negligible accuracy drop, demonstrating superior performance at high compression ratios.

Conclusion: PocketLLM provides an effective approach for extreme compression of large language models while maintaining performance, making LLMs more feasible for edge device deployment.

Abstract: As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.

</details>


### [416] [DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams](https://arxiv.org/abs/2511.17693)
*Ginés Carreto Picón,Peng Yuan Zhou,Qi Zhang,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: DeepCoT enables efficient deep transformer models for stream data by eliminating redundant computations in sliding window inference, achieving linear computational cost while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the computational redundancy in sliding window inference for stream data and overcome limitations of shallow continual transformers that restrict generalization power.

Method: Propose Deep Continual Transformer (DeepCoT) - a redundancy-free encoder-only model that can be applied to existing deep encoder architectures with minimal changes.

Result: DeepCoTs maintain comparable performance to non-continual baselines while reducing computational cost to linear complexity, achieving up to two orders of magnitude speedup in running time across audio, video, and text streams.

Conclusion: DeepCoT successfully enables efficient deep transformer models for stream data processing, overcoming previous limitations and providing substantial computational benefits without sacrificing performance.

Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.

</details>


### [417] [Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch](https://arxiv.org/abs/2511.17826)
*Ziyang Zhang,Xinheng Ding,Jiayi Yuan,Rixin Liu,Huizi Mao,Jiarong Xing,Zirui Liu*

Main category: cs.LG

TL;DR: Proposes Tree-Based Invariant Kernels (TBIK) to ensure bit-wise deterministic inference in LLMs across different tensor parallel sizes, solving the precision mismatch problem in RL training pipelines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving frameworks exhibit non-deterministic behavior due to floating-point non-associativity and inconsistent reduction orders across GPUs, creating precision mismatches between training (TP=1) and inference (multi-GPU TP) engines in RL settings.

Method: Developed TP-invariant matrix multiplication and reduction primitives using a unified hierarchical binary tree structure to align intra- and inter-GPU reduction orders, implemented in Triton and integrated into vLLM and FSDP.

Result: Achieved zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes, with bit-wise identical results between vLLM and FSDP in RL training pipelines.

Conclusion: TBIK successfully guarantees deterministic LLM inference regardless of tensor parallel configuration, solving the critical precision mismatch problem in RL training and enabling reliable deployment across different parallel strategies.

Abstract: Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.

</details>


### [418] [Majority of the Bests: Improving Best-of-N via Bootstrapping](https://arxiv.org/abs/2511.18630)
*Amin Rakhsha,Kanika Madan,Tianyu Zhang,Amir-massoud Farahmand,Amir Khasahmadi*

Main category: cs.LG

TL;DR: MoB (Majority-of-the-Bests) improves upon Best-of-N selection by using bootstrapping to estimate the output distribution and selecting the mode, achieving better performance with imperfect reward models.


<details>
  <summary>Details</summary>
Motivation: Best-of-N selection fails with imperfect reward models, but the correct answer is often the most likely outcome in the distribution, suggesting the mode could be more reliable than individual samples.

Method: Propose MoB which estimates the output distribution of BoN via bootstrapping and selects the mode rather than the highest-scoring sample.

Result: Experimental results across 5 benchmarks, 3 LLMs, and 2 reward models show consistent improvements over BoN in 25 out of 30 setups.

Conclusion: MoB serves as a simple yet strong alternative to BoN and self-consistency, motivating research in more nuanced selection mechanisms.

Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.

</details>


### [419] [How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining](https://arxiv.org/abs/2511.18903)
*Kairong Luo,Zhenbo Sun,Haodong Wen,Xinyu Shi,Jiarui Cui,Chenyi Dang,Kaifeng Lyu,Wenguang Chen*

Main category: cs.LG

TL;DR: Curriculum-based pretraining for LLMs is limited by incompatibility between ascending data quality order and decaying learning rate schedules. Two simple strategies - moderate LR decay and model averaging - can mitigate this issue and improve performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are often trained on mixed-quality data, and curriculum-based pretraining (training on data sorted by quality) should better leverage high-quality data, but prior studies show limited improvements from this approach.

Method: Identify the incompatibility between ascending data quality curriculum and decaying LR schedules. Propose two strategies: (1) moderate LR decay (final LR only moderately smaller than peak LR), and (2) replacing LR decay with model averaging of final checkpoints.

Result: Combining these strategies improves average score on standard benchmarks by 1.64% over random shuffling, validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics.

Conclusion: Curriculum-based LLM pretraining should be re-evaluated and co-designed with optimization methods, as simple adjustments to LR scheduling can unlock its potential without additional data refinement.

Abstract: Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.

</details>


### [420] [SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: SWAN is a fine-tuning-free framework that reduces KV-cache memory footprint by 50-60% through orthogonal matrix rotation and pruning, enabling direct use in attention computation without decompression overhead.


<details>
  <summary>Details</summary>
Motivation: LLMs face memory bottlenecks from KV-cache during autoregressive inference, and existing compression methods risk information loss, have fixed limits, or introduce computational overhead from decompression.

Method: Uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in attention computation without any reconstruction steps.

Result: Maintains performance close to uncompressed baseline while achieving 50-60% memory savings per-token on KV-cache, with runtime-tunable compression levels for dynamic memory footprint adjustment.

Conclusion: SWAN provides a practical and efficient solution for serving LLMs with long contexts through its decompression-free design, high performance under compression, and runtime adaptability.

Abstract: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.

</details>


### [421] [RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning](https://arxiv.org/abs/2511.19168)
*Deyi Ji,Yuekui Yang,Liqun Liu,Peng Shu,Haiyang Wu,Shaogang Tang,Xudong Chen,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.LG

TL;DR: RAVEN++ is an enhanced framework for video ad moderation that improves fine-grained violation understanding, explainability, and generalization through active reinforcement learning, hierarchical rewards, and progressive multi-stage training.


<details>
  <summary>Details</summary>
Motivation: Existing video ad moderation methods like RAVEN have limitations in fine-grained understanding, explainability, and generalization, creating significant challenges in precise violation localization for complex video advertisements.

Method: Three key innovations: 1) Active Reinforcement Learning for dynamic training adaptation, 2) Fine-Grained Violation Understanding via hierarchical reward functions and reasoning distillation, 3) Progressive Multi-Stage Training combining knowledge injection, curriculum-based passive RL, and active RL.

Result: Extensive experiments on public and proprietary datasets show RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in fine-grained violation understanding, reasoning capabilities, and generalization ability, with successful online A/B testing deployment.

Conclusion: RAVEN++ effectively addresses critical gaps in video ad moderation by providing superior fine-grained understanding, enhanced reasoning, and better generalization compared to existing approaches.

Abstract: Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.

</details>


### [422] [A Nutrition Multimodal Photoplethysmography Language Model](https://arxiv.org/abs/2511.19260)
*Kyle Verrier,Achille Nazaret,Joseph Futoma,Andrew C. Miller,Guillermo Sapiro*

Main category: cs.LG

TL;DR: NPLM integrates PPG from wearables with meal descriptions to improve caloric intake prediction by 11% over text-only methods, enabling scalable noninvasive dietary monitoring.


<details>
  <summary>Details</summary>
Motivation: Hunger and satiety dynamics are crucial for dietary behaviors but difficult to capture in everyday settings, requiring better methods for dietary monitoring.

Method: Developed Nutrition Photoplethysmography Language Model (NPLM) that projects PPG signals into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs.

Result: Improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. Validated in independent study (n=140) with controlled dining.

Conclusion: Integration of physiological measurements from consumer wearables with meal information enables effective noninvasive dietary monitoring at scale.

Abstract: Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.

</details>


### [423] [CDLM: Consistency Diffusion Language Models For Faster Sampling](https://arxiv.org/abs/2511.19269)
*Minseo Kim,Chenfeng Xu,Coleman Hooper,Harman Singh,Ben Athiwaratkun,Ce Zhang,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: CDLM introduces consistency modeling and block-wise causal attention to accelerate Diffusion Language Models, achieving 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion Language Models suffer from slow inference due to numerous refinement steps and inability to use standard KV caching, creating bottlenecks for practical deployment.

Method: Integrates consistency modeling for multi-token finalization and enforces block-wise causal attention mask during fine-tuning to enable KV caching compatibility.

Result: Achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks.

Conclusion: CDLM effectively addresses both major bottlenecks in DLMs through training-based acceleration, making them more practical for real-world applications.

Abstract: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.

</details>


### [424] [MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings](https://arxiv.org/abs/2511.19279)
*Victor Rambaud,Salvador Mascarenhas,Yair Lakretz*

Main category: cs.LG

TL;DR: MapFormers are Transformer-based architectures that learn cognitive maps from observational data through input-dependent positional encoding, enabling superior out-of-distribution generalization in navigation tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human cognitive flexibility and current AI systems by developing models that can learn internal cognitive maps for better out-of-distribution generalization.

Method: Developed MapFormers with two variants using input-dependent positional encoding to disentangle structure from content, modeling episodic and working memory respectively, and tested on 2D navigation tasks.

Result: MapFormers achieved near-perfect performance in out-of-distribution generalization (e.g., longer sequences) and learned cognitive maps of underlying spaces, outperforming current architectures.

Conclusion: Models designed to learn cognitive maps with structural bias for structure-content disentanglement (via input-dependent positional encoding in Transformers) demonstrate superiority and have broad applications in neuroscience and AI.

Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.

</details>


### [425] [Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric](https://arxiv.org/abs/2511.19350)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.LG

TL;DR: A spectral method for estimating cluster count in short text embeddings using Laplacian eigenspectrum analysis, with adaptive sampling for scalability and a new Cohesion Ratio metric for unsupervised evaluation.


<details>
  <summary>Details</summary>
Motivation: Clustering short text embeddings is challenging due to the need to specify cluster numbers in advance, and existing methods struggle with scalability and lack reliable unsupervised evaluation metrics.

Method: Spectral clustering using Laplacian eigenspectrum with cosine similarities, adaptive sampling strategy for scalability, and proposed Cohesion Ratio metric for intrinsic evaluation without ground-truth labels.

Result: Experiments on six datasets and four embedding models show K-Means and HAC with the proposed estimator outperform HDBSCAN, OPTICS, and Leiden. Cohesion Ratio correlates well with extrinsic measures like NMI and homogeneity.

Conclusion: The spectral estimator and Cohesion Ratio provide practical value for unsupervised organization and evaluation of short text data, offering scalable and reliable clustering without requiring ground-truth labels.

Abstract: Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.

</details>


### [426] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD is a novel framework that synthesizes high-quality out-of-distribution (OOD) features by perturbing in-distribution features near decision boundaries and decoding them into images using diffusion models, significantly improving OOD detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to extract effective OOD features due to difficulty identifying decision boundaries between classes in latent space, limiting OOD detection performance.

Method: BOOD learns a text-conditioned latent feature space, selects ID features closest to decision boundaries, perturbs them to cross boundaries forming OOD features, then decodes them into images using diffusion models.

Result: BOOD achieves state-of-the-art performance with 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and 7.27% improvement in average AUROC (90.15% vs. 97.42%) on CIFAR-100 dataset.

Conclusion: BOOD provides a training-efficient strategy for synthesizing informative OOD features that facilitate clearer distinctions between ID and OOD data, significantly outperforming previous methods.

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training data based on latent space features has proven effective in enhancing out-of-distribution (OOD) detection performance. However, extracting effective features outside the in-distribution (ID) boundary in latent space remains challenging due to the difficulty of identifying decision boundaries between classes. This paper proposes a novel framework called Boundary-based Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD features and generates human-compatible outlier images using diffusion models. BOOD first learns a text-conditioned latent feature space from the ID dataset, selects ID features closest to the decision boundary, and perturbs them to cross the decision boundary to form OOD features. These synthetic OOD features are then decoded into images in pixel space by a diffusion model. Compared to previous works, BOOD provides a more training efficient strategy for synthesizing informative OOD features, facilitating clearer distinctions between ID and OOD data. Extensive experimental results on common benchmarks demonstrate that BOOD surpasses the state-of-the-art method significantly, achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27% improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [427] [Saving Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2511.16520)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: cs.LG

TL;DR: FMPlug is a plug-in framework that enhances foundation flow-matching models for inverse problems through instance-guided warm-start and Gaussianity regularization, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Foundation flow-matching models currently underperform compared to domain-specific or untrained priors in inverse problems, despite their promise as universal priors.

Method: Combines instance-guided time-dependent warm-start strategy with sharp Gaussianity regularization to add problem-specific guidance while preserving Gaussian structures.

Result: Significant performance boost across image restoration and scientific inverse problems.

Conclusion: Provides a path for making foundation flow-matching models practical, reusable priors for inverse problem solving.

Abstract: Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.

</details>


### [428] [Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks](https://arxiv.org/abs/2511.17564)
*Guilherme Grancho D. Fernandes,Marco A. Barroca,Mateus dos Santos,Rafael S. Oliveira*

Main category: cs.LG

TL;DR: Bidirectional LSTM model classifies astronomical light curves from PLAsTiCC dataset into 5 categories, showing strong performance for S-Like and Periodic classes but poor results for Fast/Long classes and partial data.


<details>
  <summary>Details</summary>
Motivation: To address class imbalance in astronomical object classification and develop effective methods for classifying transient light curves from large-scale surveys.

Method: Used bidirectional LSTM neural network with masking layers, preprocessing with padding, temporal rescaling, and flux normalization on PLAsTiCC dataset reorganized into 5 categories.

Result: Strong performance for S-Like (ROC AUC 0.95) and Periodic (ROC AUC 0.99) classes, but poor for Fast/Long classes (ROC AUC 0.68 for Long). Performance degraded significantly with partial light curve data.

Conclusion: Class imbalance and limited temporal information are primary limitations; class balancing strategies and preprocessing focusing on detection moments could improve performance.

Abstract: This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.

</details>


### [429] [EgoCogNav: Cognition-aware Human Egocentric Navigation](https://arxiv.org/abs/2511.17581)
*Zhiwen Qiu,Ziang Liu,Wenqian Niu,Tapomayukh Bhattacharjee,Saleh Kalantari*

Main category: cs.LG

TL;DR: EgoCogNav is a multimodal egocentric navigation framework that predicts perceived path uncertainty and jointly forecasts trajectories and head motion by fusing scene features with sensory cues.


<details>
  <summary>Details</summary>
Motivation: To model cognitive and experiential factors in human navigation, addressing the gap in existing methods that focus on motion forecasting in fully observed scenes while neglecting human factors like how people feel and respond to space.

Method: Proposes EgoCogNav framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. Introduces CEN dataset with 6 hours of real-world egocentric recordings.

Result: EgoCogNav learns perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.

Conclusion: The framework successfully models cognitive factors in navigation and enables prediction of human-like navigation behaviors through perceived uncertainty modeling.

Abstract: Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.

</details>


### [430] [Learning Straight Flows: Variational Flow Matching for Efficient Generation](https://arxiv.org/abs/2511.17583)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.LG

TL;DR: S-VFM integrates variational latent codes to enforce straight trajectories in Flow Matching, achieving competitive performance with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Flow Matching has limited one-step generation capability due to curved trajectories, and previous solutions suffer from approximation errors, instability, and convergence issues.

Method: Integrates variational latent codes representing 'generation overview' into Flow Matching framework to explicitly enforce straight trajectories.

Result: Achieves competitive performance across three challenge benchmarks with advantages in training and inference efficiency.

Conclusion: S-VFM effectively addresses Flow Matching limitations by producing linear generation paths through variational latent code integration.

Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.

</details>


### [431] [PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis](https://arxiv.org/abs/2511.17585)
*Kang He,Boyu Chen,Yuzhe Ding,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.LG

TL;DR: PaSE framework addresses modality competition in Multimodal Sentiment Analysis by using prototype-aligned calibration and Shapley-optimized equilibrium to enhance cross-modal collaboration.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal scenarios often exhibit modality competition where dominant modalities overshadow weaker ones, leading to suboptimal performance in sentiment analysis.

Method: Uses Prototype-guided Calibration Learning with Entropic Optimal Transport for semantic alignment, followed by prototype-gated fusion and Shapley-based Gradient Modulation to adaptively adjust gradients based on modality contributions.

Result: Extensive experiments on IEMOCAP, MOSI, and MOSEI datasets confirm superior performance and effective alleviation of modality competition.

Conclusion: PaSE framework successfully enhances multimodal collaboration while mitigating modality competition through prototype alignment and Shapley optimization.

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.

</details>


### [432] [CubeletWorld: A New Abstraction for Scalable 3D Modeling](https://arxiv.org/abs/2511.17664)
*Azlaan Mustafa Samad,Hoang H. Nguyen,Lukas Berg,Henrik Müller,Yuan Xue,Daniel Kudenko,Zahra Ahmadi*

Main category: cs.LG

TL;DR: CubeletWorld introduces a privacy-preserving 3D grid framework for urban modeling using cubelets to embed diverse data without agent sensing, enabling scalable planning and prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing agent-centric urban modeling methods face scalability and privacy issues due to reliance on direct environmental sensing. There's a need for privacy-preserving approaches that can integrate heterogeneous urban data sources.

Method: CubeletWorld uses a discretized 3D grid of spatial units (cubelets) to embed diverse urban data signals like infrastructure, movement, and environmental indicators into localized states. It supports downstream tasks without agent-driven sensing.

Result: The framework enables cubelet state prediction using realistic urban datasets. Experiments show it handles spatial granularity challenges and sparsity issues while offering greater generalizability across regions and improved privacy compliance compared to existing 3D occupancy models.

Conclusion: CubeletWorld provides a flexible, extensible framework for learning from complex urban data, opening possibilities for scalable simulation and decision support in socio-demographic modeling, environmental monitoring, and emergency response.

Abstract: Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.

</details>


### [433] [pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data](https://arxiv.org/abs/2511.18066)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Mir Sazzat Hossain,Rakibul Hasan Rajib,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: pFedBBN is a personalized federated test-time adaptation framework that addresses class imbalance and domain shifts in federated learning through balanced batch normalization and client collaboration.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in federated learning causes rare classes to be underrepresented in client datasets, and existing methods fail to handle unsupervised adaptation to dynamic domains during inference under federated constraints.

Method: Uses balanced batch normalization during local client adaptation to treat all classes equally, enables client collaboration based on BBN similarity, and employs class-aware model aggregation for personalized inference without compromising privacy.

Result: Extensive experiments show pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.

Conclusion: pFedBBN effectively addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration without requiring labeled data from clients.

Abstract: Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.

</details>


### [434] [Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj](https://arxiv.org/abs/2511.18248)
*Wei Zhen Teoh*

Main category: cs.LG

TL;DR: CausalTraj is a new model for multi-agent trajectory forecasting that focuses on generating jointly probable trajectories rather than just per-agent accuracy, achieving better joint prediction performance.


<details>
  <summary>Details</summary>
Motivation: Existing models are evaluated on per-agent metrics (minADE, minFDE) but overlook whether predicted trajectories form plausible multi-agent futures, leading to incoherent joint predictions in team sports.

Method: CausalTraj is a temporally causal, likelihood-based model designed specifically to generate jointly probable multi-agent trajectory forecasts.

Result: CausalTraj achieves competitive per-agent accuracy and state-of-the-art results on joint metrics (minJADE, minJFDE) across NBA SportVU, Basketball-U, and Football-U datasets.

Conclusion: The model generates qualitatively coherent and realistic gameplay evolutions, demonstrating the importance of joint metrics for assessing multi-agent forecasting capability.

Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.

</details>


### [435] [From Tables to Signals: Revealing Spectral Adaptivity in TabPFN](https://arxiv.org/abs/2511.18278)
*Jianqiao Zheng,Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: TabPFN, a task-agnostic tabular foundation model, exhibits unique spectral properties including broader frequency capacity than MLPs and adaptive spectral response to in-context samples, enabling training-free image denoising.


<details>
  <summary>Details</summary>
Motivation: To understand the inductive biases and frequency-based behavior of TabPFN, a tabular foundation model whose origins remain poorly understood despite impressive performance.

Method: Analyzed TabPFN through signal reconstruction lens using frequency-based analysis, compared with ReLU-MLPs, studied spectral adaptivity to in-context samples, and examined positional encoding effects on frequency response.

Result: TabPFN has broader effective frequency capacity than standard MLPs without hyperparameter tuning, exhibits spectral adaptivity (frequency response adapts to in-context sample count), and positional encoding modulates frequency response similar to implicit neural representations.

Conclusion: TabPFN's spectral properties enable training-free and hyperparameter-free image denoising, revealing its potential as a task-agnostic implicit model and providing new insights into tabular foundation models' structure and inductive biases.

Abstract: Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks.

</details>


### [436] [TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis](https://arxiv.org/abs/2511.18287)
*Rui Peng,Ziru Liu,Lingyuan Ye,Yuxing Lu,Boxin Shi,Jinzhuo Wang*

Main category: cs.LG

TL;DR: TRIDENT is a cascade generative framework that synthesizes cellular morphology by conditioning on both perturbations and gene expression profiles, bridging the gap between transcriptome and phenome mapping.


<details>
  <summary>Details</summary>
Motivation: Existing methods only model direct associations like Perturbation→RNA or Perturbation→Morphology, but overlook the crucial causal link from RNA to morphology, which is essential for building an AI Virtual Cell.

Method: Proposed TRIDENT framework that synthesizes realistic cellular morphology by conditioning on both perturbation and corresponding gene expression profile, trained on MorphoGene dataset pairing L1000 gene expression with Cell Painting images for 98 compounds.

Result: TRIDENT significantly outperforms state-of-the-art approaches with up to 7-fold improvement, shows strong generalization to unseen compounds, and RNA-guided synthesis accurately produces corresponding phenotypes as validated in docetaxel case study.

Conclusion: By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves closer to a predictive virtual cell, with ablation study confirming RNA conditioning is essential for high fidelity.

Abstract: Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\rightarrow$ RNA or Perturbation $\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.

</details>


### [437] [Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection](https://arxiv.org/abs/2511.18336)
*Kaito Shiku,Kazuya Nishimura,Shinnosuke Matsuo,Yasuhiro Kojima,Ryoma Bise*

Main category: cs.LG

TL;DR: Proposed AGL method uses auxiliary gene learning to improve spatial transcriptomics analysis by leveraging ignored genes as auxiliary tasks, with DkGSB for differentiable gene selection.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics suffers from observational noise, and previous methods only use highly variable genes, ignoring potentially useful co-expression relationships with low-expression genes.

Method: AGL reformulates ignored genes as auxiliary tasks trained jointly with primary tasks. DkGSB uses prior knowledge and bi-level optimization for differentiable top-k gene selection.

Result: Experiments show the proposed method effectively incorporates auxiliary genes and outperforms conventional auxiliary task learning approaches.

Conclusion: Auxiliary gene learning with differentiable gene selection improves spatial transcriptomics analysis by leveraging previously ignored co-expression relationships.

Abstract: Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.

</details>


### [438] [Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems](https://arxiv.org/abs/2511.18417)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: The paper develops a unified theory of category-equivariant neural networks (CENNs) that generalizes various equivariant network types including group/groupoid, poset/lattice, graph, and sheaf neural networks.


<details>
  <summary>Details</summary>
Motivation: To expand equivariant deep learning beyond traditional group actions to encompass broader symmetries including geometric, contextual, and compositional symmetries through a categorical framework.

Method: Formulates equivariance as naturality in a topological category with Radon measures, developing linear and nonlinear layers in categorical setup. Proves universal approximation theorem for finite-depth CENNs.

Result: Establishes that finite-depth CENNs are dense in the space of continuous equivariant transformations. Instantiates the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves.

Conclusion: Categorical equivariant deep learning enables expansion of equivariant learning beyond group actions to include diverse symmetry types through a unified mathematical framework.

Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.

</details>


### [439] [Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels](https://arxiv.org/abs/2511.18457)
*Duncan Stothers,Ben Stothers,Emily Schaeffer,Kishore Mulpuri*

Main category: cs.LG

TL;DR: Developed an ultrasound-first policy for DDH screening that uses calibrated deferral rules to decide when radiographs are needed, achieving tunable trade-offs between ultrasound-only throughput and coverage guarantees.


<details>
  <summary>Details</summary>
Motivation: To create a radiation-preserving screening policy for developmental dysplasia of the hip that minimizes unnecessary radiographs while maintaining diagnostic accuracy.

Method: Pretrained modality-specific encoders with SimSiam on unlabeled data, froze backbones and trained small heads for DDH measurements, then calibrated conformal deferral rules on ultrasound predictions with finite sample coverage guarantees.

Result: Ultrasound measurement errors were modest (alpha MAE ~9.7°, coverage MAE ~14.0%), while radiographic measurements achieved AI MAE ~7.6° and CE MAE ~8.9°. The policy allows tunable trade-offs between ultrasound-only rates and coverage.

Conclusion: The pipeline successfully converts limited labels into interpretable measurements with tunable selective imaging curves, providing a reproducible approach suitable for clinical implementation and future validation.

Abstract: We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.
  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.

</details>


### [440] [SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation](https://arxiv.org/abs/2511.18468)
*Md Akil Raihan Iftee,Mir Sazzat Hossain,Rakibul Hasan Rajib,Tariq Iqbal,Md Mofijul Islam,M Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.LG

TL;DR: SloMo-Fast is a source-free dual-teacher framework for Continual Test-Time Adaptation that addresses long-term forgetting and enhances adaptability to evolving domains through complementary Slow-Teacher (retains past knowledge) and Fast-Teacher (rapidly adapts to new domains).


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods rely on source data/prototypes, limiting privacy-sensitive applications, and suffer from long-term forgetting as domains shift over time.

Method: Dual-teacher framework with Slow-Teacher for slow forgetting and long-term knowledge retention, and Fast-Teacher for rapid adaptation to new domains while accumulating knowledge across them.

Result: Outperforms state-of-the-art methods across Cyclic-TTA benchmark and ten other CTTA settings, demonstrating superior adaptation and generalization across evolving and revisited domains.

Conclusion: SloMo-Fast effectively addresses CTTA challenges by preserving past domain knowledge while efficiently adapting to new domains, making it suitable for privacy-sensitive and resource-constrained real-world applications.

Abstract: Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.

</details>


### [441] [TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting](https://arxiv.org/abs/2511.18539)
*Lingyu Jiang,Lingyu Xu,Peiran Li,Qianwen Ge,Dingyi Zhuang,Shuo Xing,Wenjing Chen,Xiangbo Gao,Ting-Hsuan Chen,Xueying Zhan,Xin Zhang,Ziming Zhang,Zhengzhong Tu,Michael Zielewski,Kazunori Yamada,Fangzhou Lin*

Main category: cs.LG

TL;DR: TimePre is a novel framework that combines MLP-based efficiency with Multiple Choice Learning for probabilistic time-series forecasting, using Stabilized Instance Normalization to resolve training instability and achieve state-of-the-art performance with fast inference.


<details>
  <summary>Details</summary>
Motivation: Existing probabilistic forecasting models face computational bottlenecks from iterative sampling (diffusion models) or suffer from training instability and hypothesis collapse (MCL approaches), especially when combined with efficient MLP backbones.

Method: Proposes TimePre framework with Stabilized Instance Normalization (SIN) layer that corrects channel-wise statistical shifts to stabilize the hybrid MLP-MCL architecture and prevent hypothesis collapse.

Result: Achieves state-of-the-art accuracy on six benchmark datasets, with inference speeds orders of magnitude faster than sampling-based models and stable performance scaling unlike prior MCL work.

Conclusion: TimePre successfully bridges the gap between accuracy, efficiency, and stability in probabilistic forecasting by unifying MLP efficiency with MCL distributional flexibility through the SIN stabilization mechanism.

Abstract: Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting.

</details>


### [442] [Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers](https://arxiv.org/abs/2511.18670)
*Rowan Bradbury,Aniket Srinivasan Ashok,Sai Ram Kasanagottu,Gunmay Jhingran,Shuai Meng*

Main category: cs.LG

TL;DR: DCR (Deterministic Continuous Replacement) is a method that replaces quadratic self-attention with efficient alternatives in pretrained models by blending teacher and student outputs using deterministic annealing, avoiding cold-start reinitialization issues.


<details>
  <summary>Details</summary>
Motivation: Replacing modules in pretrained models, particularly swapping quadratic self-attention for efficient attention alternatives, causes optimization instability due to cold-start reinitialization disrupting frozen backbones.

Method: Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight schedule, eliminating gate-induced gradient variance inherent to stochastic replacement methods.

Result: In single-seed studies, DCR achieves faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement tasks.

Conclusion: DCR establishes a foundation for stable heterogeneous operator swaps in pretrained models by addressing the core stability challenge through deterministic annealing.

Abstract: Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.

</details>


### [443] [VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking](https://arxiv.org/abs/2511.18692)
*Kichang Yang,Seonjun Kim,Minjae Kim,Nairan Zhang,Chi Zhang,Youngki Lee*

Main category: cs.LG

TL;DR: Neuron Chunking is an I/O-efficient sparsification method that groups neurons into chunks and selects them based on both importance and storage access cost, improving flash-based weight offloading efficiency by up to 5.76x.


<details>
  <summary>Details</summary>
Motivation: Conventional activation sparsification for edge deployment of VLMs is model-centric and ignores how storage access patterns affect flash performance, leading to suboptimal I/O efficiency.

Method: Groups neurons into contiguous memory chunks, models I/O latency through access contiguity abstraction, and selects chunks with high utility (neuron importance normalized by estimated latency).

Result: Achieves 4.65x and 5.76x I/O efficiency improvements on Jetson Orin Nano and Jetson AGX Orin respectively compared to conventional methods.

Conclusion: Aligning sparsification decisions with storage behavior through Neuron Chunking significantly enhances I/O efficiency for edge deployment of large VLMs.

Abstract: Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.

</details>


### [444] [GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction](https://arxiv.org/abs/2511.18716)
*Zesheng Liu,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: GRIT-LP is a graph transformer for polar ice-layer thickness estimation that addresses oversmoothing and weak long-range dependencies through partitioned spatial graphs and long-range skip connections, achieving 24.92% RMSE improvement.


<details>
  <summary>Details</summary>
Motivation: Accurate ice layer thickness estimation is critical for understanding snow accumulation, reconstructing past climate patterns, and reducing uncertainties in projections of future ice sheet evolution and sea level rise.

Method: Combines inductive geometric graph learning with self-attention, using partitioned spatial graph construction with overlapping fully connected local neighborhoods to preserve spatial coherence, and long-range skip connections to mitigate oversmoothing in deeper attention layers.

Result: GRIT-LP outperforms state-of-the-art methods with a 24.92% improvement in root mean squared error.

Conclusion: Graph transformers effectively model spatiotemporal patterns by capturing both localized structural features and long-range dependencies, demonstrating potential to advance data-driven understanding of cryospheric processes.

Abstract: Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.

</details>


### [445] [Sampling Control for Imbalanced Calibration in Semi-Supervised Learning](https://arxiv.org/abs/2511.18773)
*Senmao Tian,Xiang Wei,Shunli Zhang*

Main category: cs.LG

TL;DR: SC-SSL is a unified framework that addresses class imbalance in semi-supervised learning by decoupling sampling control to suppress model bias through adaptive sampling and post-hoc calibration.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods handle class imbalance coarsely by conflating data imbalance with bias from varying class-specific learning difficulties, leading to biased classification when labeled and unlabeled data have distributional mismatches.

Method: Proposes SC-SSL with decoupled sampling control: identifies key sampling variables, introduces classifier with explicit expansion capability, adaptively adjusts sampling probabilities across distributions, and applies post-hoc sampling control with optimization bias vector to calibrate logits.

Result: Extensive experiments across various benchmark datasets and distribution settings show SC-SSL achieves consistent state-of-the-art performance in addressing class imbalance in SSL.

Conclusion: SC-SSL effectively mitigates feature-level imbalance for minority classes through unified sampling control framework, demonstrating superior performance in handling class imbalance in semi-supervised learning scenarios.

Abstract: Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.

</details>


### [446] [Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning](https://arxiv.org/abs/2511.18859)
*Bo Jiang,Weijun Zhao,Beibei Wang,Xiao Wang,Jin Tang*

Main category: cs.LG

TL;DR: UAdapterGNN integrates uncertainty learning into GNN adapters to enhance robustness against graph noise and improve generalization in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing GNN adapters are prone to graph noise (noisy edges, ambiguous attributes) and have limited generalizability, creating a need for more robust fine-tuning methods.

Method: Proposes UAdapterGNN with Gaussian probabilistic adapter that automatically adjusts to noise by absorbing variance changes in the Gaussian distribution during fine-tuning.

Result: Extensive experiments show UAdapterGNN effectively enhances robustness against graph noise and improves generalization ability on downstream tasks.

Conclusion: Integrating uncertainty learning into GNN adapters successfully addresses robustness and generalization challenges in fine-tuning pre-trained GNN models for noisy graph data.

Abstract: Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.

</details>


### [447] [AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention](https://arxiv.org/abs/2511.18960)
*Lei Xiao,Jifeng Li,Juntao Gao,Feiyang Ye,Yan Jin,Jingjing Qian,Jing Zhang,Yong Wu,Xiaoyuan Yu*

Main category: cs.LG

TL;DR: AVA-VLA introduces Active Visual Attention to dynamically modulate visual processing using recurrent states, improving VLA models for embodied AI by addressing history-agnostic limitations.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models process visual inputs independently at each timestep, treating tasks as MDPs and failing to leverage historical context, which is suboptimal for sequential decision-making.

Method: Reformulates the problem as POMDP and introduces AVA module that uses recurrent state (belief state approximation) to compute soft weights for active processing of task-relevant visual tokens based on historical context.

Result: Achieves state-of-the-art performance on LIBERO and CALVIN benchmarks, with successful real-world deployment on dual-arm robot platform demonstrating robust sim-to-real transferability.

Conclusion: AVA-VLA effectively addresses history-agnostic limitations in VLA models through active visual attention based on belief states, enabling superior performance in embodied AI tasks with practical real-world applicability.

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.

</details>


### [448] [UniGame: Turning a Unified Multimodal Model Into Its Own Adversary](https://arxiv.org/abs/2511.19413)
*Zhaolong Su,Wang Lu,Hao Chen,Sharon Li,Jindong Wang*

Main category: cs.LG

TL;DR: UniGame is a self-adversarial post-training framework that addresses the inconsistency between understanding and generation in Unified Multimodal Models by using a lightweight perturber to make the generation branch challenge fragile understanding.


<details>
  <summary>Details</summary>
Motivation: UMMs suffer from fundamental inconsistency where understanding favors compact embeddings while generation favors reconstruction-rich representations, leading to misaligned decision boundaries and degraded cross-modal coherence.

Method: A self-adversarial post-training framework that applies a lightweight perturber at the shared token interface, enabling the generation branch to actively seek and challenge fragile understanding.

Result: Significantly improves consistency (+4.6%), understanding (+3.6%), generation (+0.02), and robustness (+4.8% and +6.2% on NaturalBench and AdVQA) with less than 1% additional parameters.

Conclusion: Adversarial self-play is an effective principle for enhancing coherence, stability, and unified competence of multimodal foundation models, and the framework is complementary to existing methods.

Abstract: Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame

</details>


### [449] [Flow Map Distillation Without Data](https://arxiv.org/abs/2511.19428)
*Shangyuan Tong,Nanye Ma,Saining Xie,Tommi Jaakkola*

Main category: cs.LG

TL;DR: Data-free flow map distillation that samples only from prior distribution to avoid Teacher-Data Mismatch, achieving state-of-the-art FID scores with single-step sampling.


<details>
  <summary>Details</summary>
Motivation: Conventional flow map distillation requires external datasets, risking Teacher-Data Mismatch where static datasets may not fully represent teacher's generative capabilities. This work questions if data dependency is necessary.

Method: Principled framework that learns to predict teacher's sampling path while actively correcting compounding errors, using only prior distribution sampling to ensure fidelity without external data.

Result: Achieves FID of 1.45 on ImageNet 256x256 and 1.49 on ImageNet 512x512 with only 1 sampling step, surpassing all data-based counterparts by significant margin.

Conclusion: Establishes more robust paradigm for accelerating generative models and motivates broader adoption of data-free flow map distillation.

Abstract: State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [450] [What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models](https://arxiv.org/abs/2511.19324)
*Roksana Goworek,Olivia Macmillan-Scott,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: Dense retrieval models trained for CLIR outperform lexical matching methods and gain little from document translation. Contrastive learning helps encoders with weak alignment, and re-ranking effectiveness depends on cross-encoder training data quality.


<details>
  <summary>Details</summary>
Motivation: Cross-lingual information retrieval (CLIR) faces challenges due to resource disparities, script differences, and weak cross-lingual semantic alignment in embedding models. Existing translation-based pipelines add computational overhead and noise, degrading performance.

Method: Systematically evaluated four intervention types: document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word/phrase/query-document levels, and cross-encoder re-ranking across three benchmark datasets.

Result: Dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods. Contrastive learning mitigates language biases and yields substantial improvements for weakly aligned encoders. Gains are most pronounced for low-resource and cross-script language pairs.

Conclusion: Cross-lingual search systems should prioritize semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, especially for cross-script and under-resourced languages.

Abstract: Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.

</details>


### [451] [Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval](https://arxiv.org/abs/2511.19325)
*Olivia Macmillan-Scott,Roksana Goworek,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: Query expansion using multilingual LLMs improves cross-lingual retrieval, with effectiveness depending on query length, language scripts, and training data similarity.


<details>
  <summary>Details</summary>
Motivation: To evaluate how multilingual large language models can enhance cross-lingual information retrieval through query expansion, addressing the gap between short queries and long documents.

Method: Evaluated recent mLLMs and fine-tuned variants across multiple generative expansion strategies, analyzing prompting techniques and linguistic factors.

Result: Query length determines effective prompting techniques; elaborate prompts don't yield additional gains. Substantial linguistic disparities persist, with largest improvements for languages with weakest baselines. Fine-tuning only helps when training and test data formats are similar.

Conclusion: There is a need for more balanced multilingual and cross-lingual training and evaluation resources to address persistent linguistic disparities and improve retrieval performance across different scripts.

Abstract: Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [452] [AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems](https://arxiv.org/abs/2511.18151)
*Rajat Bhattacharjya,Sing-Yao Wu,Hyunwoo Oh,Chaewon Nam,Suyeon Koo,Mohsen Imani,Elaheh Bozorgzadeh,Nikil Dutt*

Main category: cs.DC

TL;DR: AVERY enables Vision-Language Model deployment on UAVs through adaptive split computing that separates processing into context and insight streams, dynamically managed based on network conditions to balance accuracy and throughput.


<details>
  <summary>Details</summary>
Motivation: UAVs in disaster response need semantic reasoning capabilities that traditional CNNs can't provide, but VLMs are too resource-intensive for on-device deployment and cloud offloading fails in low-bandwidth disaster zone networks.

Method: Functional dual-stream split computing: high-frequency low-resolution context stream for real-time awareness and low-frequency high-fidelity insight stream for deep analysis, managed by a lightweight on-board controller that dynamically selects compression models based on network conditions.

Result: AVERY outperforms static configurations with 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution under fluctuating network conditions.

Conclusion: AVERY enables real-time, queryable intelligence on resource-constrained UAV platforms in dynamic environments by adaptively managing the accuracy-throughput trade-off through cognitive-inspired split computing.

Abstract: Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution "context stream" for real-time awareness and a low-frequency, high-fidelity "insight stream" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [453] [Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop](https://arxiv.org/abs/2511.17673)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: SCL is a modular architecture that separates agent cognition into five phases (R-CCAM) with Soft Symbolic Control, achieving zero policy violations and complete traceability while maintaining neural flexibility.


<details>
  <summary>Details</summary>
Motivation: Address fundamental problems in LLM agents: entangled reasoning/execution, memory volatility, and uncontrolled action sequences.

Method: Modular R-CCAM architecture (Retrieval, Cognition, Control, Action, Memory) with Soft Symbolic Control that applies symbolic constraints to probabilistic inference.

Result: Achieves zero policy violations, eliminates redundant tool calls, maintains complete decision traceability on multi-step conditional reasoning tasks.

Conclusion: SCL offers a practical path toward reliable, explainable, and governable AI agents by connecting expert system principles with modern LLM capabilities.

Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents. Code: https://github.com/enkiluv/scl-core-experiment Demo: https://scl-travel-planner.streamlit.app/

</details>


### [454] [Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers](https://arxiv.org/abs/2511.18036)
*Ziyi Guo,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: Introduces first standardized benchmark for automated scientific diagram generation with 3,000 paper-diagram pairs and multi-tier evaluation metrics, plus Paper2SysArch system achieving 69.0 score on complex cases.


<details>
  <summary>Details</summary>
Motivation: Manual diagram creation is time-consuming and subjective, while existing generative models lack structural control and semantic understanding for scientific architecture diagrams. No standardized benchmark exists for quantitative evaluation.

Method: Created benchmark with 3,000 research papers paired with ground-truth diagrams and three-tier evaluation metric (semantic accuracy, layout coherence, visual quality). Proposed Paper2SysArch system using multi-agent collaboration to convert papers into structured, editable diagrams.

Result: Paper2SysArch achieved composite score of 69.0 on manually curated challenging subset of papers, demonstrating viability for complex diagram generation tasks.

Conclusion: Established first large-scale foundational benchmark for reproducible research in automated scientific visualization, with proposed system serving as promising proof-of-concept for this complex task.

Abstract: The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task.

</details>


### [455] [AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning](https://arxiv.org/abs/2511.19304)
*Jiayi Zhang,Yiran Peng,Fanqi Kong,Yang Cheng,Yifan Wu,Zhaoyang Yu,Jinyu Xiang,Jianhao Ruan,Jinlin Wang,Maojia Song,HongZhang Liu,Xiangru Tang,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: AutoEnv framework enables automated generation of heterogeneous environments at low cost, creating AutoEnv-36 dataset with 358 levels. Analysis shows language models achieve 12-49% normalized reward, revealing challenges in cross-environment learning. Fixed learning methods don't scale across environments, while adaptive selection improves performance but has diminishing returns.


<details>
  <summary>Details</summary>
Motivation: Humans naturally adapt to diverse environments by learning underlying rules, but existing agents typically only improve within single domains. Cross-environment learning lacks standardized heterogeneous environment collections and unified ways to represent agent learning.

Method: Two-step approach: 1) AutoEnv framework treats environments as factorizable distributions over transitions, observations, and rewards for low-cost generation of heterogeneous worlds. 2) Formalize agent learning as component-centric process with Selection, Optimization, and Evaluation stages applied to improvable agent components.

Result: Created AutoEnv-36 dataset with 36 environments and 358 validated levels. Language models achieve 12-49% normalized reward. Fixed learning methods show decreasing gains as environments increase. Environment-adaptive selection improves performance but exhibits diminishing returns with method space expansion.

Conclusion: Fixed learning methods don't scale across heterogeneous environments. Adaptive selection helps but has limitations. AutoEnv and AutoEnv-36 provide a testbed for studying cross-environment agent learning, highlighting both necessity and current limitations of scalable cross-environment generalization.

Abstract: Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.

</details>


### [456] [PRInTS: Reward Modeling for Long-Horizon Information Seeking](https://arxiv.org/abs/2511.19314)
*Jaewoo Lee,Archiki Prasad,Justin Chih-Yao Chen,Zaid Khan,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.AI

TL;DR: PRInTS is a generative process reward model that enhances AI agents' information-seeking capabilities through dense scoring and trajectory summarization, outperforming existing methods on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing process reward models (PRMs) are limited to short reasoning with binary judgments and cannot handle the complex dimensions of information-seeking steps or growing context in long-horizon tasks.

Method: PRInTS combines dense scoring across multiple step quality dimensions (tool output interpretation, tool call informativeness) with trajectory summarization to compress context while preserving essential information for evaluation.

Result: Best-of-n sampling with PRInTS significantly improves information-seeking abilities of open-source models and specialized agents, matching or surpassing frontier models' performance with smaller backbone agents and outperforming other reward modeling baselines.

Conclusion: PRInTS effectively addresses limitations of existing PRMs by providing richer step evaluation capabilities and context management, enabling better performance in multi-step information-seeking tasks.

Abstract: Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.

</details>


### [457] [Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?](https://arxiv.org/abs/2511.17643)
*Yayan Qiu,Sean Hanna*

Main category: cs.AI

TL;DR: This paper demonstrates that pix2pix GAN can autonomously learn spatial topological relationships for architectural design, proposing a fast detection method using Grasshopper modules to evaluate and visualize the learning process.


<details>
  <summary>Details</summary>
Motivation: Current architectural design methods using image and graph-based GANs require multiple model nesting and data conversions that cause information loss, making tools complex for architects and users. The study aims to prove I2I GAN's potential for autonomous topological relationship recognition.

Method: Proposes a method using two Grasshopper-based detection modules before and after GAN to quickly detect pix2pix's ability to learn topological relationships. Provides quantitative data and visualizes the learning process, testing different input modes (greyscale vs RGB).

Result: Successfully proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. The detection method is fast, simple to operate, and fills the gap in detecting Image-based Generation GAN performance from a topological perspective.

Conclusion: The research provides theoretical foundation and data support for using GAN to preserve spatial topological characteristics in architectural design and urban renewal. The detection modules can be widely used for customizing image datasets and batch detection of topological relationships.

Abstract: Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.

</details>


### [458] [GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction](https://arxiv.org/abs/2511.18874)
*Yuzhi Chen,Yuanchang Xie,Lei Zhao,Pan Liu,Yajie Zou,Chen Wang*

Main category: cs.AI

TL;DR: GContextFormer is a map-free multimodal trajectory prediction model that uses global context-aware hybrid attention and scaled additive aggregation to achieve intention-aligned predictions without relying on HD maps.


<details>
  <summary>Details</summary>
Motivation: HD map-dependent models have issues with costly data acquisition, delayed updates, and vulnerability to corrupted inputs, while map-free approaches lack global context and suffer from motion-intention misalignment due to pairwise attention over-amplifying straight patterns.

Method: Proposes a plug-and-play encoder-decoder architecture with Motion-Aware Encoder that builds scene-level intention prior via bounded scaled additive aggregation, and Hierarchical Interaction Decoder with dual-pathway cross-attention (standard and neighbor-context-enhanced) mediated by gating module.

Result: Outperforms state-of-the-art baselines on eight highway-ramp scenarios from TOD-VT dataset, achieving greater robustness and concentrated improvements in high-curvature and transition zones with better spatial distributions.

Conclusion: GContextFormer provides interpretable, intention-aligned multimodal prediction without map reliance, with modular architecture supporting extensibility for cross-domain multimodal reasoning tasks.

Abstract: Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [459] [MURMUR: Using cross-user chatter to break collaborative language agents in groups](https://arxiv.org/abs/2511.17671)
*Atharv Singh Patlan,Peiyao Sheng,S. Ashwin Hebbar,Prateek Mittal,Pramod Viswanath*

Main category: cs.CR

TL;DR: Cross-user poisoning (CUP) attacks exploit multi-user language agents by injecting seemingly normal messages that poison shared state, causing the agent to perform unintended actions for benign users.


<details>
  <summary>Details</summary>
Motivation: As language agents expand from single-user to multi-user environments, they lack mechanisms to isolate user interactions and concurrent tasks, creating a new attack vector where adversaries can manipulate shared state to trigger malicious actions.

Method: Developed MURMUR framework to systematically study CUP attacks by composing single-user tasks into concurrent group scenarios using LLMs to generate realistic, history-aware user interactions.

Result: CUP attacks succeed at high rates and their effects persist across multiple tasks, posing fundamental risks to multi-user LLM deployments. Validated on real systems by successfully attacking popular multi-user agents.

Conclusion: Multi-user LLM deployments are vulnerable to cross-user poisoning attacks. Proposed a first-step defense using task-based clustering to mitigate this new class of vulnerability.

Abstract: Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability

</details>


### [460] [Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent Software Development Systems](https://arxiv.org/abs/2511.18467)
*Xiaoqing Wang,Keman Huang,Bin Liang,Hongyu Li,Xiaoyong Du*

Main category: cs.CR

TL;DR: LLM-driven multi-agent systems enable easy software development but introduce security risks through scenarios where either users or agents are malicious, with demonstrated attack success rates up to 93% and proposed defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the significant security risks in LLM-driven multi-agent software development systems that remain largely unexplored, particularly the threat of concealed malicious capabilities in seemingly benign applications.

Method: Introduced Implicit Malicious Behavior Injection Attack (IMBIA) to manipulate multi-agent systems and generate software with hidden malicious capabilities, and proposed Adv-IMBIA as a defense mechanism. Evaluated across ChatDev, MetaGPT, and AgentVerse frameworks.

Result: IMBIA achieved attack success rates of 93%, 45%, and 71% in Malicious User with Benign Agents scenarios, and 71%, 84%, and 45% in Benign User with Malicious Agents scenarios. Defense mechanism significantly reduced attack success rates, especially in MU-BA scenario. Compromised agents in coding and testing phases pose greatest security risks.

Conclusion: Urgent need for robust security measures in multi-agent software development systems, with practical guidelines for implementing targeted, resource-efficient defensive strategies, particularly protecting critical agents in coding and testing phases.

Abstract: The rapid advancement of Large Language Model (LLM)-driven multi-agent systems has significantly streamlined software developing tasks, enabling users with little technical expertise to develop executable applications. While these systems democratize software creation through natural language requirements, they introduce significant security risks that remain largely unexplored. We identify two risky scenarios: Malicious User with Benign Agents (MU-BA) and Benign User with Malicious Agents (BU-MA). We introduce the Implicit Malicious Behavior Injection Attack (IMBIA), demonstrating how multi-agent systems can be manipulated to generate software with concealed malicious capabilities beneath seemingly benign applications, and propose Adv-IMBIA as a defense mechanism. Evaluations across ChatDev, MetaGPT, and AgentVerse frameworks reveal varying vulnerability patterns, with IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios. Our defense mechanism reduced attack success rates significantly, particularly in the MU-BA scenario. Further analysis reveals that compromised agents in the coding and testing phases pose significantly greater security risks, while also identifying critical agents that require protection against malicious user exploitation. Our findings highlight the urgent need for robust security measures in multi-agent software development systems and provide practical guidelines for implementing targeted, resource-efficient defensive strategies.

</details>


### [461] [Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation](https://arxiv.org/abs/2511.19009)
*Junbo Zhang,Ran Chen,Qianli Zhou,Xinyang Deng,Wen Jiang*

Main category: cs.CR

TL;DR: MOSR mitigates LLM over-refusal by intervening safety representations using overlap-aware loss weighting and context-aware augmentation, achieving better safety-usability balance.


<details>
  <summary>Details</summary>
Motivation: Current LLM safety improvements cause severe over-refusal, failing to balance safety and usability, as over-refusal samples reside at the boundary between benign and malicious content.

Method: MOSR intervenes safety representations with two components: Overlap-Aware Loss Weighting (determines erasure weight by similarity to pseudo-malicious samples) and Context-Aware Augmentation (adds harmful prefixes before rejection responses).

Result: Experiments show MOSR outperforms existing approaches in mitigating over-refusal while largely maintaining safety.

Conclusion: Future defense methods should strike better balance between safety and over-refusal.

Abstract: Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal.

</details>


### [462] [FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization](https://arxiv.org/abs/2511.19248)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Aneesh Krishna*

Main category: cs.CR

TL;DR: FedPoisonTTP is a grey-box attack framework that exploits test-time personalization in federated learning to craft poisoned inputs that degrade both global and per-client performance through adversarial updates.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning work overlooks security risks during test-time local adaptation, where compromised participants can craft poisoned inputs and adversarial updates due to heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility.

Method: FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters.

Result: Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance through poisoned inputs injected during local adaptation that spread through collaborative updates.

Conclusion: The proposed FedPoisonTTP framework demonstrates significant security vulnerabilities in test-time personalization within federated learning, highlighting the need for robust defense mechanisms against such poisoning attacks.

Abstract: Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [463] [Temporal-adaptive Weight Quantization for Spiking Neural Networks](https://arxiv.org/abs/2511.17567)
*Han Zhang,Qingyan Meng,Jiaqi Wang,Baiyu Chen,Zhengyu Ma,Xiaopeng Fan*

Main category: cs.NE

TL;DR: TaWQ enables ultra-low-bit weight quantization in SNNs with minimal accuracy loss by adapting weight allocation along temporal dimension, inspired by astrocyte-mediated synaptic modulation.


<details>
  <summary>Details</summary>
Motivation: Weight quantization in spiking neural networks can reduce energy consumption but current methods sacrifice accuracy. Biological astrocyte-mediated synaptic modulation provides inspiration for better quantization approaches.

Method: Proposed Temporal-adaptive Weight Quantization (TaWQ) that incorporates weight quantization with temporal dynamics to adaptively allocate ultra-low-bit weights along the temporal dimension.

Result: Achieved high energy efficiency (4.12M, 0.63mJ) with negligible quantization loss of only 0.22% on ImageNet. Validated on both static (ImageNet) and neuromorphic (CIFAR10-DVS) datasets.

Conclusion: TaWQ successfully maintains high energy efficiency while minimizing accuracy degradation, demonstrating the effectiveness of temporal-adaptive weight quantization inspired by biological neural systems.

Abstract: Weight quantization in spiking neural networks (SNNs) could further reduce energy consumption. However, quantizing weights without sacrificing accuracy remains challenging. In this study, inspired by astrocyte-mediated synaptic modulation in the biological nervous systems, we propose Temporal-adaptive Weight Quantization (TaWQ), which incorporates weight quantization with temporal dynamics to adaptively allocate ultra-low-bit weights along the temporal dimension. Extensive experiments on static (e.g., ImageNet) and neuromorphic (e.g., CIFAR10-DVS) datasets demonstrate that our TaWQ maintains high energy efficiency (4.12M, 0.63mJ) while incurring a negligible quantization loss of only 0.22% on ImageNet.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [464] [MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/abs/2511.17889)
*Ting Huang,Dongjian Li,Rui Yang,Zeyu Zhang,Zida Yang,Hao Tang*

Main category: cs.RO

TL;DR: MobileVLA-R1 is a unified vision-language-action framework for quadruped robots that enables explicit reasoning and continuous control through chain-of-thought supervision and two-stage training.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to bridge high-level semantic reasoning with low-level actuation, leading to unstable grounding and weak generalization in real-world scenarios for quadruped robots.

Method: Constructed MobileVLA-CoT dataset with multi-granularity chain-of-thought for embodied trajectories, and introduced two-stage training combining supervised CoT alignment with GRPO reinforcement learning.

Result: Achieved approximately 5% improvement over strong baselines on VLN and VLA tasks, with robust real-world deployment on quadruped robots in complex environments.

Conclusion: MobileVLA-R1 successfully bridges the gap between semantic reasoning and continuous control, demonstrating superior performance and robust real-world applicability for quadruped robot navigation.

Abstract: Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.

</details>


### [465] [Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game](https://arxiv.org/abs/2511.17925)
*Jeonghwan Kim,Wontaek Kim,Yidan Lu,Jin Cheng,Fatemeh Zargarbashi,Zicheng Zeng,Zekun Qi,Zhiyang Dou,Nitish Sontakke,Donghoon Baek,Sehoon Ha,Tianyu Li*

Main category: cs.RO

TL;DR: Switch-JustDance is a low-cost benchmarking pipeline that uses Nintendo Switch's Just Dance game to evaluate robot whole-body control through game scoring, validated for reliability and used to benchmark three humanoid controllers.


<details>
  <summary>Details</summary>
Motivation: Standardized benchmarks for evaluating whole-body robot control in real-world settings with direct human comparison are scarce, as existing methods rely on pre-collected data or simulations that limit reproducibility and fair comparisons.

Method: The pipeline uses Just Dance on Nintendo Switch, converting in-game choreography into robot-executable motions through streaming, motion reconstruction, and retargeting modules, then evaluates controller performance using the game's built-in scoring system.

Result: Validation shows Just Dance provides consistent and interpretable performance measures suitable for benchmarking. Three state-of-the-art humanoid controllers were benchmarked on hardware, revealing their relative strengths and limitations.

Conclusion: Switch-JustDance offers a reproducible, low-cost benchmarking solution for robot whole-body control that enables direct comparison with human performance using commercially available gaming technology.

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.

</details>


### [466] [Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting](https://arxiv.org/abs/2511.18140)
*Yilong Wang,Cheng Qian,Ruomeng Fan,Edward Johns*

Main category: cs.RO

TL;DR: ObAct is a framework where one robot arm (observer) moves to find optimal camera views for another arm (actor) to improve imitation learning performance, significantly outperforming static-camera setups.


<details>
  <summary>Details</summary>
Motivation: To address occlusion and visibility issues in robotic manipulation by dynamically finding optimal camera viewpoints that provide clearer observations of both objects and grippers.

Method: Uses dual-arm robot with wrist cameras; observer arm builds 3D Gaussian Splatting representation, virtually explores for optimal camera pose, then moves there; actor arm executes policy using observer's clear observations.

Result: Significant improvements over static cameras: trajectory transfer improved by 145% (no occlusion) and 233% (with occlusion); behavior cloning improved by 75% and 143% respectively.

Conclusion: Dynamic observer-actor role assignment enables training more robust ambidextrous policies by maintaining observations closer to occlusion-free training distribution.

Abstract: We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.

</details>


### [467] [Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video](https://arxiv.org/abs/2511.18322)
*Henrik Krauss,Johann Licher,Naoya Takeishi,Annika Raatz,Takehisa Yairi*

Main category: cs.RO

TL;DR: ABCD enables data-driven learning of soft continuum robot dynamics with physical interpretability by generating attention maps that localize latent dimensions' contributions and coupling them to 2D oscillator networks for direct visualization.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between data-driven approaches (lacking interpretability) and model-based approaches (requiring prior knowledge and being computationally expensive) for soft continuum robot dynamics learning.

Method: Introduce Attention Broadcast Decoder (ABCD) for autoencoder-based latent dynamics learning, coupled with 2D oscillator networks to visualize learned dynamics directly on images without prior knowledge.

Result: ABCD-based models achieve 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on two-segment robots, enable smooth latent space extrapolation, and autonomously discover chain structures of oscillators.

Conclusion: The approach yields compact, physically interpretable models suitable for control applications, providing a fully data-driven solution with improved accuracy and interpretability.

Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.

</details>


### [468] [Enhancing UAV Search under Occlusion using Next Best View Planning](https://arxiv.org/abs/2511.18353)
*Sigrid Helene Strand,Thomas Wiedemann,Bram Burczek,Dmitriy Shutin*

Main category: cs.RO

TL;DR: This paper presents an optimized planning strategy for UAV search and rescue in dense forests, proposing two novel heuristics (geometry and visibility) to solve the next best view problem in occluded environments.


<details>
  <summary>Details</summary>
Motivation: Search and rescue missions in dense forests with high occlusion are challenging, requiring UAVs to capture clear ground views. Current approaches need better camera positioning strategies to optimize search effectiveness in such environments.

Method: Proposed two novel optimization heuristics: a geometry heuristic and a visibility heuristic for selecting optimal camera viewpoints. Developed an efficient algorithm for the next best view problem and conducted comparative evaluations in simulated and real-world settings.

Result: The visibility heuristic achieved over 90% detection of hidden objects in simulated forests and offered 10% better detection rates than the geometry heuristic. Real-world experiments showed better coverage under the canopy with the visibility heuristic.

Conclusion: The visibility heuristic demonstrates superior performance for search and rescue missions in occluded environments, significantly improving object detection and coverage in dense forest settings.

Abstract: Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.

</details>


### [469] [AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations](https://arxiv.org/abs/2511.18617)
*Litian Gong,Fatemeh Bahrani,Yutai Zhou,Amin Banayeeanzade,Jiachen Li,Erdem Biyik*

Main category: cs.RO

TL;DR: AutoFocus-IL improves visual imitation learning by using vision-language models to automatically generate temporal saliency maps that highlight task-relevant features, outperforming methods requiring costly human supervision.


<details>
  <summary>Details</summary>
Motivation: Existing saliency regularization approaches require expensive human supervision like gaze data or manual annotations, which limits scalability and practical deployment.

Method: Leverages vision-language models to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors, then uses these maps to regularize behavior cloning policies.

Result: Experiments in CARLA simulator and real-robot manipulation tasks show AutoFocus-IL outperforms standard behavior cloning and state-of-the-art baselines that use privileged human supervision like gaze data.

Conclusion: AutoFocus-IL provides an effective and scalable approach for improving data efficiency and generalization in visual imitation learning without requiring costly human supervision.

Abstract: AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.

</details>


### [470] [Stable Multi-Drone GNSS Tracking System for Marine Robots](https://arxiv.org/abs/2511.18694)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Zhizun Wang,Junming Shi,Gregory Dudek*

Main category: cs.RO

TL;DR: Multi-drone GNSS-based tracking system for marine robots using visual detection, multi-object tracking, triangulation, and EKF for real-time GNSS estimation with cross-drone ID alignment.


<details>
  <summary>Details</summary>
Motivation: GNSS signals are unreliable underwater, and traditional alternatives like inertial navigation, DVL, SLAM, and acoustic methods have issues with error accumulation, high computation, or infrastructure dependence.

Method: Combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, confidence-weighted EKF for stable GNSS estimation, and cross-drone tracking ID alignment for global consistency.

Result: Validated in diversified complex settings showing scalability and robustness of the proposed algorithm.

Conclusion: The system provides a scalable and robust solution for GNSS-based tracking of surface and near-surface marine robots using multiple drones.

Abstract: Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.

</details>


### [471] [CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection](https://arxiv.org/abs/2511.18702)
*Xueyan Oh,Leonard Loh,Shaohui Foong,Zhong Bao Andy Koh,Kow Leong Ng,Poh Kang Tan,Pei Lin Pearlin Toh,U-Xuan Tan*

Main category: cs.RO

TL;DR: Proposes an infrastructure-free method for estimating pan-tilt-zoom camera pose and localizing scan images for aircraft visual inspection, achieving high accuracy without requiring physical contact or UAVs.


<details>
  <summary>Details</summary>
Motivation: Automating General Visual Inspection of aircraft at boarding gates to minimize downtime and reduce human labor reliance, while overcoming challenges like infrastructure requirements, contact restrictions, and limited turnaround time.

Method: Uses a Deep Convolutional Neural Network fine-tuned on synthetic images with domain randomization to predict camera pose, leverages aircraft geometry in loss function, and implements workflow for initialization, scan path planning, and precise image localization.

Result: Achieved root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes in experiments with actual aircraft.

Conclusion: The proposed infrastructure-free method successfully enables automated aircraft inspection without physical contact or UAVs, meeting the practical constraints of airport operations while maintaining high pose estimation accuracy.

Abstract: General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.

</details>


### [472] [Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation](https://arxiv.org/abs/2511.18950)
*Juntao Gao,Feiyang Ye,Jing Zhang,Wenjing Qian*

Main category: cs.RO

TL;DR: Compressor-VLA is a hybrid instruction-conditioned token compression framework that reduces computational overhead in Vision-Language-Action models while preserving task-critical visual information through semantic and spatial compression modules.


<details>
  <summary>Details</summary>
Motivation: Standard token pruning techniques in VLA models are task-agnostic and struggle to preserve task-critical visual information, creating a bottleneck for real-time robotic deployment due to redundant visual token processing.

Method: Proposes a hybrid framework with two modules: Semantic Task Compressor (STC) for holistic task-relevant context distillation, and Spatial Refinement Compressor (SRC) for fine-grained spatial detail preservation, dynamically modulated by natural language instructions.

Result: Achieves competitive success rate on LIBERO benchmark while reducing FLOPs by 59% and visual token count by over 3x compared to baseline. Real-robot deployments validate sim-to-real transferability and practical applicability.

Conclusion: The instruction-guided compression effectively steers perceptual focus toward task-relevant objects, demonstrating an efficient approach for task-oriented visual information compression in VLA models suitable for real-time robotic applications.

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.

</details>


### [473] [Mixture of Horizons in Action Chunking](https://arxiv.org/abs/2511.19433)
*Dong Jing,Gang Wang,Jiaqi Liu,Weiliang Tang,Zelong Sun,Yunchao Yao,Zhenyu Wei,Yunhui Liu,Zhiwu Lu,Mingyu Ding*

Main category: cs.RO

TL;DR: Proposes Mixture of Horizons (MoH) strategy to address the trade-off between long-term foresight and fine-grained accuracy in vision-language-action models by processing multiple action horizons in parallel and fusing outputs.


<details>
  <summary>Details</summary>
Motivation: Fixed action chunk lengths in VLA models create a trade-off: longer horizons provide better global foresight but worse fine-grained control, while shorter horizons improve local precision but struggle with long-term tasks.

Method: MoH rearranges action chunks into segments with different horizons, processes them in parallel with shared action transformer, and fuses outputs using a lightweight linear gate.

Result: MoH achieves 99% average success rate on LIBERO benchmark with only 30k training iterations, enables 2.5x higher throughput than baselines while maintaining superior performance, and shows consistent gains across different policy types.

Conclusion: MoH effectively mitigates the horizon trade-off in VLA models, providing joint benefits of long-term foresight and short-term precision with minimal overhead, establishing new state-of-the-art performance.

Abstract: Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [474] [From Competition to Coordination: Market Making as a Scalable Framework for Safe and Aligned Multi-Agent LLM Systems](https://arxiv.org/abs/2511.17621)
*Brendan Gho,Suman Muppavarapu,Afnan Shaik,Tyson Tsay,James Begin,Kevin Zhu,Archana Vaidheeswaran,Vasu Sharma*

Main category: cs.MA

TL;DR: A market-making framework for multi-agent LLM coordination that organizes agent interactions as economic exchanges, enabling self-organizing verifiable reasoning without external enforcement.


<details>
  <summary>Details</summary>
Motivation: Foundation models deployed as interacting agents in multi-agent systems raise challenges for trustworthiness, transparency, and accountability. Traditional coordination mechanisms struggle to scale and obscure decision emergence.

Method: Market-making framework where each agent acts as market participant updating and trading probabilistic beliefs to converge toward shared truthful outcomes, aligning local incentives with collective epistemic goals.

Result: Empirical evaluation shows accuracy gains up to 10% over single-shot baselines across factual reasoning, ethical judgment, and commonsense inference tasks, while preserving interpretability and transparency of intermediate reasoning steps.

Conclusion: Economic coordination principles can operationalize accountability and robustness in multi-agent LLM systems, offering scalable pathway toward self-correcting, socially responsible AI capable of maintaining trust and oversight.

Abstract: As foundation models are increasingly deployed as interacting agents in multi-agent systems, their collective behavior raises new challenges for trustworthiness, transparency, and accountability. Traditional coordination mechanisms, such as centralized oversight or adversarial adjudication, struggle to scale and often obscure how decisions emerge. We introduce a market-making framework for multi-agent large language model (LLM) coordination that organizes agent interactions as structured economic exchanges. In this setup, each agent acts as a market participant, updating and trading probabilistic beliefs, to converge toward shared, truthful outcomes. By aligning local incentives with collective epistemic goals, the framework promotes self-organizing, verifiable reasoning without requiring external enforcement. Empirically, we evaluate this approach across factual reasoning, ethical judgment, and commonsense inference tasks. Market-based coordination yields accuracy gains of up to 10% over single-shot baselines while preserving interpretability and transparency of intermediate reasoning steps. Beyond these improvements, our findings demonstrate that economic coordination principles can operationalize accountability and robustness in multi-agent LLM systems, offering a scalable pathway toward self-correcting, socially responsible AI capable of maintaining trust and oversight in real world deployment scenarios.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [475] [Comparing Labeled Markov Chains: A Cantor-Kantorovich Approach](https://arxiv.org/abs/2511.18103)
*Adrien Banse,Alessandro Abate,Raphaël M. Jungers*

Main category: cs.LO

TL;DR: The paper analyzes the Cantor-Kantorovich (CK) distance for comparing Labeled Markov Chains, showing it can be expressed as a discounted sum of finite-horizon Total Variation distances, and examines its computational complexity (#P-hard), continuity properties, and approximation methods.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous theoretical foundation for the CK distance and clarify its relationship with existing distances for comparing Labeled Markov Chains, which is important for assessing model accuracy and quantifying the effect of perturbations.

Method: Framing the CK distance as a discounted sum of finite-horizon Total Variation distances, analyzing computational complexity, continuity properties, and developing approximation schemes.

Result: The exact computation of the CK distance is #P-hard, an upper bound on CK distance is provided as a function of approximation relation, bounded CK distance implies bounded error between probabilities of finite-horizon traces, and a computable approximation scheme is developed but also shown to be #P-hard.

Conclusion: The study provides comprehensive theoretical analysis of the CK distance, establishing its computational hardness and approximation properties, and clarifying its relationship with other distances for Labeled Markov Chain comparison.

Abstract: Labeled Markov Chains (or LMCs for short) are useful mathematical objects to model complex probabilistic languages. A central challenge is to compare two LMCs, for example to assess the accuracy of an abstraction or to quantify the effect of model perturbations. In this work, we study the recently introduced Cantor-Kantorovich (or CK) distance. In particular we show that the latter can be framed as a discounted sum of finite-horizon Total Variation distances, making it an instance of discounted linear distance, but arising from the natural Cantor topology. Building on the latter observation, we analyze the properties of the CK distance along three dimensions: computational complexity, continuity properties and approximation. More precisely, we show that the exact computation of the CK distance is #P-hard. We also provide an upper bound on the CK distance as a function of the approximation relation between the two LMCs, and show that a bounded CK distance implies a bounded error between probabilities of finite-horizon traces. Finally, we provide a computable approximation scheme, and show that the latter is also #P-hard. Altogether, our results provide a rigorous theoretical foundation for the CK distance and clarify its relationship with existing distances.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [476] [Multimodal Real-Time Anomaly Detection and Industrial Applications](https://arxiv.org/abs/2511.18698)
*Aman Verma,Keshav Samdani,Mohd. Samiuddin Shafi*

Main category: cs.SD

TL;DR: Development of a multimodal room-monitoring system with video and audio processing for real-time activity recognition and anomaly detection, evolving from basic to advanced architecture with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive real-time monitoring system that integrates synchronized video and audio processing for improved activity recognition and anomaly detection in both general and industrial safety applications.

Method: Two system iterations: initial version with YOLOv8, ByteTrack, and Audio Spectrogram Transformer; advanced version with multi-model audio ensembles (AST, Wav2Vec2, HuBERT), hybrid object detection (YOLO and DETR), bidirectional cross-modal attention, and multi-method anomaly detection.

Result: Experimental evaluation shows the system achieves real-time performance on standard hardware with high accuracy in general monitoring and industrial safety applications, demonstrating significant improvements in accuracy and robustness.

Conclusion: The evolution from basic to advanced multimodal architecture successfully enhances system performance, making it suitable for industrial applications while maintaining real-time operation on standard hardware.

Abstract: This paper presents the design, implementation, and evolution of a comprehensive multimodal room-monitoring system that integrates synchronized video and audio processing for real-time activity recognition and anomaly detection. We describe two iterations of the system: an initial lightweight implementation using YOLOv8, ByteTrack, and the Audio Spectrogram Transformer (AST), and an advanced version that incorporates multi-model audio ensembles, hybrid object detection, bidirectional cross-modal attention, and multi-method anomaly detection. The evolution demonstrates significant improvements in accuracy, robustness, and industrial applicability. The advanced system combines three audio models (AST, Wav2Vec2, and HuBERT) for comprehensive audio understanding, dual object detectors (YOLO and DETR) for improved accuracy, and sophisticated fusion mechanisms for enhanced cross-modal learning. Experimental evaluation shows the system's effectiveness in general monitoring scenarios as well as specialized industrial safety applications, achieving real-time performance on standard hardware while maintaining high accuracy.

</details>


### [477] [PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation](https://arxiv.org/abs/2511.18833)
*Huadai Liu,Kaicheng Luo,Wen Wang,Qian Chen,Peiwen Sun,Rongjie Huang,Xiangang Li,Jieping Ye,Wei Xue*

Main category: cs.SD

TL;DR: PrismAudio introduces a novel V2A generation framework using RL with specialized Chain-of-Thought planning to solve objective entanglement and improve performance across four perceptual dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing V2A methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment, making it difficult to balance semantic consistency, temporal synchrony, audio quality, and spatial accuracy.

Method: Decomposes reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, Spatial) with targeted rewards, uses Fast-GRPO with hybrid ODE-SDE sampling for efficient training, and introduces AudioCanvas benchmark for evaluation.

Result: Achieves state-of-the-art performance across all four perceptual dimensions on both VGGSound test set and out-of-domain AudioCanvas benchmark, demonstrating superior audio generation quality.

Conclusion: PrismAudio effectively solves the objective entanglement problem in V2A generation through multidimensional RL optimization with specialized CoT planning, while maintaining interpretability and computational efficiency.

Abstract: Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io.

</details>


### [478] [Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments](https://arxiv.org/abs/2511.19396)
*Jorge Ortigoso-Narro,Jose A. Belloch,Adrian Amor-Martin,Sandra Roger,Maximo Cobos*

Main category: cs.SD

TL;DR: An embedded system combining deep learning-based object tracking with beamforming for precise sound source localization and directional audio capture in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics, requiring systems that can maintain robust performance with multiple or moving sources.

Method: Integrates single-camera depth estimation and stereo vision for 3D object localization with a planar concentric circular microphone array using MEMS microphones for 2D beam steering. Real-time tracking adapts the array's focus to synchronize acoustic response with target position.

Result: Experimental evaluation demonstrates significant gains in signal-to-interference ratio, maintaining robust performance in the presence of multiple or moving sources.

Conclusion: The system is well-suited for teleconferencing, smart home devices, and assistive technologies by uniting learned spatial awareness with dynamic steering.

Abstract: Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [479] [TeamPath: Building MultiModal Pathology Experts with Reasoning AI Copilots](https://arxiv.org/abs/2511.17652)
*Tianyu Liu,Weihao Xuan,Hao Wu,Peter Humphrey,Marcello DiStasio,Heli Qi,Rui Yang,Simeng Han,Tinglin Huang,Fang Wu,Nan Liu,Irene Li,Hua Xu,Hongyu Zhao*

Main category: q-bio.QM

TL;DR: TeamPath is an AI system for computational pathology that uses reinforcement learning and router-enhanced solutions to assist pathologists with disease diagnosis, patch-level summarization, and cross-modality generation integrating transcriptomic data.


<details>
  <summary>Details</summary>
Motivation: Current pathology-specific visual language models lack rigorous reasoning capabilities and struggle with divergent tasks, limiting their effectiveness as AI Copilots in real clinical scenarios.

Method: The system uses reinforcement learning and router-enhanced solutions based on large-scale histopathology multimodal datasets to create a virtual assistant for pathology tasks.

Result: TeamPath can assist pathologists in working more efficiently by identifying and correcting expert conclusions and reasoning paths, as demonstrated through collaboration with Yale School of Medicine pathologists.

Conclusion: TeamPath serves as an innovative and reliable system that flexibly adapts to different needs and facilitates information communication across modalities and experts in computational pathology.

Abstract: Advances in AI have introduced several strong models in computational pathology to usher it into the era of multi-modal diagnosis, analysis, and interpretation. However, the current pathology-specific visual language models still lack capacities in making diagnosis with rigorous reasoning paths as well as handling divergent tasks, and thus challenges of building AI Copilots for real scenarios still exist. Here we introduce TeamPath, an AI system powered by reinforcement learning and router-enhanced solutions based on large-scale histopathology multimodal datasets, to work as a virtual assistant for expert-level disease diagnosis, patch-level information summarization, and cross-modality generation to integrate transcriptomic information for the clinical usage. We also collaborate with pathologists from Yale School of Medicine to demonstrate that TeamPath can assist them in working more efficiently by identifying and correcting expert conclusions and reasoning paths. Overall, TeamPath can flexibly choose the best settings according to the needs, and serve as an innovative and reliable system for information communication across different modalities and experts.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [480] [From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence](https://arxiv.org/abs/2511.18538)
*Jian Yang,Wei Zhang,Shark Liu,Jiajun Wu,Shawn Guo,Yizhi Li*

Main category: cs.SE

TL;DR: This paper provides a comprehensive analysis of code LLMs, examining their complete lifecycle from data curation to deployment, comparing general vs. specialized models, and identifying research-practice gaps in real-world software development.


<details>
  <summary>Details</summary>
Motivation: Large language models have transformed automated software development with high success rates, but there's a need for systematic examination of the complete model lifecycle and bridging the gap between academic research and practical deployment.

Method: Conducted a series of analytic and probing experiments, systematically examining data curation, post-training, advanced prompting, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents.

Result: Analyzed code capabilities of general LLMs (GPT-4, Claude, LLaMA) and specialized code LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, QwenCoder), examining techniques, design decisions, and trade-offs.

Conclusion: Identified research-practice gaps in code correctness, security, contextual awareness, and development workflow integration, while providing comprehensive analysis of scaling laws, framework selection, hyperparameters, architectures, and dataset comparisons.

Abstract: Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [481] [LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment](https://arxiv.org/abs/2511.17676)
*Xi Wang,Xianyao Ling,Kun Li,Gang Yin,Liang Zhang,Jiang Wu,Annie Wang,Weizhe Wang*

Main category: cs.DB

TL;DR: AI-driven tools like RAG and vector databases are transforming enterprise data management, with SQL generation by LLMs bridging natural language and structured data while addressing security and efficiency challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid progress in Generative AI and Agent technologies is transforming enterprise data management, creating need for new approaches that address data security, compliance, and efficient semantic querying over enterprise knowledge bases.

Method: Focuses on enterprise data analysis applications using innovative frameworks that enable complex query understanding, multi-agent collaboration, security verification, and computational efficiency through SQL generation powered by LLMs and AI agents.

Result: The paper discusses representative use cases and identifies key challenges in distributed deployment, data security, and inherent difficulties in SQL generation tasks.

Conclusion: AI-driven tools provide new pathways for semantic querying and lower barriers to enterprise data access, but challenges remain in security, distributed deployment, and SQL generation accuracy that need to be addressed.

Abstract: The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics. Traditional database applications and system deployment are fundamentally impacted by AI-driven tools, such as Retrieval-Augmented Generation (RAG) and vector database technologies, which provide new pathways for semantic querying over enterprise knowledge bases. In the meantime, data security and compliance are top priorities for organizations adopting AI technologies. For enterprise data analysis, SQL generations powered by large language models (LLMs) and AI agents, has emerged as a key bridge connecting natural language with structured data, effectively lowering the barrier to enterprise data access and improving analytical efficiency. This paper focuses on enterprise data analysis applications and system deployment, covering a range of innovative frameworks, enabling complex query understanding, multi-agent collaboration, security verification, and computational efficiency. Through representative use cases, key challenges related to distributed deployment, data security, and inherent difficulties in SQL generation tasks are discussed.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [482] [Inverse Rendering for High-Genus Surface Meshes from Multi-View Images](https://arxiv.org/abs/2511.18680)
*Xiang Gao,Xinmu Wang,Xiaolong Wu,Jiazhi Li,Jingyu Shi,Yu Guo,Yuanpeng Liu,Xiyun Song,Heather Yu,Zongfang Lin,Xianfeng David Gu*

Main category: cs.GR

TL;DR: A topology-aware inverse rendering method that reconstructs high-genus surface meshes from multi-view images using adaptive V-cycle remeshing and re-parametrized Adam optimizer to overcome gradient issues and preserve topological features.


<details>
  <summary>Details</summary>
Motivation: Existing inverse rendering methods fail on high-genus surfaces (losing key topological features) and oversmooth low-genus surfaces (losing details) due to overreliance on Adam optimizers causing vanishing/exploding gradients.

Method: Adaptive V-cycle remeshing scheme with re-parametrized Adam optimizer that periodically coarsens and refines the mesh, plus topological consistency enforcement using Gauss-Bonnet theorem to match ground truth genus numbers.

Result: Outperforms state-of-the-art methods with significant improvements in Chamfer Distance and Volume IoU, especially for high-genus surfaces, while enhancing surface details for low-genus surfaces.

Conclusion: The proposed topology-informed approach successfully reconstructs high-genus meshes by addressing gradient optimization issues and preserving essential topological features through adaptive remeshing and topological consistency enforcement.

Abstract: We present a topology-informed inverse rendering approach for reconstructing high-genus surface meshes from multi-view images. Compared to 3D representations like voxels and point clouds, mesh-based representations are preferred as they enable the application of differential geometry theory and are optimized for modern graphics pipelines. However, existing inverse rendering methods often fail catastrophically on high-genus surfaces, leading to the loss of key topological features, and tend to oversmooth low-genus surfaces, resulting in the loss of surface details. This failure stems from their overreliance on Adam-based optimizers, which can lead to vanishing and exploding gradients. To overcome these challenges, we introduce an adaptive V-cycle remeshing scheme in conjunction with a re-parametrized Adam optimizer to enhance topological and geometric awareness. By periodically coarsening and refining the deforming mesh, our method informs mesh vertices of their current topology and geometry before optimization, mitigating gradient issues while preserving essential topological features. Additionally, we enforce topological consistency by constructing topological primitives with genus numbers that match those of ground truth using Gauss-Bonnet theorem. Experimental results demonstrate that our inverse rendering approach outperforms the current state-of-the-art method, achieving significant improvements in Chamfer Distance and Volume IoU, particularly for high-genus surfaces, while also enhancing surface details for low-genus surfaces.

</details>


### [483] [ChronoGS: Disentangling Invariants and Changes in Multi-Period Scenes](https://arxiv.org/abs/2511.18794)
*Zhongtao Wang,Jiaqi Dai,Qingtian Zhu,Yilong Li,Mai Su,Fei Zhu,Meng Gai,Shaorong Wang,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.GR

TL;DR: ChronoGS is a temporally modulated Gaussian representation that reconstructs multi-period scenes using a unified anchor scaffold, enabling consistent reconstruction of evolving geometry and appearance across time periods.


<details>
  <summary>Details</summary>
Motivation: Multi-period image collections are common in real-world applications like urban mapping, construction tracking, and environmental monitoring, but existing reconstruction methods fail under long-term, discontinuous changes.

Method: Introduces ChronoGS - a temporally modulated Gaussian representation that reconstructs all periods within a unified anchor scaffold and disentangles stable and evolving components.

Result: ChronoGS consistently outperforms baselines in reconstruction quality and temporal consistency. The authors also release the ChronoScene dataset as a benchmark for multi-period scene reconstruction.

Conclusion: ChronoGS successfully addresses the challenge of reconstructing multi-period scenes with discontinuous changes, providing temporally consistent reconstruction and enabling future research through the released dataset and code.

Abstract: Multi-period image collections are common in real-world applications. Cities are re-scanned for mapping, construction sites are revisited for progress tracking, and natural regions are monitored for environmental change. Such data form multi-period scenes, where geometry and appearance evolve. Reconstructing such scenes is an important yet underexplored problem. Existing pipelines rely on incompatible assumptions: static and in-the-wild methods enforce a single geometry, while dynamic ones assume smooth motion, both failing under long-term, discontinuous changes. To solve this problem, we introduce ChronoGS, a temporally modulated Gaussian representation that reconstructs all periods within a unified anchor scaffold. It's also designed to disentangle stable and evolving components, achieving temporally consistent reconstruction of multi-period scenes. To catalyze relevant research, we release ChronoScene dataset, a benchmark of real and synthetic multi-period scenes, capturing geometric and appearance variation. Experiments demonstrate that ChronoGS consistently outperforms baselines in reconstruction quality and temporal consistency. Our code and the ChronoScene dataset are publicly available at https://github.com/ZhongtaoWang/ChronoGS.

</details>


### [484] [MatMart: Material Reconstruction of 3D Objects via Diffusion](https://arxiv.org/abs/2511.18900)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.GR

TL;DR: TTT is a novel diffusion-based framework for 3D material reconstruction that combines accurate material prediction from inputs with prior-guided generation for unobserved views, achieving high-fidelity results through end-to-end optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of physically-based material estimation and generation for 3D objects using diffusion models, providing high-fidelity reconstruction with flexibility and scalability.

Method: Two-stage reconstruction: accurate material prediction from inputs followed by prior-guided generation for unobserved views; uses progressive inference and view-material cross-attention (VMCA) for arbitrary input images; single diffusion model with end-to-end optimization.

Result: Superior performance in material reconstruction compared to existing methods, with strong scalability, flexibility, and enhanced stability across various object types.

Conclusion: TTT effectively combines material prediction and generation in a unified framework, demonstrating state-of-the-art performance in 3D material reconstruction without requiring additional pre-trained models.

Abstract: Applying diffusion models to physically-based material estimation and generation has recently gained prominence. In this paper, we propose \ttt, a novel material reconstruction framework for 3D objects, offering the following advantages. First, \ttt\ adopts a two-stage reconstruction, starting with accurate material prediction from inputs and followed by prior-guided material generation for unobserved views, yielding high-fidelity results. Second, by utilizing progressive inference alongside the proposed view-material cross-attention (VMCA), \ttt\ enables reconstruction from an arbitrary number of input images, demonstrating strong scalability and flexibility. Finally, \ttt\ achieves both material prediction and generation capabilities through end-to-end optimization of a single diffusion model, without relying on additional pre-trained models, thereby exhibiting enhanced stability across various types of objects. Extensive experiments demonstrate that \ttt\ achieves superior performance in material reconstruction compared to existing methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [485] [Deep Learning-based Lightweight RGB Object Tracking for Augmented Reality Devices](https://arxiv.org/abs/2511.17508)
*Alice Smith,Bob Johnson,Xiaoyu Zhu,Carol Lee*

Main category: cs.HC

TL;DR: A lightweight RGB object tracking algorithm for AR devices using compact Siamese networks with optimization techniques to enable real-time performance on resource-constrained hardware.


<details>
  <summary>Details</summary>
Motivation: AR applications need real-time object tracking, but current deep learning trackers are too computationally heavy for wearable AR devices.

Method: Compact Siamese neural network architecture with model pruning, quantization, and knowledge distillation to reduce model size and inference cost while maintaining accuracy.

Result: Achieves comparable accuracy to state-of-the-art trackers while running at 30 FPS on mobile AR headsets - over 10x faster than prior high-performance trackers on same hardware.

Conclusion: Enables practical, robust object tracking for AR use-cases, making interactive and dynamic AR experiences possible on lightweight devices.

Abstract: Augmented Reality (AR) applications often require robust real-time tracking of objects in the user's environment to correctly overlay virtual content. Recent advances in computer vision have produced highly accurate deep learning-based object trackers, but these models are typically too heavy in computation and memory for wearable AR devices. In this paper, we present a lightweight RGB object tracking algorithm designed specifically for resource-constrained AR platforms. The proposed tracker employs a compact Siamese neural network architecture and incorporates optimization techniques such as model pruning, quantization, and knowledge distillation to drastically reduce model size and inference cost while maintaining high tracking accuracy. We train the tracker offline on large video datasets using deep convolutional neural networks and then deploy it on-device for real-time tracking. Experimental results on standard tracking benchmarks show that our approach achieves comparable accuracy to state-of-the-art trackers, yet runs in real-time on a mobile AR headset at around 30 FPS -- more than an order of magnitude faster than prior high-performance trackers on the same hardware. This work enables practical, robust object tracking for AR use-cases, opening the door to more interactive and dynamic AR experiences on lightweight devices.

</details>

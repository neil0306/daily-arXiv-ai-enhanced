<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 102]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.LG](#cs.LG) [Total: 13]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: The paper introduces LLM-as-a-qualitative-judge, an LLM-based evaluation method for NLG systems, focusing on structured reports of common issues rather than numerical scores. It includes open-ended analysis and issue clustering, validated with human annotations.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-judge approaches are quantitative, lacking insights for NLG system improvements. This work aims to provide actionable qualitative feedback.

Method: Proposes a two-step approach: open-ended per-instance issue analysis and clustering using a cumulative algorithm. Evaluated with ~300 annotations from 12 NLG datasets.

Result: LLM-as-a-qualitative-judge correctly identifies issues in 2/3 cases and produces reports similar to human annotators.

Conclusion: The method effectively provides meaningful insights for NLG system improvements, bridging the gap between quantitative and qualitative evaluation.

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: A phrase dictionary biasing method improves speech translation by leveraging phrase mappings, outperforming phrase list biasing by 21% and enhancing multimodal models by 85% in phrase recall.


<details>
  <summary>Details</summary>
Motivation: Phrases are crucial for understanding conversations but are rare in training data, making their translation challenging in speech tasks.

Method: Proposes a phrase dictionary biasing method to utilize source-to-target phrase mappings, applied to streaming speech translation and multimodal large language models.

Result: The method outperforms phrase list biasing by 21% for streaming models and achieves 85% relative improvement in phrase recall for multimodal models.

Conclusion: Phrase dictionary biasing effectively enhances translation accuracy and phrase recall in speech and multimodal models.

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [3] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

TL;DR: The study explores how generative CNNs generalize phonotactic rules from raw audio, testing a narrow FC bottleneck and proposing a novel probing technique.


<details>
  <summary>Details</summary>
Motivation: To understand if DNNs can generalize phonotactic rules independently of lexical learning and how architectural changes affect this.

Method: Train generative CNNs on raw audio waveforms, shrink the FC bottleneck, and probe generalizations by bypassing the FC with randomized feature maps.

Result: Convolutional layers generalize phonetic dependencies beyond FC-constrained configurations, as shown by phonotactic bias in generated outputs.

Conclusion: CNNs can dynamically generalize phonotactic rules, with the convolutional block playing a key role independent of the FC bottleneck.

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [4] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: Transformers can generalize to longer inputs by leveraging task associations, where training on related longer tasks aids generalization in target tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer models generalize to longer inputs by exploring task associations and transfer learning.

Method: Investigate length generalization via task association, testing transfer across algorithmic tasks like arithmetic and string transformations.

Result: Models generalize better when trained on related longer tasks, with evidence of attention head reuse.

Conclusion: Transformers reuse inductive structures across tasks, enhancing out-of-distribution generalization.

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [5] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: The paper introduces a novel Self-Anchored Attention Model (SAAM) to detect and classify prosocial behaviors in in-game chats, improving performance by 7.9% over existing methods, and applies it to Call of Duty: Modern Warfare II.


<details>
  <summary>Details</summary>
Motivation: Prior research focused on toxic content detection, but prosocial communication is equally important for moderation. Limited datasets and models exist for identifying prosocial behaviors in game chats.

Method: Unsupervised discovery combined with expert collaboration to categorize prosocial behaviors, followed by the development of SAAM, which uses the entire training set as anchors to improve performance with scarce data.

Result: SAAM achieved a 7.9% improvement over existing techniques, enabling the first automated system for classifying prosocial behaviors in low-resource settings.

Conclusion: The research shifts moderation focus from penalizing toxicity to encouraging positive interactions, demonstrating effectiveness in Call of Duty: Modern Warfare II.

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [6] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: A framework for measuring the faithfulness of LLM-generated self-NLE by comparing them with interpretations of the model's internal hidden states.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating self-NLE faithfulness lack examination of neural activity, leading to unfaithful explanations.

Method: Introduces a novel framework comparing self-NLE with hidden state interpretations to measure faithfulness.

Result: Provides deep insights into self-NLE faithfulness and connects explanations to model reasoning.

Conclusion: Advances understanding of self-NLE faithfulness and aids in generating more reliable explanations.

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [7] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: The paper introduces $(RSA)^2$, a framework for interpreting figurative language by modeling rhetorical strategies, achieving state-of-the-art performance on irony interpretation.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with figurative language due to mismatched literal and intended meanings, requiring setting-specific motivation modeling.

Method: The $(RSA)^2$ framework extends RSA by incorporating rhetorical strategies, avoiding explicit motivation modeling.

Result: $(RSA)^2$ achieves top performance on the PragMega+ irony dataset when combined with LLMs.

Conclusion: The framework enables human-compatible figurative language interpretation without explicit motivation modeling, advancing probabilistic pragmatics.

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [8] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: The paper improves Alzheimer's dementia (AD) detection using Mistral-7B, boosting accuracy by 3.33% over existing methods and 6.35% over ADReSS 2020 benchmarks. It also highlights interpretable decision boundaries and potential for model interpretation.


<details>
  <summary>Details</summary>
Motivation: To enhance AD detection accuracy and interpretability using advanced language models, addressing limitations of current methods.

Method: Extends paired perplexity approach with Mistral-7B, fine-tunes the model, and analyzes decision boundaries and language patterns.

Result: Improved accuracy (3.33% over best paired perplexity method, 6.35% over ADReSS 2020). Clear, interpretable decision boundaries.

Conclusion: The approach effectively detects AD, offers interpretability, and reveals learned AD language patterns, enabling new interpretation and augmentation methods.

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [9] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

TL;DR: The paper introduces novel methodologies (Lion, WebR, LTE, BMC, and FollowBench) to improve the alignment of large language models (LLMs) with human expectations through data collection, training, and evaluation enhancements.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with human expectations efficiently and effectively is a critical challenge, as current methods rely on manual curation or proprietary models and lack comprehensive evaluation.

Method: Proposes Lion for adversarial distillation, WebR for automated data synthesis, LTE for knowledge integration, BMC for preference optimization, and FollowBench for evaluation.

Result: Achieves state-of-the-art zero-shot reasoning, improves data diversity and scalability, enhances knowledge updates, and better aligns models with constraints.

Conclusion: The introduced frameworks address key alignment challenges, revealing weaknesses in current models and guiding future improvements.

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [10] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: The paper explores whether LLMs possess a theory of mind by investigating their ability to infer intentions in cooperative multi-agent reinforcement learning (MARL) settings.


<details>
  <summary>Details</summary>
Motivation: Understanding if LLMs can model others' intentions is crucial for effective collaboration between humans and AI systems.

Method: The study uses cooperative MARL with LLM-based agents to simulate and analyze social reasoning.

Result: The research aims to enhance AI agents' adaptability and cooperation with humans and other AI systems.

Conclusion: This work advances hybrid human-AI collaboration, with significant implications for future human-artificial interaction.

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [11] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: RePO improves RL for LLMs by using diverse replay strategies, outperforming GRPO with higher efficiency and performance gains.


<details>
  <summary>Details</summary>
Motivation: Address the high computational costs and low data efficiency of GRPO in optimizing LLMs.

Method: Introduces RePO, leveraging off-policy samples from a replay buffer for more diverse policy optimization.

Result: Achieves significant performance gains (18.4 and 4.1 points for two models) and increases effective optimization steps by 48%.

Conclusion: RePO is a computationally efficient and effective alternative to GRPO for RL in LLMs.

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [12] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: Latent multi-head attention (MLA) with rotary positional embeddings (RoPE) in small GPT models reduces memory usage by 45% with minimal quality loss, outperforming standard attention.


<details>
  <summary>Details</summary>
Motivation: To explore efficiency-quality trade-offs in small language models using latent multi-head attention (MLA) and rotary positional embeddings (RoPE).

Method: Benchmarked three variants (MHA, MLA, MLA+RoPE) on 30M-parameter GPT models trained on synthetic stories, testing memory, speed, and quality.

Result: MLA+RoPE with half-rank latent dimensions reduced KV-cache memory by 45% with only a 0.3% validation loss increase, outperforming MHA in quality and speed.

Conclusion: MLA+RoPE is a Pareto-efficient solution for memory-constrained deployments, offering significant memory savings and speedups without compromising quality.

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [13] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

TL;DR: OmniDRCA is a parallel speech-text foundation model using joint autoregressive modeling, dual-resolution speech representations, and contrastive cross-modal alignment, achieving SOTA performance in Spoken Question Answering benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing methods for speech generation with LLMs, which either lack modality awareness or rely on interleaved modeling, by proposing a parallel joint modeling approach.

Method: OmniDRCA employs joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment to process speech and text in parallel.

Result: OmniDRCA achieves SOTA performance in Spoken Question Answering benchmarks and shows competitive results compared to interleaved models.

Conclusion: The framework demonstrates potential for full-duplex conversational scenarios, highlighting its versatility and effectiveness in joint speech-text modeling.

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [14] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: DIVE, a Diversity-Enhanced method, reconstructs dense LLMs into MoE LLMs by leveraging pruning diversity, improving training efficiency with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing MoE reconstruction methods often neglect expert diversity, leading to redundancy. DIVE addresses this by exploiting pruning-induced diversity.

Method: DIVE involves domain affinity mining, pruning-based expert reconstruction, and efficient retraining of routers, experts, and normalization modules.

Result: DIVE outperforms existing methods in training efficiency and accuracy with the same activated parameters.

Conclusion: DIVE offers a cost-effective way to enhance MoE LLMs by leveraging pruning diversity, balancing efficiency and performance.

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [15] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

TL;DR: The paper explores using LLMs to evaluate semantic and weak semantic equivalence in Text-to-SQL systems, addressing challenges in ambiguous queries and multiple valid SQL interpretations.


<details>
  <summary>Details</summary>
Motivation: The challenge of evaluating semantic equivalence in NL2SQL systems due to ambiguous user queries and multiple valid SQL interpretations.

Method: Using LLMs to assess semantic and weak semantic equivalence, analyzing common patterns of SQL equivalence and inequivalence.

Result: Insights into challenges and patterns of SQL equivalence in LLM-based evaluation.

Conclusion: LLMs can aid in evaluating SQL equivalence but face challenges in handling ambiguity and multiple valid interpretations.

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [16] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: COGENT is a curriculum-oriented framework for generating grade-appropriate educational content, addressing challenges in AI-generated STEM education materials.


<details>
  <summary>Details</summary>
Motivation: Generative AI struggles to align with curriculum standards and maintain grade-appropriate readability, especially in STEM education.

Method: COGENT incorporates curriculum components, controls readability, and uses a "wonder-based" approach for engagement. Evaluation involves LLM and human analysis.

Result: COGENT produces grade-appropriate content comparable or superior to human references.

Conclusion: The framework provides a scalable solution for high-quality, adaptive learning resources.

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [17] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

TL;DR: CoLMbo, a Speaker Language Model, enhances speaker recognition by generating detailed captions and capturing demographic attributes like dialect, gender, and age using prompt-based conditioning.


<details>
  <summary>Details</summary>
Motivation: Traditional speaker recognition systems lack the ability to provide detailed speaker characteristics or context-rich descriptions, limiting their utility.

Method: CoLMbo integrates a speaker encoder with prompt-based conditioning to dynamically adapt to new speaker traits and generate customized descriptions.

Result: The model excels in zero-shot scenarios and improves speaker profiling by capturing nuanced attributes like dialect and age.

Conclusion: CoLMbo represents a significant advancement in speaker recognition by enabling rich, structured descriptions of speaker characteristics.

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [18] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

TL;DR: The paper investigates automatic differentiation of perceived low-quality vs. high-quality news headlines/links using machine learning, achieving strong results with traditional and deep learning models.


<details>
  <summary>Details</summary>
Motivation: The rise of online news necessitates tools to distinguish low-quality from high-quality content automatically.

Method: Twelve ML models were tested on a balanced dataset of 57.5M news links/headlines (2018-2024) with 115 linguistic features, using expert-derived binary labels.

Result: Bagging classifier achieved 88.1% accuracy, while fine-tuned DistilBERT reached 90.3%, with trade-offs in training time.

Conclusion: Both traditional and deep learning models effectively differentiate news quality, balancing performance and computational cost.

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [19] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

TL;DR: Large language models (LLMs) show politeness strategies but overuse negative politeness, raising concerns about pragmatic alignment.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs use context-sensitive politeness strategies like humans.

Method: Compare human and LLM responses in constrained and open-ended tasks, analyzing linguistic strategies.

Result: Larger LLMs replicate human politeness preferences but overuse negative politeness, even in positive contexts.

Conclusion: LLMs handle politeness well but exhibit subtle misalignments, highlighting challenges for AI pragmatic alignment.

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [20] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

TL;DR: KT$^2$ is a probabilistic knowledge tracing framework using a tree-structured hierarchy of concepts, outperforming baselines in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Address challenges in low-resource KT settings by leveraging hierarchical knowledge concepts for better performance.

Method: Proposes KT$^2$, a Hidden Markov Tree Model, using EM algorithm for mastery estimation and incremental updates for predictions.

Result: KT$^2$ consistently outperforms baselines in online, low-resource scenarios.

Conclusion: KT$^2$ effectively leverages hierarchical knowledge structures to improve knowledge tracing in data-sparse environments.

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [21] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: Token Constraint Decoding (TCD) improves LLM robustness to input noise, boosting performance by up to 39% for weaker models.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to minor input perturbations despite strong MCQA performance.

Method: TCD enforces token-level alignment during inference, paired with prompt engineering.

Result: TCD restores degraded performance, with up to 39% gains for models like Gemma3 1B.

Conclusion: TCD is a practical, model-agnostic method for enhancing LLM robustness in real-world applications.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [22] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

TL;DR: PGDA-KGQA is a prompt-guided generative framework for KGQA that enhances data diversity and multi-hop reasoning through innovative augmentation strategies, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current KGQA methods, such as data scarcity, semantic distortion, and lack of multi-hop reasoning samples, by leveraging LLMs for diverse data generation.

Method: Uses a unified prompt-design paradigm to generate (question, logical form) pairs, including single-hop pseudo questions, semantic-preserving rewriting, and answer-guided reverse path exploration for multi-hop questions.

Result: Achieves improvements on WebQSP (2.8% F1, 1.2% Hits@1, 3.1% Accuracy) and ComplexWebQuestions (1.8% F1, 1.1% Hits@1, 2.4% Accuracy).

Conclusion: PGDA-KGQA effectively addresses data diversity and reasoning challenges in KGQA, demonstrating superior performance over existing methods.

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [23] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

TL;DR: The paper evaluates LLMs and LMMs for deception detection across datasets, showing fine-tuned LLMs excel in text tasks while LMMs struggle with cross-modal cues.


<details>
  <summary>Details</summary>
Motivation: Detecting deception digitally is critical but challenging, prompting evaluation of LLMs and LMMs.

Method: Assessed LLMs and LMMs on RLTD, MU3D, and OpSpam datasets using zero-shot, few-shot, and fine-tuning setups.

Result: Fine-tuned LLMs achieve top performance in text tasks; LMMs underperform with cross-modal data.

Conclusion: LLMs show promise for deception detection but have limitations, especially in multimodal contexts.

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [24] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: A novel SFT method reduces catastrophic forgetting in LLMs without needing original pre-training data, preserving general capabilities while improving task-specific performance.


<details>
  <summary>Details</summary>
Motivation: SFT often diminishes general capabilities of LLMs and exacerbates catastrophic forgetting, especially without access to original data.

Method: Reconstructs likely SFT instruction distribution, uses multi-model screening to select optimal data, and mixes it with new data for SFT.

Result: Preserves generalization in general domains while enhancing task-specific performance.

Conclusion: The proposed method offers a cost-effective solution to mitigate catastrophic forgetting in SFT without original data.

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [25] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: The paper introduces GigaChat, a family of Russian LLMs, detailing their architecture, training, and performance on benchmarks, with open-source models released for broader use.


<details>
  <summary>Details</summary>
Motivation: Limited development of Russian-specific foundational LLMs due to high computational costs.

Method: Developed GigaChat models (various sizes), detailed architecture, pre-training, and instruction-tuning. Evaluated on Russian/English benchmarks and compared to multilingual models.

Result: Performance benchmarks show competitive results. Open-source models released for research and industrial use.

Conclusion: GigaChat expands Russian NLP research and applications, with accessible models via API, Telegram, and Web.

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [26] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: UniToMBench is a unified benchmark combining SimToM and TOMBENCH to evaluate and improve Theory of Mind (ToM) in LLMs using multi-interaction tasks and evolving scenarios. It shows GPT-4o models excel in emotional/belief tasks but struggle with knowledge-based ones.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of LLMs accurately predicting human mental states (ToM) by creating a comprehensive benchmark.

Method: Developed UniToMBench with 1,000 hand-written scenarios, integrating multi-interaction tasks, perspective-taking techniques, and diverse metrics.

Result: GPT-4o models achieve >80% accuracy in emotional/belief tasks but vary in knowledge-based tasks, revealing LLM strengths/limitations.

Conclusion: UniToMBench is a valuable tool for advancing LLMs' ToM capabilities, highlighting areas for improvement.

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [27] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: POET addresses the reward-generation gap in DAAs by equal-length truncation of responses, improving alignment and performance.


<details>
  <summary>Details</summary>
Motivation: DAAs like DPO and SimPO suffer from a reward-generation gap due to misalignment between training objectives and inference performance.

Method: Introduces POET, which truncates responses to equal length, ensuring optimization converges across all positions and emphasizes prefix tokens.

Result: POET improves DPO and SimPO, achieving up to 15.6 points in AlpacaEval 2 and better downstream task performance.

Conclusion: Addressing the reward-generation gap in DAAs is crucial for better alignment and generation performance.

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [28] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: The study analyzes suicidal behaviors on YouTube using computational and expert-driven methods, identifying unique digital indicators and motivations behind sharing suicide attempts.


<details>
  <summary>Details</summary>
Motivation: Suicide is a major cause of death, and social media data can provide new insights into suicidal behavior.

Method: Combined computational (LLM-based topic modeling), hybrid (expert-reviewed topics), and top-down (psychological assessment) approaches on YouTube channel data.

Result: Identified five suicide-attempt-related topics, with two showing temporal changes. Experts missed platform-specific indicators like YouTube Engagement. Motivations for sharing attempts differed between pre- and post-attempt uploaders.

Conclusion: Integrating digital behavior and clinical insights offers a nuanced understanding of suicidality, highlighting the value of bottom-up discovery.

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [29] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
*Jiayi Yuan,Hao Li,Xinheng Ding,Wenya Xie,Yu-Jhe Li,Wentian Zhao,Kun Wan,Jing Shi,Xia Hu,Zirui Liu*

Main category: cs.CL

TL;DR: LLM performance reproducibility is fragile due to system configuration changes, with significant variations in accuracy and response length. Root cause is non-associative floating-point arithmetic. LayerCast, a lightweight pipeline, is proposed to balance precision and stability.


<details>
  <summary>Details</summary>
Motivation: To investigate and address the fragility of LLM performance reproducibility caused by system configuration changes and numerical precision issues.

Method: Systematic experiments across hardware, software, and precision settings to quantify output divergence. Development of LayerCast, a pipeline using 16-bit weights and FP32 computations.

Result: Up to 9% accuracy variation and 9,000 tokens response length difference due to GPU and batch size changes. LayerCast improves numerical stability.

Conclusion: Floating-point precision is critical for reproducibility but often overlooked. LayerCast offers a practical solution to balance efficiency and stability.

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [30] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

TL;DR: A unified rotary position embedding (RoPE) method is proposed to integrate Transformers and State Space Models (SSMs), improving speed and accuracy in long-context modeling.


<details>
  <summary>Details</summary>
Motivation: The challenge of integrating Transformers and SSMs due to incompatible positional encoding mechanisms (explicit RoPE vs. implicit convolutions) leads to performance issues.

Method: Introduces a unified RoPE methodology and a hybrid architecture (model) combining Transformer and SSM layers under this scheme.

Result: At 4K sequence length, model is 42.3% faster in training and 29.5% faster in inference, with over 4% higher accuracy than standard Transformers. Scaling also improves, with a 7.22% accuracy gain for larger versions.

Conclusion: Unified positional encoding resolves incompatibility, enabling efficient and high-performance long-context modeling in hybrid architectures.

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [31] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

TL;DR: ReasonMed introduces a large medical reasoning dataset (370k examples) and a multi-agent verification process to improve LLMs' medical QA performance. Fine-tuning with detailed CoT reasoning and concise summaries yields the best results, with ReasonMed-7B outperforming prior models.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance LLMs' capabilities in knowledge-intensive medical question answering, which remains underexplored despite their success in other domains.

Method: Constructed ReasonMed dataset via multi-agent verification and refinement, using an Error Refiner to correct reasoning paths. Fine-tuned models with detailed CoT reasoning and concise summaries.

Result: ReasonMed-7B outperforms prior sub-10B models by 4.17% and exceeds LLaMA3.1-70B on PubMedQA by 4.60%.

Conclusion: ReasonMed and its fine-tuning strategy significantly improve medical reasoning in LLMs, setting new benchmarks for smaller models.

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [32] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
*Dingjun Wu,Yukun Yan,Zhenghao Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: KG-Infused RAG enhances Retrieval-Augmented Generation by integrating knowledge graphs (KGs) and cognitive mechanisms like spreading activation, improving factual accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods rely on single knowledge sources and lack cognitive mechanisms for knowledge activation, limiting their effectiveness.

Method: Proposes KG-Infused RAG, which retrieves KG facts, expands queries, and combines structured facts with corpus passages. Uses preference learning to refine key pipeline stages.

Result: Outperforms vanilla RAG by 3.8% to 13.8% on QA benchmarks and enhances Self-RAG, showing versatility as a plug-and-play module.

Conclusion: KG-Infused RAG effectively integrates structured and unstructured knowledge, improving performance and interpretability in RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [33] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
*Georgios Chatzichristodoulou,Despoina Kosmopoulou,Antonios Kritikos,Anastasia Poulopoulou,Efthymios Georgiou,Athanasios Katsamanis,Vassilis Katsouros,Alexandros Potamianos*

Main category: cs.CL

TL;DR: MEDUSA, a multimodal framework with a four-stage training pipeline, addresses class imbalance and emotion ambiguity in SER, achieving top performance in a 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: The subjective nature of human emotions and uneven representation in naturalistic conditions make SER challenging.

Method: MEDUSA uses a four-stage pipeline: ensemble training with DeepSER, Manifold MixUp regularization, and meta-classifier optimization with soft targets and multitask learning.

Result: Ranked 1st in Task 1 of the Interspeech 2025 SER Challenge.

Conclusion: MEDUSA effectively handles SER challenges through its multimodal approach and advanced training techniques.

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [34] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
*Eleni Gkovedarou,Joke Daems,Luna De Bruyne*

Main category: cs.CL

TL;DR: The study examines gender bias in Google Translate and DeepL for English-to-Greek translations, introducing GendEL, a dataset to test bias in three areas. GPT-4o is explored as a mitigation tool.


<details>
  <summary>Details</summary>
Motivation: Concern over MT systems reinforcing gender stereotypes, especially in understudied language pairs like English-to-Greek.

Method: Analysis of gender bias in two MT systems using GendEL, a 240-sentence dataset, and testing GPT-4o for bias mitigation.

Result: Persistent gender bias in MT systems; GPT-4o shows promise but retains some biases.

Conclusion: MT systems struggle with gender inclusivity; GPT-4o offers potential but requires further refinement.

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [35] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
*Stefan Krsteski,Matea Tashkovska,Borjan Sazdov,Hristijan Gjoreski,Branislav Gerazov*

Main category: cs.CL

TL;DR: The paper introduces resources to advance LLMs for Macedonian, including a large corpus, instruction dataset, and evaluation suite. They train an 8B-parameter model (domestic-yak) that outperforms baselines and matches larger models.


<details>
  <summary>Details</summary>
Motivation: To address the limited capabilities of LLMs for low-resource languages like Macedonian by creating foundational resources.

Method: Collecting a 40GB Macedonian corpus, a 106k-instruction dataset, and constructing an evaluation suite. Training an 8B-parameter model (domestic-yak) and benchmarking it against baselines.

Result: domestic-yak outperforms all 8B-parameter models and matches larger models. Native speakers prefer it for grammatical correctness and cultural appropriateness.

Conclusion: The resources and model advance LLM adoption for underrepresented languages, with all materials openly released.

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [36] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: Survey on integrating Knowledge Graphs (KGs) with Large Language Models (LLMs), categorizing approaches into KG-enhanced LLMs and LLM-augmented KGs, highlighting gaps and future directions.


<details>
  <summary>Details</summary>
Motivation: To enhance factual grounding and reasoning in LLMs by leveraging structured knowledge from KGs, addressing gaps like scalability and data quality.

Method: Systematic examination and categorization of existing approaches, focusing on KG-enhanced LLMs and LLM-augmented KGs.

Result: Identified mutual benefits, critical gaps, and future directions like neuro-symbolic integration and dynamic KG updating.

Conclusion: Proposes future research to improve scalability, efficiency, and ethical considerations for advanced knowledge tasks.

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [37] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
*Stefan Arnold*

Main category: cs.CL

TL;DR: The paper explores how Intrinsic Dimension (ID) affects memorization in language models, finding that high-ID sequences are less likely to be memorized, especially in overparameterized models and sparse exposure.


<details>
  <summary>Details</summary>
Motivation: To understand how the latent structure (measured by ID) influences memorization in LMs, addressing privacy and intellectual property concerns.

Method: Investigates the role of ID in modulating memorization, analyzing its effects in overparameterized models and under sparse exposure.

Result: High-ID sequences are less likely to be memorized compared to low-ID sequences, particularly in overparameterized models and sparse exposure.

Conclusion: ID acts as a suppressive signal for memorization, highlighting the interplay between scale, exposure, and complexity in shaping memorization.

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [38] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
*Nicolas Audinet de Pieuchon,Adel Daoud,Connor T. Jerzak,Moa Johansson,Richard Johansson*

Main category: cs.CL

TL;DR: The paper compares debiasing methods (DSL and PPI) for LLM annotations, analyzing their performance with varying expert annotation sizes and highlighting DSL's bias reduction but inconsistent results.


<details>
  <summary>Details</summary>
Motivation: LLMs are cost-effective for text annotation but inconsistent compared to experts, potentially biasing downstream analyses. Debiasing methods like DSL and PPI aim to address this but lack finite-sample comparisons.

Method: The study evaluates DSL and PPI by scaling expert annotations and comparing their performance across tasks, focusing on bias reduction and empirical efficiency.

Result: DSL often outperforms PPI in bias reduction and efficiency but is less consistent across datasets. Both methods work well with large datasets.

Conclusion: The findings reveal a bias-variance tradeoff in debiasing methods, emphasizing the need for metrics to assess their efficiency in finite samples.

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


### [39] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
*Anna Stein,Kevin Tang*

Main category: cs.CL

TL;DR: The study compares probabilistic predictors (N-gram, NDL, and NDL with info-theory) for modeling acoustic word duration. N-gram outperforms NDL, but info-theory improves NDL.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of NDL vs. probabilistic predictors in modeling acoustic word duration and explore the role of information theory.

Method: Three models were tested: N-gram probabilistic, traditional NDL, and NDL with info-theoretic formulas, using the Buckeye corpus.

Result: N-gram model outperformed both NDL models, but info-theoretic NDL improved over traditional NDL.

Conclusion: Highlights the need for combining info-theoretic metrics and discriminative learning in modeling acoustic reduction.

Abstract: This study compares probabilistic predictors based on information theory with
Naive Discriminative Learning (NDL) predictors in modeling acoustic word
duration, focusing on probabilistic reduction. We examine three models using
the Buckeye corpus: one with NDL-derived predictors using information-theoretic
formulas, one with traditional NDL predictors, and one with N-gram
probabilistic predictors. Results show that the N-gram model outperforms both
NDL models, challenging the assumption that NDL is more effective due to its
cognitive motivation. However, incorporating information-theoretic formulas
into NDL improves model performance over the traditional model. This research
highlights a) the need to incorporate not only frequency and contextual
predictability but also average contextual predictability, and b) the
importance of combining information-theoretic metrics of predictability and
information derived from discriminative learning in modeling acoustic
reduction.

</details>


### [40] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
*Harry Walsh,Maksym Ivashechkin,Richard Bowden*

Main category: cs.CL

TL;DR: The paper proposes using Sign Language Production techniques to augment small sign language datasets, improving Sign Language Translation models by up to 19%.


<details>
  <summary>Details</summary>
Motivation: Sign languages are low-resource, making data collection challenging. The paper aims to address this by enhancing datasets for better translation models.

Method: Three techniques are used: skeleton-based production, sign stitching, and generative models (SignGAN and SignSplat) to create variation in signer appearance and motion.

Result: The methods improved Sign Language Translation model performance by up to 19%.

Conclusion: The approach effectively augments datasets and enhances translation models, even in resource-limited settings.

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [41] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: RAPL is a novel framework for graph retrieval in KGQA, addressing limitations of existing methods with a two-stage labeling strategy, model-agnostic graph transformation, and path-based reasoning, achieving superior performance and generalizability.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability and structured reasoning of LLMs in KGQA by addressing the limitations of existing RAG pipelines and graph-based retrievers.

Method: Proposes RAPL, which includes a two-stage labeling strategy, model-agnostic graph transformation, and path-based reasoning to enhance retrieval and reasoning in KGQA.

Result: RAPL outperforms state-of-the-art methods by 2.66%-20.34%, reduces performance gaps between LLM sizes, and improves cross-dataset generalizability.

Conclusion: RAPL significantly advances graph retrieval in KGQA, offering a more efficient and effective solution with strong empirical results.

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [42] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
*Nikolas Evkarpidi,Elena Tutubalina*

Main category: cs.CL

TL;DR: A system for QA over tabular data integrates text-to-SQL, text-to-code, self-correction, RAG, and an E2E module, achieving 80% accuracy and top-13 ranking in SemEval 2025 Task 8.


<details>
  <summary>Details</summary>
Motivation: To improve QA performance over tabular data by combining multiple modules and leveraging LLMs.

Method: Integration of text-to-SQL, text-to-code, self-correction, RAG, and E2E modules orchestrated by an LLM.

Result: 80% accuracy in competition, top-13 ranking, and performance comparable to proprietary LLMs.

Conclusion: The pipeline significantly improves accuracy for open-source models and addresses key challenges in QA over tables.

Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.

</details>


### [43] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
*Lihu Chen,Gaël Varoquaux*

Main category: cs.CL

TL;DR: The paper proposes a method to detect knowledge boundaries in Large Language Models using Query-Level Uncertainty, introducing a training-free technique called Internal Confidence for adaptive inference.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs' awareness of their knowledge limits to improve adaptive inference (e.g., RAG, abstention) for efficient and trustworthy AI.

Method: Proposes Internal Confidence, a training-free method leveraging self-evaluations across layers and tokens to detect knowledge boundaries.

Result: Empirical results show Internal Confidence outperforms baselines in factual QA and mathematical reasoning, and aids efficient RAG and model cascading.

Conclusion: The method effectively identifies knowledge boundaries, reducing inference costs while maintaining performance, benefiting adaptive AI systems.

Abstract: It is important for Large Language Models to be aware of the boundary of
their knowledge, the mechanism of identifying known and unknown queries. This
type of awareness can help models perform adaptive inference, such as invoking
RAG, engaging in slow and deep thinking, or adopting the abstention mechanism,
which is beneficial to the development of efficient and trustworthy AI. In this
work, we propose a method to detect knowledge boundaries via Query-Level
Uncertainty, which aims to determine if the model is able to address a given
query without generating any tokens. To this end, we introduce a novel and
training-free method called \emph{Internal Confidence}, which leverages
self-evaluations across layers and tokens. Empirical results on both factual QA
and mathematical reasoning tasks demonstrate that our internal confidence can
outperform several baselines. Furthermore, we showcase that our proposed method
can be used for efficient RAG and model cascading, which is able to reduce
inference costs while maintaining performance.

</details>


### [44] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Main category: cs.CL

TL;DR: The paper addresses issues in Unstructured Knowledge Editing (UKE) for LLMs by introducing datasets for locality evaluation and optimizing fine-tuning methods, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To improve UKE by addressing the lack of locality evaluation and the failure of fine-tuning methods, enabling better knowledge updates in LLMs.

Method: Constructed datasets (UnKEBench-Loc and AKEW-Loc) for locality evaluation and identified factors affecting fine-tuning performance, leading to an optimized FT-UKE method.

Result: FT-UKE outperforms existing SOTA methods, with performance advantages increasing in batch editing scenarios.

Conclusion: The study provides a systematic approach to UKE, demonstrating the effectiveness of optimized fine-tuning and offering a training recipe for future research.

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [45] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
*Haoyi Song,Ruihan Ji,Naichen Shi,Fan Lai,Raed Al Kontar*

Main category: cs.CL

TL;DR: The paper introduces a probabilistic framework for uncertainty quantification (UQ) in large language models (LLMs), proposing a new measure called Inv-Entropy and a perturbation algorithm (GAAP). It also introduces a new evaluation metric, TSU, and demonstrates superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing UQ methods for LLMs are heuristic and lack a probabilistic foundation, necessitating a more rigorous approach.

Method: The paper proposes a dual random walk perspective, modeling input-output pairs as Markov chains, and introduces a probabilistic framework with Inv-Entropy. It also develops GAAP for perturbation and TSU for evaluation.

Result: Inv-Entropy outperforms existing semantic UQ methods, validated through extensive experiments.

Conclusion: The proposed framework provides a flexible, probabilistic approach to UQ in LLMs, with practical tools like GAAP and TSU enhancing its applicability.

Abstract: Large language models (LLMs) have transformed natural language processing,
but their reliable deployment requires effective uncertainty quantification
(UQ). Existing UQ methods are often heuristic and lack a probabilistic
foundation. This paper begins by providing a theoretical justification for the
role of perturbations in UQ for LLMs. We then introduce a dual random walk
perspective, modeling input-output pairs as two Markov chains with transition
probabilities defined by semantic similarity. Building on this, we propose a
fully probabilistic framework based on an inverse model, which quantifies
uncertainty by evaluating the diversity of the input space conditioned on a
given output through systematic perturbations. Within this framework, we define
a new uncertainty measure, Inv-Entropy. A key strength of our framework is its
flexibility: it supports various definitions of uncertainty measures,
embeddings, perturbation strategies, and similarity metrics. We also propose
GAAP, a perturbation algorithm based on genetic algorithms, which enhances the
diversity of sampled inputs. In addition, we introduce a new evaluation metric,
Temperature Sensitivity of Uncertainty (TSU), which directly assesses
uncertainty without relying on correctness as a proxy. Extensive experiments
demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code
to reproduce the results can be found at
https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [46] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: ComfyUI-R1 is a large reasoning model for automated workflow generation in AI art, trained with a two-stage framework and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The steep learning curve for crafting effective AI workflows on platforms like ComfyUI necessitates an automated solution.

Method: A two-stage framework: (1) CoT fine-tuning for domain adaptation, (2) reinforcement learning with rule-metric hybrid rewards.

Result: Achieves 97% format validity, high pass rates, and superior F1 scores, outperforming GPT-4o and Claude models.

Conclusion: ComfyUI-R1 demonstrates the potential of long chain-of-thought reasoning for enhancing AI art creation.

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [47] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
*Andreas Säuberli,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: The paper evaluates if LLMs can mimic human responses in educational tests, finding that while larger models can be calibrated to be more human-like, they aren't reliable for zero-shot test piloting.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs can replace human participants in pilot studies for educational assessments, speeding up test development.

Method: Analyzed 18 instruction-tuned LLMs using psychometric frameworks (classical test theory and item response theory) on multiple-choice test items in reading, U.S. history, and economics.

Result: Larger models are overly confident but can be calibrated for human-like responses. LLMs correlate better with humans in reading but not strongly enough for zero-shot use.

Conclusion: LLMs should not replace human participants in educational test piloting without further calibration or context.

Abstract: Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [48] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT is a post-training framework that improves Large Reasoning Models (LRMs) by teaching them to efficiently use Code Interpreters (CI) for complex math tasks, achieving significant performance gains and token efficiency.


<details>
  <summary>Details</summary>
Motivation: LRMs struggle with complex mathematical operations despite progress in natural language reasoning. Integrating computational tools like CI is promising but technically challenging due to inefficiencies in direct combination.

Method: CoRT uses Hint-Engineering to synthesize code-integrated reasoning data, post-training models (1.5B to 32B) with supervised fine-tuning, rejection fine-tuning, and reinforcement learning.

Result: Hint-Engineering models achieve 4-8% absolute improvements on mathematical reasoning tasks and reduce token usage by 30-50%.

Conclusion: CoRT effectively bridges LRMs and CI, enhancing performance and efficiency in mathematical reasoning.

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [49] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CL

TL;DR: The paper introduces EmoNet-Voice, a new resource for speech emotion detection, including a large-scale dataset and benchmark, to address limitations in current SER datasets. It leverages synthetic audio and expert validation for fine-grained emotion evaluation.


<details>
  <summary>Details</summary>
Motivation: Current speech emotion recognition datasets lack emotional granularity, privacy, and rely on acted portrayals. EmoNet-Voice aims to overcome these limitations.

Method: The paper introduces EmoNet-Voice Big (a pre-training dataset) and EmoNet-Voice Bench (a benchmark dataset), using synthetic audio snippets validated by psychology experts.

Result: Empathic Insight Voice models achieve high agreement with human experts, with findings like high-arousal emotions being easier to detect than low-arousal ones.

Conclusion: EmoNet-Voice provides a robust, privacy-preserving benchmark for evaluating SER models, advancing emotional understanding in AI systems.

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [50] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
*Omar Sherif,Ali Hamdi*

Main category: cs.CL

TL;DR: EGPA introduces synthetic skeleton data to address data imbalance and movement error detection in rehabilitation, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Challenges like data imbalance and detecting subtle movement errors in home-based rehabilitation assessments.

Method: Error-Guided Pose Augmentation (EGPA) simulates clinically relevant movement mistakes, combined with an attention-based graph convolutional network.

Result: Reduces mean absolute error by 27.6% and improves error classification accuracy by 45.8%.

Conclusion: EGPA enhances automated movement quality assessment in clinical and home-based rehabilitation.

Abstract: Effective rehabilitation assessment is essential for monitoring patient
progress, particularly in home-based settings. Existing systems often face
challenges such as data imbalance and difficulty detecting subtle movement
errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method
that generates synthetic skeleton data by simulating clinically relevant
movement mistakes. Unlike standard augmentation techniques, EGPA targets
biomechanical errors observed in rehabilitation. Combined with an
attention-based graph convolutional network, EGPA improves performance across
multiple evaluation metrics. Experiments demonstrate reductions in mean
absolute error of up to 27.6 percent and gains in error classification accuracy
of 45.8 percent. Attention visualizations show that the model learns to focus
on clinically significant joints and movement phases, enhancing both accuracy
and interpretability. EGPA offers a promising approach for improving automated
movement quality assessment in both clinical and home-based rehabilitation
contexts.

</details>


### [51] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: The paper addresses the detection of out-of-context and misattributed imagery in media, introducing a dataset (News Media Provenance Dataset) and two tasks (LOR and DTOR) to improve detection, with baseline results from LLMs showing promise for LOR but limitations for DTOR.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting media manipulation often fail when imagery semantically aligns with text narratives, missing deeper manipulation. This work aims to improve detection by focusing on provenance (origin and time) of images.

Method: The authors introduce the News Media Provenance Dataset, tagging images with provenance data. They define two tasks (LOR and DTOR) and evaluate six large language models (LLMs) for baseline performance.

Result: Zero-shot performance on LOR is promising, but DTOR performance is lacking, indicating the need for specialized architectures.

Conclusion: The dataset and tasks provide a foundation for better media manipulation detection, though DTOR requires further research and model specialization.

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [52] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: A causal framework addresses sufficiency and necessity in Chain-of-Thought (CoT) prompting, improving reasoning efficiency and reducing token usage without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of ensuring comprehensive (sufficiency) and essential (necessity) intermediate steps in CoT reasoning for LLMs.

Method: Proposes a causal framework using Probability of Sufficiency and Necessity to quantify step influence, enabling automated step addition/pruning.

Result: Substantial improvements in reasoning efficiency and reduced token usage across benchmarks, maintaining accuracy.

Conclusion: The framework enhances LLM reasoning performance and cost-effectiveness, offering a promising direction for future work.

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [53] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Main category: cs.CL

TL;DR: A novel method detects hallucinations in LLMs by analyzing hidden-state distribution divergence, outperforming baselines with deep learnable kernels.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs are problematic; current methods rely on external knowledge or auxiliary models, which are inefficient.

Method: Analyzes probabilistic divergence between prompt and response hidden-state distributions, using deep learnable kernels for sensitivity.

Result: Hallucinated responses show smaller deviations from prompts; the method outperforms baselines on benchmarks.

Conclusion: The approach offers a robust, scalable solution for hallucination detection without external dependencies.

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [54] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Main category: cs.CL

TL;DR: LLMs develop a language-agnostic parameter space, with shared neurons enabling abstract thought, challenging the assumption they "think" in English. Neuron-specific training strategies are proposed.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that LLMs "think" in English by investigating their multilingual performance and identifying a core language-agnostic parameter space.

Method: Identify language-related neurons (shared or exclusive) and analyze their evolution in LLMs. Propose neuron-specific training strategies.

Result: Shared neurons increase in importance, forming a core language-agnostic space, while exclusive neurons diminish. Neuron-specific training improves performance.

Conclusion: LLMs develop abstract thought through shared neurons, and tailored training strategies enhance their multilingual capabilities.

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [55] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: PersonaLens is a benchmark for evaluating personalization in task-oriented AI assistants, addressing gaps in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture the complexities of personalized task-oriented assistance, limiting evaluation of LLM-based assistants.

Method: Introduces PersonaLens with diverse user profiles, interaction histories, and two LLM-based agents (user and judge) for evaluation.

Result: Experiments show significant variability in personalization capabilities of current LLM assistants.

Conclusion: PersonaLens provides crucial insights for advancing conversational AI systems by improving personalization evaluation.

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [56] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
*Wendi Zhou,Ameer Saadat-Yazd,Nadin Kokciyan*

Main category: cs.CL

TL;DR: Proposes ASESUM, a novel summarization system for generating aspect-centric summaries from reviews without pre-defined aspects, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The impracticality of manually summarizing vast online reviews necessitates automated systems that can capture and summarize prominent opinions grounded in product aspects.

Method: ASESUM framework extracts aspect-centric arguments, measures their salience and validity, and adapts to varying domains without pre-defined aspects.

Result: Experiments on a real-world dataset show ASESUM's superiority in capturing diverse perspectives compared to existing methods.

Conclusion: ASESUM effectively automates opinion summarization by focusing on critical aspects, offering adaptability and improved performance over traditional approaches.

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


### [57] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: The paper introduces VerIF, a verification method combining rule-based and LLM-based verification to improve reinforcement learning (RL) for instruction-following in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored best practices for RL in instruction-following and the verification challenges in this context.

Method: The method, VerIF, integrates rule-based code verification with LLM-based verification using a large reasoning model (e.g., QwQ-32B). A dataset, VerInstruct, with 22,000 instances and verification signals is created to support this.

Result: RL training with VerIF improves performance on instruction-following benchmarks, achieving state-of-the-art results for models of comparable size and generalizing well to unseen constraints. General capabilities remain unaffected.

Conclusion: VerIF can enhance RL recipes for LLMs without compromising general capabilities. The authors release datasets, codes, and models to support future research.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [58] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
*Wuwei Zhang,Fangcong Yin,Howard Yen,Danqi Chen,Xi Ye*

Main category: cs.CL

TL;DR: The paper introduces QRHEAD and QR-RETRIEVER, improving retrieval in long-context LMs, achieving significant performance gains and strong zero-shot results.


<details>
  <summary>Details</summary>
Motivation: To enhance retrieval from long-context language models by identifying and utilizing query-focused attention heads (QRHEAD).

Method: QRHEAD is identified by aggregating attention scores relative to input queries, and QR-RETRIEVER uses these scores for efficient retrieval.

Result: Over 10% performance gains on multi-hop reasoning tasks and strong zero-shot performance on BEIR benchmark.

Conclusion: The work provides a general-purpose retriever and insights into long-context LM capabilities.

Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of
attention heads responsible for retrieving salient information in long-context
language models (LMs), as measured by their copy-paste behavior in
Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused
Retrieval Head), an improved set of attention heads that enhance retrieval from
long context. We identify QRHEAD by aggregating attention scores with respect
to the input query, using a handful of examples from real-world tasks (e.g.,
long-context QA). We further introduce QR- RETRIEVER, an efficient and
effective retriever that uses the accumulated attention mass of QRHEAD as
retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting
the most relevant parts with the highest retrieval scores. On multi-hop
reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains
over full context and outperforms strong dense retrievers. We also evaluate
QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves
strong zero-shot performance, outperforming other LLM-based re-rankers such as
RankGPT. Further analysis shows that both the querycontext attention scoring
and task selection are crucial for identifying QRHEAD with strong downstream
utility. Overall, our work contributes a general-purpose retriever and offers
interpretability insights into the long-context capabilities of LMs.

</details>


### [59] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Resa, a family of 1.5B reasoning models, uses SAE-Tuning to efficiently elicit strong reasoning in language models, reducing costs and training time significantly while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To explore cost-effective methods for enhancing reasoning abilities in language models by leveraging their underlying representations.

Method: SAE-Tuning: trains a sparse autoencoder (SAE) to capture reasoning abilities from a source model, then uses it to guide supervised fine-tuning in a target model.

Result: Achieves >97% of RL-trained performance with >2000x cost reduction (~$1) and >450x faster training (~20 minutes). Also improves performance on benchmarks like AIME24 and AMC23.

Conclusion: SAE-Tuning is efficient, generalizable, and modular, enabling cost-effective reasoning enhancement without retraining.

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [60] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
*Hillary Dawkins,Kathleen C. Fraser,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: Detecting AI-generated social media posts is challenging due to short, informal text. A dataset of 505,159 AI-generated posts was created, showing detection drops when attackers don't release fine-tuned models. Human studies and ablation experiments confirm vulnerabilities in detection methods.


<details>
  <summary>Details</summary>
Motivation: Social media is a key attack vector for influence campaigns using AI-generated content, making detection crucial despite challenges like short text and informal language.

Method: Created a dataset of 505,159 AI-generated posts from various LLMs (open-source, closed-source, fine-tuned) across 11 controversial topics. Evaluated detection under realistic assumptions (no model release).

Result: Detection is effective under typical research assumptions but drops significantly when attackers withhold fine-tuned models. Human studies and ablation experiments confirm this vulnerability.

Conclusion: Fine-tuning LLMs poses a major challenge to detection methods, with broad implications across detection domains due to its realistic applicability.

Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.

</details>


### [61] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
*Hiroshi Matsuda,Chunpeng Ma,Masayuki Asahara*

Main category: cs.CL

TL;DR: A novel step-by-step instruction strategy for LLMs improves dependency parsing accuracy by using universal POS tagging and a simplified CoNLL-U format, achieving SOTA results across 17 languages.


<details>
  <summary>Details</summary>
Motivation: Standard prompting in LLMs often fails to produce structurally valid outputs in dependency parsing, prompting the need for a better method.

Method: Proposes a step-by-step instruction strategy: universal POS tagging first, then predicting syntactic heads and dependency labels, using a simplified CoNLL-U format.

Result: Achieves state-of-the-art accuracy on Universal Dependencies datasets for 17 languages, with no hallucination or contamination. Multilingual fine-tuning also enhances cross-language generalization.

Conclusion: Explicit reasoning steps in LLM-based parsing are effective, offering a scalable, format-consistent alternative to bracket-based approaches.

Abstract: Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.

</details>


### [62] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
*Amel Muminovic,Amela Kadric Muminovic*

Main category: cs.CL

TL;DR: The study evaluates large language models' performance in detecting toxic comments in Serbian, Croatian, and Bosnian, showing that context-augmented prompts improve recall and F1 scores.


<details>
  <summary>Details</summary>
Motivation: Online toxic language harms communities, especially in regions with limited moderation tools and labeled data.

Method: Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, Claude 3 Opus) were tested in zero-shot and context-augmented modes on a manually labeled dataset of 4,500 YouTube/TikTok comments.

Result: Context-augmented mode improved recall by 0.12 and F1 by up to 0.10. Gemini performed best (F1: 0.82, accuracy: 0.82), while GPT-4.1 led in precision.

Conclusion: Minimal context enhances toxic language detection in low-resource settings, with practical strategies like improved prompt design and threshold calibration.

Abstract: Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.

</details>


### [63] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
*Yang Li,Qiang Sheng,Yehan Yang,Xueyao Zhang,Juan Cao*

Main category: cs.CL

TL;DR: The paper introduces a data-and-model solution for partial detection in LLM moderation, proposing FineHarm dataset and Streaming Content Monitor (SCM) to reduce latency while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing moderation methods for LLMs suffer from high latency or performance gaps when applied to partial outputs. The paper aims to bridge this gap by natively supporting partial detection.

Method: Constructs FineHarm dataset (29K prompt-response pairs) and proposes SCM, trained with dual supervision (response- and token-level labels) for real-time harmfulness judgment.

Result: SCM achieves 0.95+ macro F1 score by viewing only 18% of tokens, matching full detection performance. It also improves safety alignment.

Conclusion: SCM effectively reduces latency while maintaining accuracy, offering a practical solution for real-time LLM moderation.

Abstract: Though safety alignment has been applied to most large language models
(LLMs), LLM service providers generally deploy a subsequent moderation as the
external safety guardrail in real-world products. Existing moderators mainly
practice a conventional full detection, which determines the harmfulness based
on the complete LLM output, causing high service latency. Recent works pay more
attention to partial detection where moderators oversee the generation midway
and early stop the output if harmfulness is detected, but they directly apply
moderators trained with the full detection paradigm to incomplete outputs,
introducing a training-inference gap that lowers the performance. In this
paper, we explore how to form a data-and-model solution that natively supports
partial detection. For the data, we construct FineHarm, a dataset consisting of
29K prompt-response pairs with fine-grained annotations to provide reasonable
supervision for token-level training. Then, we propose the streaming content
monitor, which is trained with dual supervision of response- and token-level
labels and can follow the output stream of LLM to make a timely judgment of
harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is
comparable to full detection, by only seeing the first 18% of tokens in
responses on average. Moreover, the SCM can serve as a pseudo-harmfulness
annotator for improving safety alignment and lead to a higher harmlessness
score than DPO.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [64] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: ReStNet proposes a dynamic hybrid network by stitching pre-trained models, addressing deployment challenges in IoT with flexible resource adaptation.


<details>
  <summary>Details</summary>
Motivation: Deploying fixed pre-trained models in diverse IoT devices is inefficient due to heterogeneous resources. Traditional compression methods lack flexibility.

Method: ReStNet stitches models by calculating layer-wise similarity (CKA), retains early layers from a larger model, appends deeper layers from a smaller one, and fine-tunes only the stitching layer.

Result: ReStNet achieves flexible accuracy-efficiency trade-offs, reduces training costs, and supports homogeneous and heterogeneous model stitching.

Conclusion: ReStNet offers a scalable, efficient solution for dynamic model deployment in resource-constrained IoT environments.

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [65] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: The paper proposes an inference-time defense strategy for Med-VLMs to mitigate harmful queries while avoiding over-defense, balancing security and performance.


<details>
  <summary>Details</summary>
Motivation: Security vulnerabilities in Med-VLMs, such as handling harmful queries without degrading general performance, remain underexplored.

Method: A novel inference-time defense strategy using synthetic clinical demonstrations, tested on diverse medical imaging datasets.

Result: The strategy enhances safety without significant performance loss; increasing demonstration budgets alleviates over-defense.

Conclusion: A mixed demonstration strategy balances security and performance under few-shot constraints.

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [66] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: BG-HOP is a generative prior for modeling bimanual hand-object interactions in 3D, addressing data scarcity by extending single-hand priors.


<details>
  <summary>Details</summary>
Motivation: Limited bimanual interaction data hinders modeling such interactions, prompting the need for a generative solution.

Method: Extends existing single-hand generative priors to model bimanual interactions and synthesize grasps.

Result: Preliminary results show the model can generate bimanual interactions and object grasps.

Conclusion: BG-HOP demonstrates potential for bimanual interaction modeling, with code and models made available.

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [67] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

TL;DR: SAAF is a multimodal semantic-guided model for automatic segmentation of building facades, combining NLP and image features to enhance accuracy and automation.


<details>
  <summary>Details</summary>
Motivation: Improving efficiency in building information models and CAD by automating wall and window segmentation.

Method: Uses multimodal semantic collaboration (text and image features) and an end-to-end training framework for autonomous learning.

Result: Outperforms existing methods in mIoU metric, showing high precision and robustness across diverse datasets.

Conclusion: Advances architectural computer vision and explores multimodal learning applications in architecture.

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [68] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: The paper introduces two datasets, DarkEventInfer and MixVidQA, and a model, VersaVid-R1, to advance video-based reasoning under the Reason-Then-Respond paradigm, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Video-based reasoning lags behind image-based reasoning due to lack of high-quality data and effective training methods.

Method: Developed DarkEventInfer and MixVidQA datasets and trained VersaVid-R1 using reinforcement learning with diverse reward functions.

Result: VersaVid-R1 outperforms existing models in video understanding, reasoning, and captioning tasks.

Conclusion: The introduced datasets and model successfully bridge the gap in video-based reasoning, demonstrating superior performance.

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [69] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM is an open-source framework for evaluating multimodal models across vision-language tasks, offering flexible integration, efficiency, and accurate insights.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and efficient tool for assessing multimodal models, addressing the need for flexible evaluation in vision-language tasks.

Method: Decouples model inference from evaluation, uses advanced tools (e.g., vLLM, SGLang) and asynchronous data loading for efficiency.

Result: FlagEvalMM delivers accurate and efficient evaluation, highlighting model strengths and limitations.

Conclusion: FlagEvalMM is a valuable resource for advancing multimodal research, publicly available on GitHub.

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [70] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: AVA-Bench is a new benchmark that evaluates vision foundation models (VFMs) by disentangling 14 atomic visual abilities (AVAs), addressing gaps in current VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current evaluation protocols for VFMs have blind spots, such as misaligned training-test data and inability to pinpoint specific visual shortcomings.

Method: AVA-Bench decouples AVAs (e.g., localization, depth estimation) and matches training-test distributions for each, enabling precise VFM assessment.

Result: AVA-Bench reveals distinct ability fingerprints of VFMs and shows a smaller LLM (0.5B) can rank VFMs as effectively as a larger one (7B) with 8x less GPU time.

Conclusion: AVA-Bench provides a transparent and efficient framework for evaluating VFMs, aiming to advance future model development.

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [71] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

TL;DR: BakuFlow is a semi-automatic labeling tool for computer vision, enhancing efficiency with features like pixel-precise corrections, data augmentation, label propagation, and a flexible YOLOE-based auto-labeling module.


<details>
  <summary>Details</summary>
Motivation: Manual data labeling is slow and error-prone, especially for large-scale tasks, necessitating more efficient tools.

Method: BakuFlow integrates a live magnifier, interactive data augmentation, label propagation for videos, and a modified YOLOE framework for auto-labeling.

Result: The tool significantly reduces labeling workload and improves efficiency, especially for object detection and tracking.

Conclusion: BakuFlow is a practical solution for dynamic, real-world datasets, streamlining annotation in computer vision.

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [72] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: The paper examines bias in unconditional generative AI models, finding small attribute shifts but highlighting sensitivity to classifier choice in bias evaluation.


<details>
  <summary>Details</summary>
Motivation: Address concerns about representational harm and discriminatory outcomes in generative AI, focusing on bias mechanisms in unconditional generation.

Method: Train unconditional image generative models and use a bias evaluation framework to study shifts between training and generated distributions.

Result: Detected attribute shifts are small but sensitive to the classifier used, especially for attributes on a spectrum.

Conclusion: Emphasizes the need for better labeling practices, scrutiny of evaluation frameworks, and understanding the social complexity of attributes in bias assessment.

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [73] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

TL;DR: CAIRe is a new metric for evaluating cultural relevance in text-to-image models, outperforming baselines by 28% F1 and aligning well with human judgments.


<details>
  <summary>Details</summary>
Motivation: Addressing cross-cultural biases in text-to-image models, which are hindered by trade-offs and lack of reliable measurement tools.

Method: Introduces CAIRe, a framework grounding image entities in a knowledge base for graded cultural relevance judgments.

Result: CAIRe surpasses baselines by 28% F1 and achieves Pearson's correlations of 0.56 and 0.66 with human ratings.

Conclusion: CAIRe effectively measures cultural relevance, aligning with human judgment across diverse image sources.

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [74] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
*Yu Gao,Haoyuan Guo,Tuyen Hoang,Weilin Huang,Lu Jiang,Fangyuan Kong,Huixia Li,Jiashi Li,Liang Li,Xiaojie Li,Xunsong Li,Yifu Li,Shanchuan Lin,Zhijie Lin,Jiawei Liu,Shu Liu,Xiaonan Nie,Zhiwu Qing,Yuxi Ren,Li Sun,Zhi Tian,Rui Wang,Sen Wang,Guoqiang Wei,Guohong Wu,Jie Wu,Ruiqi Xia,Fei Xiao,Xuefeng Xiao,Jiangqiao Yan,Ceyuan Yang,Jianchao Yang,Runkai Yang,Tao Yang,Yihang Yang,Zilyu Ye,Xuejiao Zeng,Yan Zeng,Heng Zhang,Yang Zhao,Xiaozheng Zheng,Peihao Zhu,Jiaxin Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.0 is a high-performance video generation model addressing prompt following, motion plausibility, and visual quality through multi-source data, efficient architecture, post-training optimizations, and acceleration.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle to balance prompt adherence, motion quality, and visual fidelity. Seedance 1.0 aims to overcome these challenges.

Method: The model integrates multi-source data curation, an efficient architecture for multi-shot generation, optimized post-training (fine-tuning and RLHF), and acceleration techniques like distillation.

Result: Seedance 1.0 generates high-quality 1080p videos in 41.4 seconds, outperforming state-of-the-art models in spatiotemporal fluidity, prompt adherence, and narrative coherence.

Conclusion: Seedance 1.0 advances video generation by combining technical improvements for superior performance and efficiency.

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements
in video generation, yet current foundational model still face critical
challenges in simultaneously balancing prompt following, motion plausibility,
and visual quality. In this report, we introduce Seedance 1.0, a
high-performance and inference-efficient video foundation generation model that
integrates several core technical improvements: (i) multi-source data curation
augmented with precision and meaningful video captioning, enabling
comprehensive learning across diverse scenarios; (ii) an efficient architecture
design with proposed training paradigm, which allows for natively supporting
multi-shot generation and jointly learning of both text-to-video and
image-to-video tasks. (iii) carefully-optimized post-training approaches
leveraging fine-grained supervised fine-tuning, and video-specific RLHF with
multi-dimensional reward mechanisms for comprehensive performance improvements;
(iv) excellent model acceleration achieving ~10x inference speedup through
multi-stage distillation strategies and system-level optimizations. Seedance
1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds
(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance
1.0 stands out with high-quality and fast video generation having superior
spatiotemporal fluidity with structural stability, precise instruction
adherence in complex multi-subject contexts, native multi-shot narrative
coherence with consistent subject representation.

</details>


### [75] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/abs/2506.09229)
*Sungwon Hwang,Hyojin Jang,Kinam Kim,Minho Park,Jaegul choo*

Main category: cs.CV

TL;DR: The paper introduces CREPA, a method to improve video diffusion model fine-tuning by aligning hidden states across frames for better semantic consistency and visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning video diffusion models (VDMs) for user-specific attributes is challenging and underexplored, despite its practical importance.

Method: The authors adapt Representation Alignment (REPA) for VDMs and propose Cross-frame Representation Alignment (CREPA) to align hidden states with neighboring frames' features.

Result: CREPA enhances visual fidelity and semantic coherence in large-scale VDMs like CogVideoX-5B and Hunyuan Video, validated across diverse datasets.

Conclusion: CREPA is a broadly applicable solution for improving VDM fine-tuning, outperforming REPA in preserving semantic consistency.

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate
videos that reflect specific attributes of training data presents notable
challenges, yet remains underexplored despite its practical importance.
Meanwhile, recent work such as Representation Alignment (REPA) has shown
promise in improving the convergence and quality of DiT-based image diffusion
models by aligning, or assimilating, its internal hidden states with external
pretrained visual features, suggesting its potential for VDM fine-tuning. In
this work, we first propose a straightforward adaptation of REPA for VDMs and
empirically show that, while effective for convergence, it is suboptimal in
preserving semantic consistency across frames. To address this limitation, we
introduce Cross-frame Representation Alignment (CREPA), a novel regularization
technique that aligns hidden states of a frame with external features from
neighboring frames. Empirical evaluations on large-scale VDMs, including
CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual
fidelity and cross-frame semantic coherence when fine-tuned with
parameter-efficient methods such as LoRA. We further validate CREPA across
diverse datasets with varying attributes, confirming its broad applicability.
Project page: https://crepavideo.github.io

</details>


### [76] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: PatchGuard introduces an adversarially robust method for Anomaly Detection (AD) and Anomaly Localization (AL) using pseudo anomalies and a Vision Transformer (ViT)-based architecture, outperforming previous methods by 53.2% in AD and 68.5% in AL.


<details>
  <summary>Details</summary>
Motivation: Current AD and AL methods are vulnerable to adversarial attacks due to limited training data (only normal samples). PatchGuard aims to address this by incorporating pseudo anomalies and improving robustness.

Method: PatchGuard uses Foreground-Aware Pseudo-Anomalies in a ViT-based framework with adversarial training guided by a novel loss function. Theoretical insights into attention mechanisms enhance robustness.

Result: PatchGuard achieves significant performance gains (53.2% in AD, 68.5% in AL) in adversarial settings while maintaining competitive accuracy in non-adversarial scenarios.

Conclusion: PatchGuard is a robust solution for AD and AL, validated by theoretical and experimental results, and outperforms existing methods in adversarial conditions.

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [77] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: UFM is a unified model for dense image correspondence, outperforming specialized methods in both flow and matching tasks with higher accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Address the gap between wide-baseline scenarios and optical flow estimation by unifying training for dense correspondence.

Method: Uses a simple transformer architecture to regress (u,v) flow directly, trained on unified data for co-visible pixels.

Result: 28% more accurate than Unimatch, 62% less error and 6.7x faster than RoMa.

Conclusion: UFM proves unified training can surpass specialized approaches, enabling fast, general-purpose correspondence and new research directions.

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [78] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: A lightweight, energy-efficient YOLOv4-Tiny model is optimized via INT8 quantization for aerial emergency imagery, achieving comparable performance to YOLOv5-small while reducing size and improving speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of publicly available drone-view emergency imagery, the paper focuses on real-time object detection for emergency response using low-power edge devices.

Method: Deploying YOLOv4-Tiny with post-training INT8 quantization, trained on a custom aerial emergency dataset of 10,820 annotated images.

Result: Quantized YOLOv4-Tiny reduces model size by 71% (to 6.4 MB) and improves inference speed by 44%, with comparable mAP and F1 scores.

Conclusion: The quantized YOLOv4-Tiny is highly suitable for real-time emergency detection on edge devices, balancing performance and efficiency.

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [79] [Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/abs/2506.09300)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: A quantized YOLOv4-Tiny model was deployed on a Raspberry Pi 5 for real-time object detection in aerial emergency imagery, showing reduced power usage while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable real-time object detection in emergency scenarios using resource-constrained edge devices.

Method: The YOLOv4-Tiny model was quantized to INT8 precision using TensorFlow Lite post-training quantization and evaluated for speed, power, and thermal performance.

Result: The quantized model achieved 28.2 ms inference time per image with 13.85 W power consumption, maintaining robust accuracy for emergency classes.

Conclusion: Low-power embedded AI systems are viable for real-time safety-critical emergency response applications.

Abstract: This paper presents the deployment and performance evaluation of a quantized
YOLOv4-Tiny model for real-time object detection in aerial emergency imagery on
a resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model
was quantized to INT8 precision using TensorFlow Lite post-training
quantization techniques and evaluated for detection speed, power consumption,
and thermal feasibility under embedded deployment conditions. The quantized
model achieved an inference time of 28.2 ms per image with an average power
consumption of 13.85 W, demonstrating a significant reduction in power usage
compared to its FP32 counterpart. Detection accuracy remained robust across key
emergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These
results highlight the potential of low-power embedded AI systems for real-time
deployment in safety-critical emergency response applications.

</details>


### [80] [MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning](https://arxiv.org/abs/2506.09327)
*Tong Wang,Guanzhou Chen,Xiaodong Zhang,Chenxi Liu,Jiaqi Wang,Xiaoliang Tan,Wenchao Guo,Qingyuan Yang,Kaiqi Zhang*

Main category: cs.CV

TL;DR: A multi-modal self-supervised learning framework for remote sensing image interpretation, leveraging RGB, multi-spectral data, and DSM, outperforms existing methods on 15 datasets.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled data for remote sensing is costly and time-consuming to acquire, necessitating efficient self-supervised learning approaches.

Method: Proposes a framework with adaptive masking, cross-modal masking, and multi-task objectives to capture inter-modal correlations and intra-modal features.

Result: Achieves superior performance on tasks like semantic segmentation (78.30% mIoU), depth estimation (RMSE 0.182), and change detection (47.51% mIoU).

Conclusion: The framework effectively reduces reliance on labeled data and outperforms existing pretraining methods across diverse remote sensing tasks.

Abstract: Remote sensing image interpretation plays a critical role in environmental
monitoring, urban planning, and disaster assessment. However, acquiring
high-quality labeled data is often costly and time-consuming. To address this
challenge, we proposes a multi-modal self-supervised learning framework that
leverages high-resolution RGB images, multi-spectral data, and digital surface
models (DSM) for pre-training. By designing an information-aware adaptive
masking strategy, cross-modal masking mechanism, and multi-task self-supervised
objectives, the framework effectively captures both the correlations across
different modalities and the unique feature structures within each modality. We
evaluated the proposed method on multiple downstream tasks, covering typical
remote sensing applications such as scene classification, semantic
segmentation, change detection, object detection, and depth estimation.
Experiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.
The results demonstrate that the proposed method outperforms existing
pretraining approaches in most tasks. Specifically, on the Potsdam and
Vaihingen semantic segmentation tasks, our method achieved mIoU scores of
78.30\% and 76.50\%, with only 50\% train-set. For the US3D depth estimation
task, the RMSE error is reduced to 0.182, and for the binary change detection
task in SECOND dataset, our method achieved mIoU scores of 47.51\%, surpassing
the second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and
HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.

</details>


### [81] [CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation](https://arxiv.org/abs/2506.09343)
*Yuxing Long,Jiyao Zhang,Mingjie Pan,Tianshu Wu,Taewhan Kim,Hao Dong*

Main category: cs.CV

TL;DR: The paper introduces CheckManual, a benchmark for manual-based appliance manipulation, addressing gaps in previous research by leveraging manuals for robot tasks.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks focus on multi-page manuals for appliance manipulation, limiting robots' ability to perform tasks like heating bread in a microwave.

Method: A large model-assisted human-revised pipeline generates manuals from CAD models, creating benchmarks, challenges, and simulator environments. The ManualPlan model is proposed as a baseline.

Result: CheckManual provides a framework for evaluating manual-based manipulation, with ManualPlan setting initial performance benchmarks.

Conclusion: The work bridges the gap in manual comprehension for appliance manipulation, offering tools and models for future research.

Abstract: Correct use of electrical appliances has significantly improved human life
quality. Unlike simple tools that can be manipulated with common sense,
different parts of electrical appliances have specific functions defined by
manufacturers. If we want the robot to heat bread by microwave, we should
enable them to review the microwave manual first. From the manual, it can learn
about component functions, interaction methods, and representative task steps
about appliances. However, previous manual-related works remain limited to
question-answering tasks while existing manipulation researchers ignore the
manual's important role and fail to comprehend multi-page manuals. In this
paper, we propose the first manual-based appliance manipulation benchmark
CheckManual. Specifically, we design a large model-assisted human-revised data
generation pipeline to create manuals based on CAD appliance models. With these
manuals, we establish novel manual-based manipulation challenges, metrics, and
simulator environments for model performance evaluation. Furthermore, we
propose the first manual-based manipulation planning model ManualPlan to set up
a group of baselines for the CheckManual benchmark.

</details>


### [82] [An Effective End-to-End Solution for Multimodal Action Recognition](https://arxiv.org/abs/2506.09345)
*Songping Wang,Xiantao Hu,Yueming Lyu,Caifeng Shan*

Main category: cs.CV

TL;DR: A tri-modal action recognition solution using data enhancement, transfer learning, and multimodal spatial-temporal feature extraction achieves high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in tri-modal action recognition due to scarce data by leveraging multimodal information.

Method: Optimized data enhancement, transfer learning, 2D CNNs with TSM for spatial-temporal features, and prediction enhancement techniques like SWA, Ensemble, and TTA.

Result: Achieved Top-1 accuracy of 99% and Top-5 accuracy of 100% on the competition leaderboard.

Conclusion: The proposed solution demonstrates superiority in tri-modal action recognition by effectively utilizing multimodal data and advanced techniques.

Abstract: Recently, multimodal tasks have strongly advanced the field of action
recognition with their rich multimodal information. However, due to the
scarcity of tri-modal data, research on tri-modal action recognition tasks
faces many challenges. To this end, we have proposed a comprehensive multimodal
action recognition solution that effectively utilizes multimodal information.
First, the existing data are transformed and expanded by optimizing data
enhancement techniques to enlarge the training scale. At the same time, more
RGB datasets are used to pre-train the backbone network, which is better
adapted to the new task by means of transfer learning. Secondly, multimodal
spatial features are extracted with the help of 2D CNNs and combined with the
Temporal Shift Module (TSM) to achieve multimodal spatial-temporal feature
extraction comparable to 3D CNNs and improve the computational efficiency. In
addition, common prediction enhancement methods, such as Stochastic Weight
Averaging (SWA), Ensemble and Test-Time augmentation (TTA), are used to
integrate the knowledge of models from different training periods of the same
architecture and different architectures, so as to predict the actions from
different perspectives and fully exploit the target information. Ultimately, we
achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the
competition leaderboard, demonstrating the superiority of our solution.

</details>


### [83] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: AAPT transforms a pre-trained latent video diffusion model into a real-time, interactive video generator using autoregressive adversarial post-training, achieving 24fps at 736x416 resolution on a single H100.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models are too computationally intensive for real-time, interactive applications.

Method: Autoregressive adversarial post-training (AAPT) with 1NFE per frame, leveraging adversarial training and KV cache for efficiency.

Result: Real-time 24fps video generation at 736x416 on one H100 or 1280x720 on 8xH100, up to 1440 frames.

Conclusion: AAPT enables efficient, real-time, and interactive video generation with reduced error accumulation.

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [84] [A new approach for image segmentation based on diffeomorphic registration and gradient fields](https://arxiv.org/abs/2506.09357)
*Junchao Zhou*

Main category: cs.CV

TL;DR: A novel variational framework for 2D image segmentation using shape analysis and diffeomorphic transformations, avoiding reliance on large datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and deep learning approaches for image segmentation have limitations, such as extensive training data requirements. This work aims to provide a flexible, theoretically grounded alternative.

Method: The method models segmentation as a template curve deformation via diffeomorphic transformations using the LDDMM framework, guided by a loss function comparing the curve to the image gradient field. Implemented in Python with GPU acceleration.

Result: The framework achieves accurate segmentation without needing large datasets, leveraging a flexible and theoretically sound approach.

Conclusion: The proposed variational framework offers a promising alternative for image segmentation, combining accuracy, flexibility, and reduced data dependency.

Abstract: Image segmentation is a fundamental task in computer vision aimed at
delineating object boundaries within images. Traditional approaches, such as
edge detection and variational methods, have been widely explored, while recent
advances in deep learning have shown promising results but often require
extensive training data. In this work, we propose a novel variational framework
for 2D image segmentation that integrates concepts from shape analysis and
diffeomorphic transformations. Our method models segmentation as the
deformation of a template curve via a diffeomorphic transformation of the image
domain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)
framework. The curve evolution is guided by a loss function that compares the
deformed curve to the image gradient field, formulated through the varifold
representation of geometric shapes. The approach is implemented in Python with
GPU acceleration using the PyKeops library. This framework allows for accurate
segmentation with a flexible and theoretically grounded methodology that does
not rely on large datasets.

</details>


### [85] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: SAGE introduces semantic-augment erasing and a global-local retention mechanism to improve safety in diffusion models by erasing unsafe concepts while preserving irrelevant ones.


<details>
  <summary>Details</summary>
Motivation: Address safety risks in diffusion models (DMs) like unsafe content and copyright infringement by improving concept erasure methods.

Method: Semantic-augment erasing transforms word erasure into domain erasure using cyclic self-check and self-erasure. A global-local retention mechanism preserves irrelevant concepts.

Result: SAGE outperforms other methods in safe generation, balancing erasure and retention effectively.

Conclusion: SAGE offers a superior solution for safe DM generation, with open-sourced code and weights.

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [86] [ScaleLSD: Scalable Deep Line Segment Detection Streamlined](https://arxiv.org/abs/2506.09369)
*Zeran Ke,Bin Tan,Xianwei Zheng,Yujun Shen,Tianfu Wu,Nan Xue*

Main category: cs.CV

TL;DR: The paper introduces ScaleLSD, a scalable self-supervised learning model for Line Segment Detection (LSD) that outperforms traditional non-deep LSD methods in accuracy and versatility across various tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a domain-agnostic, robust LSD model that works well for any natural images by leveraging scalable self-supervised learning.

Method: Revisits and streamlines fundamental LSD designs to create ScaleLSD, a high-performing and efficient learner trained on over 10M unlabeled real-world images.

Result: ScaleLSD detects more line segments accurately than non-deep LSD methods and excels in tasks like 3D geometry estimation, line segment matching, and 3D line mapping.

Conclusion: ScaleLSD is the first deep LSD approach to surpass non-deep methods in all tested aspects, significantly enhancing image line geometry characterization.

Abstract: This paper studies the problem of Line Segment Detection (LSD) for the
characterization of line geometry in images, with the aim of learning a
domain-agnostic robust LSD model that works well for any natural images. With
the focus of scalable self-supervised learning of LSD, we revisit and
streamline the fundamental designs of (deep and non-deep) LSD approaches to
have a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the
curation of line geometry at scale from over 10M unlabeled real-world images.
Our ScaleLSD works very well to detect much more number of line segments from
any natural images even than the pioneered non-deep LSD approach, having a more
complete and accurate geometric characterization of images using line segments.
Experimentally, our proposed ScaleLSD is comprehensively testified under
zero-shot protocols in detection performance, single-view 3D geometry
estimation, two-view line segment matching, and multiview 3D line mapping, all
with excellent performance obtained. Based on the thorough evaluation, our
ScaleLSD is observed to be the first deep approach that outperforms the
pioneered non-deep LSD in all aspects we have tested, significantly expanding
and reinforcing the versatility of the line geometry of images. Code and Models
are available at https://github.com/ant-research/scalelsd

</details>


### [87] [UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images](https://arxiv.org/abs/2506.09378)
*Qijian Tian,Xin Tan,Jingyu Gong,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: UniForward is a feed-forward model for real-time 3D scene and semantic field reconstruction from sparse-view images, achieving state-of-the-art performance without needing camera parameters or ground truth depth.


<details>
  <summary>Details</summary>
Motivation: To unify 3D scene and semantic field reconstruction for better environment perception, addressing challenges like embedding semantics into 3D representations and generalizable real-time reconstruction.

Method: Proposes UniForward, a model predicting 3D Gaussians with semantic features from uncalibrated images, using a dual-branch decoder and loss-guided view sampling for training.

Result: Achieves high-quality 3D scene rendering and view-consistent semantic features, enabling open-vocabulary dense segmentation masks.

Conclusion: UniForward outperforms existing methods in novel view synthesis and segmentation, unifying 3D and semantic reconstruction effectively.

Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and
semantic field reconstruction. Combining 3D scenes with semantic fields
facilitates the perception and understanding of the surrounding environment.
However, key challenges include embedding semantics into 3D representations,
achieving generalizable real-time reconstruction, and ensuring practical
applicability by using only images as input without camera parameters or ground
truth depth. To this end, we propose UniForward, a feed-forward model to
predict 3D Gaussians with anisotropic semantic features from only uncalibrated
and unposed sparse-view images. To enable the unified representation of the 3D
scene and semantic field, we embed semantic features into 3D Gaussians and
predict them through a dual-branch decoupled decoder. During training, we
propose a loss-guided view sampler to sample views from easy to hard,
eliminating the need for ground truth depth or masks required by previous
methods and stabilizing the training process. The whole model can be trained
end-to-end using a photometric loss and a distillation loss that leverages
semantic features from a pre-trained 2D semantic model. At the inference stage,
our UniForward can reconstruct 3D scenes and the corresponding semantic fields
in real time from only sparse-view images. The reconstructed 3D scenes achieve
high-quality rendering, and the reconstructed 3D semantic field enables the
rendering of view-consistent semantic features from arbitrary views, which can
be further decoded into dense segmentation masks in an open-vocabulary manner.
Experiments on novel view synthesis and novel view segmentation demonstrate
that our method achieves state-of-the-art performances for unifying 3D scene
and semantic field reconstruction.

</details>


### [88] [ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model](https://arxiv.org/abs/2506.09385)
*Jialong Zuo,Yongtai Deng,Mengdan Tan,Rui Jin,Dongyue Wu,Nong Sang,Liang Pan,Changxin Gao*

Main category: cs.CV

TL;DR: The paper introduces OM-ReID, a multi-modal person re-identification problem, and proposes ORBench, a high-quality dataset, and ReID5o, a novel learning framework for effective retrieval across varied modalities.


<details>
  <summary>Details</summary>
Motivation: Existing methods and datasets for person re-identification are limited to few modalities, failing to meet real-world needs for multi-modal queries.

Method: The authors construct ORBench, a dataset with 1,000 identities across five modalities, and propose ReID5o, a framework for unified encoding and multi-expert routing to handle arbitrary modality combinations.

Result: ReID5o outperforms other models on ORBench, demonstrating its effectiveness in multi-modal person re-identification.

Conclusion: The work advances multi-modal ReID with a practical dataset and framework, paving the way for future research.

Abstract: In real-word scenarios, person re-identification (ReID) expects to identify a
person-of-interest via the descriptive query, regardless of whether the query
is a single modality or a combination of multiple modalities. However, existing
methods and datasets remain constrained to limited modalities, failing to meet
this requirement. Therefore, we investigate a new challenging problem called
Omni Multi-modal Person Re-identification (OM-ReID), which aims to achieve
effective retrieval with varying multi-modal queries. To address dataset
scarcity, we construct ORBench, the first high-quality multi-modal dataset
comprising 1,000 unique identities across five modalities: RGB, infrared, color
pencil, sketch, and textual description. This dataset also has significant
superiority in terms of diversity, such as the painting perspectives and
textual information. It could serve as an ideal platform for follow-up
investigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal
learning framework for person ReID. It enables synergistic fusion and
cross-modal alignment of arbitrary modality combinations in a single model,
with a unified encoding and multi-expert routing mechanism proposed. Extensive
experiments verify the advancement and practicality of our ORBench. A wide
range of possible models have been evaluated and compared on it, and our
proposed ReID5o model gives the best performance. The dataset and code will be
made publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.

</details>


### [89] [Improving Out-of-Distribution Detection via Dynamic Covariance Calibration](https://arxiv.org/abs/2506.09399)
*Kaiyu Guo,Zijian Wang,Brian C. Lovell,Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: The paper proposes a dynamic method to improve Out-of-Distribution (OOD) detection by adjusting prior geometry in real-time, addressing distortions from ill-distributed samples.


<details>
  <summary>Details</summary>
Motivation: Current subspace-based OOD detection methods fail to handle geometry distortions caused by ill-distributed samples due to static prior extraction.

Method: The approach dynamically updates the prior covariance matrix using real-time input features, refining information while preserving essential data characteristics.

Result: Experiments on CIFAR and ImageNet-1k, including the DINO model, show significant OOD detection improvements.

Conclusion: Dynamic adjustment of prior geometry effectively enhances OOD detection, outperforming static methods.

Abstract: Out-of-Distribution (OOD) detection is essential for the trustworthiness of
AI systems. Methods using prior information (i.e., subspace-based methods) have
shown effective performance by extracting information geometry to detect OOD
data with a more appropriate distance metric. However, these methods fail to
address the geometry distorted by ill-distributed samples, due to the
limitation of statically extracting information geometry from the training
distribution. In this paper, we argue that the influence of ill-distributed
samples can be corrected by dynamically adjusting the prior geometry in
response to new data. Based on this insight, we propose a novel approach that
dynamically updates the prior covariance matrix using real-time input features,
refining its information. Specifically, we reduce the covariance along the
direction of real-time input features and constrain adjustments to the residual
space, thus preserving essential data characteristics and avoiding effects on
unintended directions in the principal space. We evaluate our method on two
pre-trained models for the CIFAR dataset and five pre-trained models for
ImageNet-1k, including the self-supervised DINO model. Extensive experiments
demonstrate that our approach significantly enhances OOD detection across
various models. The code is released at https://github.com/workerbcd/ooddcc.

</details>


### [90] [SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2506.09403)
*Xinya Liu,Jianghao Wu,Tao Lu,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: Proposes SRPL-SFDA, a SAM-guided method for Source-Free Domain Adaptation in medical image segmentation, improving pseudo-label quality and outperforming state-of-the-art SFDA methods.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in SFDA, such as insufficient supervision in target domains with unlabeled data, while leveraging SAM's zero-shot inference for better adaptation.

Method: Introduces T3IE for input enhancement, a reliable pseudo-label selection module (CMSO), and reliability-aware training with entropy minimization.

Result: SRPL-SFDA enhances pseudo-label quality, outperforms SFDA methods, and approaches supervised training performance in target domains.

Conclusion: SRPL-SFDA is effective for SFDA in medical image segmentation, leveraging SAM and reliability-aware training for robust adaptation.

Abstract: Domain Adaptation (DA) is crucial for robust deployment of medical image
segmentation models when applied to new clinical centers with significant
domain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal
with privacy concerns and access constraints on source-domain data during
adaptation to target-domain data. However, SFDA faces challenges such as
insufficient supervision in the target domain with unlabeled images. In this
work, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels
method for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch
Intensity Enhancement (T3IE) that not only improves quality of raw
pseudo-labels in the target domain, but also leads to SAM-compatible inputs
with three channels to better leverage SAM's zero-shot inference ability for
refining the pseudo-labels; 2) A reliable pseudo-label selection module that
rejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs
(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training
procedure in the unlabeled target domain where reliable pseudo-labels are used
for supervision and unreliable parts are regularized by entropy minimization.
Experiments conducted on two multi-domain medical image segmentation datasets
for fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA
effectively enhances pseudo-label quality in the unlabeled target domain, and
improves SFDA performance by leveraging the reliability-aware training; 2)
SRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is
close to that of supervised training in the target domain. The code of this
work is available online: https://github.com/HiLab-git/SRPL-SFDA.

</details>


### [91] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Main category: cs.CV

TL;DR: Proposes a method for generating synthetic human action videos using pose transfer to address uncanny features in synthetic data, improving action recognition performance and dataset scalability.


<details>
  <summary>Details</summary>
Motivation: Synthetic data for video understanding tasks often has uncanny features, limiting its use in tasks like sign language translation and gesture recognition. This paper aims to overcome this limitation.

Method: Uses controllable 3D Gaussian avatar models for pose transfer to generate synthetic human action video data. Evaluated on Toyota Smarthome and NTU RGB+D datasets.

Result: Improves action recognition performance and effectively scales few-shot datasets by adding underrepresented groups and diverse backgrounds.

Conclusion: The method enhances synthetic data quality and utility, demonstrated by improved performance and scalability, with open-sourced tools and a new dataset (RANDOM People).

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [92] [Noise Conditional Variational Score Distillation](https://arxiv.org/abs/2506.09416)
*Xinyu Peng,Ziyang Zheng,Yaoming Wang,Han Li,Nuowen Kan,Wenrui Dai,Chenglin Li,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: NCVSD distills diffusion models into generative denoisers by leveraging the unconditional score function, enabling fast generation and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between diffusion models and efficient generative denoisers while preserving iterative refinement benefits.

Method: Integrates the unconditional score function into Variational Score Distillation (VSD) for scalable learning of denoisers across noise levels.

Result: Outperforms teacher diffusion models, matches larger consistency models, and achieves record LPIPS with fewer NFEs.

Conclusion: NCVSD offers a scalable, efficient solution for generative denoising with flexible sampling and improved performance.

Abstract: We propose Noise Conditional Variational Score Distillation (NCVSD), a novel
method for distilling pretrained diffusion models into generative denoisers. We
achieve this by revealing that the unconditional score function implicitly
characterizes the score function of denoising posterior distributions. By
integrating this insight into the Variational Score Distillation (VSD)
framework, we enable scalable learning of generative denoisers capable of
approximating samples from the denoising posterior distribution across a wide
range of noise levels. The proposed generative denoisers exhibit desirable
properties that allow fast generation while preserve the benefit of iterative
refinement: (1) fast one-step generation through sampling from pure Gaussian
noise at high noise levels; (2) improved sample quality by scaling the
test-time compute with multi-step sampling; and (3) zero-shot probabilistic
inference for flexible and controllable sampling. We evaluate NCVSD through
extensive experiments, including class-conditional image generation and inverse
problem solving. By scaling the test-time compute, our method outperforms
teacher diffusion models and is on par with consistency models of larger sizes.
Additionally, with significantly fewer NFEs than diffusion-based methods, we
achieve record-breaking LPIPS on inverse problems.

</details>


### [93] [ODG: Occupancy Prediction Using Dual Gaussians](https://arxiv.org/abs/2506.09417)
*Yunxiao Shi,Yinhao Zhu,Shizhong Han,Jisoo Jeong,Amin Ansari,Hong Cai,Fatih Porikli*

Main category: cs.CV

TL;DR: ODG combines BEV and sparse points representations for efficient 3D occupancy prediction, addressing limitations of each method.


<details>
  <summary>Details</summary>
Motivation: Existing methods (BEV or sparse points) have trade-offs in accuracy and efficiency, especially for small or large objects.

Method: Dual-branch design: query-based sparse points branch and BEV branch, with cross-attention for information sharing.

Result: Superior performance on Occ3D benchmarks and competitive inference speed.

Conclusion: ODG effectively balances accuracy and efficiency for 3D scene understanding in autonomous driving.

Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene
understanding which is critical for autonomous driving. Most existing methods,
however, carry high compute costs, requiring dense 3D feature volume and
cross-attention to effectively aggregate information. More recent works have
adopted Bird's Eye View (BEV) or sparse points as scene representation with
much reduced cost, but still suffer from their respective shortcomings. More
concretely, BEV struggles with small objects that often experience significant
information loss after being projected to the ground plane. On the other hand,
points can flexibly model little objects in 3D, but is inefficient at capturing
flat surfaces or large objects. To address these challenges, in this paper, we
present a novel 3D occupancy prediction approach, ODG, which combines BEV and
sparse points based representations. We propose a dual-branch design: a
query-based sparse points branch and a BEV branch. The 3D information learned
in the sparse points branch is shared with the BEV stream via cross-attention,
which enriches the weakened signals of difficult objects on the BEV plane. The
outputs of both branches are finally fused to generate predicted 3D occupancy.
We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo
benchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG
also delivers competitive inference speed when compared to the latest efficient
approaches.

</details>


### [94] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: InterSyn, a large-scale multimodal dataset, and SynJudge, an evaluation tool, are introduced to improve LMMs' ability to generate interleaved image-text outputs.


<details>
  <summary>Details</summary>
Motivation: Current LMMs struggle with tightly interleaved image-text outputs due to limited training datasets.

Method: InterSyn is created using the SEIR method for quality refinement, and SynJudge evaluates multimodal outputs.

Result: SEIR improves dataset quality, and LMMs trained on InterSyn show performance gains.

Conclusion: InterSyn and SynJudge advance multimodal systems by enhancing dataset quality and evaluation.

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [95] [A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning](https://arxiv.org/abs/2506.09429)
*Swadhin Das,Divyansh Mundra,Priyanshu Dayal,Raksha Sharma*

Main category: cs.CV

TL;DR: A lightweight transformer architecture is proposed for remote sensing image captioning, reducing computational costs and enhancing fine-grained feature capture through knowledge distillation and edge-aware enhancement.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models for remote sensing image captioning face high computational costs and neglect fine-grained structural features like edges and contours.

Method: The approach uses a lightweight transformer with reduced encoder dimensionality, a distilled GPT-2 decoder, knowledge distillation, and edge-aware enhancement.

Result: The proposed method significantly improves caption quality over state-of-the-art models.

Conclusion: The lightweight architecture effectively balances performance and efficiency while capturing fine-grained spatial details.

Abstract: Transformer-based models have achieved strong performance in remote sensing
image captioning by capturing long-range dependencies and contextual
information. However, their practical deployment is hindered by high
computational costs, especially in multi-modal frameworks that employ separate
transformer-based encoders and decoders. In addition, existing remote sensing
image captioning models primarily focus on high-level semantic extraction while
often overlooking fine-grained structural features such as edges, contours, and
object boundaries. To address these challenges, a lightweight transformer
architecture is proposed by reducing the dimensionality of the encoder layers
and employing a distilled version of GPT-2 as the decoder. A knowledge
distillation strategy is used to transfer knowledge from a more complex teacher
model to improve the performance of the lightweight network. Furthermore, an
edge-aware enhancement strategy is incorporated to enhance image representation
and object boundary understanding, enabling the model to capture fine-grained
spatial details in remote sensing images. Experimental results demonstrate that
the proposed approach significantly improves caption quality compared to
state-of-the-art methods.

</details>


### [96] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Main category: cs.CV

TL;DR: TOGA is a vision-language model for weakly supervised, temporally grounded video QA, achieving state-of-the-art performance without temporal annotations.


<details>
  <summary>Details</summary>
Motivation: Addressing video QA with temporal grounding in a weakly supervised setup, lacking temporal annotations.

Method: Proposes TOGA, instruct-tuned to jointly generate answers and temporal grounding using pseudo labels and consistency constraints.

Result: State-of-the-art performance on NExT-GQA, MSVD-QA, and ActivityNet-QA benchmarks.

Conclusion: Jointly generating answers and grounding improves performance, validating TOGA's effectiveness in weakly supervised setups.

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [97] [Harmonizing and Merging Source Models for CLIP-based Domain Generalization](https://arxiv.org/abs/2506.09446)
*Yuhe Ding,Jian Liang,Bo Jiang,Zi Wang,Aihua Zheng,Bin Luo*

Main category: cs.CV

TL;DR: HAM is a novel framework for CLIP-based domain generalization that harmonizes and merges source models to mitigate conflicts and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from sample and optimization conflicts in multi-source training, hindering generalization.

Method: HAM enriches source samples, harmonizes model updates, and merges models redundantly-aware.

Result: Achieves state-of-the-art performance on five benchmark datasets.

Conclusion: HAM effectively consolidates domain information and enhances model generalization.

Abstract: CLIP-based domain generalization aims to improve model generalization to
unseen domains by leveraging the powerful zero-shot classification capabilities
of CLIP and multiple source datasets. Existing methods typically train a single
model across multiple source domains to capture domain-shared information.
However, this paradigm inherently suffers from two types of conflicts: 1)
sample conflicts, arising from noisy samples and extreme domain shifts among
sources; and 2) optimization conflicts, stemming from competition and
trade-offs during multi-source training. Both hinder the generalization and
lead to suboptimal solutions. Recent studies have shown that model merging can
effectively mitigate the competition of multi-objective optimization and
improve generalization performance. Inspired by these findings, we propose
Harmonizing and Merging (HAM), a novel source model merging framework for
CLIP-based domain generalization. During the training process of the source
models, HAM enriches the source samples without conflicting samples, and
harmonizes the update directions of all models. Then, a redundancy-aware
historical model merging method is introduced to effectively integrate
knowledge across all source models. HAM comprehensively consolidates source
domain information while enabling mutual enhancement among source models,
ultimately yielding a final model with optimal generalization capabilities.
Extensive experiments on five widely used benchmark datasets demonstrate the
effectiveness of our approach, achieving state-of-the-art performance.

</details>


### [98] [Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization](https://arxiv.org/abs/2506.09460)
*Amirreza Khoshbakht,Erchan Aptoula*

Main category: cs.CV

TL;DR: Proposes a novel open-set domain generalization framework for hyperspectral image classification, addressing unknown classes and domain shift without target data access.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle unknown classes and domain shift in target domains, leading to negative transfer and poor performance.

Method: Combines Spectrum-Invariant Frequency Disentanglement (SIFD), Dual-Channel Residual Network (DCRN), Evidential Deep Learning (EDL), and Spectral-Spatial Uncertainty Disentanglement (SSUD) for domain-agnostic feature extraction, robust learning, uncertainty quantification, and reliable open-set classification.

Result: Achieves performance comparable to state-of-the-art domain adaptation methods without target domain data during training.

Conclusion: The framework effectively addresses open-set domain generalization challenges, with potential for broader applications.

Abstract: Open-set domain generalization(OSDG) for hyperspectral image classification
presents significant challenges due to the presence of unknown classes in
target domains and the need for models to generalize across multiple unseen
domains without target-specific adaptation. Existing domain adaptation methods
assume access to target domain data during training and fail to address the
fundamental issue of domain shift when unknown classes are present, leading to
negative transfer and reduced classification performance. To address these
limitations, we propose a novel open-set domain generalization framework that
combines four key components: Spectrum-Invariant Frequency Disentanglement
(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network
(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning
(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty
Disentanglement (SSUD) for reliable open-set classification. The SIFD module
extracts domain-invariant spectral features in the frequency domain through
attention-weighted frequency analysis and domain-agnostic regularization, while
DCRN captures complementary spectral and spatial information via parallel
pathways with adaptive fusion. EDL provides principled uncertainty estimation
using Dirichlet distributions, enabling the SSUD module to make reliable
open-set decisions through uncertainty-aware pathway weighting and adaptive
rejection thresholding. Experimental results on three cross-scene hyperspectral
classification tasks show that our approach achieves performance comparable to
state-of-the-art domain adaptation methods while requiring no access to the
target domain during training. The implementation will be made available at
https://github.com/amir-khb/SSUDOSDG upon acceptance.

</details>


### [99] [Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing](https://arxiv.org/abs/2506.09469)
*Maria Damanaki,Nikos Piperigkos,Alexandros Gkillas,Aris S. Lalos*

Main category: cs.CV

TL;DR: A novel Cooperative MOT framework for 3D LiDAR scenes improves tracking by fusing multi-agent data via graph topology-aware optimization, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Single-agent MOT struggles with occlusions and sensor failures, necessitating multi-agent integration for better environmental understanding.

Method: Proposes a graph topology-aware optimization using Graph Laplacian to refine bounding box positions and associate them with tracked objects in two stages.

Result: Outperforms DMSTrack and V2V4Real baselines in real-world evaluations using the V2V4Real dataset.

Conclusion: The framework enhances tracking accuracy by leveraging multi-agent coherence, proving effective in real-world autonomous driving scenarios.

Abstract: Multi-Object Tracking (MOT) plays a crucial role in autonomous driving
systems, as it lays the foundations for advanced perception and precise path
planning modules. Nonetheless, single agent based MOT lacks in sensing
surroundings due to occlusions, sensors failures, etc. Hence, the integration
of multiagent information is essential for comprehensive understanding of the
environment. This paper proposes a novel Cooperative MOT framework for tracking
objects in 3D LiDAR scene by formulating and solving a graph topology-aware
optimization problem so as to fuse information coming from multiple vehicles.
By exploiting a fully connected graph topology defined by the detected bounding
boxes, we employ the Graph Laplacian processing optimization technique to
smooth the position error of bounding boxes and effectively combine them. In
that manner, we reveal and leverage inherent coherences of diverse multi-agent
detections, and associate the refined bounding boxes to tracked objects at two
stages, optimizing localization and tracking accuracies. An extensive
evaluation study has been conducted, using the real-world V2V4Real dataset,
where the proposed method significantly outperforms the baseline frameworks,
including the state-of-the-art deep-learning DMSTrack and V2V4Real, in various
testing sequences.

</details>


### [100] [Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning](https://arxiv.org/abs/2506.09473)
*Cheng Chen,Yunpeng Zhai,Yifan Zhao,Jinyang Gao,Bolin Ding,Jia Li*

Main category: cs.CV

TL;DR: The paper introduces a reinforcement learning framework for adaptive multi-modal demonstration selection in Large Vision-Language Models (LVLMs), improving in-context learning performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for in-context learning rely on pre-defined or heuristic demonstration selection, which are inadequate for diverse tasks and ignore interactions between demonstrations.

Method: A reinforcement learning framework is proposed to explore and exploit policies for multi-modal demonstration selection, optimizing LVLMs through self-exploration.

Result: The approach outperforms existing methods on four Visual Question-Answering datasets, enhancing few-shot LVLM generalization.

Conclusion: The framework effectively improves in-context learning by autonomously selecting optimal demonstrations, advancing LVLM capabilities.

Abstract: In-context learning (ICL), a predominant trend in instruction learning, aims
at enhancing the performance of large language models by providing clear task
guidance and examples, improving their capability in task understanding and
execution. This paper investigates ICL on Large Vision-Language Models (LVLMs)
and explores the policies of multi-modal demonstration selection. Existing
research efforts in ICL face significant challenges: First, they rely on
pre-defined demonstrations or heuristic selecting strategies based on human
intuition, which are usually inadequate for covering diverse task requirements,
leading to sub-optimal solutions; Second, individually selecting each
demonstration fails in modeling the interactions between them, resulting in
information redundancy. Unlike these prevailing efforts, we propose a new
exploration-exploitation reinforcement learning framework, which explores
policies to fuse multi-modal information and adaptively select adequate
demonstrations as an integrated whole. The framework allows LVLMs to optimize
themselves by continually refining their demonstrations through
self-exploration, enabling the ability to autonomously identify and generate
the most effective selection policies for in-context learning. Experimental
results verify the superior performance of our approach on four Visual
Question-Answering (VQA) datasets, demonstrating its effectiveness in enhancing
the generalization capability of few-shot LVLMs.

</details>


### [101] [Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries](https://arxiv.org/abs/2506.09476)
*Tianxiang Hao,Lixian Zhang,Yingjia Zhang,Mengxuan Chen,Jinxiao Zhang,Haohuan Fu*

Main category: cs.CV

TL;DR: The paper introduces Urban1960SatBench, an annotated segmentation dataset for historical satellite imagery, and Urban1960SatUSM, an unsupervised segmentation framework, to address challenges in analyzing early urban development.


<details>
  <summary>Details</summary>
Motivation: Historical satellite imagery suffers from quality degradation and lack of annotations, hindering semantic segmentation. This work aims to bridge this gap for better urban development understanding.

Method: Urban1960SatUSM uses a confidence-aware alignment mechanism and focal-confidence loss in a self-supervised learning architecture to generate robust pseudo-labels for unsupervised segmentation.

Result: Urban1960SatUSM outperforms existing methods on Urban1960SatBench, demonstrating effectiveness for historical urban scene segmentation.

Conclusion: The work provides a pioneering benchmark and framework for unsupervised segmentation of historical imagery, enabling quantitative urban change studies.

Abstract: Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,
offers rare insights into understanding early urban development and long-term
transformation. However, severe quality degradation (e.g., distortion,
misalignment, and spectral scarcity) and annotation absence have long hindered
semantic segmentation on such historical RS imagery. To bridge this gap and
enhance understanding of urban development, we introduce
$\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on
historical satellite imagery with the earliest observation time among all
existing segmentation datasets, along with a benchmark framework for
unsupervised segmentation tasks, $\textbf{Urban1960SatUSM}$. First,
$\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic
segmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering
1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the
earliest segmentation dataset of its kind, it provides a pioneering benchmark
for historical urban understanding. Second,
$\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel
unsupervised semantic segmentation framework for historical RS imagery. It
employs a confidence-aware alignment mechanism and focal-confidence loss based
on a self-supervised learning architecture, which generates robust
pseudo-labels and adaptively prioritizes prediction difficulty and label
reliability to improve unsupervised segmentation on noisy historical data
without manual supervision. Experiments show Urban1960SatUSM significantly
outperforms existing unsupervised segmentation methods on Urban1960SatSeg for
segmenting historical urban scenes, promising in paving the way for
quantitative studies of long-term urban change using modern computer vision.
Our benchmark and supplementary material are available at
https://github.com/Tianxiang-Hao/Urban1960SatSeg.

</details>


### [102] [TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation](https://arxiv.org/abs/2506.09479)
*Zetian Song,Jiaye Fu,Jiaqi Zhang,Xiaohan Lu,Chuanmin Jia,Siwei Ma,Wen Gao*

Main category: cs.CV

TL;DR: TinySplat introduces a feedforward method to compress 3D Gaussian Splatting (3DGS) representations, reducing storage costs by 100x while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Feedforward 3DGS methods face high storage costs, and existing compression techniques are incompatible. TinySplat addresses this by eliminating redundancy without scene-wise optimization.

Method: TinySplat integrates View-Projection Transformation (VPT) for geometric redundancy, Visibility-Aware Basis Reduction (VABR) for perceptual redundancy, and a video codec for spatial redundancy.

Result: Achieves 100x compression, 6% storage size of state-of-the-art, with 25% encoding time and 1% decoding time.

Conclusion: TinySplat offers an efficient, high-quality compression solution for feedforward 3DGS, overcoming storage and compatibility limitations.

Abstract: The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a
new paradigm to reconstruct 3D scenes. Using neural networks trained on
large-scale multi-view datasets, it can directly infer 3DGS representations
from sparse input views. Although the feedforward approach achieves high
reconstruction speed, it still suffers from the substantial storage cost of 3D
Gaussians. Existing 3DGS compression methods relying on scene-wise optimization
are not applicable due to architectural incompatibilities. To overcome this
limitation, we propose TinySplat, a complete feedforward approach for
generating compact 3D scene representations. Built upon standard feedforward
3DGS methods, TinySplat integrates a training-free compression framework that
systematically eliminates key sources of redundancy. Specifically, we introduce
View-Projection Transformation (VPT) to reduce geometric redundancy by
projecting geometric parameters into a more compact space. We further present
Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy
by aligning feature energy along dominant viewing directions via basis
transformation. Lastly, spatial redundancy is addressed through an
off-the-shelf video codec. Comprehensive experimental results on multiple
benchmark datasets demonstrate that TinySplat achieves over 100x compression
for 3D Gaussian data generated by feedforward methods. Compared to the
state-of-the-art compression approach, we achieve comparable quality with only
6% of the storage size. Meanwhile, our compression framework requires only 25%
of the encoding time and 1% of the decoding time.

</details>


### [103] [Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression](https://arxiv.org/abs/2506.09482)
*Dingcheng Zhen,Qian Qiao,Tan Yu,Kangxi Wu,Ziwei Zhang,Siyuan Liu,Shunshun Yin,Ming Tao*

Main category: cs.CV

TL;DR: TransDiff combines AR Transformer and diffusion models for image generation, achieving superior performance and faster inference. MRAR further enhances quality by referencing multiple images.


<details>
  <summary>Details</summary>
Motivation: To improve image generation by integrating AR Transformer and diffusion models, addressing limitations of standalone methods.

Method: TransDiff encodes labels/images into semantic features and uses diffusion for sample distribution. MRAR references multiple images for autoregressive generation.

Result: Achieves FID 1.61, IS 293.4, and faster inference. MRAR reduces FID to 1.42.

Conclusion: TransDiff and MRAR advance image generation, offering a new paradigm with improved performance and efficiency.

Abstract: We introduce TransDiff, the first image generation model that marries
Autoregressive (AR) Transformer with diffusion models. In this joint modeling
framework, TransDiff encodes labels and images into high-level semantic
features and employs a diffusion model to estimate the distribution of image
samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms
other image generation models based on standalone AR Transformer or diffusion
models. Specifically, TransDiff achieves a Fr\'echet Inception Distance (FID)
of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster
inference latency compared to state-of-the-art methods based on AR Transformer
and x112 faster inference compared to diffusion-only models. Furthermore,
building on the TransDiff model, we introduce a novel image generation paradigm
called Multi-Reference Autoregression (MRAR), which performs autoregressive
generation by predicting the next image. MRAR enables the model to reference
multiple previously generated images, thereby facilitating the learning of more
diverse representations and improving the quality of generated images in
subsequent iterations. By applying MRAR, the performance of TransDiff is
improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open
up a new frontier in the field of image generation.

</details>


### [104] [Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals](https://arxiv.org/abs/2506.09510)
*Changhao Peng,Yuqi Ye,Wei Gao*

Main category: cs.CV

TL;DR: The paper introduces a generalized Gaussian entropy model and a Mean Error Discriminator (MED) to improve probability estimation and dynamic likelihood interval adjustment in point cloud attribute compression, enhancing rate-distortion performance.


<details>
  <summary>Details</summary>
Motivation: Current entropy models (Gaussian/Laplacian) underutilize information in entropy parameters, limiting accurate probability estimation and compression efficiency.

Method: Proposes a generalized Gaussian entropy model with a shape parameter for tail control and a MED to dynamically adjust likelihood intervals during arithmetic coding.

Result: Significantly improves rate-distortion performance on VAE-based models for point cloud attribute compression, with potential applicability to image/video compression.

Conclusion: The generalized Gaussian model and MED enhance compression efficiency by better utilizing entropy parameters and dynamically optimizing likelihood intervals.

Abstract: Gaussian and Laplacian entropy models are proved effective in learned point
cloud attribute compression, as they assist in arithmetic coding of latents.
However, we demonstrate through experiments that there is still unutilized
information in entropy parameters estimated by neural networks in current
methods, which can be used for more accurate probability estimation. Thus we
introduce generalized Gaussian entropy model, which controls the tail shape
through shape parameter to more accurately estimate the probability of latents.
Meanwhile, to the best of our knowledge, existing methods use fixed likelihood
intervals for each integer during arithmetic coding, which limits model
performance. We propose Mean Error Discriminator (MED) to determine whether the
entropy parameter estimation is accurate and then dynamically adjust likelihood
intervals. Experiments show that our method significantly improves
rate-distortion (RD) performance on three VAE-based models for point cloud
attribute compression, and our method can be applied to other compression
tasks, such as image and video compression.

</details>


### [105] [HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene](https://arxiv.org/abs/2506.09518)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Chengxuan Qian,Juyuan Kang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: HAIF-GS is a framework for dynamic 3D scene reconstruction from monocular videos, addressing limitations like redundant Gaussian updates and weak motion modeling through sparse anchor-driven deformation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with structured and temporally consistent motion representations in dynamic scenes, leading to inefficiencies and poor quality.

Method: HAIF-GS uses an Anchor Filter for motion-relevant regions, a self-supervised flow-guided deformation module, and hierarchical anchor propagation for fine-grained deformations.

Result: HAIF-GS outperforms prior methods in rendering quality, temporal coherence, and efficiency on synthetic and real-world benchmarks.

Conclusion: HAIF-GS provides a robust solution for dynamic 3D reconstruction, improving coherence and efficiency.

Abstract: Reconstructing dynamic 3D scenes from monocular videos remains a fundamental
challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time
rendering in static settings, extending it to dynamic scenes is challenging due
to the difficulty of learning structured and temporally consistent motion
representations. This challenge often manifests as three limitations in
existing methods: redundant Gaussian updates, insufficient motion supervision,
and weak modeling of complex non-rigid deformations. These issues collectively
hinder coherent and efficient dynamic reconstruction. To address these
limitations, we propose HAIF-GS, a unified framework that enables structured
and consistent dynamic modeling through sparse anchor-driven deformation. It
first identifies motion-relevant regions via an Anchor Filter to suppresses
redundant updates in static areas. A self-supervised Induced Flow-Guided
Deformation module induces anchor motion using multi-frame feature aggregation,
eliminating the need for explicit flow labels. To further handle fine-grained
deformations, a Hierarchical Anchor Propagation mechanism increases anchor
resolution based on motion complexity and propagates multi-level
transformations. Extensive experiments on synthetic and real-world benchmarks
validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in
rendering quality, temporal coherence, and reconstruction efficiency.

</details>


### [106] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: ReVisiT is a decoding method for LVLMs that improves visual grounding by referencing vision tokens during text generation, achieving better performance with less computational cost.


<details>
  <summary>Details</summary>
Motivation: Conventional LVLM decoding strategies often fail to utilize visual information effectively, leading to ungrounded responses. Existing solutions require extra training or resources.

Method: ReVisiT projects vision tokens into the text token space, dynamically selects relevant tokens, and refines the output distribution to incorporate visual semantics.

Result: ReVisiT enhances visual grounding on benchmarks, outperforming baselines while reducing computational costs by up to 2x.

Conclusion: ReVisiT offers a simple, effective solution for improving LVLM visual grounding without additional training or resources.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [107] [Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS](https://arxiv.org/abs/2506.09534)
*Tao Wang,Mengyu Li,Geduo Zeng,Cheng Meng,Qiong Zhang*

Main category: cs.CV

TL;DR: A novel optimal transport-based method for compacting 3D Gaussian Splatting (3DGS) reduces redundancy while maintaining rendering quality, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: 3DGS often uses millions of redundant Gaussian primitives, straining memory and rendering budgets. Existing compaction methods lack global fidelity guarantees.

Method: Proposes a global Gaussian mixture reduction via optimal transport, minimizing transport divergence over a KD-tree partition and decoupling appearance from geometry.

Result: Achieves negligible rendering quality loss (PSNR, SSIM, LPIPS) with only 10% Gaussians, outperforming state-of-the-art compaction techniques.

Conclusion: The method offers an efficient, agnostic solution for lightweight neural rendering, applicable to any 3DGS pipeline stage.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance
field rendering, but it typically requires millions of redundant Gaussian
primitives, overwhelming memory and rendering budgets. Existing compaction
approaches address this by pruning Gaussians based on heuristic importance
scores, without global fidelity guarantee. To bridge this gap, we propose a
novel optimal transport perspective that casts 3DGS compaction as global
Gaussian mixture reduction. Specifically, we first minimize the composite
transport divergence over a KD-tree partition to produce a compact geometric
representation, and then decouple appearance from geometry by fine-tuning color
and opacity attributes with far fewer Gaussian primitives. Experiments on
benchmark datasets show that our method (i) yields negligible loss in rendering
quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;
and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.
Notably, our method is applicable to any stage of vanilla or accelerated 3DGS
pipelines, providing an efficient and agnostic pathway to lightweight neural
rendering.

</details>


### [108] [AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches](https://arxiv.org/abs/2506.09538)
*Wenjun Ji,Yuxiang Fu,Luyang Ying,Deng-Ping Fan,Yuyi Wang,Ming-Ming Cheng,Ivor Tsang,Qing Guo*

Main category: cs.CV

TL;DR: The paper addresses the angle robustness of text-to-image (T2I) adversarial patches, introduces Angle-Robust Concept Learning (AngleRoCL), and demonstrates its effectiveness in enhancing attack robustness across multiple views.


<details>
  <summary>Details</summary>
Motivation: Existing T2I adversarial patches lack angle robustness, and the impact of text on patch robustness is understudied. The paper aims to address these gaps.

Method: Proposes AngleRoCL, which learns generalizable text embeddings to guide T2I models in generating angle-robust adversarial patches.

Result: AngleRoCL significantly improves angle robustness, with over 50% average relative improvement in attack effectiveness across multiple angles.

Conclusion: The research advances understanding of angle-robust patches and highlights the link between textual concepts and physical properties in T2I-generated content.

Abstract: Cutting-edge works have demonstrated that text-to-image (T2I) diffusion
models can generate adversarial patches that mislead state-of-the-art object
detectors in the physical world, revealing detectors' vulnerabilities and
risks. However, these methods neglect the T2I patches' attack effectiveness
when observed from different views in the physical world (i.e., angle
robustness of the T2I adversarial patches). In this paper, we study the angle
robustness of T2I adversarial patches comprehensively, revealing their
angle-robust issues, demonstrating that texts affect the angle robustness of
generated patches significantly, and task-specific linguistic instructions fail
to enhance the angle robustness. Motivated by the studies, we introduce
Angle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that
learns a generalizable concept (i.e., text embeddings in implementation)
representing the capability of generating angle-robust patches. The learned
concept can be incorporated into textual prompts and guides T2I models to
generate patches with their attack effectiveness inherently resistant to
viewpoint variations. Through extensive simulation and physical-world
experiments on five SOTA detectors across multiple views, we demonstrate that
AngleRoCL significantly enhances the angle robustness of T2I adversarial
patches compared to baseline methods. Our patches maintain high attack success
rates even under challenging viewing conditions, with over 50% average relative
improvement in attack effectiveness across multiple angles. This research
advances the understanding of physically angle-robust patches and provides
insights into the relationship between textual concepts and physical properties
in T2I-generated contents.

</details>


### [109] [3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection](https://arxiv.org/abs/2506.09541)
*Yi Zhang,Yi Wang,Yawen Cui,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 3DGeoDet is a geometry-aware 3D object detection method using RGB images, improving performance by leveraging explicit and implicit 3D representations without 3D supervision.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of 3D geometric cues in image-based 3D object detection, which causes ambiguity in image-to-3D correspondences.

Method: Uses predicted depth to create explicit voxel occupancy and implicit TSDF representations, enhancing 3D awareness without 3D supervision.

Result: Outperforms state-of-the-art methods on SUN RGB-D, ScanNetV2, and KITTI datasets with significant mAP improvements.

Conclusion: 3DGeoDet demonstrates superior performance by effectively integrating 3D geometric cues into image-based detection.

Abstract: This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection
approach that effectively handles single- and multi-view RGB images in indoor
and outdoor environments, showcasing its general-purpose applicability. The key
challenge for image-based 3D object detection tasks is the lack of 3D geometric
cues, which leads to ambiguity in establishing correspondences between images
and 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D
geometric representations in both explicit and implicit manners based on
predicted depth information. Specifically, we utilize the predicted depth to
learn voxel occupancy and optimize the voxelized 3D feature volume explicitly
through the proposed voxel occupancy attention. To further enhance 3D
awareness, the feature volume is integrated with an implicit 3D representation,
the truncated signed distance function (TSDF). Without requiring supervision
from 3D signals, we significantly improve the model's comprehension of 3D
geometry by leveraging intermediate 3D representations and achieve end-to-end
training. Our approach surpasses the performance of state-of-the-art
image-based methods on both single- and multi-view benchmark datasets across
diverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D
dataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19
AP3D@0.7 improvement on the KITTI dataset. The project page is available at:
https://cindy0725.github.io/3DGeoDet/.

</details>


### [110] [GLD-Road:A global-local decoding road network extraction model for remote sensing images](https://arxiv.org/abs/2506.09553)
*Ligao Deng,Yupeng Deng,Yu Meng,Jingbo Chen,Zhihao Xi,Diyou Liu,Qifeng Chu*

Main category: cs.CV

TL;DR: GLD-Road is a two-stage model for efficient and precise road network extraction, combining global efficiency and local refinement, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of road networks is costly, and existing deep learning methods either sacrifice accuracy for speed or vice versa. GLD-Road aims to balance both.

Method: GLD-Road uses a two-stage approach: global detection of road nodes and connections, followed by local iterative refinement to fix broken roads, reducing computation.

Result: GLD-Road improves APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3), while reducing retrieval time by 40% vs. Sat2Graph and 92% vs. RNGDet++.

Conclusion: GLD-Road effectively combines global and local methods for road network extraction, achieving superior performance in accuracy and efficiency.

Abstract: Road networks are crucial for mapping, autonomous driving, and disaster
response. While manual annotation is costly, deep learning offers efficient
extraction. Current methods include postprocessing (prone to errors), global
parallel (fast but misses nodes), and local iterative (accurate but slow). We
propose GLD-Road, a two-stage model combining global efficiency and local
precision. First, it detects road nodes and connects them via a Connect Module.
Then, it iteratively refines broken roads using local searches, drastically
reducing computation. Experiments show GLD-Road outperforms state-of-the-art
methods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also
reduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++
(local). The experimental results are available at
https://github.com/ucas-dlg/GLD-Road.

</details>


### [111] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/abs/2506.09557)
*Zhaoyang Wei,Chenhui Qiang,Bowen Jiang,Xumeng Han,Xuehui Yu,Zhenjun Han*

Main category: cs.CV

TL;DR: AD^2-Bench is the first Chain-of-Thought benchmark for autonomous driving in adverse weather and complex scenes, featuring 5.4k annotated instances to rigorously evaluate MLLMs' reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack rigorous evaluation of CoT processes in challenging autonomous driving scenarios.

Method: AD^2-Bench is designed with comprehensive data coverage, fine-grained annotations, and a dedicated evaluation framework.

Result: State-of-the-art MLLMs achieve below 60% accuracy, demonstrating the benchmark's difficulty.

Conclusion: AD^2-Bench advances research by improving MLLMs' reasoning in autonomous driving.

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [112] [SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields](https://arxiv.org/abs/2506.09565)
*Qijing Li,Jingxiang Sun,Liang An,Zhaoqi Su,Hongwen Zhang,Yebin Liu*

Main category: cs.CV

TL;DR: SemanticSplat is a feed-forward method for holistic 3D scene understanding, combining 3D Gaussians with semantic attributes to improve geometry, appearance, and semantics modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack holistic scene comprehension, suffer from poor geometry reconstruction, or require dense input views, limiting practicality.

Method: SemanticSplat uses 3D Gaussians with latent semantic attributes, fuses diverse feature fields, and employs a two-stage distillation framework for multi-modal semantic feature reconstruction.

Result: The method achieves coherent and accurate scene comprehension, excelling in tasks like promptable and open-vocabulary segmentation.

Conclusion: SemanticSplat advances 3D scene understanding by unifying geometry, appearance, and semantics in a practical, feed-forward framework.

Abstract: Holistic 3D scene understanding, which jointly models geometry, appearance,
and semantics, is crucial for applications like augmented reality and robotic
interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM)
are limited to extracting language-based semantics from scenes, failing to
achieve holistic scene comprehension. Additionally, they suffer from
low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene
optimization methods rely on dense input views, which reduces practicality and
increases complexity during deployment. In this paper, we propose
SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which
unifies 3D Gaussians with latent semantic attributes for joint
geometry-appearance-semantics modeling. To predict the semantic anisotropic
Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a
cost volume representation that stores cross-view feature similarities,
enhancing coherent and accurate scene comprehension. Leveraging a two-stage
distillation framework, SemanticSplat reconstructs a holistic multi-modal
semantic feature field from sparse-view images. Experiments demonstrate the
effectiveness of our method for 3D scene understanding tasks like promptable
and open-vocabulary segmentation. Video results are available at
https://semanticsplat.github.io.

</details>


### [113] [Consistent Story Generation with Asymmetry Zigzag Sampling](https://arxiv.org/abs/2506.09612)
*Mingxiao LI,mang ning,Marie-Francine Moens*

Main category: cs.CV

TL;DR: A novel training-free sampling strategy, Zigzag Sampling with Asymmetric Prompts and Visual Sharing, improves subject consistency in text-to-image generation for visual storytelling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for maintaining subject consistency in visual storytelling are either resource-intensive or yield limited success.

Method: The approach uses zigzag sampling with asymmetric prompts and a visual sharing module to enhance consistency.

Result: The method outperforms previous approaches in generating coherent and consistent visual stories, as shown by quantitative and qualitative evaluations.

Conclusion: The proposed training-free strategy effectively enhances subject consistency in visual story generation.

Abstract: Text-to-image generation models have made significant progress in producing
high-quality images from textual descriptions, yet they continue to struggle
with maintaining subject consistency across multiple images, a fundamental
requirement for visual storytelling. Existing methods attempt to address this
by either fine-tuning models on large-scale story visualization datasets, which
is resource-intensive, or by using training-free techniques that share
information across generations, which still yield limited success. In this
paper, we introduce a novel training-free sampling strategy called Zigzag
Sampling with Asymmetric Prompts and Visual Sharing to enhance subject
consistency in visual story generation. Our approach proposes a zigzag sampling
mechanism that alternates between asymmetric prompting to retain subject
characteristics, while a visual sharing module transfers visual cues across
generated images to %further enforce consistency. Experimental results, based
on both quantitative metrics and qualitative evaluations, demonstrate that our
method significantly outperforms previous approaches in generating coherent and
consistent visual stories. The code is available at
https://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.

</details>


### [114] [ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting](https://arxiv.org/abs/2506.09626)
*Giacomo Rosin,Muhammad Rameez Ur Rahman,Sebastiano Vascon*

Main category: cs.CV

TL;DR: The paper introduces ECAM, a contrastive learning-based module to improve collision avoidance in human trajectory forecasting by integrating environmental context.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory forecasting methods often neglect environmental impact, leading to collisions. ECAM addresses this gap.

Method: ECAM uses contrastive learning to enhance collision avoidance with the environment and integrates into existing models.

Result: Experiments on ETH/UCY dataset show a 40-50% reduction in collision rates when ECAM is added to state-of-the-art methods.

Conclusion: ECAM effectively improves collision avoidance in trajectory forecasting, with significant quantitative and qualitative benefits.

Abstract: Human trajectory forecasting is crucial in applications such as autonomous
driving, robotics and surveillance. Accurate forecasting requires models to
consider various factors, including social interactions, multi-modal
predictions, pedestrian intention and environmental context. While existing
methods account for these factors, they often overlook the impact of the
environment, which leads to collisions with obstacles. This paper introduces
ECAM (Environmental Collision Avoidance Module), a contrastive learning-based
module to enhance collision avoidance ability with the environment. The
proposed module can be integrated into existing trajectory forecasting models,
improving their ability to generate collision-free predictions. We evaluate our
method on the ETH/UCY dataset and quantitatively and qualitatively demonstrate
its collision avoidance capabilities. Our experiments show that
state-of-the-art methods significantly reduce (-40/50%) the collision rate when
integrated with the proposed module. The code is available at
https://github.com/CVML-CFU/ECAM.

</details>


### [115] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/abs/2506.09634)
*Yanzhao Shi,Xiaodan Zhang,Junzhong Ji,Haoning Jiang,Chengxin Zheng,Yinong Wang,Liangqiong Qu*

Main category: cs.CV

TL;DR: HSENet improves 3D medical image analysis by combining dual-3D vision encoders and Spatial Packer for better vision-language understanding, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs focus on 2D medical images, limiting their ability to capture 3D anatomical structures, leading to diagnostic errors.

Method: HSENet uses dual-3D vision encoders for global and fine-grained details, pre-trained with diagnostic reports, and Spatial Packer for efficient 3D spatial compression.

Result: Achieves state-of-the-art in 3D language-visual retrieval, report generation, and visual question answering.

Conclusion: HSENet effectively bridges 3D medical imaging and language understanding, enhancing diagnostic accuracy and workflow efficiency.

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [116] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Main category: cs.CV

TL;DR: DGAE improves autoencoder performance under high compression by using a diffusion model to guide the decoder, achieving compact latent representations and faster diffusion model convergence.


<details>
  <summary>Details</summary>
Motivation: Address training instability in autoencoders under high compression ratios and minimize latent space dimensionality for efficient representations.

Method: Propose DGAE, which employs a diffusion model to guide the decoder in recovering undecoded signals, enhancing expressiveness.

Result: DGAE mitigates performance degradation under high compression, achieves state-of-the-art performance with a 2x smaller latent space, and improves diffusion model convergence.

Conclusion: DGAE effectively addresses autoencoder challenges, offering compact and efficient latent representations while enhancing generative model performance.

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [117] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces a new task of textual reference-guided human action segmentation in multi-person settings, proposing a dataset (RHAS133) and a framework (HopaDIFF) that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing action segmentation methods focus on single-person activities, neglecting multi-person scenarios. This work addresses this gap by introducing textual descriptions to specify target persons.

Method: The authors propose HopaDIFF, a framework combining holistic-partial reasoning with Fourier-conditioned diffusion and cross-input gate attentional xLSTM for fine-grained control.

Result: HopaDIFF achieves state-of-the-art performance on the new RHAS133 dataset, outperforming existing methods.

Conclusion: The work advances multi-person action segmentation by introducing a novel dataset and framework, demonstrating superior performance and potential for future research.

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [118] [Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation](https://arxiv.org/abs/2506.09663)
*Haowen Wang,Xiaoping Yuan,Zhao Jin,Zhen Zhao,Zhengping Che,Yousong Xue,Jin Tian,Yakun Huang,Jian Tang*

Main category: cs.CV

TL;DR: DeGSS introduces a unified framework for 3D representation of articulated objects using deformable Gaussian fields, enabling unsupervised part segmentation and precise kinematic modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to create unified representations for articulated objects without human annotation.

Method: DeGSS encodes objects as deformable 3D Gaussian fields, modeling interaction states as smooth deformations and guiding part segmentation.

Result: The method outperforms existing approaches in accuracy and stability, validated on synthetic and real datasets.

Conclusion: DeGSS provides a compact, continuous, and decoupled representation for articulated objects, advancing unsupervised 3D modeling.

Abstract: Articulated objects are ubiquitous in everyday life, and accurate 3D
representations of their geometry and motion are critical for numerous
applications. However, in the absence of human annotation, existing approaches
still struggle to build a unified representation for objects that contain
multiple movable parts. We introduce DeGSS, a unified framework that encodes
articulated objects as deformable 3D Gaussian fields, embedding geometry,
appearance, and motion in one compact representation. Each interaction state is
modeled as a smooth deformation of a shared field, and the resulting
deformation trajectories guide a progressive coarse-to-fine part segmentation
that identifies distinct rigid components, all in an unsupervised manner. The
refined field provides a spatially continuous, fully decoupled description of
every part, supporting part-level reconstruction and precise modeling of their
kinematic relationships. To evaluate generalization and realism, we enlarge the
synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset
that pairs RGB captures with accurately reverse-engineered 3D models. Extensive
experiments demonstrate that our method outperforms existing methods in both
accuracy and stability.

</details>


### [119] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Main category: cs.CV

TL;DR: CINeMA is a novel framework for creating high-resolution, multimodal brain atlases in low-data settings, outperforming traditional methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Studying rapid neurodevelopment in fetal and neonatal brains requires high-resolution atlases, but existing methods rely on large datasets, which are scarce for pathological cases.

Method: CINeMA operates in latent space, avoiding compute-intensive registration and enabling flexible conditioning on anatomical features like age and pathologies.

Result: CINeMA reduces atlas construction time from days to minutes and supports tasks like segmentation, age prediction, and synthetic data creation.

Conclusion: CINeMA is a versatile and efficient tool for brain research, with code and atlases publicly available.

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [120] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/abs/2506.09677)
*Bin Zhu,Hailong Yin,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper evaluates reasoning models' robustness against misleading inputs, revealing significant accuracy drops under gaslighting negation prompts. It introduces GaslightingBench-R, a new benchmark, showing even worse performance (53% drop).


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored vulnerability of reasoning models to manipulative user feedback.

Method: Systematic evaluation of three state-of-the-art reasoning models (OpenAI's o4-mini, Claude-3.7-Sonnet, Gemini-2.5-Flash) across multimodal benchmarks (MMMU, MathVista, CharXiv). Introduces GaslightingBench-R for deeper analysis.

Result: Significant accuracy drops (25-29% on average) under gaslighting prompts; GaslightingBench-R induces even larger drops (53%).

Conclusion: Reasoning models have fundamental robustness limitations, highlighting a gap between step-by-step reasoning and belief persistence.

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [121] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: The paper proposes an inference-time technique to improve compositionality in dual encoder Vision-Language Models (VLMs) by structuring image-text matching without additional training.


<details>
  <summary>Details</summary>
Motivation: Dual encoder VLMs like CLIP struggle with compositionality, limiting retrieval performance. While training approaches exist, inference-time techniques are understudied.

Method: The method involves dividing images into crops, extracting text segments (objects, attributes, relations), aligning crops with segments using a VLM, and aggregating similarities for final scoring.

Result: The approach consistently improves VLM performance on compositionality tasks without training, especially in attribute-object binding. Image crop processing is key to gains.

Conclusion: Inference-time techniques show promise for enhancing VLMs, with identified areas for further improvement.

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [122] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/abs/2506.09695)
*Changwei Wu,Yifei Chen,Yuxin Du,Jinying Zong,Jie Dong,Mingxuan Liu,Yong Peng,Jin Fan,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: FasterSNN, a hybrid spiking neural network, improves Alzheimer's Disease diagnosis by combining LIF neurons, region-adaptive convolution, and multi-scale spiking attention for efficient and stable 3D MRI processing.


<details>
  <summary>Details</summary>
Motivation: Early AD diagnosis is challenging due to subjective assessments and costly imaging. Deep learning methods are inefficient, while SNNs, though promising, suffer from weak expressiveness and unstable training.

Method: Proposes FasterSNN, integrating LIF neurons, region-adaptive convolution, and multi-scale spiking attention for sparse, efficient 3D MRI processing.

Result: FasterSNN achieves competitive performance with improved efficiency and stability on benchmark datasets.

Conclusion: FasterSNN offers a practical, low-power solution for AD screening, with potential for real-world deployment.

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [123] [CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings](https://arxiv.org/abs/2506.09699)
*Mattia Nardon,Mikel Mujika Agirre,Ander González Tomé,Daniel Sedano Algarabel,Josep Rueda Collell,Ana Paola Caro,Andrea Caraffa,Fabio Poiesi,Paul Ian Chippendale,Davide Boscaini*

Main category: cs.CV

TL;DR: CHIP is a new dataset for 6D pose estimation of chairs in industrial settings, addressing gaps in existing benchmarks. It includes 77,811 RGBD images with ground-truth poses and evaluates three zero-shot methods, revealing challenges and room for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for 6D pose estimation lack realism for industrial applications, focusing on domestic settings or artificial setups. CHIP aims to fill this gap by providing a dataset for chairs in real-world industrial environments.

Method: CHIP introduces a dataset with seven chairs captured using three RGBD sensors, featuring challenges like distractor objects and occlusions. Ground-truth poses are derived from robot kinematics. Three zero-shot 6D pose estimation methods are benchmarked.

Result: Benchmarking shows significant room for improvement, highlighting challenges such as sensor variability, occlusion, and fine-grained distractors.

Conclusion: CHIP addresses a critical gap in 6D pose estimation benchmarks for industrial settings and will be publicly released to foster further research.

Abstract: Accurate 6D pose estimation of complex objects in 3D environments is
essential for effective robotic manipulation. Yet, existing benchmarks fall
short in evaluating 6D pose estimation methods under realistic industrial
conditions, as most datasets focus on household objects in domestic settings,
while the few available industrial datasets are limited to artificial setups
with objects placed on tables. To bridge this gap, we introduce CHIP, the first
dataset designed for 6D pose estimation of chairs manipulated by a robotic arm
in a real-world industrial environment. CHIP includes seven distinct chairs
captured using three different RGBD sensing technologies and presents unique
challenges, such as distractor objects with fine-grained differences and severe
occlusions caused by the robotic arm and human operators. CHIP comprises 77,811
RGBD images annotated with ground-truth 6D poses automatically derived from the
robot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP
using three zero-shot 6D pose estimation methods, assessing performance across
different sensor types, localization priors, and occlusion levels. Results show
substantial room for improvement, highlighting the unique challenges posed by
the dataset. CHIP will be publicly released.

</details>


### [124] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/abs/2506.09718)
*Xulin Ma,Jiankai Tang,Zhang Jiang,Songqin Cheng,Yuanchun Shi,Dong LI,Xin Liu,Daniel McDuff,Xiaojing Liu,Yuntao Wang*

Main category: cs.CV

TL;DR: The paper introduces LADH, the first long-term rPPG dataset for health monitoring in challenging environments, combining RGB and IR videos to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Address challenges in long-term rPPG applications like lighting variations and occlusions in high-altitude personal care scenarios.

Method: Uses synchronized RGB and IR facial videos from 21 participants across five scenarios, with ground-truth physiological signals.

Result: Combining RGB and IR inputs reduces heart rate estimation error to 4.99 BPM; multi-task learning improves performance.

Conclusion: LADH dataset and fusion of RGB-IR inputs enhance non-contact physiological monitoring robustness and accuracy.

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [125] [The Four Color Theorem for Cell Instance Segmentation](https://arxiv.org/abs/2506.09724)
*Ye Zhang,Yu Zhou,Yifeng Wang,Jun Xiao,Ziyue Wang,Yongbing Zhang,Jianxu Chen*

Main category: cs.CV

TL;DR: A novel cell instance segmentation method inspired by the four-color theorem simplifies instance differentiation by transforming it into a constrained semantic segmentation problem with four predicted classes.


<details>
  <summary>Details</summary>
Motivation: Accurately distinguishing tightly touching cells in biomedical images remains challenging, with existing methods struggling to balance performance and computational efficiency.

Method: The method uses a four-color encoding scheme (inspired by the four-color theorem) to label adjacent cells distinctly, reformulating instance segmentation as a constrained semantic segmentation problem. An asymptotic training strategy and encoding transformation method address training instability.

Result: Extensive experiments show state-of-the-art performance across various modes.

Conclusion: The proposed method effectively simplifies and improves cell instance segmentation, achieving high performance while addressing computational efficiency.

Abstract: Cell instance segmentation is critical to analyzing biomedical images, yet
accurately distinguishing tightly touching cells remains a persistent
challenge. Existing instance segmentation frameworks, including
detection-based, contour-based, and distance mapping-based approaches, have
made significant progress, but balancing model performance with computational
efficiency remains an open problem. In this paper, we propose a novel cell
instance segmentation method inspired by the four-color theorem. By
conceptualizing cells as countries and tissues as oceans, we introduce a
four-color encoding scheme that ensures adjacent instances receive distinct
labels. This reformulation transforms instance segmentation into a constrained
semantic segmentation problem with only four predicted classes, substantially
simplifying the instance differentiation process. To solve the training
instability caused by the non-uniqueness of four-color encoding, we design an
asymptotic training strategy and encoding transformation method. Extensive
experiments on various modes demonstrate our approach achieves state-of-the-art
performance. The code is available at https://github.com/zhangye-zoe/FCIS.

</details>


### [126] [MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition](https://arxiv.org/abs/2506.09735)
*Chuang Ma,Shaokai Zhao,Dongdong Zhou,Yu Pei,Zhiguo Luo,Liang Xie,Ye Yan,Erwei Yin*

Main category: cs.CV

TL;DR: The paper introduces MPFNet, a Multi-Prior Fusion Network for micro-expression recognition, leveraging progressive training and dual encoders to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MER methods rely on simplistic prior knowledge, limiting performance. The paper aims to exploit multi-source information for better accuracy.

Method: Proposes MPFNet with Generic and Advanced Feature Encoders (GFE, AFE) based on I3D and CA mechanisms. Two variants (MPFNet-P, MPFNet-C) mimic infant cognitive development.

Result: Achieves state-of-the-art accuracies: 0.811 (SMIC), 0.924 (CASME II), 0.857 (SAMM).

Conclusion: MPFNet significantly improves MER accuracy by effectively integrating multi-source prior knowledge.

Abstract: Micro-expression recognition (MER), a critical subfield of affective
computing, presents greater challenges than macro-expression recognition due to
its brief duration and low intensity. While incorporating prior knowledge has
been shown to enhance MER performance, existing methods predominantly rely on
simplistic, singular sources of prior knowledge, failing to fully exploit
multi-source information. This paper introduces the Multi-Prior Fusion Network
(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We
propose two complementary encoders: the Generic Feature Encoder (GFE) and the
Advanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with
Coordinate Attention (CA) mechanisms, to improve the model's ability to capture
spatiotemporal and channel-specific features. Inspired by developmental
psychology, we present two variants of MPFNet--MPFNet-P and
MPFNet-C--corresponding to two fundamental modes of infant cognitive
development: parallel and hierarchical processing. These variants enable the
evaluation of different strategies for integrating prior knowledge. Extensive
experiments demonstrate that MPFNet significantly improves MER accuracy while
maintaining balanced performance across categories, achieving accuracies of
0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.
To the best of our knowledge, our approach achieves state-of-the-art
performance on the SMIC and SAMM datasets.

</details>


### [127] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/abs/2506.09736)
*Yuting Li,Lai Wei,Kaipeng Zheng,Jingyuan Huang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CV

TL;DR: Language-only models with image captions can outperform MLLMs, suggesting poor visual integration. A visual perturbation framework improves robustness without extra training, boosting reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs generate accurate visual descriptions but struggle with integration during reasoning, prompting a need for better visual processing.

Method: Proposes a visual perturbation framework with three strategies: distractor concatenation, dominance-preserving mixup, and random rotation, integrated into post-training pipelines.

Result: Consistent improvements in mathematical reasoning across datasets, with competitive performance in open-source models. Each perturbation type uniquely aids visual reasoning.

Conclusion: Visual perturbation is crucial for multimodal reasoning, enhancing performance without algorithmic changes. 'Better reasoning begins with better seeing.'

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [128] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Main category: cs.CV

TL;DR: The paper evaluates pixel-text alignment in diffusion models using zero-shot referring image segmentation, identifies misalignment causes, and proposes ELBO-T2IAlign for calibration.


<details>
  <summary>Details</summary>
Motivation: Current methods assume perfect text-image alignment in diffusion models, which is unrealistic. The paper aims to assess and improve this alignment.

Method: Uses zero-shot referring image segmentation to evaluate alignment, analyzes misalignment causes, and introduces ELBO-T2IAlign for calibration.

Result: Misalignment occurs with small, occluded, or rare objects. ELBO-T2IAlign effectively calibrates alignment without training.

Conclusion: ELBO-T2IAlign is a generic, training-free solution for improving pixel-text alignment in diffusion models, validated by experiments.

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [129] [Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets](https://arxiv.org/abs/2506.09745)
*Yangrui Zhu,Junhua Bao,Yipan Wei,Yapeng Li,Bo Du*

Main category: cs.CV

TL;DR: The paper addresses the challenge of multimodal learning with inconsistent category sets, proposing a new setting (MMHCL) and a model (CSCF) to align and fuse cross-modal information effectively.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal data often has inconsistent category distributions, limiting model performance. The paper aims to solve this by enabling learning and recognition across heterogeneous category sets.

Method: Proposes CSCF, which aligns modality-specific features to a shared semantic space, selects discriminative modalities via uncertainty estimation, and fuses information based on class similarity.

Result: CSCF outperforms SOTA methods on benchmark datasets, effectively handling the MMHCL task.

Conclusion: The CSCF model successfully addresses the MMHCL challenge, improving cross-modal recognition in heterogeneous category settings.

Abstract: Existing multimodal methods typically assume that different modalities share
the same category set. However, in real-world applications, the category
distributions in multimodal data exhibit inconsistencies, which can hinder the
model's ability to effectively utilize cross-modal information for recognizing
all categories. In this work, we propose the practical setting termed
Multi-Modal Heterogeneous Category-set Learning (MMHCL), where models are
trained in heterogeneous category sets of multi-modal data and aim to recognize
complete classes set of all modalities during test. To effectively address this
task, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).
Specifically, CSCF aligns modality-specific features to a shared semantic space
to enable knowledge transfer between seen and unseen classes. It then selects
the most discriminative modality for decision fusion through uncertainty
estimation. Finally, it integrates cross-modal information based on class
similarity, where the auxiliary modality refines the prediction of the dominant
one. Experimental results show that our method significantly outperforms
existing state-of-the-art (SOTA) approaches on multiple benchmark datasets,
effectively addressing the MMHCL task.

</details>


### [130] [Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints](https://arxiv.org/abs/2506.09748)
*Xiangkai Zhang,Xiang Zhou,Mao Chen,Yuchen Lu,Xu Yang,Zhiyong Liu*

Main category: cs.CV

TL;DR: A hierarchical cross-source image matching method for UAV absolute localization, combining semantic-aware coarse matching and fine-grained matching, outperforms existing methods in GNSS-denied scenarios.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in vision-based absolute localization for UAVs due to cross-source discrepancies and temporal variations when GNSS signals are unavailable.

Method: Integrates a semantic-aware coarse matching module with a lightweight fine-grained matching module, leveraging vision foundation models and pixel-level correspondences.

Result: Demonstrates superior accuracy and robustness on benchmark datasets, including the new CS-UAV dataset.

Conclusion: The proposed method effectively addresses limitations of traditional image matching, offering a reliable solution for UAV absolute localization without GNSS.

Abstract: Absolute localization, aiming to determine an agent's location with respect
to a global reference, is crucial for unmanned aerial vehicles (UAVs) in
various applications, but it becomes challenging when global navigation
satellite system (GNSS) signals are unavailable. Vision-based absolute
localization methods, which locate the current view of the UAV in a reference
satellite map to estimate its position, have become popular in GNSS-denied
scenarios. However, existing methods mostly rely on traditional and low-level
image matching, suffering from difficulties due to significant differences
introduced by cross-source discrepancies and temporal variations. To overcome
these limitations, in this paper, we introduce a hierarchical cross-source
image matching method designed for UAV absolute localization, which integrates
a semantic-aware and structure-constrained coarse matching module with a
lightweight fine-grained matching module. Specifically, in the coarse matching
module, semantic features derived from a vision foundation model first
establish region-level correspondences under semantic and structural
constraints. Then, the fine-grained matching module is applied to extract fine
features and establish pixel-level correspondences. Building upon this, a UAV
absolute visual localization pipeline is constructed without any reliance on
relative localization techniques, mainly by employing an image retrieval module
before the proposed hierarchical image matching modules. Experimental
evaluations on public benchmark datasets and a newly introduced CS-UAV dataset
demonstrate superior accuracy and robustness of the proposed method under
various challenging conditions, confirming its effectiveness.

</details>


### [131] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/abs/2506.09777)
*Anton Razzhigaev,Matvey Mikhalchuk,Klim Kireev,Igor Udovichenko,Andrey Kuznetsov,Aleksandr Petiushko*

Main category: cs.CV

TL;DR: DarkerBB reconstructs facial images from black-box models using only similarity scores, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy threats by tackling the challenging scenario of model inversion with limited information (similarity scores only).

Method: Zero-order optimization within a PCA-derived eigenface space.

Result: Achieves state-of-the-art verification accuracies on LFW, AgeDB-30, and CFP-FP benchmarks with competitive query efficiency.

Conclusion: DarkerBB effectively reconstructs faces using minimal information, highlighting privacy risks in black-box models.

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [132] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/abs/2506.09782)
*Nicola Farronato,Florian Scheidegger,Mattia Rigotti,Cristiano Malossi,Michele Magno,Haotong Qin*

Main category: cs.CV

TL;DR: Q-SAM2 introduces a low-bit quantization method for SAM2, improving efficiency while maintaining accuracy through calibration and QAT.


<details>
  <summary>Details</summary>
Motivation: Address the high computational and memory costs of SAM2 for resource-constrained applications.

Method: Uses linear layer calibration for low-bit initialization and a QAT pipeline with clipping to adapt to quantization thresholds.

Result: Q-SAM2 achieves highly accurate inference, outperforming state-of-the-art methods, especially in ultra-low 2-bit quantization.

Conclusion: Q-SAM2 is effective for both quantization-aware training and post-training quantization, significantly improving accuracy and efficiency.

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [133] [Accurate and efficient zero-shot 6D pose estimation with frozen foundation models](https://arxiv.org/abs/2506.09784)
*Andrea Caraffa,Davide Boscaini,Fabio Poiesi*

Main category: cs.CV

TL;DR: FreeZeV2 is a training-free method for 6D pose estimation of unseen objects, improving accuracy and efficiency over its predecessor through sparse feature extraction, feature-aware scoring, and modular design.


<details>
  <summary>Details</summary>
Motivation: Generalizing 6D pose estimation to novel objects without task-specific training, addressing computational inefficiencies of existing methods.

Method: Leverages pre-trained geometric and vision foundation models, introduces sparse feature extraction, feature-aware scoring, and modular design for segmentation model ensembles.

Result: Achieves state-of-the-art on BOP Benchmark, 8x speedup and 5% accuracy improvement over FreeZe, with additional 8% accuracy gain using segmentation ensembles.

Conclusion: FreeZeV2 demonstrates that task-specific training is unnecessary for accurate 6D pose estimation, offering a scalable and efficient solution.

Abstract: Estimating the 6D pose of objects from RGBD data is a fundamental problem in
computer vision, with applications in robotics and augmented reality. A key
challenge is achieving generalization to novel objects that were not seen
during training. Most existing approaches address this by scaling up training
on synthetic data tailored to the task, a process that demands substantial
computational resources. But is task-specific training really necessary for
accurate and efficient 6D pose estimation of novel objects? To answer No!, we
introduce FreeZeV2, the second generation of FreeZe: a training-free method
that achieves strong generalization to unseen objects by leveraging geometric
and vision foundation models pre-trained on unrelated data. FreeZeV2 improves
both accuracy and efficiency over FreeZe through three key contributions: (i) a
sparse feature extraction strategy that reduces inference-time computation
without sacrificing accuracy; (ii) a feature-aware scoring mechanism that
improves both pose selection during RANSAC-based 3D registration and the final
ranking of pose candidates; and (iii) a modular design that supports ensembles
of instance segmentation models, increasing robustness to segmentation masks
errors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,
where it establishes a new state-of-the-art in 6D pose estimation of unseen
objects. When using the same segmentation masks, FreeZeV2 achieves a remarkable
8x speedup over FreeZe while also improving accuracy by 5%. When using
ensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy
while still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall
Method at the BOP Challenge 2024.

</details>


### [134] [DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision](https://arxiv.org/abs/2506.09814)
*Xiandong Zou,Ruihao Xia,Hongsong Wang,Pan Zhou*

Main category: cs.CV

TL;DR: The paper introduces 3D-MeshPref, a large-scale unpaired 3D preference dataset, and RewardCS, a reward model trained on it, to improve text-to-3D generation by aligning with human preferences without paired comparisons. DreamCS integrates RewardCS into text-to-3D pipelines, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D methods struggle with human preference alignment due to reliance on 2D reward models, causing geometric artifacts. The goal is to address this by leveraging unpaired 3D preference data.

Method: Construct 3D-MeshPref dataset, train RewardCS using Cauchy-Schwarz divergence, and integrate it into DreamCS for text-to-3D generation.

Result: DreamCS produces 3D assets that are geometrically faithful and human-preferred, outperforming existing methods.

Conclusion: The approach effectively aligns 3D generation with human preferences using unpaired data, improving quality and reducing artifacts.

Abstract: While text-to-3D generation has attracted growing interest, existing methods
often struggle to produce 3D assets that align well with human preferences.
Current preference alignment techniques for 3D content typically rely on
hardly-collected preference-paired multi-view 2D images to train 2D reward
models, when then guide 3D generation -- leading to geometric artifacts due to
their inherent 2D bias. To address these limitations, we construct 3D-MeshPref,
the first large-scale unpaired 3D preference dataset, featuring diverse 3D
meshes annotated by a large language model and refined by human evaluators. We
then develop RewardCS, the first reward model trained directly on unpaired
3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling
effective learning of human-aligned 3D geometric preferences without requiring
paired comparisons. Building on this, we propose DreamCS, a unified framework
that integrates RewardCS into text-to-3D pipelines -- enhancing both implicit
and explicit 3D generation with human preference feedback. Extensive
experiments show DreamCS outperforms prior methods, producing 3D assets that
are both geometrically faithful and human-preferred. Code and models will be
released publicly.

</details>


### [135] [MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion](https://arxiv.org/abs/2506.09834)
*Chuang Maa,Yu Peia,Jianhang Zhanga,Shaokai Zhaoa,Bowen Jib,Liang Xiea,Ye Yana,Erwei Yin*

Main category: cs.CV

TL;DR: The paper introduces MMME, a novel dataset for micro-expression (ME) analysis, combining visual and physiological signals to enhance recognition and spotting performance.


<details>
  <summary>Details</summary>
Motivation: Existing ME research is limited to single visual modality, missing rich emotional data from physiological signals, hindering practical applications.

Method: The study develops MMME, a dataset with synchronized facial action signals, EEG, and peripheral physiological signals (PPG, RSP, SKT, EDA, ECG).

Result: Experiments show integrating MEs with physiological signals significantly improves recognition and spotting performance.

Conclusion: MMME advances ME research by enabling multimodal fusion, shifting from single-modality analysis to exploring visual-physiological synergies.

Abstract: Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an
individual's genuine emotional state. Their analysis has attracted considerable
interest due to its promising applications in fields such as healthcare,
criminal investigation, and human-computer interaction. However, existing ME
research is limited to single visual modality, overlooking the rich emotional
information conveyed by other physiological modalities, resulting in ME
recognition and spotting performance far below practical application needs.
Therefore, exploring the cross-modal association mechanism between ME visual
features and physiological signals (PS), and developing a multimodal fusion
framework, represents a pivotal step toward advancing ME analysis. This study
introduces a novel ME dataset, MMME, which, for the first time, enables
synchronized collection of facial action signals (MEs), central nervous system
signals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming
the constraints of existing ME corpora, MMME comprises 634 MEs, 2,841
macro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,
establishing a robust foundation for investigating ME neural mechanisms and
conducting multimodal fusion-based analyses. Extensive experiments validate the
dataset's reliability and provide benchmarks for ME analysis, demonstrating
that integrating MEs with PS significantly enhances recognition and spotting
performance. To the best of our knowledge, MMME is the most comprehensive ME
dataset to date in terms of modality diversity. It provides critical data
support for exploring the neural mechanisms of MEs and uncovering the
visual-physiological synergistic effects, driving a paradigm shift in ME
research from single-modality visual analysis to multimodal fusion. The dataset
will be publicly available upon acceptance of this paper.

</details>


### [136] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Main category: cs.CV

TL;DR: DynaSplat extends Gaussian Splatting for dynamic scenes using dynamic-static separation and hierarchical motion modeling, outperforming state-of-the-art methods in accuracy and realism.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with real-world dynamic scene complexity, prompting the need for a more robust solution.

Method: Classifies static/dynamic elements via deformation offset statistics and 2D motion flow, uses hierarchical motion modeling, and integrates opacity estimation.

Result: DynaSplat achieves superior accuracy and realism in dynamic scene reconstruction.

Conclusion: DynaSplat offers an efficient, intuitive, and compact solution for dynamic scene reconstruction.

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [137] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/abs/2506.09839)
*Chen Gao,Liankai Jin,Xingyu Peng,Jiazhao Zhang,Yue Deng,Annan Li,He Wang,Si Liu*

Main category: cs.CV

TL;DR: The paper introduces OctoNav-Bench and OctoNav-R1 to develop generalist navigation agents capable of following free-form, multi-modal instructions, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: To address the fragmentation in navigation research by creating a unified benchmark and method for generalist navigation agents.

Method: Proposes OctoNav-Bench (a benchmark with diverse instruction-trajectory pairs) and OctoNav-R1 (a VLA-type model trained via Hybrid Training Paradigm with TBA-SFT and Nav-GPRO stages).

Result: OctoNav-R1 demonstrates superior performance over existing methods.

Conclusion: The work advances generalist navigation by integrating multi-modal capabilities and reasoning, validated by the success of OctoNav-R1.

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [138] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/abs/2506.09846)
*Panagiotis Kaliosis,John Pavlopoulos*

Main category: cs.CV

TL;DR: A novel loss function using Wasserstein distance improves handwritten text recognition by aligning predicted character distributions with target distributions, enhancing accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Handwritten text recognition faces challenges due to evolving character sets and shifting frequency distributions, causing models to underperform on specific subsets.

Method: Proposes a loss function incorporating Wasserstein distance to penalize divergence from expected character distributions and integrates it into guided decoding for inference-time improvements.

Result: Experimental results show enhanced generalization and performance across datasets and architectures.

Conclusion: The method effectively addresses intra-dataset shifts and improves model performance without retraining, with code made publicly available.

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [139] [IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments](https://arxiv.org/abs/2506.09849)
*Florian Bordes,Quentin Garrido,Justine T Kao,Adina Williams,Michael Rabbat,Emmanuel Dupoux*

Main category: cs.CV

TL;DR: IntPhys 2 is a video benchmark evaluating deep learning models' intuitive physics understanding, focusing on four core principles. Models struggle, performing at chance levels, unlike humans.


<details>
  <summary>Details</summary>
Motivation: To assess and improve deep learning models' intuitive physics understanding, inspired by early childhood development.

Method: Uses a violation of expectation framework in controlled virtual environments to test four principles: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.

Result: State-of-the-art models perform poorly (50% accuracy), far below human performance, revealing a significant gap.

Conclusion: Current models lack human-like intuitive physics understanding, calling for improved architectures and training methods.

Abstract: We present IntPhys 2, a video benchmark designed to evaluate the intuitive
physics understanding of deep learning models. Building on the original IntPhys
benchmark, IntPhys 2 focuses on four core principles related to macroscopic
objects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.
These conditions are inspired by research into intuitive physical understanding
emerging during early childhood. IntPhys 2 offers a comprehensive suite of
tests, based on the violation of expectation framework, that challenge models
to differentiate between possible and impossible events within controlled and
diverse virtual environments. Alongside the benchmark, we provide performance
evaluations of several state-of-the-art models. Our findings indicate that
while these models demonstrate basic visual understanding, they face
significant challenges in grasping intuitive physics across the four principles
in complex scenes, with most models performing at chance levels (50%), in stark
contrast to human performance, which achieves near-perfect accuracy. This
underscores the gap between current models and human-like intuitive physics
understanding, highlighting the need for advancements in model architectures
and training methodologies.

</details>


### [140] [Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation](https://arxiv.org/abs/2506.09881)
*Siyu Chen,Ting Han,Chengzheng Fu,Changshe Zhang,Chaolei Wang,Jinhe Su,Guorong Cai,Meiliu Wu*

Main category: cs.CV

TL;DR: Vireo is a novel framework for Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS), combining OVSS and DGSS. It uses Visual Foundation Models (VFMs) and depth to extract domain-invariant features, with three key components for robust performance.


<details>
  <summary>Details</summary>
Motivation: Address the need for pixel-level segmentation of unseen categories and domains, crucial for real-world applications like autonomous driving in adverse conditions.

Method: Leverages frozen VFMs and depth VFMs for domain-invariant features. Introduces GeoText Prompts, CMPE, and DOV-VEH to align visual and textual modalities under domain shift.

Result: Achieves state-of-the-art performance, outperforming existing methods in domain generalization and open-vocabulary recognition.

Conclusion: Vireo provides a unified, scalable solution for robust visual understanding in diverse environments.

Abstract: Open-Vocabulary semantic segmentation (OVSS) and domain generalization in
semantic segmentation (DGSS) highlight a subtle complementarity that motivates
Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS
aims to generate pixel-level masks for unseen categories while maintaining
robustness across unseen domains, a critical capability for real-world
scenarios such as autonomous driving in adverse conditions. We introduce Vireo,
a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS
and DGSS for the first time. Vireo builds upon the frozen Visual Foundation
Models (VFMs) and incorporates scene geometry via Depth VFMs to extract
domain-invariant structural features. To bridge the gap between visual and
textual modalities under domain shift, we propose three key components: (1)
GeoText Prompts, which align geometric features with language cues and
progressively refine VFM encoder representations; (2) Coarse Mask Prior
Embedding (CMPE) for enhancing gradient flow for faster convergence and
stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding
Head (DOV-VEH), which fuses refined structural and semantic features for robust
prediction. Comprehensive evaluation on these components demonstrates the
effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art
performance and surpasses existing methods by a large margin in both domain
generalization and open-vocabulary recognition, offering a unified and scalable
solution for robust visual understanding in diverse and dynamic environments.
Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.

</details>


### [141] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/abs/2506.09883)
*Seonho Lee,Jiho Choi,Inha Kang,Jiwook Kim,Junsung Park,Hyunjung Shim*

Main category: cs.CV

TL;DR: A lightweight framework, Geometric Distillation, enhances 2D-trained VLMs with 3D spatial understanding by injecting geometric cues without architectural changes.


<details>
  <summary>Details</summary>
Motivation: VLMs lack 3D spatial understanding despite strong performance in visual and linguistic tasks.

Method: Uses sparse correspondences, relative depth relations, and dense cost volumes from 3D models to fine-tune VLMs.

Result: Outperforms prior methods in 3D reasoning benchmarks with lower computational cost.

Conclusion: Provides a scalable way to equip VLMs with 3D understanding for spatially grounded tasks.

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [142] [The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge](https://arxiv.org/abs/2506.09885)
*Haoru Wang,Kai Ye,Yangyan Li,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: The paper explores reducing 3D knowledge dependence in novel view synthesis (NVS), showing that less reliance on 3D biases scales better with data. A new framework is proposed, achieving comparable results without 3D biases or pose annotations.


<details>
  <summary>Details</summary>
Motivation: To investigate the role of 3D knowledge in NVS and reduce its dependence, leveraging large-scale data for better performance.

Method: Proposes a framework minimizing 3D inductive bias and pose dependence, learning implicit 3D awareness from sparse 2D images.

Result: Generates photorealistic, 3D-consistent novel views, matching performance of methods using posed inputs.

Conclusion: Reducing 3D knowledge dependence is feasible and effective, especially with large-scale data.

Abstract: We consider the problem of generalizable novel view synthesis (NVS), which
aims to generate photorealistic novel views from sparse or even unposed 2D
images without per-scene optimization. This task remains fundamentally
challenging, as it requires inferring 3D structure from incomplete and
ambiguous 2D observations. Early approaches typically rely on strong 3D
knowledge, including architectural 3D inductive biases (e.g., embedding
explicit 3D representations, such as NeRF or 3DGS, into network design) and
ground-truth camera poses for both input and target views. While recent efforts
have sought to reduce the 3D inductive bias or the dependence on known camera
poses of input views, critical questions regarding the role of 3D knowledge and
the necessity of circumventing its use remain under-explored. In this work, we
conduct a systematic analysis on the 3D knowledge and uncover a critical trend:
the performance of methods that requires less 3D knowledge accelerates more as
data scales, eventually achieving performance on par with their 3D
knowledge-driven counterparts, which highlights the increasing importance of
reducing dependence on 3D knowledge in the era of large-scale data. Motivated
by and following this trend, we propose a novel NVS framework that minimizes 3D
inductive bias and pose dependence for both input and target views. By
eliminating this 3D knowledge, our method fully leverages data scaling and
learns implicit 3D awareness directly from sparse 2D images, without any 3D
inductive bias or pose annotation during training. Extensive experiments
demonstrate that our model generates photorealistic and 3D-consistent novel
views, achieving even comparable performance with methods that rely on posed
inputs, thereby validating the feasibility and effectiveness of our
data-centric paradigm. Project page:
https://pku-vcl-geometry.github.io/Less3Depend/ .

</details>


### [143] [EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks](https://arxiv.org/abs/2506.09895)
*Athinoulla Konstantinou,Georgios Leontidis,Mamatha Thota,Aiden Durrant*

Main category: cs.CV

TL;DR: EquiCaps, a capsule-based method, leverages intrinsic pose-awareness for self-supervised learning, outperforming state-of-the-art equivariant methods in pose estimation tasks.


<details>
  <summary>Details</summary>
Motivation: To explore capsule networks' inherent ability to learn interpretable pose-aware representations without relying on specialized predictors for equivariance.

Method: Introduces EquiCaps, a capsule-based approach, and tests it with multi-geometric transformations using the 3DIEBench-T dataset.

Result: EquiCaps achieves a supervised-level R² of 0.78 in rotation prediction, outperforming SIE and CapsIE by 0.05 and 0.04 R², respectively.

Conclusion: EquiCaps demonstrates robust equivariant performance under combined transformations, highlighting the potential of predictor-free capsule architectures.

Abstract: Learning self-supervised representations that are invariant and equivariant
to transformations is crucial for advancing beyond traditional visual
classification tasks. However, many methods rely on predictor architectures to
encode equivariance, despite evidence that architectural choices, such as
capsule networks, inherently excel at learning interpretable pose-aware
representations. To explore this, we introduce EquiCaps (Equivariant Capsule
Network), a capsule-based approach to pose-aware self-supervision that
eliminates the need for a specialised predictor for enforcing equivariance.
Instead, we leverage the intrinsic pose-awareness capabilities of capsules to
improve performance in pose estimation tasks. To further challenge our
assumptions, we increase task complexity via multi-geometric transformations to
enable a more thorough evaluation of invariance and equivariance by introducing
3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical
results demonstrate that EquiCaps outperforms prior state-of-the-art
equivariant methods on rotation prediction, achieving a supervised-level $R^2$
of 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE
and CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to
non-capsule-based equivariant approaches, EquiCaps maintains robust equivariant
performance under combined geometric transformations, underscoring its
generalisation capabilities and the promise of predictor-free capsule
architectures.

</details>


### [144] [CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects](https://arxiv.org/abs/2506.09897)
*Tao Liu,Zhenchao Cui*

Main category: cs.CV

TL;DR: E-FPN-BS addresses the issue of untrained high-level features in Tiny Object Detection by enhancing low-level features with wasted high-level semantics, using modules like CEM and FBSM, and a Dynamic Gradient-Balanced Loss.


<details>
  <summary>Details</summary>
Motivation: Standard label assignment in feature pyramid networks leaves high-level features untrained, causing semantic dead-ends and poor low-level feature context.

Method: Proposes E-FPN-BS with Context Enhancement Module (CEM) for global-local fusion, Foreground-Background Separation Module (FBSM) for dynamic region amplification, and Dynamic Gradient-Balanced Loss (DCLoss) for scale-aware optimization.

Result: Outstanding performance and generalization across multiple benchmark datasets.

Conclusion: E-FPN-BS effectively converts wasted high-level semantics into useful low-level enhancements, improving tiny object detection.

Abstract: Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid
networks: high-level features (P5-P6) frequently receive zero positive anchors
under standard label assignment protocols, leaving their semantic
representations untrained due to exclusion from loss computation. This creates
dual deficiencies: (1) Stranded high-level features become semantic dead-ends
without gradient updates, while (2) low-level features lack essential semantic
context for robust classification. We propose E-FPN-BS that systematically
converts wasted high-level semantics into low-level feature enhancements. To
address these issues, we propose E-FPN-BS, a novel architecture integrating
multi-scale feature enhancement and adaptive optimization. First, our Context
Enhancement Module(CEM) employs dual-branch processing to align and compress
high-level features for effective global-local fusion. Second, the
Foreground-Background Separation Module (FBSM) generates spatial gating masks
that dynamically amplify discriminative regions. To address gradient imbalance
across object scales, we further propose a Dynamic Gradient-Balanced Loss
(DCLoss) that automatically modulates loss contributions via scale-aware
gradient equilibrium. Extensive experiments across multiple benchmark datasets
demonstrate the outstanding performance and generalization ability of our
approach.

</details>


### [145] [Only-Style: Stylistic Consistency in Image Generation without Content Leakage](https://arxiv.org/abs/2506.09916)
*Tilemachos Aravanis,Panagiotis Filntisis,Petros Maragos,George Retsinas*

Main category: cs.CV

TL;DR: Proposes Only-Style, a method to mitigate content leakage in style-consistent image generation by adaptive tuning and localization, with a novel evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of separating semantic content from stylistic elements in image generation to avoid content leakage.

Method: Only-Style localizes content leakage during inference and adaptively tunes style alignment parameters, functioning as a standalone component.

Result: Significant improvement over state-of-the-art methods, achieving robust stylistic consistency without content leakage.

Conclusion: Only-Style effectively balances stylistic consistency and leakage elimination, validated by a novel evaluation framework.

Abstract: Generating images in a consistent reference visual style remains a
challenging computer vision task. State-of-the-art methods aiming for
style-consistent generation struggle to effectively separate semantic content
from stylistic elements, leading to content leakage from the image provided as
a reference to the targets. To address this challenge, we propose Only-Style: a
method designed to mitigate content leakage in a semantically coherent manner
while preserving stylistic consistency. Only-Style works by localizing content
leakage during inference, allowing the adaptive tuning of a parameter that
controls the style alignment process, specifically within the image patches
containing the subject in the reference image. This adaptive process best
balances stylistic consistency with leakage elimination. Moreover, the
localization of content leakage can function as a standalone component, given a
reference-target image pair, allowing the adaptive tuning of any
method-specific parameter that provides control over the impact of the
stylistic reference. In addition, we propose a novel evaluation framework to
quantify the success of style-consistent generations in avoiding undesired
content leakage. Our approach demonstrates a significant improvement over
state-of-the-art methods through extensive evaluation across diverse instances,
consistently achieving robust stylistic consistency without undesired content
leakage.

</details>


### [146] [MetricHMR: Metric Human Mesh Recovery from Monocular Images](https://arxiv.org/abs/2506.09919)
*He Zhang,Chentao Song,Hongwen Zhang,Tao Yu*

Main category: cs.CV

TL;DR: MetricHMR introduces a method for accurate metric human mesh recovery from monocular images, addressing scale and depth ambiguity in existing HMR methods.


<details>
  <summary>Details</summary>
Motivation: Existing HMR methods struggle with scale and depth ambiguity, leading to inaccurate global translations. MetricHMR aims to resolve this by leveraging the standard perspective projection model.

Method: The approach analyzes camera models, validates ambiguity ranges, and introduces a ray map to encode bounding-box info, camera parameters, and geometric cues for End2End metric HMR.

Result: MetricHMR achieves state-of-the-art performance in metric pose, shape, and global translation estimation across diverse scenarios.

Conclusion: The method effectively addresses ambiguity in HMR, providing geometrically reasonable reconstructions without additional regularization modules.

Abstract: We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric
human mesh recovery with accurate global translation from monocular images. In
contrast to existing HMR methods that suffer from severe scale and depth
ambiguity, MetricHMR is able to produce geometrically reasonable body shape and
global translation in the reconstruction results. To this end, we first
systematically analyze previous HMR methods on camera models to emphasize the
critical role of the standard perspective projection model in enabling
metric-scale HMR. We then validate the acceptable ambiguity range of metric HMR
under the standard perspective projection model. Finally, we contribute a novel
approach that introduces a ray map based on the standard perspective projection
to jointly encode bounding-box information, camera parameters, and geometric
cues for End2End metric HMR without any additional metric-regularization
modules. Extensive experiments demonstrate that our method achieves
state-of-the-art performance, even compared with sequential HMR methods, in
metric pose, shape, and global translation estimation across both indoor and
in-the-wild scenarios.

</details>


### [147] [Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering](https://arxiv.org/abs/2506.09920)
*Jianhan Qi,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: The paper introduces a method for hyperspectral image clustering using a structural-spectral graph convolutional operator and evidence-guided adaptive edge learning to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs fail to fully exploit spectral information in hyperspectral images, and inaccurate superpixel graphs can confuse class semantics.

Method: Proposes SSGCO for spatial-spectral feature extraction and EGAEL for adaptive edge refinement, integrated into a contrastive learning framework.

Result: Improves clustering accuracy by 2.61% to 6.06% over existing methods on four datasets.

Conclusion: The proposed method effectively enhances clustering accuracy by better leveraging spectral and spatial features.

Abstract: Hyperspectral image (HSI) clustering assigns similar pixels to the same class
without any annotations, which is an important yet challenging task. For
large-scale HSIs, most methods rely on superpixel segmentation and perform
superpixel-level clustering based on graph neural networks (GNNs). However,
existing GNNs cannot fully exploit the spectral information of the input HSI,
and the inaccurate superpixel topological graph may lead to the confusion of
different class semantics during information aggregation. To address these
challenges, we first propose a structural-spectral graph convolutional operator
(SSGCO) tailored for graph-structured HSI superpixels to improve their
representation quality through the co-extraction of spatial and spectral
features. Second, we propose an evidence-guided adaptive edge learning (EGAEL)
module that adaptively predicts and refines edge weights in the superpixel
topological graph. We integrate the proposed method into a contrastive learning
framework to achieve clustering, where representation learning and clustering
are simultaneously conducted. Experiments demonstrate that the proposed method
improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best
compared methods on four HSI datasets. Our code is available at
https://github.com/jhqi/SSGCO-EGAEL.

</details>


### [148] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Main category: cs.CV

TL;DR: The paper introduces a dataset for visually grounded dialogue in videos, requiring models to combine visual recognition with external knowledge for accurate responses.


<details>
  <summary>Details</summary>
Motivation: To extend OK-VQA to a conversational setting, addressing the need for models to integrate visual details over time and external knowledge, while considering dialogue context.

Method: Creation of a dataset with 2,017 videos and 5,986 dialogues (40,954 turns), where questions require external knowledge not in the visuals. Baselines are provided.

Result: The dataset is publicly available, and baseline evaluations highlight challenges in combining visual and external knowledge for dialogue.

Conclusion: The work advances OK-VQA to dialogue settings, offering a dataset and insights into the challenges of integrating visual and external knowledge for conversation.

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [149] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.CV

TL;DR: HadaNorm, a novel linear transformation, improves quantization for diffusion models by normalizing activations and using Hadamard transformations, achieving better efficiency-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face high memory and computational demands, limiting deployment on resource-constrained devices. Standard PTQ methods struggle with outliers and require transformations for higher compression.

Method: Proposes HadaNorm, a linear transformation that normalizes activations and applies Hadamard transformations to mitigate outliers and enable aggressive quantization.

Result: HadaNorm reduces quantization error across transformer blocks and outperforms state-of-the-art methods in efficiency-performance trade-offs.

Conclusion: HadaNorm effectively addresses quantization challenges for diffusion models, enabling deployment on resource-constrained devices with superior performance.

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [150] [LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation](https://arxiv.org/abs/2506.09935)
*Jiangyong Huang,Xiaojian Ma,Xiongkun Linghu,Yue Fan,Junchao He,Wenxin Tan,Qing Li,Song-Chun Zhu,Yixin Chen,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: LEO-VL, a 3D-VL model using condensed feature grid (CFG), achieves state-of-the-art performance by addressing data scalability and representation efficiency, supported by a large-scale dataset and novel post-training objective.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current 3D-VL models in capability and robustness, and to bridge the gap with 2D counterparts by improving data scalability and scene representation.

Method: Proposes LEO-VL, built on CFG for efficient scene representation, and curates a large-scale dataset (700k+ samples) across multiple domains and tasks. Introduces SceneDPO for post-training robustness.

Result: Achieves SOTA on benchmarks like SQA3D, MSQA, and Beacon3D. Ablations confirm CFG efficiency, task diversity importance, and data curation validity.

Conclusion: LEO-VL advances scalable and robust 3D-VL generalists, with potential for broader impact in the field.

Abstract: Developing 3D-VL generalists capable of understanding 3D scenes and following
natural language instructions to perform a wide range of tasks has been a
long-standing goal in the 3D-VL community. Despite recent progress, 3D-VL
models still lag behind their 2D counterparts in capability and robustness,
falling short of the generalist standard. A key obstacle to developing 3D-VL
generalists lies in data scalability, hindered by the lack of an efficient
scene representation. We propose LEO-VL, a 3D-VL model built upon condensed
feature grid (CFG), an efficient scene representation that bridges 2D
perception and 3D spatial structure while significantly reducing token
overhead. This efficiency unlocks large-scale training towards 3D-VL
generalist, for which we curate over 700k high-quality 3D-VL data spanning four
domains of real-world indoor scenes and five tasks such as captioning and
dialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA
benchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the
efficiency of our representation, the importance of task and scene diversity,
and the validity of our data curation principle. Furthermore, we introduce
SceneDPO, a novel post-training objective that enhances the robustness of 3D-VL
models. We hope our findings contribute to the advancement of scalable and
robust 3D-VL generalists.

</details>


### [151] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/abs/2506.09943)
*Aaron Foss,Chloe Evans,Sasha Mitts,Koustuv Sinha,Ammar Rizvi,Justine T. Kao*

Main category: cs.CV

TL;DR: CausalVQA is a new VQA benchmark dataset focusing on causality in real-world videos, challenging models with five question types to test deep visual understanding. Current models lag behind humans, especially in anticipation and hypothetical questions.


<details>
  <summary>Details</summary>
Motivation: Existing VQA benchmarks lack focus on causality in real-world scenarios, limiting models' ability to predict outcomes of actions and events. CausalVQA addresses this gap.

Method: The dataset includes five question types (counterfactual, hypothetical, anticipation, planning, descriptive) with quality controls to prevent shortcuts, ensuring deep visual understanding.

Result: Frontier multimodal models perform significantly worse than humans, particularly on anticipation and hypothetical questions, revealing gaps in spatial-temporal reasoning and physical understanding.

Conclusion: CausalVQA highlights the need for improved models capable of leveraging spatial-temporal reasoning and physical principles for accurate real-world predictions.

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [152] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: UniPre3D is a unified pre-training method for point clouds of any scale, using Gaussian primitives and differentiable rendering for pixel-level supervision. It integrates 2D features for texture knowledge and shows effectiveness across object- and scene-level tasks.


<details>
  <summary>Details</summary>
Motivation: The diversity in point cloud scales lacks unified representation learning techniques, with no existing pre-training method equally effective for both object- and scene-level data.

Method: Predicts Gaussian primitives as the pre-training task, uses differentiable Gaussian splatting for rendering, and integrates 2D features from pre-trained image models.

Result: Validated through extensive experiments, UniPre3D demonstrates universal effectiveness across diverse object- and scene-level tasks.

Conclusion: UniPre3D is the first unified pre-training method for any-scale point clouds, offering seamless application and improved performance.

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [153] [Vision Generalist Model: A Survey](https://arxiv.org/abs/2506.09954)
*Ziyi Wang,Yongming Rao,Shuofeng Sun,Xinrun Liu,Yi Wei,Xumin Yu,Zuyan Liu,Yanbo Wang,Hongmin Liu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: A review of vision generalist models, their design, performance techniques, and applications, highlighting challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The success of generalist models in NLP inspires their application to diverse vision tasks, despite challenges in unifying representations.

Method: Review background, analyze frameworks, explore performance-enhancing techniques, and examine related domains.

Result: Comprehensive insights into vision generalist models, their capabilities, and interconnections with other fields.

Conclusion: Identifies real-world applications, challenges, and future research directions for vision generalist models.

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [154] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: Kvasir-VQA-x1 is a large-scale dataset for GI endoscopy, expanding the original Kvasir-VQA with 159,549 new question-answer pairs to test deeper clinical reasoning and model robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in existing MedVQA datasets, which lack clinical complexity and visual diversity, to improve clinical decision support systems.

Method: Systematic generation of question-answer pairs using large language models, stratified by complexity, and visual augmentations to mimic imaging artifacts.

Result: A challenging, clinically relevant benchmark dataset supporting standard VQA performance and robustness testing against visual perturbations.

Conclusion: Kvasir-VQA-x1 accelerates reliable multimodal AI development for clinical use, adhering to FAIR data principles and being openly accessible.

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [155] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/abs/2506.09965)
*Junfei Wu,Jian Guan,Kaituo Feng,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tieniu Tan*

Main category: cs.CV

TL;DR: The paper introduces a novel paradigm, 'drawing to reason in space,' for enhancing multimodal reasoning in LVLMs by enabling visual manipulation through drawing operations, outperforming existing methods by 18.4%.


<details>
  <summary>Details</summary>
Motivation: Existing text-centric multimodal reasoning methods lack precise geometric understanding and spatial tracking, limiting performance in spatial tasks.

Method: Proposes a three-stage training framework: cold-start training with synthetic data, reflective rejection sampling, and reinforcement learning, to equip LVLMs with drawing operations for visual reasoning.

Result: The model, VILASR, shows an average 18.4% improvement over existing methods in diverse spatial reasoning benchmarks.

Conclusion: Visual manipulation through drawing operations significantly enhances LVLMs' spatial reasoning capabilities, addressing limitations of text-centric approaches.

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [156] [Vectorized Region Based Brush Strokes for Artistic Rendering](https://arxiv.org/abs/2506.09969)
*Jeripothula Prudviraj,Vikram Jamwal*

Main category: cs.CV

TL;DR: A method for creating stroke-by-stroke artwork evolution with semantic guidance, improved stroke alignment, and high-fidelity rendering.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between static artwork and its creation process by addressing limitations in current stroke-based painting systems.

Method: Uses semantic guidance for targeted regions, computes brush stroke parameters, and sequences strokes for rendering.

Result: Produces high-fidelity paintings with superior stroke quality across various input types.

Conclusion: The method effectively aligns with artistic principles and intent, outperforming existing systems.

Abstract: Creating a stroke-by-stroke evolution process of a visual artwork tries to
bridge the emotional and educational gap between the finished static artwork
and its creation process. Recent stroke-based painting systems focus on
capturing stroke details by predicting and iteratively refining stroke
parameters to maximize the similarity between the input image and the rendered
output. However, these methods often struggle to produce stroke compositions
that align with artistic principles and intent. To address this, we explore an
image-to-painting method that (i) facilitates semantic guidance for brush
strokes in targeted regions, (ii) computes the brush stroke parameters, and
(iii) establishes a sequence among segments and strokes to sequentially render
the final painting. Experimental results on various input image types, such as
face images, paintings, and photographic images, show that our method aligns
with a region-based painting strategy while rendering a painting with high
fidelity and superior stroke quality.

</details>


### [157] [Efficient Part-level 3D Object Generation via Dual Volume Packing](https://arxiv.org/abs/2506.09980)
*Jiaxiang Tang,Ruijie Lu,Zhaoshuo Li,Zekun Hao,Xuan Li,Fangyin Wei,Shuran Song,Gang Zeng,Ming-Yu Liu,Tsung-Yi Lin*

Main category: cs.CV

TL;DR: A new framework for part-level 3D object generation from a single image, enabling editable and semantically meaningful parts.


<details>
  <summary>Details</summary>
Motivation: Existing methods generate fused meshes, limiting part-level editing. The challenge is handling varying numbers of parts.

Method: Uses a dual volume packing strategy to organize parts into complementary volumes for complete and interleaved assembly.

Result: Achieves better quality, diversity, and generalization compared to previous image-based part-level generation methods.

Conclusion: The proposed framework effectively addresses the limitations of fused mesh generation, enabling flexible part-level manipulation.

Abstract: Recent progress in 3D object generation has greatly improved both the quality
and efficiency. However, most existing methods generate a single mesh with all
parts fused together, which limits the ability to edit or manipulate individual
parts. A key challenge is that different objects may have a varying number of
parts. To address this, we propose a new end-to-end framework for part-level 3D
object generation. Given a single input image, our method generates
high-quality 3D objects with an arbitrary number of complete and semantically
meaningful parts. We introduce a dual volume packing strategy that organizes
all parts into two complementary volumes, allowing for the creation of complete
and interleaved parts that assemble into the final object. Experiments show
that our model achieves better quality, diversity, and generalization than
previous image-based part-level generation methods.

</details>


### [158] [ReSim: Reliable World Simulation for Autonomous Driving](https://arxiv.org/abs/2506.09981)
*Jiazhi Yang,Kashyap Chitta,Shenyuan Gao,Long Chen,Yuqian Shao,Xiaosong Jia,Hongyang Li,Andreas Geiger,Xiangyu Yue,Li Chen*

Main category: cs.CV

TL;DR: The paper introduces ReSim, a controllable world model for simulating diverse driving scenarios, including hazardous behaviors, by combining real-world and simulator data. It improves fidelity, controllability, and planning performance.


<details>
  <summary>Details</summary>
Motivation: Existing driving world models rely on safe expert trajectories, limiting their ability to simulate non-expert or hazardous behaviors. This restricts their use in tasks like policy evaluation.

Method: The authors enrich real-world data with non-expert simulator data (e.g., CARLA) and develop a diffusion transformer-based video generator with enhanced conditioning for controllability. They also introduce a Video2Reward module for reward estimation.

Result: ReSim achieves 44% higher visual fidelity, over 50% better controllability for expert and non-expert actions, and improves planning and policy selection by 2% and 25%, respectively.

Conclusion: ReSim bridges the gap between high-fidelity simulation and practical applications, enabling reliable simulation of diverse driving scenarios and improving downstream tasks like planning and policy evaluation.

Abstract: How can we reliably simulate future driving scenarios under a wide range of
ego driving behaviors? Recent driving world models, developed exclusively on
real-world driving data composed mainly of safe expert trajectories, struggle
to follow hazardous or non-expert behaviors, which are rare in such data. This
limitation restricts their applicability to tasks such as policy evaluation. In
this work, we address this challenge by enriching real-world human
demonstrations with diverse non-expert data collected from a driving simulator
(e.g., CARLA), and building a controllable world model trained on this
heterogeneous corpus. Starting with a video generator featuring a diffusion
transformer architecture, we devise several strategies to effectively integrate
conditioning signals and improve prediction controllability and fidelity. The
resulting model, ReSim, enables Reliable Simulation of diverse open-world
driving scenarios under various actions, including hazardous non-expert ones.
To close the gap between high-fidelity simulation and applications that require
reward signals to judge different actions, we introduce a Video2Reward module
that estimates a reward from ReSim's simulated future. Our ReSim paradigm
achieves up to 44% higher visual fidelity, improves controllability for both
expert and non-expert actions by over 50%, and boosts planning and policy
selection performance on NAVSIM by 2% and 25%, respectively.

</details>


### [159] [AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation](https://arxiv.org/abs/2506.09982)
*Zijie Wu,Chaohui Yu,Fan Wang,Xiang Bai*

Main category: cs.CV

TL;DR: AnimateAnyMesh is a feed-forward framework for text-driven animation of 3D meshes, using DyMeshVAE and Rectified Flow for efficient, high-quality generation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of creating high-quality animated 3D models due to spatio-temporal complexity and lack of 4D training data.

Method: Leverages DyMeshVAE to compress/reconstruct dynamic mesh sequences and employs Rectified Flow for text-conditional generation.

Result: Generates semantically accurate, temporally coherent animations quickly, outperforming existing methods.

Conclusion: Advances 4D content creation, making it more accessible; data, code, and models will be open-released.

Abstract: Recent advances in 4D content generation have attracted increasing attention,
yet creating high-quality animated 3D models remains challenging due to the
complexity of modeling spatio-temporal distributions and the scarcity of 4D
training data. In this paper, we present AnimateAnyMesh, the first feed-forward
framework that enables efficient text-driven animation of arbitrary 3D meshes.
Our approach leverages a novel DyMeshVAE architecture that effectively
compresses and reconstructs dynamic mesh sequences by disentangling spatial and
temporal features while preserving local topological structures. To enable
high-quality text-conditional generation, we employ a Rectified Flow-based
training strategy in the compressed latent space. Additionally, we contribute
the DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text
annotations. Experimental results demonstrate that our method generates
semantically accurate and temporally coherent mesh animations in a few seconds,
significantly outperforming existing approaches in both quality and efficiency.
Our work marks a substantial step forward in making 4D content creation more
accessible and practical. All the data, code, and models will be open-released.

</details>


### [160] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/abs/2506.09984)
*Zhenzhi Wang,Jiaqi Yang,Jianwen Jiang,Chao Liang,Gaojie Lin,Zerong Zheng,Ceyuan Yang,Dahua Lin*

Main category: cs.CV

TL;DR: A novel framework for multi-concept human animation with precise per-identity control using region-specific conditions from text, image, and audio.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack precise control for multiple concepts and interactions in videos, limiting applications.

Method: Leverages a mask predictor to infer layout from reference images and injects local audio conditions iteratively for alignment.

Result: Enables high-quality, controllable multi-concept human-centric videos with explicit layout control.

Conclusion: The framework outperforms implicit methods, validated by empirical results and ablation studies.

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [161] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Main category: cs.CV

TL;DR: The paper introduces the Minimal Video Pairs (MVP) benchmark to address shortcut solutions in video QA benchmarks, ensuring accurate assessment of video-language models' physical understanding.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are prone to score inflation due to superficial cues, leading to inaccurate model performance evaluation.

Method: MVP includes 55K multiple-choice QA examples with minimal-change pairs—visually similar videos with opposing answers—to eliminate shortcuts.

Result: Human performance is 92.9%, while the best model achieves 40.2% (random: 25%), showing MVP's effectiveness.

Conclusion: MVP successfully mitigates shortcuts, providing a robust benchmark for evaluating video-language models' physical understanding.

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


### [162] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Main category: cs.CV

TL;DR: EditInspector is a benchmark for evaluating text-guided image edits, revealing current models' shortcomings and proposing improved methods.


<details>
  <summary>Details</summary>
Motivation: The rise of text-guided image editing necessitates a framework to verify and assess edit quality.

Method: EditInspector uses human annotations to evaluate SoTA models across multiple dimensions and introduces two novel methods.

Result: Current models perform poorly in comprehensive edit evaluation and hallucinate; the proposed methods outperform them.

Conclusion: EditInspector addresses gaps in edit evaluation, with new methods showing superior performance.

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [163] [Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes](https://arxiv.org/abs/2506.09989)
*Yiming Dou,Wonseok Oh,Yuqing Luo,Antonio Loquercio,Andrew Owens*

Main category: cs.CV

TL;DR: A method to predict sounds of human hands interacting with 3D scenes using rectified flow models trained on action-sound pairs.


<details>
  <summary>Details</summary>
Motivation: To enable interactive 3D scene reconstructions by predicting sounds of physical interactions.

Method: Record videos of hand manipulations, train a rectified flow model on action-sound pairs, and query the model for sound predictions based on hand poses.

Result: Generated sounds accurately reflect material properties and actions, often indistinguishable from real sounds.

Conclusion: The approach successfully predicts realistic sounds for hand interactions in 3D scenes.

Abstract: We study the problem of making 3D scene reconstructions interactive by asking
the following question: can we predict the sounds of human hands physically
interacting with a scene? First, we record a video of a human manipulating
objects within a 3D scene using their hands. We then use these action-sound
pairs to train a rectified flow model to map 3D hand trajectories to their
corresponding audio. At test time, a user can query the model for other
actions, parameterized as sequences of hand poses, to estimate their
corresponding sounds. In our experiments, we find that our generated sounds
accurately convey material properties and actions, and that they are often
indistinguishable to human observers from real sounds. Project page:
https://www.yimingdou.com/hearing_hands/

</details>


### [164] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces TAIR, a task for restoring images with textual fidelity, and proposes TeReDiff, a multi-task diffusion framework, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based restoration methods struggle with accurately reconstructing text in degraded images, often producing incorrect text-like patterns.

Method: The authors propose TeReDiff, a multi-task diffusion framework integrating diffusion models with text-spotting, leveraging joint training and text prompts.

Result: TeReDiff outperforms state-of-the-art methods, significantly improving text recognition accuracy.

Conclusion: The work advances image restoration by addressing text-image hallucination and introduces a large-scale benchmark (SA-Text) for future research.

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [165] [PlayerOne: Egocentric World Simulator](https://arxiv.org/abs/2506.09995)
*Yuanpeng Tu,Hao Luo,Xi Chen,Xiang Bai,Fan Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: PlayerOne is the first egocentric realistic world simulator, enabling immersive exploration and dynamic environment modeling from user-provided images, with precise motion alignment and part-level control.


<details>
  <summary>Details</summary>
Motivation: To create a realistic egocentric world simulator for immersive exploration and dynamic environment modeling, addressing the lack of such tools in the community.

Method: Uses a coarse-to-fine training pipeline: pretraining on text-video pairs for coarse understanding, followed by fine-tuning on motion-video data. Includes part-disentangled motion injection and a joint reconstruction framework for scene consistency.

Result: Demonstrates strong generalization in controlling human movements and modeling diverse scenarios, ensuring world consistency in long-form video generation.

Conclusion: PlayerOne pioneers egocentric real-world simulation, opening new possibilities for world modeling and its applications.

Abstract: We introduce PlayerOne, the first egocentric realistic world simulator,
facilitating immersive and unrestricted exploration within vividly dynamic
environments. Given an egocentric scene image from the user, PlayerOne can
accurately construct the corresponding world and generate egocentric videos
that are strictly aligned with the real scene human motion of the user captured
by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that
first performs pretraining on large-scale egocentric text-video pairs for
coarse-level egocentric understanding, followed by finetuning on synchronous
motion-video data extracted from egocentric-exocentric video datasets with our
automatic construction pipeline. Besides, considering the varying importance of
different components, we design a part-disentangled motion injection scheme,
enabling precise control of part-level movements. In addition, we devise a
joint reconstruction framework that progressively models both the 4D scene and
video frames, ensuring scene consistency in the long-form video generation.
Experimental results demonstrate its great generalization ability in precise
control of varying human movements and worldconsistent modeling of diverse
scenarios. It marks the first endeavor into egocentric real-world simulation
and can pave the way for the community to delve into fresh frontiers of world
modeling and its diverse applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [166] [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
*Yibin Lei,Tao Shen,Andrew Yates*

Main category: cs.IR

TL;DR: ThinkQE improves web search query expansion by promoting exploration and diversity, outperforming prior methods without additional training.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based query expansion methods often lack diversity and comprehensive semantic exploration, limiting their effectiveness.

Method: ThinkQE uses a thinking-based expansion process and corpus-interaction strategy for iterative refinement.

Result: ThinkQE outperforms prior approaches on benchmarks (DL19, DL20, BRIGHT).

Conclusion: ThinkQE addresses limitations of existing methods, enhancing retrieval performance and diversity.

Abstract: Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [167] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: UTGenerator and UTBoost address insufficient test cases in SWE-Bench by generating and augmenting test cases, uncovering erroneous patches and improving rankings.


<details>
  <summary>Details</summary>
Motivation: Manually written test cases in SWE-Bench often fail to catch unresolved issues, leading to incorrect evaluations of code generation agents.

Method: UTGenerator, an LLM-driven tool, analyzes codebases to generate test cases. UTBoost extends this for test case augmentation.

Result: Identified 36 task instances with insufficient tests, uncovered 345 erroneous patches, and improved leaderboard rankings.

Conclusion: UTGenerator and UTBoost enhance SWE-Bench's reliability by addressing test case inadequacies.

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [168] [WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras](https://arxiv.org/abs/2506.09098)
*Yangjie Cui,Boyang Gao,Yiwei Zhang,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: WD-DETR is a wavelet denoising-enhanced transformer network for event cameras, improving detection by filtering noise and achieving real-time performance.


<details>
  <summary>Details</summary>
Motivation: Addressing noise in dense event representations to enhance detection quality and reduce missed detections.

Method: Proposes WD-DETR: wavelet transform for noise filtering, transformer-based object prediction, and Dynamic Reorganization Convolution Block (DRCB) for faster inference.

Result: Outperforms state-of-the-art methods on DSEC, Gen1, and 1Mpx datasets; achieves ~35 FPS on NVIDIA Jetson Orin NX.

Conclusion: WD-DETR is effective for real-time event-based object detection, suitable for robotic systems.

Abstract: Previous studies on event camera sensing have demonstrated certain detection
performance using dense event representations. However, the accumulated noise
in such dense representations has received insufficient attention, which
degrades the representation quality and increases the likelihood of missed
detections. To address this challenge, we propose the Wavelet
Denoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event
cameras. In particular, a dense event representation is presented first, which
enables real-time reconstruction of events as tensors. Then, a wavelet
transform method is designed to filter noise in the event representations. Such
a method is integrated into the backbone for feature extraction. The extracted
features are subsequently fed into a transformer-based network for object
prediction. To further reduce inference time, we incorporate the Dynamic
Reorganization Convolution Block (DRCB) as a fusion module within the hybrid
encoder. The proposed method has been evaluated on three event-based object
detection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that
WD-DETR outperforms tested state-of-the-art methods. Additionally, we implement
our approach on a common onboard computer for robots, the NVIDIA Jetson Orin
NX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,
which is exceptionally well-suited for real-time perception of onboard robotic
systems.

</details>


### [169] [Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule](https://arxiv.org/abs/2506.09217)
*Boyu Jiang,Liang Shi,Zhengzhi Lin,Loren Stowe,Feng Guo*

Main category: cs.RO

TL;DR: The paper introduces Perception Characteristics Distance (PCD), a new metric for evaluating perception systems in autonomous driving, accounting for uncertainty and distance. It also presents the SensorRainFall dataset for benchmarking under varying conditions.


<details>
  <summary>Details</summary>
Motivation: Current evaluation metrics for perception systems in autonomous driving are static and fail to capture variability due to distance, dynamics, and weather. PCD addresses this gap.

Method: The authors propose PCD, which quantifies reliable detection distance, and use the SensorRainFall dataset (collected under controlled conditions) to analyze detection confidence variance.

Result: Statistical analysis shows PCD captures reliability differences under varying weather, which static metrics miss. The mPCD metric summarizes overall perception characteristics.

Conclusion: PCD offers a robust, distribution-aware evaluation method, enhancing ADS safety. The SensorRainFall dataset and open-source code support further research.

Abstract: The performance of perception systems in autonomous driving systems (ADS) is
strongly influenced by object distance, scene dynamics, and environmental
conditions such as weather. AI-based perception outputs are inherently
stochastic, with variability driven by these external factors, while
traditional evaluation metrics remain static and event-independent, failing to
capture fluctuations in confidence over time. In this work, we introduce the
Perception Characteristics Distance (PCD) -- a novel evaluation metric that
quantifies the farthest distance at which an object can be reliably detected,
incorporating uncertainty in model outputs. To support this, we present the
SensorRainFall dataset, collected on the Virginia Smart Road using a
sensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear
and daylight-rain scenarios, with precise ground-truth distances to the target
objects. Statistical analysis reveals the presence of change points in the
variance of detection confidence score with distance. By averaging the PCD
values across a range of detection quality thresholds and probabilistic
thresholds, we compute the mean PCD (mPCD), which captures the overall
perception characteristics of a system with respect to detection distance.
Applying state-of-the-art perception models shows that mPCD captures meaningful
reliability differences under varying weather conditions -- differences that
static metrics overlook. PCD provides a principled, distribution-aware measure
of perception performance, supporting safer and more robust ADS operation,
while the SensorRainFall dataset offers a valuable benchmark for evaluation.
The SensorRainFall dataset is publicly available at
https://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the
evaluation code is open-sourced at
https://github.com/datadrivenwheels/PCD_Python.

</details>


### [170] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Main category: cs.RO

TL;DR: UAD (Unsupervised Affordance Distillation) distills affordance knowledge from foundation models into a task-conditioned affordance model without manual annotations, enabling generalization in robotic and human activity scenes.


<details>
  <summary>Details</summary>
Motivation: Existing affordance prediction methods rely on manual annotations or predefined tasks, limiting their applicability in unstructured environments.

Method: UAD leverages large vision and vision-language models to automatically annotate a dataset with instruction-affordance pairs, training a lightweight decoder on frozen features.

Result: UAD generalizes well to real-world robotic scenes and human activities, even when trained on simulated data. An imitation learning policy using UAD's affordances shows strong generalization with minimal training.

Conclusion: UAD offers a scalable, annotation-free approach to affordance learning, demonstrating robust generalization and potential for real-world robotic applications.

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [171] [DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects](https://arxiv.org/abs/2506.09491)
*Guanghu Xie,Zhiduo Jiang,Yonglong Zhang,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: DCIRNet improves depth estimation for transparent/reflective objects by fusing RGB and depth data, enhancing grasping success by 44%.


<details>
  <summary>Details</summary>
Motivation: Transparent/reflective objects challenge depth sensors, causing incomplete/incorrect depth data, which hampers vision tasks like object recognition and robotic manipulation.

Method: Proposes DCIRNet, a multimodal depth completion network with feature fusion and multi-stage supervision to refine depth maps.

Result: Achieves 44% higher grasp success rate for transparent/reflective objects and outperforms on public datasets.

Conclusion: DCIRNet effectively addresses depth estimation issues for transparent/reflective objects, showing strong generalization.

Abstract: Transparent and reflective objects in everyday environments pose significant
challenges for depth sensors due to their unique visual properties, such as
specular reflections and light transmission. These characteristics often lead
to incomplete or inaccurate depth estimation, which severely impacts downstream
geometry-based vision tasks, including object recognition, scene
reconstruction, and robotic manipulation. To address the issue of missing depth
information in transparent and reflective objects, we propose DCIRNet, a novel
multimodal depth completion network that effectively integrates RGB images and
depth maps to enhance depth estimation quality. Our approach incorporates an
innovative multimodal feature fusion module designed to extract complementary
information between RGB images and incomplete depth maps. Furthermore, we
introduce a multi-stage supervision and depth refinement strategy that
progressively improves depth completion and effectively mitigates the issue of
blurred object boundaries. We integrate our depth completion model into
dexterous grasping frameworks and achieve a $44\%$ improvement in the grasp
success rate for transparent and reflective objects. We conduct extensive
experiments on public datasets, where DCIRNet demonstrates superior
performance. The experimental results validate the effectiveness of our
approach and confirm its strong generalization capability across various
transparent and reflective objects.

</details>


### [172] [Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments](https://arxiv.org/abs/2506.09552)
*Fatemeh Mohammadi Amin,Darwin G. Caldwell,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: The paper introduces a dual-stream network (FUSION) combining DGCNN and CNN for Sim2Real domain adaptation in 3D point cloud semantic segmentation, achieving 97.76% accuracy for HRC applications.


<details>
  <summary>Details</summary>
Motivation: Robust 3D environment interpretation is critical for safe and efficient human-robot collaboration (HRC), but real-world annotated data is scarce. Sim2Real adaptation addresses this gap.

Method: Proposes FUSION, a dual-stream network with DGCNN and CNN (augmented with residual layers) for Sim2Real domain adaptation in industrial HRC environments.

Result: Achieves 97.76% segmentation accuracy and superior robustness on real-world and simulated industrial point clouds.

Conclusion: The FUSION model effectively bridges the Sim2Real gap, enhancing semantic segmentation for safer and more efficient HRC.

Abstract: The robust interpretation of 3D environments is crucial for human-robot
collaboration (HRC) applications, where safety and operational efficiency are
paramount. Semantic segmentation plays a key role in this context by enabling a
precise and detailed understanding of the environment. Considering the intense
data hunger for real-world industrial annotated data essential for effective
semantic segmentation, this paper introduces a pioneering approach in the
Sim2Real domain adaptation for semantic segmentation of 3D point cloud data,
specifically tailored for HRC. Our focus is on developing a network that
robustly transitions from simulated environments to real-world applications,
thereby enhancing its practical utility and impact on a safe HRC.
  In this work, we propose a dual-stream network architecture (FUSION)
combining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional
Neural Networks (CNN) augmented with residual layers as a Sim2Real domain
adaptation algorithm for an industrial environment. The proposed model was
evaluated on real-world HRC setups and simulation industrial point clouds, it
showed increased state-of-the-art performance, achieving a segmentation
accuracy of 97.76%, and superior robustness compared to existing methods.

</details>


### [173] [From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models](https://arxiv.org/abs/2506.09930)
*Irving Fang,Juexiao Zhang,Shengbang Tong,Chen Feng*

Main category: cs.RO

TL;DR: The paper introduces a simulation-based task suite to evaluate Vision-Language-Action (VLA) models, revealing gaps between perceptual understanding and precise motor execution, and releases the suite as a benchmark.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of VLAs are insufficient, lacking language instructions and standardized benchmarks, hindering reproducibility and understanding of VLM contributions.

Method: A unified probing suite of 50 simulation-based tasks across 10 subcategories is introduced to systematically evaluate VLA architectures.

Result: VLM backbones provide robust perception and planning but falter in precise motor execution for out-of-distribution tasks; finetuning can erode generalist reasoning.

Conclusion: The released task suite aims to standardize VLA evaluations and drive research on bridging the perception-to-action gap.

Abstract: One promise that Vision-Language-Action (VLA) models hold over traditional
imitation learning for robotics is to leverage the broad generalization
capabilities of large Vision-Language Models (VLMs) to produce versatile,
"generalist" robot policies. However, current evaluations of VLAs remain
insufficient. Traditional imitation learning benchmarks are unsuitable due to
the lack of language instructions. Emerging benchmarks for VLAs that
incorporate language often come with limited evaluation tasks and do not intend
to investigate how much VLM pretraining truly contributes to the generalization
capabilities of the downstream robotic policy. Meanwhile, much research relies
on real-world robot setups designed in isolation by different institutions,
which creates a barrier for reproducibility and accessibility. To address this
gap, we introduce a unified probing suite of 50 simulation-based tasks across
10 subcategories spanning language instruction, vision, and objects. We
systematically evaluate several state-of-the-art VLA architectures on this
suite to understand their generalization capability. Our results show that
while VLM backbones endow VLAs with robust perceptual understanding and high
level planning, which we refer to as good intentions, this does not reliably
translate into precise motor execution: when faced with out-of-distribution
observations, policies often exhibit coherent intentions, but falter in action
execution. Moreover, finetuning on action data can erode the original VLM's
generalist reasoning abilities. We release our task suite and evaluation code
to serve as a standardized benchmark for future VLAs and to drive research on
closing the perception-to-action gap. More information, including the source
code, can be found at https://ai4ce.github.io/INT-ACT/

</details>


### [174] [Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers](https://arxiv.org/abs/2506.09934)
*Jared Lawson,Rohan Chitale,Nabil Simaan*

Main category: cs.RO

TL;DR: A method using custom radiopaque markers enables accurate shape and pose estimation of microcatheters under biplane fluoroscopy, reducing errors and aiding autonomous navigation.


<details>
  <summary>Details</summary>
Motivation: Current methods for catheter tracking in neurointervention are either mentally taxing for interventionalists or incompatible with microcatheters due to bulky instrumentation.

Method: Equipping catheters with custom radiopaque markers arranged to minimize sensitivity to tracking uncertainty, enabling shape and pose estimation under biplane fluoroscopy.

Result: Achieved shape tracking errors <1mm and roll errors <40 degrees in phantom vasculature for microcatheters <2mm OD.

Conclusion: This approach facilitates autonomous navigation of steerable catheters under biplane imaging, improving precision and reducing reliance on manual reconstruction.

Abstract: Safe navigation of steerable and robotic catheters in the cerebral
vasculature requires awareness of the catheters shape and pose. Currently, a
significant perception burden is placed on interventionalists to mentally
reconstruct and predict catheter motions from biplane fluoroscopy images.
Efforts to track these catheters are limited to planar segmentation or bulky
sensing instrumentation, which are incompatible with microcatheters used in
neurointervention. In this work, a catheter is equipped with custom radiopaque
markers arranged to enable simultaneous shape and pose estimation under biplane
fluoroscopy. A design measure is proposed to guide the arrangement of these
markers to minimize sensitivity to marker tracking uncertainty. This approach
was deployed for microcatheters smaller than 2mm OD navigating phantom
vasculature with shape tracking errors less than 1mm and catheter roll errors
below 40 degrees. This work can enable steerable catheters to autonomously
navigate under biplane imaging.

</details>


### [175] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: Chain-of-Action (CoA) is a visuo-motor policy paradigm using backward reasoning for trajectory generation, achieving state-of-the-art performance in RLBench and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: To improve trajectory generation by enforcing global-to-local action constraints through backward reasoning, unlike conventional forward-prediction methods.

Method: CoA uses Trajectory Autoregressive Modeling with backward reasoning, keyframe actions, and complementary designs like continuous action tokens and dynamic stopping.

Result: CoA achieves top performance in 60 RLBench tasks and 8 real-world manipulation tasks.

Conclusion: CoA's backward reasoning and unified structure enhance spatial generalization while maintaining policy flexibility.

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [176] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Main category: cs.LG

TL;DR: A unified threat model is proposed for comparing jailbreaking attacks on LLMs, using an N-gram language model for LLM-agnostic evaluation. Discrete optimization attacks outperform LLM-based ones, and effective attacks exploit rare bigrams.


<details>
  <summary>Details</summary>
Motivation: To provide a principled comparison of jailbreaking attacks on safety-tuned LLMs, addressing variability in fluency and computational effort.

Method: Develop an N-gram language model on 1T tokens for LLM-agnostic, nonparametric evaluation. Adapt and benchmark popular attacks under this threat model.

Result: Attack success rates are lower than previously reported; discrete optimization attacks outperform LLM-based ones. Effective attacks exploit rare bigrams.

Conclusion: The threat model offers interpretable and comprehensive analysis, revealing that successful attacks rely on infrequent bigrams.

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [177] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: The paper explores the trade-off between memorization and generalization in LLMs, showing that model size influences whether a model excels at memorization or generalization, but not both.


<details>
  <summary>Details</summary>
Motivation: To understand how memorization and generalization interact in LLMs, especially how model capacity affects these learning modes.

Method: Pre-training capacity-limited Transformer models on synthetic tasks (arithmetic extrapolation for generalization, factual recall for memorization), testing models of varying sizes.

Result: Small models generalize but fail to memorize; larger models memorize but fail to generalize. No model excels at both when tasks are combined.

Conclusion: Pre-training inherently favors one learning mode over the other, with implications for designing small language models.

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [178] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: SensorLM is a sensor-language foundation model for interpreting wearable sensor data with natural language, leveraging a hierarchical caption pipeline to create a large dataset and outperforming state-of-the-art methods in various tasks.


<details>
  <summary>Details</summary>
Motivation: Aligning and interpreting sensor data with language is challenging due to the lack of annotated datasets, motivating the development of SensorLM.

Method: A hierarchical caption generation pipeline captures statistical, structural, and semantic information from sensor data, extending multimodal pretraining architectures like CLIP and CoCa.

Result: SensorLM achieves superior performance in zero-shot recognition, few-shot learning, and cross-modal retrieval, with capabilities like scaling behaviors and zero-shot generalization.

Conclusion: SensorLM advances sensor-language understanding, demonstrating strong performance and versatility in real-world applications.

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [179] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: A novel LLM agent framework enhances planning via in-context learning, atomic fact augmentation, and recursive lookahead search, improving adaptability and performance in interactive tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs often need extensive guidance or history for effective performance in complex environments, and existing methods struggle with adaptation and multi-step reasoning without fine-tuning.

Method: The framework uses atomic fact augmentation and recursive lookahead search to dynamically enhance prompts for action proposal, simulation, and state-value estimation, enabling online learning without weight updates.

Result: The agent shows improved performance and adaptability in tasks like TextFrozenLake and ALFWorld, refining behavior with accumulated experience.

Conclusion: The proposed framework effectively leverages in-context learning and experience to enhance LLM agent planning and decision-making in interactive environments.

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [180] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: InstructPro, a protein generative model, uses natural language instructions and ligand formulas to design ligand-binding proteins, outperforming baselines with high docking success rates and low RMSD.


<details>
  <summary>Details</summary>
Motivation: Designing proteins for specific functions (e.g., ligand binding) is vital in biology and chemistry, but AI models face data scarcity. Human-curated text descriptions offer an alternative.

Method: Proposes InstructPro, trained on a dataset (InstructProBench) of 9.5M triples (function description, ligand formula, protein sequence). Two variants (1B and 3B parameters) are developed.

Result: InstructPro-1B achieves 81.52% docking success and 4.026Å RMSD; InstructPro-3B reduces RMSD to 2.527Å, outperforming ProGen2, ESM3, and Pinal.

Conclusion: InstructPro effectively generates ligand-binding proteins from natural language instructions, demonstrating superior performance over existing models.

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [181] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: The paper introduces the Stained Glass Transform to enhance privacy in LLM deployments by transforming word embeddings, balancing privacy and utility.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in shared or multi-tenant AI compute infrastructures where plaintext data exposure is a risk.

Method: Proposes the Stained Glass Transform, a stochastic, sequence-dependent transformation of LLM word embeddings, linked to Gaussian Mixture Models' mutual information theory.

Result: Demonstrates privacy preservation via mutual information estimates and maintains model utility through standard benchmarks.

Conclusion: The Stained Glass Transform effectively balances privacy and utility in LLM deployments.

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [182] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: Athena-PRM is a multimodal process reward model for evaluating reasoning steps, using weak-strong completer consistency for high-quality labels. It achieves top performance with minimal data and improves PRMs via ORM initialization and negative data up-sampling.


<details>
  <summary>Details</summary>
Motivation: High-performance PRMs require costly step-level annotations, and conventional methods like Monte Carlo estimation are noisy and expensive. Athena-PRM aims to efficiently generate reliable labels.

Method: Leverages prediction consistency between weak and strong completers for reliable labels, uses ORM initialization and up-sampling for negative data to enhance PRMs.

Result: Achieves superior performance with 5,000 samples, improves test time scaling by 10.2 and 7.1 points on benchmarks, and sets SoTA in VisualProcessBench with a 3.9 F1-score gain.

Conclusion: Athena-PRM is highly effective for evaluating reasoning steps, outperforming baselines and setting new benchmarks, proving its robustness and efficiency.

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [183] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: The paper introduces Verbalized Rejection Sampling (VRS) to improve LLMs' sampling accuracy for Bernoulli distributions, reducing bias without modifying model internals.


<details>
  <summary>Details</summary>
Motivation: LLMs can describe probability distributions but struggle to generate accurate samples, limiting their use in stochastic tasks.

Method: VRS adapts classical rejection sampling to natural language, prompting LLMs to reason about and accept/reject samples.

Result: VRS reduces sampling bias across models and outperforms direct sampling under mild assumptions.

Conclusion: VRS demonstrates how classical probabilistic tools can enhance LLM reliability without heavy engineering.

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


### [184] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Main category: cs.LG

TL;DR: MultiNet is an open-source benchmark for evaluating multimodal action models (VLMs/VLAs) with standardized protocols, a large dataset, and tools for research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous evaluation tools for general-purpose agentic systems combining vision, language, and action.

Method: Introduces MultiNet, a benchmark with standardized evaluation protocols, open-source software, and a composite dataset (1.3T tokens) for diverse tasks.

Result: Provides a framework for assessing VLMs/VLAs and supports downstream research on model generalization limitations.

Conclusion: MultiNet advances multimodal action model evaluation and fosters further research in the field.

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [185] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Main category: cs.LG

TL;DR: LPO introduces a novel method for optimizing GUI interactions using locational data and entropy, outperforming existing SFT and reinforcement learning approaches.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents struggle with accurate positional perception, limiting interaction precision. Existing methods like SFT and reinforcement learning are inadequate.

Method: LPO uses information entropy to predict interaction zones and a dynamic location reward function. It is supported by GRPO for GUI exploration.

Result: LPO achieves state-of-the-art performance in offline benchmarks and real-world evaluations.

Conclusion: LPO significantly enhances GUI interaction precision and outperforms existing methods, with code to be released publicly.

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [186] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Main category: cs.LG

TL;DR: FedVLMBench is the first benchmark for federated fine-tuning of Vision-Language Models (VLMs), evaluating architectures, strategies, and FL algorithms across diverse datasets and tasks.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in VLM deployment, especially in domains like healthcare, by introducing federated learning (FL) and providing a standardized evaluation framework.

Method: FedVLMBench integrates two VLM architectures, four fine-tuning strategies, five FL algorithms, and six datasets, covering single-task and multitask scenarios.

Result: Optimal configuration for encoder-based VLMs in FL is a 2-layer MLP connector with concurrent tuning. FL methods are more sensitive to data heterogeneity in vision-centric tasks.

Conclusion: FedVLMBench offers tools, datasets, and insights to advance privacy-preserving federated training of multimodal models, setting a standardized research platform.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [187] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: AtmosMJ, a deep convolutional network, achieves stable long-range weather forecasts on standard latitude-longitude grids without spherical remapping, challenging the need for non-standard data representations.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the assumption that non-standard spatial domains are necessary for stable long-range weather forecasting, aiming to prove comparable performance on standard grids.

Method: Introduces AtmosMJ with a Gated Residual Fusion (GRF) mechanism to prevent error accumulation, operating directly on ERA5 data without spherical remapping.

Result: AtmosMJ produces stable forecasts for ~500 days, matches 10-day accuracy of leading models, and requires only 5.7 days of V100 GPU training.

Conclusion: Efficient architectural design, not non-standard data representation, is key to stable, computationally efficient long-range weather prediction.

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [188] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: The paper introduces CLAReps, latent codes for CDMs that preserve class-defining features while discarding irrelevant context, enabling interpretable and robust representation learning via the CaDistill method.


<details>
  <summary>Details</summary>
Motivation: CDMs entangle class-defining features with irrelevant context, making it hard to extract robust and interpretable representations.

Method: Proposes CLAReps to distill core class semantics and develops CaDistill, a diffusion-based feature-distillation paradigm using CLAReps as a compact teacher signal.

Result: The student model achieves strong adversarial robustness and generalization, focusing on class signals rather than spurious cues.

Conclusion: CDMs can serve as compact, interpretable teachers for robust representation learning, not just as image generators.

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [189] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Main category: quant-ph

TL;DR: A hybrid quantum-classical model for Devanagari handwritten digit recognition achieves 99.80% accuracy, outperforming classical CNNs with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Handwritten digit recognition in regional scripts like Devanagari is vital for multilingual digitization and cultural preservation, but complex structures and limited datasets challenge conventional models.

Method: The paper introduces a hybrid architecture combining a CNN for spatial feature extraction with a 10-qubit variational quantum circuit (VQC) for quantum-enhanced classification.

Result: The model achieves 99.80% test accuracy, 0.2893 test loss, and 0.9980 F1-score, surpassing classical CNNs in accuracy and robustness.

Conclusion: This work sets a new benchmark for regional script recognition, showcasing quantum machine learning's potential in low-resource language applications.

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [190] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Main category: eess.IV

TL;DR: AI-powered assistive technology for ASD diagnosis using transfer learning and eye gaze variables, improving efficiency and privacy.


<details>
  <summary>Details</summary>
Motivation: Address the time-intensive and costly nature of current ASD diagnostic methods.

Method: Integrates transfer learning with image transforms from eye gaze variables for diagnosis.

Result: Enables in-home diagnosis, reduces stress, and ensures privacy.

Conclusion: The proposed method offers timely, accessible ASD diagnosis while improving outcomes and privacy.

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [191] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/abs/2506.09095)
*Vivien van Veldhuizen,Vanessa Botha,Chunyao Lu,Melis Erdal Cesur,Kevin Groot Lipman,Edwin D. de Jong,Hugo Horlings,Clárisa Sanchez,Cees Snoek,Ritse Mann,Eric Marcus,Jonas Teuwen*

Main category: eess.IV

TL;DR: Foundation models (FMs) revolutionize medical image analysis by leveraging unlabeled data for pre-training, enabling adaptation to clinical tasks with minimal supervision. This review covers their development, applications, and challenges across pathology, radiology, and ophthalmology.


<details>
  <summary>Details</summary>
Motivation: To explore how FMs transform medical image analysis by reducing reliance on manual annotations and improving adaptability across clinical tasks.

Method: Review of over 150 studies, focusing on FM pipelines, architectures, self-supervised learning, and downstream adaptation strategies.

Result: FMs show promise in pathology, radiology, and ophthalmology, with varied design choices and applications.

Conclusion: While FMs offer significant potential, challenges remain, highlighting the need for future research to address open questions.

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [192] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Main category: eess.IV

TL;DR: LoREIN is an unsupervised, dual-prior-integrated framework for accelerated 3D multi-parametric qMRI reconstruction, combining low-rank and continuity priors for high-fidelity results.


<details>
  <summary>Details</summary>
Motivation: Current qMRI reconstruction methods relying on single priors or physics-informed models yield suboptimal results due to the ill-posed nature of the problem.

Method: LoREIN integrates low-rank prior (via LRR) and continuity prior (via INR) to enhance reconstruction fidelity, leveraging INR's continuous representation for optimal spatial bases.

Result: The framework achieves high-fidelity reconstruction of weighted images and quantitative parameter maps, aided by multi-contrast guidance.

Conclusion: LoREIN advances medical imaging with a zero-shot learning paradigm, promising broader applications in complex image reconstruction tasks.

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


### [193] [An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation](https://arxiv.org/abs/2506.09161)
*Rajan Das Gupta,Md Imrul Hasan Showmick,Mushfiqur Rahman Abir,Shanjida Akter,Md. Yeasin Rahat,Md. Jakir Hossen*

Main category: eess.IV

TL;DR: A deep learning system using MobileNet V2 and ResNet-50 detects brain tumors and strokes from MRI images with high accuracy, offering potential for clinical use.


<details>
  <summary>Details</summary>
Motivation: Early and accurate detection of brain abnormalities like tumors and strokes is crucial for timely intervention and better patient outcomes.

Method: Used convolutional neural networks (MobileNet V2 and ResNet-50) with transfer learning to classify MRI scans into five categories. Applied dropout and data augmentation to prevent overfitting.

Result: Achieved 93% training accuracy and 88% validation accuracy. ResNet-50 performed slightly better, but MobileNet V2 is suitable for low-resource settings.

Conclusion: The system provides a practical AI solution for early brain abnormality detection, with potential for clinical deployment and future improvements.

Abstract: Early and accurate detection of brain abnormalities, such as tumors and
strokes, is essential for timely intervention and improved patient outcomes. In
this study, we present a deep learning-based system capable of identifying both
brain tumors and strokes from MRI images, along with their respective stages.
We have executed two groundbreaking strategies involving convolutional neural
networks, MobileNet V2 and ResNet-50-optimized through transfer learning to
classify MRI scans into five diagnostic categories. Our dataset, aggregated and
augmented from various publicly available MRI sources, was carefully curated to
ensure class balance and image diversity. To enhance model generalization and
prevent overfitting, we applied dropout layers and extensive data augmentation.
The models achieved strong performance, with training accuracy reaching 93\%
and validation accuracy up to 88\%. While ResNet-50 demonstrated slightly
better results, Mobile Net V2 remains a promising option for real-time
diagnosis in low resource settings due to its lightweight architecture. This
research offers a practical AI-driven solution for early brain abnormality
detection, with potential for clinical deployment and future enhancement
through larger datasets and multi modal inputs.

</details>


### [194] [The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset](https://arxiv.org/abs/2506.09162)
*Tyler J. Richards,Adam E. Flanders,Errol Colak,Luciano M. Prevedello,Robyn L. Ball,Felipe Kitamura,John Mongan,Maryam Vazirabad,Hui-Ming Lin,Anne Kendell,Thanat Kanthawang,Salita Angkurawaranon,Emre Altinmakas,Hakan Dogan,Paulo Eduardo de Aguiar Kuriki,Arjuna Somasundaram,Christopher Ruston,Deniz Bulja,Naida Spahovic,Jennifer Sommer,Sirui Jiang,Eduardo Moreno Judice de Mattos Farina,Eduardo Caminha Nunes,Michael Brassil,Megan McNamara,Johanna Ortiz,Jacob Peoples,Vinson L. Uytana,Anthony Kam,Venkata N. S. Dola,Daniel Murphy,David Vu,Dataset Contributor Group,Dataset Annotator Group,Competition Data Notebook Group,Jason F. Talbott*

Main category: eess.IV

TL;DR: The RSNA LumbarDISC dataset is the largest public MRI lumbar spine dataset, annotated for degenerative changes, aimed at advancing machine learning research for improved patient care.


<details>
  <summary>Details</summary>
Motivation: To facilitate research in machine learning for lumbar spine imaging, enhancing clinical efficiency and patient care.

Method: Dataset includes 2,697 patients with 8,593 MRI series, annotated by expert radiologists for degenerative changes.

Result: A publicly available dataset for non-commercial use, supporting deep learning model development for spine degeneration grading.

Conclusion: The dataset serves as a valuable resource for advancing research in lumbar spine imaging and AI applications.

Abstract: The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging
Spine Classification (LumbarDISC) dataset is the largest publicly available
dataset of adult MRI lumbar spine examinations annotated for degenerative
changes. The dataset includes 2,697 patients with a total of 8,593 image series
from 8 institutions across 6 countries and 5 continents. The dataset is
available for free for non-commercial use via Kaggle and RSNA Medical Imaging
Resource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine
Degenerative Classification competition where competitors developed deep
learning models to grade degenerative changes in the lumbar spine. The degree
of spinal canal, subarticular recess, and neural foraminal stenosis was graded
at each intervertebral disc level in the lumbar spine. The images were
annotated by expert volunteer neuroradiologists and musculoskeletal
radiologists from the RSNA, American Society of Neuroradiology, and the
American Society of Spine Radiology. This dataset aims to facilitate research
and development in machine learning and lumbar spine imaging to lead to
improved patient care and clinical efficiency.

</details>


### [195] [A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma](https://arxiv.org/abs/2506.09661)
*Garima Jain,Sanghamitra Pati,Mona Duggal,Amit Sethi,Abhijeet Patil,Gururaj Malekar,Nilesh Kowe,Jitender Kumar,Jatin Kashyap,Divyajeet Rout,Deepali,Hitesh,Nishi Halduniya,Sharat Kumar,Heena Tabassum,Rupinder Singh Dhaliwal,Sucheta Devi Khuraijam,Sushma Khuraijam,Sharmila Laishram,Simmi Kharb,Sunita Singh,K. Swaminadtan,Ranjana Solanki,Deepika Hemranjani,Shashank Nath Singh,Uma Handa,Manveen Kaur,Surinder Singhal,Shivani Kalhan,Rakesh Kumar Gupta,Ravi. S,D. Pavithra,Sunil Kumar Mahto,Arvind Kumar,Deepali Tirkey,Saurav Banerjee,L. Sreelakshmi*

Main category: eess.IV

TL;DR: A large, multicenter oral cytology dataset is introduced to improve AI-driven early detection of oral squamous cell carcinoma (OSCC), addressing challenges in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Early detection of OSCC improves survival rates, but traditional histopathology is invasive and resource-intensive. AI can enhance minimally invasive oral cytology methods.

Method: A labeled, multi-source dataset of oral cytology slides (PAP and MGG stains) from ten Indian medical centers was created for AI training.

Result: The dataset aims to advance AI models for automated OSCC detection, reducing diagnostic errors and improving accessibility.

Conclusion: This resource can enhance early OSCC diagnosis in low-resource settings, reducing mortality and improving patient outcomes globally.

Abstract: Oral squamous cell carcinoma OSCC is a major global health burden,
particularly in several regions across Asia, Africa, and South America, where
it accounts for a significant proportion of cancer cases. Early detection
dramatically improves outcomes, with stage I cancers achieving up to 90 percent
survival. However, traditional diagnosis based on histopathology has limited
accessibility in low-resource settings because it is invasive,
resource-intensive, and reliant on expert pathologists. On the other hand, oral
cytology of brush biopsy offers a minimally invasive and lower cost
alternative, provided that the remaining challenges, inter observer variability
and unavailability of expert pathologists can be addressed using artificial
intelligence. Development and validation of robust AI solutions requires access
to large, labeled, and multi-source datasets to train high capacity models that
generalize across domain shifts. We introduce the first large and multicenter
oral cytology dataset, comprising annotated slides stained with
Papanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten
tertiary medical centers in India. The dataset is labeled and annotated by
expert pathologists for cellular anomaly classification and detection, is
designed to advance AI driven diagnostic methods. By filling the gap in
publicly available oral cytology datasets, this resource aims to enhance
automated detection, reduce diagnostic errors, and improve early OSCC diagnosis
in resource-constrained settings, ultimately contributing to reduced mortality
and better patient outcomes worldwide.

</details>


### [196] [Sampling Theory for Super-Resolution with Implicit Neural Representations](https://arxiv.org/abs/2506.09949)
*Mahrokh Najaf,Gregory Ongie*

Main category: eess.IV

TL;DR: The paper explores the sample complexity of estimating images using implicit neural representations (INRs) in linear inverse problems, focusing on recovery from low-pass Fourier samples.


<details>
  <summary>Details</summary>
Motivation: To understand the sampling requirements for exact recovery of images represented by INRs, addressing a gap in knowledge compared to traditional pixel representations.

Method: The study uses a single hidden-layer INR with ReLU activation and Fourier features, employing generalized weight decay regularization. It connects non-convex optimization to convex penalties in infinite-dimensional spaces.

Result: Identifies sufficient Fourier samples for exact INR recovery and validates the theory empirically, including super-resolution of phantom images.

Conclusion: INRs can achieve exact image recovery under certain conditions, demonstrating their potential for solving inverse problems in computer vision and imaging.

Abstract: Implicit neural representations (INRs) have emerged as a powerful tool for
solving inverse problems in computer vision and computational imaging. INRs
represent images as continuous domain functions realized by a neural network
taking spatial coordinates as inputs. However, unlike traditional pixel
representations, little is known about the sample complexity of estimating
images using INRs in the context of linear inverse problems. Towards this end,
we study the sampling requirements for recovery of a continuous domain image
from its low-pass Fourier samples by fitting a single hidden-layer INR with
ReLU activation and a Fourier features layer using a generalized form of weight
decay regularization. Our key insight is to relate minimizers of this
non-convex parameter space optimization problem to minimizers of a convex
penalty defined over an infinite-dimensional space of measures. We identify a
sufficient number of Fourier samples for which an image realized by an INR is
exactly recoverable by solving the INR training problem. To validate our
theory, we empirically assess the probability of achieving exact recovery of
images realized by low-width single hidden-layer INRs, and illustrate the
performance of INRs on super-resolution recovery of continuous domain phantom
images.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [197] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Main category: cs.SD

TL;DR: A scalable method for synthesizing classroom noise using game engines is introduced, creating the SimClass dataset for robust speech model development.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale classroom speech data and dedicated noise corpora limits AI-driven educational speech models.

Method: Synthesize classroom noise with game engines and pair public children's speech with YouTube lectures to simulate classroom interactions.

Result: SimClass closely approximates real classroom speech, aiding robust speech recognition and enhancement models.

Conclusion: SimClass is a valuable resource for advancing AI-driven educational speech technologies.

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [198] [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
*Yui Sudo,Yusuke Fujita,Atsushi Kojima,Tomoya Mizumoto,Lianbo Liu*

Main category: cs.SD

TL;DR: The paper integrates contextual biasing (CB) with OWSM v3.1 to improve recognition of rare words, achieving better performance without retraining the SFM.


<details>
  <summary>Details</summary>
Motivation: SFMs like OWSM struggle with rare/unseen words, and existing CB methods lack pre-trained knowledge, leading to lower performance.

Method: Freezes OWSM v3.1's pre-trained parameters and integrates a CB method to leverage SFM knowledge for effective biasing.

Result: Improves B-WER by 11.6 points, overall WER by 0.9 points, and reduces real-time factor by 7.5%.

Conclusion: The proposed method enhances rare word recognition while preserving SFM advantages, even with limited data.

Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models
(OWSM), are trained on massive datasets to achieve accurate automatic speech
recognition. However, even SFMs struggle to accurately recognize rare and
unseen words. While contextual biasing (CB) is a promising approach to improve
recognition of such words, most CB methods are trained from scratch, resulting
in lower performance than SFMs due to the lack of pre-trained knowledge. This
paper integrates an existing CB method with OWSM v3.1 while freezing its
pre-trained parameters. By leveraging the knowledge embedded in SFMs, the
proposed method enables effective CB while preserving the advantages of SFMs,
even with a small dataset. Experimental results show that the proposed method
improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9
point improvement in the overall WER while reducing the real-time factor by
7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean
set.

</details>


### [199] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Main category: cs.SD

TL;DR: Factorized MKL-VC improves cross-lingual voice conversion with just 5 seconds of reference audio by replacing kNN regression with a factorized optimal transport map, outperforming kNN-VC and matching FACodec.


<details>
  <summary>Details</summary>
Motivation: To enhance cross-lingual voice conversion quality and robustness with minimal reference audio, addressing limitations of the original kNN-VC pipeline.

Method: Replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear solution, to handle non-uniform variance.

Result: Significantly improves content preservation and robustness, especially with short reference audio, outperforming kNN-VC and matching FACodec in cross-lingual tasks.

Conclusion: Factorized MKL-VC is a training-free, efficient solution for high-quality any-to-any cross-lingual voice conversion with minimal reference audio.

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [200] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni is a unified multimodal model for processing images, text, audio, and video, excelling in speech and image generation without needing separate models or task-specific adjustments.


<details>
  <summary>Details</summary>
Motivation: To create a single model capable of handling diverse multimodal tasks efficiently, eliminating the need for multiple specialized models.

Method: Uses dedicated encoders and a MoE architecture (Ling) with modality-specific routers to process and fuse multimodal inputs. Includes advanced audio and image decoders for generation tasks.

Result: Demonstrates strong performance in unified perception and generation across modalities, matching GPT-4o in modality support.

Conclusion: Ming-Omni is a powerful, open-source solution for multimodal tasks, encouraging further research and development.

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [201] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: The paper critiques fully autonomous AI agents, advocating for LLM-based Human-Agent Systems (LLM-HAS) where AI collaborates with humans for better reliability and adaptability.


<details>
  <summary>Details</summary>
Motivation: Questioning the reliability and transparency of autonomous AI systems, the paper promotes human-AI collaboration to address these issues.

Method: Proposes LLM-HAS, where humans guide AI, with examples from healthcare, finance, and software development.

Result: Demonstrates that human-AI teamwork outperforms standalone AI in complex tasks.

Conclusion: AI progress should focus on enhancing human capabilities through collaboration, not autonomy.

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [202] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: IFG (Intent Factored Generation) improves LLM sample diversity by splitting generation into intent sampling and response generation, enhancing reasoning and conversational tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for diversity in LLM outputs are token-level, leading to repetitive or unengaging results, especially in reasoning and conversational tasks.

Method: IFG factorizes sampling into two stages: sampling a semantically dense intent (e.g., summary/keywords) and generating the final response conditioned on the prompt and intent. Higher temperature for intent promotes diversity, while lower temperature ensures coherence.

Result: IFG improves pass@k and RL from Verifier Feedback on math/code tasks, enhances conversational diversity without sacrificing reward, and maintains quality in general language modeling.

Conclusion: IFG is a simple, effective method to increase LLM sample diversity while maintaining performance, easily integrable into existing algorithms.

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [203] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Main category: cs.AI

TL;DR: The paper presents V-JEPA 2, a self-supervised model trained on internet-scale video data and minimal robot interaction data, achieving state-of-the-art performance in motion understanding, action anticipation, and video QA tasks, and enabling robotic planning without task-specific training.


<details>
  <summary>Details</summary>
Motivation: To develop AI models that understand and act in the physical world by leveraging self-supervised learning from large-scale video data and limited interaction data.

Method: Pre-train V-JEPA 2 on 1M+ hours of video data, align it with a language model, and post-train V-JEPA 2-AC on 62 hours of robot videos for robotic planning.

Result: V-JEPA 2 excels in motion understanding (77.3 top-1 accuracy) and action anticipation (39.7 recall-at-5). V-JEPA 2-AC enables zero-shot robotic planning for object manipulation.

Conclusion: Self-supervised learning from web-scale data and minimal robot interaction can create world models capable of physical-world planning without task-specific training.

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [204] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: A simple Transformer-based framework for motion in-betweening challenges the need for complex models, emphasizing data modeling choices like pose representation and velocity features.


<details>
  <summary>Details</summary>
Motivation: To simplify motion in-betweening by reducing reliance on complex models and highlighting the importance of data modeling.

Method: Uses a single Transformer encoder, focusing on data volume, pose representation, and velocity input features.

Result: Demonstrates that data choices (e.g., pose representation, velocity features) significantly improve motion transitions, rivaling complex models.

Conclusion: Model complexity isn't the sole factor for quality; a data-centric approach can yield superior motion interpolation.

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [205] [VideoMat: Extracting PBR Materials from Video Diffusion Models](https://arxiv.org/abs/2506.09665)
*Jacob Munkberg,Zian Wang,Ruofan Liang,Tianchang Shen,Jon Hasselgren*

Main category: cs.GR

TL;DR: The paper presents a method to generate high-quality materials for 3D models using video diffusion models, intrinsic decomposition, and differentiable rendering, conditioned on text prompts or single images.


<details>
  <summary>Details</summary>
Motivation: To create realistic and coherent materials for 3D models efficiently, leveraging advances in diffusion models and differentiable rendering.

Method: 1. Use a finetuned video diffusion model conditioned on geometry and lighting to generate multi-view videos. 2. Extract intrinsic properties (color, roughness, metallic) from the video. 3. Apply differentiable path tracing to derive PBR materials.

Result: High-quality, physically-based materials compatible with standard content creation tools.

Conclusion: The approach effectively bridges generative models and 3D content creation, enabling realistic material synthesis from minimal inputs.

Abstract: We leverage finetuned video diffusion models, intrinsic decomposition of
videos, and physically-based differentiable rendering to generate high quality
materials for 3D models given a text prompt or a single image. We condition a
video diffusion model to respect the input geometry and lighting condition.
This model produces multiple views of a given 3D model with coherent material
properties. Secondly, we use a recent model to extract intrinsics (base color,
roughness, metallic) from the generated video. Finally, we use the intrinsics
alongside the generated video in a differentiable path tracer to robustly
extract PBR materials directly compatible with common content creation tools.

</details>


### [206] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: DGS-LRM is a feed-forward method for predicting deformable 3D Gaussian splats from monocular videos, excelling in dynamic scene reconstruction and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward models struggle with dynamic scenes due to data scarcity and representation challenges. DGS-LRM aims to address these gaps.

Method: Uses a synthetic dataset, deformable 3D Gaussian representation, and a transformer network for real-time, generalizable reconstruction.

Result: Matches optimization-based methods in quality and surpasses predictive dynamic reconstruction methods in real-world performance.

Conclusion: DGS-LRM is a robust solution for dynamic scene reconstruction, with accurate 3D deformation and tracking capabilities.

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [207] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Main category: cs.CR

TL;DR: The paper introduces Dynamic Contextual Perturbation (DCP), a novel adversarial attack method for NLP models that dynamically generates context-aware perturbations, ensuring semantic fidelity and fluency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attack methods on NLP models focus on word-level or local text alterations, often producing detectable or semantically inconsistent perturbations, highlighting the need for a more context-aware approach.

Method: DCP leverages pre-trained language models to iteratively refine perturbations across sentences, paragraphs, and documents, balancing misclassification and text naturalness through an adversarial objective function.

Result: Experiments on various NLP models and datasets show DCP's effectiveness in challenging state-of-the-art systems, producing subtle and impactful adversarial examples.

Conclusion: DCP underscores the importance of context in adversarial attacks and advances the development of robust NLP systems resistant to sophisticated adversarial strategies.

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


### [208] [DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt](https://arxiv.org/abs/2506.09353)
*Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.CR

TL;DR: DAVSP introduces Visual Safety Prompt and Deep Alignment to enhance LVLMs' resistance to malicious queries while maintaining utility on benign inputs.


<details>
  <summary>Details</summary>
Motivation: Existing alignment approaches fail to effectively resist malicious queries without compromising benign input utility.

Method: Proposes Visual Safety Prompt (trainable padding around images) and Deep Alignment (training via activation space supervision).

Result: DAVSP effectively resists malicious queries and preserves utility, with strong cross-model generalization.

Conclusion: DAVSP's innovations are essential for robust LVLM alignment, demonstrated across benchmarks.

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress across
various applications but remain vulnerable to malicious queries that exploit
the visual modality. Existing alignment approaches typically fail to resist
malicious queries while preserving utility on benign ones effectively. To
address these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),
which is built upon two key innovations. First, we introduce the Visual Safety
Prompt, which appends a trainable padding region around the input image. It
preserves visual features and expands the optimization space. Second, we
propose Deep Alignment, a novel approach to train the visual safety prompt
through supervision in the model's activation space. It enhances the inherent
ability of LVLMs to perceive malicious queries, achieving deeper alignment than
prior works. Extensive experiments across five benchmarks on two representative
LVLMs demonstrate that DAVSP effectively resists malicious queries while
preserving benign input utility. Furthermore, DAVSP exhibits great cross-model
generation ability. Ablation studies further reveal that both the Visual Safety
Prompt and Deep Alignment are essential components, jointly contributing to its
overall effectiveness. The code is publicly available at
https://github.com/zhangyitonggg/DAVSP.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [209] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Main category: q-fin.ST

TL;DR: The study uses LSTM and GBC models to forecast USD/BDT exchange rates, achieving high accuracy and outperforming traditional methods, though GBC results in a net loss.


<details>
  <summary>Details</summary>
Motivation: Accurate forex prediction is crucial for trade, investments, and economic stability.

Method: LSTM neural network and Gradient Boosting Classifier (GBC) are applied to historical USD/BDT data (2018-2023).

Result: LSTM achieves 99.449% accuracy and low RMSE (0.9858), while GBC yields a 40.82% profitable trade rate but a net loss.

Conclusion: Deep learning shows promise for forex forecasting, with potential for improvement via sentiment analysis and real-time data.

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [210] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: The paper addresses the challenge of ensuring task-oriented LLM-based agents adhere to policies while resisting adversarial attacks, proposing CRAFT for red-teaming and tau-break for robustness evaluation.


<details>
  <summary>Details</summary>
Motivation: To protect policy-adherent agents from malicious users exploiting them for personal gain.

Method: Introduces CRAFT, a multi-agent red-teaming system, and tau-break benchmark for evaluating robustness.

Result: CRAFT outperforms conventional jailbreak methods; defenses tested are insufficient.

Conclusion: Stronger safeguards are needed to protect policy-adherent agents from adversarial attacks.

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [211] [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
*Ünal Ege Gaznepoglu,Anna Leschanowsky,Ahmad Aloradi,Prachi Singh,Daniel Tenbrinck,Emanuël A. P. Habets,Nils Peters*

Main category: eess.AS

TL;DR: The paper evaluates speaker anonymization by testing ASV systems using BERT on linguistic content, revealing biases in datasets and questioning global EER for privacy assessments.


<details>
  <summary>Details</summary>
Motivation: To assess the privacy benefits of speaker anonymization systems by examining the impact of intra-speaker linguistic content similarity in ASV attacks.

Method: Adapts BERT as an ASV system to evaluate the VoicePrivacy Attacker Challenge datasets, focusing on textual content.

Result: Achieves a mean EER of 35%, with some speakers at 2%, and identifies biases due to semantically similar keywords in datasets.

Conclusion: Calls for reworking datasets to ensure fair evaluation and challenges the use of global EER for privacy assessments.

Abstract: Speaker anonymization systems hide the identity of speakers while preserving
other information such as linguistic content and emotions. To evaluate their
privacy benefits, attacks in the form of automatic speaker verification (ASV)
systems are employed. In this study, we assess the impact of intra-speaker
linguistic content similarity in the attacker training and evaluation datasets,
by adapting BERT, a language model, as an ASV system. On the VoicePrivacy
Attacker Challenge datasets, our method achieves a mean equal error rate (EER)
of 35%, with certain speakers attaining EERs as low as 2%, based solely on the
textual content of their utterances. Our explainability study reveals that the
system decisions are linked to semantically similar keywords within utterances,
stemming from how LibriSpeech is curated. Our study suggests reworking the
VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge
the reliance on global EER for privacy evaluations.

</details>


### [212] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
*Suhas BN,Andrew M. Sherrill,Jyoti Alaparthi,Dominik Mattioli,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: eess.AS

TL;DR: A method for automatic temporal localization of key PE therapy fidelity elements using audio and transcripts, achieving a mean absolute error of 5.3 seconds.


<details>
  <summary>Details</summary>
Motivation: Manual review of PE therapy sessions for fidelity evaluation is labor-intensive; automation can improve scalability.

Method: Fine-tunes Qwen2-Audio with LoRA on 30-second audio-transcript windows, using LLM-based prompting for fidelity labels.

Result: Best configuration (LoRA rank 8, 30s windows) achieves MAE of 5.3 seconds on 313 PE sessions.

Conclusion: The framework offers scalable fidelity tracking for PE therapy, aiding clinician training and quality assurance.

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


### [213] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: The paper explores regularization methods to improve neural front-ends in ASR systems, addressing overfitting and outperforming traditional feature extraction.


<details>
  <summary>Details</summary>
Motivation: Neural front-ends for ASR often underperform due to overfitting, prompting investigation into better regularization techniques.

Method: The study examines audio perturbation and modifies SpecAugment with STFT-domain masking to enhance regularization.

Result: The proposed methods significantly improve performance, closing the gap between neural and traditional front-ends.

Conclusion: Effective regularization, including STFT-domain masking, enables neural front-ends to match or surpass classical methods.

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [214] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: CryoSPIRE introduces a hierarchical Gaussian mixture model for 3D reconstruction in cryo-EM, handling conformational and compositional variability, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of modeling non-rigid conformational flexibility and compositional variation in cryo-EM images.

Method: A 3D reconstruction framework using a hierarchical Gaussian mixture model, inspired by Gaussian Splatting, with part-based segmentation.

Result: Demonstrates biologically meaningful structures on complex datasets and sets a new benchmark on CryoBench.

Conclusion: CryoSPIRE advances cryo-EM reconstruction by effectively handling variability and improving accuracy.

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>

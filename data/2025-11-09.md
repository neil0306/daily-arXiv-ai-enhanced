<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 63]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 10]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: A novel pipeline for controlling LLM personality traits using Big Five personality framework through hidden state activations and low-rank subspace discovery, enabling precise behavioral steering without affecting model capabilities.


<details>
  <summary>Details</summary>
Motivation: The need for effective mechanisms to control and align LLM personality traits, as current methods lack reliable behavioral manipulation during generation, and the relationship between psychological constructs and LLM representations remains underexplored.

Method: Extracts hidden state activations from transformer layers using Big Five Personality Traits, applies low-rank subspace discovery methods, identifies trait-specific optimal layers across model architectures, and implements a flexible steering framework with dynamic layer selection.

Result: Personality traits occupy a low-rank shared subspace, and these latent structures can be transformed into actionable mechanisms for effective steering through careful perturbations without impacting fluency, variance, and general capabilities.

Conclusion: The approach bridges the gap between psychological theory and practical model alignment, enabling precise control of trait expression in LLM outputs through personality-aligned directions.

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: TextualVerifier is a self-verification framework that addresses the verification gap in TextGrad by using chain-of-thought reasoning and majority voting with LLMs, improving reasoning validity and optimization performance.


<details>
  <summary>Details</summary>
Motivation: TextGrad lacks self-verification mechanisms for ensuring reasoning validity in text-based decision making, creating a need for verification frameworks in text-based optimization systems.

Method: TextualVerifier implements a four-stage workflow: chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. It integrates with TextGrad at loss function and optimization result verification stages using LLM-based techniques.

Result: TextualVerifier improves reasoning step validity by 29% in standalone evaluation. When integrated with TextGrad, it yields 2.2 percentage point improvement (68.2% to 70.4%) with moderate overhead of 5.9 LLM calls. Versioning evaluations show 8.08, 10.71, and 3.92 percentage point improvements on GPQA, MMLU-ML, and MMLU-CP benchmarks respectively.

Conclusion: TextualVerifier presents the first self-verification framework for TextGrad through LLM-based techniques without requiring numerical gradients, enabling more reliable reasoning and opening new directions for verification in text-based optimization.

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: Extended Greek Dialectal Dataset (GRDD+) with 6.4M words covering 10 Greek varieties, used to fine-tune LLMs and compare with frontier models.


<details>
  <summary>Details</summary>
Motivation: To create the first comprehensive Greek dialectal dataset with significant variation and size, enabling evaluation of dialectal data's impact on language models.

Method: Extended existing GRDD dataset with more Cretan, Cypriot, Pontic, and Northern Greek data, plus six new varieties. Fine-tuned three 8B-parameter models (Llama-3-8B, Llama-3.1-8B, Krikri-8B) and compared with frontier models.

Result: Created GRDD+ dataset with 6,374,939 words across 10 Greek varieties - the largest and most varied Greek dialectal dataset to date.

Conclusion: The study demonstrates the value of comprehensive dialectal datasets for improving LLM performance on diverse Greek linguistic varieties.

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: PLLuM is the largest open-source family of Polish language foundation models developed to address the English-centric bias in LLMs, featuring a new 140B token corpus, custom datasets, and Responsible AI framework.


<details>
  <summary>Details</summary>
Motivation: Address the limited support for non-English languages in LLM development and create high-quality, culturally relevant Polish language models beyond the English-centric commercial landscape.

Method: Developed by consortium of Polish research institutions using a new 140B token Polish corpus, 77k custom instructions dataset, 100k preference optimization dataset, and a Responsible AI framework with data governance and safety filtering.

Result: Successfully created the largest open-source family of Polish foundation models with demonstrated utility in public administration downstream tasks.

Conclusion: PLLuM aims to foster open research and strengthen sovereign AI technologies in Poland by providing publicly available, culturally relevant Polish language models.

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: STARS is a decoding-time algorithm that improves LLM alignment through segment-level token sampling and rejection, outperforming fine-tuning methods while being more computationally efficient than Best-of-N sampling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning LLMs with human values are either computationally expensive (fine-tuning) or require impractical computation (Best-of-N sampling), creating a need for more efficient alignment approaches.

Method: STARS uses segment-level token alignment with rejection sampling, iteratively sampling, scoring, and rejecting/accepting short, fixed-size token segments during decoding to enable early correction of generation paths.

Result: STARS outperforms Supervised Fine-Tuning by up to 14.9 percentage points and Direct Preference Optimization by up to 4.3 percentage points on win-rates across six LLMs, while remaining competitive with Best-of-N baselines.

Conclusion: Granular, reward-guided sampling provides a generalizable, robust, and efficient alternative to traditional fine-tuning and full-sequence ranking methods for aligning LLMs with human values.

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [6] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: Efficient multi-label text classification using LLMs by reformulating tasks as sequences of yes/no decisions for each label dimension, with prefix caching for efficiency and distillation to smaller models.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of traditional multi-label classification approaches with LLMs, which often generate all labels in a single structured response, by creating a more scalable and efficient framework.

Method: Reformulate multi-label classification as sequences of dichotomic (yes/no) decisions for each target dimension, use prefix caching for efficiency, apply LLM-to-SLM distillation where a powerful annotator model (DeepSeek-V3) provides multiple annotations per text to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B).

Result: Fine-tuned models show significant improvements over zero-shot baselines, particularly on dimensions seen during training, with substantial efficiency gains for short-text inference without loss of accuracy.

Conclusion: Decomposing multi-label classification into dichotomic queries combined with distillation and cache-aware inference offers a scalable and effective framework for LLM-based classification, applicable across domains beyond the affective text analysis validation.

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [7] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: Analysis of gender representation in MT datasets for low-resourced languages (Afan Oromo, Amharic, Tigrinya) reveals male gender skew and harmful content against women, showing quantity doesn't guarantee quality.


<details>
  <summary>Details</summary>
Motivation: To investigate quality issues in MT datasets for low-resourced languages, particularly gender representation biases and harmful content, as prioritizing quantity over quality risks poor performance and perpetuating societal biases.

Method: Analyzed Machine Translation datasets for three low-resourced languages (Afan Oromo, Amharic, Tigrinya) with focus on gender representation across domains, names, grammatical gender, and stereotypical depictions.

Result: Found large male gender skew in names, verb gender, and stereotypical depictions; training data dominated by political/religious domains while benchmarks focus on news/health/sports; harmful toxic depictions against women, worse for languages with more data.

Conclusion: Quantity doesn't guarantee quality in low-resourced language datasets; work aims to inspire further inquiry and early mitigation of harmful content in such datasets.

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [8] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: GRAD is a decoding-time method that mitigates hallucinations in LLMs by constructing token transition graphs from retrieved corpus evidence and adaptively fusing them with model logits during generation.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination mitigation approaches rely on external knowledge sources through fragile prompting or costly symbolic knowledge integration, creating a need for lightweight, corpus-based grounding methods.

Method: Constructs sparse token transition graphs by accumulating next-token logits across retrieved corpus in single forward pass, then max-normalizes and adaptively fuses graph-retrieved logits with model logits during decoding.

Result: Achieves up to 9.7% higher intrinsic accuracy, 8.6% lower hallucination rates, and 6.9% greater correctness compared to greedy decoding across three models and multiple QA benchmarks.

Conclusion: GRAD provides a lightweight plug-and-play alternative to contrastive decoding and knowledge graph augmentation, demonstrating statistical evidence from corpus token transitions can effectively steer generation toward truthful outputs.

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [9] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: Testing humans vs vision-language models on iterated reference games shows models perform poorly without context but improve dramatically with relevant context, though abstract referents remain challenging.


<details>
  <summary>Details</summary>
Motivation: To test agents' ability for context-sensitive pragmatic reasoning in multi-turn linguistic environments through iterated reference games.

Method: Compared humans and vision-language models on iterated reference game trials, varying context amount, order, and relevance.

Result: Models performed above chance but worse than humans without relevant context. With relevant context, model performance increased dramatically over trials. Abstract referents remained difficult for models.

Conclusion: Few-shot reference games with abstract referents remain a challenging task for machine learning models despite improvements with relevant context.

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [10] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: The paper introduces HFGI, a high-resolution index measuring human flourishing using 2.6 billion geolocated tweets analyzed by fine-tuned LLMs across 48 indicators, validated against established measures.


<details>
  <summary>Details</summary>
Motivation: To quantify multidimensional human flourishing beyond economic indicators with fine spatial and temporal resolution that existing measures lack.

Method: Analyzed 2.6 billion geolocated US tweets (2013-2023) using fine-tuned large language models to classify expressions across 48 indicators aligned with Harvard's Global Flourishing Study framework.

Result: Created monthly/yearly county- and state-level indicators of flourishing-related discourse, validated to accurately represent constructs and show expected correlations with established indicators.

Conclusion: HFGI enables unprecedented resolution for analyzing well-being, inequality, and social change dynamics across the US over the past decade through social media discourse.

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [11] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: LLMs can communicate via semantic vector translations instead of plain tokens, enabling direct cross-model latent communication while preserving computational stability.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent LLM systems pass messages as plain tokens, discarding latent semantics which constrains information transfer and adds computational overhead.

Method: Use learned vector translations between model representation spaces via dual-encoder translators, with conservative vector injection at 30% blending strength.

Result: Achieved 0.538 average cosine alignment between Llama-2-7B and Mistral-7B, with 2.01:1 transfer asymmetry showing general-purpose models yield more transferable representations.

Conclusion: Cross-model latent communication is feasible, enabling collaborative AI systems that share meaning rather than tokens while maintaining computational stability.

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [12] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: A framework integrating abductive inference into RAG systems to handle incomplete evidence by generating and validating missing premises, improving accuracy and faithfulness.


<details>
  <summary>Details</summary>
Motivation: RAG pipelines often fail when retrieved evidence is incomplete, leaving gaps in reasoning that abductive inference can help bridge.

Method: Detects insufficient evidence, generates candidate missing premises, and validates them through consistency and plausibility checks.

Result: Experimental results on abductive reasoning and multi-hop QA benchmarks show improved answer accuracy and reasoning faithfulness.

Conclusion: Abductive inference is a promising direction for enhancing robustness and explainability of RAG systems.

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [13] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: Proposes Weakly Supervised Transducer (WST) for ASR that handles noisy transcripts without needing confidence scores or pre-trained models, maintaining performance with up to 70% transcription errors.


<details>
  <summary>Details</summary>
Motivation: RNN-T models require large-scale high-quality annotated data which is costly and difficult to obtain. Need methods that work with imperfect transcripts.

Method: WST integrates a flexible training graph that robustly handles transcript errors without requiring confidence estimation or auxiliary pre-trained models.

Result: WST maintains performance with up to 70% transcription error rates, outperforming existing CTC-based weakly supervised approaches like BTC and OTC on synthetic and industrial datasets.

Conclusion: WST demonstrates practical utility and robustness in realistic ASR settings, providing an effective solution for training with noisy transcripts.

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [14] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: T-FIX is a benchmark for evaluating LLM explanations' alignment with expert intuition across seven knowledge-intensive domains, addressing limitations of current evaluation schemes.


<details>
  <summary>Details</summary>
Motivation: Current LLM explanation evaluations focus on plausibility and faithfulness but fail to assess whether explanations align with expert-level reasoning in knowledge-intensive domains like medicine and science.

Method: Developed T-FIX benchmark spanning seven knowledge-intensive domains in collaboration with domain experts, creating novel metrics to measure LLM explanation alignment with expert judgment.

Result: The paper introduces formalized expert alignment as an evaluation criterion and presents the T-FIX benchmark with domain-expert-developed metrics.

Conclusion: Expert alignment is a crucial criterion for evaluating LLM explanations in knowledge-intensive settings where users are domain experts requiring explanations that reflect expert reasoning.

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [15] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: PoK framework enhances LLMs for temporal reasoning by decomposing questions into sub-objectives and retrieving temporally aligned facts from TKGs, achieving state-of-the-art performance on TKGQA tasks.


<details>
  <summary>Details</summary>
Motivation: Existing TKGQA methods fail to fully understand complex temporal constraints, while LLMs have strong semantic understanding but limited temporal reasoning ability and suffer from hallucination.

Method: Proposes Plan of Knowledge (PoK) framework with contrastive temporal retriever that decomposes questions into sub-objectives and constructs Temporal Knowledge Store with contrastive retrieval to selectively retrieve temporally aligned facts.

Result: Significantly improves retrieval precision and reasoning accuracy of LLMs, surpassing state-of-the-art TKGQA methods by up to 56.0% on four benchmark datasets.

Conclusion: PoK effectively enhances interpretability and factual consistency of temporal reasoning by combining structured planning with temporal knowledge retrieval.

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [16] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: Comparison of word associations between humans and LLMs shows moderate overlap, with LLMs amplifying emotional load and being more predictable/less creative than humans.


<details>
  <summary>Details</summary>
Motivation: To understand if large language models generate word associations similarly to humans, especially for emotionally loaded words, and explore the creative aspects of associative thinking.

Method: Comparative analysis of associative behavior by examining responses to emotionally loaded word cues from both human participants and large language models.

Result: Moderate overlap between human and LLM associations, but LLMs tend to amplify emotional load of stimuli, produce more predictable responses, and show less creativity than humans.

Conclusion: LLMs exhibit different associative patterns than humans - they amplify emotional content and are less creative, suggesting fundamental differences in how they process and associate concepts compared to human cognition.

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [17] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: Transformer-based model trained on large multimodal radiology datasets achieves state-of-the-art PHI de-identification, outperforming commercial systems with F1 scores up to 0.996.


<details>
  <summary>Details</summary>
Motivation: To enhance automated de-identification of radiology reports by scaling transformer models with extensive training data and benchmarking against commercial cloud systems for PHI detection.

Method: Fine-tuned transformer-based PHI de-identification pipeline on two large annotated radiology corpora from Stanford, introduced AGE category, evaluated on test sets from Stanford and Penn, assessed synthetic PHI generation stability and commercial system performance.

Result: Achieved overall F1 scores of 0.973 (Penn) and 0.996 (Stanford), outperformed previous models, consistent synthetic PHI detectability (F1: 0.959), and outperformed all vendor systems (F1: 0.960 vs. 0.632-0.754).

Conclusion: Large-scale multimodal training improves cross-institutional generalization, transformer-based model trained on diverse radiology datasets outperforms academic and commercial systems, establishing new benchmark for secure clinical text processing.

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [18] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: The paper studies k-list language identification in the limit, where learners can output k guesses per time step. It provides an exact characterization of when language collections can be k-list identified and shows connections to statistical learning rates.


<details>
  <summary>Details</summary>
Motivation: To overcome the classical impossibility results for language identification in the limit by allowing learners to produce multiple guesses (k-list identification), building on recent positive results for language generation problems.

Method: Develops a recursive characterization based on Angluin's original work, showing that k-list identification is equivalent to decomposing the language collection into k subcollections that are each identifiable with single guesses. Also analyzes statistical learning rates for i.i.d. streams.

Result: Exact characterization of k-list identifiable language collections: they can be decomposed into k subcollections each identifiable with single guesses. For identifiable collections, exponential learning rates are achievable and optimal; for non-identifiable collections, no positive learning rate exists.

Conclusion: The k-list identification framework provides a natural extension to classical language identification, with clean characterizations and strong connections to statistical learning theory, showing that identifiability directly determines achievable learning rates.

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [19] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: Batch prompting improves reasoning accuracy and reduces token usage by 3x-5x while regularizing model behavior through suppression of overthinking and hedging language.


<details>
  <summary>Details</summary>
Motivation: To explore batch prompting benefits beyond inference cost amortization, specifically its regularization effects on multi-step reasoning in Large Reasoning Models.

Method: Comprehensive study across 13 diverse benchmarks analyzing batched inference behavior, including detailed behavioral analysis of overthinking, hedging language, and collective effects.

Result: Batching improves accuracy while reducing reasoning token usage by 3x-5x, suppresses overthinking and repetitive self-corrections, and shows emergent collective effects where models generalize patterns across examples.

Conclusion: Batching serves as a powerful inference-time regularizer for more efficient and reliable LLM reasoning, beyond just throughput optimization.

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [20] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: RIDE is an adversarial question-rewriting framework that uses Item Response Theory to generate more challenging mathematical problems, revealing LLMs' limited robustness in mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations may be inflated by training data leakage or superficial pattern matching rather than genuine mathematical reasoning ability, requiring adversarial perturbation-based evaluation.

Method: Proposes RIDE framework that leverages Item Response Theory to measure question difficulty, uses 35 LLMs to simulate students and build a difficulty ranker, then employs reinforcement learning to guide question rewriting across difficulty levels.

Result: RIDE generates perturbed versions of competition-level math problems that degrade advanced LLM performance by an average 21.73% across 26 models, exposing limited robustness in mathematical reasoning.

Conclusion: The framework successfully exposes LLMs' limited robustness in mathematical reasoning and validates the adversarial evaluation approach for measuring true reasoning ability.

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [21] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: CantoASR integrates forced alignment, LoRA-finetuned Whisper, and instruction-tuned Qwen-Audio to improve Cantonese ASR by combining acoustic cues with language model reasoning.


<details>
  <summary>Details</summary>
Motivation: Low-resource Cantonese ASR is challenging due to limited annotated data, six lexical tones, tone sandhi, and accent variation, with existing models like Whisper suffering from high word error rates.

Method: Collaborative ASR-LALM error correction framework using forced alignment for acoustic feature extraction, LoRA-finetuned Whisper for tone discrimination, and instruction-tuned Qwen-Audio for prosody-aware correction.

Result: Substantial character error rate (CER) gains over Whisper-Large-V3 on spontaneous Cantonese data.

Conclusion: Integrating acoustic cues with large audio-language model reasoning provides a scalable strategy for low-resource tonal and dialectal ASR.

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [22] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: The paper explores multi-agent LLM pipelines for Text-to-SQL generation, showing that multi-agent discussion and planner-coder approaches can significantly improve performance of smaller models, with the LLM Reasoner-Coder pipeline achieving the best results.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with SQL generation from natural language due to large schema sizes and complex reasoning, while prior work focuses on complex pipelines using flagship models, overlooking smaller efficient models.

Method: Three multi-agent LLM pipelines: (1) Multi-agent discussion with iterative critique and refinement, (2) Planner-Coder with stepwise SQL generation plans, (3) Coder-Aggregator with multiple coders and reasoning agent selection.

Result: Multi-agent discussion improved small model performance by up to 10.6% (Qwen2.5-7b-Instruct). LLM Reasoner-Coder pipeline achieved best results, boosting Gemma 3 27B IT accuracy from 52.4% to 56.4% using DeepSeek-R1-32B and QwQ-32B planners.

Conclusion: Multi-agent LLM pipelines can effectively enhance Text-to-SQL performance, particularly benefiting smaller models, with the planner-coder approach showing the most promise for practical deployment.

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [23] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: LAAC positions LLMs as communication intermediaries to enable authentic knowledge exchange, but systematic evaluation reveals trust gaps in information fidelity, reproducibility, and response integrity that must be addressed before reliable deployment.


<details>
  <summary>Details</summary>
Motivation: Current AI communication creates a 'theater' where senders inflate content with LLMs and recipients compress it back, preventing authentic engagement. LAAC aims to shift this paradigm by using LLMs as intelligent intermediaries for genuine knowledge exchange.

Method: Systematically evaluates LAAC's trustworthiness across three dimensions: information capture fidelity (accuracy of intent extraction), reproducibility (consistency across interactions), and query response integrity (reliability without hallucination). Uses controlled experiments with multi-agent architecture across multiple use cases.

Result: Preliminary findings reveal measurable trust gaps in all three dimensions that must be addressed before LAAC can be reliably deployed in high-stakes communication scenarios.

Conclusion: While LAAC offers a promising paradigm shift for authentic communication, significant trustworthiness challenges in information fidelity, consistency, and reliability must be resolved before practical deployment in critical communication contexts.

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [24] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: This paper introduces a computational Turing test framework to validate LLMs' ability to simulate human behavior, finding that even calibrated LLM outputs remain distinguishable from human text with a trade-off between human-likeness and semantic fidelity.


<details>
  <summary>Details</summary>
Motivation: Current validation of LLMs for social science simulations relies on unreliable human-judgment-based evaluations, lacking robust tools to assess realism and calibrate models to real-world data.

Method: Developed a computational Turing test framework combining aggregate metrics (BERT-based detectability, semantic similarity) with interpretable linguistic features (stylistic markers, topical patterns), and systematically compared 9 open-weight LLMs across 5 calibration strategies on X, Bluesky, and Reddit data.

Result: LLM outputs remain clearly distinguishable from human text even after calibration, especially in affective tone and emotional expression. Instruction-tuned models underperform base models, and scaling model size doesn't improve human-likeness. Found trade-off between optimizing for human-likeness and semantic fidelity.

Conclusion: Provides a scalable validation framework for LLM simulations but cautions about current limitations in capturing human communication, with clear trade-offs between realism and semantic accuracy.

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [25] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: Current LLMs cannot pass Poland's National Appeal Chamber qualification exam, failing the practical written judgment part despite decent knowledge test scores, with LLM-as-judge evaluations diverging from official committee judgments.


<details>
  <summary>Details</summary>
Motivation: To empirically assess whether current large language models can pass official legal qualification exams and evaluate the feasibility of using LLMs as exam candidates and automated evaluators in legal contexts.

Method: Tested multiple LLMs (GPT-4.1, Claude 4 Sonnet, Bielik-11B-v2.6) in closed-book and Retrieval-Augmented Generation settings on Poland's National Appeal Chamber exam, using a hybrid information recovery pipeline and comparing LLM-as-judge evaluations with official committee judgments.

Result: LLMs achieved satisfactory scores in multiple-choice knowledge tests but none met passing threshold in practical written judgments. LLM-as-judge evaluations often diverged from official committee assessments.

Conclusion: Current LLMs cannot replace human judges or examiners in Polish public procurement adjudication due to hallucinations, incorrect legal citations, weak logical argumentation, and the need for legal expert collaboration.

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [26] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: REMIND is a novel evaluation method for machine unlearning that detects residual memorization in semantically similar examples by analyzing loss landscape patterns, outperforming existing single-point evaluation approaches.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for machine unlearning only assess forgetting at individual input level, potentially overlooking residual influence in semantically similar examples that can compromise privacy and cause indirect information leakage.

Method: REMIND analyzes the model's loss over small input variations to reveal patterns unnoticed by single-point evaluations, detecting flatter loss landscapes for unlearned data versus sharper patterns for retained data.

Result: REMIND outperforms existing methods under query-based access constraints, demonstrates robustness across different models, datasets, and paraphrased inputs, and provides more sensitive and interpretable measures of unlearning effectiveness.

Conclusion: REMIND offers a reliable framework to assess unlearning in language models and provides a novel perspective on memorization and unlearning, making it practical for real-world deployment.

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [27] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: Pre-training leaves significant information unused in datasets; retrieval augmentation at test time provides 5x compute multiplier gains on MMLU and up to 10 percentage point improvements.


<details>
  <summary>Details</summary>
Motivation: To quantify how much dataset value is left behind by pre-training and understand efficiency of knowledge extraction from pre-training data.

Method: Use retrieval augmented generation with test-time compute to measure unused dataset value, analyzing standard open-sourced datasets across different model scales.

Result: Significant accuracy gains in MMLU, Math-500, and SimpleQA through retrieval, with 5x compute multiplier effect on MMLU and 10 percentage point improvement for LLaMA 3.1 8B.

Conclusion: Current pre-training methods underutilize information in existing datasets, indicating substantial room for improvement in knowledge extraction efficiency.

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [28] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: A graph-based approach for topic labeling that assigns meaningful labels to topic modeling outputs by enriching topic words with semantically related terms and analyzing their relationships, achieving comparable performance to ChatGPT-3.5 with better computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Topic modeling produces topics as word distributions that lack clear interpretability, and existing computational methods are resource-intensive. The paper aims to provide effective topic labeling with fewer computational resources.

Method: Proposes a graph-based approach that enriches topic words with semantically related terms and explores relationships among them to derive suitable labels that accurately capture each topic's meaning.

Result: The method achieved consistently better results than traditional benchmarks in BERTScore and cosine similarity, and produced results comparable to ChatGPT-3.5 while remaining computationally efficient across two datasets.

Conclusion: The graph-based approach provides an effective alternative for topic labeling that balances performance and computational efficiency, with future directions focusing on enhancing interpretability and automation.

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [29] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: SSPO introduces sentence-level importance ratio for RLVR, balancing between token-level GRPO and response-level GSPO to avoid training collapse and improve data utilization, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR algorithms like GRPO suffer from unstable policy updates due to token-level importance ratio being affected by outliers, while GSPO has low data utilization due to response-level importance ratio causing entire responses to be discarded.

Method: SSPO applies sentence-level importance ratio and uses sentence entropy to adjust PPO-CLIP clipping bounds, encouraging exploration for high-entropy tokens while narrowing clipping range for low-entropy tokens.

Result: SSPO achieves average score of 46.57 across five datasets, surpassing GRPO (43.01) and GSPO (44.42), and wins state-of-the-art performance on three datasets.

Conclusion: SSPO effectively leverages generated data by taking advantages of GSPO while avoiding its shortcomings, demonstrating superior performance in RLVR for LLM reasoning.

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [30] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: A data selection method for fine-tuning machine translation that uses learnability scores and batch selection to improve data efficiency and translation performance.


<details>
  <summary>Details</summary>
Motivation: Data quality and effective selection are fundamental for improving machine translation model performance and achieving robust systems.

Method: Leverages synergy between learner and pre-trained reference models with learnability scores to evaluate data utility, plus batch selection considering interdependencies among data points.

Result: Achieves up to 5x improvement in data efficiency compared to iid baseline, 24x computational efficiency with cached embeddings, and superior translation performance vs random selection.

Conclusion: The proposed data selection methodology effectively enhances machine translation fine-tuning through improved data efficiency, computational efficiency, and generalization.

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [31] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: LLMs were tested on temporal reasoning using 1940 Norwegian trivia questions, with better performance in English than Norwegian, and improved results from larger models.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' temporal reasoning capabilities by having them answer historical trivia questions as if it were 1940, testing both language and model size effects.

Method: Used a 1940 Norwegian trivia book, prompted LLMs to answer questions in both English and Norwegian as if it were 1940, employed LLM-as-judge grading with human verification.

Result: English prompting consistently outperformed Norwegian, larger LLMs showed better performance, tested DeepSeek-R1, Gemma3, Qwen3, Llama3.1 families and largest Norwegian-specific LLM.

Conclusion: LLMs demonstrate temporal reasoning capabilities but show unexpected language performance differences, with model size being a key factor in accuracy.

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [32] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: PTTSD is a probabilistic framework for depression detection from clinical interviews that predicts PHQ-8 scores with uncertainty estimates and temporal modeling, achieving state-of-the-art performance on text-only systems.


<details>
  <summary>Details</summary>
Motivation: Existing depression detection models lack uncertainty estimates and temporal modeling, which are essential for clinical decision support and interpretable predictions.

Method: Proposes PTTSD with sequence-to-sequence and sequence-to-one variants using bidirectional LSTMs, self-attention, residual connections, and Gaussian/Student-t output heads trained via negative log-likelihood.

Result: Achieves state-of-the-art performance on E-DAIC and DAIC-WOZ datasets (MAE = 3.85 on E-DAIC, 3.55 on DAIC) with well-calibrated prediction intervals, and outperforms MentalBERT in comparisons.

Conclusion: PTTSD provides interpretable and clinically relevant uncertainty-aware forecasting for depression severity prediction, with attention and probabilistic modeling being key components.

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [33] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: ThaiOCRBench is the first comprehensive benchmark for evaluating vision-language models on Thai text-rich visual understanding tasks, addressing the underrepresentation of Thai in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal benchmarks predominantly focus on high-resource languages, leaving Thai underrepresented, especially in tasks requiring document structure understanding.

Method: Created a diverse, human-annotated dataset with 2,808 samples across 13 task categories and evaluated state-of-the-art VLMs in zero-shot settings, including both proprietary and open-source systems.

Result: Proprietary models (e.g., Gemini 2.5 Pro) significantly outperform open-source counterparts, with fine-grained text recognition and handwritten content extraction showing the steepest performance drops among open-source models.

Conclusion: ThaiOCRBench provides a standardized framework for assessing VLMs in low-resource, script-complex settings and identifies key challenges like language bias, structural mismatch, and hallucinated content for improving Thai-language document understanding.

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [34] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: RUST-BENCH is a new benchmark for tabular reasoning using 7966 questions from 2031 real-world tables across science and sports domains, designed to test LLMs on complex, heterogeneous data that better represents real-world complexity than existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing tabular reasoning benchmarks use small, uniform tables that don't reflect real-world data complexity, giving an incomplete view of LLMs' reasoning abilities on long, heterogeneous, domain-specific tables with multi-hop reasoning requirements.

Method: Created RUST-BENCH with 7966 questions from 2031 real-world tables across two domains: RB-Science (NSF grant records) and RB-Sports (NBA statistics), evaluating LLMs across scale, heterogeneity, domain specificity, and reasoning complexity.

Result: Experiments show LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies.

Conclusion: RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research by better representing real-world table complexity.

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [35] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: The OUNLP system for TSAR-2025 uses LLM-prompting for readability-controlled text simplification, proposing multi-round methods (MRS-Rule and MRS-Joint) that leverage the gap between source and target CEFR levels.


<details>
  <summary>Details</summary>
Motivation: The research was motivated by discovering that text simplification performance correlates with the gap between source and target CEFR levels, leading to the development of multi-round simplification approaches.

Method: Proposed two multi-round simplification methods using GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based LLM simplification (MRS-Joint), where the latter uses LLM-simplified candidates as starting points.

Result: The submitted systems ranked 7 out of 20 teams, with later improvements showing that using LLM-simplified candidates as starting points further boosts multi-round simplification performance.

Conclusion: Multi-round simplification methods, particularly MRS-Joint that incorporates LLM-simplified candidates, effectively improve text simplification performance by leveraging the CEFR level gap between source and target texts.

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [36] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: This paper evaluates personality traits in six LLMs using the Big Five Inventory-2 framework, finding significant differences across personality dimensions and temperature sensitivity in Neuroticism and Extraversion.


<details>
  <summary>Details</summary>
Motivation: As LLMs become integral to human-centered applications, understanding their personality-like behaviors is important for responsible development and deployment.

Method: Systematically evaluated six LLMs using the Big Five Inventory-2 (BFI-2) framework under varying sampling temperatures, with hierarchical clustering analysis.

Result: Found significant differences across four of five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Hierarchical clustering revealed distinct model clusters suggesting architectural predispositions.

Conclusion: Results offer insights into personality-like patterns in LLMs and provide new perspective on model tuning, selection, and ethical governance of AI systems.

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [37] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: RAGalyst is an automated, human-aligned agentic framework for evaluating domain-specific RAG systems, featuring synthetic QA dataset generation and optimized LLM-as-a-Judge metrics that correlate with human judgment.


<details>
  <summary>Details</summary>
Motivation: Existing RAG evaluation frameworks use heuristic metrics that fail to capture domain-specific nuances and LLM-as-a-Judge approaches that lack validated alignment with human judgment, especially in safety-critical domains.

Method: RAGalyst uses an agentic pipeline to generate synthetic QA datasets from source documents with filtering for data fidelity, and refines Answer Correctness and Answerability metrics through prompt optimization to achieve human alignment.

Result: Evaluation across military operations, cybersecurity, and bridge engineering domains showed that RAG performance is highly context-dependent with no universally optimal configuration, and provided analysis of common low Answer Correctness reasons.

Conclusion: Systematic evaluation frameworks like RAGalyst are necessary to uncover domain-specific trade-offs and enable informed design choices for building reliable RAG systems in specialized domains.

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [38] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: This paper introduces a framework to quantify explicit and implicit uncertainty in radiology reports, creating an uncertainty-aware benchmark called Lunguage++ for improved clinical decision-making and automated analysis.


<details>
  <summary>Details</summary>
Motivation: Radiology reports contain valuable clinical information but suffer from two types of uncertainty: explicit uncertainty from hedging phrases and implicit uncertainty from omitted reasoning, making automated analysis challenging.

Method: Two-part framework: (1) Quantify explicit uncertainty using LLM-based reference ranking of hedging phrases mapped to probability values, (2) Model implicit uncertainty through expansion framework adding characteristic sub-findings from expert-defined diagnostic pathways for 14 common diagnoses.

Result: Created Lunguage++, an expanded uncertainty-aware version of the Lunguage benchmark with fine-grained structured radiology reports that enables uncertainty-aware image classification and faithful diagnostic reasoning.

Conclusion: The framework successfully addresses uncertainty in radiology reports, providing an enriched resource for clinical applications and enabling new investigations into diagnostic uncertainty impact.

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [39] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: Language models implicitly represent alternate reasoning paths through hidden activations, which can predict uncertainty and future outcomes during chain-of-thought reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand if language models represent the alternate reasoning paths they could take during generation, since token selection leads to different paths making uncertainty hard to quantify.

Method: Using hidden activations to control and predict language model uncertainty during chain-of-thought reasoning, testing correlation between uncertainty and steering effectiveness.

Result: Found clear correlation between model uncertainty and steering effectiveness via activation control; hidden activations can predict future outcome distribution.

Conclusion: Activation interventions are most effective when models have alternate paths available (not committed to final answer), and models implicitly represent possible reasoning paths through hidden activations.

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [40] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof is an interactive system that analyzes argumentative essays using LLMs to create argumentation graphs, visualize relations, and provide justifications and coherence measures while maintaining human oversight.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between structural semantics of argumentative essays and user understanding, providing better analysis tools than existing automated essay scoring systems.

Method: Structures essays as argumentation graphs with claims as nodes, evidence as properties, and supporting/attacking relations as edges. Uses LLMs for initial classification and scoring, then visualizes for enhanced understanding.

Result: Developed an interactive system that provides justifications for classifications, quantitative coherence measures, and tools for natural language understanding of argumentative essays and their graphs.

Conclusion: IntelliProof enables rapid exploration of argumentative quality while retaining human oversight, with a live demo available for testing.

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [41] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: LLM-based coding assistants generate security vulnerabilities that persist in latest models, prompting a new severity metric (Prompt Exposure) and Model Exposure score to prioritize vulnerability mitigation.


<details>
  <summary>Details</summary>
Motivation: As LLM coding assistants become critical in software development, their generated bugs pose significant cybersecurity risks, yet existing security benchmarks haven't effectively improved model security.

Method: Introduces Prompt Exposure (PE) metric combining vulnerability severity, generation chance, and prompt formulation, and Model Exposure (ME) score to measure vulnerability severity and prevalence.

Result: Even latest open-weight LLMs remain vulnerable to previously reported security scenarios, indicating safety-functionality trade-offs prevent effective vulnerability patching.

Conclusion: The new PE and ME metrics provide a framework to prioritize and mitigate the most serious and prevalent LLM-generated vulnerabilities in coding assistants.

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [42] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: This paper introduces BanglaMedQA and BanglaMMedBench, the first large-scale Bangla biomedical MCQ datasets, and benchmarks various RAG strategies to improve medical QA accuracy in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of developing accurate biomedical QA systems in low-resource languages like Bangla, ensuring equitable access to reliable medical knowledge.

Method: Applied and benchmarked several RAG strategies (Traditional, Zero-Shot Fallback, Agentic, Iterative Feedback, Aggregate RAG) combining textbook-based and web retrieval with generative reasoning. Integrated Bangla medical textbook corpus via OCR and implemented Agentic RAG pipeline for dynamic strategy selection.

Result: Agentic RAG achieved highest accuracy of 89.54% with openai/gpt-oss-120b, outperforming other configurations and demonstrating superior rationale quality.

Conclusion: RAG-based methods show strong potential to enhance reliability and accessibility of Bangla medical QA, establishing foundation for future multilingual medical AI research.

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [43] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: DeReC is a lightweight fact verification framework that uses dense retrieval and classification to outperform LLM-based methods in both accuracy and efficiency, reducing runtime by up to 95%.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based fact verification systems face computational barriers and hallucination risks in real-world deployments, necessitating more efficient and robust alternatives.

Method: Combines dense retrieval with specialized classification using general-purpose text embeddings instead of autoregressive LLM-based approaches.

Result: Achieves 65.58% F1 score on RAWFC (surpassing L-Defense's 61.20%), reduces runtime by 95% on RAWFC and 92% on LIAR-RAW compared to LLM methods.

Conclusion: Carefully engineered retrieval-based systems can match or exceed LLM performance in specialized tasks while being significantly more practical for real-world deployment.

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [44] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: LEASH is a training-free decoding algorithm that adaptively stops rationale generation in Chain-of-Thought prompting by monitoring token-level entropy slope and top-logit margin improvement, reducing token usage by 30-35% and latency by 27% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought prompting generates full fixed-length rationales which is computationally wasteful, increasing both token usage and latency.

Method: LEASH monitors two intrinsic signals: slope of token-level entropy and improvement in top-logit margin, terminating generation once both signals plateau indicating stable reasoning state.

Result: Across four instruction-tuned models on GSM8K and AQuA-RAT benchmarks, LEASH reduces average token generation by 30-35% and latency by 27%, with only 10 percentage point accuracy drop relative to CoT.

Conclusion: LEASH is model-agnostic, requires no additional training or supervision, and offers a simple and efficient alternative to CoT decoding.

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

TL;DR: LoRA-Edge enables efficient on-device CNN fine-tuning using tensor-train assisted LoRA, reducing trainable parameters by up to 100x while maintaining near-full fine-tuning accuracy for edge applications like HAR.


<details>
  <summary>Details</summary>
Motivation: Full fine-tuning of CNNs is infeasible for edge devices due to strict memory, compute, and energy constraints, especially for applications like Human Activity Recognition that need to handle domain shifts.

Method: Applies Tensor-Train SVD to pre-trained convolutional layers, selectively updates only the output-side core with zero-initialization, and fuses updates back into dense kernels without changing inference cost.

Result: Achieves accuracy within 4.7% of full fine-tuning while updating only 1.49% of parameters, with 1.4-3.8x faster convergence on Jetson Orin Nano, outperforming prior parameter-efficient methods.

Conclusion: LoRA-Edge makes structure-aligned, parameter-efficient on-device CNN adaptation practical for edge platforms by preserving convolutional structure and maintaining inference efficiency.

Abstract: On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [46] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: SILVI is an open-source labeling software that enables annotation of behaviors and interactions in video data, bridging the gap between behavioral labeling and individual localization for computer vision applications.


<details>
  <summary>Details</summary>
Motivation: Existing annotation tools either support behavioral labeling without localization or localization without interaction detection, creating a gap for analyzing social and individualized animal behavior.

Method: Developed SILVI - an open-source labeling software that integrates both behavioral labeling and individual localization functionalities, enabling annotation of behaviors and interactions directly within video data.

Result: SILVI generates structured outputs suitable for training and validating computer vision models, facilitating the development of automated approaches for fine-grained behavioral analyses.

Conclusion: SILVI bridges behavioral ecology with computer vision and could be broadly useful for annotating human interactions in videos requiring dynamic scene graph extraction.

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [47] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: This paper investigates using noise injection techniques during training to improve generalization of COVID-19 detection models from chest X-rays to out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for COVID-19 detection from chest X-rays fail to generalize to new clinical sources due to learning source-specific shortcuts rather than meaningful biomarkers.

Method: Applied fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during model training to improve robustness to distribution shifts.

Result: Noise injection significantly reduced the performance gap between in-distribution and out-of-distribution evaluation from 0.10-0.20 to 0.01-0.06 across key metrics including AUC, F1, accuracy, recall and specificity.

Conclusion: Simple noise injection techniques can effectively improve model generalization to out-of-distribution data in medical imaging applications like COVID-19 detection from chest X-rays.

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [48] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: Imitation learning policies for X-ray-guided spine cannula insertion achieve 68.5% first-attempt success in simulation, generalizing to complex anatomy and showing promise for CT-free robotic spinal navigation.


<details>
  <summary>Details</summary>
Motivation: To explore whether imitation learning applies to X-ray-guided spine procedures, specifically bi-plane-guided cannula insertion, given the complexity of multi-view X-ray interpretation.

Method: Developed an in silico sandbox for realistic X-ray-guided spine procedure simulation, curated dataset of correct trajectories and bi-planar X-ray sequences, trained imitation learning policies for planning and open-loop control using visual information only.

Result: Policy achieved 68.5% first-attempt success rate, maintained safe intra-pedicular trajectories across diverse vertebral levels, generalized to complex anatomy including fractures, remained robust to varied initializations, and produced plausible trajectories on real bi-planar X-rays despite simulation-only training.

Conclusion: Preliminary results are promising but limitations exist in entry point precision; full closed-loop control requires more frequent feedback; with robust priors and domain knowledge, such models could enable lightweight CT-free robotic spinal navigation.

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [49] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: Proposes an enhanced real-time object detection framework using pruned YOLOv12 with Self-Adversarial Training and specialized data augmentation for automated waste detection in desert environments, achieving optimal balance of accuracy and efficiency for drone deployment.


<details>
  <summary>Details</summary>
Motivation: Address the global waste crisis, particularly in remote/harsh environments like deserts where traditional waste collection is inefficient and hazardous, and fill the research gap in detecting organic/hazardous waste in underexplored terrains.

Method: Enhanced real-time object detection framework based on pruned, lightweight YOLOv12 integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies, tested using DroneTrashNet dataset.

Result: Significant improvements in precision, recall, and mean average precision (mAP) with low latency and compact model size suitable for deployment on resource-constrained aerial drones, outperforming state-of-the-art lightweight YOLO variants.

Conclusion: Validates the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments.

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [50] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: Class-Based Image Composition (CoImg) improves deep learning performance on imbalanced medical datasets by fusing multiple images of the same class into composite inputs, achieving near-perfect accuracy (99.6%) on OCT retinal scans.


<details>
  <summary>Details</summary>
Motivation: Address challenges of small, imbalanced datasets and poor image quality that lead to high false prediction rates in deep learning models for medical diagnosis.

Method: Create Composite Input Images (CoImg) by fusing multiple images of the same class into combined visual composites arranged in 3x1 layouts, applied to create a balanced version (Co-OCTDL) of the original imbalanced OCT dataset.

Result: Achieved near-perfect performance: 99.6% accuracy, 0.995 F1-score, and 0.9996 AUC, with significantly reduced false prediction rates compared to baseline model trained on raw dataset.

Conclusion: The method effectively handles class imbalance and small sample sizes, producing high-quality predictions even with weak datasets, demonstrating practical value for medical imaging applications.

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [51] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: Unsupervised anomaly detection framework for medical imaging that incrementally expands normal samples without anomaly labels, using lightweight adapters and uncertainty-gated admission to prevent drift.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of unknown anomaly detection in medical imaging where labeled anomalies are scarce and expert supervision is costly.

Method: Uses frozen pretrained vision backbone with tiny convolutional adapters for domain adaptation, k-NN anomaly scoring with compact coreset, and dual probabilistic gates (z-score threshold and SWAG-based epistemic uncertainty) for safe sample admission.

Result: Substantial performance improvements: COVID-CXR ROC-AUC from 0.9489 to 0.9982, Pneumonia CXR ROC-AUC from 0.6834 to 0.8968, Brain MRI ND-5 ROC-AUC from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211.

Conclusion: The framework is effective and efficient for real-world, label-scarce medical imaging applications, steadily refining normality as unlabeled data arrive.

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [52] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: BDR improves boundary detection via signed-distance regression instead of classification, while ATR adaptively allocates computation based on boundary difficulty, achieving better performance with less computation.


<details>
  <summary>Details</summary>
Motivation: Current temporal action localization methods apply uniform computation despite significant variations in boundary difficulty, leading to inefficient resource allocation.

Method: Two complementary approaches: Boundary Distance Regression (BDR) for optimal localization via signed-distance regression, and Adaptive Temporal Refinement (ATR) for differentiable computation allocation via continuous depth selection.

Result: BDR achieves 43% sharper boundary peaks and 1.8-3.1% mAP@0.7 improvements across architectures. ATR achieves 56.5% mAP@0.7 at 162G FLOPs vs 53.6% at 198G for uniform processing (2.9% improvement with 18% less compute).

Conclusion: The proposed methods provide significant performance gains with reduced computational cost, validated across multiple benchmarks with statistical testing.

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [53] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: Unified framework for joint optimization of mesh geometry and appearance using Gaussian-guided differentiable rendering, enabling high-quality 3D reconstruction for downstream editing tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods separate geometry and appearance optimization, which limits performance in downstream editing applications like 3D editing, AR/VR, and digital content creation.

Method: Simultaneously optimizes mesh geometry (vertex positions and faces) and vertex colors via Gaussian-guided mesh differentiable rendering, using photometric consistency from input images and geometric regularization from normal and depth maps.

Result: Obtains high-quality 3D reconstruction that can be effectively used in downstream editing tasks such as relighting and shape deformation.

Conclusion: The proposed unified treatment of geometry and appearance optimization through Gaussian-mesh joint optimization provides superior results for 3D reconstruction and editing applications.

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [54] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: Proposes a linear fractional transformation parameter to decouple main lens and micro lens array in light field camera calibration, with analytical solution and nonlinear refinement.


<details>
  <summary>Details</summary>
Motivation: Accurate calibration of internal parameters is crucial but challenging for 3D reconstruction using light field cameras.

Method: Uses linear fractional transformation parameter α to decouple main lens and MLA, includes analytical solution based on least squares followed by nonlinear refinement, and introduces feature detection from raw images.

Result: Experimental results on both physical and simulated data verify the method's performance. The model enables faster simulation of raw light field images.

Conclusion: The proposed calibration method is effective and enables faster image simulation, which is important for data-driven deep learning approaches in light field imaging.

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [55] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: A synthetic dataset called Room Envelopes is introduced to help predict occluded structural elements like walls, floors, and ceilings in scene reconstruction, enabling direct supervision for monocular geometry estimators.


<details>
  <summary>Details</summary>
Motivation: Current scene reconstruction methods produce incomplete results by only recovering visible surfaces, missing occluded structural elements. These structural elements (walls, floors, ceilings) are relatively easy to predict due to their planar, repetitive, and simple nature.

Method: Created a synthetic dataset with RGB images and two pointmaps per image: one for visible surfaces and one for structural layout surfaces (after removing fittings and fixtures). This enables direct supervision for feed-forward monocular geometry estimators.

Result: The dataset facilitates training models to predict both visible surfaces and structural layout surfaces, providing understanding of scene extent and object shapes/locations.

Conclusion: The Room Envelopes dataset enables progress in predicting occluded structural elements in scene reconstruction, offering a more complete understanding of scenes through direct supervision of geometry estimators.

Abstract: Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [56] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

TL;DR: Humans use 3D pose information for social interaction recognition, which outperforms current AI vision models. A compact set of 3D social pose features (face position and direction) matches full joint data and improves AI performance.


<details>
  <summary>Details</summary>
Motivation: To understand why humans excel at social interaction recognition while AI struggles, and test if 3D visuospatial pose information is the key factor missing in most AI models.

Method: Extracted 3D joint positions from videos using pose and depth estimation algorithms, compared with AI vision models, and derived minimal 3D social pose features describing face position and direction.

Result: 3D joint positions outperformed most AI vision models. Minimal 3D social pose features matched full joint data's predictive strength and improved AI model performance when combined. Representation of these features in AI models predicted their social judgment accuracy.

Conclusion: Human social scene understanding relies on explicit 3D pose representations and can be supported by simple visuospatial primitives, suggesting AI vision systems should incorporate 3D pose information.

Abstract: Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [57] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

TL;DR: CaRF introduces a camera-aware framework for 3D Gaussian segmentation that achieves multi-view consistency by incorporating camera geometry into Gaussian-text interactions and using paired view supervision during training.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with cross-view consistency due to reliance on 2D rendered pseudo supervision and view-specific feature learning, limiting reliable 3D scene understanding.

Method: Proposes Gaussian Field Camera Encoding (GFCE) to incorporate camera geometry into Gaussian-text interactions, and In-Training Paired View Supervision (ITPVS) to align per-Gaussian logits across calibrated views during training.

Result: Achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state-of-the-art methods on Ref LERF, LERF OVS, and 3D OVS datasets respectively.

Conclusion: CaRF promotes more reliable and view-consistent 3D scene understanding with potential applications in embodied AI, AR/VR interaction, and autonomous perception.

Abstract: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [58] [PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection](https://arxiv.org/abs/2511.03997)
*Peiyao Wang,Weining Wang,Qi Li*

Main category: cs.CV

TL;DR: PhysCorr is a framework for improving physical consistency in text-to-video generation by introducing PhysicsRM reward model and PhyDPO optimization, achieving better physical realism while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video generation models produce high-quality visuals but often violate physical plausibility with implausible object dynamics and incoherent interactions, limiting their use in embodied AI, robotics, and simulation domains.

Method: Proposes PhysCorr framework with PhysicsRM (dual-dimensional reward model for intra-object stability and inter-object interactions) and PhyDPO (direct preference optimization with contrastive feedback and physics-aware reweighting).

Result: Extensive experiments show PhysCorr achieves significant improvements in physical realism while preserving visual fidelity and semantic alignment across multiple benchmarks.

Conclusion: This work represents a critical step toward physically grounded and trustworthy video generation that can be integrated into various video diffusion and transformer-based models.

Abstract: Recent advances in text-to-video generation have achieved impressive
perceptual quality, yet generated content often violates fundamental principles
of physical plausibility - manifesting as implausible object dynamics,
incoherent interactions, and unrealistic motion patterns. Such failures hinder
the deployment of video generation models in embodied AI, robotics, and
simulation-intensive domains. To bridge this gap, we propose PhysCorr, a
unified framework for modeling, evaluating, and optimizing physical consistency
in video generation. Specifically, we introduce PhysicsRM, the first
dual-dimensional reward model that quantifies both intra-object stability and
inter-object interactions. On this foundation, we develop PhyDPO, a novel
direct preference optimization pipeline that leverages contrastive feedback and
physics-aware reweighting to guide generation toward physically coherent
outputs. Our approach is model-agnostic and scalable, enabling seamless
integration into a wide range of video diffusion and transformer-based
backbones. Extensive experiments across multiple benchmarks demonstrate that
PhysCorr achieves significant improvements in physical realism while preserving
visual fidelity and semantic alignment. This work takes a critical step toward
physically grounded and trustworthy video generation.

</details>


### [59] [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008)
*Mahmoud Soliman,Omar Abdelaziz,Ahmed Radwan,Anand,Mohamed Shehata*

Main category: cs.CV

TL;DR: GNN-MoE enhances Parameter-Efficient Fine-Tuning for domain generalization in Vision Transformers using a Mixture-of-Experts framework with Graph Neural Network routing on inter-patch graphs.


<details>
  <summary>Details</summary>
Motivation: Standard fine-tuning of pretrained Vision Transformers for domain generalization is computationally expensive and can harm generalization performance, requiring more efficient adaptation methods.

Method: Proposes GNN-MoE framework that uses GNN routers (GCN, GAT, SAGE) on inter-patch graphs to dynamically assign patches to specialized experts, replacing token-based routing with context-aware graph-based routing.

Result: Achieves state-of-the-art or competitive performance on domain generalization benchmarks while maintaining high parameter efficiency.

Conclusion: Graph-based contextual routing is effective for robust and lightweight domain generalization in Vision Transformers.

Abstract: Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.

</details>


### [60] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman,Islam Osman,Mohamed S. Shehata,Rasika Rajapakshe*

Main category: cs.CV

TL;DR: MedDChest is a foundational Vision Transformer pre-trained from scratch on 1.2M thoracic medical images, featuring a novel Guided Random Resized Crops augmentation that outperforms ImageNet-pretrained models on various diagnostic tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the domain gap between natural images and medical imaging that hinders vision model performance in medical applications.

Method: Pre-trained ViT from scratch on 1.2M curated multimodal thoracic images (X-ray, CT) using novel Guided Random Resized Crops augmentation that biases sampling towards anatomically relevant regions.

Result: Significantly outperforms strong ImageNet-pretrained models on diverse downstream diagnostic tasks through comprehensive experiments.

Conclusion: Large-scale in-domain pre-training combined with domain-specific data augmentation provides a powerful feature extractor for thoracic diagnostic tasks, establishing MedDChest as a superior starting point.

Abstract: The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.

</details>


### [61] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: Faithful Contouring is a sparse voxelized representation for 3D meshes that achieves near-lossless fidelity at 2048+ resolutions without requiring field functions or isosurface extraction.


<details>
  <summary>Details</summary>
Motivation: Existing voxelized representations based on iso-surface methods rely on water-tightening or rendering optimization, which compromise geometric fidelity and cannot preserve sharp features or internal structures.

Method: Proposes Faithful Contouring - a sparse voxelized representation that directly encodes meshes without converting to field functions, and designs a dual-mode autoencoder for scalable shape reconstruction.

Result: Achieves distance errors at 10^-5 level for direct representation, and for mesh reconstruction yields 93% reduction in Chamfer Distance and 35% improvement in F-score over strong baselines.

Conclusion: Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction, confirming superior fidelity as a representation for 3D learning tasks.

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [62] [A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals](https://arxiv.org/abs/2511.04037)
*Arfina Rahman,Mahesh Banavar*

Main category: cs.CV

TL;DR: A lightweight PPG-based biometric authentication system using fingertip videos achieves 98% accuracy with a hybrid CVT-ConvMixer-LSTM model that processes time-frequency scalograms of PPG signals.


<details>
  <summary>Details</summary>
Motivation: PPG signals offer non-invasive, liveness-detecting biometric authentication suitable for wearable devices, but face challenges from motion artifacts, illumination changes, and physiological variability that require robust feature extraction.

Method: PPG signals from low-frame-rate fingertip videos undergo preprocessing (baseline removal, PCA motion artifact suppression, filtering, resampling, normalization), are converted to time-frequency scalograms via CWT, and processed by a hybrid CVT-ConvMixer-LSTM model combining spatial and temporal features.

Result: The system achieves 98% authentication accuracy on the CFIHSR dataset with 46 subjects, demonstrating robustness to noise and inter-subject variability.

Conclusion: The proposed framework is efficient, scalable, and suitable for real-world mobile and embedded biometric security applications due to its liveness detection capability and robustness.

Abstract: Photoplethysmography (PPG) signals, which measure changes in blood volume in
the skin using light, have recently gained attention in biometric
authentication because of their non-invasive acquisition, inherent liveness
detection, and suitability for low-cost wearable devices. However, PPG signal
quality is challenged by motion artifacts, illumination changes, and
inter-subject physiological variability, making robust feature extraction and
classification crucial. This study proposes a lightweight and cost-effective
biometric authentication framework based on PPG signals extracted from
low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings
from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The
raw PPG signals undergo a standard preprocessing pipeline involving baseline
drift removal, motion artifact suppression using Principal Component Analysis
(PCA), bandpass filtering, Fourier-based resampling, and amplitude
normalization. To generate robust representations, each one-dimensional PPG
segment is converted into a two-dimensional time-frequency scalogram via the
Continuous Wavelet Transform (CWT), effectively capturing transient
cardiovascular dynamics. We developed a hybrid deep learning model, termed
CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision
Transformer (CVT) and ConvMixer branches with temporal features from a Long
Short-Term Memory network (LSTM). The experimental results on 46 subjects
demonstrate an authentication accuracy of 98%, validating the robustness of the
model to noise and variability between subjects. Due to its efficiency,
scalability, and inherent liveness detection capability, the proposed system is
well-suited for real-world mobile and embedded biometric security applications.

</details>


### [63] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng,Chenqi Zhang,Mingru Wang,Minuo Wei,Shiwei Cheng,Cuntai Guan,Ting Han*

Main category: cs.CV

TL;DR: Bratrix is a multimodal framework that aligns visual and brain representations with language embeddings to improve neural signal decoding, featuring uncertainty-aware weighting and hierarchical semantic decomposition.


<details>
  <summary>Details</summary>
Motivation: Existing methods align neural activity directly with visual embeddings, but visual-only representations fail to capture latent semantic dimensions, limiting interpretability and robustness due to subject variability and entangled visual features.

Method: Bratrix decouples visual stimuli into hierarchical visual and linguistic components, projects both visual and brain representations into a shared latent space, and incorporates an uncertainty perception module with uncertainty-aware weighting. It uses learnable language-anchored semantic matrices and a two-stage training strategy (single-modality pretraining followed by multimodal fine-tuning).

Result: Extensive experiments on EEG, MEG, and fMRI benchmarks show Bratrix improves retrieval, reconstruction, and captioning performance, surpassing state-of-the-art methods by 14.3% in 200-way EEG retrieval task.

Conclusion: Bratrix successfully achieves multimodal Language-Anchored Vision-Brain alignment, demonstrating superior performance in decoding neural signals and enabling more interpretable and robust brain-computer interfaces.

Abstract: Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

</details>


### [64] [Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score](https://arxiv.org/abs/2511.04083)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: Comparison of CycleGAN-based residual translator and Noise2Score for CT image denoising in unpaired and self-supervised settings, with CycleGAN achieving superior final image quality while Noise2Score provides robust pair-free alternative.


<details>
  <summary>Details</summary>
Motivation: To address CT image denoising in scenarios where paired training data is unavailable, evaluating training-data-efficient paradigms for unpaired and self-supervised denoising.

Method: Evaluated two approaches: CycleGAN-based residual translator with U-Net backbone (optimized parameters: lambda_cycle=30, lambda_iden=2, ngf=ndf=64) and Noise2Score score-matching denoiser under common evaluation protocol.

Result: CycleGAN improved noisy input from 34.66 dB/0.9234 SSIM to 38.913 dB/0.971 SSIM, achieving estimated score of 1.9441 and unseen-set score of 1.9343. Noise2Score achieved large gains on very noisy inputs despite slightly lower absolute PSNR/SSIM.

Conclusion: CycleGAN offers strongest final image quality, while Noise2Score provides robust pair-free alternative with competitive performance, highlighting their respective utilities in different scenarios.

Abstract: We study CT image denoising in the unpaired and self-supervised regimes by
evaluating two strong, training-data-efficient paradigms: a CycleGAN-based
residual translator and a Noise2Score (N2S) score-matching denoiser. Under a
common evaluation protocol, a configuration sweep identifies a simple standard
U-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =
64) as the most reliable setting; we then train it to convergence with a longer
schedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234
SSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an
unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly
behind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,
highlighting its utility when clean pairs are unavailable. Overall, CycleGAN
offers the strongest final image quality, whereas Noise2Score provides a robust
pair-free alternative with competitive performance. Source code is available at
https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.

</details>


### [65] [When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.04084)
*Nishchal Sapkota,Haoyan Shi,Yejia Zhang,Xianshi Ma,Bofang Zheng,Danny Z. Chen*

Main category: cs.CV

TL;DR: UKAST integrates rational-function based Kolmogorov-Arnold Networks into Swin Transformer encoders for medical image segmentation, achieving state-of-the-art performance with improved data efficiency and reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation faces challenges with complex anatomical structures and limited annotated data. CNNs struggle with long-range dependencies while Transformers are data-hungry and computationally expensive.

Method: Proposed UKAST architecture that integrates rational-function based KANs into Swin Transformer encoders, using Group Rational KANs (GR-KANs) from KAT to address inefficiencies of vanilla spline-based KANs.

Result: Achieves state-of-the-art performance on four 2D/3D medical image segmentation benchmarks, surpassing CNN- and Transformer-based baselines. Superior accuracy in data-scarce settings with reduced FLOPs and minimal parameter increase.

Conclusion: KAN-enhanced Transformers show strong potential for advancing data-efficient medical image segmentation, addressing limitations of both CNNs and standard Vision Transformers.

Abstract: Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST

</details>


### [66] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu,Yuanzhi Liang*

Main category: cs.CV

TL;DR: SpatialLock is a novel framework that uses perception signals and grounding information to achieve precise object localization in text-to-image generation, achieving state-of-the-art IOU scores above 0.9.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image synthesis methods fail to fully utilize positional information, leading to inadequate understanding of object spatial layouts and imprecise control over object localization.

Method: SpatialLock incorporates two components: Position-Engaged Injection (PoI) that integrates spatial information through attention layers, and Position-Guided Learning (PoG) that uses perception-based supervision to refine object localization.

Result: Experiments show SpatialLock achieves IOU scores above 0.9 across multiple datasets, setting a new state-of-the-art for precise object positioning.

Conclusion: The proposed SpatialLock framework effectively enables precise spatial control in text-to-image generation, improving both object localization accuracy and visual quality of generated images.

Abstract: Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.

</details>


### [67] [Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration](https://arxiv.org/abs/2511.04117)
*Yunghee Lee,Byeonghyun Pak,Junwha Hong,Hoseong Kim*

Main category: cs.CV

TL;DR: THG is a training-free diffusion sampling acceleration method that treats classifier-free guidance as a multirate ODE system, allowing different integration frequencies for noise estimation and guidance terms to reduce computation by up to 30% without quality loss.


<details>
  <summary>Details</summary>
Motivation: Conventional diffusion solvers treat all components equally, failing to exploit the different sensitivity of noise estimation and guidance terms to numerical errors, leading to computational redundancy.

Method: Reformulates CFG ODE as multirate system, uses tortoise equation (fine grid) for noise estimate and hare equation (coarse grid) for guidance, plus adaptive timestep sampler and guidance-scale scheduler.

Result: Reduces NFE by up to 30% with minimal fidelity loss (ΔImageReward ≤ 0.032), outperforms state-of-the-art CFG-based training-free accelerators under same computation budget.

Conclusion: Multirate formulations offer significant potential for diffusion solver optimization, enabling real-time high-quality image synthesis without model retraining.

Abstract: In this paper, we propose Tortoise and Hare Guidance (THG), a training-free
strategy that accelerates diffusion sampling while maintaining high-fidelity
generation. We demonstrate that the noise estimate and the additional guidance
term exhibit markedly different sensitivity to numerical error by reformulating
the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our
error-bound analysis shows that the additional guidance branch is more robust
to approximation, revealing substantial redundancy that conventional solvers
fail to exploit. Building on this insight, THG significantly reduces the
computation of the additional guidance: the noise estimate is integrated with
the tortoise equation on the original, fine-grained timestep grid, while the
additional guidance is integrated with the hare equation only on a coarse grid.
We also introduce (i) an error-bound-aware timestep sampler that adaptively
selects step sizes and (ii) a guidance-scale scheduler that stabilizes large
extrapolation spans. THG reduces the number of function evaluations (NFE) by up
to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward
$\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free
accelerators under identical computation budgets. Our findings highlight the
potential of multirate formulations for diffusion solvers, paving the way for
real-time high-quality image synthesis without any model retraining. The source
code is available at https://github.com/yhlee-add/THG.

</details>


### [68] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li,Shikui Tu,Lei Xu*

Main category: cs.CV

TL;DR: A training-free diffusion framework for sketch generation that enables precise style control using text prompts and reference sketches, with mechanisms to reduce content leakage and support multi-style generation.


<details>
  <summary>Details</summary>
Motivation: Existing sketch generation methods lack precise control over sketch styles and often suffer from content leakage when using reference sketches for style transfer.

Method: Proposes a diffusion-based framework that incorporates reference features as auxiliary information with linear smoothing and style-content guidance, avoiding overwriting self-attention matrices. Extends to multi-style generation using joint AdaIN module.

Result: Achieves high-quality sketch generation with accurate style alignment, reduced content leakage, and improved flexibility in style control, especially effective when reference and target sketches have low structural similarity.

Conclusion: The proposed M3S framework provides effective style control for sketch generation while maintaining content integrity, offering a flexible solution for precise style manipulation in sketch synthesis.

Abstract: Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.

</details>


### [69] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

TL;DR: Automated tennis match analysis pipeline using deep learning for player/ball detection, court keypoint identification, and performance analytics.


<details>
  <summary>Details</summary>
Motivation: To provide automated, real-time tennis match analysis for coaches, broadcasters, and players to gain actionable insights into game dynamics.

Method: Integrated deep learning framework using YOLOv8 for player detection, custom YOLOv5 for ball tracking, and ResNet50-based architecture for court keypoint detection.

Result: Robust performance in varying court conditions and match scenarios, generating annotated videos with detailed performance metrics including player movement patterns, ball speed, shot accuracy, and reaction times.

Conclusion: The complete pipeline successfully enables automated tennis match analysis with actionable insights for various stakeholders in the tennis ecosystem.

Abstract: This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [70] [DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms](https://arxiv.org/abs/2511.04128)
*Shengyu Tang,Zeyuan Lu,Jiazhi Dong,Changdong Yu,Xiaoyu Wang,Yaohui Lyu,Weihao Xia*

Main category: cs.CV

TL;DR: DMSORT is an efficient dual-branch maritime multi-object tracking method that handles camera motion through affine compensation, combining object detection/ReID with camera motion estimation for robust tracking in challenging marine environments.


<details>
  <summary>Details</summary>
Motivation: Accurate marine environment perception is crucial for safe vessel navigation and maritime surveillance, but camera motion and visual degradation in complex maritime environments pose significant challenges to multi-object tracking.

Method: Proposes a parallel tracker with affine compensation featuring: 1) Object detection and ReID branch with Reversible Columnar Detection Network and lightweight Transformer-based appearance extractor; 2) Dedicated branch for dynamic camera motion estimation that decouples platform-induced and target-intrinsic motion; 3) Clustering-optimized feature fusion module combining motion and appearance cues.

Result: Extensive evaluations on Singapore Maritime Dataset show DMSORT achieves state-of-the-art performance with the fastest runtime among existing ReID-based MOT frameworks, while maintaining high identity consistency and robustness to jitter and occlusion.

Conclusion: DMSORT provides an efficient and robust solution for maritime multi-object tracking that effectively handles camera motion challenges while maintaining high performance and computational efficiency.

Abstract: Accurate perception of the marine environment through robust multi-object
tracking (MOT) is essential for ensuring safe vessel navigation and effective
maritime surveillance. However, the complicated maritime environment often
causes camera motion and subsequent visual degradation, posing significant
challenges to MOT. To address this challenge, we propose an efficient
Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the
framework is a parallel tracker with affine compensation, which incorporates an
object detection and re-identification (ReID) branch, along with a dedicated
branch for dynamic camera motion estimation. Specifically, a Reversible
Columnar Detection Network (RCDN) is integrated into the detection module to
leverage multi-level visual features for robust object detection. Furthermore,
a lightweight Transformer-based appearance extractor (Li-TAE) is designed to
capture global contextual information and generate robust appearance features.
Another branch decouples platform-induced and target-intrinsic motion by
constructing a projective transformation, applying platform-motion compensation
within the Kalman filter, and thereby stabilizing true object trajectories.
Finally, a clustering-optimized feature fusion module effectively combines
motion and appearance cues to ensure identity consistency under noise,
occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset
demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT
attains the fastest runtime among existing ReID-based MOT frameworks while
maintaining high identity consistency and robustness to jitter and occlusion.
Code is available at:
https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.

</details>


### [71] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

TL;DR: A framework that enables computer-use agents to learn from online video tutorials at inference time by retrieving, filtering, and converting videos into structured demonstration trajectories, then dynamically selecting relevant guidance during execution.


<details>
  <summary>Details</summary>
Motivation: Computer-use agents lag behind humans in tasks requiring domain-specific procedural knowledge, while humans can effectively learn from video tutorials by selectively imitating relevant segments.

Method: Proposes a framework that retrieves tutorial videos, converts them into structured trajectories using a VLM to infer UI actions and segment videos, assigns textual objectives to subsequences, and uses two-stage dynamic trajectory selection during execution.

Result: Experiments on two benchmarks show consistent outperformance over base agents and text-only variants, with analyses highlighting the importance of trajectory segmentation, selection, action filtering, and visual information.

Conclusion: Online videos can be systematically distilled into actionable guidance that improves computer-use agents at inference time, bridging the gap between automated agents and human performance.

Abstract: Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [72] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: This paper introduces OCR-Rotation-Bench (ORB), a benchmark for evaluating OCR robustness to image rotations, and presents a fast rotation classification pipeline that achieves high accuracy and significantly boosts OCR performance.


<details>
  <summary>Details</summary>
Motivation: Accurate rotation correction is essential for enhancing OCR performance in real-world settings where misalignment commonly occurs due to user errors during document capture.

Method: Developed a fast, robust and lightweight rotation classification pipeline using Phi-3.5-Vision model's vision encoder with dynamic image cropping, fine-tuned specifically for 4-class rotation classification.

Result: Achieved 96% and 92% accuracy on ORB-En and ORB-Indic datasets respectively, and demonstrated significant OCR performance improvements: up to 14% for closed-source models and up to 4x for open-weights models.

Conclusion: The proposed rotation classification module is critical for boosting OCR performance in real-world scenarios with rotated document images.

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [73] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

TL;DR: CycleGAN color transformation achieves the lowest registration errors for aligning H&E stained images with non-linear multimodal images in digital pathology.


<details>
  <summary>Details</summary>
Motivation: Accurate registration of images from different modalities is essential in digital pathology for applications like biomarker analysis and tissue reconstruction, but existing methods may not optimize alignment between different stain types.

Method: Used 20 tissue sample pairs with various preprocessing steps including CycleGAN, Macenko, Reinhard, and Vahadane color transformations, followed by VALIS registration (rigid then non-rigid registration). Evaluated using relative Target Registration Error (rTRE) and custom point-based evaluation.

Result: CycleGAN color transformation achieved the lowest registration errors in both original and inverted multimodal image scenarios, while other methods showed higher errors.

Conclusion: Applying color transformation before registration improves alignment between images from different modalities and supports more reliable analysis in digital pathology.

Abstract: Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [74] [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190)
*Josef Mayr,Anna Reithmeir,Maxime Di Folco,Julia A. Schnabel*

Main category: cs.CV

TL;DR: Covariance descriptors from pre-trained vision encoders outperform handcrafted features for medical image classification, with SPDNet showing superior performance when combined with DINOv2 features.


<details>
  <summary>Details</summary>
Motivation: Covariance descriptors have shown strong performance in general computer vision but remain underexplored in medical imaging, creating an opportunity to investigate their effectiveness for medical image classification.

Method: Construct covariance descriptors from features extracted by pre-trained general vision encoders (DINOv2 and MedSAM) and compare with handcrafted descriptors across eleven datasets from MedMNSIT benchmark using SPDNet classification network.

Result: Covariance descriptors from GVE features consistently outperform handcrafted features, and SPDNet yields superior performance to state-of-the-art methods when combined with DINOv2 features.

Conclusion: Combining covariance descriptors with powerful pretrained vision encoders shows significant potential for medical image analysis.

Abstract: Covariance descriptors capture second-order statistics of image features.
They have shown strong performance in general computer vision tasks, but remain
underexplored in medical imaging. We investigate their effectiveness for both
conventional and learning-based medical image classification, with a particular
focus on SPDNet, a classification network specifically designed for symmetric
positive definite (SPD) matrices. We propose constructing covariance
descriptors from features extracted by pre-trained general vision encoders
(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and
MedSAM - are evaluated across eleven binary and multi-class datasets from the
MedMNSIT benchmark. Our results show that covariance descriptors derived from
GVE features consistently outperform those derived from handcrafted features.
Moreover, SPDNet yields superior performance to state-of-the-art methods when
combined with DINOv2 features. Our findings highlight the potential of
combining covariance descriptors with powerful pretrained vision encoders for
medical image analysis.

</details>


### [75] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: Proposes Adaptive Statistics Fusor (AStF) with skewness and kurtosis for better motion style transfer beyond traditional mean/variance methods.


<details>
  <summary>Details</summary>
Motivation: Traditional image style transfer methods using mean and variance are insufficient for motion data due to complex dynamic patterns and spatiotemporal coherence.

Method: AStF with Style Disentanglement Module and High-Order Multi-Statistics Attention, trained with Motion Consistency Regularization discriminator.

Result: Shows proficiency superiority in motion style transfer over state-of-the-art methods by modeling spatiotemporal statistical patterns more comprehensively.

Conclusion: Incorporating higher-order statistics (skewness and kurtosis) provides more effective motion style transfer than traditional mean/variance approaches.

Abstract: Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [76] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

TL;DR: MedSapiens adapts human-centric foundation models for anatomical landmark detection in medical imaging, achieving state-of-the-art performance across multiple datasets with significant improvements over both generalist and specialist models.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of human-centric foundation models for anatomical landmark detection, leveraging their inherent optimization for spatial pose localization as strong priors for medical imaging tasks.

Method: Adaptation of Sapiens (human-centric foundation model for pose estimation) to medical imaging through multi-dataset pretraining, creating MedSapiens model.

Result: Achieved up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in average success detection rate (SDR); also showed 2.69% improvement in few-shot settings.

Conclusion: Human-centric foundation models provide strong priors for anatomical landmark detection and can be effectively adapted to medical imaging, establishing new state-of-the-art performance across multiple datasets.

Abstract: This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [77] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

TL;DR: Proto-LeakNet is an interpretable attribution framework that detects signal leaks in diffusion model outputs for AI image and deepfake forensics, achieving 98.13% Macro AUC without retraining for unseen generators.


<details>
  <summary>Details</summary>
Motivation: The increasing sophistication of synthetic image and deepfake generation models has made source attribution and authenticity verification a critical challenge, as diffusion pipelines unintentionally imprint persistent statistical traces (signal leaks) in their outputs.

Method: The framework operates in the latent domain of diffusion models, re-simulating partial forward diffusion to expose generator-specific cues. It uses a temporal attention encoder to aggregate multi-step latent features and a feature-weighted prototype head to structure the embedding space for transparent attribution.

Result: Proto-LeakNet achieves a Macro AUC of 98.13%, learns a latent geometry that remains robust under post-processing, surpasses state-of-the-art methods, and achieves strong separability between known and unseen generators.

Conclusion: Modeling signal-leak bias in latent space enables reliable and interpretable AI-image and deepfake forensics, demonstrating the effectiveness of the proposed framework for source attribution and authenticity verification.

Abstract: The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [78] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang,Shuang Li,Jun Ye,Neng Dong,Fan Li,Huafeng Li*

Main category: cs.CV

TL;DR: DinoGRL framework leverages DINOv2 to learn gait features for cross-modal video person re-identification, combining silhouette learning with appearance cues through bidirectional multi-granularity enhancement.


<details>
  <summary>Details</summary>
Motivation: Existing VVI-ReID methods focus on modality-invariant visual features but overlook gait features, which are modality-invariant and rich in temporal dynamics, limiting their ability to model spatiotemporal consistency for cross-modal video matching.

Method: Proposed DinoGRL framework with SASGL model that generates silhouette representations using DINOv2 semantic priors and PBMGE module that enables bidirectional interactions between gait and appearance streams across multiple spatial granularities.

Result: Extensive experiments on HITSZ-VCM and BUPT datasets demonstrate superiority, significantly outperforming existing state-of-the-art methods.

Conclusion: The proposed approach effectively leverages gait features complementary to appearance cues, achieving robust sequence-level representations for cross-modal video person re-identification.

Abstract: Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.

</details>


### [79] [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283)
*Shiwei Ren,Tianci Wen,Yongchun Fang,Biao Lu*

Main category: cs.CV

TL;DR: FastGS is a novel acceleration framework for 3D Gaussian splatting that uses multi-view consistency-based densification and pruning to achieve 3.32-15.45× training speedup while maintaining comparable rendering quality.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS acceleration methods fail to properly regulate Gaussian numbers during training, causing redundant computational overhead and inefficient training.

Method: Proposes a densification and pruning strategy based on multi-view consistency, eliminating the need for budgeting mechanisms to efficiently manage Gaussian importance.

Result: Achieves 3.32× training acceleration vs DashGaussian on Mip-NeRF 360 and 15.45× vs vanilla 3DGS on Deep Blending, with comparable rendering quality and strong generality across various tasks.

Conclusion: FastGS provides a simple yet effective acceleration framework that significantly improves training efficiency while maintaining rendering quality across diverse 3D reconstruction tasks.

Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

</details>


### [80] [Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment](https://arxiv.org/abs/2511.04288)
*Leire Benito-Del-Valle,Artzai Picón,Daniel Mugica,Manuel Ramos,Eva Portillo,Javier Romero,Carlos Javier Jimenez,Ramón Navarra-Mestre*

Main category: cs.CV

TL;DR: Domain-specific vision foundation model for herbicide trials outperforms general-purpose models in species identification and damage classification, especially under unseen conditions, while reducing annotation requirements.


<details>
  <summary>Details</summary>
Motivation: General-purpose vision models have limited performance in agriculture due to fine-grained distinctions needed for species identification and herbicide damage assessment in diverse environments.

Method: Adapted a general-purpose vision foundation model using self-supervised learning on a large curated agricultural dataset to learn domain-specific representations optimized for herbicide trial images.

Result: Significant performance improvements: species identification F1 from 0.91 to 0.94, damage classification from 0.26 to 0.33. Under unseen conditions, gains were even greater (species from 0.56 to 0.66, damage from 0.17 to 0.27). Achieved 5.4% higher F1 with 80% fewer labeled samples.

Conclusion: Domain-specific foundation models offer superior generalization capabilities for herbicide trial analysis, significantly reducing manual annotation efforts and providing scalable automated solutions.

Abstract: Herbicide field trials require accurate identification of plant species and
assessment of herbicide-induced damage across diverse environments. While
general-purpose vision foundation models have shown promising results in
complex visual domains, their performance can be limited in agriculture, where
fine-grained distinctions between species and damage types are critical.
  In this work, we adapt a general-purpose vision foundation model to herbicide
trial characterization. Trained using a self-supervised learning approach on a
large, curated agricultural dataset, the model learns rich and transferable
representations optimized for herbicide trials images.
  Our domain-specific model significantly outperforms the best general-purpose
foundation model in both species identification (F1 score improvement from 0.91
to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions
(new locations and other time), it achieves even greater gains (species
identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In
domain-shift scenarios, such as drone imagery, it maintains strong performance
(species classification from 0.49 to 0.60).
  Additionally, we show that domain-specific pretraining enhances segmentation
accuracy, particularly in low-annotation regimes. An annotation-efficiency
analysis reveals that, under unseen conditions, the domain-specific model
achieves 5.4% higher F1 score than the general-purpose model, while using 80%
fewer labeled samples.
  These results demonstrate the generalization capabilities of domain-specific
foundation models and their potential to significantly reduce manual annotation
efforts, offering a scalable and automated solution for herbicide trial
analysis.

</details>


### [81] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

TL;DR: This study uses synthetic and real Sentinel-1 satellite imagery with YOLOv10 models to detect offshore infrastructure, showing that synthetic data improves model performance and geographic transferability.


<details>
  <summary>Details</summary>
Motivation: The expansion of marine infrastructure requires effective monitoring systems, but current detection models struggle with scarce and unbalanced datasets, especially for underrepresented object classes.

Method: Training YOLOv10 object detection models with synthetic and real Sentinel-1 satellite imagery from four regions, then evaluating geographic transferability on three unseen regions using region-holdout evaluation.

Result: The model detected 3,529 offshore platforms across three test regions and achieved an F1 score of 0.85, which improved to 0.90 with synthetic data, demonstrating successful generalization beyond training areas.

Conclusion: Synthetic data generation effectively addresses dataset imbalance challenges in remote sensing and enables globally transferable detection of offshore infrastructure using deep learning.

Abstract: The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [82] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang,Litong Gong,Yinglin Zheng,Yansong Liu,Wentao Jiang,Mingyi Xu,Biao Wang,Tiezheng Ge,Ming Zeng*

Main category: cs.CV

TL;DR: RISE-T2V integrates prompt rephrasing and semantic feature extraction into a single step using a Rephrasing Adapter, enabling text-to-video models to generate higher quality videos from concise prompts by better understanding user intent.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video diffusion models fail to maintain video quality with concise prompts due to limited textual semantics understanding, and cannot rephrase prompts online to better align with user intentions, limiting scalability and usability.

Method: Introduces a Rephrasing Adapter that enables diffusion models to use text hidden states during LLM's next token prediction as video generation condition, implicitly rephrasing basic prompts into comprehensive representations matching user intent.

Result: Extensive experiments show RISE-T2V is a versatile framework applicable to different video diffusion model architectures, significantly enhancing T2V models' ability to generate high-quality videos aligned with user intent.

Conclusion: RISE-T2V provides a universal solution that can be applied to various pre-trained LLMs and video diffusion models, overcoming limitations of current T2V models by integrating prompt rephrasing and semantic understanding in a single seamless process.

Abstract: Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.

</details>


### [83] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: Proposes a two-stage method using voxel sparsification and submanifold sparse convolutional networks for automated 3D tumor segmentation in CT scans, achieving state-of-the-art accuracy with significant computational efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Manual tumor delineation in radiological images is time-consuming and specialized, creating bottlenecks for clinical quantitative analysis. Current 3D segmentation methods face impractical computational demands requiring downsampling or patch-based approaches.

Method: Two-stage approach combining voxel sparsification with submanifold sparse convolutional networks, enabling high-resolution 3D segmentation while reducing computational resource requirements.

Result: Achieved competitive results on KiTS23 renal cancer dataset: 95.8% Dice for kidneys+masses, 85.7% for tumors+cysts, 80.3% for tumors alone. Computational improvements: 60% reduction in inference time and 75% reduction in VRAM usage compared to dense architectures.

Conclusion: The proposed sparse methodology enables high-accuracy 3D tumor segmentation with significantly reduced computational demands, making automated quantitative analysis more feasible for clinical deployment.

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [84] [Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset](https://arxiv.org/abs/2511.04344)
*Muhammad Annas Shaikh,Hamza Zaman,Arbaz Asif*

Main category: cs.CV

TL;DR: Evaluation of 9 CNN architectures for horse/motorcycle classification on VOC 2008 dataset, addressing class imbalance with data augmentation. ConvNeXt-Tiny achieved best performance with 95.53% AP for horses and 89.12% for motorcycles.


<details>
  <summary>Details</summary>
Motivation: To address class imbalance problems in binary classification tasks and evaluate how different neural network architectures perform on imbalanced datasets, specifically for object detection.

Method: Comprehensive evaluation of nine CNN architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and Vision Transformer using minority-class augmentation techniques on VOC 2008 dataset.

Result: Substantial performance variations observed across architectures. ConvNeXt-Tiny achieved highest Average Precision (95.53% for horses, 89.12% for motorcycles). Data augmentation significantly improved minority class detection, especially benefiting deeper architectures.

Conclusion: Provides insights for architecture selection in imbalanced binary classification and quantifies the positive impact of data augmentation strategies in mitigating class imbalance issues in object detection.

Abstract: This paper presents a comprehensive evaluation of nine convolutional neural
network architectures for binary classification of horses and motorcycles in
the VOC 2008 dataset. We address the significant class imbalance problem by
implementing minority-class augmentation techniques. Our experiments compare
modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and
Vision Transformer across multiple performance metrics. Results demonstrate
substantial performance variations, with ConvNeXt-Tiny achieving the highest
Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle
detection. We observe that data augmentation significantly improves minority
class detection, particularly benefiting deeper architectures. This study
provides insights into architecture selection for imbalanced binary
classification tasks and quantifies the impact of data augmentation strategies
in mitigating class imbalance issues in object detection.

</details>


### [85] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar,Tim Brophy,Eoin Martino Grua,Ganesh Sistu,Valentina Donzella,Ciaran Eising*

Main category: cs.CV

TL;DR: BEVFusion architecture analysis reveals LiDAR is more critical than cameras for 3D object detection under sensor occlusions, with LiDAR occlusion causing 26.8% performance drop vs 4.1% for camera occlusion.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of sensor occlusions (from fog, haze, obstructions) on 3D object detection accuracy in BEV-based fusion systems, as this remains underexplored despite BEV's strong performance.

Method: Evaluated BEVFusion architecture on nuScenes dataset, measuring detection performance using mAP and NDS metrics under simulated camera and LiDAR occlusions.

Result: Camera-only detection drops 41.3% under moderate occlusion; LiDAR drops 47.3% under heavy occlusion; fused detection drops 4.1% with camera occlusion vs 26.8% with LiDAR occlusion, showing stronger reliance on LiDAR.

Conclusion: BEVFusion relies more heavily on LiDAR for 3D detection, highlighting need for occlusion-aware evaluation methods and improved fusion techniques to handle sensor degradation in adverse conditions.

Abstract: Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.

</details>


### [86] [A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications](https://arxiv.org/abs/2511.04349)
*Puneet Mishra,Martijntje Vollebregt,Yizhou Ma,Maria Font-i-Furnols*

Main category: cs.CV

TL;DR: This tutorial provides step-by-step guidance for using existing deep learning models to extract spatial features from imaging data in analytical chemistry, addressing the adoption gap despite model availability.


<details>
  <summary>Details</summary>
Motivation: Deep learning can enhance spatial information extraction from imaging data in analytical chemistry, but adoption is limited due to lack of structured guidance for implementation.

Method: Provides MATLAB code tutorials demonstrating how to use existing open-source deep learning models to extract deep features from various imaging modalities without training new models.

Result: The tutorial enables analytical chemists to process imaging data from different modalities and integrate spatial features with other data sources like spectral information.

Conclusion: This work bridges the gap between deep learning model availability and practical implementation in analytical chemistry by providing accessible, step-by-step guidance for spatial feature extraction.

Abstract: Background In analytical chemistry, spatial information about materials is
commonly captured through imaging techniques, such as traditional color cameras
or with advanced hyperspectral cameras and microscopes. However, efficiently
extracting and analyzing this spatial information for exploratory and
predictive purposes remains a challenge, especially when using traditional
chemometric methods. Recent advances in deep learning and artificial
intelligence have significantly enhanced image processing capabilities,
enabling the extraction of multiscale deep features that are otherwise
challenging to capture with conventional image processing techniques. Despite
the wide availability of open-source deep learning models, adoption in
analytical chemistry remains limited because of the absence of structured,
step-by-step guidance for implementing these models.
  Results This tutorial aims to bridge this gap by providing a step-by-step
guide for applying deep learning approaches to extract spatial information from
imaging data and integrating it with other data sources, such as spectral
information. Importantly, the focus of this work is not on training deep
learning models for image processing but on using existing open source models
to extract deep features from imaging data.
  Significance The tutorial provides MATLAB code tutorial demonstrations,
showcasing the processing of imaging data from various imaging modalities
commonly encountered in analytical chemistry. Readers must run the tutorial
steps on their own datasets using the codes presented in this tutorial.

</details>


### [87] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

TL;DR: Multi-task framework using LoRA-tuned Florence-2 model for medical VQA, explanation generation, and visual grounding, achieving improved accuracy and interpretability over single-task baselines.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive medical VQA system that produces both accurate answers and interpretable explanations with visual grounding, addressing the need for trustworthy AI in medical applications.

Method: LoRA-tuned Florence-2 model trained on three curated datasets: Kvasir-VQA-x1 for Q&A, synthetic explanation dataset for medical reasoning, and text-to-region pairs for visual grounding.

Result: Substantial improvements over single-task baselines in both answer accuracy and visual localization, demonstrating the effectiveness of grounded multi-task learning.

Conclusion: Multi-task learning with visual grounding enables medical VQA systems to produce more accurate and interpretable responses, making them more suitable for real-world medical applications.

Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


### [88] [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388)
*Chang Liu,Juan Li,Sheng Zhang,Chang Liu,Jie Li,Xu Zhang*

Main category: cs.CV

TL;DR: BoRe-Depth is a lightweight monocular depth estimation model with 8.7M parameters that achieves high accuracy and improved boundary quality on embedded systems, running at 50.7 FPS on NVIDIA Jetson Orin.


<details>
  <summary>Details</summary>
Motivation: Existing monocular depth estimation methods suffer from poor performance and blurred object boundaries on embedded systems, despite their cost advantage for unmanned systems requiring 3D perception.

Method: Proposes BoRe-Depth with Enhanced Feature Adaptive Fusion Module (EFAF) for adaptive depth feature fusion and boundary enhancement, and integrates semantic knowledge into the encoder to improve object recognition and boundary perception.

Result: The model significantly outperforms previous lightweight models on multiple datasets, achieves 50.7 FPS on NVIDIA Jetson Orin, and demonstrates improved boundary quality in depth estimation.

Conclusion: BoRe-Depth provides an efficient and accurate solution for monocular depth estimation on embedded systems, addressing key challenges of performance and boundary quality while maintaining real-time operation.

Abstract: Depth estimation is one of the key technologies for realizing 3D perception
in unmanned systems. Monocular depth estimation has been widely researched
because of its low-cost advantage, but the existing methods face the challenges
of poor depth estimation performance and blurred object boundaries on embedded
systems. In this paper, we propose a novel monocular depth estimation model,
BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate
depth maps on embedded systems and significantly improves boundary quality.
Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which
adaptively fuses depth features to enhance boundary detail representation.
Secondly, we integrate semantic knowledge into the encoder to improve the
object recognition and boundary perception capabilities. Finally, BoRe-Depth is
deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We
demonstrate that the proposed model significantly outperforms previous
lightweight models on multiple challenging datasets, and we provide detailed
ablation studies for the proposed methods. The code is available at
https://github.com/liangxiansheng093/BoRe-Depth.

</details>


### [89] [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](https://arxiv.org/abs/2511.04394)
*Ke Du,Yimin Peng,Chao Gao,Fan Zhou,Siqiao Xue*

Main category: cs.CV

TL;DR: DORAEMON is an open-source PyTorch library that unifies visual object modeling and representation learning across scales with a YAML-driven workflow, offering 1000+ pretrained models and reproducible recipes.


<details>
  <summary>Details</summary>
Motivation: To consolidate datasets, models, and training techniques into one platform for rapid experimentation in visual recognition and representation learning, bridging research and deployment.

Method: Provides a single YAML-driven workflow covering classification, retrieval and metric learning; exposes 1000+ pretrained backbones through timm-compatible interface with modular losses, augmentations and distributed-training utilities.

Result: Reproducible recipes match or exceed reference results on ImageNet-1K, MS-Celeb-1M and Stanford online products; enables one-command export to ONNX or HuggingFace.

Conclusion: DORAEMON offers a scalable foundation for efficient transfer of research advances to real-world applications by unifying visual object modeling and representation learning.

Abstract: DORAEMON is an open-source PyTorch library that unifies visual object
modeling and representation learning across diverse scales. A single
YAML-driven workflow covers classification, retrieval and metric learning; more
than 1000 pretrained backbones are exposed through a timm-compatible interface,
together with modular losses, augmentations and distributed-training utilities.
Reproducible recipes match or exceed reference results on ImageNet-1K,
MS-Celeb-1M and Stanford online products, while one-command export to ONNX or
HuggingFace bridges research and deployment. By consolidating datasets, models,
and training techniques into one platform, DORAEMON offers a scalable
foundation for rapid experimentation in visual recognition and representation
learning, enabling efficient transfer of research advances to real-world
applications. The repository is available at https://github.com/wuji3/DORAEMON.

</details>


### [90] [HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats](https://arxiv.org/abs/2511.04426)
*Alan de Aguiar,Michaella Pereira Andrade,Charles Morphy D. Santos,João Paulo Gois*

Main category: cs.CV

TL;DR: HideAndSeg is a minimally supervised AI tool that combines SAM2 and YOLOv11 to automatically segment octopuses in videos, using unsupervised metrics for quality evaluation and handling occlusions in natural environments.


<details>
  <summary>Details</summary>
Motivation: Analyzing octopuses in their natural habitats is challenging due to camouflage, rapid appearance changes, non-rigid deformations, occlusions, and variable underwater conditions, with a lack of large-scale annotated datasets.

Method: Integrates SAM2 with custom-trained YOLOv11 detector. Users provide initial point coordinates for SAM2 masks, which train YOLO model. Then automates pipeline with bounding box prompts to SAM2. Uses unsupervised metrics (temporal consistency DICE_t and new component count NC_t) for evaluation.

Result: Achieves satisfactory performance, reduces segmentation noise compared to manual prompting, and successfully re-identifies octopuses after complete occlusion where manual model fails.

Conclusion: Provides a practical tool that reduces manual analysis needs and enables more efficient behavioral studies of wild cephalopods.

Abstract: Analyzing octopuses in their natural habitats is challenging due to their
camouflage capability, rapid changes in skin texture and color, non-rigid body
deformations, and frequent occlusions, all of which are compounded by variable
underwater lighting and turbidity. Addressing the lack of large-scale annotated
datasets, this paper introduces HideAndSeg, a novel, minimally supervised
AI-based tool for segmenting videos of octopuses. It establishes a quantitative
baseline for this task. HideAndSeg integrates SAM2 with a custom-trained
YOLOv11 object detector. First, the user provides point coordinates to generate
the initial segmentation masks with SAM2. These masks serve as training data
for the YOLO model. After that, our approach fully automates the pipeline by
providing a bounding box prompt to SAM2, eliminating the need for further
manual intervention. We introduce two unsupervised metrics - temporal
consistency $DICE_t$ and new component count $NC_t$ - to quantitatively
evaluate segmentation quality and guide mask refinement in the absence of
ground-truth data, i.e., real-world information that serves to train, validate,
and test AI models. Results show that HideAndSeg achieves satisfactory
performance, reducing segmentation noise compared to the manually prompted
approach. Our method can re-identify and segment the octopus even after periods
of complete occlusion in natural environments, a scenario in which the manually
prompted model fails. By reducing the need for manual analysis in real-world
scenarios, this work provides a practical tool that paves the way for more
efficient behavioral studies of wild cephalopods.

</details>


### [91] [Solving Convex Partition Visual Jigsaw Puzzles](https://arxiv.org/abs/2511.04450)
*Yaniv Ohayon,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: This paper presents a computational solver for convex partition jigsaw puzzles, expanding beyond traditional square puzzles by using geometric and pictorial compatibilities with a greedy algorithm approach.


<details>
  <summary>Details</summary>
Motivation: Most existing jigsaw puzzle solvers only handle square puzzles, which severely limits practical applications. The authors aim to expand computational puzzle solving to handle convex partitions - a major subset of polygonal puzzles with convex pieces.

Method: The authors utilize both geometrical and pictorial compatibilities between puzzle pieces and introduce a greedy solver algorithm specifically designed for convex partition puzzles.

Result: The paper reports several performance measures and introduces the first benchmark dataset specifically for convex partition puzzles.

Conclusion: This work significantly expands the types of puzzles that can be handled computationally beyond square jigsaw puzzles, enabling practical applications in various domains through the development of solvers for convex partition puzzles.

Abstract: Jigsaw puzzle solving requires the rearrangement of unordered pieces into
their original pose in order to reconstruct a coherent whole, often an image,
and is known to be an intractable problem. While the possible impact of
automatic puzzle solvers can be disruptive in various application domains, most
of the literature has focused on developing solvers for square jigsaw puzzles,
severely limiting their practical use. In this work, we significantly expand
the types of puzzles handled computationally, focusing on what is known as
Convex Partitions, a major subset of polygonal puzzles whose pieces are convex.
We utilize both geometrical and pictorial compatibilities, introduce a greedy
solver, and report several performance measures next to the first benchmark
dataset of such puzzles.

</details>


### [92] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao,Qiuna Tan,Minghan Yang,Guanting Dong,Peiqing Yang,Shiqiang Lang,Enhui Wan,Xiaowan Wang,Yida Xu,Lan Yang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.CV

TL;DR: V-Thinker is a multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning and automated dataset evolution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating image interaction with long-horizon reasoning in Large Multimodal Models, moving beyond limited visual tool spaces and task-specific workflows.

Method: Uses a Data Evolution Flywheel for automated dataset synthesis/evolution/verification, and Visual Progressive Training Curriculum with two-stage reinforcement learning for perception alignment and interactive reasoning.

Result: V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios on the VTBench benchmark.

Conclusion: The approach provides valuable insights for advancing image-interactive reasoning applications and represents a significant step in vision-centric multimodal reasoning.

Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.

</details>


### [93] [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474)
*Wenwen Li,Sizhe Wang,Hyunho Lee,Chenyan Lu,Sujit Roy,Rahul Ramachandran,Chia-Yu Hsu*

Main category: cs.CV

TL;DR: GeoFMs, particularly Prithvi-EO-2.0, outperform traditional models in landslide mapping across different sensors, regions, and limited data conditions through a three-axis framework of sensor, label, and domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Landslides cause severe damage, but conventional deep learning models struggle with cross-sensor, cross-region applications and limited training data, requiring more robust solutions.

Method: Three-axis analytical framework (sensor, label, domain) for adapting geospatial foundation models, using Prithvi-EO-2.0 with global pretraining, self-supervision, and adaptable fine-tuning.

Result: Consistently outperforms task-specific CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other GeoFMs (TerraMind, SatMAE), showing resilience to spectral variation, accuracy under label scarcity, and better generalization across datasets.

Conclusion: GeoFMs represent a step toward more robust and scalable approaches for landslide risk reduction, though challenges remain in computational cost and limited AI-ready training data availability.

Abstract: Landslides cause severe damage to lives, infrastructure, and the environment,
making accurate and timely mapping essential for disaster preparedness and
response. However, conventional deep learning models often struggle when
applied across different sensors, regions, or under conditions of limited
training data. To address these challenges, we present a three-axis analytical
framework of sensor, label, and domain for adapting geospatial foundation
models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a
series of experiments, we show that it consistently outperforms task-specific
CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other
GeoFMs (TerraMind, SatMAE). The model, built on global pretraining,
self-supervision, and adaptable fine-tuning, proved resilient to spectral
variation, maintained accuracy under label scarcity, and generalized more
reliably across diverse datasets and geographic settings. Alongside these
strengths, we also highlight remaining challenges such as computational cost
and the limited availability of reusable AI-ready training data for landslide
research. Overall, our study positions GeoFMs as a step toward more robust and
scalable approaches for landslide risk reduction and environmental monitoring.

</details>


### [94] [THEval. Evaluation Framework for Talking Head Video Generation](https://arxiv.org/abs/2511.04520)
*Nabyl Quignon,Baptiste Chopin,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

TL;DR: Proposes a comprehensive evaluation framework with 8 metrics across quality, naturalness, and synchronization dimensions to address the gap in talking head video generation assessment.


<details>
  <summary>Details</summary>
Motivation: Current evaluation metrics for talking head generation are limited and lag behind rapid advances in video generation technology, necessitating more comprehensive assessment methods.

Method: Developed an evaluation framework focusing on efficiency and human preference alignment, analyzing fine-grained dynamics of head, mouth, eyebrows, and face quality through 8 specific metrics.

Result: Extensive experiments on 85,000 videos from 17 state-of-the-art models revealed that while algorithms excel in lip synchronization, they struggle with generating expressiveness and artifact-free details.

Conclusion: The proposed benchmark framework provides a comprehensive evaluation tool for talking head generation methods, with plans for public release and regular updates to track field progress.

Abstract: Video generation has achieved remarkable progress, with generated videos
increasingly resembling real ones. However, the rapid advance in generation has
outpaced the development of adequate evaluation metrics. Currently, the
assessment of talking head generation primarily relies on limited metrics,
evaluating general video quality, lip synchronization, and on conducting user
studies. Motivated by this, we propose a new evaluation framework comprising 8
metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
synchronization. In selecting the metrics, we place emphasis on efficiency, as
well as alignment with human preferences. Based on this considerations, we
streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
well as face quality. Our extensive experiments on 85,000 videos generated by
17 state-of-the-art models suggest that while many algorithms excel in lip
synchronization, they face challenges with generating expressiveness and
artifact-free details. These videos were generated based on a novel real
dataset, that we have curated, in order to mitigate bias of training data. Our
proposed benchmark framework is aimed at evaluating the improvement of
generative methods. Original code, dataset and leaderboards will be publicly
released and regularly updated with new methods, in order to reflect progress
in the field.

</details>


### [95] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou,Santiago Barbarisi,Lucy Culshaw,Jayna Patel,Evangelos B. Mazomenos,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: STC-Net is a novel framework for automated surgical complexity assessment in Laparoscopic Cholecystectomy using the Parkland Grading Scale, operating directly on full videos with weak temporal supervision.


<details>
  <summary>Details</summary>
Motivation: Accurate assessment of surgical complexity is crucial in LC as severe inflammation leads to longer operative times and increased complication risks. Current methods lack automation for full video analysis without manual curation.

Method: STC-Net performs joint temporal localization and grading through localization, window proposal, and grading modules. It uses a novel loss combining hard/soft localization objectives and background-aware grading supervision, operating under weak temporal supervision on full videos.

Result: On 1,859 LC videos, STC-Net achieved 62.11% accuracy and 61.42% F1-score, outperforming non-localized baselines by over 10% in both metrics.

Conclusion: STC-Net provides a scalable and effective approach for automated PGS-based surgical complexity estimation from full LC videos, showing promise for post-operative analysis and surgical training.

Abstract: Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.

</details>


### [96] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: Introduces 'Thinking with Video' paradigm using video generation models like Sora-2 to overcome limitations of text-only and image-only reasoning, achieving strong performance on both vision-centric and text-centric tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of 'Thinking with Text' and 'Thinking with Images' paradigms: images capture only single moments and fail to represent dynamic processes, and the separation of text and vision hinders unified multimodal understanding.

Method: Developed VideoThinkBench with vision-centric tasks (e.g., Eyeballing Puzzles) and text-centric tasks (e.g., subsets of GSM8K, MMMU), using Sora-2 video generation model as a unified reasoning framework with self-consistency and in-context learning techniques.

Result: Sora-2 achieves comparable performance to SOTA VLMs on vision-centric tasks, surpasses VLMs on Eyeballing Games, and achieves 92% accuracy on MATH and 75.53% on MMMU for text-centric tasks. Self-consistency and in-context learning improve performance.

Conclusion: Video generation models like Sora-2 demonstrate potential as unified multimodal understanding and generation models, positioning 'thinking with video' as a unified multimodal reasoning paradigm.

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


### [97] [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595)
*Chen Shi,Shaoshuai Shi,Xiaoyang Lyu,Chunyang Liu,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

TL;DR: UniSplat is a feed-forward framework for dynamic 3D scene reconstruction in autonomous driving that addresses sparse, non-overlapping camera views through unified latent spatio-temporal fusion and a dual-branch decoder.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with sparse, non-overlapping camera views and complex scene dynamics in autonomous driving scenarios.

Method: Constructs a 3D latent scaffold using pretrained foundation models, implements efficient spatio-temporal fusion within the scaffold, and uses a dual-branch decoder combining point-anchored refinement with voxel-based generation.

Result: Achieves state-of-the-art performance in novel view synthesis and provides robust, high-quality renderings even for viewpoints outside original camera coverage.

Conclusion: UniSplat enables complete and detailed dynamic scene reconstruction with persistent memory for streaming scene completion beyond current camera coverage.

Abstract: Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,
yet existing methods struggle with the joint challenges of sparse,
non-overlapping camera views and complex scene dynamics. We present UniSplat, a
general feed-forward framework that learns robust dynamic scene reconstruction
through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent
scaffold, a structured representation that captures geometric and semantic
scene context by leveraging pretrained foundation models. To effectively
integrate information across spatial views and temporal frames, we introduce an
efficient fusion mechanism that operates directly within the 3D scaffold,
enabling consistent spatio-temporal alignment. To ensure complete and detailed
reconstructions, we design a dual-branch decoder that generates dynamic-aware
Gaussians from the fused scaffold by combining point-anchored refinement with
voxel-based generation, and maintain a persistent memory of static Gaussians to
enable streaming scene completion beyond current camera coverage. Extensive
experiments on real-world datasets demonstrate that UniSplat achieves
state-of-the-art performance in novel view synthesis, while providing robust
and high-quality renderings even for viewpoints outside the original camera
coverage.

</details>


### [98] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao,Yu Chen,Haoxuan Ma,Jiale Hong,Caorui Li,Lingxiang Wu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: PixCLIP enhances CLIP's fine-grained image-text alignment by incorporating visual prompts and processing long textual descriptions using LLM text encoder, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP models have token length limitations that prevent processing granular textual information in long sequences, while MLLMs show that detailed textual descriptions can improve fine-grained alignment.

Method: Proposes PixCLIP framework with automated annotation pipeline for pixel-level localized long-form descriptions, constructs LongGRIT dataset, replaces CLIP text encoder with LLM, and implements three-branch pixel-text alignment learning.

Result: PixCLIP achieves breakthroughs in pixel-level interaction and long-form text handling, showing state-of-the-art performance.

Conclusion: The proposed approach successfully synergizes visual and textual granularity enhancement, overcoming CLIP's inherent limitations for fine-grained vision-language alignment.

Abstract: While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.

</details>


### [99] [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615)
*Tushar Kataria,Shikha Dubey,Mary Bronner,Jolanta Jedrzkiewicz,Ben J. Brintz,Shireen Y. Elhabian,Beatrice S. Knudsen*

Main category: cs.CV

TL;DR: Proposes an automated framework to evaluate virtual IHC stain quality using stain accuracy metrics instead of conventional image fidelity metrics, showing that paired models outperform unpaired ones.


<details>
  <summary>Details</summary>
Motivation: Current evaluation metrics for virtual IHC stains focus on image fidelity rather than staining accuracy, making it difficult to assess the clinical utility of these models.

Method: Uses color deconvolution to generate masks of IHC-positive pixels from real and virtual IHC images, then computes stain accuracy metrics (Dice, IoU, Hausdorff distance) without needing expert annotations.

Result: Conventional metrics (FID, PSNR, SSIM) correlate poorly with stain accuracy and pathologist assessment. Paired models achieve highest accuracy, while unpaired diffusion/GAN models are less reliable. WSI-level evaluation reveals performance issues not visible in patch-based analysis.

Conclusion: The framework provides a reproducible approach for assessing virtual IHC model quality, which is crucial for clinical translation and routine use by pathologists.

Abstract: Deep learning models can generate virtual immunohistochemistry (IHC) stains
from hematoxylin and eosin (H&E) images, offering a scalable and low-cost
alternative to laboratory IHC. However, reliable evaluation of image quality
remains a challenge as current texture- and distribution-based metrics quantify
image fidelity rather than the accuracy of IHC staining. Here, we introduce an
automated and accuracy grounded framework to determine image quality across
sixteen paired or unpaired image translation models. Using color deconvolution,
we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by
each virtual IHC model. We use the segmented masks of real and virtual IHC to
compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly
quantify correct pixel - level labeling without needing expert manual
annotations. Our results demonstrate that conventional image fidelity metrics,
including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),
and structural similarity (SSIM), correlate poorly with stain accuracy and
pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE
achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based
models are less reliable in providing accurate IHC positive pixel labels.
Moreover, whole-slide images (WSI) reveal performance declines that are
invisible in patch-based evaluations, emphasizing the need for WSI-level
benchmarks. Together, this framework defines a reproducible approach for
assessing the quality of virtual IHC models, a critical step to accelerate
translation towards routine use by pathologists.

</details>


### [100] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla,Alexander Moore,Amar Saini,Carmen Carrano*

Main category: cs.CV

TL;DR: A scalable streaming VQA model that is both no-reference and opinion-unaware, using temporal-aware CNN to predict FR metrics from degraded video without references.


<details>
  <summary>Details</summary>
Motivation: Existing VQA methods have limitations: FR metrics need clean references, NR models require costly human labels, and most opinion-unaware methods ignore temporal context crucial for video.

Method: Leverages synthetic degradations of DAVIS dataset to train temporal-aware convolutional architecture that predicts FR metrics (LPIPS, PSNR, SSIM) directly from degraded video without references.

Result: Outperforms image-based baseline by generalizing across diverse degradations, and achieves higher correlation with FR metrics compared to BRISQUE baseline.

Conclusion: Temporal modeling is valuable for scalable VQA in real-world vision systems, and the opinion-unaware temporal approach is effective.

Abstract: Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.

</details>


### [101] [Polarization-resolved imaging improves eye tracking](https://arxiv.org/abs/2511.04652)
*Mantas Žurauskas,Tom Bu,Sanaz Alali,Beyza Kalkanli,Derek Shi,Fernando Alamos,Gauresh Pandit,Christopher Mei,Ali Behrooz,Ramin Mirjalili,Dave Stronks,Alexander Fix,Dmitri Model*

Main category: cs.CV

TL;DR: Polarization-enabled eye tracking (PET) uses polarized near-infrared imaging to improve gaze tracking accuracy by 10-16% compared to intensity-only methods, especially under challenging conditions like eyelid occlusions.


<details>
  <summary>Details</summary>
Motivation: To enhance eye tracking by adding polarization contrast to traditional intensity-based imaging, enabling better feature detection on ocular tissues like sclera and cornea.

Method: Uses a polarization-filter-array camera with linearly polarized near-infrared illuminator to capture polarization state of light reflected from ocular tissues, combined with convolutional neural network models trained on data from 346 participants.

Result: Reduced median 95th-percentile absolute gaze error by 10-16% relative to intensity-only baselines, particularly effective under eyelid occlusions, eye-relief changes, and pupil-size variations.

Conclusion: PET provides practical gains in human-computer interaction and represents a simple, robust sensing modality suitable for future wearable devices.

Abstract: Polarization-resolved near-infrared imaging adds a useful optical contrast
mechanism to eye tracking by measuring the polarization state of light
reflected by ocular tissues in addition to its intensity. In this paper we
demonstrate how this contrast can be used to enable eye tracking. Specifically,
we demonstrate that a polarization-enabled eye tracking (PET) system composed
of a polarization--filter--array camera paired with a linearly polarized
near-infrared illuminator can reveal trackable features across the sclera and
gaze-informative patterns on the cornea, largely absent in intensity-only
images. Across a cohort of 346 participants, convolutional neural network based
machine learning models trained on data from PET reduced the median
95th-percentile absolute gaze error by 10--16\% relative to capacity-matched
intensity baselines under nominal conditions and in the presence of eyelid
occlusions, eye-relief changes, and pupil-size variation. These results link
light--tissue polarization effects to practical gains in human--computer
interaction and position PET as a simple, robust sensing modality for future
wearable devices.

</details>


### [102] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown,Jihan Yang,Shusheng Yang,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: The paper proposes a framework to diagnose and debias multimodal benchmarks by identifying non-visual shortcuts that models exploit, using test-set stress-testing and iterative bias pruning.


<details>
  <summary>Details</summary>
Motivation: Current multimodal benchmarks can be gamed without strong visual understanding due to biases, linguistic priors, and superficial patterns, undermining their reliability for evaluating true visual capabilities.

Method: Two-component framework: 1) Test-set Stress-Test (TsT) using k-fold cross-validation on textual inputs to reveal shortcuts and assign bias scores; 2) Iterative Bias Pruning (IBP) to filter high-bias samples.

Result: Applied to four benchmarks (VSI-Bench, CV-Bench, MMMU, VideoMME), revealing pervasive non-visual biases. Created VSI-Bench-Debiased showing reduced non-visual solvability and wider vision-blind performance gap.

Conclusion: Benchmark designers should proactively game their own tests using diagnostic procedures to identify and mitigate non-visual biases, ensuring benchmarks truly measure visual understanding rather than exploiting shortcuts.

Abstract: Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.

</details>


### [103] [SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding](https://arxiv.org/abs/2511.04668)
*Ellis Brown,Arijit Ray,Ranjay Krishna,Ross Girshick,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: SIMS-V uses 3D simulators to generate spatially-rich video training data for multimodal language models, enabling efficient spatial reasoning training with minimal question types that outperforms larger models on real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal language models struggle with spatial reasoning across time and space, and obtaining diverse real-world video data with precise spatial annotations is a bottleneck.

Method: A systematic data-generation framework leveraging 3D simulators' privileged information to create spatially-rich video training data, with systematic ablations of question types, mixes, and scales.

Result: Identified three key question categories (metric measurement, perspective-dependent reasoning, temporal tracking) that enable efficient training; a 7B-parameter model trained on 25K simulated examples outperforms 72B baseline and achieves competitive performance with proprietary models on real-world spatial reasoning benchmarks.

Conclusion: Simulated data can effectively develop transferable spatial intelligence, enabling robust generalization to real-world spatial tasks while maintaining general video understanding performance.

Abstract: Despite impressive high-level video comprehension, multimodal language models
struggle with spatial reasoning across time and space. While current spatial
training approaches rely on real-world video data, obtaining diverse footage
with precise spatial annotations remains a bottleneck. To alleviate this
bottleneck, we present SIMS-V -- a systematic data-generation framework that
leverages the privileged information of 3D simulators to create spatially-rich
video training data for multimodal language models. Using this framework, we
investigate which properties of simulated data drive effective real-world
transfer through systematic ablations of question types, mixes, and scales. We
identify a minimal set of three question categories (metric measurement,
perspective-dependent reasoning, and temporal tracking) that prove most
effective for developing transferable spatial intelligence, outperforming
comprehensive coverage despite using fewer question types. These insights
enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
25K simulated examples outperforms the larger 72B baseline and achieves
competitive performance with proprietary models on rigorous real-world spatial
reasoning benchmarks. Our approach demonstrates robust generalization,
maintaining performance on general video understanding while showing
substantial improvements on embodied and real-world spatial tasks.

</details>


### [104] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: The paper proposes a shift from reactive multimodal systems to 'supersensing' - a broader paradigm for spatial intelligence involving semantic perception, event cognition, 3D spatial reasoning, and predictive world modeling.


<details>
  <summary>Details</summary>
Motivation: Current multimodal AI systems are limited to reactive, task-driven approaches and brute-force context expansion, failing to achieve true spatial intelligence and world modeling capabilities.

Method: Introduces VSI-SUPER benchmark with VSR (visual spatial recall) and VSC (visual spatial counting) tasks, curates VSI-590K dataset, trains Cambrian-S model, and proposes predictive sensing with self-supervised next-latent-frame prediction using surprise-driven memory.

Result: Achieved +30% improvement on VSI-Bench while maintaining general capabilities, but performance on VSI-SUPER remains limited, showing scale alone is insufficient. Predictive sensing approach substantially outperforms proprietary baselines on VSI-SUPER.

Conclusion: Spatial supersensing requires models that anticipate, select, and organize experience through predictive world modeling, not just scale or reactive processing.

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


### [105] [InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation](https://arxiv.org/abs/2511.04675)
*Jinlai Liu,Jian Han,Bin Yan,Hui Wu,Fengda Zhu,Xing Wang,Yi Jiang,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityStar is a unified spacetime autoregressive framework for high-resolution image and video synthesis that outperforms existing autoregressive models and some diffusion competitors, generating 720p videos 10x faster than diffusion methods.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework that jointly captures spatial and temporal dependencies for various generation tasks (text-to-image, text-to-video, image-to-video, long interactive video) using autoregressive modeling.

Method: A purely discrete autoregressive approach that jointly models spatial and temporal dependencies within a single architecture, supporting multiple generation tasks through straightforward temporal autoregression.

Result: Achieves 83.74 on VBench, outperforming all autoregressive models by large margins and surpassing some diffusion competitors like HunyuanVideo. Generates 5s, 720p videos approximately 10x faster than leading diffusion-based methods.

Conclusion: InfinityStar is the first discrete autoregressive video generator capable of producing industrial-level 720p videos, demonstrating superior performance and efficiency compared to existing approaches.

Abstract: We introduce InfinityStar, a unified spacetime autoregressive framework for
high-resolution image and dynamic video synthesis. Building on the recent
success of autoregressive modeling in both vision and language, our purely
discrete approach jointly captures spatial and temporal dependencies within a
single architecture. This unified design naturally supports a variety of
generation tasks such as text-to-image, text-to-video, image-to-video, and long
interactive video synthesis via straightforward temporal autoregression.
Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,
outperforming all autoregressive models by large margins, even surpassing some
diffusion competitors like HunyuanVideo. Without extra optimizations, our model
generates a 5s, 720p video approximately 10x faster than leading
diffusion-based methods. To our knowledge, InfinityStar is the first discrete
autoregressive video generator capable of producing industrial level 720p
videos. We release all code and models to foster further research in efficient,
high-quality video generation.

</details>


### [106] [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678)
*Yihong Sun,Xinyu Yang,Jennifer J. Sun,Bharath Hariharan*

Main category: cs.CV

TL;DR: Track Any State: A zero-shot system called TubeletGraph that tracks objects through transformations while detecting and describing state changes, achieving SOTA performance on the VOST-TAS benchmark.


<details>
  <summary>Details</summary>
Motivation: Real-world objects frequently undergo state transformations (e.g., apple being cut, butterfly emerging), but existing tracking methods lose track after significant appearance changes.

Method: TubeletGraph system identifies overlooked tracks using semantic and proximity priors, integrates them, and generates state graphs describing object transformations over time.

Result: Achieves state-of-the-art tracking performance under transformations while demonstrating deep understanding of object transformations and capabilities in temporal grounding and semantic reasoning.

Conclusion: TubeletGraph successfully addresses the limitation of tracking objects through transformations and provides a comprehensive framework for understanding object state evolution.

Abstract: Real-world objects frequently undergo state transformations. From an apple
being cut into pieces to a butterfly emerging from its cocoon, tracking through
these changes is important for understanding real-world objects and dynamics.
However, existing methods often lose track of the target object after
transformation, due to significant changes in object appearance. To address
this limitation, we introduce the task of Track Any State: tracking objects
through transformations while detecting and describing state changes,
accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we
present TubeletGraph, a zero-shot system that recovers missing objects after
transformation and maps out how object states are evolving over time.
TubeletGraph first identifies potentially overlooked tracks, and determines
whether they should be integrated based on semantic and proximity priors. Then,
it reasons about the added tracks and generates a state graph describing each
observed transformation. TubeletGraph achieves state-of-the-art tracking
performance under transformations, while demonstrating deeper understanding of
object transformations and promising capabilities in temporal grounding and
semantic reasoning for complex object transformations. Code, additional
results, and the benchmark dataset are available at
https://tubelet-graph.github.io.

</details>


### [107] [Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping](https://arxiv.org/abs/2511.04680)
*Rafe Loya,Andrew Hamara,Benjamin Estell,Benjamin Kilpatrick,Andrew C. Freeman*

Main category: cs.CV

TL;DR: This paper addresses the problem of generating multiple distinct aesthetic crops from images, motivated by social media applications, and introduces a new dataset with human labels.


<details>
  <summary>Details</summary>
Motivation: Modern social media applications require multiple distinct crops from single images, but existing methods focus only on producing singular crops.

Method: Evaluated several single-crop models combined with an image partitioning algorithm as pre-processing step, using a new dataset of 277 images with human labels.

Result: Introduced a dataset of 277 relevant images with human labels, available publicly for research purposes.

Conclusion: The paper establishes the importance of multiple crop generation for social media applications and provides a benchmark dataset for future research.

Abstract: Automatic image cropping is a method for maximizing the human-perceived
quality of cropped regions in photographs. Although several works have proposed
techniques for producing singular crops, little work has addressed the problem
of producing multiple, distinct crops with aesthetic appeal. In this paper, we
motivate the problem with a discussion on modern social media applications,
introduce a dataset of 277 relevant images and human labels, and evaluate the
efficacy of several single-crop models with an image partitioning algorithm as
a pre-processing step. The dataset is available at
https://github.com/RafeLoya/carousel.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [108] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: An AI-powered chatbot for BRAC University students that combines BM25 lexical ranking with ChromaDB semantic retrieval, using LLaMA-3.3-70B to provide personalized mentoring and guidance.


<details>
  <summary>Details</summary>
Motivation: University students lack personalized on-demand guidance at scale, with existing digital tools failing to provide customized coaching for newcomers.

Method: Hybrid retrieval approach combining BM25 lexical ranking with ChromaDB semantic retrieval, using LLaMA-3.3-70B for response generation, with an efficient data ingestion pipeline from CSV files and university webpages.

Result: High semantic relevance with BERTScore of 0.831 and METEOR score of 0.809; efficient data pipeline (106.82 seconds for updates vs 368.62 seconds for new data).

Conclusion: The chatbot effectively helps students with queries, university life understanding, and semester planning in the open-credit university system.

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [109] [Black-Box Guardrail Reverse-engineering Attack](https://arxiv.org/abs/2511.04215)
*Hongwei Yao,Yun Xia,Shuo Shao,Haoran Shi,Tong Qiao,Cong Wang*

Main category: cs.CR

TL;DR: This paper presents GRA, a reinforcement learning-based framework that reverse-engineers LLM guardrails using genetic algorithm-driven data augmentation to approximate victim guardrail policies with high accuracy and low cost.


<details>
  <summary>Details</summary>
Motivation: LLM guardrails create observable decision patterns that introduce new vulnerabilities, making it possible to reverse-engineer safety mechanisms and bypass ethical constraints.

Method: Proposed GRA framework uses reinforcement learning with genetic algorithm-driven data augmentation, iteratively collecting input-output pairs, prioritizing divergence cases, and applying targeted mutations and crossovers to approximate guardrail policies.

Result: Achieved rule matching rate exceeding 0.92 on three commercial systems (ChatGPT, DeepSeek, Qwen3) with less than $85 in API costs, demonstrating practical feasibility of guardrail extraction.

Conclusion: Current LLM guardrail designs have critical vulnerabilities that enable practical reverse-engineering attacks, highlighting urgent need for more robust defense mechanisms in LLM deployment.

Abstract: Large language models (LLMs) increasingly employ guardrails to enforce
ethical, legal, and application-specific constraints on their outputs. While
effective at mitigating harmful responses, these guardrails introduce a new
class of vulnerabilities by exposing observable decision patterns. In this
work, we present the first study of black-box LLM guardrail reverse-engineering
attacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement
learning-based framework that leverages genetic algorithm-driven data
augmentation to approximate the decision-making policy of victim guardrails. By
iteratively collecting input-output pairs, prioritizing divergence cases, and
applying targeted mutations and crossovers, our method incrementally converges
toward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on
three widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3,
and demonstrate that it achieves an rule matching rate exceeding 0.92 while
requiring less than $85 in API costs. These findings underscore the practical
feasibility of guardrail extraction and highlight significant security risks
for current LLM safety mechanisms. Our findings expose critical vulnerabilities
in current guardrail designs and highlight the urgent need for more robust
defense mechanisms in LLM deployment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [110] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: A collaborative multi-agent framework for automatic math question generation that uses multiple agents to iteratively refine questions, improving control over complexity and cognitive demands.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based models struggle to precisely control problem complexity and cognitive demands in automatic question generation for mathematics education.

Method: A collaborative multi-agent framework that incorporates inference-time computation, where multiple agents iteratively refine generated question-answer pairs to balance complexity and cognitive demand.

Result: Preliminary evaluations show improved quality of educational content with better balance between cognitive challenge and clarity, meeting five meta-evaluation criteria: relevance, importance, clarity, difficulty matching, and answerability.

Conclusion: Collaborative multi-agent workflows can produce more controlled, pedagogically valuable content that advances automated educational content generation and adaptive learning environments.

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [111] [Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study](https://arxiv.org/abs/2511.03876)
*Jinyuxuan Guo,Gurnoor Singh Khurana,Alejandro Gonzalo Grande,Juan C. del Alamo,Francisco Contijoch*

Main category: eess.IV

TL;DR: SinoFlow uses sinogram data directly for CT-based blood flow estimation, outperforming traditional image-based methods by avoiding reconstruction errors and working better with various CT settings.


<details>
  <summary>Details</summary>
Motivation: CT imaging is widely used for cardiovascular assessment but lacks direct methods to estimate blood flow velocity from contrast evolution movies. Current approaches using reconstructed images introduce errors.

Method: Used computational fluid dynamics to generate pulsatile flow in 2D vessel bifurcation, simulated CT scans with varying parameters, and compared PINN-based flow estimation using reconstructed images (ImageFlow) vs. direct sinogram data (SinoFlow).

Result: SinoFlow significantly improved flow estimation by avoiding filtered backprojection errors, was robust across gantry rotation speeds, produced lower errors than ImageFlow, and worked well with pulsed-mode imaging.

Conclusion: SinoFlow demonstrates strong potential for CT-based flow estimation, providing a more accurate non-invasive blood flow assessment method that can inform future PINN applications in medical imaging.

Abstract: Background: Non-invasive imaging-based assessment of blood flow plays a
critical role in evaluating heart function and structure. Computed Tomography
(CT) is a widely-used imaging modality that can robustly evaluate
cardiovascular anatomy and function, but direct methods to estimate blood flow
velocity from movies of contrast evolution have not been developed.
  Purpose: This study evaluates the impact of CT imaging on Physics-Informed
Neural Networks (PINN)-based flow estimation and proposes an improved
framework, SinoFlow, which uses sinogram data directly to estimate blood flow.
  Methods: We generated pulsatile flow fields in an idealized 2D vessel
bifurcation using computational fluid dynamics and simulated CT scans with
varying gantry rotation speeds, tube currents, and pulse mode imaging settings.
We compared the performance of PINN-based flow estimation using reconstructed
images (ImageFlow) to SinoFlow.
  Results: SinoFlow significantly improved flow estimation performance by
avoiding propagating errors introduced by filtered backprojection. SinoFlow was
robust across all tested gantry rotation speeds and consistently produced lower
mean squared error and velocity errors than ImageFlow. Additionally, SinoFlow
was compatible with pulsed-mode imaging and maintained higher accuracy with
shorter pulse widths.
  Conclusions: This study demonstrates the potential of SinoFlow for CT-based
flow estimation, providing a more promising approach for non-invasive blood
flow assessment. The findings aim to inform future applications of PINNs to CT
images and provide a solution for image-based estimation, with reasonable
acquisition parameters yielding accurate flow estimates.

</details>


### [112] [Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images](https://arxiv.org/abs/2511.03890)
*Linchen Qian,Jiasong Chen,Ruonan Gong,Wei Sun,Minliang Liu,Liang Liang*

Main category: eess.IV

TL;DR: A template-fitting pipeline with deep neural networks generates structured quadrilateral meshes from 3D CT images for aortic valve modeling, ensuring consistent correspondence and high mesh quality across patients.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches produce irregular triangular meshes with poor element quality and inconsistent correspondence due to anatomical variations between patients, making biomechanical analysis and patient-specific simulations challenging.

Method: A template-fitting pipeline using deep neural networks that remeshes aortic valves with a common quad mesh template, employing a simplified loss function with only geometry reconstruction and smoothness regularization terms.

Result: The approach produces high-quality aortic valve surface meshes with improved smoothness and shape quality, requiring fewer explicit regularization terms than traditional methods.

Conclusion: Using structured quad meshes for templates and neural network training ensures mesh correspondence and quality while simplifying the training process, enhancing the effectiveness and efficiency of aortic valve modeling.

Abstract: Accurate geometric modeling of the aortic valve from 3D CT images is
essential for biomechanical analysis and patient-specific simulations to assess
valve health or make a preoperative plan. However, it remains challenging to
generate aortic valve meshes with both high-quality and consistency across
different patients. Traditional approaches often produce triangular meshes with
irregular topologies, which can result in poorly shaped elements and
inconsistent correspondence due to inter-patient anatomical variation. In this
work, we address these challenges by introducing a template-fitting pipeline
with deep neural networks to generate structured quad (i.e., quadrilateral)
meshes from 3D CT images to represent aortic valve geometries. By remeshing
aortic valves of all patients with a common quad mesh template, we ensure a
uniform mesh topology with consistent node-to-node and element-to-element
correspondence across patients. This consistency enables us to simplify the
learning objective of the deep neural networks, by employing a loss function
with only two terms (i.e., a geometry reconstruction term and a smoothness
regularization term), which is sufficient to preserve mesh smoothness and
element quality. Our experiments demonstrate that the proposed approach
produces high-quality aortic valve surface meshes with improved smoothness and
shape quality, while requiring fewer explicit regularization terms compared to
the traditional methods. These results highlight that using structured quad
meshes for the template and neural network training not only ensures mesh
correspondence and quality but also simplifies the training process, thus
enhancing the effectiveness and efficiency of aortic valve modeling.

</details>


### [113] [$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography via Implicit Neural Representation](https://arxiv.org/abs/2511.04510)
*Shihan Zhao,Jianru Zhang,Yanan Wu,Linlin Li,Siyuan Shen,Xingjun Zhu,Guoyan Zheng,Jiahua Jiang,Wuwei Ren*

Main category: eess.IV

TL;DR: μNeuFMT is a self-supervised FMT reconstruction framework that jointly optimizes fluorescence distribution and optical properties, eliminating need for precise prior knowledge of tissue optics or pre-conditioned training data.


<details>
  <summary>Details</summary>
Motivation: FMT reconstruction is challenging due to ill-posedness and reliance on inaccurate/unknown tissue optical properties. Supervised deep learning methods have limited generalization beyond training data.

Method: Integrates implicit neural-based scene representation with explicit physical modeling of photon propagation. Jointly optimizes both fluorescence distribution and optical properties (μ) during reconstruction.

Result: Robustly recovers accurate fluorophore distributions and optical coefficients even with severely erroneous initial values (0.5× to 2× of ground truth). Outperforms conventional and supervised deep learning approaches across diverse heterogeneous scenarios.

Conclusion: Establishes a new paradigm for robust and accurate FMT reconstruction, paving the way for more reliable molecular imaging in complex clinically related scenarios like fluorescence guided surgery.

Abstract: Fluorescence Molecular Tomography (FMT) is a promising technique for
non-invasive 3D visualization of fluorescent probes, but its reconstruction
remains challenging due to the inherent ill-posedness and reliance on
inaccurate or often-unknown tissue optical properties. While deep learning
methods have shown promise, their supervised nature limits generalization
beyond training data. To address these problems, we propose $\mu$NeuFMT, a
self-supervised FMT reconstruction framework that integrates implicit
neural-based scene representation with explicit physical modeling of photon
propagation. Its key innovation lies in jointly optimize both the fluorescence
distribution and the optical properties ($\mu$) during reconstruction,
eliminating the need for precise prior knowledge of tissue optics or
pre-conditioned training data. We demonstrate that $\mu$NeuFMT robustly
recovers accurate fluorophore distributions and optical coefficients even with
severely erroneous initial values (0.5$\times$ to 2$\times$ of ground truth).
Extensive numerical, phantom, and in vivo validations show that $\mu$NeuFMT
outperforms conventional and supervised deep learning approaches across diverse
heterogeneous scenarios. Our work establishes a new paradigm for robust and
accurate FMT reconstruction, paving the way for more reliable molecular imaging
in complex clinically related scenarios, such as fluorescence guided surgery.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [114] [MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI](https://arxiv.org/abs/2511.03731)
*Fengming Liu,Shubin Yu*

Main category: cs.HC

TL;DR: MimiTalk is a dual-agent constitutional AI framework for ethical conversational data collection that reduces interview anxiety and outperforms human interviews in information richness and coherence while maintaining ethical standards.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable and ethical framework for conversational data collection in social science research that addresses limitations of traditional human interviews while ensuring quality and replicability.

Method: Dual-agent constitutional AI framework with supervisor model for oversight and conversational model for question generation. Three studies: usability evaluation (20 participants), comparison with human interviews using NLP metrics and propensity score matching (121 AI vs 1,271 human interviews), and blind thematic analysis by interdisciplinary researchers.

Result: MimiTalk reduces interview anxiety, maintains conversational coherence, and outperforms human interviews in information richness, coherence, and stability. AI interviews excel at eliciting technical insights and candid views on sensitive topics, while human interviews better capture cultural and emotional nuances.

Conclusion: Dual-agent constitutional AI supports effective human-AI collaboration, enabling replicable, scalable and quality-controlled qualitative research that complements traditional human interviews.

Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for
scalable and ethical conversational data collection in social science research.
The framework integrates a supervisor model for strategic oversight and a
conversational model for question generation. We conducted three studies: Study
1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews
to 1,271 human interviews from the MediaSum dataset using NLP metrics and
propensity score matching; Study 3 involved 10 interdisciplinary researchers
conducting both human and AI interviews, followed by blind thematic analysis.
Results across studies indicate that MimiTalk reduces interview anxiety,
maintains conversational coherence, and outperforms human interviews in
information richness, coherence, and stability. AI interviews elicit technical
insights and candid views on sensitive topics, while human interviews better
capture cultural and emotional nuances. These findings suggest that dual-agent
constitutional AI supports effective human-AI collaboration, enabling
replicable, scalable and quality-controlled qualitative research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [115] [GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies](https://arxiv.org/abs/2511.04357)
*Maëlic Neau,Zoe Falomir,Paulo E. Santos,Anne-Gwenn Bosser,Cédric Buche*

Main category: cs.RO

TL;DR: GraSP-VLA is a neuro-symbolic framework that uses Continuous Scene Graphs to generate symbolic representations from human demonstrations, enabling automatic planning domain generation and orchestrating low-level Vision-Language Action policies for long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches have limitations: Vision-Language Action models lack high-level symbolic planning for long-horizon tasks, while symbolic Action Model Learning approaches lack generalization and scalability.

Method: Uses Continuous Scene Graph representation to generate symbolic representations from human demonstrations, which then generates new planning domains during inference and orchestrates low-level VLA policies.

Result: Effective for modeling symbolic representations in automatic planning domain generation from observations, and shows potential for orchestrating low-level VLA policies in long-horizon real-world tasks.

Conclusion: GraSP-VLA successfully bridges the gap between neuro-symbolic approaches, scaling up the number of actions that can be reproduced in sequence and addressing limitations of both pure VLA and symbolic AML methods.

Abstract: Deploying autonomous robots that can learn new skills from demonstrations is
an important challenge of modern robotics. Existing solutions often apply
end-to-end imitation learning with Vision-Language Action (VLA) models or
symbolic approaches with Action Model Learning (AML). On the one hand, current
VLA models are limited by the lack of high-level symbolic planning, which
hinders their abilities in long-horizon tasks. On the other hand, symbolic
approaches in AML lack generalization and scalability perspectives. In this
paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that
uses a Continuous Scene Graph representation to generate a symbolic
representation of human demonstrations. This representation is used to generate
new planning domains during inference and serves as an orchestrator for
low-level VLA policies, scaling up the number of actions that can be reproduced
in a row. Our results show that GraSP-VLA is effective for modeling symbolic
representations on the task of automatic planning domain generation from
observations. In addition, results on real-world experiments show the potential
of our Continuous Scene Graph representation to orchestrate low-level VLA
policies in long-horizon tasks.

</details>


### [116] [Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/abs/2511.04555)
*Tao Lin,Yilei Zhong,Yuxin Du,Jingjing Zhang,Jiting Liu,Yinxinyu Chen,Encheng Gu,Ziyan Liu,Hongyi Cai,Yanwen Zou,Lixing Zou,Zhaoye Zhou,Gen Li,Bo Zhao*

Main category: cs.RO

TL;DR: Evo-1 is a lightweight Vision-Language-Action model that achieves state-of-the-art performance without robot data pretraining, using only 0.77B parameters while maintaining high inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Current VLA models are computationally expensive, require massive robot data pretraining, and often degrade perceptual representations, limiting their deployability and generalization.

Method: Builds on native multimodal VLM with cross-modulated diffusion transformer and optimized integration module, using two-stage training that progressively aligns action with perception while preserving VLM representations.

Result: Achieves SOTA on Meta-World (12.4% improvement) and RoboTwin (6.9% improvement), 94.8% on LIBERO, and 78% success rate in real-world with high inference frequency and low memory overhead.

Conclusion: Evo-1 demonstrates that lightweight VLA models can achieve strong performance without robot data pretraining, offering efficient deployment for real-time robotic applications.

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that
unifies perception, language, and control, enabling robots to perform diverse
tasks through multimodal understanding. However, current VLA models typically
contain massive parameters and rely heavily on large-scale robot data
pretraining, leading to high computational costs during training, as well as
limited deployability for real-time inference. Moreover, most training
paradigms often degrade the perceptual representations of the vision-language
backbone, resulting in overfitting and poor generalization to downstream tasks.
In this work, we present Evo-1, a lightweight VLA model that reduces
computation and improves deployment efficiency, while maintaining strong
performance without pretraining on robot data. Evo-1 builds on a native
multimodal Vision-Language model (VLM), incorporating a novel cross-modulated
diffusion transformer along with an optimized integration module, together
forming an effective architecture. We further introduce a two-stage training
paradigm that progressively aligns action with perception, preserving the
representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1
achieves state-of-the-art results on the Meta-World and RoboTwin suite,
surpassing the previous best models by 12.4% and 6.9%, respectively, and also
attains a competitive result of 94.8% on LIBERO. In real-world evaluations,
Evo-1 attains a 78% success rate with high inference frequency and low memory
overhead, outperforming all baseline methods. We release code, data, and model
weights to facilitate future research on lightweight and efficient VLA models.

</details>


### [117] [Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions](https://arxiv.org/abs/2511.04665)
*Kaifeng Zhang,Shuo Sha,Hanxiao Jiang,Matthew Loper,Hyunjong Song,Guangyan Cai,Zhuo Xu,Xiaochen Hu,Changxi Zheng,Yunzhu Li*

Main category: cs.RO

TL;DR: A real-to-sim framework using 3D Gaussian Splatting creates photorealistic digital twins from real videos to evaluate robotic manipulation policies for deformable objects, showing strong correlation between simulated and real-world performance.


<details>
  <summary>Details</summary>
Motivation: Real-world evaluation of robotic manipulation policies is costly and difficult to reproduce, especially for deformable objects, while existing simulators fail to capture the visual and physical complexity of soft-body interactions.

Method: Constructs soft-body digital twins from real-world videos using 3D Gaussian Splatting to render robots, objects, and environments with photorealistic fidelity.

Result: Validated on deformable manipulation tasks (plush toy packing, rope routing, T-block pushing), showing simulated rollouts strongly correlate with real-world execution performance and reveal key behavioral patterns.

Conclusion: Combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies.

Abstract: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

</details>


### [118] [X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations](https://arxiv.org/abs/2511.04671)
*Maximus A. Pace,Prithwish Dan,Chuanruo Ning,Atiksh Bhardwaj,Audrey Du,Edward W. Duan,Wei-Chiu Ma,Kushal Kedia*

Main category: cs.RO

TL;DR: X-Diffusion is a framework that enables effective use of human video demonstrations for robot learning by leveraging diffusion processes to filter out embodiment-specific motion differences while preserving high-level task guidance.


<details>
  <summary>Details</summary>
Motivation: Human videos are abundant training data for robots, but direct kinematic retargeting produces physically infeasible motions due to embodiment differences between humans and robots.

Method: Uses forward diffusion process: trains a classifier to distinguish human vs robot actions, then incorporates human actions only after adding sufficient noise where the classifier cannot discern embodiment. Robot actions supervise fine-grained denoising, human actions provide coarse guidance at higher noise levels.

Result: X-Diffusion achieves 16% higher average success rate across five manipulation tasks compared to best baseline, while naive co-training degrades performance.

Conclusion: The framework successfully leverages human data without learning infeasible motions by exploiting diffusion noise to filter embodiment differences while preserving task-level guidance.

Abstract: Human videos can be recorded quickly and at scale, making them an appealing
source of training data for robot learning. However, humans and robots differ
fundamentally in embodiment, resulting in mismatched action execution. Direct
kinematic retargeting of human hand motion can therefore produce actions that
are physically infeasible for robots. Despite these low-level differences,
human demonstrations provide valuable motion cues about how to manipulate and
interact with objects. Our key idea is to exploit the forward diffusion
process: as noise is added to actions, low-level execution differences fade
while high-level task guidance is preserved. We present X-Diffusion, a
principled framework for training diffusion policies that maximally leverages
human data without learning dynamically infeasible motions. X-Diffusion first
trains a classifier to predict whether a noisy action is executed by a human or
robot. Then, a human action is incorporated into policy training only after
adding sufficient noise such that the classifier cannot discern its embodiment.
Actions consistent with robot execution supervise fine-grained denoising at low
noise levels, while mismatched human actions provide only coarse guidance at
higher noise levels. Our experiments show that naive co-training under
execution mismatches degrades policy performance, while X-Diffusion
consistently improves it. Across five manipulation tasks, X-Diffusion achieves
a 16% higher average success rate than the best baseline. The project website
is available at https://portal-cornell.github.io/X-Diffusion/.

</details>


### [119] [GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction](https://arxiv.org/abs/2511.04679)
*Qingzhou Lu,Yao Feng,Baiyu Shi,Michael Piseno,Zhenan Bao,C. Karen Liu*

Main category: cs.RO

TL;DR: GentleHumanoid integrates impedance control into whole-body motion tracking to achieve upper-body compliance for humanoid robots, enabling safe and natural physical interactions.


<details>
  <summary>Details</summary>
Motivation: Current RL policies for humanoid robots emphasize rigid tracking and suppress external forces, lacking compliance needed for safe human-robot interaction in human-centered environments.

Method: Uses a unified spring-based formulation that models both resistive contacts (restoring forces) and guiding contacts (pushes/pulls from human data), ensuring kinematically consistent forces across shoulder, elbow, and wrist with task-adjustable force thresholds.

Result: Evaluated in simulation and on Unitree G1 humanoid, the policy consistently reduces peak contact forces while maintaining task success in gentle hugging, sit-to-stand assistance, and safe object manipulation tasks.

Conclusion: The framework enables smoother, more natural human-robot interactions and represents progress toward humanoid robots that can safely collaborate with humans and handle objects in real-world environments.

Abstract: Humanoid robots are expected to operate in human-centered environments where
safe and natural physical interaction is essential. However, most recent
reinforcement learning (RL) policies emphasize rigid tracking and suppress
external forces. Existing impedance-augmented approaches are typically
restricted to base or end-effector control and focus on resisting extreme
forces rather than enabling compliance. We introduce GentleHumanoid, a
framework that integrates impedance control into a whole-body motion tracking
policy to achieve upper-body compliance. At its core is a unified spring-based
formulation that models both resistive contacts (restoring forces when pressing
against surfaces) and guiding contacts (pushes or pulls sampled from human
motion data). This formulation ensures kinematically consistent forces across
the shoulder, elbow, and wrist, while exposing the policy to diverse
interaction scenarios. Safety is further supported through task-adjustable
force thresholds. We evaluate our approach in both simulation and on the
Unitree G1 humanoid across tasks requiring different levels of compliance,
including gentle hugging, sit-to-stand assistance, and safe object
manipulation. Compared to baselines, our policy consistently reduces peak
contact forces while maintaining task success, resulting in smoother and more
natural interactions. These results highlight a step toward humanoid robots
that can safely and effectively collaborate with humans and handle objects in
real-world environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [120] [A convolutional neural network deep learning method for model class selection](https://arxiv.org/abs/2511.03743)
*Marios Impraimakis*

Main category: eess.SY

TL;DR: A novel deep convolutional neural network method for model class selection using only response data, without needing input information or full system identification, with optional Kalman filter enhancement for signal fusion.


<details>
  <summary>Details</summary>
Motivation: To develop a method that can select model classes from response-only data, eliminating the need for system input information or complete system identification, which is valuable for structural health monitoring applications.

Method: Uses a one-dimensional convolutional neural network trained on responses from a unique degree of freedom along with class information. Optional enhancement with Kalman filter to fuse acceleration and displacement data using kinematics constraints.

Result: The method successfully selects model classes even with slight signal variations due to damping or hysteresis behavior, working on both linear and nonlinear dynamic systems, as well as a 3D building finite element model.

Conclusion: The approach provides a powerful tool for structural health monitoring applications by enabling model class selection from response-only data with robustness to signal variations and applicability across different system types.

Abstract: The response-only model class selection capability of a novel deep
convolutional neural network method is examined herein in a simple, yet
effective, manner. Specifically, the responses from a unique degree of freedom
along with their class information train and validate a one-dimensional
convolutional neural network. In doing so, the network selects the model class
of new and unlabeled signals without the need of the system input information,
or full system identification. An optional physics-based algorithm enhancement
is also examined using the Kalman filter to fuse the system response signals
using the kinematics constraints of the acceleration and displacement data.
Importantly, the method is shown to select the model class in slight signal
variations attributed to the damping behavior or hysteresis behavior on both
linear and nonlinear dynamic systems, as well as on a 3D building finite
element model, providing a powerful tool for structural health monitoring
applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [121] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: This paper evaluates NLP tokenization models for assembly code analysis, examining how tokenizer choices impact vocabulary size, semantic coverage, and downstream task performance like function signature prediction.


<details>
  <summary>Details</summary>
Motivation: Tokenization is fundamental in assembly code analysis but remains underexplored, with significant impact on vocabulary size, semantic coverage, and downstream task performance.

Method: Systematic evaluation of various tokenization models using intrinsic metrics (tokenization efficiency, vocabulary compression, representational fidelity) and extrinsic evaluation with pre-trained models (Llama 3.2, BERT, BART) on assembly code tasks.

Result: Tokenizer choice significantly influences downstream performance, with intrinsic metrics providing partial but incomplete predictability of extrinsic outcomes. Complex trade-offs exist between intrinsic tokenizer properties and practical utility.

Conclusion: The study provides valuable insights for optimizing tokenization models in assembly code analysis, contributing to more robust and scalable NLM-based binary analysis workflows.

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [122] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: LLMs show cultural bias despite language diversity, with responses systematically favoring values from Netherlands, Germany, US, and Japan, regardless of prompt language or cultural framing.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can represent cultural diversity given their broad global user base and documented imbalances in training data and optimization objectives.

Method: Probed 10 LLMs with 63 items from Hofstede Values Survey Module and World Values Survey, translated into 11 languages, with prompts formulated with and without explicit cultural perspectives.

Result: Prompt language and cultural perspective produce variation in LLM outputs, but models show systematic bias toward values from specific countries. Cultural framing improves alignment with human values more than targeted language.

Conclusion: LLMs occupy an uncomfortable middle ground - responsive enough to produce variation but too anchored to specific cultural defaults to adequately represent cultural diversity.

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [123] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: LLMs can replicate human decision-making patterns in game theory experiments, with some models closely matching human cooperation behaviors while others align with rational choice predictions.


<details>
  <summary>Details</summary>
Motivation: To understand how closely LLMs mirror actual human decision-making, as misalignment could produce harmful outcomes in practical applications and failure to replicate human behavior makes LLMs ineffective for social simulations.

Method: Developed a digital twin of game-theoretic experiments with systematic prompting and probing framework for machine-behavioral evaluation, testing three open-source models (Llama, Mistral, Qwen).

Result: Llama reproduced human cooperation patterns with high fidelity, capturing human deviations from rational choice theory, while Qwen aligned closely with Nash equilibrium predictions. Achieved population-level behavioral replication without persona-based prompting.

Conclusion: Appropriately calibrated LLMs can replicate aggregate human behavioral patterns and enable systematic exploration of unexplored experimental spaces, offering a complementary approach to traditional social science research that generates new empirical predictions.

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [124] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: Jr. AI Scientist is an autonomous AI system that mimics novice researcher workflow, analyzes paper limitations, formulates hypotheses, conducts experiments, and writes papers, outperforming fully automated systems but still having limitations.


<details>
  <summary>Details</summary>
Motivation: To understand current capabilities and risks of AI Scientist systems for ensuring trustworthy AI-driven scientific progress while preserving academic integrity.

Method: Developed Jr. AI Scientist that follows a defined research workflow using modern coding agents for complex implementations, with evaluation through AI Reviewers, author-led assessments, and submissions to Agents4Science venue.

Result: Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems, but limitations were identified from evaluations indicating risks of direct application.

Conclusion: Current AI Scientist systems show progress but have important limitations and risks that need addressing for future development, providing insights into current state and challenges.

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [125] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: The paper reframes query ambiguity in natural language interfaces to tabular data as cooperative interaction, developing a framework to distinguish resolvable cooperative queries from uncooperative ones.


<details>
  <summary>Details</summary>
Motivation: Current approaches treat query ambiguity as a deficiency, but the authors argue it should be viewed as a feature of cooperative interaction where query specification is shared between user and system.

Method: Developed a principled framework to classify queries as cooperative (resolvable) vs uncooperative (unresolvable), then analyzed queries in 15 popular tabular datasets to evaluate current evaluation practices.

Result: Found that current datasets mix query types in uncontrolled ways, making them inadequate for evaluating both execution accuracy and interpretation capabilities of systems.

Conclusion: The framework shifts perspective from fixing ambiguity to embracing cooperation in query resolution, enabling more informed design and evaluation of natural language interfaces for tabular data.

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [126] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: DR.WELL is a decentralized neurosymbolic framework for cooperative multi-agent planning that uses symbolic planning and a two-phase negotiation protocol to enable robust coordination without requiring detailed trajectory alignment.


<details>
  <summary>Details</summary>
Motivation: Cooperative multi-agent planning faces challenges with partial information, limited communication, and brittle coordination at the trajectory level where small timing deviations can cause conflicts.

Method: Two-phase negotiation protocol: agents first propose candidate roles with reasoning, then commit to joint allocation under consensus and constraints. After commitment, agents independently generate symbolic plans for their roles without revealing detailed trajectories, using a shared world model that encodes current state.

Result: Experiments on cooperative block-push tasks show improved task completion rates and efficiency, with the dynamic world model capturing reusable patterns and enabling evolving collaboration strategies, though with some time overhead.

Conclusion: Symbolic planning enables higher-level operations that are reusable, synchronizable, and interpretable, avoiding brittle step-level alignment while supporting adaptive collaboration across episodes.

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [127] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: VeriCoT is a neuro-symbolic method that extracts and verifies formal logical arguments from Chain-of-Thought reasoning using first-order logic and automated solvers to identify flawed reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs using Chain-of-Thought reasoning cannot reliably verify their own logic, even when reaching correct answers, which undermines trust in high-stakes scenarios.

Method: Extracts CoT reasoning steps into first-order logic, identifies premises grounded in source context/commonsense/prior steps, uses symbolic representation for automated verification, and enables human/system identification of ungrounded reasoning.

Result: Experiments on ProofWriter, LegalBench, and BioASQ show VeriCoT effectively identifies flawed reasoning and serves as strong predictor of final answer correctness. Also enables inference-time self-reflection, supervised fine-tuning, and preference fine-tuning with verification-based rewards.

Conclusion: VeriCoT improves reasoning validity and accuracy by providing formal verification of CoT reasoning, enabling better trust and performance in high-stakes applications.

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [128] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: A survey of modern RLHF alignment methods beyond text, covering multi-modal alignment, cultural fairness, and low-latency optimization.


<details>
  <summary>Details</summary>
Motivation: To address critical gaps in current alignment research and synthesize the new frontier beyond canonical text-based RLHF methods.

Method: Systematic review of foundational algorithms (PPO, DPO, GRPO) and detailed analysis of latest innovations in multi-modal alignment, cultural fairness, and optimization.

Result: Comparative synthesis of alignment techniques and identification of open challenges in the field.

Conclusion: This work provides an essential roadmap for building more robust, efficient, and equitable AI systems through advanced alignment methods.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [129] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: Efficient synthetic data generation method for meta-learning decision trees that achieves comparable performance to real data pre-training while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Decision trees are crucial in high-stakes fields due to interpretability, but meta-learning them requires large datasets which are computationally expensive to generate using optimal trees.

Method: Samples near-optimal decision trees synthetically to create large-scale datasets, then uses MetaTree transformer architecture for meta-learning.

Result: Achieves performance comparable to pre-training on real-world data or with computationally expensive optimal decision trees.

Conclusion: This approach significantly reduces computational costs, enhances data generation flexibility, and enables scalable meta-learning of interpretable decision tree models.

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [130] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: DartQuant is an efficient distribution-aware rotational calibration method that accelerates quantization of large models by constraining activation distributions and using QR-Orth optimization, achieving 47x speedup and 10x memory savings for 70B models.


<details>
  <summary>Details</summary>
Motivation: End-to-end fine-tuning of rotational optimization algorithms for quantization incurs high computational costs and is prone to overfitting, making it impractical for large-scale models in resource-constrained environments.

Method: Proposes DartQuant with distribution-aware rotational calibration that constrains activation distributions after rotation, and introduces QR-Orth optimization scheme to replace expensive alternating optimization with efficient solutions.

Result: Achieves 47x acceleration and 10x memory savings for rotational optimization on 70B models. Successfully completes rotational calibration for 70B model on single 3090 GPU, making large language model quantization feasible in resource-limited settings.

Conclusion: DartQuant provides an efficient solution for quantizing large-scale models by reducing computational complexity and overfitting risks through distribution-aware rotational calibration and QR-Orth optimization, enabling practical deployment in resource-constrained environments.

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [131] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: This paper benchmarks post-training quantization methods for LLMs using the new MXFP4 format, identifies incompatibility issues with rotation-based approaches, analyzes the root cause, and proposes a block rotation strategy to adapt these methods to MXFP4.


<details>
  <summary>Details</summary>
Motivation: Large language models are becoming prohibitively expensive to deploy due to their scale, and while post-training quantization offers a solution, existing methods are designed for INT4 formats. The emergence of MXFP4 hardware support raises questions about whether current PTQ techniques work effectively with this new format.

Method: The authors establish a comprehensive benchmark of PTQ methods under MXFP4 format, systematically evaluate existing approaches, analyze the root cause of incompatibility with rotation-based methods, and propose a block rotation strategy that adapts rotation-based methods to work with MXFP4's power-of-two block scaling.

Result: GPTQ consistently performs well with MXFP4, but rotation-based methods suffer severe incompatibility. The analysis reveals a fundamental mismatch between MXFP4's PoT block scaling and the global rotation used by state-of-the-art approaches. The proposed block rotation strategy substantially improves accuracy across diverse LLMs.

Conclusion: The findings provide clear guidance for practitioners on PTQ method selection for MXFP4 and establish a foundation for advancing PTQ research under emerging low-precision formats. The proposed block rotation effectively resolves the incompatibility issue between rotation-based methods and MXFP4.

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [132] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: Current uncertainty quantification methods for LLMs fail on ambiguous data, performing close to random despite working well on unambiguous tasks. The authors create ambiguous QA datasets with ground-truth distributions and show theoretical limitations of existing approaches.


<details>
  <summary>Details</summary>
Motivation: Real-world language is inherently ambiguous, but existing UQ methods are only benchmarked on unambiguous tasks, creating a gap between research and practical deployment needs.

Method: Introduce MAQA* and AmbigQA* datasets with ground-truth answer distributions from factual co-occurrence, and evaluate three UQ paradigms: predictive distribution, internal representations, and model ensembles.

Result: All current uncertainty estimators degrade to near-random performance on ambiguous data, with theoretical analysis showing fundamental limitations of predictive-distribution and ensemble-based methods under ambiguity.

Conclusion: Current UQ methods have a critical shortcoming in handling ambiguity, requiring a rethinking of modeling paradigms for trustworthy LLM deployment in real-world scenarios.

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [133] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: SynthKGQA is a framework for generating synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, enabling better benchmarking and training of KG retrievers.


<details>
  <summary>Details</summary>
Motivation: There's a lack of challenging QA datasets with ground-truth targets for graph retrieval, making comparison of methods difficult.

Method: Developed SynthKGQA framework to generate synthetic KGQA datasets from any Knowledge Graph, providing full ground-truth facts for reasoning. Applied it to Wikidata to create GTSQA dataset.

Result: Enables more informative benchmarking of KG retrievers and allows training better models. Created GTSQA dataset to test zero-shot generalization of KG retrievers.

Conclusion: SynthKGQA provides a solution for generating high-quality synthetic datasets that improve KG retriever evaluation and training, addressing the challenge of limited ground-truth data in graph retrieval.

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [134] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: Common-O benchmark reveals major gap between multimodal models' perception performance and real-world reasoning ability, with best models achieving only 35% on reasoning across scenes.


<details>
  <summary>Details</summary>
Motivation: To address the gap between multimodal models' strong performance on perception benchmarks and their poor real-world reasoning, particularly hallucinations when reasoning about scenes.

Method: Built Common-O benchmark with 10.5k examples using new images not in web training data, probing reasoning across scenes by asking 'what's in common?' inspired by cognitive tests.

Result: Best model achieves only 35% on Common-O and 1% on Common-O Complex; models hallucinate more when similar objects are present; multi-image trained models show bigger improvements than scaling alone.

Conclusion: Reasoning across scenes remains very challenging despite perception saturation; multi-image training shows promise; benchmark released to address hallucination in scene reasoning.

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [135] [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)
*NVIDIA,:,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping,Boxin Wang,Zhuolin Yang,Nayeon Lee,Shaokun Zhang,Fuxiao Liu,Zhiqi Li,Di Zhang,Greg Heinrich,Hongxu,Yin,Song Han,Pavlo Molchanov,Parth Mannan,Yao Xu,Jane Polak Scowcroft,Tom Balough,Subhashree Radhakrishnan,Paris Zhang,Sean Cha,Ratnesh Kumar,Zaid Pervaiz Bhat,Jian Zhang,Darragh Hanley,Pritam Biswas,Jesse Oliver,Kevin Vasques,Roger Waleffe,Duncan Riach,Oluwatobi Olabiyi,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Pritam Gundecha,Khanh Nguyen,Alexandre Milesi,Eugene Khvedchenia,Ran Zilberstein,Ofri Masad,Natan Bagrov,Nave Assaf,Tomer Asida,Daniel Afrimi,Amit Zuker,Netanel Haber,Zhiyu Cheng,Jingyu,Xin,Di,Wu,Nik Spirin,Maryam Moosaei,Roman Ageev,Vanshil Atul Shah,Yuting Wu,Daniel Korzekwa,Unnikrishnan Kizhakkemadam Sreekumar,Wanli Jiang,Padmavathy Subramanian,Alejandra Rico,Sandip Bhaskar,Saeid Motiian,Kedi Wu,Annie Surla,Chia-Chih Chen,Hayden Wolff,Matthew Feinberg,Melissa Corpuz,Marek Wawrzos,Eileen Long,Aastha Jhunjhunwala,Paul Hendricks,Farzan Memarian,Benika Hall,Xin-Yu Wang,David Mosallanezhad,Soumye Singhal,Luis Vega,Katherine Cheung,Krzysztof Pawelec,Michael Evans,Katherine Luna,Jie Lou,Erick Galinkin,Akshay Hazare,Kaustubh Purandare,Ann Guan,Anna Warno,Chen Cui,Yoshi Suhara,Shibani Likhite,Seph Mard,Meredith Price,Laya Sleiman,Saori Kaji,Udi Karpas,Kari Briski,Joey Conway,Michael Lightstone,Jan Kautz,Mohammad Shoeybi,Mostofa Patwary,Jonathen Cohen,Oleksii Kuchaiev,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron Nano V2 VL is an advanced vision-language model that improves document understanding, video comprehension, and reasoning through architectural enhancements and token reduction techniques.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient model for real-world document understanding, long video comprehension, and reasoning tasks with higher inference throughput.

Method: Builds on Nemotron Nano V2 (hybrid Mamba-Transformer LLM) with innovative token reduction techniques and enhancements in model architecture, datasets, and training recipes.

Result: Significant improvements over previous model Llama-3.1-Nemotron-Nano-VL-8B across all vision and text domains.

Conclusion: The model is released with checkpoints in multiple formats (BF16, FP8, FP4) and shares datasets, recipes, and training code.

Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.

</details>


### [136] [On the Equivalence of Regression and Classification](https://arxiv.org/abs/2511.04422)
*Jayadeva,Naman Dwivedi,Hari Krishnan,N. M. Anoop Krishnan*

Main category: cs.LG

TL;DR: The paper establishes a formal equivalence between regression and classification, showing that regression with M samples on a hyperplane is equivalent to a linearly separable classification task with 2M samples, leading to new regression formulations and a "regressability" measure.


<details>
  <summary>Details</summary>
Motivation: To bridge the formal gap between regression and classification, particularly justifying margin maximization in support vector regression beyond just regularization.

Method: Proving one-to-one equivalence between regression problems and classification tasks, using this equivalence to derive new regression formulations and develop a regressability measure.

Result: Demonstrated that margin maximization on equivalent classification tasks leads to different regression formulations, and showed practical applications including estimating regression difficulty without model training and training neural networks for linearizing maps.

Conclusion: The established equivalence provides theoretical justification for margin-based regression approaches and enables new practical tools for regression analysis and model training.

Abstract: A formal link between regression and classification has been tenuous. Even
though the margin maximization term $\|w\|$ is used in support vector
regression, it has at best been justified as a regularizer. We show that a
regression problem with $M$ samples lying on a hyperplane has a one-to-one
equivalence with a linearly separable classification task with $2M$ samples. We
show that margin maximization on the equivalent classification task leads to a
different regression formulation than traditionally used. Using the
equivalence, we demonstrate a ``regressability'' measure, that can be used to
estimate the difficulty of regressing a dataset, without needing to first learn
a model for it. We use the equivalence to train neural networks to learn a
linearizing map, that transforms input variables into a space where a linear
regressor is adequate.

</details>


### [137] [Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks](https://arxiv.org/abs/2511.04494)
*Alper Kalle,Theo Rudkiewicz,Mohamed-Oumar Ouerfelli,Mohamed Tamaazousti*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Neural networks are widely used for image-related tasks but typically demand
considerable computing power. Once a network has been trained, however, its
memory- and compute-footprint can be reduced by compression. In this work, we
focus on compression through tensorization and low-rank representations.
Whereas classical approaches search for a low-rank approximation by minimizing
an isotropic norm such as the Frobenius norm in weight-space, we use
data-informed norms that measure the error in function space. Concretely, we
minimize the change in the layer's output distribution, which can be expressed
as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is
the square root of the covariance matrix of the layer's input and $W$,
$\widetilde{W}$ are the original and compressed weights. We propose new
alternating least square algorithms for the two most common tensor
decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike
conventional compression pipelines, which almost always require
post-compression fine-tuning, our data-informed approach often achieves
competitive accuracy without any fine-tuning. We further show that the same
covariance-based norm can be transferred from one dataset to another with only
a minor accuracy drop, enabling compression even when the original training
dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50,
and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100)
confirm the advantages of the proposed method.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [138] [Sub-exponential Growth in Online Word Usage: A Piecewise Power-Law Model](https://arxiv.org/abs/2511.04106)
*Hayafumi Watanabe*

Main category: physics.soc-ph

TL;DR: This paper introduces a piecewise power-law model to analyze social diffusion patterns, finding that sub-exponential growth (slower than exponential) is prevalent in about 55% of cases studied using large-scale web data from Japanese blogs and search trends.


<details>
  <summary>Details</summary>
Motivation: To understand the role of sub-exponential growth in social diffusion phenomena, which has been largely overlooked compared to conventional S-shaped models like the logistic curve.

Method: Used a piecewise power-law model to analyze growth curves from approximately one billion Japanese blog articles linked to Wikipedia vocabulary and web search trend data in English, Spanish, and Japanese.

Result: 55% of 2,965 items showed no abrupt jumps and were well captured by one or two segments. Single-segment curves revealed: (i) prevalent sub-exponential growth (alpha ≈ 0.5), (ii) diffusion scale mainly determined by growth rate R, and (iii) alpha varies with topic nature (smaller for niche topics).

Conclusion: Sub-exponential growth is a common pattern in social diffusion, and the proposed model provides a practical framework for describing, comparing, and interpreting diverse growth curves, with alpha serving as an index of outward-oriented communication preference.

Abstract: The diffusion of ideas and language in society has conventionally been
described by S-shaped models, such as the logistic curve. However, the role of
sub-exponential growth -a slower than exponential pattern known in
epidemiology- has been largely overlooked in broader social phenomena. Here, we
present a piecewise power-law model to characterize complex growth curves with
a few parameters. We systematically analyzed a large-scale dataset of
approximately one billion Japanese blog articles linked to Wikipedia
vocabulary, and observed consistent patterns in web search trend data (English,
Spanish, and Japanese). Our analysis of the 2,965 selected items reveals that
about 55% (1,625 items) were found to have no abrupt jumps and were well
captured by one or two segments. For single-segment curves, we found that (i)
the mode of the shape parameter alpha was near 0.5, indicating prevalent
sub-exponential growth; (ii) the ultimate diffusion scale is primarily
determined by the growth rate R, with minor contributions from alpha or the
duration T; and (iii) alpha showed a tendency to vary with the nature of the
topic, being smaller for niche/local topics and larger for widely shared ones.
Furthermore, a micro-behavioral model distinguishing outward contact with
strangers from inward interaction within their community suggests that alpha
can be interpreted as an index of the preference for outward-oriented
communication. These findings suggest that sub-exponential growth is a common
pattern of social diffusion, and our model provides a practical framework for
consistently describing, comparing, and interpreting complex and diverse growth
curves.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [139] [Explorability in Pushdown Automata](https://arxiv.org/abs/2511.04048)
*Ayaan Bedi,Karoliina Lehtinen*

Main category: cs.FL

TL;DR: Explorability is a measure of nondeterminism in pushdown automata that generalizes history-determinism, forming an infinite hierarchy where each level is strictly more expressive than the previous one, with exponential explorability capturing all context-free languages.


<details>
  <summary>Details</summary>
Motivation: To study a more refined measure of nondeterminism in pushdown automata that lies between history-deterministic and fully nondeterministic automata, providing operational meaning to different degrees of nondeterminism.

Method: Define k-explorable PDAs where k concurrent runs suffice to find acceptance, introduce parameterized explorability where the number of runs depends on input length, and analyze the expressiveness and succinctness hierarchy.

Result: Explorable PDAs form a strict hierarchy between history-deterministic and fully nondeterministic PDAs, with exponential explorability capturing context-free languages, and significant succinctness gaps exist between different levels.

Conclusion: Explorability provides a robust and operationally meaningful measure of nondeterminism for pushdown systems, revealing a rich hierarchy with precise expressiveness and succinctness relationships.

Abstract: We study explorability, a measure of nondeterminism in pushdown automata,
which generalises history-determinism. An automaton is k-explorable if, while
reading the input, it suffices to follow k concurrent runs, built step-by-step
based only on the input seen so far, to construct an accepting one, if it
exists. We show that the class of explorable PDAs lies strictly between
history-deterministic and fully nondeterministic PDAs in terms of both
expressiveness and succinctness. In fact increasing explorability induces an
infinite hierarchy: each level k defines a strictly more expressive class than
level k-1, yet the entire class remains less expressive than general
nondeterministic PDAs. We then introduce a parameterized notion of
explorability, where the number of runs may depend on input length, and show
that exponential explorability precisely captures the context-free languages.
Finally, we prove that explorable PDAs can be doubly exponentially more
succinct than history-deterministic ones, and that the succinctness gap between
deterministic and 2-explorable PDAs is not recursively enumerable. These
results position explorability as a robust and operationally meaningful measure
of nondeterminism for pushdown systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [140] [MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation](https://arxiv.org/abs/2511.03942)
*Shih-Lun Wu,Yoon Kim,Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: MIDI-LLM is a language model that generates multitrack MIDI music from text prompts by expanding vocabulary to include MIDI tokens and using two-stage training, achieving better quality and faster inference than Text2midi.


<details>
  <summary>Details</summary>
Motivation: To create a system that can generate multitrack MIDI music from free-form text descriptions, enabling more natural and intuitive music creation through text prompts.

Method: Expands a text LLM's vocabulary to include MIDI tokens and uses a two-stage training approach while preserving the original LLM's parameter structure to leverage vLLM for accelerated inference.

Result: MIDI-LLM achieves higher quality music generation, better text control, and faster inference compared to the recent Text2midi model.

Conclusion: The approach successfully enables text-to-MIDI music generation with improved performance and efficiency, making it a practical tool for music creation from text descriptions.

Abstract: We present MIDI-LLM, an LLM for generating multitrack MIDI music from
free-form text prompts. Our approach expands a text LLM's vocabulary to include
MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI
abilities. By preserving the original LLM's parameter structure, we can
directly leverage the vLLM library for accelerated inference. Experiments show
that MIDI-LLM achieves higher quality, better text control, and faster
inference compared to the recent Text2midi model. Live demo at
https://midi-llm-demo.vercel.app.

</details>
